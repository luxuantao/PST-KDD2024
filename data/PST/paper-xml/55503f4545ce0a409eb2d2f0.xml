<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Versatile, scalable, and accurate simulation of distributed applications and platforms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014-07-10">10 July 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">J</forename><surname>Parallel</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Distrib</forename><surname>Comput</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Henri</forename><surname>Casanova</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information and Computer Sciences</orgName>
								<orgName type="institution">University of Hawai&apos;i at Manoa</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Arnaud</forename><surname>Giersch</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">FEMTO-ST</orgName>
								<orgName type="institution">University of Franche-Comté</orgName>
								<address>
									<settlement>Belfort</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Arnaud</forename><surname>Legrand</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">LIG</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Grenoble University</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Martin</forename><surname>Quinson</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">LORIA</orgName>
								<orgName type="institution">Université de Lorraine</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Frédéric</forename><surname>Suter</surname></persName>
							<email>frederic.suter@cc.in2p3.fr</email>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">IN2P3 Computing Center</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">IN2P3</orgName>
								<address>
									<settlement>Lyon-Villeurbanne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="laboratory">LIP</orgName>
								<orgName type="institution" key="instit1">INRIA</orgName>
								<orgName type="institution" key="instit2">ENS Lyon</orgName>
								<address>
									<settlement>Lyon</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution" key="instit1">IN2P3 Computing Center</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">IN2P3</orgName>
								<address>
									<settlement>Lyon-Villeurbanne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Versatile, scalable, and accurate simulation of distributed applications and platforms</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2014-07-10">10 July 2014</date>
						</imprint>
					</monogr>
					<idno type="MD5">A6EE5D4BBF468459632CC9FF107B2BF8</idno>
					<idno type="DOI">10.1016/j.jpdc.2014.06.008</idno>
					<note type="submission">Received 6 September 2013 Received in revised form 10 June 2014 Accepted 19 June 2014</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Simulation Validation Scalability Versatility SimGrid</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>h i g h l i g h t s</head><p>• We provide a presentation of the improvements done in SimGrid in the last 10 years.</p><p>• We rebut popular wisdom that specialization allows for ''better'' simulation.</p><p>• We claim that versatility leads to better accuracy and better scalability.</p><p>• We back up this claim with multiple use cases and (in)validation studies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The use of parallel and distributed computing platforms is pervasive in a wide range of contexts and for a wide range of applications. High Performance Computing (HPC) has been a consumer of and driver for these platforms. In particular, commodity clusters built from off-the-shelf computers interconnected with switches have been used for applications in virtually all fields of science and engineering, and exascale systems with millions of cores are already envisioned. Platforms that aggregate multiple clusters over wide-area networks, or grids, have received a lot of attention over the last decade with both specific software infrastructures and application deployments. Distributed applications and platforms have also come to prominence in the peer-to-peer and volunteer computing domains (e.g., for content sharing, scientific computing, data storage and retrieval, media streaming), enabled by the impressive capabilities of personal computers and high-speed personal Internet connections. Finally, cloud computing relies on the use of large-scale distributed platforms that host virtualized resources leased to consumers of compute cycles and storage space.</p><p>While large-scale production platforms have been deployed and used successfully in all these domains, many open questions remain. Relevant challenges include resource management, resource discovery and monitoring, application scheduling, data management, decentralized algorithms, electrical power management, resource economics, fault-tolerance, scalability, and performance. Regardless of the specific context and of the research question at hand, studying and understanding the behavior of applications on distributed platforms is difficult. The goal is to assess the quality of competing algorithmic and system designs with respect to precise objective metrics. Theoretical analysis is typically tractable only when using stringent and ultimately unrealistic assumptions. As a result, relevant research is mostly empirical and proceeds as follows. An experiment consists in executing a software application on a target hardware platform. We use the term ''application'' in a broad sense here, encompassing a parallel scientific simulation, a peer-to-peer file sharing system, a cloud computing brokering system, etc. The application execution on the platform generates a time-stamped trace of events, from which relevant metrics can be computed (e.g., execution time, throughput, power consumption). Finally, research questions are answered by comparing these metrics across multiple experiments.</p><p>One can distinguish three classes of experiments. In in vivo experiments an actual implementation of the application is executed on a real-world platform. Unfortunately, real-world production platforms may not be available for the purpose of experiments. Even if a testbed platform is available, experiments can only be conducted for (subsets of) the platform configuration at hand, limiting the range of experimental scenarios. Finally, conducting reproducible in vivo experiments often proves difficult due to changing workload and resource conditions. An alternative that obviates these concerns is in vitro experiments, i.e., using emulation (e.g., virtual machines, network emulation). A problem with both in vivo and in vitro experiments is that experiments may be prohibitively time consuming. This problem is exacerbated not only by the need to study long-running applications but also by the fact that large numbers of experiments are typically needed to obtain results with reasonable statistical significance. Furthermore, when studying large-scale applications and platforms, commensurate amounts of hardware resources are required. Even if the necessary resources are available, power consumption considerations must be taken into account: using large-scale platforms merely for performance evaluation experiments may be an unacceptable expense and a waste of natural resources. The third approach consists in running (an abstraction of) the application in silico, i.e., using simulation. This approach is typically less labor-intensive, and often less costly in terms of hardware resources, when compared to in vivo or in vitro experiments. Consequently, it should be no surprise that many published results in the field are obtained in silico.</p><p>Two key concerns for simulation are accuracy (the ability to run in silico experiments with no or little result bias when compared to their in vivo counterparts) and scalability (the ability to run large and/or fast in silico experiments). A simulator relies on one or more simulation models to describe the interaction between the simulated application and the simulated platform. There is a widely acknowledged trade-off between model accuracy and model scalability (e.g., an analytical model based on equations may be less accurate than a complex event-driven procedure but its evaluation would also be less memory-and CPU-intensive). Simulation has been used in some areas of Computer Science for decades, e.g., for microprocessor and network protocol design, but its use in the field of parallel and distributed computing is less developed. While the scalability of a simulator can be easily quantified, evaluating its accuracy is painstaking and time consuming. As a result, published validation results often focus on a few scenarios, which may be relevant to a particular scope, instead of engaging in a systematic and critical evaluation methodology. Consequently, countless published research results are obtained with simulation methods whose accuracy is more or less unknown.</p><p>An important observation is that simulators used by parallel and distributed computing researchers are domain-specific (e.g., peer-to-peer simulators, grid simulators, HPC simulators). In some cases, domain-specificity is justified. For instance, wireless networks are markedly different from wired networks and in this work, for instance, we only consider wired networks. But, in general, many simulators are developed by researchers for their own research projects and these researchers are domain experts, not simulation experts. The popular wisdom seems to be that developing a versatile simulator that applies across domains is not a worthwhile endeavor because specialization allows for ''better'' simulation, i.e., simulations that achieve a desirable trade-off between accuracy and scalability. In this work, we rebut popular wisdom and claim that, when developing a simulation framework, aiming for versatility is the way to achieve better accuracy and better scalability. Our main contribution is that we confirm this claim by synthesizing the experience gained during the 10-year development of the SimGrid discrete-event simulation framework, presenting results relating to both simulation design and simulation implementation. Some of these results have been previously published in conference proceedings, as referenced hereafter, while others are novel contributions.</p><p>The rest of this article is organized as follows. Section 1 presents related work. Section 2 discusses the current design and design goals of SimGrid. Sections 3 and 4 explain how striving for versatility has led to advances in accuracy and scalability, respectively. While these sections include several short case studies, Section 5 presents a full-fledged case study in the HPC domain. Finally, Section 6 concludes the paper with a brief summary of findings and with perspectives on future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Related work</head><p>In this section we discuss popular simulators that have been used in the last decade and whose goal is to enable ''fast'' simulation of grid, cloud, peer-to-peer, volunteer, or HPC applications and platforms, meaning that the simulation time (i.e., the runtime of the in silico experiment) should be orders of magnitude faster than the simulated time (i.e., the simulated runtime of the application). Most of these simulators share the same design with three components: (i) simulation models; (ii) platform specification; and (iii) application specification. Simulation models are used to implement the evolution of simulated application activities (computations, data transfers) that use simulated resources (compute devices, network elements, storage devices) throughout simulated time. More specifically, given all the application activities that use a set of resources, resource models are used to compute the completion date of the activity that completes the earliest and the progress made by all other activities by that date. Platform specification mechanisms allow users to instantiate platform scenarios without having to modify the simulation models or the simulation's implementation. Each resource must be described using an instantiated simulation model, and resources can be connected together (e.g., a particular set of network links and routers is used for end-to-end communication between two compute resources). Application specification refers to the set of mechanisms and abstractions for users to describe the nature and sequence of activities that must be simulated. Existing simulators provide many options for </p><formula xml:id="formula_0">✓ ✓ ✓ ✓ ✓ ✓ ✓ LogGOPSim [44] ✓ ✓ ✓ ✓ ✓ BigSim [87] ✓ ✓ ✓ ✓ ✓ ✓ ✓ MPI-SIM [2] ✓ ✓ ✓ ✓ ✓ ✓ OptorSim [8] ✓ ✓ ✓ ✓ ✓ ✓ ✓ GridSim [15] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ GroudSim [62] ✓ ✓ ✓ ✓ ✓ ✓ ✓ CloudSim [16] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ iCanCloud [61] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ SimBA [75] ✓ ✓ ✓ ✓ ✓ EmBOINC [31] ✓ ✓ ✓ ✓ ✓ SimBOINC [51] ✓ ✓ ✓ ✓ ✓ ✓ PeerSim [59] ✓ ✓ ✓ OverSim [5] ✓ ✓ ✓ ✓ ✓ SimGrid [18] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓</formula><p>application specification ranging from abstract finite automata to actual application implementations.</p><p>In Section 1.1, we introduce the most prominent and relevant simulation frameworks. We then discuss the design choices in these simulators, and the rationales for these choices, for each of the three components above in Sections 1.2-1.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related simulators</head><p>The simulation of parallel and distributed applications has received a fair amount of attention in the literature. Many simulators have been developed that employ a wide range of simulation techniques. Table <ref type="table" target="#tab_0">1</ref> summarizes the key features of prominent simulators most relevant to this work. The content of each row is based on the content of published research articles as well as on our own source code inspection whenever available. While some of these simulators are actively used, others seem to be more or less inactive at the time this article is being written. For each simulator we indicate the research community from which it has emerged, the way in which it models the simulated application, and the type of models it uses to simulate network, compute, and storage resources. Details on the particular models are given throughout the rest of this article.</p><p>The simulation of parallel applications on parallel computing platforms has a long history in HPC, in particular in the context of applications based on MPI (Message Passing Interface) <ref type="bibr" target="#b40">[41]</ref>. Two main approaches are used: off-line and on-line simulation. In offline simulation, a time-stamped log of computation and communication events is first obtained by running the application on a real platform. The simulator then replays this sequence of events as if they were occurring on another platform with different hardware characteristics. We list the three most representative such simulators in Table 1 <ref type="bibr" target="#b77">[79,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b85">87]</ref> but several others are available (e.g., <ref type="bibr" target="#b84">[86,</ref><ref type="bibr" target="#b78">80]</ref>). One issue with off-line simulation is that event logs are tied to a particular application execution (e.g., number of processors, block size, data distribution schemes) so that a new log must be obtained for each simulation scenario. However, extrapolation is feasible as proposed for instance in LogGOPSim <ref type="bibr" target="#b43">[44]</ref>. The alternative to off-line simulation is on-line simulation in which actual application code is executed on a host platform that attempts to mimic the behavior of the target platform. Part of the instruction stream is intercepted and passed to a simulator. Many on-line simulators have been developed with various features and capabilities (e.g., <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b52">53]</ref>). In these simulators, the amounts of hardware resources required to run the simulated application on the host platform are commensurate to (or in fact larger than) those needed to run the actual application on the target platform. Simulation scalability is thus achieved by ''throwing more hardware'' at the problem. In this work we limit our scope to simulations that can be executed on a single computer, so that simulation scalability must be achieved in software. Due to this fundamental difference between SimGrid and on-line HPC simulators, we only show one such simulator in Table <ref type="table" target="#tab_0">1</ref> (MPI-SIM <ref type="bibr" target="#b1">[2]</ref>).</p><p>Simulators have also been developed in the area of grid computing, most of which are only intended for use by their own developers. Some were made available to the community but proved short-lived, such as OptorSim <ref type="bibr" target="#b7">[8]</ref> (shown in the table) or ChicSim <ref type="bibr" target="#b67">[68]</ref>. Besides SimGrid, the other simulator widely used in grid computing research is GridSim <ref type="bibr" target="#b14">[15]</ref>. More recently, simulators have been proposed for simulating cloud computing platforms and applications. GroudSim <ref type="bibr" target="#b61">[62]</ref> is a framework that enables the simulation of both grid and cloud systems. CloudSim <ref type="bibr" target="#b15">[16]</ref> builds on the same simulation internals as GridSim but exposes specific interfaces for simulating systems that support cloud services. iCanCloud <ref type="bibr" target="#b60">[61]</ref> has been specifically developed to simulate cloud platforms and applications.</p><p>Simulators have also been developed for simulating volunteer computing systems, i.e., systems that consist of large numbers of individually owned and volatile hosts. BOINC <ref type="bibr" target="#b8">[9]</ref> is the most popular volunteer computing infrastructure today, and these simulators attempt to simulate (parts of) BOINC's functionalities. In fact, BOINC itself embeds in its source code a simple time-driven simulator for running the actual client scheduler code in simulation mode. The SimBA simulator <ref type="bibr" target="#b74">[75]</ref> models BOINC clients as finitestate automata based on probabilistic models of availability, and makes it possible to study server-side scheduling policies in simulation. The same authors later developed EmBOINC <ref type="bibr" target="#b30">[31]</ref>. Unlike SimBA, EmBOINC executes actual BOINC production code to emulate the BOINC server. SimBOINC <ref type="bibr" target="#b50">[51]</ref> goes further and simulates the full BOINC system by linking the BOINC code with SimGrid, thus allowing for multiple servers and for the simulation of client-side scheduling.</p><p>Another area in which simulators have been developed is peerto-peer computing <ref type="bibr" target="#b3">[4]</ref>. Most of these simulators trade off accuracy for scalability, so as to make it possible to simulate up to millions of peers. For instance, it is common to simulate network transfers as fixed delays since message count is a useful metric to evaluate peer-to-peer systems. PeerSim <ref type="bibr" target="#b58">[59]</ref> is likely the most widely used simulators for theoretical peer-to-peer studies, and relies on simplistic but scalable simulation models. OverSim <ref type="bibr" target="#b4">[5]</ref> relies on the OMNeT++ <ref type="bibr" target="#b79">[81]</ref> discrete-event simulation kernel for implementing more realistic packet-level network simulation. Several other simulators have emerged, such as P2PSim <ref type="bibr" target="#b37">[38]</ref> or Planet-Sim <ref type="bibr" target="#b35">[36]</ref>, which have been short-lived and are no longer maintained. It is thus difficult to say whether more recent proposals, e.g., D-P2P-Sim <ref type="bibr" target="#b70">[71]</ref>, will perdure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Simulation models</head><p>Many simulation models for compute, network, and storage resources have been proposed in the literature, ranging from simple mathematical equations to complex processes. For instance, for a hard drive, access time could be modeled as a seek time plus the data size divided by a bandwidth, or instead emerge from a detailed discrete-event simulation that accounts for platters, sectors, blocks, hardware/software buffers, file system overheads, etc. At one extreme the first model would be very scalable but likely inaccurate, while at the other extreme the second model would be unscalable but (hopefully) accurate. In general, different models achieve different trade-offs between the time it takes to evaluate them and the level of detail with which they capture the behavior of physical resources.</p><p>In what follows we discuss the simulation models implemented as part of state-of-the-art simulators of parallel and distributed computing applications. For CPU and storage models, there is a strong consensus among the simulators: the vast majority are at one extreme (simplistic analytical models) for both types of resources. By contrast, the design space is much larger for network resource models and we see more diversity in state-of-the-art simulators. This larger design space provides more of an opportunity to seek models that achieve judicious trade-offs between accuracy and scalability (as seen in Section 3). This is why the discussions of CPU and storage resource models (Sections 1.2.1 and 1.2.2) are much shorter than the discussion of network models (Section 1.2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.1.">CPU models</head><p>The high-accuracy, low-scalability extreme for simulating CPUs is cycle-level (also called cycle-accurate) simulation, which has been used occasionally in the HPC context <ref type="bibr" target="#b52">[53]</ref> but precludes fast simulations due to CPU-intensiveness. Furthermore, it is laborintensive to instantiate a cycle-level simulator to simulate a precise architecture. To complicate matters, it has been shown that in some instances cycle-level simulators do not necessarily lead to accurate results, or at least not as accurate as one might expect <ref type="bibr" target="#b83">[85]</ref>. For these reasons, the common approach used by all simulators in Table <ref type="table" target="#tab_0">1</ref> is to employ a simple analytical model of compute delay. More specifically, task execution times are computed by dividing a compute cost (e.g., number of instructions) by a compute speed (e.g., number of instructions per time unit), with possibly a random component. The compute speed can be instantiated for various simulated resources based on benchmark results obtained on corresponding real-world resources. This model can lead to reasonable results for simple CPU-bound computation and can even be used to model simple multi-core parallelization of computation. But, in general, it is limited because it does not account for architecture-specific features of the simulated compute resource (memory hierarchy, CPU architecture, GPU architecture, on-chip buses and networks, etc.). While one could envision more sophisticated analytical models that capture some of these features without resorting to cycle-level simulation, designing such models is an open research question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.2.">Storage models</head><p>It may seem surprising that only a few simulators in Table <ref type="table" target="#tab_0">1</ref> provide a notion of simulated storage resources. We hypothesize that the reason why storage resource simulation is rarely done is twofold. First, not all users require simulation of storage resources and it is assumed that those who do can implement their own storage simulation models. Second, accurate modeling of storage resources such as hard drives and solid-state drives is known to be extremely challenging. The high-accuracy, low-scalability option is the discrete-event simulation of storage resources as for instance done in the DiskSim simulator <ref type="bibr" target="#b12">[13]</ref>. This simulator models the operation of the storage hardware precisely and could serve as a basis for implementing a storage system simulator that models other hardware components (e.g., buses and networks) and software components (e.g., file systems). This is the approach used in <ref type="bibr" target="#b59">[60]</ref>, for instance, which targets fine-grain discrete-event simulation of storage area networks. However, this approach is rarely used when simulating parallel and distributed applications due to long simulation times and because correctly instantiating such complex models is difficult.</p><p>A few simulators from the grid computing and cloud computing domains provide simple storage access time models. For instance GridSim, CloudSim, and SimGrid model data access times using a simple model based on a (fixed or randomly generated) seek time and a fixed data transfer rate. This model is not truly representative of real storage resources since caching, locality, and file system effects are not captured. These effects are known to be performance drivers but are also known to make the accurate analytical modeling of storage systems an open question. Among the simulators in Table <ref type="table" target="#tab_0">1</ref>, iCanCloud provides the most sophisticated model: it considers individual disk blocks and simulates seek times based on the locality of block accesses. Instantiating such a model in a realistic way is, however, non-trivial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.3.">Network models</head><p>All simulators of parallel and distributed applications implement some model for the simulated platform's network. The simulators listed in Table <ref type="table" target="#tab_0">1</ref> are diverse in their approach to network modeling. One network simulation approach that is acknowledged to have high accuracy because it captures most real-world phenomena is packet-level simulation. Packet-level simulators implement full-fledged network protocols and are used extensively for network research (e.g., the ns-3 simulator [78]). The issue with packet-level simulation in our context, that is the simulation of large-scale and perhaps long-running parallel and distributed applications, is the lack of scalability due to long simulation times, which can be orders of magnitude larger than simulated time <ref type="bibr" target="#b33">[34]</ref>. As a result, packet-level simulation is not usable for typical grid/cloud computing simulations (some authors have used it for simulating HPC applications on clusters <ref type="bibr" target="#b62">[63]</ref> but with long simulation times). Some of the simulators in Table <ref type="table" target="#tab_0">1</ref> provide packet-level simulation as an option. For instance SimGrid does provide an interface to ns-3. Users who can tolerate long simulation times may then benefit from more accurate network simulations when needed. But the vast majority of users need fast simulations, which can only be achieved by using analytical, and thus potentially less accurate, network models. Some simulators implement analytical models that by design ignore network phenomena that are deemed irrelevant to the target simulation domain. For instance, PeerSim ignores bandwidth effects because it is designed for simulations with many small messages and for users who care about message counts more than about data transfer rates. Likewise, volunteer computing simulators such as SimBA or EmBOINC only model network latencies. GroudSim does not model network contention that may happen in the core of the network and simply assumes that each host is bandwidth-limited by its own Internet connection. Network contention is also ignored by most HPC simulators such as LogGOPSim, BigSim, or MPI-SIM. The LogGOPSim authors simply state that the platforms they target have sufficient network provisioning so that contention does not occur. Such design choices severely limit the versatility of the simulator, but these simulators are admittedly not designed for versatility and they can claim accuracy for those scenarios for which they were designed.</p><p>The accuracy of a simulator can be evaluated by confronting simulation results to a ground truth. While experiments on a real network can provide this ground truth, these experiments are typically limited to a few network configurations. Instead, a more feasible approach is to use results obtained with a packet-level simulator as the ground truth. Many published works that propose a simulator include a section devoted to evaluating the accuracy of the network model. Unfortunately, most of these evaluations are either merely qualitative or consist in exhibiting a few ''good cases'' in which simulation results lead to reasonable quantitative trends. Few direct comparisons to packet-level simulations or real executions are actually attempted.</p><p>Given the above, it is fair to say that the majority of simulation results published in the area of parallel and distributed computing are of unknown and thus questionable validity. Even using popular simulators, it is often straightforward to construct simple and relevant use cases for which plainly invalid results are obtained <ref type="bibr" target="#b81">[83]</ref>. Some authors are explicit about the validity limitations of their simulators. For instance, the authors of GroudSim acknowledge in <ref type="bibr" target="#b61">[62]</ref> that their bandwidth sharing model is flawed when competing flows have heterogeneous bottleneck bandwidth constraints, which unfortunately is a common case in real-world networks. Similarly, the authors of OptorSim document in <ref type="bibr" target="#b7">[8]</ref> that in their network model the bandwidth share that each flow receives on a congested network link does not take into account the fact that some of these flows may be limited by other links in their paths. For both these simulators the implication is a waste of available network bandwidth when compared to real networks but, at least, the authors provide a sense of how much trust should be put into simulation results.</p><p>In many cases the validity limits of a simulator are undocumented. This is the case for the GridSim <ref type="bibr" target="#b14">[15]</ref> grid simulator and its follow-up cloud computing simulator, CloudSim <ref type="bibr" target="#b15">[16]</ref>. These simulators are extremely popular in their communities (e.g., the Grid-Sim distribution has been downloaded 20,000 times since 2007 or over 10 times a day). These simulators have thus been the basis for hundreds of published articles as well as for other recent simulators <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b48">49]</ref>. CloudSim builds on GridSim to provide a simulator for cloud computing. In <ref type="bibr" target="#b15">[16]</ref> the authors state the following rationale: ''Since SimJava and GridSim have been extensively utilized in conducting cutting edge research in Grid resource management by several researchers, bugs that may compromise the validity of the simulation have been already detected and fixed''. Unfortunately, this claim is simply unrealistic given how rarely true validation studies are attempted. And indeed, a quick inspection of both GridSim's and CloudSim's code suggests very simple invalidating cases <ref type="bibr" target="#b81">[83]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Platform specification</head><p>Once models of resources have been chosen, it is necessary to (i) instantiate each resource model with appropriate parameters; and (ii) describe the interconnections of the resources.</p><p>The instantiation of resource models for individual resources is done by specifying a few parameters. For the simulators in Table 1, the CPU resource model takes one parameter (the computation rate, in FLOPS or MIPS), the storage resource model takes up to two parameters (seek time and I/O bandwidth), and the network resource model takes up to two parameters (link latency and link bandwidth). These parameters can be chosen as constants or sampled from relevant probability distributions. These model instantiations can be provided by users either via text description files or via a programmatic interface. A text interface offers a clear separation between the simulated application and the specification of the simulated execution environment. A programmatic interface provides increased expressiveness power since repetitive patterns can get generated from compact programmed descriptions.</p><p>Given a set of resources, each instantiated with a model, a platform description must list all network-reachable elements (hosts, routers, links) and the topological interconnections of these elements, i.e., allowed network paths. Most simulators allow us to interconnect links and hosts by expressing one-hop routes, and then route messages using the shortest path on the topology graph. This approach is scalable because shortest paths can be computed in polynomial time and one only needs to store the topological graph. However, it may be inaccurate because in real networks routing exhibits irregularities (e.g., asymmetric paths). A more accurate solution is to specify explicit routing tables, making it possible to describe more realistic networks, thus placing a higher burden on the user. This approach is less scalable because routing tables must be stored and routes computed from these tables, leading to potentially large memory and CPU requirements for simulating large-scale networks. As discussed in Section 4.2 it is possible to exploit repetitive patterns so as to allow for platform descriptions that are both accurate and scalable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.">Applications specification</head><p>Given a fully specified platform, one must express how its resources are used by the simulated application. There are three main approaches: (i) off-line simulation; (ii) formal description; and (iii) programmatic description.</p><p>Off-line simulation consists in replaying event traces captured during the execution of the application on a real platform. This approach is commonplace in the HPC community for simulating the execution of MPI applications. It may not be scalable as event traces can be large, but it describes the execution of the application accurately since the trace is generated from a real execution. A drawback is that in many cases researchers do not have access to an actual application implementation, and in fact they may want to explore various application design schemes in simulation before committing to developing such an implementation.</p><p>The second approach does not require an application implementation, but instead consists in developing a formal description of the application execution, e.g., as finite automata. This is the approach used for instance by PeerSim and SimBA. These particular simulators opt for a formal description because it is compact and thus affords scalability. However, this description can be too constraining since complex application logic may be too difficult to describe within such a rigid formalism. In this case the formal description may be only an approximation of the actual application to be simulated.</p><p>Most simulators follow the third approach, by which users describe simulated applications programmatically as sets of functions/methods that describe Concurrent Sequential Processes (CSP). The description is thus still scalable because it is compact, and is more accurate than formal descriptions based on automata. For this reason, PeerSim provides such a programmatic description as an alternative to automata. Once programmatically described, the simulated application can then be executed by virtualizing each simulated process into one execution context. Several technologies can be used in this view, the most natural approach being the encapsulation of each simulated process into a thread, as done in GridSim for instance. The use of threads suffers from scalability limitations. These limitations can be alleviated by using continuations instead of threads (see Section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SimGrid design and objectives</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Software stack</head><p>Fig. <ref type="figure" target="#fig_0">1</ref> shows the main components in the design of SimGrid and depicts some of the key concepts in this design. The top part of the figure shows the three APIs through which users can develop simulators. The MSG API allows users to describe a simulated application as a set of concurrent processes. These processes execute code implemented by the user (in C, C++, Java, Lua, or Ruby), and place MSG calls to simulate computation and communication activities. The SMPI API is also used to simulate applications as sets of concurrent processes, but these processes are created automatically from an existing application written in C or Fortran that uses the MPI standard. SMPI also includes a runtime system, not shown in the figure, that implements necessary MPI-specific functionalities (e.g., process startup, collective communications). MSG thus makes it possible to simulate any arbitrary application, while SMPI makes it possible to simulate existing, unmodified MPI applications. The mechanisms for simulating the concurrent processes for both MSG and SMPI are implemented as part of a layer called SIMIX, which is a kernel (in the Operating Systems sense of the term) that provides process control and synchronization abstractions. The set of concurrent processes is depicted in the SIMIX box in the figure. All processes synchronize on a set of condition variables, also shown in the figure. Each condition variable corresponds to a simulated activity, computation or data transfer, and is used to ensure that concurrent processes wait on activity completions to make progress throughout (simulated) time. The third API, SimDAG, does not use concurrent processes but instead allows users to specify an abstract task graph of communicating computational tasks with noncyclic dependencies.</p><p>Regardless of the API used, the simulation application consists of a set of communication and computation activities which are to be executed on simulated hardware resources. Compute resources are defined in terms of compute capacities (e.g., CPU cycles per time unit). They are interconnected via a network topology that comprises network links and routing elements, defined by bandwidth capacities and latencies. All resources can be optionally associated with time-stamped traces of available capacity values including possible downtime. An example specification of available resources is depicted in the bottom-right of Fig. <ref type="figure" target="#fig_0">1</ref>, highlighting three network links (L 1 , L 2 , L m ) and one compute resource (P 1 ).</p><p>The simulation core, i.e., the component that simulates the execution of activities on resources, is called SURF and is shown in the bottom-left of the figure. Each activity is defined by a total amount of work to accomplish (e.g., number of CPU cycles to execute, number of bytes to transfer) and a remaining amount of work. When its remaining amount of work reaches zero the activity completes, signaling the corresponding SIMIX condition variable or resolving a task dependency in SimDAG. Activity i corresponds to a variable, x i , which represents a resource share used by the activity. A set of constraints over these variables, with one constraint per simulated resource, then describes how the activities compete for using the resources. An example is shown in the figure. The righthand side of each constraint is a resource capacity, denoted by C x where x is a given resource (e.g., C L 2 is the capacity of network link L 2 ). The first activity requires performing 435 units of work, 372 of which remains to be performed at the current simulated date, and is associated with the variable x 1 . The variable x 1 participates in two constraints, for resources L 2 and L 1 , meaning that in this example the first activity is a data transfer that uses network links L 2 and L 1 , among others. The third activity is also a data transfer, with 50 units of work remaining out of 664. This transfer uses links L 1 and L m , meaning that it shares the bandwidth capacity of L 1 with the first activity (as seen in the x 1 + x 3 C L 1 constraint). The n-th activity uses links L 2 and L m , and its corresponding variable x n thus appears in those two constraints. Finally, the second activity in this example corresponds to a computation and its variable x 2 appears in the constraint x 2 C P 1 , showing that this resource is not shared with any other compute activity. Note that the second and n-th activities in this example have yet to begin as their remaining works are equal to their total works. Based on these constraints the simulation core computes resource allocations so as to optimize a relevant objective function, as explained in upcoming sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Accuracy, scalability, and versatility</head><p>We have seen that the two primary concerns of the users of a simulator are accuracy and scalability. These two concerns typically conflict and the simulators mentioned in Section 1 often explicitly trade off one for the other. Nevertheless, throughout the history of the SimGrid project we have striven to improve both accuracy and scalability. The primary motivation has been to achieve a third objective, versatility. It is important to distinguish versatility from genericity. A selling point of many simulators is that their design is generic. This is arguably always desirable as it makes it possible to augment/replace functionality within the same overall software design (e.g., the network model is accessed via a well-defined interface so that new models can be implemented and integrated). SimGrid does provide some genericity, which we view simply as good software engineering. By contrast, versatility implies that the simulator's implementation provides the necessary capabilities to run simulations for multiple domains accurately and scalably. In this sense, many simulators discussed in the previous section are generic but not versatile.</p><p>A simulator that aims for versatility must provide simulation models that subsume and improve on models used by domainspecific simulators; it must allow for the description of arbitrary distributed applications to cover a spectrum of domains (e.g., from HPC to peer-to-peer computing); and it must allow for the description of arbitrary simulated platforms in which resources can be described by a range of instantiated models. To provide such capabilities, a simulator should not only provide different simulation model implementations but should also be designed with versatility in mind, which has led us to use the design shown in Fig. <ref type="figure" target="#fig_0">1</ref>. The key aspect of this design, which in hindsight may seem natural but is not necessarily used by the simulators reviewed in Section 1, is the complete separation between simulated resource specification, simulated application execution, and resource sharing models.</p><p>SimGrid, at least its most recent versions, has achieved versatility and is used in domains including cluster, grid, cloud, volunteer, and peer-to-peer computing, as well as various other distributed computing settings. The natural expectation was that aiming for versatility would have detrimental effects on accuracy and scalability, or at least on the trade-off between the two. Our main observation is that, instead, striving for versatility has been key for improving both accuracy and scalability. In fact, we contend that several of the improvements we have achieved would not have been possible without considering versatility as a primary objective. As a side-effect, increased versatility has provided a stronger motivation to improve accuracy and scalability, since improvements translate to multiple simulation domains and thus to a larger user community. The next two sections describe several accuracy and scalability advances made in SimGrid, emphasizing the role of the versatility design objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Versatile yet accurate simulations</head><p>SimGrid uses a unified model for simulating the execution of activities on simulated resources. This model is purely analytical so as to afford scalability by avoiding cycle-, block-, and packetlevel simulation of compute, storage, and network resource usage.</p><p>Formally, given a resource r, and a set of simulated activities, A, the model specifies the following constrained Max-Min optimization problem:</p><formula xml:id="formula_1">Maximize min a∈A ϱ a under constraints   a∈A using resource r ϱ a C r ,<label>(1)</label></formula><p>where C r denotes the capacity of resource r, and ϱ a denotes the resource share allocated to activity a. Solving this optimization problem, which boils down to solving a linear system, yields instantaneous resource shares given which resources are used by which activities. Given these computed resource shares at simulated time t 0 , for all simulated resources, the SURF component of SimGrid computes the first activity that will complete, advances the simulated clock to that time, say t 1 , removes the completed activity from consideration, accounts for the progress of each activity given its resource shares and the simulated elapsed time t 1 -t 0 , and possibly adds newly created activities.</p><p>The optimization problem in Eq. ( <ref type="formula" target="#formula_1">1</ref>) is at the core of the SURF component of SimGrid (see Fig. <ref type="figure" target="#fig_0">1</ref>), which implements efficient algorithms and data structures to solve the corresponding linear system quickly (see Section 4.1). The key aspect of this model is that it is general and can be used to simulate CPU, storage, and network resources.</p><p>Regarding CPU resources, we have seen that relevant simulators in Table <ref type="table" target="#tab_0">1</ref> all use a simple analytical model by which the CPU is shared fairly among concurrent simulated compute activities. Fair sharing is subsumed by the optimization problem in Eq. (1) (maximizing the minimum of n resource shares that all sum to C r leads to all shares equal to C r /n). SimGrid, like other simulators, also allows the notion of compute priorities, so that resource shares are scaled by normalized priorities in Eq. <ref type="bibr" target="#b0">(1)</ref>.</p><p>The analytical models for storage resources used by the simulators in Table <ref type="table" target="#tab_0">1</ref> (but for iCanCloud, which uses block-level simulation) use fair sharing of disk bandwidth, with optionally an extra fixed seek time. Like for CPU resources, the optimization problem in Eq. ( <ref type="formula" target="#formula_1">1</ref>) can be used to simulate fair sharing of disk bandwidth. The seek time is added as a fixed initial delay when advancing the simulation clock.</p><p>For both CPU and storage resources, SimGrid thus complies with the analytical simulation models used by state-of-the-art simulators. These models are simplistic (see Sections 1.2.1 and 1.2.2) but developing more accurate models is an open question, which we do not address in this work. By contrast, there is a clear opportunity when simulating network resources. In Section 1.2.3 we have seen that state-of-the-art analytical network simulation models used by the simulators in Table <ref type="table" target="#tab_0">1</ref> can lead to documented or undocumented invalid behaviors, even for simple simulation scenarios. It is thus a fair question to ask whether accurate analytical network models are even feasible, and if not then one is left with unscalable packet-level simulation. It turns out that the level of detail provided by packet-level simulation is not necessary for studying large-scale applications that exchange large amounts of data. This provides an opportunity to develop an analytical network model that is scalable and accurate, within reasonable and identifiable limits, thereby bridging the accuracy gap between inaccurate scalable analytical models and accurate unscalable packet-level models. In what follows we discuss how this opportunity is seized in SimGrid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">An empirically informed model of TCP for moderate size grids</head><p>Analytical ''flow-level'' models have been proposed in the networking literature, mostly to study the theoretical behavior of TCP protocols. Inspired be these developments, in SimGrid we use a flow-level model for the purpose of network simulation. In a flow-level network model the individual packets of an end-to-end communication are abstracted into a single entity, a flow, which is characterized by a data transfer rate, or bandwidth. This bandwidth depends on the network topology and on the interactions with other ongoing network flows. It is assumed that the flows have reached steady-state, and the goal is to define analytical bandwidth sharing models that capture the bandwidth sharing behaviors of actual network protocols. In the context of SimGrid, we have focused on TCP as it is ubiquitous in grids and wide-area networks, but also in clusters. Eq. ( <ref type="formula" target="#formula_1">1</ref>) corresponds to a popular bandwidth sharing model, Max-Min fairness <ref type="bibr" target="#b9">[10]</ref>, by which the bandwidth allocation is such that increasing the allocation of any flow would necessarily require decreasing the allocation of a less favored flow. A single network flow can use multiple network links, and the flow is allocated the same bandwidth on all the links it traverses. As a result, we can write a network-specific version of Eq. ( <ref type="formula" target="#formula_1">1</ref>) as</p><formula xml:id="formula_2">Maximize min f ∈F ϱ f , under constraints  ∀l ∈ L,  f ∈F going through l ϱ f B l ,<label>(2)</label></formula><p>where L is the set of all network links, l ∈ L denotes a network link with bandwidth capacity B l , F is the set of all simulated network flows, and f denotes a flow with assigned bandwidth ϱ f .</p><p>The above model is simple to implement, has low computational complexity, and as a result is implemented in several simulators <ref type="bibr" target="#b85">[87,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b38">39]</ref>. Unfortunately, it is known that TCP does not implement Max-Min fairness <ref type="bibr" target="#b20">[21]</ref>. As a result different and more sophisticated bandwidth sharing models have been proposed <ref type="bibr" target="#b53">[54]</ref>. In <ref type="bibr" target="#b81">[83]</ref>, we have described how we improved on standard Max-Min fairness in several ways via the use of additional parameters, and we have shown that this model compares favorably with the models in <ref type="bibr" target="#b53">[54]</ref>. These improvements were achieved thanks to a thorough invalidation study that highlighted key characteristics of TCP that are not captured by the simple Max-Min model. Most published simulator evaluation studies focus on demonstrating ''good'' results for particular cases. Instead we followed the critical method <ref type="bibr" target="#b64">[65]</ref>, which places model invalidation at the center of the scientific endeavor, thus striving to exhibit ''bad'' cases so as to understand and hopefully extend the validity limits of our simulation models.</p><p>The first weakness of the formulation in Eq. ( <ref type="formula" target="#formula_2">2</ref>) is that it does not account for TCP's flow control mechanism, which is known to prevent full bandwidth usage as flows may be limited by large latencies <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b47">48]</ref>. Therefore, the transmission rate ϱ f of a flow f should be bounded by W max /RTT f where W max is the configured maximum congestion window size and RTT f is the round trip time (RTT) of the packets in the flow. Additionally, the sophisticated Adaptive Increase Multiplicative Decrease congestion window mechanism of TCP leads to RTT-unfairness <ref type="bibr" target="#b54">[55]</ref>. In versions of TCP like Reno, two flows contending on the same bottleneck link receive bandwidth shares inversely proportional to their RTTs. This behavior can be captured by modifying the constraints in Eq. <ref type="bibr" target="#b1">(2)</ref>. Finally, flow throughput can be dramatically affected by reverse-traffic <ref type="bibr" target="#b42">[43]</ref>. Such a phenomenon is generally very poorly captured by flow-level models <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b72">73</ref>]. Yet, simple modifications of the constraints in Eq. ( <ref type="formula" target="#formula_2">2</ref>) make it possible to capture throughput degradation due to reverse-traffic, at least locally. All these improvements are described in detail in <ref type="bibr" target="#b81">[83]</ref> and precursor articles referenced therein.</p><p>The improved model provides more accurate bandwidth shares, but simulating a flow requires a model of the flow's execution time given its bandwidth share. The common approach is to model the execution time of a flow that transfers S bytes of data as the latency plus S divided by the bandwidth:</p><formula xml:id="formula_3">T f (S) = ℓ f + S/ϱ f ,<label>(3)</label></formula><p>where ϱ f is the bandwidth share computed by the bandwidth sharing model and ℓ f is the end-to-end latency. This model was shown to lead to good result only for large data sizes on the order of 10 MiB <ref type="bibr" target="#b33">[34]</ref>. This is because TCP's slow-start behavior is not captured. While there is no hope for a flow-level model to capture this behavior perfectly, a more accurate empirical model can be derived:</p><formula xml:id="formula_4">T improved = αℓ f + S βϱ f ,<label>(4)</label></formula><p>where α and β (typically α ∈ <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15]</ref> and β ∈ [0.8, 1], depending on the version of TCP <ref type="bibr" target="#b80">[82,</ref><ref type="bibr" target="#b81">83]</ref>) are two additional positive real parameters. Packet-level simulations are used to calibrate parameter values, i.e., to determine the parameter values that minimize modeling error for a set of synthetic simulation scenarios. We found the model to be accurate for data sizes as low as 100 KiB, i.e., about two orders of magnitude smaller than previously achieved. For smaller data sizes the flow-level model leads to transfer times that are too short. Below this limit the assumption that the transfer time is a linear function of the data size breaks down because data is exchanged as discrete network packets. Users wanting to simulate small-size data transfers over wide-area networks have two options: either configure SimGrid to use the ns-3 packet-level simulator or account for optimistic simulated transfer times when drawing conclusions from simulation results obtained with the above model. Overall, results in <ref type="bibr" target="#b80">[82,</ref><ref type="bibr" target="#b81">83]</ref> show that the flow-level model, once improved with the above additional parameters, can capture key characteristics at the macroscopic level of TCP, allows us to account for slow start (to some extent), protocol overhead, RTT-unfairness, reverse-traffic, and flow-control limitation, and leads to better results than the simplified model used in earlier versions of SimGrid <ref type="bibr" target="#b18">[19]</ref>. This simplified model had already led to drastic improvements compared to the simulators discussed in Section 1.2.3 but its limitations motivated the development of the improved model <ref type="bibr" target="#b33">[34]</ref>.</p><p>As an illustration, Fig. <ref type="figure" target="#fig_2">2</ref> compares typical outcomes of invalidation studies presented in <ref type="bibr" target="#b81">[83]</ref>. One hundred 100 MiB transfers are launched between random pairs of endpoints for several dozens of randomly generated network topologies such as the ones depicted in Fig. <ref type="figure" target="#fig_2">2(a)</ref>. With the improved model, most scenarios lead to good accuracy, as seen for instance in Fig. <ref type="figure" target="#fig_2">2(b</ref>). Only a few situations remain, as that shown in Fig. <ref type="figure" target="#fig_2">2(c</ref>), where we seem to reach the limits of the flow-level approximation. These situations occur for highly contended scenarios (i.e., with extremely small latencies and low bandwidth capacities). In these scenarios the high error is due to the discrete nature of the TCP protocol, which, by design, is not captured by the flow-level approximation.</p><p>Our flow-level network model was originally designed for the grid computing domain. However, with the above improvements it is applicable to other scenarios (e.g., under-provisioned networks, applications that exchange as few as a few hundreds of KiB of data), thereby making SimGrid more versatile.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Extending the model for HPC simulations</head><p>The improved model described in the previous section expands the versatility of the simulator toward a broader range of wide-area networks, provided the simulated applications exchange messages on the order of a few hundred KiB. At the other end of the spectrum, many users wish to simulate cluster platforms that consist of compute nodes connected via (a hierarchy of) switches. The goal is to simulate a single cluster, or to simulate intra-cluster phenomena in a grid or cloud platform. In these settings the communication workload often comprises many small messages that consist of a few KiB or even only a few bytes.</p><p>Our improved model fails to capture some fundamental aspects of cluster interconnects with TCP and popular MPI implementations, e.g., OpenMPI <ref type="bibr" target="#b34">[35]</ref> or MPICH2 <ref type="bibr" target="#b39">[40]</ref>, over Gigabit Ethernet switches. For instance, a message under 1 KiB fits within an IP frame, in which case the achieved data transfer rate is higher than for larger messages although latency is generally also larger. More importantly, implementations for MPI_Send typically switch from buffered to synchronous mode above a certain message size. The former involves an extra data copy, while the latter avoids it because copying large amounts of data has high overhead. This ''protocol switching'' feature is seen in both OpenMPI and MPICH2. Due to such effects, instead of being a linear function of message size as in Eq. ( <ref type="formula" target="#formula_4">4</ref>), communication time is rather piece-wise linear. Furthermore, depending on the mechanism, communications may be overlapped or not by computations or other communications, which ideally the simulation model should capture. Such synchronization and overlapping aspects can be partially accounted for by the classical LogP family of models <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b45">46]</ref>, and in particular LogGPS <ref type="bibr" target="#b45">[46]</ref>. Fig. <ref type="figure" target="#fig_3">3</ref> shows elapsed time vs. message size, using logarithmic scales for both axes, obtained from an experiment conducted with OpenMPI 1.6 on the graphene cluster of the Grid'5000 experimental testbed. This cluster comprises 144 2.53 GHz Intel Xeon Quad-core X3440 nodes spread across four cabinets, and interconnected by a hierarchy of 10 GB Ethernet switches. A full description of the interconnection network is available online. <ref type="foot" target="#foot_0">1</ref> The measurements were obtained as follows. To avoid measurement bias the message size is exponentially and randomly sampled between 1 byte and 100 MiB, for two types of experiments: ''ping'' and ''ping-pong''. The ping experiments aim at measuring the time spent in MPI_Send (resp. MPI_Recv) by ensuring that the receiver (resp. sender) is always ready to communicate. The ping-pong experiment consists in sending a message and immediately receiving a message of the same size, which allows us to measure the transmission delay. The goal is to study the behavior of MPI from the application's point of view, without any a priori assumptions about the way in which the MPI implementation switches between communication protocols depending on message size, which is in general difficult to determine based solely on the MPI configuration parameters.</p><p>Protocol switching effects are clearly seen in Fig. <ref type="figure" target="#fig_3">3</ref>. For messages below 32 KiB fully asynchronous communication is used, for messages above 256 KiB fully synchronous communication is used, and in between partially synchronous or ''detached'' communication is used. Each protocol leads to elapsed times that can be accurately modeled through linear regression. Up to three linear regressions, however, should be used for asynchronous communication depending on message size. For instance, a separate mode is required to capture accurately the case when the message is small enough to fit inside a single TCP frame. Overall, we have five distinct modes (modes 2 and 3 are almost identical for the MPI_Send() and the Ping-Pong results), for an overall behavior that is discontinuous and piece-wise linear. The simple linear model in the previous section would be reasonably accurate for small and large messages, but largely inaccurate in between (for more than 30% average error overall, with a worst case at 127%). Likewise, the classical LogP models <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b45">46]</ref> do not model the piece-wise linear behavior accurately. The closest contender would be LogGPS <ref type="bibr" target="#b45">[46]</ref>, but it distinguishes between only two kinds of message sizes (small and large).</p><p>In <ref type="bibr" target="#b22">[23]</ref> we have proposed a (non-necessarily continuous) piecewise linear simulation model that can be instantiated for an arbitrary number of linear segments. This model was extended later in <ref type="bibr" target="#b6">[7]</ref> to include the overlapping and synchronization aspects, as modeled by LogGPS. This new piece-wise linear model makes it possible to simulate accurately applications that use a wide range of message sizes <ref type="bibr" target="#b6">[7]</ref>. The accuracy improvements due to piecewise linear model rather than the linear model are large enough to justify the increased number of parameters (2 parameters per mode). In addition, the values of these parameters are easily determined via straightforward experiments. 2 Furthermore, results in <ref type="bibr" target="#b22">[23]</ref> show that the instantiation of the piece-wise linear model is robust: the instantiation computed on one cluster can be reused accurately for modeling other clusters with similar interconnect technology but different compute nodes. While these results are for two machines connected to the same switch, other results also show that our approach remains reasonably accurate when applied to a sequence of switches. This is important because large compute clusters are often organized as networks of switches.</p><p>An important implication of our accurate piece-wise linear model of point-to-point communication is that, when combined with the bandwidth sharing model described in the previous section, it leads to an immediate simulation model for collective communication operations. The collective operations are accessible to the SimGrid user via the SMPI API (Fig. <ref type="figure" target="#fig_0">1</ref>). Just like any MPI implementation, the SMPI runtime implements collective communications as sets of point-to-point communications that may contend with each other on the network. This is to be contrasted with monolithic modeling of collective communications, as done in <ref type="bibr" target="#b77">[79]</ref> for instance. These monolithic models rely on coarse approximations to model contention and/or on extensive calibration experiments that must be performed for each type of collective operation. Instead SimGrid provides implementations of collective communications based on current versions of both OpenMPI and MPICH. The current version of the SMPI runtime (v3.10) thus makes <ref type="bibr" target="#b1">2</ref> The R code which allows us to extract model parameters from a set of such measurements is available at http://mescal.imag.fr/membres/arnaud.legrand/ research/smpi/smpi_loggps.php. it possible to simulate the execution of MPI applications while accounting for network topology and contention in high-speed TCP networks. Extensions to other kind of topologies and other network technologies are underway. Results in <ref type="bibr" target="#b6">[7]</ref> show that SimGrid can simulate collective communications effectively and has consistently a better predictive power than classical LogP-based models for a wide range of scenarios including both established HPC benchmarks and real applications. Section 5 presents a case study of the simulation of a full-fledged HPC application that performs many collective operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Extending the model for large networks</head><p>In a view to increasing the versatility of SimGrid, a worthwhile goal is to be applicable to peer-to-peer and volunteer computing domains in which hundreds of thousands of hosts on large widearea networks must be simulated. Several simulators in that domain opt for packet-level simulation, e.g., OverSim <ref type="bibr" target="#b4">[5]</ref>, which can model contention in the network (close to the peers) as well as the network distance between the peers (which some peerto-peer applications exploit). Our coarse-grain network model is applicable to these large network, within the limits identified in Section 3.1, and is more scalable than fine-grain packet-level models.</p><p>Regardless of whether one uses packet-level simulation or our model, one challenge is the instantiation of the simulation. It is not always possible for a user to instantiate a large network topology description in which every link is assigned a realistic latency and bandwidth value. It turns out that for many peer-to-peer and volunteer computing simulations there is no need to instantiate a complete network topology. Popular peer-to-peer simulators, such as PeerSim <ref type="bibr" target="#b58">[59]</ref>, implement instead a fixed-delay model by which a communication between two peers takes a fixed amount of time, or latency, which is deemed sufficient in terms of accuracy and has scalability advantages as well. A popular refinement of this model is to account for peer proximity by embedding the peers into a multi-dimensional space and using the distance between two points in this space as an estimate of the latency between two peers. Solutions have been proposed to realize such embedding in practice, such as the well-known decentralized Vivaldi <ref type="bibr" target="#b24">[25]</ref> system. Our network model, as described in Section 3.1 can be used to simulate end-to-end latencies based on coordinates generated by systems like Vivaldi.</p><p>There is a large gap between the packet-level approach and the fixed-delay, coordinate-based approach. In particular, the latter does not account for network contention even at the peers themselves because it does not model bandwidths. SimGrid attempts to bridge that gap with a model that accounts for both end-to-end latencies and bandwidths, while still abstracting away the details of network topology so that the model is easily instantiable.</p><p>Previous research has shown that bandwidth at the edges of the network reflects the bandwidth available on full end-to-end paths. In other words, bandwidth bottlenecks are located within only a few hops of Internet end-points. For instance, Hu et al. <ref type="bibr" target="#b44">[45]</ref> show that 60% of wide-area end-to-end paths hit a bandwidth bottleneck in the first or second hop. Similar findings have been reported for broadband access networks <ref type="bibr" target="#b27">[28]</ref>. In <ref type="bibr" target="#b5">[6]</ref> it is found that most end-to-end paths are limited by bandwidth at the end-points on the PlanetLab testbed <ref type="bibr" target="#b21">[22]</ref>. These observations suggest a network model, which we term the ''last mile'' model, in which each host</p><p>x is described by two bandwidths: an upload bandwidth β out x and a download bandwidth β in x . A communication from a host x to a host y is then allocated bandwidth β xy = min(β out x , β in y ). Note that this model does not capture the fact that two end-points may be on the same local network, in which case the bandwidth available between these two end-points would be orders of magnitude larger than β xy as computed above.</p><p>The network model in Section 3.1 can be trivially extended to support the last mile model. Furthermore, previous work shows that this model can be instantiated in practice. In <ref type="bibr" target="#b5">[6]</ref>, a decentralized algorithm is proposed that instantiates the model based on end-to-end bandwidth measurements on a real-world platform. Using a 308-host PlanetLab dataset for which full endto-end bandwidth measurements are available, the model achieves good accuracy using a small number of measurements (each host only performs bandwidth measurements with 16 other hosts). In the end, the model is simple, instantiable, and more accurate than the fixed-delay model. Furthermore, it is with the network model in SimGrid, thus further increasing versatility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Versatile yet scalable simulations</head><p>Striving to make SimGrid more versatile (so that it can be used for, e.g., exascale HPC simulations as well as peer-to-peer simulations) has led us to tackling the scalability challenge along several directions. Scalability is preconditioned on the use of an analytical simulation model, such as that described in Section 3. But three major scalability concerns, both in terms of memory footprint and CPU time, remain: (i) the efficiency of the implementation of the simulation model; (ii) the description of large platforms; and (iii) the simulation of large numbers of concurrent processes. In the next three sections we describe how SimGrid addresses these three concerns. We also provide quantitative comparisons to state-ofthe-art domain-specific simulators for relevant case studies in the areas of volunteer computing, grid computing, and peer-to-peer computing. Section 5 demonstrates the scalability and accuracy of SimGrid simulations via a full-fledged case study in the HPC domain. Together, these case studies illustrate how our scalability solutions developed for specific domains can in fact be combined and applied to different domains. In other words, striving to make the simulator more versatile leads to scalability improvements across the board.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Efficient simulation kernel</head><p>As explained in Section 3, the base simulation model in SimGrid relies on a steady-state assumption to compute resource shares allocated to pending simulated activities. As a result, each time the set of these activities changes (a new activity is started, a current activity completes), the resource shares must be reevaluated, which amounts to solving a linear system of equations (Eq. ( <ref type="formula" target="#formula_1">1</ref>)). The computed resource shares are then used to determine (i) by how much the simulated clock should be advanced and (ii) the progress of each pending activity.</p><p>Since simulated activities dynamically appear and disappear during the simulation, our implementation uses an ad hoc dynamic and sparse data structure to represent Eq. ( <ref type="formula" target="#formula_1">1</ref>). The Max-Min fairness algorithm is straightforward, but it iterates several times over a dynamic set of variables. We first developed an implementation that was optimal in terms of number of operations, but it suffered from poor L1 and L2 cache reuse. To obtain an efficient cacheoblivious implementation we split the data structure into half so as to group together the few fields that are heavily used by the bandwidth sharing algorithm in contiguous arrays. The use of arrays instead of linked lists improves locality and hence the prefetch efficiency. This trimming of the data structures leads to improved cache utilization, and thus lowers simulation time, at the cost of significantly increased implementation complexity. Besides this data structure optimization, we have also implemented two algorithmic optimizations motivated by actual large-scale simulation scenarios <ref type="bibr" target="#b28">[29]</ref>, as detailed in the next two paragraphs.</p><p>Lazy activity updates. Originally, SimGrid was intended for the simulation of applications that comprise many communicating tasks running on computers connected by hierarchical networks. In this setting any event related to a simulated activity or resource can impact a large fraction of the other simulated activities and resources. However, when simulating large-scale platforms, such as those used for peer-to-peer or volunteer computing applications, most activities are independent of each other. In this case, reevaluating the full model becomes a performance bottleneck because all activities are examined even though many can simply be ignored most of the time. Our approach is thus to avoid solving the whole linear system in Eq. ( <ref type="formula" target="#formula_1">1</ref>) by only recomputing the parts of it that are likely to be impacted by newly arrived or newly terminated activities. Furthermore, if between two resolutions of the linear system only a few variables have changed, then only the state of the corresponding activities needs to be updated. Using a heap as a future event set, and efficiently detecting the set of variables that are impacted by activity removal and addition, we are able to lower the computational complexity of the simulation significantly. We term this technique ''lazy updates'', since the state of a simulated activity is modified only when needed.</p><p>Trace integration. Our second efficiency improvement targets the management of resources whose capacities change frequently. In SimGrid, the user can specify the capacity of a resource as a time-stamped trace to simulate fluctuating availability due to some out-of-band load (a capacity of zero means a downtime). The linear system in Eq. ( <ref type="formula" target="#formula_1">1</ref>) must be reevaluated each time the capacity of a resource changes. In extreme scenarios, many such reevaluations may occur before a single activity completes, which would slow the simulation down unnecessarily. For instance, let us consider a situation in which the capacity of a resource is specified to change 100 times according to a user-specified time-stamped trace. Furthermore, let us assume that all pending activities still have large amounts of remaining work so that the earliest activity completion occurs after the 100th resource capacity change. In this case, 100 reevaluations of the linear system would take place even though it would be possible to perform a single reevaluation. More formally, given current remaining work amounts, one can compute the next activity completion date given all future resource capacity values before this date. This computation can be performed efficiently using ''trace integration''. Essentially, instead of storing a trace as capacity values, one stores its integral. Finding the last resource capacity change before the next activity completion can then be performed using a binary search, i.e., with logarithmic time complexity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case study: volunteer computing simulation</head><p>The scalability enhancements described in this section were initially motivated by the need to simulate large volunteer computing systems efficiently <ref type="bibr" target="#b28">[29]</ref>. Let us consider a volunteer computing scenario with N hosts. Each host computes sequentially P tasks, and the compute rate of each host changes T times before the completion of the simulation. With our original implementation the time complexity of this simulation is O(N 2 (P + T )). With lazy activity updates it becomes O(N(P +T ) log N), and O(NP(log(N)+log(T )))</p><p>when adding trace integration. We have implemented such a simulation, using traces of MFlop/sec rates for SETI@home hosts available from <ref type="bibr" target="#b51">[52]</ref>. Compute tasks have uniformly distributed random compute costs in MFlop between 0 and 8.10 12 (i.e., up to roughly one day for a median host). Note that such simulation scenarios are commonplace when studying volunteer computing, and in fact this particular scenario was suggested to us by the authors of <ref type="bibr" target="#b41">[42]</ref> to highlight scalability issues in previous versions of SimGrid. Fig. <ref type="figure" target="#fig_4">4</ref> shows simulation time measured on a 2.2 GHz AMD Opteron processor vs. N for the initial design, the addition of lazy activity updates, and the addition of trace integration, using a logarithmic scale on the vertical axis. Results make it plain that both proposed improvements decrease simulation time dramatically. For instance, for a simulation with 2560 hosts, the simulation time is almost at 3 h without the enhancements, around 1 min with lazy updates, and under 10 s with lazy updates and trace integration. A comparison with the state-of-the-art SimBA simulator <ref type="bibr" target="#b74">[75]</ref>, based on timing results published therein and the use of a similar benchmark machine, shows that with our improvements Sim-Grid achieves simulation times more than 25 times faster. We refer the interested reader to <ref type="bibr" target="#b28">[29]</ref> for more details on the experimental setup. This is an important result given that SimGrid is more versatile than SimBA. In fact, the behaviors of the network and the software are simulated in much more detail in SimGrid (i.e., flow-level model of TCP, programmatic specification) than in SimBA (i.e., fixed latency, finite automata).</p><p>The trace integration mechanism is specific to CPU simulation, but the lazy update mechanism applies across all resources and activities. To quantify the impact of lazy updates on the speed of network simulation, Fig. <ref type="figure">5</ref> shows simulation times when simulating various numbers of flows (10, 50, 100, or 150) that are opened and closed at random dates between random pairs of nodes for 1000 s of simulated time, for three representative platforms. A randomized factorial set of experiments with 50 measurements for each combination is run on a 3.3 GHz Core i7 processor and we report the 95% confidence intervals of the average time needed to perform the simulation (platform description parsing time is not included). The first platform consists of 1740 independent hosts each with its own upstream link and downstream link, using the ''last mile'' model discussed in Section 3.3. When peer A communicates with peer B, a network flow using the upstream link of A and the downstream link of B is created and the latency of this flow is computed from the link latencies and the Vivaldi <ref type="bibr" target="#b24">[25]</ref> network coordinates of the peers. This platform is thus very loosely coupled, and as such we see in the leftmost graph in Fig. <ref type="figure">5</ref> that Fig. <ref type="figure">5</ref>. Simulation time vs. number of network flows for three different network topologies without and with lazy updates. On loosely connected platforms (e.g., independent peers), the lazy update mechanism reduces simulation time significantly. On more tightly coupled platforms it can increase simulation time. the use of lazy updates reduces the simulation time significantly. The second platform comprises 90 hosts and 20 routers and was created with the Tiers algorithm <ref type="bibr" target="#b16">[17]</ref>, which uses a threestep space-based hierarchical approach. The resulting topology is hierarchical with low bisection bandwidth, and has thus both global (in the core of the network) and local (on the edges of the network) bottleneck links. The third platform comprises 200 nodes and is generated with the Waxman model <ref type="bibr" target="#b82">[84]</ref> and the BRITE generator <ref type="bibr" target="#b56">[57]</ref>. In this platform there are more alternate network paths but the unstructured communication patterns of the simulated application lead here also to interference among flows in the network. The results in Fig. <ref type="figure">5</ref> for the second and third platforms show that lazy updates actually increase simulation time. This is because in these platforms the probability that a random flow interferes with another is high. As a result, our lazy updates' implementation suffers from some overhead when determining that the resource shares of most flows need to be recomputed. For this reason, lazy updates can be deactivated by the user. However, since lazy updates only incur marginal slowdowns but bring significant speedup when there is locality in the communication patterns, SimGrid enables them by default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Scalable platform descriptions</head><p>As stated in Section 1.3, the most expressive way to describe a network interconnect is to describe the routing table of each simulated network element explicitly. Unfortunately, this method is memory consuming. The last mile model in Section 3.3 provides a partial solution that is applicable in some domains. More generally, SimGrid uses a scalable, efficient, and modular network representation technique, which also drastically reduces the platform description burden placed on users.</p><p>SimGrid's platform representation exploits the hierarchical structure of real-world (large-scale) network infrastructures <ref type="bibr" target="#b10">[11]</ref>, relying on the concept of autonomous systems (AS), including local networks as well as the classical Internet definition. In addition, the representation is recursive within each AS so that regular patterns can be exploited whenever possible. SimGrid provides stock implementations of well-known routing schemes, including Dijkstra, Dijkstra with cache, Floyd, Flat (i.e., full routing table), Empty routing with Vivaldi network coordinates (see Section 3.3), and cluster (i.e., a regular topology where each node has its own private links and communicates with the others through an additional shared link). For the time being, and to favor scalability, SimGrid assumes that the routing is static over time. This assumption is reasonable (see for instance the study in <ref type="bibr" target="#b13">[14]</ref>, which shows that less than 20% of Internet paths change in a 24-hour period). Besides, routing changes in real-world networks are known to affect traffic on backbone links. Usually, these links are not communication bottlenecks. Therefore, routing changes can likely be ignored without a large impact on simulation accuracy. Fig. <ref type="figure" target="#fig_5">6</ref> shows an example hierarchical network representation in SimGrid.</p><p>Each AS declares a number of gateways, which are used to compute routes between ASes comprised within a higherlevel AS. This mechanism is used to determine routes between hosts that belong to different ASes: simply search for the first common ancestor in the hierarchy and resolve the path recursively. The network representation and this route computation method provide a compact and effective representation for hierarchical networks. Since real-world networks are not purely hierarchical, SimGrid provides ''bypassing'' rules that can be used to declare alternate routes between ASes manually.</p><p>The above semantic principles of network representation are implemented by the user via an XML file. For convenience, SimGrid provides a set of XML tags that simplify the definition of two standard and ubiquitous ASes: homogeneous clusters and sets of Internet peers. The cluster tag creates a set of homogeneous hosts interconnected through private links and a backbone, which all share a common gateway. The peer tag allows for the easy creation of peer-to-peer platforms by defining at the same time a host and a connection to the rest of the network (with different upload and download characteristics and network coordinates, as explained in Section 3.3). SimGrid also provides an API for generating inmemory network descriptions directly without requiring an XML file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case study: grid computing simulations</head><p>SimGrid's network description approach makes it possible to describe large platforms with low memory footprint and without significant computational overhead. For instance, it allows us to represent the full Grid'5000 platform <ref type="bibr" target="#b11">[12]</ref> (10 sites, 40 clusters, 1500 nodes) with only 61 KiB. By comparison, the flat representation with SimGrid v3.2 required 1065 MiB <ref type="bibr" target="#b32">[33]</ref>. It takes more than 4 min to parse the flat representation on a 1.6 GHz Intel Core2 Duo with 5 GiB of memory and an SSD drive while the current representation is parsed in less than 150 ms.</p><p>We now compare the scalability of SimGrid to the widely used GridSim toolkit <ref type="bibr" target="#b14">[15]</ref> (version 5.2 released on Nov. <ref type="bibr" target="#b24">25,</ref><ref type="bibr">2010)</ref>. The experimental scenario is a simple master-worker execution where the master distributes T fixed size tasks to W workers in a round-robin fashion. In GridSim we did not define any network topology, hence only the output and input baud rates are used to determine data transfer rates. By contrast, with SimGrid we used the aforementioned Grid'5000 network representation, which models clusters and their cabinets as well as the wide-area network interconnecting the different sites. Furthermore, SimGrid uses the flow network model described in Section 3. Experiments were conducted using a 2.4 GHz Intel Xeon Quad-core with 8 GiB of RAM. We refer the interested reader to <ref type="bibr" target="#b10">[11]</ref> for more details on the experimental setup.</p><p>The number of tasks, T , is uniformly sampled in the <ref type="bibr">[1; 500, 000]</ref> interval and the number of workers, W , is uniformly sampled in the [100; 2000] interval. We perform 139 experiments for GridSim and 1000 for SimGrid (as it was significantly faster), and measure the wall-clock time (in seconds) and the memory consumption (using the Maximum Resident Set Size in KiB as a measurement). As expected, the sizes (input and output data, and amount of computation) of the tasks have no influence. Experimental results are shown as polynomial fits in Table <ref type="table" target="#tab_1">2</ref>. The goodness of fit is high (all coefficients of determinations, or R 2 , for all fits are above 0.9972).</p><p>The simulation time for GridSim is quadratic in T and linear in W . Surprisingly, GridSim's memory footprint is not polynomial in T and W . Rather, it appears to be piece-wise linear in both (with a very steep slope at first, and less steep as values increase). Furthermore there are a few outlier points that exhibit particularly low or high memory usages (leading to R 2 = 0.9871). This is likely explained by the Java garbage collection. For this reason, in Table <ref type="table" target="#tab_1">2</ref> we only report results for scenarios where T is larger than 200,000, which removes most outliers.</p><p>Analyzing the results for SimGrid shows that its simulation time and memory footprint are stable and several orders of magnitude lower. The results reported in Table <ref type="table" target="#tab_1">2</ref> mean that 5.2 MiB is required to represent the Grid'5000 platform and the internals of SimGrid, with a payload of 80 KiB per worker. By comparison GridSim uses 2.5 GiB with an additional 300 MiB payload per worker. Furthermore, in SimGrid T has no impact on the memory footprint, which is not the case in GridSim. We conclude that SimGrid, with its flow network model and a fine-detailed network topology, is several orders of magnitude faster and more memoryefficient than GridSim, with its delay-based model and no network topology. For instance, while GridSim requires more than one hour and 4.4 GiB of memory to simulate the execution of 500,000 tasks with 2000 workers, SimGrid performs this same simulation in less than 14 s and with only 165 MiB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Efficient simulation of concurrent processes</head><p>SimGrid allows users to describe the simulated application programmatically as a set of independent but communicating concurrent processes. The goal is to allow users to implement the simulated application in a way that is similar to but simpler than the way in which a real application would be implemented. Due to the optimizations described in Section 4.1, for many largescale simulations the most computationally intensive portion of the simulation is not the evaluation of the simulation model, but instead the execution and the synchronization of the simulated processes! As a result, increasing scalability requires going beyond vanilla implementations, e.g., based on threads and standard synchronization primitives.</p><p>Since the simulation models in SimGrid can be computed quickly, it is possible and in fact efficient to have a unique execution context (such as a thread) handle all the simulation model computations. We call this context the core context, and it interacts with the execution contexts of the simulated concurrent processes. This has led to the layered design shown in Fig. <ref type="figure" target="#fig_0">1</ref>. At the bottom is the SURF component that runs in the core context and deals with the simulation of the resources and of their usage by the activities issued by the simulated concurrent processes. At the top are the concurrent processes themselves, implemented as user code that places calls to a SimGrid API (MSG or SMPI) to define activities. In between is a synchronization kernel, SIMIX, that mediates every interaction between the simulated processes and the core context.</p><p>The synchronization kernel is conceptually close to the kernel of a classical operating system and it emulates a system call interface called simcalls. Simcalls are used by simulated processes to interact with the core context. When a simulated process issues a simcall the request and its arguments are stored in a private memory location. The process is then blocked until the completion of the request (e.g., completion of the corresponding simulated activity). When all user processes are blocked in this manner control is passed to the core context. The core context handles the requests sequentially in an arbitrary but deterministic order based on process IDs, and it is the only context that accesses the simulation state. A sequential core context makes for simplified simulation logic due to vastly reduced numbers of context switches between the core context and the simulated processes. To the best of our knowledge it is the first time that this classical OS design is applied to distributed system simulation. An alternate design in which simulated entities actively interact with each other, such as that used for instance in GridSim <ref type="bibr" target="#b14">[15]</ref>, may seem more intuitive but leads to more complex simulation logic due to multi-step interactions between processes/threads.</p><p>Our design is scalable only if mechanisms are available to execute thousands or even millions of processes on a single host (standard virtual machine techniques cannot be used to execute our simulated processes as at most dozens of virtual machines instances can run efficiently on a host). The use of regular threads seems like a natural approach, with the code of each simulated concurrent process running in its own thread. But with standard threads, one can scale up to ''only'' a few thousand simulated processes, thus severely limiting the scale of the simulation. For instance, GridSim, which uses threads, cannot simulate more than 10,000 processes/hosts <ref type="bibr" target="#b25">[26]</ref>. Instead, we employ cooperative, lightweight, non-preemptive threads (known as continuations). They are ideally suited to our needs since our synchronization kernel has to finely control the scheduling of the simulated processes anyway. Additionally, they are much simpler to implement than regular threads. The Windows operating system provides such light-weight execution contexts as fibers, while they are called ucontexts (for user-contexts) on Unix operating systems, including Mac OSX. In SimGrid we have aggressively re-implemented a similar mechanism directly in assembly so as to remove a costly and unnecessary system call found in standard implementations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case study: scalable peer-to-peer simulation</head><p>In <ref type="bibr" target="#b65">[66]</ref>, we compare the scalability of SimGrid for a peer-topeer simulated scenario to that of two popular and reported-to-bescalable simulators in that domain: OverSim <ref type="bibr" target="#b4">[5]</ref> and PeerSim <ref type="bibr" target="#b58">[59]</ref>. Fig. <ref type="figure">7</ref> shows the simulation time of the Chord protocol <ref type="bibr" target="#b71">[72]</ref> vs. the number of simulated peers. For SimGrid and OverSim we use the experimental scenario initially proposed in <ref type="bibr" target="#b4">[5]</ref>: each peer joins the Chord ring at time t = 0, then performs a stabilize operation every 20 s, a fix fingers operation every 120 s, and an arbitrary lookup request every 10 s. The simulation ends at t = 1000 s. For PeerSim, the implementation that is publicly available does not make this experimental scenario possible since there are not stabilize or fix fingers operations. So in the PeerSim experiments a single lookup is generated every 120 s.</p><p>We use the Chord implementations that are publicly available for OverSim and PeerSim, while we have implemented the Chord protocol ourselves for SimGrid. Therefore, there may be differences (parameters, features, optimizations, or even bugs) among the three implementations of the protocol. To ensure that experiments are comparable in spite of such differences, we tune the simulated scenario parameters to make sure that the numbers of application messages exchanged during the simulation, and thus the load on the simulator, are comparable across experiments (10,000 peers, 25 million messages). More specifically, we conservatively ensure that more messages are exchanged in the SimGrid simulation than in the OverSim and PeerSim simulations. Note that Fig. <ref type="figure">7</ref>. Simulation time vs. number of peers for a Chord simulation with SimGrid (with constant and precise network models), OverSim (with a simple underlay and using the OMNeT++ bindings), and PeerSim. the three simulators in our comparisons record different information (i.e., simulation event traces), leading to different tracing overheads. However, as seen hereafter, our results show orders of magnitude improvements for SimGrid over its competitors.</p><p>Experiments were conducted on one core of a two-CPU 1.7 GHz AMD Opteron 6164 HE (12 cores per CPU) with 48 GiB of RAM running Linux. OverSim (v20101103) is implemented in C++ (gcc v4.4.5), SimGrid (SimGrid v3.7-beta, git revision 918d6192) in C (gcc v4.4.5), and PeerSim (v1.0.5) in Java (HotSpot JVM v1.6.0-26).</p><p>In our experiments, we configured PeerSim so that its simulation model assumes that every communication takes a uniform random amount of time. We configured OverSim to use a simple model in which communication times are based on the Euclidean distance between processes (instead of the less scalable OMNeT++ bindings). By contrast, for this experiment SimGrid uses its flowlevel model that accounts for more complex network behavior (i.e., contention and TCP congestion avoidance). Therefore, the simulation model of SimGrid subsumes and is thus strictly more realistic than the simulation models in OverSim and PeerSim. We refer the interested reader to <ref type="bibr" target="#b65">[66]</ref> for full details.</p><p>Results show that the largest scenario that we managed to run in less than 12 h using PeerSim was for 100,000 peers (4 h 36 min). This poor result is likely due to the Java implementation. With OverSim, we managed to simulate 300,000 peers in 10 h. With SimGrid we were able to simulate 2,000,000 peers in 6 h 43 min. Simulating 300,000 peers took 32 min. The memory footprint for simulating 2 million peers with SimGrid was about 36 GiB, which amounts to 18 KiB per peer, including 16 KiB for the stack devoted to the user code.</p><p>We conclude that SimGrid leads to drastic scalability improvements when compared to state-of-the-art peer-to-peer simulators, even though these simulators were designed specifically for scalable simulations. For instance, SimGrid is 15 times faster than OverSim and can simulate scenarios that are 10 times larger even though it uses much more sophisticated (network) simulation models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Accurate and scalable HPC simulations</head><p>In this section we present a case study in the HPC domain. Developing a simulator that makes it possible to simulate a few standard HPC benchmarks with reasonable accuracy requires a fair amount of effort, and has merit to demonstrate the potential of the simulator. However, the ultimate goal is for the simulator to be usable (i.e., accurate and scalable) for simulating real applications. For this reason, we have evaluated SMPI, the component of SimGrid that makes it possible to simulate MPI applications, for both benchmarks and complex applications, including the full LinPACK suite <ref type="bibr" target="#b29">[30]</ref>, the Sweep3D <ref type="bibr" target="#b2">[3]</ref> benchmark, the BigDFT Density Functional Theory application <ref type="bibr" target="#b36">[37]</ref>, and the SpecFEM3D geodynamics application <ref type="bibr" target="#b63">[64]</ref> that is part of the PRACE benchmark.</p><p>SMPI is tested on a daily basis for 80% of the MPICH2 test suite and against a large subset of the MPICH3 test suite.</p><p>In <ref type="bibr" target="#b6">[7]</ref> we have demonstrated the ability of SMPI to simulate a real, large, and complex MPI application and we report here a part of these results to illustrate the effectiveness of the models presented in Section 3.2. To this end, we use BigDFT, which is the sole electronic structure code based on systematic basis sets that can use hybrid supercomputers and has good scaling (95% efficiency with 4096 nodes on the Curie supercomputer). For this reason, BigDFT was selected as one of the eleven scientific applications in the Mont-Blanc project <ref type="bibr" target="#b57">[58]</ref>. The goal of this project is to assess the potential of low-power embedded components, such as commercially available ARM processing and network components, for building exascale clusters. The first Mont-Blanc prototype is expected to become available during 2014. It will include Samsung Exynos 5 Dual Cortex A15 processors with an embedded Mali T604 GPU and will be using Ethernet for communication. To evaluate the applications before the prototype is available, a small cluster of ARM systems-on-chip was built at the Barcelona Supercomputing Center, Tibidabo <ref type="bibr" target="#b66">[67]</ref>, which uses NVIDIA Tegra2 chips, each with a dual-core ARM Cortex A-9 processor. The PCI Express support of Tegra2 is used to connect a 1 GbE NIC, and the boards are interconnected hierarchically using 48-port 1 GbE switches. The application execution results that are presented in this section have been obtained on Tibidabo. The OpenMP and GPU extensions of BigDFT were disabled so as to focus on the behavior of the MPI operations. We used MPICH 3.0.4 <ref type="bibr" target="#b76">[77]</ref> and we refer the interested reader to <ref type="bibr" target="#b6">[7]</ref> for more details on the experimental setup.</p><p>BigDFT alternates between computation bursts and intensive collective communications. The collective operations that are used are diverse and can change depending on the instance, hence requiring accurate modeling of a broad range of collective communications for the purpose of simulating BigDFT executions. BigDFT can be simulated with SMPI with minimal source code modification. BigDFT has a large memory footprint, which precludes running it on a single machine. However, thanks to the memory folding and partial execution techniques implemented as part of SMPI (see <ref type="bibr" target="#b22">[23]</ref>), we were able to simulate the execution of BigDFT with 128 processes, with a peak memory footprint estimated at 71 GiB, on a 1.6 GHz Intel Core2 Duo processor with less than 2.5 GiB of RAM.</p><p>Fig. <ref type="figure" target="#fig_6">8</ref> shows parallel speedup vs. number of compute nodes, as measured on the Tibidabo cluster for an instance of the BigDFT application. This instance has a relatively low communication to computation ratio in spite of Tibidabo's relatively slow compute nodes (around 20% of time is spent communicating when using 128 nodes), and uses the following collective operations: MPI_Alltoall, MPI_Alltoallv, MPI_Allgather, MPI_Allgatherv and MPI_Allreduce. This particular instance is a difficult case for simulation. This is because the large number of collective communication operations severely limits the scalability of the application, thus requiring precise simulation of these operations. Yet, accurately assessing such scalability limits in simulation is crucial for deciding how to provision a platform before it is actually purchased and deployed.</p><p>In addition to the real speedup measurements, Fig. <ref type="figure" target="#fig_6">8</ref> shows the speedup computed based on simulation results obtained with SimGrid, as well as the speedup computed according to the LogGPS model <ref type="bibr" target="#b45">[46]</ref>. As expected, both are more optimistic than the real execution. However, while SimGrid tracks the trend of the real measurements well (within 8%), LogGPS is overly optimistic (up to 40% error). As explained in Section 3.2, unlike models from the LogP family, SimGrid relies on a model that combines flow-level models (to account for contention on arbitrary network topologies), a piece-wise linear model (to model the  protocol switching feature of MPI implementations), and a LogP model (to model the computation/communication overlap and the communication synchronization semantic). The results in Fig. <ref type="figure" target="#fig_6">8</ref> show that this model is significantly more accurate than the LogGPS model. In particular, unlike LogGPS, it successfully accounts for the slowdown of BigDFT incurred by the hierarchical and irregular network topology of the Tibidabo platform.</p><p>One interesting question is whether the higher accuracy of SimGrid when compared to the use of the LogGPS model comes at an acceptable loss in simulation scalability. In other words, how long does the SimGrid simulation take? To answer this question we compare the scalability of SimGrid to that of LogGOPSim 1.1 <ref type="bibr" target="#b43">[44]</ref>, a recent simulator designed specifically to simulate the execution of MPI applications on large-scale HPC systems. LogGOPSim relies on the LogGPS model. We use the same experimental setting described in Section 4.1.2 of <ref type="bibr" target="#b43">[44]</ref>, i.e., the execution of a binomial broadcast on various numbers of nodes. Unfortunately, as the input traces used therein were not available, we compare our results to the published results instead of reproducing the experiments with LogGOPSim. We use SimGrid v3.7-beta (git revision 918d6192 and gcc v4.4.5) to simulate a platform interconnected with a hierarchy of 64-port switches. The binomial broadcast is implemented with the SimDAG API. The original evaluation of LogGOPSim was done on a 1.15 GHz Opteron workstation with 13 GiB of memory. Instead, we use one core of a node with two AMD Opteron 6164 HE 12-core CPUs at 1.7 GHz with 48 GiB of memory, which we scale down to 1 GHz to allow for a fair comparison. We refer the interested reader to <ref type="bibr" target="#b10">[11]</ref> for more details on the experimental setup. Fig. <ref type="figure" target="#fig_7">9</ref> shows simulation time vs. the number of simulated nodes for both SimGrid and LogGOPSim. While using significantly more elaborate platform and communication models, and thus leading in general to much improved accuracy (see Fig. <ref type="figure" target="#fig_6">8</ref>), SimGrid is only about 75% slower than LogGOPSim. This percentage slowdown is almost constant up to large scales with millions of simulated nodes. SimGrid's memory usage for 2 23 nodes in this experiment is 15 GiB, which is larger than what is achieved in <ref type="bibr" target="#b43">[44]</ref> (whose experiments were conducted on a machine with 13 GiB of RAM). The incurred scalability penalties in terms of simulation time and memory footprint are likely worthwhile for most users given the large improvement in simulation accuracy.</p><p>Our broad conclusion, from this and the other case studies presented in this work, is that a simulator can have both high accuracy and high scalability. It is interesting to note that SimGrid initially targeted grid computing applications, which led to the development of flow-level network models that account for network contention (Section 3.1). SimGrid then began being used for peerto-peer and volunteer computing simulations, which required an optimization of the simulation model evaluation algorithm (Section 4.1), the design of efficient platform models (Section 3.3) and representations (Section 4.2), and the optimization of the simulation of concurrent processes (Section 4.3). Targeting the simulation of HPC systems required improving the network model (Section 3.2). In the end, while these advances were motivated by various domains, their benefits are felt across all these domains. The results achieved for the HPC case study presented in this section would not have been possible had not all these advances been accomplished within a single versatile simulation framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this article we have given an overview of the SimGrid project and have highlighted recent scientific and engineering advances in the context of this project. These advances have led to improvements in both simulation accuracy and scalability. The important lesson is that most of these advances have been successful and yet motivated by the need to push the versatility of SimGrid beyond its original target domain. This is contrary to the popular wisdom that specialization allows for better simulations. And indeed, as seen in the results we have presented, SimGrid outperforms several more specialized state-of-the-art simulators. We claim that this is not in spite of its increased versatility but because of it. A clear benefit of this versatility is that it is now possible to conduct SimGrid simulations that cross multiple domains. For instance, one can combine peer-to-peer and high-performance computing simulations (e.g., to simulate a set of commodity clusters interconnected via highperformance backbones augmented with a large set of Internetconnected peers).</p><p>An important future direction for SimGrid is the development of new or improved simulation models for (i) memory hierarchies, which have a large impact on the performance of HPC applications; (ii) storage resources, which are often a performance bottleneck in HPC and cloud environments; and (iii) power consumption of the simulated application/platform, which is an overriding concern for all large-scale platforms be they cloud infrastructures or petascale/exascale HPC platforms. Another direction relates to the design and analysis of simulation experiments. In many fields, conducting experiments to acquire sample data is expensive (e.g., industrial processes). Given the relatively low number of samples, practitioners must rely on sound statistical techniques. By contrast, because computer simulation experiments are cheap, most computer scientists acquire large numbers of samples via thousands of simulation experiments with the informal rationale that statistical significance is achieved by large numbers. As a result, although a broad generalization is likely unfair, computer scientists often seem to use poor statistical techniques. Our own recent use of solid statistical techniques has, unsurprisingly, proved extremely beneficial both in terms of result confidence and of simulation times. Popularizing the use of these techniques, by providing a simulation design and analysis framework as part of SimGrid, would represent a major step toward better scientific practice in this field.</p><p>In experimental sciences the ability to reproduce published results is the necessary foundation for obtaining universal and enduring knowledge, and part of the ''Open Science'' approach widely adopted in fields such as physics or chemistry. To date, most simulation results in the parallel and distributed computing literature are obtained with simulators that are ad hoc, unavailable, undocumented, and/or no longer maintained. For instance, in 2013, the authors in <ref type="bibr" target="#b3">[4]</ref> point out that out of 125 recent papers they surveyed that study peer-to-peer systems, 52% use simulation and mention a simulator, but 72% of them use a custom simulator. As a result, most published simulation results are impossible to reproduce by researchers other than the authors. There is thus a strong need for recognized simulation frameworks by which simulation results can be reproduced and further analyzed. Our goal is for SimGrid to fill this need, which is why SimGrid welcomes contributors and is publicly available at http://simgrid.org/.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Design and internals of SimGrid.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Typical situation: the SimGrid flow-level model achieves an excellent prediction when compared to a packet-level simulation.(c) The most extreme situation we stumbled upon where the flow-level model significantly differs from a packet-level model for several flows. The lower part depicts the evolution of the throughput of the last finishing flow in the packet-level simulation and for which the flow-level model has a large error. Because of high contention, this flow stalls 65% of the time. When there is no other remaining connection in the network, it does not transmit a single byte for 380 s, and finally completes the transfer in a few seconds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Comparison of flow completion times with flow-level simulation (top timeline) and packet-level simulation (bottom timeline); lines connecting timeline markers correspond to the same flow in both simulations and are darker for larger completion time mismatches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Duration of MPI_Send, MPI_Recv, and a ping-pong (a send immediately followed by a receive of the same size) vs. message size. Different modes can be seen depending on message size. Solid lines represent piece-wise linear regressions.</figDesc><graphic coords="10,81.41,63.90,454.39,155.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Simulation time vs. number of simulated hosts for a volunteer computing simulation using the initial design, with lazy updates, and with lazy updates and trace integration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Example hierarchical network representation of an AS, AS1. Circles represent compute resources and squares represent network routers. Bold lines represent communication links. Routing schemes are indicated for each AS. AS2 models the core of a national network interconnecting a small cluster (AS4), a larger hierarchical cluster (AS5), a subset of a LAN (AS6), and a set of peers scattered across the Internet (AS3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Parallel speedup vs. number of compute nodes for BigDFT on Tibidabo, for real executions, SimGrid simulations, and LogGPS models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Simulation time vs. number of simulated nodes for SimGrid and LogGOPSim when simulating a binomial broadcast.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>State-of-the-art simulators from various communities and modeling approaches.</figDesc><table><row><cell>Simulator</cell><cell cols="3">Community of origin</cell><cell></cell><cell></cell><cell cols="2">Application</cell><cell></cell><cell cols="2">Network</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CPU</cell><cell></cell><cell>Disk</cell><cell></cell></row><row><cell></cell><cell>High performance computing</cell><cell>Grid computing</cell><cell>Cloud computing</cell><cell>Volunteer computing</cell><cell>Peer-to-peer computing</cell><cell>Execution trace</cell><cell>Abstract specification</cell><cell>Programmatic specification</cell><cell>Latency</cell><cell>Bandwidth</cell><cell>Store-and-forward</cell><cell>ad hoc flow</cell><cell>TCP flow-level model</cell><cell>Packet-level</cell><cell>Scaled measured delay</cell><cell>Scaled user-provided delay</cell><cell>Capacity</cell><cell>Seek + transfer</cell><cell>Block-level</cell></row><row><cell>PSINS [79]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Polynomial fits of simulation times and peak memory footprints of GridSim and SimGrid for a master-worker simulation with W workers and N tasks. The simulation time is quadratic with T in GridSim while it is linear in SimGrid. The peek memory footprint of GridSim is several orders of magnitude larger than that of SimGrid. GiB + 300 MiB × W + 3 KiB × T SimGrid 0.1 ms × W + 26 µs × T 5.2 MiB + 80 KiB × W</figDesc><table><row><cell></cell><cell>Simulation time</cell><cell>Peak memory footprint</cell></row><row><cell>GridSim</cell><cell>56 ms × W + 14 ns × T 2</cell><cell>2.5</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://www.grid5000.fr/mediawiki/index.php/Nancy:Network.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like the thank SimGrid team members and collaborators whose work has contributed ideas and content for this article: Olivier Beaumont, Laurent Bobelin, Pierre-Nicolas Clauss, Augustin Degomme, Bruno Donassolo, Lionel Eyraud-Dubois, Stéphane Genaud, George Markomanolis, David González Márquez, Pierre Navarro, Christian Rosa, Lucas Schnorr, Mark Stillwell, Christophe Thiéry, Pedro Velho, Jean-Marc Vincent, Young Joon Won. This work has been supported by ANR (French National Agency for Research) through project references ANR 08 SEGI 022 (USS SimGrid) and ANR 11 INFRA 13 (SONGS), by CNRS (French National Center for Scientific Research) through PICS 5473 grant, and by Inria through an ADT (software and technological development actions) and internship programs. Experiments presented in this article were carried out using the Grid'5000 experimental testbed, being developed under the INRIA ALADDIN development action with support from CNRS, RENATER and several Universities as well as other funding bodies (see https://www.grid5000.fr).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">incorporating long messages into the logp model-one step closer towards a realistic model for parallel computation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alexandrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Schauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Scheiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loggp</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 7th ACM Symposium on Parallel Algorithms and Architectures</title>
		<meeting>of the 7th ACM Symposium on Parallel Algorithms and Architectures<address><addrLine>Santa Barbara, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="95" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Parallel simulation of large-scale parallel applications</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bagrodia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Deelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Phan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. High Perform. Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="12" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An s n algorithm for the massively parallel CM-200 computer</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Koch</surname></persName>
		</author>
		<ptr target="http://wwwc3.lanl.gov/pal/software/sweep3d/" />
	</analytic>
	<monogr>
		<title level="j">Nucl. Sci. Eng</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="312" to="320" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The state of peer-to-peer network simulators</title>
		<author>
			<persName><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stanier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Naicken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Wakeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Gurbani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">46</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">OverSim: a flexible overlay network simulation framework</title>
		<author>
			<persName><forename type="first">I</forename><surname>Baumgart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Heep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 10th IEEE Global Internet Symposium</title>
		<meeting>of the 10th IEEE Global Internet Symposium</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="79" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Using the last-mile model as a distributed scheme for available bandwidth prediction</title>
		<author>
			<persName><forename type="first">O</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eyraud-Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Won</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 17th International European Conference on Parallel and Distributed Computing</title>
		<meeting>of the 17th International European Conference on Parallel and Distributed Computing</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">6852</biblScope>
			<biblScope unit="page" from="103" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Toward better simulation of MPI applications on ethernet/TCP networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bedaride</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Degomme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Genaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Legrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Markomanolis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Quinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stillwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Suter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Videau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 4th International Workshop on Performance Modeling, Benchmarking and Simulation of High Performance Computer Systems</title>
		<meeting>of the 4th International Workshop on Performance Modeling, Benchmarking and Simulation of High Performance Computer Systems<address><addrLine>Denver, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">OptorSim-a grid simulator for studying dynamic data replication strategies</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Cameron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Millar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Capozza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stockinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. High Perform. Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="403" to="416" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Berkeley open infrastructure for network computing</title>
		<ptr target="http://boinc.berkeley.edu" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Gallager</surname></persName>
		</author>
		<title level="m">Data Networks</title>
		<imprint>
			<publisher>Prenctice Hall</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scalable multi-purpose network representation for large scale distributed system simulation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bobelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Legrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Gonzàlez Màrquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Quinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Suter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Thiery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th IEEE International Symposium on Cluster Computing and the Grid, (CCGrid&apos;12)</title>
		<meeting>the 12th IEEE International Symposium on Cluster Computing and the Grid, (CCGrid&apos;12)</meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="220" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Grid&apos;5000: a large scale and highly reconfigurable experimental grid testbed</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bolze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cappello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daydé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Desprez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jeannot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lanteri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leduc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Melab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mornet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Namyst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Primet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Quetier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-G</forename><surname>Talbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Touche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. High Perform. Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="481" to="494" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The DiskSim simulation environment version 4.0 reference manual</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Bucy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Schlosser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Ganger</surname></persName>
		</author>
		<idno>CMU-PDL-08-101</idno>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University, Parallel Data Lab</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Optimizing BGP security by exploiting path stability</title>
		<author>
			<persName><forename type="first">K</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Aiello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 13th ACM Conference on Computer and Communications Security</title>
		<meeting>of the 13th ACM Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="298" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">GridSim: a toolkit for the modeling and simulation of distributed resource management and scheduling for grid computing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Buyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Murshed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurr. Comput.: Pract. Exper</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">13-15</biblScope>
			<biblScope unit="page" from="1175" to="1220" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">CloudSim: a toolkit for modeling and simulation of cloud computing environments and evaluation of resource provisioning algorithms</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Calheiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beloglazov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A F</forename><surname>De Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Buyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Softw. -Pract. Exp</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="50" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<author>
			<persName><forename type="first">K</forename><surname>Calvert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Doar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zegura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Modeling Internet topology</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="160" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SimGrid: a generic framework for large-scale distributed experiments</title>
		<author>
			<persName><forename type="first">H</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Legrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Quinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 10th IEEE International Conference on Computer Modeling and Simulation</title>
		<meeting>of the 10th IEEE International Conference on Computer Modeling and Simulation</meeting>
		<imprint>
			<publisher>UKSim</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A network model for simulation of grid application</title>
		<author>
			<persName><forename type="first">H</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LIP</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>ÉNS de Lyon</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep. 2002-40</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">WorkflowSim: a toolkit for simulating scientific workflows in distributed environments</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Deelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 8th IEEE International Conference on eScience</title>
		<meeting>of the 8th IEEE International Conference on eScience</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Some observations on fairness of bandwidth sharing</title>
		<author>
			<persName><forename type="first">D.-M</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 5th IEEE Symposium on Computers and Communications (ISCC)</title>
		<meeting>of the 5th IEEE Symposium on Computers and Communications (ISCC)<address><addrLine>Antibes, France</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="125" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">PlanetLab: an overlay testbed for broad-coverage services</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Culler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Roscoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bavier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wawrzoniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGCOMM Comput. Commun. Rev</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="3" to="12" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Single node on-line simulation of MPI applications with SMPI</title>
		<author>
			<persName><forename type="first">P.-N</forename><surname>Clauss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stillwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Genaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Suter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Quinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 25th IEEE International Parallel and Distributed Processing Symp. (IPDPS)</title>
		<meeting>of the 25th IEEE International Parallel and Distributed essing Symp. (IPDPS)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="661" to="672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">LogP: towards a realistic model of parallel computation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Culler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Karp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sahay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Schauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Subramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Von Eicken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPOPP</title>
		<meeting>of the fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPOPP<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Vivaldi: a decentralized network coordinate system</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dabek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGCOMM</title>
		<meeting>of ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="15" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scalability of grid simulators: an evaluation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Depoorter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>De Moor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vanmechelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Broeckhove</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 14th EuroPar Conference</title>
		<meeting>of the 14th EuroPar Conference</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5168</biblScope>
			<biblScope unit="page" from="544" to="553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Parallelized direct execution simulation of message-passing parallel programs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dickens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Heidelberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nicol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Parallel Distrib. Syst</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1090" to="1105" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Characterizing residential broadband networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dischinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Haeberlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Gummadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saroiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 7th ACM SIGCOMM Conference on Internet Measurement</title>
		<meeting>of the 7th ACM SIGCOMM Conference on Internet Measurement</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="43" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast and scalable simulation of volunteer computing systems using SimGrid</title>
		<author>
			<persName><forename type="first">B</forename><surname>Donassolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Legrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Velho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Second Workshop on Large-Scale System and Application Performance</title>
		<meeting>of the Second Workshop on Large-Scale System and Application Performance</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="605" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The LINPACK benchmark: past, present, and future</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luszczek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petitet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurr. Comput.: Pract. Exper</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="803" to="820" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">EmBOINC: an emulator for performance analysis of BOINC projects</title>
		<author>
			<persName><forename type="first">T</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taufer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Workshop on Large-Scale and Volatile Desktop Grids</title>
		<meeting>of the Workshop on Large-Scale and Volatile Desktop Grids</meeting>
		<imprint>
			<publisher>PCGrid</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Promoting the use of end-to-end congestion control in the Internet</title>
		<author>
			<persName><forename type="first">S</forename><surname>Floyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Netw</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="458" to="472" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Handling very large platforms with the new SimGrid platform description formalism</title>
		<author>
			<persName><forename type="first">M.-E</forename><surname>Frincu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Quinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Suter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INRIA</title>
		<imprint>
			<biblScope unit="volume">348</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Speed and accuracy of network simulation in the SimGrid framework</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fujiwara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Casanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the First International Workshop on Network Simulation Tools (NSTools)</title>
		<meeting>of the First International Workshop on Network Simulation Tools (NSTools)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Open MPI: goals, concept, and design of a next generation MPI implementation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fagg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bosilca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Angskun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Squyres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sahay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kambadur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lumsdaine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Castain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Woodall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 11th European PVM/MPI Users&apos; Group Meeting</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting>of the 11th European PVM/MPI Users&apos; Group Meeting</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3241</biblScope>
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">PlanetSim: a new overlay network simulation framework</title>
		<author>
			<persName><forename type="first">P</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pairot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mondéjar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pujol Ahulló</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tejedor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rallo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 4th International Workshop on Software Engineering and Middleware</title>
		<meeting>of the 4th International Workshop on Software Engineering and Middleware</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3437</biblScope>
			<biblScope unit="page" from="123" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Daubechies wavelets as a basis set for density functional pseudopotential calculations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Genovese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goedecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Deutsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Ghasemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Willand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Caliste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Zilberberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rayson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Phys</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<date>014109</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">P2PSim, a simulator for peerto-peer protocols</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stribling</surname></persName>
		</author>
		<ptr target="http://pdos.csail.mit.edu/p2psim/" />
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Narses: a scalable flow-based network simulator</title>
		<author>
			<persName><forename type="first">T</forename><surname>Giuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baker</surname></persName>
		</author>
		<idno>cs.PF/0211024</idno>
		<ptr target="http://arxiv.org/abs/cs.PF/0211024" />
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<author>
			<persName><forename type="first">W</forename><surname>Gropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MPICH2: a new start for MPI implementations, in: 9th European PVM/MPI Users&apos; Group Meeting Recent Advances in Parallel Virtual Machine and Message Passing Interface</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2474</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Using MPI: Portable Parallel Programming with the Message Passing Interface</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gropp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lusk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Skjellum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scientific and Engineering Computation Series</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Computing low latency batches with unreliable workers in volunteer computing environments</title>
		<author>
			<persName><forename type="first">E</forename><surname>Heien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hagihara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Second Workshop on Large-Scale, Volatile Desktop Grids</title>
		<meeting>of the Second Workshop on Large-Scale, Volatile Desktop Grids</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Two-way TCP connections: old problem, new insight</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heusse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Merritt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">X</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Duda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Commun. Rev</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="5" to="15" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">LogGOPSim-simulating large-scale applications in the LogGOPS model</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lumsdaine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACM Workshop on Large-Scale System and Application Performance</title>
		<meeting>of the ACM Workshop on Large-Scale System and Application Performance</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="597" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A measurement study of Internet bottlenecks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Steenkiste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 24th Annual Joint Conference of the IEEE Computer and Communications Societies, INFOCOM</title>
		<meeting>of the 24th Annual Joint Conference of the IEEE Computer and Communications Societies, INFOCOM</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1689" to="1700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">LogGPS: a parallel computational model for synchronization analysis</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hagihara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the eighth ACM SIGPLAN Symposium on Principles and Practices of Parallel Programming</title>
		<meeting>of the eighth ACM SIGPLAN Symposium on Principles and Practices of Parallel Programming<address><addrLine>PPoPP, Snowbird, UT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">ACKclocking dynamics: modelling the interaction between windows and the network</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jacobsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hjalmarsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Low</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 27th IEEE International Conference on Computer Communications, INFOCOM</title>
		<meeting>of the 27th IEEE International Conference on Computer Communications, INFOCOM</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="2146" to="2152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">The TCP bandwidth-delay product revisited: network buffering, cross traffic, and socket buffer auto-sizing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dovrolis</surname></persName>
		</author>
		<idno>GIT- CERCS-03-02</idno>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Georgia Institute of Technology</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MR-CloudSim: designing and implementing MapReduce computing model on CloudSim</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="504" to="509" />
		</imprint>
	</monogr>
	<note>International Conference on ICT Convergence</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Fast measurement of LogP parameters for message passing platforms</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kielmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Bal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Verstoep</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 15 IPDPS 2000 Workshops on Parallel and Distributed Processing, IPDPS&apos;00</title>
		<meeting>of the 15 IPDPS 2000 Workshops on Parallel and Distributed essing, IPDPS&apos;00<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1176" to="1183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">SimBOINC: a simulator for desktop grids and volunteer computing systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kondo</surname></persName>
		</author>
		<ptr target="http://simboinc.gforge.inria.fr/" />
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The failure trace archive: enabling comparative analysis of failures in diverse distributed systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kondo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Javadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Iosup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Epema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 10th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGrid)</title>
		<meeting>eeding of the 10th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGrid)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="398" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Instruction-level simulation of a cluster at scale</title>
		<author>
			<persName><forename type="first">E</forename><surname>León</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Riesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maccabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bridges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference for High Performance Computing and Communications</title>
		<meeting>of the International Conference for High Performance Computing and Communications<address><addrLine>SC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A duality model of TCP and queue management algorithms</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Low</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Netw</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="525" to="536" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">TCP Libra: exploring RTT-fairness for TCP</title>
		<author>
			<persName><forename type="first">G</forename><surname>Marfia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Palazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gerla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sanadidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roccetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 6th International IFIP-TC6 Networking Conference on Ad Hoc and Sensor Networks</title>
		<meeting>of the 6th International IFIP-TC6 Networking Conference on Ad Hoc and Sensor Networks</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4479</biblScope>
			<biblScope unit="page" from="1005" to="1013" />
		</imprint>
	</monogr>
	<note>Wireless Networks, Next Generation Internet</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The macroscopic behavior of the TCP congestion avoidance algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mathis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Semke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Commun. Rev</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="67" to="82" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">BRITE: an approach to universal topology generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Medina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lakhina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Byers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Workshop on Modeling, Analysis and Simulation of Computer and Telecommunications Systems, MASCOTS</title>
		<meeting>of the International Workshop on Modeling, Analysis and Simulation of Computer and Telecommunications Systems, MASCOTS</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="346" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><surname>Mont-Blanc</surname></persName>
		</author>
		<ptr target="http://www.montblanc-project.eu/" />
		<title level="m">European approach towards energy efficient high performance, Montblanc</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">PeerSim: a scalable P2P simulator</title>
		<author>
			<persName><forename type="first">A</forename><surname>Montresor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jelasity</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 9th International Conference on Peer-to-Peer</title>
		<meeting>of the 9th International Conference on Peer-to-Peer</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="99" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">New techniques for simulating high performance MPI applications on large storage networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Núñez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carretero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Supercomput</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="40" to="57" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Design of a new cloud computing simulation platform</title>
		<author>
			<persName><forename type="first">A</forename><surname>Núñez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vázquez-Poletti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Caminero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carretero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Llorente</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 11th International Conference on Computational Science and its Applications</title>
		<meeting>of the 11th International Conference on Computational Science and its Applications</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="582" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Dynamic cloud provisioning for scientific grid workflows</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ostermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prodan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fahringer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 11th ACM/IEEE International Conference on Grid Computing, Grid</title>
		<meeting>of the 11th ACM/IEEE International Conference on Grid Computing, Grid</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">MPI-NeTSim: a network simulation module for MPI</title>
		<author>
			<persName><forename type="first">B</forename><surname>Penoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tüxen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Rüngeler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 15th International Conference on Parallel and Distributed Systems</title>
		<meeting>of the 15th International Conference on Parallel and Distributed Systems</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="464" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Forward and adjoint simulations of seismic wave propagation on fully unstructured hexahedral meshes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Komatitsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Le</forename><surname>Goff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Casarotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Le Loher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Magnoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nissen-Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Basini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tromp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Geophys. J. Int</title>
		<imprint>
			<biblScope unit="volume">186</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="721" to="739" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Objective Knowledge: An Evolutionary Approach</title>
		<author>
			<persName><forename type="first">K</forename><surname>Popper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1972">1972</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Parallel simulation of peer-to-peer systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Quinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Thiéry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 12th IEEE International Symposium on Cluster Computing and the Grid</title>
		<meeting>of the 12th IEEE International Symposium on Cluster Computing and the Grid</meeting>
		<imprint>
			<publisher>CCGrid</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">The low-power architecture approach towards exascale computing</title>
		<author>
			<persName><forename type="first">N</forename><surname>Rajovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vilanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Villavieja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Puzovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramirez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="439" to="443" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Decoupling computation and data scheduling in distributed data-intensive applications</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 11th IEEE International Symposium on High Performance Distributed Computing, HPDC</title>
		<meeting>of the 11th IEEE International Symposium on High Performance Distributed Computing, HPDC</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="352" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A hybrid MPI simulator</title>
		<author>
			<persName><forename type="first">R</forename><surname>Riesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Cluster Computing</title>
		<meeting>of the IEEE International Conference on Cluster Computing</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">An energy-efficient scheme for cloud resource provisioning based on CloudSim</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Cluster Computing, CLUSTER</title>
		<meeting>of the IEEE International Conference on Cluster Computing, CLUSTER</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="595" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Sioutas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papaloukopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sakkopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tsichlas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Manolopoulos</surname></persName>
		</author>
		<title level="m">Proc. of the 18th ACM Conference on Information and Knowledge Management</title>
		<editor>
			<persName><forename type="first">D-P2p-</forename><surname>Sim</surname></persName>
		</editor>
		<meeting>of the 18th ACM Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>CIKM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2069" to="2070" />
		</imprint>
	</monogr>
	<note>A novel distributed P2P simulator architecture</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Chord: a scalable peer-to-peer lookup protocol for Internet applications</title>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liben-Nowell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Karger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dabek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Balakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Netw</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="32" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Queue dynamics with window flow control</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jacobsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hjalmarsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Low</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Netw</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1422" to="1435" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Window flow control: macroscopic properties from microscopic factors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jacobsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hjalmarsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 27th IEEE International Conference on Computer Communications, INFOCOM</title>
		<meeting>of the 27th IEEE International Conference on Computer Communications, INFOCOM</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="91" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">SimBA: a discrete event simulator for performance prediction of volunteer computing projects</title>
		<author>
			<persName><forename type="first">M</forename><surname>Taufer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kerstens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Flores</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Teller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 21st International Workshop on Principles of Advanced and Distributed Simulation</title>
		<meeting>of the 21st International Workshop on Principles of Advanced and Distributed Simulation</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="189" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">SimMapReduce: a simulator for modeling MapReduce framework</title>
		<author>
			<persName><forename type="first">F</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Magoulès</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th FTRA International Conference on Multimedia and Ubiquitous Engineering</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="277" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Optimization of collective communication operations in MPICH</title>
		<author>
			<persName><forename type="first">R</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rabenseifner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. High Perform. Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="66" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">PSINS: an open source event tracer and execution simulator for MPI applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tikir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Laurenzano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carrington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 15th International Euro-Par Conference on Parallel Processing</title>
		<meeting>of the 15th International Euro-Par Conference on Parallel essing</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">5704</biblScope>
			<biblScope unit="page" from="135" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Simulation-based performance analysis and tuning for a two-level directly connected system</title>
		<author>
			<persName><forename type="first">E</forename><surname>Totoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhatele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bohm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mokos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kalé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 17th International Conference on Parallel and Distributed Systems</title>
		<meeting>of the 17th International Conference on Parallel and Distributed Systems</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="340" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">The OMNeT++ discrete event simulation system</title>
		<author>
			<persName><forename type="first">A</forename><surname>Varga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 15th European Simulation Multiconference</title>
		<meeting>of the 15th European Simulation Multiconference</meeting>
		<imprint>
			<publisher>ESM</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Accuracy study and improvement of network simulation in the SimGrid framework</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Legrand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2nd International Conference on Simulation Tools and Techniques, SIMUTools&apos;09</title>
		<meeting>of the 2nd International Conference on Simulation Tools and Techniques, SIMUTools&apos;09</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">On the validity of flowlevel TCP network models for grid and cloud simulations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Schnorr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Legrand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Model. Comput. Simul</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Routing of multipoint connections</title>
		<author>
			<persName><forename type="first">B</forename><surname>Waxman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Areas Commun</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1617" to="1622" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Are cycle accurate simulations a waste of time?</title>
		<author>
			<persName><forename type="first">V</forename><surname>Weaver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mckee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 7th Workshop on Duplicating, Deconstruction and Debunking</title>
		<meeting>of the 7th Workshop on Duplicating, Deconstruction and Debunking<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">PHANTOM: predicting performance of parallel applications on large-scale parallel machines using a single node</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</title>
		<meeting>of the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="305" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Henri Casanova is an Associate Professor in the Information and Computer Science Dept. at the University of Hawaii at Manoa. His research is in the broad area of high performance computing, and in particular the scheduling and the simulation of parallel and distributed applications</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kakulapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kalé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 18th International Parallel and Distributed Processing Symposium, IPDPS</title>
		<meeting>of the 18th International Parallel and Distributed essing Symposium, IPDPS<address><addrLine>France; Toulouse, France; Knoxville</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">2004. 1993. 1994. 1998</date>
		</imprint>
		<respStmt>
			<orgName>Universite Paul Sabatier</orgName>
		</respStmt>
	</monogr>
	<note>BigSim: a parallel simulator for performance prediction of extremely large parallel machines. and his Ph.D. from the University of Tennessee</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
