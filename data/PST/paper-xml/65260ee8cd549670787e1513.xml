<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">COGVLM: VISUAL EXPERT FOR LARGE LANGUAGE MODELS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Weihan</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qingsong</forename><surname>Lv</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Zhipu</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenmeng</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Zhipu</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ji</forename><surname>Qi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Zhipu</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junhui</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Zhipu</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Zhipu</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xixuan</forename><surname>Song</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiazheng</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xu</forename><surname>Bin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
							<email>ming.ding@zhipuai.cn</email>
							<affiliation key="aff0">
								<address>
									<settlement>Zhipu</settlement>
									<region>AI</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
							<email>jietang@tsinghua.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">RefCOCO RefCOCO+ RefCOCOg Visual7W GQA ScienceQA-IMG OKVQA TextVQA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">COGVLM: VISUAL EXPERT FOR LARGE LANGUAGE MODELS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce CogVLM, a powerful open-source visual language foundation model. Different from the popular shallow alignment method which maps image features into the input space of language model, CogVLM bridges the gap between the frozen pretrained language model and image encoder by a trainable visual expert module in the attention and FFN layers. As a result, CogVLM enables deep fusion of vision language features without sacrificing any performance on NLP tasks. CogVLM-17B achieves state-of-the-art performance on 10 classic cross-modal benchmarks, including NoCaps, Flicker30k captioning, Re-fCOCO, RefCOCO+, RefCOCOg, Visual7W, GQA, ScienceQA, VizWiz VQA and TDIUC, and ranks the 2nd on VQAv2, OKVQA, TextVQA, COCO captioning, etc., surpassing or matching PaLI-X 55B. Codes and checkpoints are available at https://github.com/THUDM/CogVLM.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Visual language models (VLMs) are versatile and powerful. Many vision and cross-modality tasks can be formulated as next token prediction, e.g., image captioning <ref type="bibr" target="#b0">(Agrawal et al., 2019)</ref>, visual question answering <ref type="bibr" target="#b2">(Antol et al., 2015)</ref>, visual grounding <ref type="bibr" target="#b55">(Yu et al., 2016)</ref> and even segmenta-Preprint tion <ref type="bibr">(Chen et al., 2022a)</ref>. Useful abilities like in-context learning <ref type="bibr" target="#b46">(Tsimpoukelli et al., 2021</ref>) also emerge along with the improvement of downstream tasks when scaling up VLMs. However, to train a large language model is already non-trivial, and it is more challenging to train a VLM from scratch with the same NLP performance as well-trained pure language models like LLaMA2 <ref type="bibr" target="#b45">(Touvron et al., 2023)</ref>. Therefore, it is natural to investigate how to train a VLM from an off-the-shelf pretrained language model. The popular shallow alignment methods represented by <ref type="bibr">BLIP-2 (Li et al., 2023)</ref> connect a frozen pretrained vision encoder and language model via a trainable Q-Former or a linear layer, mapping the image features into the input embedding space of the language model. This method converges fast, but the performance (BLIP-2 NoCaps CIDEr 121.6) is not as good as jointly training the vision and language modules, e.g., PaLI-X (NoCaps CIDEr 126.3). As for chat-style VLM trained by shallow alignment methods, e.g., <ref type="bibr">MiniGPT-4 (Zhu et al., 2023)</ref>, LLAVA <ref type="bibr">(Liu et al., 2023b)</ref>, and VisualGLM (Appendix D), the weak visual understanding ability manifests as hallucination. So, is it possible to retain the NLP capabilities of the large language model while adding top-notch visual understanding abilities to it? CogVLM gives a "yes" answer. In our opinion, the root cause of the inferior performance of shallow alignment methods lies in the lack of deep fusion between vision and language information. This inspiration arises from the comparison between p-tuning <ref type="bibr">(Liu et al., 2023e)</ref> and LoRA <ref type="bibr">(Hu et al., 2021)</ref> in efficient finetuning, where p-tuning learns a task prefix embedding in the input while LoRA adapts the model weights in each layer via a low-rank matrix. As a result, LoRA performs better and more stable. A similar phenomenon might also exist in VLM, because in the shallow alignment methods, the image features act like the prefix embedding in p-tuning. More detailed reasons for the performance degradation of p-tuning and shallow alignment include:</p><p>1. The frozen weights in the language model are trained for text tokens. Visual features do not have a perfect counterpart in the input text space. Therefore, after multi-layer transformations, the visual features might no longer match the input distribution of the weights in the deep layers.</p><p>2. During pretraining, the prior of the image captioning task, for example, the writing style and caption length, can only be encoded into the visual features in the shallow alignment methods. It weakens the consistency between visual features and content.</p><p>A possible solution is to adapt the language model to the image-text joint training, which is adopted by PaLI <ref type="bibr">(Chen et al., 2022b)</ref> and Qwen-VL <ref type="bibr">(Bai et al., 2023a)</ref>. However, in this way, the NLP ability is avoidably impaired, which might affect text-centered tasks, such as image-based poetry creation or introducing the background story of images. According to PaLM-E <ref type="bibr" target="#b14">(Driess et al., 2023)</ref>, making the language model trainable during VLM pretraining will lead to catastrophic forgetting, and drop 87.3% NLG performance for 8B language model.</p><p>CogVLM instead adds a trainable visual expert to the language model. In each layer, the image features in the sequence use a new different QKV matrix and MLP layer with the text features.</p><p>Visual expert doubles the number of parameters while keeping the FLOPs the same. Since all the parameters in the original language model are fixed, the behaviors are the same as the original language model if the input sequence contains no image.</p><p>Our CogVLM-17B trained from Vicuna-7B achieves state-of-the-art or the second-best performance on 14 classic cross-modal benchmarks, including 1) image captioning datasets: No-Caps, Flicker30k, COCO, 2) VQA datasets: VQAv2, OKVQA, GQA, TextVQA, VizWiz, 3) visual grounding datasets: RefCOCO, RefCOCO+, RefCOCOg, Visual7W, 4) multiple choice datasets: TDIUC, ScienceQA. We also trained a CogVLM-28B-zh from ChatGLM-12B <ref type="bibr" target="#b15">(Du et al., 2021)</ref> to support both English and Chinese for commercial use, which is not included in this paper.</p><p>Since most previous famous VLMs are close-source, including Flamingo <ref type="bibr" target="#b1">(Alayrac et al., 2022)</ref>, SimVLM <ref type="bibr" target="#b52">(Wang et al., 2021)</ref>, Coca <ref type="bibr" target="#b54">(Yu et al., 2022)</ref>, BEIT-3(1.9B) <ref type="bibr">(Wang et al., 2022c)</ref>, GIT2 <ref type="bibr">(Wang et al., 2022a)</ref>, PaLI <ref type="bibr">(Chen et al., 2022b)</ref>, PaLI-X <ref type="bibr">(Chen et al., 2023b)</ref>, we anticipate that the open-sourcing of CogVLM will greatly help the research and industrial application of visual understanding. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHOD 2.1 ARCHITECTURE</head><p>CogVLM model comprises four fundamental components: a vision transformer (ViT) encoder, an MLP adapter, a pretrained large language model (GPT), and a visual expert module. Figure <ref type="figure" target="#fig_1">3</ref> shows an overview of the CogVLM architecture. The components' design and implementation details are provided below:</p><p>ViT encoder. We utilize pretrained EVA2-CLIP-E <ref type="bibr" target="#b44">(Sun et al., 2023)</ref> in CogVLM-17B. The final layer of ViT encoder is removed because it specializes in aggregating the [CLS] features for contrastive learning.</p><p>MLP adapter. The MLP adapter is a two-layer MLP (SwiGLU (Shazeer, 2020)) to map the output of ViT into the same space as the text features from word embedding. All image features share the same position id in the language model.</p><p>Pretrained large language model. CogVLM's model design is compatible with any off-theshelf GPT-style pretrained large language model. Specifically, CogVLM-17B adopts Vicuna-7B-v1.5 <ref type="bibr" target="#b10">(Chiang et al., 2023)</ref> for further training. A causal mask is applied to all the attention operations, including the attention between image features.</p><p>Visual expert module. We add a visual expert module to each layer to enable deep visual-language feature alignment. Specifically, the visual expert module in each layer consists of a QKV matrix and an MLP in each layer. The shapes of the QKV matrix and MLP are identical to those in the pretrained language model and initialized from them. The motivation is that each attention head in the language model captures a certain aspect of semantic information, while a trainable visual expert can transform the image features to align with the different heads, therefore enabling deep fusion.</p><p>Formally, suppose that the input hidden states of an attention layer are X ? R B?H?(L I +L T )?D , where B is the batch size, L I and L T are the lengths of image and text sequences, H is the number of attention heads, and D is the hidden size. In the attention with visual expert, X is first split as Preprint image hidden states X I and text hidden states X T , and the attention is computed as:</p><formula xml:id="formula_0">Attention(X, W I , W T ) = softmax( Tril(QK T ) ? D )V,<label>(1)</label></formula><formula xml:id="formula_1">Q = concat(X I W Q I , X T W Q T ), K = concat(X I W K I , X T W K T ), V = concat(X I W V I , X T W V T ),<label>(2)</label></formula><p>where W I , W T are the QKV matrices of the visual expert and original language model, and Tril(?) means lower-triangular mask. The visual expert in FFN layers performs similarly,</p><formula xml:id="formula_2">FFN(X) = concat(FFN I (X I ), FFN T (X T )),<label>(3)</label></formula><p>where FFN I and FFN T are the FFN of the visual expert and original language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">PRETRAINING</head><p>Data. The image-text pairs for pretraining are all publicly available, including LAION-2B and COYO-700M. After removing the broken URLs, NSFW images, images with noisy captions, images with political bias and images with an aspect ratio &gt; 6 or &lt; 1/6, about 1.5B images are left for pretraining.</p><p>We also crafted a visual grounding dataset of 40M images. Each noun in the image caption is associated with bounding boxes to indicate the positions in the image. The construction process basically follows <ref type="bibr">Peng et al., which extracts nouns via spaCy (Honnibal &amp; Johnson, 2015)</ref> and predicts the bounding boxes using GLIPv2 <ref type="bibr" target="#b56">(Zhang et al., 2022)</ref>. The image-text pairs are sampled from LAION-115M, a subset of LAION-400M filtered by <ref type="bibr">Li et al. (2023)</ref>. We filter and retain a subset of 40 million images to ensure that over 75% of images contain at least two bounding boxes.</p><p>Training. The first stage of pretraining is for image captioning loss, i.e. next token prediction in the text part. We train the CogVLM-17B model on the 1.5B image-text pairs introduced above for 120,000 iterations with a batch size of 8,192. The second stage of pretraining is a mixture of image captioning and Referring Expression Comprehension (REC). REC is a task to predict the bounding box in the image given the text description of an object, which is trained in the form of VQA, i.e., "Question: Where is the object?" and "Answer: [[x 0 , y 0 , x 1 , y 1 ]]". Both x and y coordinates range from 000 to 999, meaning the normalized position in the image. We only consider the loss of the next token prediction in the "Answer" part. We pretrain the second stage for 60,000 iterations with a batch size of 1,024 on the text-image pairs and visual grounding datasets introduced above. During the final 30,000 iterations, we change the input resolution from 224 ? 224 to 490 ? 490. The total number of trainable parameters is 6.5B and the pretraining consumes about 4,096 A100?days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">ALIGNMENT</head><p>We further finetune CogVLM on a broad range of tasks, so as to align CogVLM with free-form instructions of any topic. We name the finetuned model CogVLM-Chat. As the examples in Figure <ref type="figure" target="#fig_0">2</ref> and Appendix show, CogVLM-Chat can successfully align with diverse instructions, thus enabling flexible interaction with humans.</p><p>Data. The high-quality data for supervised finetuning (SFT) is collected from LLaVA-Instruct <ref type="bibr">(Liu et al., 2023b)</ref>, LRV-Instruction <ref type="bibr">(Liu et al., 2023a)</ref>, LLaVAR <ref type="bibr" target="#b58">Zhang et al. (2023)</ref> and an in-house dataset, with a total of about 500,000 VQA pairs. The quality of SFT data is of vital importance, but the LLaVA-Instruct is generated by a pipeline involving language-only GPT-4 so that errors are inevitable. Particularly, we corrected the errors in the LLaVA-Instruct dataset via manual inspection and annotation.</p><p>SFT. For supervised finetuning, we train 8,000 iterations with a batch size of 640, a learning rate of 10 -5 and 50 warm-up iterations.</p><p>In order to prevent overfitting the text answer of the dataset, we leverage a smaller learning rate (10% the learning rate of the other parameters) to update the pretrained language model. All the parameters except ViT encoder are trainable during SFT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>To rigorously validate the superior performance and robust generalization of our base model, we conduct quantitative evaluations on an array of multi-modal benchmarks. These benchmarks can be categorized into three broad areas covering a comprehensive range of measurement<ref type="foot" target="#foot_0">1</ref> :</p><p>? Image Captioning. The main purpose of these tasks is to generate textual captions summarizing the major content of a given image. We utilize prominent datasets including No-Caps <ref type="bibr" target="#b0">(Agrawal et al., 2019)</ref>, COCO <ref type="bibr" target="#b25">(Lin et al., 2014</ref><ref type="bibr">), Flickr30K (Plummer et al., 2015)</ref>, and TextCaps <ref type="bibr" target="#b42">(Sidorov et al., 2020)</ref> for evaluation.</p><p>? Visual Question Answering. The VQA tasks require models to answer questions that may focus on distinct visual contents based on the given image. Our assessment covers diverse datasets, including VQAv2 <ref type="bibr" target="#b2">(Antol et al., 2015)</ref>, OKVQA <ref type="bibr" target="#b35">(Marino et al., 2019)</ref>, TextVQA <ref type="bibr" target="#b43">(Singh et al., 2019)</ref>, VizWiz-VQA <ref type="bibr" target="#b16">(Gurari et al., 2018)</ref>, OCRVQA <ref type="bibr" target="#b36">(Mishra et al., 2019)</ref>, ScienceQA <ref type="bibr">(Lu et al., 2022b)</ref>, and TDIUC <ref type="bibr" target="#b41">(Shrestha et al., 2019)</ref>.</p><p>? Visual Grounding. Visual grounding involves a set of tasks that establish referential links between textual mentions in a sentence and specific regions in an image. We evaluate our model on the typical datasets, including Visual7w <ref type="bibr" target="#b60">(Zhu et al., 2016)</ref>, RefCOCO <ref type="bibr" target="#b28">(Liu et al., 2017)</ref>, RefCOCO+, and RefCOCOg to ensure completeness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">IMAGE CAPTIONING</head><p>We evaluate the image captioning capability of our pretrained base model on the aforementioned four benchmarks. In a zero-shot evaluation on the Nocaps and Flickr datasets, we assess the precision of our model in describing long-tail visual concepts. Additionally, we present results from finetuning on the COCO and TextCaps datasets.</p><p>The detailed performance is shown in Table <ref type="table" target="#tab_1">1</ref>. Overall, our model achieves the SOTA or compatible performance across the board. Specifically, on the NoCaps benchmark, our base model outperforms the previous best method, GIT2, across four splits with a maximum of 5.7 points in the out-domain set while only consuming 10% of the pretraining data (1.5B vs 12.9B). On the Flickr benchmark, our model achieves a SOTA score of 94.9 surpassing the concurrently released Qwen-VL model by 9.1 points. These results demonstrate a remarkable capability and robustness of our pretrained model on the image captioning task. We also evaluate on the COCO <ref type="bibr" target="#b25">(Lin et al., 2014)</ref> and TextCaps, where the latter is specifically designed to integrate the textual information of the given image into captions.</p><p>Though training without the dedicated OCR data, encouragingly, our base model reveals a significant text-reading ability and obtains a competitive performance with PaLI-X-55B, and outperforms the previous best model of the same scale, PaLI-17B, by 9.1 points score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">VISUAL QUESTION ANSWERING</head><p>Visual Question Answering is a task of validating general multi-modal capabilities of models, which requires a mastery of skills including vision-language understanding and commonsense reasoning. We evaluate our model on 7 VQA benchmarks: VQAv2, OKVQA, GQA, VizWiz-QA, OCRVQA, TextVQA, ScienceQA, covering a wide range of visual scenes. We train our base model on the training sets and evaluate it on the publicly available val/test sets for all benchmarks, where both procedures adopt the open-vocabulary generation settings without OCR pipeline input. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint</head><p>Generalist performance. In order to fairly compare with Unified-IO <ref type="bibr">(Lu et al., 2022a)</ref>, Qwen-VL <ref type="bibr">(Bai et al., 2023a)</ref>, mPLUG-DocOwl <ref type="bibr" target="#b53">(Ye et al., 2023)</ref> and other models trained in a generalist paradigm across multi-modal tasks, we further trained a unified model using data composed of dozens of multi-modal datasets and utilized a consistent checkpoint for evaluation. The datasets encompass 14 QA datasets such as VQAv2, OKVQA, and extending to TextVQA, as well as caption datasets including COCO caption, TextCaps, and those used during the pre-training phase. Experimental results show that multitask learning does not significantly reduce the model's performance on individual tasks, and CogVLM remains leading in performance across all tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">VISUAL GROUNDING</head><p>In order to endow our model with consistent, interactive visual grounding capabilities, we collect a high-quality dataset covering 4 types of grounding data: (1) Grounded Captioning (GC) -image captioning datasets where each noun phrase within the caption is followed by the corresponding referential bounding boxes; (2) Referring Expression Generation (REG) -image-oriented datasets that each bounding box in the image is annotated with a descriptive textual expression that accurately characterizes and refers to the content within the specific region;</p><p>(3) Referring Expression Comprehension (REC) -text-oriented datasets that each textual description is annotated with multiple referential links associating the phrases with corresponding boxes; (4) Grounded Visual Question Answering (GroundedVQA) -VQA-style datasets where the questions may contain region references in a given image. The sources of grounding data are all publicly available, including Flickr30K Entities <ref type="bibr" target="#b39">(Plummer et al., 2015)</ref>, RefCOCO <ref type="bibr" target="#b20">(Kazemzadeh et al., 2014;</ref><ref type="bibr" target="#b34">Mao et al., 2016;</ref><ref type="bibr" target="#b55">Yu et al., 2016)</ref>, Visual7W <ref type="bibr" target="#b60">(Zhu et al., 2016)</ref>, VisualGenome <ref type="bibr" target="#b22">(Krishna et al., 2017)</ref> and Grounded CoT-VQA <ref type="bibr">(Chen et al., 2023a)</ref>.</p><p>[box] in this section is in the format of [[x 0 , y 0 , x 1 , y 1 ]].</p><p>After the second pretraining stage using our 40M visual grounding dataset, we continue to train our model on this high-quality dataset, resulting in a generalist grounding-enhanced model, CogVLM-Grounding. It is noteworthy that the curated datasets exhibit a versatility of visual grounding capabilities, and many datasets can be adapted and repurposed across different tasks. For instance, grounded captioning datasets can be reformulated to suit REG and REC tasks. Taking the example of "A man [box 1 ] and a woman [box 2 ] are walking together.", this can be reframed into question answering pairs like ("Describe this region [box 2 ].", "A woman.") and ("Where is the man?", "[box 1 ]"). Similarly, REC datasets can be translated into REG tasks by switching the input and output, and vice versa. However, certain conversions might lead to ambiguities. For example, when presented with the isolated query "Where is another man?" from the caption "A man [box 1 ] is running, while another man [box 2 ] is looking.", the distinction between [box 1 ] and [box 2 ] becomes unclear, potentially leading to errors.</p><p>Table <ref type="table" target="#tab_4">4</ref> shows the result on the standard visual grounding benchmarks. We find that our generalist model achieves state-of-the-art performance across the board, with a significant advantage over the previous or concurrent models. Moreover, we also evaluate the specialist performance of our model finetuned on each individual training set of benchmarks for fair comparison with the best models dedicated on each task. As shown in the bottom part of Table <ref type="table" target="#tab_4">4</ref>, our model achieves the SOTA performance over 5 of 9 splits, and the compatible result on the other subsets. These results suggest a remarkable visual grounding capability of our model incorporating our training paradigm.</p><p>Preprint </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">INSTRUCTION FOLLOWING IN REAL-WORLD USER BEHAVIOR</head><p>To evaluate the CogVLM-Chat model's capacity under real-world user behavior, we further employ TouchStone <ref type="bibr">(Bai et al., 2023b)</ref>, an extensive benchmark for multimodal language models. Table <ref type="table" target="#tab_5">5</ref> shows the <ref type="bibr">GPT-4 (OpenAI, 2023)</ref> similarity scores of the generated and standard answer, suggesting CogVLM-Chat significantly outperforms all the other publicly available VLMs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">ABLATION STUDY</head><p>To understand the impact of various components and settings on our model's performance, we conduct an extensive ablation study for 6,000 iterations and a batch size of 8,192. Table <ref type="table" target="#tab_6">6</ref> summarizes the results about the following aspects:</p><p>Model structure and tuned parameters. We investigate the effectiveness of tuning only the MLP Adapter layer or tuning all LLM parameters and the Adapter without adding VE, as well as modifying the VE architecture to add full VE at every 4th LLM layer or only the FFN-equipped VE at all layers. From the results we can see that only tuning the adapter layer (e.g., BLIP2) may result in a shallow alignment with significantly inferior performance, and decreasing either the number of VE layers or the VE parameters at each LLM layer suffers a prominent degradation.</p><p>Initialization Method. We investigate the effectiveness of initializing VE weights from LLM, and the slight decrease in performance suggests a positive impact of this method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint</head><p>Visual Attention Mask. We empirically find that using a causal mask on visual tokens will yield a better result in comparison with a full mask. We hypothesize the possible explanation for this phenomenon is that the causal mask better fits the inherent structure of LLM.</p><p>Image SSL Loss. We also investigated the self-supervised learning loss on image features, where each visual feature predicts the CLIP feature of the next position for visual self-supervision. Align with the observation from PaLI-X <ref type="bibr">(Chen et al., 2023b)</ref>, we find it brings no improvement on downstream tasks, although we indeed observed improvements in small models in our early experiments.</p><p>EMA. We utilize EMA (Exponential Moving Average) during pretraining, which often brings improvements across various tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSION</head><p>In this paper, we introduce CogVLM, an open visual language foundation model. CogVLM shifts the paradigm for VLM training from shallow alignment to deep fusion, achieving state-of-the-art performance on 10 classic multi-modal benchmarks.</p><p>The VLM training is still in its infancy, and there are many directions to explore, for example, better SFT alignment, RLHF and anti-hallucination. Since the previous famous VLMs are mostly closed-source, we believe CogVLM will be a solid foundation for future multi-modal research.</p><p>? TextVQA <ref type="bibr" target="#b43">(Singh et al., 2019)</ref> TextVQA is a dataset with 45,336 questions on 28,408 images that challenges models to detect, read, and reason about text within images to provide answers.</p><p>A.2.4 GROUNDING ? Visual7W <ref type="bibr" target="#b60">(Zhu et al., 2016)</ref>. The Visual7W dataset is predominantly designed for VQA tasks, with a dedicated subset crafted for grounded VQA. In this subset, models are presented with an image accompanied by a "which"-type question, such as "Which is the small computer in the corner?". Participants are then given four bounding boxes within the image, from which they must select the correct one as the answer. The grounded Visual7W part consists of 25,733 images and 188,068 questions.</p><p>? Flickr30K-Entities <ref type="bibr" target="#b39">(Plummer et al., 2015)</ref>. The Flickr30K Entities dataset, a precursor in the realm of grounded captioning, encompasses a collection of 31,783 images accompanied by 158k captioning annotations. Every caption in this dataset has been meticulously annotated such that each noun phrase is linked with a manually delineated referential bounding box. In total, there are 276k such annotated bounding boxes provided within this dataset.</p><p>? VisualGenome <ref type="bibr" target="#b22">(Krishna et al., 2017)</ref>. The VisualGenome dataset stands as a cornerstone in understanding the multifaceted relationships present within images. With a collection of over 100k images, each image is annotated in detail, capturing an average of 21 objects, 18 attributes, and 18 inter-object relationships. A unique aspect of this dataset is the alignment of objects, attributes, relationships, and region descriptions with standardized terminologies from WordNet. Specifically tailored for the REG and REC tasks, each annotated region in an image comes with a corresponding descriptive text, making it a rich resource for image understanding and semantic modeling. We use the subset with around 86k images and 3.6 million region-caption pairs for visual grounding. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Qualitative examples generated by CogVLM.</figDesc><graphic url="image-1.png" coords="2,108.00,67.68,396.00,572.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The architecture of CogVLM. (a) The illustration about the input, where an image is processed by a pretrained ViT and mapped into the same space as the text features. (b) The Transformer block in the language model. The image features have a different QKV matrix and FFN. Only the purple parts are trainable.</figDesc><graphic url="image-2.png" coords="4,108.00,81.86,396.00,236.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance on TDIUC benchmark with fine-grained questions classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>, our model achieves state-of-the-art performance on 6 of 7 benchmarks com-</cell></row><row><cell>pared with models of similar scales, such as PALI-17B and Qwen-VL. Our model even surpasses</cell></row><row><cell>models of much larger scale on multiple benchmarks, such as PaLI-X-55B on VizWiz-QA (test-std</cell></row><row><cell>+5.1, test-dev +3.8), PALM-E-84B on VQAv2 (test-dev +4.2) and OKVQA(+1.4), Flamingo-80B</cell></row></table><note><p><p><p>on VQAv2 (test-dev +2.7, test-std +2.6), VizWiz-QA (test-dev +10.7, test-std +10.4) and TextVQA (+15.6). Our model also achieves the optimal scores of 92.71 on the multi-modal split (i.e., IMG) of ScienceQA</p>(Lu et al., 2022b)</p>, achieving a new SOTA. These results suggest that our base model can serve as a strong multi-modal backbone capable of solving various visual question answering tasks.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance on Image Captioning benchmarks, where all tasks use CIDEr as the evaluation metric. OOD refers to out-of-domain test set. Karp. refers to the Karpathy test split.</figDesc><table><row><cell>Method</cell><cell>Train Data</cell><cell cols="7">NoCaps val OOD overall OOD overall Karp. Karp. test NoCaps test Flickr COCO TextCaps</cell></row><row><cell>Human</cell><cell>-</cell><cell>95.7</cell><cell>87.1</cell><cell>91.6</cell><cell>85.3</cell><cell>-</cell><cell>-</cell><cell>125.1</cell></row><row><cell>VinVL (Zhang et al., 2021)</cell><cell cols="2">8.9M 83.8</cell><cell>94.3</cell><cell>78.0</cell><cell>92.5</cell><cell>-</cell><cell cols="2">130.8 -</cell></row><row><cell>SimVLM (Wang et al., 2021)</cell><cell>1.8B</cell><cell cols="5">115.2 112.2 109.5 110.3 -</cell><cell cols="2">143.3 -</cell></row><row><cell>CoCa (Yu et al., 2022)</cell><cell>4.8B</cell><cell>-</cell><cell cols="2">122.4 -</cell><cell cols="2">120.6 -</cell><cell cols="2">143.6 -</cell></row><row><cell>LEMON (Hu et al., 2022)</cell><cell>2B</cell><cell cols="5">120.2 117.3 110.1 114.3 -</cell><cell cols="2">139.1 -</cell></row><row><cell>Flamingo (Alayrac et al., 2022)</cell><cell>2.3B</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>67.2</cell><cell cols="2">138.1 -</cell></row><row><cell>Prismer (Liu et al., 2023c)</cell><cell cols="4">12.7M 113.5 112.9 -</cell><cell cols="2">110.8 -</cell><cell cols="2">136.5 -</cell></row><row><cell>BLIP-2 (Li et al., 2023)</cell><cell cols="4">129M 124.8 121.6 -</cell><cell>-</cell><cell>-</cell><cell cols="2">144.5 -</cell></row><row><cell>InstructBLIP (Dai et al., 2023)</cell><cell cols="2">129M -</cell><cell cols="2">123.1 -</cell><cell>-</cell><cell>82.4</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">UniversalCap (Cornia et al., 2021) 35M</cell><cell cols="5">123.4 122.1 114.3 119.3 -</cell><cell cols="2">143.4 -</cell></row><row><cell>GIT (Wang et al., 2022a)</cell><cell>0.8B</cell><cell cols="5">127.1 125.5 122.0 123.4 49.6</cell><cell cols="2">144.8 138.2</cell></row><row><cell>GIT2 (Wang et al., 2022a)</cell><cell cols="6">12.9B 130.6 126.9 122.3 124.8 50.7</cell><cell cols="2">145.0 145.0</cell></row><row><cell>Qwen-VL (Bai et al., 2023a)</cell><cell>1.4B</cell><cell>-</cell><cell cols="2">121.4 -</cell><cell>-</cell><cell>85.8</cell><cell>-</cell><cell>-</cell></row><row><cell>PaLI-17B (Chen et al., 2022b)</cell><cell>1.6B</cell><cell>-</cell><cell cols="2">127.0 -</cell><cell cols="2">124.4 -</cell><cell cols="2">149.1 135.4</cell></row><row><cell cols="2">PaLI-X-55B (Chen et al., 2023b) -</cell><cell>-</cell><cell cols="2">126.3 -</cell><cell cols="2">124.3 -</cell><cell cols="2">149.2 147.0</cell></row><row><cell>CogVLM (ours)</cell><cell>1.5B</cell><cell cols="5">132.6 128.3 128.0 126.4 94.9</cell><cell cols="2">148.7 144.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance on Visual Question Answering benchmarks, where the results labeled with * refers to the few-shot or zero-shot setting.</figDesc><table><row><cell>Method</cell><cell cols="2">VQAv2</cell><cell cols="7">OKVQA GQA VizWizQA OCRVQA TextVQA SciQA</cell></row><row><cell></cell><cell>test-</cell><cell>test-</cell><cell>val</cell><cell>test-</cell><cell>test-</cell><cell>test-</cell><cell>test</cell><cell>test</cell><cell>IMG</cell></row><row><cell></cell><cell>dev</cell><cell>std</cell><cell></cell><cell>balanced</cell><cell>dev</cell><cell>std</cell><cell></cell><cell></cell></row><row><cell>Closed-ended classification models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SimVLM (Wang et al., 2021)</cell><cell cols="2">80.0 80.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CoCa (Yu et al., 2022)</cell><cell cols="2">82.3 82.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>OFA (Wang et al., 2022b)</cell><cell cols="2">82.0 82.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BEiT-3 Wang et al. (2022c)</cell><cell cols="2">84.2 84.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Open-ended generation models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GIT (Wang et al., 2022a)</cell><cell cols="2">78.6 78.8</cell><cell>-</cell><cell>-</cell><cell cols="2">68.0 67.5</cell><cell>68.1</cell><cell>59.8</cell><cell>-</cell></row><row><cell>GIT2 (Wang et al., 2022a)</cell><cell cols="2">81.7 81.9</cell><cell>-</cell><cell>-</cell><cell cols="2">71.0 70.1</cell><cell>70.3</cell><cell>67.3</cell><cell>-</cell></row><row><cell cols="3">Flamingo-80B (Alayrac et al., 2022) 82.0 82.1</cell><cell cols="2">57.8* -</cell><cell cols="2">65.7 65.4</cell><cell>-</cell><cell>54.1</cell><cell>-</cell></row><row><cell>BLIP-2 (Li et al., 2023)</cell><cell cols="2">82.2 82.3</cell><cell cols="2">59.3 44.7*</cell><cell>-</cell><cell>-</cell><cell>72.7</cell><cell>-</cell><cell>89.5</cell></row><row><cell>InstructBLIP (Dai et al., 2023)</cell><cell>-</cell><cell>-</cell><cell cols="2">62.1 49.5*</cell><cell cols="2">34.5* -</cell><cell>73.3</cell><cell>50.7*</cell><cell>90.7</cell></row><row><cell>PaLI-17B Chen et al. (2022b)</cell><cell cols="2">84.3 84.3</cell><cell cols="2">64.5 -</cell><cell cols="2">71.6 70.7</cell><cell>-</cell><cell>58.8</cell><cell>-</cell></row><row><cell>PaLI-X-55B (Chen et al., 2023b)</cell><cell cols="2">86.0 86.1</cell><cell cols="2">66.1 -</cell><cell cols="2">72.6 70.9</cell><cell>75.0</cell><cell>71.4</cell><cell>-</cell></row><row><cell cols="3">PaLM-E-84B (Driess et al., 2023) 80.5 -</cell><cell cols="2">63.3 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CogVLM (ours)</cell><cell cols="2">84.7 84.7</cell><cell cols="2">64.7 65.2</cell><cell cols="2">76.4 75.8</cell><cell>74.5</cell><cell>69.7</cell><cell>92.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Generalist performance on Image Captioning and VQA benchmarks.</figDesc><table><row><cell>Method</cell><cell cols="8">COCO TextCaps NoCaps Flickr VQAv2 OKVQA TextVQA OCRVQA Karp.-test val val Karp.-test test-dev val val test</cell></row><row><cell>Qwen-VL (Bai et al., 2023a)</cell><cell>-</cell><cell>-</cell><cell>121.4</cell><cell>85.8</cell><cell>79.5</cell><cell>58.6</cell><cell>63.8</cell><cell>75.7</cell></row><row><cell>mPLUG-DocOwl (Ye et al., 2023)</cell><cell>-</cell><cell>111.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>52.6</cell><cell>-</cell></row><row><cell>Unified-IO (Lu et al., 2022a)</cell><cell>122.3</cell><cell>-</cell><cell>100.0</cell><cell>-</cell><cell>77.9</cell><cell>54.0</cell><cell>-</cell><cell>-</cell></row><row><cell>CogVLM (single task)</cell><cell>148.7</cell><cell>149.8</cell><cell>128.3</cell><cell>94.9</cell><cell>84.7</cell><cell>64.7</cell><cell>69.3</cell><cell>74.5</cell></row><row><cell>CogVLM (generalist)</cell><cell cols="8">147.0(-1.7) 151.3(+1.5) 126.2(-2.1) 92.7(-2.2) 83.4(-1.3) 58.9(-5.8) 68.1(-1.2) 74.1(-0.4)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results on Referring Expression Comprehension and Grounded Visual Question Answering.</figDesc><table><row><cell>Type</cell><cell>Model</cell><cell></cell><cell>RefCOCO</cell><cell></cell><cell></cell><cell>RefCOCO+</cell><cell></cell><cell cols="2">RefCOCOg Visual7W</cell></row><row><cell></cell><cell></cell><cell cols="8">val test-A test-B val test-A test-B val test</cell><cell>test</cell></row><row><cell></cell><cell>OFA-L* (Wang et al., 2022b)</cell><cell cols="8">79.96 83.67 76.39 68.29 76.00 61.75 67.57 67.58</cell><cell>-</cell></row><row><cell></cell><cell cols="2">VisionLLM-H (Wang et al., 2023b) -</cell><cell>86.70</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Shikra-7B (Chen et al., 2023a)</cell><cell cols="8">87.01 90.61 80.24 81.60 87.36 72.12 82.27 82.19</cell><cell>-</cell></row><row><cell>Generalist</cell><cell>Shikra-13B</cell><cell cols="8">87.83 91.11 81.81 82.89 87.79 74.41 82.64 83.16 85.33</cell></row><row><cell></cell><cell>Qwen-VL (Bai et al., 2023a)</cell><cell cols="8">89.36 92.26 85.34 83.12 88.25 77.21 85.58 85.48</cell><cell>-</cell></row><row><cell></cell><cell>CogVLM</cell><cell cols="8">92.51 93.95 88.73 87.52 91.81 81.43 89.46 90.09 90.96</cell></row><row><cell></cell><cell>G-DINO-L Liu et al. (2023d)</cell><cell cols="8">90.56 93.19 88.24 82.75 88.95 75.92 86.13 87.02</cell><cell>-</cell></row><row><cell>Specialist</cell><cell cols="9">UNINEXT-H (Lin et al., 2023) ONE-PEACE (Wang et al., 2023a) 92.58 94.18 89.26 88.77 92.21 83.23 89.22 89.27 92.64 94.33 91.46 85.24 89.63 79.79 88.73 89.37</cell><cell>--</cell></row><row><cell></cell><cell>CogVLM (single task)</cell><cell cols="8">93.40 94.06 90.28 87.76 93.02 81.81 90.07 90.53 91.17</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Evaluation results on TouchStone in English.</figDesc><table><row><cell cols="8">Models MiniGPT4 InstructBLIP LLaMA-AdapterV2 LLaVA mPLUG-Owl Qwen-VL-Chat CogVLM-Chat</cell></row><row><cell>Score</cell><cell>531.7</cell><cell>552.4</cell><cell>590.1</cell><cell>602.7</cell><cell>605.4</cell><cell>645.4</cell><cell>662.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Ablation studies for various components and training settings.</figDesc><table><row><cell cols="2">Ablated Aspects Original (CogVLM)</cell><cell>Ablated Setting</cell><cell cols="4">Trainable COCO NoCaps OKVQA TextVQA VQAv2 params CIDEr? CIDEr? top1? top1? top1?</cell></row><row><cell>Tuned Parameters</cell><cell>VE-full every layer + MLP Adapter</cell><cell cols="2">MLP Adapter LLM+MLP Adapter 6.9B 140.3 118.5 140M 131.2 111.5 VE-full every 4th layer 1.7B 138.7 117.4 VE-FFN every layer 4.4B 140.0 118.7</cell><cell>55.1 56.8 58.9 58.2</cell><cell>40.7 44.7 44.1 45.1</cell><cell>73.8 78.9 77.6 78.6</cell></row><row><cell>Init method</cell><cell>From LLM</cell><cell>Random init</cell><cell>6.6B 138.0 117.9</cell><cell>55.9</cell><cell>44.0</cell><cell>79.1</cell></row><row><cell>Visual attention mask</cell><cell>Causal mask</cell><cell>Full mask</cell><cell>6.6B 141.0 117.2</cell><cell>57.4</cell><cell>45.1</cell><cell>79.6</cell></row><row><cell>Image SSL loss</cell><cell></cell><cell>(clip feature)</cell><cell>6.6B 142.9 119.8</cell><cell>58.7</cell><cell>45.9</cell><cell>79.7</cell></row><row><cell>EMA</cell><cell></cell><cell></cell><cell>6.6B 143.1 119.2</cell><cell>57.1</cell><cell>43.8</cell><cell>79.4</cell></row><row><cell>CogVLM (ours)</cell><cell>-</cell><cell>-</cell><cell>6.6B 142.8 120.1</cell><cell>59.3</cell><cell>45.3</cell><cell>80.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>?</head><label></label><figDesc>RefCOCO/RefCOCO+<ref type="bibr" target="#b28">(Liu et al., 2017)</ref> RefCOCO and RefCOCO+ evolved from the ReferItGame. Both subsets focus on images with two or more similar objects. Ref-COCO, with 142,209 expressions across 19,994 images, places no linguistic constraints. Conversely, RefCOCO+ emphasizes appearance-centric descriptions, omitting locational terms, and comprises 141,564 expressions over 19,992 images. ? RefCOCOg Mao et al. (2016) The RefCOCOg subset was amassed through Amazon Mechanical Turk, where workers penned natural referring expressions for objects in MSCOCO images; it boasts 85,474 referring expressions spanning 26,711 images, each containing 2 to 4 objects of the same category.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Detailed summary of all benchmarks and corresponding metrics are available at Appendix A.2.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/THUDM/VisualGLM-6B</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/THUDM/SwissArmyTransformer</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We thank <rs type="person">Xiaohan Zhang</rs> for managing the labeling team, <rs type="person">Zhao Xue</rs> for data management, and <rs type="person">Yue Cao</rs>, <rs type="person">Li Dong</rs> from <rs type="affiliation">MSRA and Chang Zhou from Alibaba</rs> for their discussion.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 DETAILS OF TRAINING SETTINGS</head><p>We report the details of parameter settings during pre-training and multitask training in Table <ref type="table">7</ref> and<ref type="table">Table 8</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 DETAILS OF ASSOCIATED DATASETS</head><p>In this section, we introduce the details of datasets and their use in our evaluation process for all associated benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 IMAGE CAPTIONING</head><p>? COCO <ref type="bibr" target="#b25">(Lin et al., 2014)</ref> The Captions in COCO dataset are collected using Amazon's Mechanical Turk (AMT) workers who are given instructions to control the quality. The dataset contains 330K images, where the train, validation and test sets contain 413,915 captions for 82,783 images, 202,520 captions for 40,504 images, and 379,249 captions for 40,775 images respectively.</p><p>? NoCaps <ref type="bibr" target="#b0">(Agrawal et al., 2019)</ref>. NoCaps is a large-scale benchmark for novel object captioning, containing nearly 400 novel object classes compared to COCO. The validation and test set comprised of 4,500 and 10,600 images, respectively, sourced from the Open Preprint Images <ref type="bibr" target="#b21">(Krasin et al., 2017)</ref> and annotated with 11 human-generated captions per image, and each set is subdivided into three domains: "in", "near", and "out", with objects in the "out-domain" never appearing in the COCO dataset.</p><p>? <ref type="bibr">Flickr30K (Plummer et al., 2015)</ref>. Flickr30K is a high-quality dataset consists of 31,783 images of everyday life activities, envets and scenes (all harvested from the online website Flickr) and 158,915 captions (obtained via crodsourcing). Each image in this dataset is described independently by five annotators who are not familiar with the specific entities and circumstances depicted in them.</p><p>? TextCaps <ref type="bibr" target="#b42">(Sidorov et al., 2020)</ref>  ? OKVQA <ref type="bibr" target="#b35">(Marino et al., 2019)</ref> The OK-VQA (Outside Knowledge Visual Question Answering) dataset is specifically designed to probe visual question answering capabilities that necessitate external knowledge or common sense beyond image content. It has 14,055 open-ended questions and 5 ground truth answers per question.</p><p>? VizWiz-VQA <ref type="bibr" target="#b16">(Gurari et al., 2018)</ref> The VizWiz-VQA dataset is derived from blind individuals capturing images and voicing related questions, accompanied by 10 crowdsourced responses per query. The central challenge of this dataset involves predicting the visual question's answer and determining if it's unanswerable.</p><p>? ScienceQA <ref type="bibr">(Lu et al., 2022b)</ref> The ScienceQA dataset comprises 21,208 multimodal multiple-choice questions spanning three diverse subjects: natural science, language science, and social science. Each question is annotated with explanations linked to relevant lectures.</p><p>? TDIUC <ref type="bibr" target="#b41">(Shrestha et al., 2019)</ref> The TDIUC dataset features 1.6M questions across 170K images from MS COCO and Visual Genome. Categorized into 12 distinct question types, it ranges from basic tasks like identifying objects or colors to more advanced reasoning like counting or positional discernment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.3 TEXT-ORIENTED VQA</head><p>? OCRVQA <ref type="bibr" target="#b36">(Mishra et al., 2019)</ref> OCR-VQA consists of 207,572 book cover images with over 1 million question-answer pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ADDITIONAL FINE-GRAINED EXPERIMENTS</head><p>To comprehensively investigate the proposed model on specific topics and question types, we further conduct extensive experiments on a representative benchmark, TDIUC <ref type="bibr" target="#b19">(Kafle &amp; Kanan, 2017)</ref>. We use the publicly available split of val set as evaluation data, and the VQA accuracy calculated from their official scripts as the evaluation metric.</p><p>The experimental results on TDIUC compare our model against the specialist SOTA method MUREL <ref type="bibr" target="#b5">(Cadene et al., 2019)</ref> are shown in Figure <ref type="figure">4</ref>. From the experimental result, we can see that our model consistently outperforms the previous model on 12 specific question types, resulting in a 94.0 accuracy score compared to the previous SOTA of 88.2 on the overall dataset. These results demonstrate that our model exhibits comprehensive problem-solving skills on general VQA tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C COMPUTATIONAL EFFICIENCY</head><p>In this section, we compare the computational efficiency of our model with other state-of-theart models, considering both pretraining and finetuning data from datasets such as VQAv2 and TextVQA. Owing to an optimized architecture and the utilization of high-quality pretraining data, our model demonstrates a marked reduction in resource consumption during training relative to models with comparable parameter magnitudes. This training method aligns visual information well to the semantic space of ChatGLM. In the subsequent fine-tuning phase, the model is trained on long visual question answering data to generate answers that align with human preferences.</p><p>VisualGLM-6B is trained using the SwissArmyTransformer 3 (abbreviated as sat) library, a utility library for flexible modification and training of Transformer, supporting efficient fine-tuning methods like LoRA and p-tuning. This project provides a user-friendly huggingface interface, as well as an interface based on sat.</p><p>However, as a shallow-alignment model, VisualGLM-6B is known to have quite a few limitations, such as factual inaccuracy, or model hallucination, in image description, lack of capturing image detail information, and some limitations from the language model.</p><p>With model quantization, one can deploy locally on consumer-grade graphics cards (requiring as little as 8.7G memory under INT4 quantization level).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nocaps: Novel object captioning at scale</title>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8948" to="8957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Flamingo: a visual language model for few-shot learning</title>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yana</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karel</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="23716" to="23736" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Jinze</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shusheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.12966</idno>
		<title level="m">Qwen-vl: A frontier large vision-language model with versatile abilities</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shusheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinze</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.16890</idno>
		<title level="m">Evaluating vision-language models by language models</title>
		<meeting><address><addrLine>Touchstone</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Murel: Multimodal relational reasoning for visual question answering</title>
		<author>
			<persName><forename type="first">Remi</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hedi</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1989" to="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Keqin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weili</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.15195</idno>
		<title level="m">Shikra: Unleashing multimodal llm&apos;s referential dialogue magic</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A generalist framework for panoptic segmentation of images and videos</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lala</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.06366</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Padlewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Salz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basil</forename><surname>Grycner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.06794</idno>
		<title level="m">A jointly-scaled multilingual language-image model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Josip</forename><surname>Preprint Xi Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basil</forename><surname>Padlewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soravit</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><forename type="middle">Riquelme</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Tay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.18565</idno>
		<title level="m">On scaling up a multilingual vision and language model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<ptr target="https://vicuna.lmsys.org" />
		<imprint>
			<date type="published" when="2023-04">April 2023</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Universal captioner: Long-tail vision-and-language model training through content-style separation</title>
		<author>
			<persName><forename type="first">Marcella</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Fiameni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12727</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Instructblip: Towards general-purpose vision-language models with instruction tuning</title>
		<author>
			<persName><forename type="first">Wenliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huat</forename><surname>Tiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weisheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cogview: Mastering text-to-image generation via transformers</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">19822-19835, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Danny</forename><surname>Driess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corey</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayzaan</forename><surname>Ichter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Wahid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhe</forename><surname>Vuong</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.03378</idno>
		<title level="m">Palm-e: An embodied multimodal language model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><surname>Glm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10360</idno>
		<title level="m">General language model pretraining with autoregressive blank infilling</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Matthew Honnibal and Mark Johnson. An improved non-monotonic transition system for dependency parsing</title>
		<author>
			<persName><forename type="first">Danna</forename><surname>Gurari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abigale</forename><forename type="middle">J</forename><surname>Stangl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anhong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">P</forename><surname>Bigham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 conference on empirical methods in natural language processing</title>
		<meeting>the 2015 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2015">2018. 2015</date>
			<biblScope unit="page" from="1373" to="1378" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE conference on computer vision and pattern recognition</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Lora</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
		<title level="m">Low-rank adaptation of large language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scaling up vision-language pre-training for image captioning</title>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yumao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="17980" to="17989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An analysis of visual question answering algorithms</title>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1965" to="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Referitgame: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName><forename type="first">Sahar</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="787" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Openimages: A public dataset for large-scale multi-label and multi-class image classification</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<ptr target="https://github.com/openimages" />
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Preprint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.12597</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Uninext: Exploring a unified architecture for vision recognition</title>
		<author>
			<persName><forename type="first">Fangjian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sitong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.13700</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitnick</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014: 13th European Conference</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">September 6-12, 2014. 2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
	<note>Proceedings, Part V 13</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Aligning large multi-modal model with robust instruction tuning</title>
		<author>
			<persName><forename type="first">Fuxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaser</forename><surname>Yacoob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.14565</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong Jae</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.08485</idno>
		<title level="m">Visual instruction tuning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Referring expression generation and comprehension via attributes</title>
		<author>
			<persName><forename type="first">Jingyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4856" to="4864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Prismer: A vision-language model with an ensemble of experts</title>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linxi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaowei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.02506</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Grounding dino: Marrying dino with grounded pre-training for open-set object detection</title>
		<author>
			<persName><forename type="first">Shilong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhe</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.05499</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Gpt understands, too. AI Open, 2023e</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Unified-io: A unified model for vision, language, and multi-modal tasks</title>
		<author>
			<persName><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.08916</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learn to explain: Multimodal reasoning via thought chains for science question answering</title>
		<author>
			<persName><forename type="first">Pan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanglin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Kalyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2507" to="2521" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oana</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ok-vqa: A visual question answering benchmark requiring external knowledge</title>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/cvf conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/cvf conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3195" to="3204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ocr-vqa: Visual question answering by reading text in images</title>
		<author>
			<persName><forename type="first">Anand</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashank</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajeet</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirban</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 international conference on document analysis and recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="947" to="952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
		<respStmt>
			<orgName>OpenAI</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Kosmos-2: Grounding multimodal large language models to the world</title>
		<author>
			<persName><forename type="first">Zhiliang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaru</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.14824</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models</title>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Bryan A Plummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2641" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Noam</forename><surname>Preprint</surname></persName>
		</author>
		<author>
			<persName><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05202</idno>
		<title level="m">Glu variants improve transformer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Answer them all! toward universal visual question answering models</title>
		<author>
			<persName><forename type="first">Robik</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10472" to="10481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Textcaps: a dataset for image captioning with reading comprehension</title>
		<author>
			<persName><forename type="first">Oleksii</forename><surname>Sidorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">August 23-28, 2020. 2020</date>
			<biblScope unit="page" from="742" to="758" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II 16</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Towards vqa models that can read</title>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meet</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8317" to="8326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Quan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.15389</idno>
		<title level="m">Eva-clip: Improved training techniques for clip at scale</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Llama 2: Open foundation and fine-tuned chat models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multimodal few-shot learning with frozen language models</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Tsimpoukelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><forename type="middle">L</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serkan</forename><surname>Cabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="200" to="212" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Git: A generative image-to-text transformer for vision and language</title>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.14100</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="23318" to="23340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">One-peace: Exploring one general representation model toward unlimited modalities</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.11172</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Large language model is also an open-ended decoder for vision-centric tasks</title>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiannan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.11175</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Image as a foreign language: Beit pretraining for all vision and vision-language tasks</title>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Bjorck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiliang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kriti</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Owais</forename><surname>Khan Mohammed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saksham</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhojit</forename><surname>Som</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.10442</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><surname>Simvlm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.10904</idno>
		<title level="m">Simple visual language model pretraining with weak supervision</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Jiabo</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anwen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenlin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guohai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junfeng</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.02499</idno>
		<title level="m">mplug-docowl: Modularized multimodal large language model for document understanding</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Legg</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mojtaba</forename><surname>Seyedhosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01917</idno>
		<title level="m">Coca: Contrastive captioners are image-text foundation models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Modeling context in referring expressions</title>
		<author>
			<persName><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Poirson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2016: 14th European Conference, Amsterdam</title>
		<meeting><address><addrLine>The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">October 11-14, 2016. 2016</date>
			<biblScope unit="page" from="69" to="85" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II 14</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Glipv2: Unifying localization and visionlanguage understanding</title>
		<author>
			<persName><forename type="first">Pengchuan</forename><surname>Preprint Haotian Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yen-Chun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liunian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenq-Neng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="36067" to="36080" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Revisiting visual representations in vision-language models</title>
		<author>
			<persName><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Vinvl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5579" to="5588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Llavar: Enhanced visual instruction tuning for text-rich image understanding</title>
		<author>
			<persName><forename type="first">Yanzhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiuxiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nedim</forename><surname>Lipka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Minigpt-4: Enhancing vision-language understanding with advanced large language models</title>
		<author>
			<persName><forename type="first">Deyao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqian</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.10592</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Visual7w: Grounded question answering in images</title>
		<author>
			<persName><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4995" to="5004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
