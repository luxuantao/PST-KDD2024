<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning From and About Others: Towards Using Imitation to Bootstrap the Social Understanding of Others by Robots</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Cynthia</forename><surname>Breazeal</surname></persName>
							<email>cynthiab@media.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">MIT Media Lab</orgName>
								<address>
									<addrLine>77 Massachusetts Avenue NE18</addrLine>
									<postCode>5th f loor, 02142</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daphna</forename><surname>Buchsbaum</surname></persName>
							<email>daphna@media.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">MIT Media Lab</orgName>
								<address>
									<addrLine>77 Massachusetts Avenue NE18</addrLine>
									<postCode>5th f loor, 02142</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jesse</forename><surname>Gray</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Media Lab</orgName>
								<address>
									<addrLine>77 Massachusetts Avenue NE18</addrLine>
									<postCode>5th f loor, 02142</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Gatenby</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Media Lab</orgName>
								<address>
									<addrLine>77 Massachusetts Avenue NE18</addrLine>
									<postCode>5th f loor, 02142</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bruce</forename><surname>Blumberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Media Lab</orgName>
								<address>
									<addrLine>77 Massachusetts Avenue NE18</addrLine>
									<postCode>5th f loor, 02142</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning From and About Others: Towards Using Imitation to Bootstrap the Social Understanding of Others by Robots</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C9E3AB825E79F533B94B541A61B1F496</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Human-robot interaction</term>
					<term>facial imitation</term>
					<term>theory of mind</term>
					<term>learning to imitate</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We want to build robots capable of rich social interactions with humans, including natural communication and cooperation. This work explores how imitation as a social learning and teaching process may be applied to building socially intelligent robots, and summarizes our progress toward building a robot capable of learning how to imitate facial expressions from simple imitative games played with a human, using biologically inspired mechanisms. It is possible for the robot to bootstrap from this imitative ability to infer the affective reaction of the human with whom it interacts and then use this affective assessment to guide its subsequent behavior. Our approach is heavily influenced by the ways human infants learn to communicate with their caregivers and come to understand the actions and expressive behavior of others in intentional and motivational terms. Specifically, our approach is guided by the hypothesis that imitative interactions between infant and caregiver, starting with facial mimicry, are a significant stepping-stone to developing appropriate social behavior, to predicting others' actions, and ultimately to understanding people as social beings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Humans (and many other animals) display a remarkably flexible and rich array of social competencies, demonstrating the ability to interpret, predict, and react appropriately to the behavior of others, as well as to engage others in a variety of complex social interactions. Developing computational systems that have these same sorts of social abilities is a critical step in designing robots, animated characters, and other computer agents that appear intelligent and capable in their interactions with humans (and each other), that are able to cooperate with people as capable partners, that are able to learn from natural human instruction, and that are intuitive and engaging for humans to interact with.</p><p>Yet, today, many current technologies (animated agents, computers, etc.) interact with us in a manner characteristic of socially impaired people. In the best cases they know what to do, but often lack the social intelligence to do it in a socially appropriate manner. As a result, they frustrate us, and we quickly dismiss them even though they can be useful. This is a problem in that some of the most exciting new applications for robots require them to cooperate with humans as capable and socially savvy partners (see <ref type="bibr" target="#b33">[34]</ref> for a review). For instance, robots are being developed to provide the elderly with assistance in the home. Such robots should be persuasive in ways that are sensitive to the human's needs, such as helping to remind them when to take medication, without being annoying or upsetting.</p><p>In other applications, robots are being developed to serve as members of human-robot teamssuch as NASA's humanoid robot, Robonaut <ref type="bibr" target="#b0">[1]</ref>. This robot is envisioned to serve as an astronaut's assistant to help its human counterparts maintain the space station or to explore distant planets. To provide a human teammate with the right kinds of assistance at the right time, a robot partner must not only recognize what the person is doing (i.e., his observable actions), but also understand the intentions or goals being enacted. This style of human-robot cooperation strongly motivates the development of robots that can infer and reason about the mental states of others within the context of the interaction they share.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview</head><p>As robot designers, it is possible to gain valuable insight into how social and communicative competencies might be acquired by a machine by looking to the field of human cognitive and social development. An increasing amount of evidence suggests that the ability to learn by watching others (and in particular the ability to imitate) could be a crucial precursor to the development of appropriate social behavior -and ultimately the ability to reason about the thoughts, intents, beliefs, and desires of others. For instance, Meltzoff <ref type="bibr" target="#b55">[56]</ref> hypothesizes that the human infant's ability to translate the perception of another's action into the production of her own provides a basis for learning about self-other similarities, and for learning the connection between observable behavior and the mental states that produce it. Such theories could provide a foothold for ultimately endowing machines with human-style social skills and understanding.</p><p>This article presents a biologically inspired implementation of early facial imitation based on the AIM model proposed by Meltzoff and Moore <ref type="bibr" target="#b61">[62]</ref>. Although there are competing theories to explain early facial imitation (such as an innate-releasing-mechanism model where fixed-action patterns are triggered by the demonstrator's behavior, or viewing it as a by-product of neonatal synesthesia where the infant confuses input from visual and proprioceptive modalities) <ref type="bibr" target="#b54">[55]</ref>, Meltzoff and Decety present a compelling account of the representational nature and goal-directedness of early facial imitation, and how this enables further social growth and understanding <ref type="bibr" target="#b58">[59]</ref>. It is the implications and extensibility of the AIM model that are of particular interest to us, rather than the ability to imitate facial expressions per se. Next, we present our computational model of facial imitation for a robot (demonstrated on its simulated counterpart) and discuss the key aspects of early facial imitation that it captures. Afterwards, we briefly discuss how our approach compares with prior work on creating imitative robots (and other imitative systems), especially as it relates to the problem of bootstrapping social understanding.</p><p>Finally, we present a model for how our robot can bootstrap from its imitative ability to engage in social referencing. This capability is based on the social referencing capabilities displayed in early childhood whereby a child adopts his mother's emotional reaction to a novel situation to decide whether to explore or avoid the unknown <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b81">81]</ref>. Similarly, the robot should be able to infer the affective state of the human who interacts with it, using the human's appraisal to evaluate a novel situation in order to guide its own subsequent behavior (see <ref type="bibr">Section 8)</ref>. Thus, whereas other robots have demonstrated the ability to imitate observable behavior, our model argues for how a robot could use this capacity to infer the mental states (such as affective and attentional states) that underlie observable behavior. This is a fundamental aspect of our approach to building robots that understand people in social terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Toward Robots That Understand Other Minds</head><p>For robots to cooperate with people in a humanlike way, they must be able to infer the mental states of others (their thoughts, intents, beliefs, desires, etc.) from observable behavior (their gestures, facial expressions, speech, actions, etc.). In humans, this competence is referred to as a theory of mind (ToM) <ref type="bibr" target="#b68">[69]</ref>, folk psychology <ref type="bibr" target="#b38">[39]</ref>, mind reading <ref type="bibr" target="#b84">[84]</ref>, or social common sense <ref type="bibr" target="#b61">[62]</ref>.</p><p>In humans, this ability is accomplished in part by each participant treating the other as a conspecific -viewing the other as being ''like me'' <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b60">61]</ref>. Perceiving similarities between self and other is an important part of the ability to take the role or perspective of another, allowing people to relate to and to empathize with their social partners. This sort of perspective shift may help us to predict and explain other's emotions, behaviors, and other mental states, and to formulate appropriate responses based on this understanding. For instance, it enables us to infer the intent or goal enacted by another's behavior-an important skill for enabling richly cooperative behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Simulation Theory and Theory of Mind</head><p>Simulation theory (ST) is one of the dominant hypotheses about the nature of the cognitive mechanisms that underlie the TOM <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42]</ref>. It can perhaps best be summarized by the cliche ´''to know a man is to walk a mile in his shoes.'' ST posits that by simulating another person's actions and the stimuli they are experiencing using our own behavioral and stimulus processing mechanisms, we can make predictions about the behaviors and mental states of others based on the mental states and behaviors that we would possess in their situation. In short, by thinking ''as if '' we were the other person, we can use our own cognitive, behavioral, and motivational systems to understand what is going on in the heads of others.</p><p>From a design perspective, ST is appealing in that it suggests that instead of requiring a separate set of mechanisms for simulating other persons, we can make predictions about others by using our own cognitive mechanisms to recreate how we would think, feel, and act in their situation -thereby providing us some insight into their emotions, beliefs, desires, intensions, and so on. We argue that a ST-based mechanism could also be used by robots to understand people in a similar way. Importantly, it is a strategy that naturally lends itself to representing the internal state of the robot and human in comparable terms. This would facilitate a robot's ability to compare its own internal state with that of the person it is interacting with in order to infer the human's mental states and to learn from observing the human's behavior. Such theories could provide a foothold for ultimately endowing machines with human-style social skills, learning abilities, and social understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Imitation and Simulation Theory</head><p>Meltzoff proposes that the way in which infants learn to simulate others is through imitative interactions. For instance, Meltzoff <ref type="bibr" target="#b55">[56]</ref> hypothesizes that the human infant's ability to translate the perception of another's action into the production of their own action provides a basis for learning about self-other similarities, and for learning the connection between behaviors and the mental states producing them.</p><p>ST rests on the assumption that the other is enough ''like me'' that he can be simulated using one's own machinery. Thus, in order to successfully imitate and be imitated, the infant must be able to recognize structural congruence between himself and the adult model (i.e., notice when his body is ''like'' that of the caregiver, or when the caregiver's body is ''like'' his own). The initial ''like me'' experiences provided by imitative exchanges could lay the foundation for learning about additional behavioral and mental similarities between self and other.</p><p>There are a number of ways in which imitation could help bootstrap a ST-type ToM <ref type="bibr" target="#b58">[59]</ref>. To begin with, imitating another's expression or movement is a literal simulation of their behavior. By physically copying what the adult is doing, the infant must, in a primitive sense, generate many of the same mental phenomena the adult is experiencing, such as the motor plans for the movement. Meltzoff notes that to the extent to which a motor plan can be considered a low-level intention, imitation provides the opportunity to begin learning connections between perceived behaviors and the intentions that produce them. Additionally, facial imitation and other forms of cross-modal imitation require the infant to compare the seen movements of the adult with his own felt movements. This provides an opportunity to begin learning the relationship between the visual perception of an action and the sensation of that action.</p><p>Emotional empathy and social referencing are two of the earliest forms of social understanding that facial imitation could facilitate. Experiments have shown that producing a facial expression generally associated with a particular emotion is sufficient for eliciting that emotion <ref type="bibr" target="#b78">[79]</ref>. Hence, simply mimicking the facial expressions of others could cause the infant to feel what the other is feeling, thereby allowing the infant to learn how to interpret emotional states of others from facial expressions and body language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Mirror Neurons</head><p>Interestingly, a recently discovered class of neurons in monkeys, labeled mirror neurons, has been proposed as a possible neurological mechanism underlying both imitative abilities and ST-type prediction of other's behaviors and mental states <ref type="bibr" target="#b85">[85,</ref><ref type="bibr" target="#b35">36]</ref>. Within area F5 of the monkey's premotor cortex, these neurons show similar activity when a primate observes a goal-directed action of another (such as grasping or manipulating an object) and when it carries out that same goal-directed action <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b36">37]</ref>.</p><p>This firing pattern has led researchers to hypothesize that there exists a common coding between perceived and generated actions <ref type="bibr" target="#b69">[70]</ref>. These neurons may play an important role in the mechanisms used by humans and other animals to relate their own actions to the actions of others. To date, it is unknown if mirror neurons are innate in humans, learned through experience, or both. Interesting computational models have been proposed for how they might be learned <ref type="bibr" target="#b65">[66]</ref>.</p><p>Mirror neurons are seen as part of a possible neural mechanism for ST. By activating the same neural areas while perceiving an action as while carrying it out, it may be not only possible but also necessary to recreate additional mental states frequently associated with that action. A mirrorneuron-like structure could be an important building block in a mechanism for making predictions about someone else's intentions and beliefs by first locating the perceived action within the observer's own action system, identifying one's own beliefs or intentions typically possessed while carrying out that action, and then attributing them to the other person.</p><p>To summarize, there are a variety of ways in which having the ability to imitate others and the mechanisms and structures that ability entails could help a robot begin to interpret and make predictions about others' behavior. In the next section we highlight key aspects of early infant imitation that we want to capture in our implementation in order to bootstrap our robot's ability to socially learn from people and to understand them as social beings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Characteristics of Imitation in Human Infants</head><p>Early infant imitation occurs within an interpersonal context. According to Meltzoff <ref type="bibr" target="#b55">[56]</ref>, ''human parents are prolific imitators of their young infants.'' Caregivers continually shadow and mirror their infant's animated movements, facial expressions, and vocalizations. In turn, infants seem to recognize when their behavior has been matched. They preferentially attend to adults whose actions are contingent on their own, and especially to adults who are imitating them <ref type="bibr" target="#b59">[60]</ref>. Specifically, they seem to recognize both temporal contingency (i.e., when the infant performs action x, the adult performs action y, where x and y differ in form), and structural congruence (i.e., when x and y have the same form). When matched, infants often respond by smiling and visually attending to the caregiver for longer periods of time. Meltzoff posits that infants are in fact intrinsically motivated to imitate their conspecifics, and that the act of successful imitation is its own reward.</p><p>This early imitative capability continues to develop over time to become more versatile and sophisticated. Meltzoff suggests a four-stage progression of imitative abilities (for a review, see <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b70">71]</ref>. The first stage is called body babbling (akin to vocal babbling) and involves random experimentation with body movements in order to learn a set of motor primitives that allow him to achieve elementary body configurations. Through trial-and-error learning, even starting in utero, the neonate builds up a directory for mapping movements to goal states that can be monitored proprioceptively. Eventually the neonate acquires an act space that enables new body configurations to be interpolated within this space.</p><p>Next, the infant is able to imitate body movements. Just hours (and even minutes) after birth, neonates can imitate facial acts that they have never seen themselves perform. This suggests an innate mapping between the observation and execution of movements in humans. It has been shown that 12-to 21-day-old infants can identify and imitate the movement of a specific body part and imitate differential action patterns with the same body part <ref type="bibr" target="#b61">[62]</ref>. This is called organ identification.</p><p>At 6 weeks, infants have been shown to perform deferred imitation from long-term memory after seeing the target facial act performed 24 hours earlier <ref type="bibr" target="#b62">[63]</ref>. They are able to correct their imitative response in a goal-directed manner from memory without requiring any feedback from the model. This presents further evidence that the observation-execution pathway is mediated by a representational structure.</p><p>Meltzoff argues that this structure is represented within an intermodal space into which infants are able to map all expressions and movements that they perceive, regardless of their source. In other words, the intermodal space functions as a universal format for representing gestures and posesthose the infant feels himself doing, and those he sees the adult carrying out. The universal format is in terms of the movement primitives within his act space. Thus the perceived expression is translated into the same movement representation that the infant's motor system uses (recall the discussion of mirror neurons in Section 3.3) making their comparison much simpler. The imitative link between movement perception and production is forged in the intermodal space.</p><p>Once infants are several months old, they can imitate novel actions upon objects. By 1 to 1.5 years they are adept at imitating body movements and actions on objects (such as toys) in a variety of contexts. At 18 months, they are able to read beyond perceived behavior to infer the underlying goals and intensions of the actor <ref type="bibr" target="#b56">[57]</ref>. This is demonstrated by their ability to imitate the goal of an attempt that was enacted unsuccessfully. For instance, the adult may try to perform a manipulation on an object where her hand slips several times so the goal remains unachieved. The infant does not imitate the literal action, but rather performs the action correctly (or even uses novel means) to achieve the intended goal. This brings the infant to the threshold of understanding the behavior of others in terms of their underlying mental states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">A Robot Architecture for Facial Imitation</head><p>To bring our robot to a similar point, it is important to capture these key aspects of infant imitation in our implementation. Much as infants' earliest social interactions involve imitating facial expressions, our first step towards creating a robot capable of social understanding is an implementation of facial mimicry. In order for a robot to imitate, it must be able to translate between seeing and doing. Specifically, to solve the facial imitation task the robot must be able to:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Locate and recognize the facial features of a demonstrator</head><p>Find the correspondence between the perceived features and its own Identify a desired expression from this correspondence Move its features into the desired configuration Use the perceived configuration to judge its own success Meltzoff and Moore <ref type="bibr" target="#b61">[62]</ref> proposed a descriptive model for how an infant might accomplish these tasks, known as the active intermodal mapping (AIM) hypothesis. A schematic of the AIM model is presented in Figure <ref type="figure" target="#fig_0">1</ref>. In general, the AIM model suggests that a combination of innate knowledge and specialized learning mechanisms underlies infants' ability to imitate in a cross-modal, goal-directed manner. Specifically, AIM presents three key components of the imitative process as discussed in the previous section: motor babbling, organ identification, and the intermodal space. Taken together, this model suggests mechanisms for identifying and attending to key perceptual features of faces, mapping the model's face onto the imitator's, generating appropriate movements, and gauging the correspondence between produced and perceived expressions. We have used this model  <ref type="bibr" target="#b61">[62]</ref>). AIM models the mechanisms necessary for infant facial imitation (see Section 4). This figure depicts the flow of data between the external world, the infant's internal representation of perceived expressions (the adult's expressions and his own), and the infant's motor system. Representations of the adult expression and the infant's own expression are compared in terms of organ relations. If the infant's current expression is not a good match for the adult's, the movement-end state directory (previously generated by the infant through motor babbling) is searched for a better match, which is then executed by the motor system. If subsequent comparisons still find the match between perceived and produced expressions to be inadequate, the motor system may execute a localized search of the motor space.</p><p>to guide our own implementation as summarized in Table <ref type="table" target="#tab_0">1</ref> (with allowances made for the differing physical limitations of babies and robots). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Leonardo the Robot</head><p>Our experimental platform is a robot called Leonardo (Leo) -a 64 degree of freedom (DoF) fully embodied humanoid robot that stands approximately 2.5 feet tall (see Figure <ref type="figure" target="#fig_1">2</ref>). The robot's feet are permanently affixed to its base, but the robot is otherwise fully articulated. The design is targeted for rich social exchanges with humans as well as physical interactions with the environment. Hence, the robot is designed to be able to communicate and gesture to people as well as physically manipulate objects. The robot has an expressive silicone face (24 DoFs, not including the ears) capable of nearhuman-level expression, and an active binocular vision system (4 DoFs), making it an ideal platform for implementing facial mimicry. In addition, the robot is equipped with two 6 DoF arms, two 3 DoF hands, two actively steerable 3 DoF ears, and a 5 DoF neck, with the remainder of the DoFs in the shoulders, waist, and hips.</p><p>We have also developed a simulated version of Leonardo ( Virtual Leonardo, shown in Figure <ref type="figure" target="#fig_1">2</ref>), which shares the same kinematics, sensory input, and cognitive architecture as the physical robot. This animated version is an exact joint-for-joint model of the real-world Leonardo, and both use the same behavioral and motor systems (described in the following sections). Thus, the implementation presented in this article works with both the physical and the simulated robot. Because the silicone face is not currently on the physical robot, the results in this article are presented on the animated version so that the expressions are readable.</p><p>In order to give Leonardo the ability to locate and identify the facial features of a human partner, we use the visual sensing software from Nevengineering Inc. (www.nevengineering.com). Their Axiom ff T software locates a face in an attached camera's field of view and tracks its features, returning a set of normalized 2D coordinates for 22 points on the user's face: 2 points for each eyebrow, 3 for each eye, 4 for the nose, and 8 for the mouth (see Figure <ref type="figure" target="#fig_2">3</ref>). For the results presented with Virtual Leonardo we used a statically mounted camera. On the physical robot, the software runs on a camera mounted within one of Leonardo's eyes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Cognitive Architecture Overview</head><p>Leonardo's imitative ability is implemented within an existing cognitive, affective, and behavioral framework <ref type="bibr" target="#b18">[19]</ref>. As a result, interacting with Leonardo is more like interacting with a creature than like interacting with robot that is specialized for one skill. Figure <ref type="figure" target="#fig_3">4</ref> presents an overview of the cognitive architecture of our system. In this section, we briefly describe the system components most relevant to the imitative task at hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Perception System</head><p>We use a hierarchical mechanism called a percept tree to extract state information from sensory input. Each node in the tree is called a percept, and more specific percepts are closer to the leaves. Percepts are atomic perception units, with arbitrarily complex logic, whose job is to recognize and extract features from raw sensory data. For example, a face percept might recognize the presence of faces in the visual field, and its children might recognize the presence of specific features such as the eyes and nose. The root of the tree is the most general percept, which we call True, since it is always active.</p><p>Our current imitation architecture has a perception system that receives sensory input from the Axiom ff T software, and implements a number of simple percepts. In addition to the True percept, there is a face percept, which fires whenever it receives Axiom ff T data indicating the presence of a  Leonardo learns how to map perceived facial expressions into its intermodal space (its own joint space) by having the human participant imitate the robot. Leonardo generates a variety of poses by motor babbling. When the human's movements are contingent on its own, the robot decides it is being imitated, and uses the human's current expression and its own current expression to train a set of neural nets that it uses for mapping the human's expression into the intermodal space. Once these nets are trained to encode this mapping, Leonardo can convert data into its intermodal representation and classify the pose as one of its own. This allows the robot to produce a similar pose, thereby imitating the human. This diagram shows an overview of how these steps are accomplished within the robot's cognitive architecture.</p><p>human face. Similarly, this percept has child percepts corresponding to facial organs -the eyebrows, eyes, nose, and mouth. There are a number of movement percepts, which detect when the human's facial features have moved, and contingency percepts, which detect when they have moved in response to Leonardo's own movements (the details of which are again described in Section 6.1.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Action System</head><p>The robot's action system is responsible for behavior arbitration -choosing what behavior the robot engages in and when it does so. Individual behaviors are represented in our system as action tuples <ref type="bibr" target="#b10">[11]</ref>. For our purposes here, the key components of the action tuple are its action and its trigger context. The action is a piece of code primarily responsible for sending high-level requests for movements or movement sequences to the motor system that commands the robot's actuators. The request can range from something relatively simple such as to ''look at'' something, to more complex actions such as ''press a button.'' The trigger context is responsible for deciding when the action should be activated. In general, there are a variety of internal states (e.g., motivations) and external states (e.g., perceptions) that might trigger a particular action.</p><p>Action tuples are grouped into action groups that are responsible for deciding at each moment which single action tuple will be executed. Each action group can have a unique action selection scheme. For our imitation architecture we use a single action group with an action selection scheme that activates an action tuple any time its trigger context goes high.</p><p>Leonardo's facial imitation architecture requires two key actions: a motor babbling action and an imitation action, each of which is wrapped in an action tuple. The function and details of these two actions are presented in Section 6.1 and Section 6.2, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Motor System</head><p>Once the action system has selected an action for the robot to perform, the motor system is responsible for executing the movements required to carry out that action. In our system, motor movements are represented as paths through a directed weighted graph, known as the creature's posegraph <ref type="bibr" target="#b27">[28]</ref>. Each node (or pose) in the graph is an annotated configuration of the creature's joints, and can be thought of as a single body configuration.</p><p>It is worth noting that this motor system design is quite similar to that hypothesized by AIM. Poses can be seen as variations on organ relations, with the posegraph being a specific implementation of the movement-end state directory structure that AIM proposes. For the purposes of implementing facial mimicry, Leonardo was provided with a posegraph containing a small set of basis facial poses (presented in Figure <ref type="figure" target="#fig_4">5</ref>). They can be seen as analogous to the initial movement-end state pairs that AIM suggests infants discover in utero and are born knowing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.1">Movement Primitives</head><p>A link between two poses represents an allowed transition between joint body configurations. These links are designed to permit only biologically plausible and safe movements, which will not put the robot into unnatural body configurations or potentially dangerous ones. Together, the poses and the paths between them define the robot's space of possible movements (its pose space), with entire movement trajectories existing as routes through this space. For example, a pointing gesture might be represented as a path through 15 poses.</p><p>In addition to the posegraph, the motor system contains motor programs that are capable of generating paths through pose space in response to requests from actions. These programs may be quite simple (essentially no more than playing out a particular animation) or more complex (for example, trying to touch or pick up an object).</p><p>For our facial imitation architecture, we use a basic posegraph where all of Leo's basis poses are directly connected to each other. Our motor program takes the current and desired poses and smoothly transitions between them by slowly rotating each joint into its new position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.2">Interpolating Primitives</head><p>The motor system allows for a wide range of safe, realistic-looking motor actions, which can be easily created, stored, and recreated. However, it is often impractical to represent all of a robot's desired poses explicitly. The motor system therefore also allows for the creation of blended poses: poses that are a weighted average of other poses (the weights used for blending are known as blend weights). Using blended poses creates an exponential increase in the size of the creature's pose space, allowing whole ranges of positions and actions to be generated from only a few explicit examples (for instance, the robot's button-pressing behavior along a continuous line can be generated by blending the poses in three example routes: press-left-button, press-right-button, and press-center-button). The final pose is computed on a per joint basis, as follows: For each joint angle J k in the robot:</p><formula xml:id="formula_0">J k ¼ X NumExemplars i¼1 E k;i Â W i<label>ð1Þ</label></formula><p>where E k,i is the kth joint angle in the i th exemplar, and W i is the weight of the i th exemplar.</p><p>The motor system is able to treat blended poses just like regular poses. Together, Leo's basis facial poses define the convex hull of a facial pose space, and Leonardo can achieve all the poses within that space by blending the basis poses with different weights. This system is a nice analogue to Meltzoff 's suggestion that motor primitives within the infant's repertoire can be interpolated to generate new movements. This is important for the matching-to-target process that is characteristic of early facial imitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.3">Motor Subsystems</head><p>Finally, the robot can be given even greater movement f lexibility by using a number of motor subsystems, each of which is responsible for controlling a closely associated set of joints (left arm, right arm, torso, etc.). Each motor subsystem is able to search the posegraph and execute movements independently, allowing each subset of joints, or body organ, to be in a different part of the posegraph simultaneously. Once again, this allows the robot a greater range of motions from fewer poses.</p><p>Our facial imitation implementation uses three motor subsystems within Leo's face, corresponding to his mouth region, left eye region, and right eye region (see Figure <ref type="figure" target="#fig_4">5</ref>). This allows Leo to move each of these regions independently of each other to generate novel expressions. In this article, when we refer to the motor system as searching for a pose in the posegraph or executing a pose, this is shorthand for the motor system delegating these tasks to the three subsystems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Learning from Imitation Games</head><p>The overall structure of an imitative interaction consists of two parts: a first stage, where the human participant imitates Leonardo's facial expressions, and a second stage, where Leonardo mimics the human's expressions. The interaction is summarized in Figure <ref type="figure">6</ref>. Leonardo takes advantage of the bidirectional structure of the imitative exchange by accomplishing different tasks during different parts. During the first stage of the interaction Leo solidifies his representation of the correspondence between the human's facial features and his own (the intermodal representation). During the second stage, Leonardo uses this correspondence to model and imitate the human's expression in a goaldirected fashion. Data flow paths for each stage within the cognitive architecture are presented in Figure <ref type="figure" target="#fig_3">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Human Participant Imitates Leonardo</head><p>The imitative interaction begins with the human participant approaching Leonardo. Leonardo relies on the Axiom ffT software (described in Section 5.2.1) to detect when a human face is present in the robot's field of view. When data from the facial feature tracker indicates that Leo is seeing a human face, the face percept in Leo's perception system becomes active and triggers the robot's motor babbling action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Motor Babbling</head><p>Similar to the motor babbling exhibited by infants in the AIM model to physically explore their motor space, Leonardo's motor babbling action causes the robot to physically explore its pose space. While Leo's motor babbling action is active, it randomly selects a pose from the basis set used to create its posegraph, requests that the motor system go to that pose and hold it for a moment (approximately 4 seconds), and then selects a new pose. While Leonardo is motor babbling, the human participant tries to imitate Leo's facial expressions.</p><p>Motor babbling serves a number of purposes in the imitative interaction. First, by becoming more active when the user approaches, Leo can communicate in a simple way its awareness of the human participant. Leonardo beginning to motor babble when it sees the person can be seen as analogous to an infant becoming more active in the presence of an interested caregiver. Second, our primary reason for having Leonardo perform motor babbling is to help the robot learn to map perceived human expressions onto an intermodal space, like the one used by infants in the AIM model. By detecting when the human participant is likely to be imitating it, Leonardo can use its own pose (generated through motor babbling) and the human's imitation of this pose to improve the robot's ability to map the human's facial expression to its own intermodal space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Intermodal Representation</head><p>According to Meltzoff 's model, infants use the same internal representation for their own expressions and those they see an adult perform. Furthermore, this representation is the same one used within the infant's motor system to describe how the infant must move in order to achieve a given expression. As such, this representation bears a strong resemblance to the function of mirror neurons <ref type="bibr" target="#b58">[59]</ref>. The intermodal representation allows the infant to discover correspondences between his own expressions and those of the human model, by providing a format in which they can be directly compared.</p><p>In our motor system, Leonardo's expressions are represented as poses, and the motions to achieve them are represented as routes through Leonardo's posegraph. We chose to use poses in Leonardo's own joint space as its intermodal representation. Therefore, the human expressions that Leonardo perceives must be mapped from the set of 2D absolute coordinates provided by the facial feature tracking software onto the robot's joint space. This process is complicated by the fact that there is not a one-to-one correspondence between the tracked facial features and Leo's joints. To solve this problem, Leonardo learns the intermodal representation from experience while the human participant is imitating the robot. This is a rough analogy to learning mirror neurons for encoding and representing perceived movement in terms of motor primitives <ref type="bibr" target="#b65">[66]</ref>. The robot models the intermodal map using a separate neural network for each facial region corresponding to the right eye, left eye, and mouth (see Section 6.1.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Detecting Contingency</head><p>In order for Leo to successfully train the neural nets, the robot must provide the networks with example input-output pairs. Within the framework of the imitative interaction, one way for Leonardo to acquire this data is for the robot to identify when the human participant is imitating it, and to then store a snapshot of the current facial feature data and the robot's own current joint configuration. Figure <ref type="figure">6</ref> . Typical imitative interaction. This schematic shows the ordering of events in a typical imitative exchange with Leonardo. In general, the interaction consists of two stages: the first stage, where the human participant imitates Leonardo, and the second stage, where Leonardo imitates the human participant. Figure <ref type="figure" target="#fig_3">4</ref> presents the processing that occurs in each of these stages within the cognitive architecture. The dashed arrow represents the transition that occurs until Leonardo has learned how to represent the human's expression in its own joint space.</p><p>Unfortunately, before the neural networks are trained, Leo cannot detect an exact correspondence between the human's facial features and its own pose. Identifying when the robot is being imitated is tricky at this stage.</p><p>The literature on infant imitation indicates that infants are especially responsive to adult movements that appear to be contingent on their own. Similarly, Leonardo determines when a person is imitating Leo contingently, based on the elapsed time (less than a couple of seconds) between the start of Leo's movement and the human's response. To avoid false positive detections of human movement due to sensor noise, thresholds for human movement were set per dimension relative to the standard deviation of data for that dimension. In addition, the human's movement must be surrounded by a few seconds of stillness, so as not to classify constant motion as contingent. Some error is still possible with this metric; for instance, if the human moves contingently but is not imitating Leo. Overall, however, we found that using contingent motion to detect imitative interactions produced more accurately trained neural nets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.4">Organ Identification</head><p>We found that during the training process, people often only imitate a particular region of the robot's face (e.g., the mouth or the eyebrows) rather than the robot's entire expression. If, for instance, the human chooses to imitate only Leo's mouth, then the rest of the human's face provides irrelevant data for the training of the included regions. To address this problem, we partition the incoming facial feature data and Leo's DoFs into three independent groups of features that are handled separately: the left eye-eyebrow area, the right eye-eyebrow area, and the mouth. The data from each facial region of the human's face is collected using three separate contingency detectors. These groupings allow Leo to start with a rough idea of which of its organs correspond to those of the human participant, an advantage the AIM model proposes infants share.</p><p>Inside each area, the exact relationship between the coordinate data from the facial feature tracking software and the joints in Leo's face is not yet known and must be learned individually, using separate neural networks. For each, we used a two-layer network, with seven hidden nodes (seven was established to be a good number after we varied it for several tests). The inputs to the networks are the relevant DoFs from the Axiom ffT data: the x and y positions of facial features, normalized to be invariant to scaling of the face, facial translation, and rotation. The outputs are the angles for relevant joints in Leo's face. Each joint in the virtual robot is restricted to one DoF of rotation, just as the motors in the actual robot are.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.5">Representation of Novel Expressions</head><p>Once the separate neural networks are trained, they are able to input the data from visual perception of a human expression, and output the intermodal representation of that expression in terms of the robot's joint angles. The separation into facial regions has an important advantage: Leo can create an intermodal representation of the human pose separately for each group of features. This allows it to generalize and create overall expressions that may never have been in the babbling set. For example, if none of Leo's babbled poses have asymmetric eyebrows, a neural network for the entire face would never allow it to create an intermodal representation with one cocked eyebrow. With this method, however, the eyebrows each respond separately to produce a representation of the novel facial expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Leonardo Imitates the Human Participant</head><p>Once Leo is capable of representing perceived facial expressions in intermodal space, the robot begins trying to imitate the human (the imitation action is triggered once Leo has acquired a predetermined number of facial snapshots). Leo physically manifests its switch in focus by ceasing to motor babble. Instead, Leo becomes still, and begins trying to detect an appropriate expression of the human participant to imitate. Meltzoff notes that young infants don't imitate facial expressions that are presented statically. Rather, in order to imitate, infants must see the adult assume the facial expression, perhaps because the preceding movement is a clue that the expression that follows is worth imitating. Correspondingly, we decided to have Leonardo use motion cues to determine when to begin imitating.</p><p>Like an infant, Leo attempts to reproduce the human model's facial expression when it is a stable expression that directly follows a movement. Using our previously described methods for detecting stability and motion in the human facial feature data, we created a collection of percepts, each of which fires when the human significantly moves an organ, and a corresponding trigger context, which activates Leonardo's imitation action. Leo's imitation action mediates his imitative behavior, by working closely with his motor system to generate and evaluate successive approximation of the perceived pose as it is represented in intermodal space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Goal-Directed Search</head><p>To imitate the observed expression, Leonardo's motor system searches for the pose in the posegraph that is the closest match to the intermodal equivalent. This step is essentially an implementation of the mechanism AIM posits for looking up organ relations from the intermodal space in the movement-end state directory. Finding this pose is a critical step in the imitation process. Next, Leo's motor system executes this pose, producing the robot's first imitative attempt. However, infants do not end their attempt at imitating with this first approximation. Rather, infants use their initial solution as the starting point for a goal-directed search of their motor space, more accurately imitating the adult's expression, and refining their motor knowledge.</p><p>In a similar manner, Leonardo searches for a more accurate imitative pose by blending the initially closest pose with others in its posegraph, incrementally adjusting the blend weights until it has found the best local match. Currently, Leonardo's imitation action executes this search using a simple hillclimbing algorithm (Table <ref type="table">2</ref>). Using the initial basis pose as a starting point, the algorithm iteratively searches for a set of weights defining the blended pose that is the local best match to Leo's representation of the human's expression.</p><p>The distance metric that the hill climber uses is a simple implementation of the equivalence detector described in the AIM model -to find the distance between the human pose represented in intermodal space and Leo's pose, we sum the average angular and translational distances across all joints. While we were initially uncertain this would be a sufficient measure of equivalence between poses, our results so far have found that this distance metric functions adequately, and seems to accurately reflect the visual match judgments made by human observers (see <ref type="bibr">Section 7)</ref>. Leo identifies the closest pose by finding the pose P in his basis set with the minimum distance to the intermodal pose I, using the following equation:</p><formula xml:id="formula_1">DðI; PÞ ¼ 1 n X n k¼1 AngularDistðIJ k ; PJ k Þ Á W k<label>ð2Þ</label></formula><p>where IJ k (PJ k ) is the kth joint in pose I (P), and W k is weight of joint k.</p><p>The hill-climbing algorithm continues iterating until it can no longer find a combination of blend weights that produces a better matching pose than the result of the last iteration. Once Leonardo has carried out the final blended pose, the robot has imitated the human's pose as best it can, and the imitation cycle is complete. Leo's imitation action deactivates, and the robot begins attending to the motions and expressions of the human participant again, trying to detect another appropriate pose to imitate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Facial Imitation Results</head><p>Our implementation has been tested on the simulated version of Leonardo (Virtual Leonardo), because the physical robot's silicone face is not yet mounted. The same software system is used to drive the animated and the physical robot. We have found the system to produce a satisfactory match between the human input and Leonardo's successive approximations. The realism of Leonardo's produced expressions is also reasonable, especially when its output is contrasted with the raw pose data, which is often noisy.</p><p>The entire interaction with Leonardo occurs in real time, with the human participant imitating Leonardo for approximately 5 minutes, followed by Leonardo imitating the human until the human terminates the interaction. The intermodal representation learned in the first phase can be acquired by interacting with a different person than the one that Leo imitates in the second phase of the game. Hence a new intermodal representation does not have to be learned for each person Leonardo interacts with ( however, this mapping seems to be more robust for the mouth region and more person-specific for the eye region).</p><p>Figure <ref type="figure">7</ref> presents three imitative interactions, including the human facial expression, the representation of the human's pose in Leo's joint space (the human pose represented in intermodal space), and Leo's final approximation of the human's pose. The images show Leo imitating a number of facial expressions presented by a human participant involving the mouth and eyebrows. The learned intermodal representation of the human pose is shown, as well as Leo's best approximation of it via goal-directed search of its blend space.</p><p>Figure <ref type="figure" target="#fig_6">8</ref> highlights the improvements made by Leonardo's motor system on the raw neural net output. While Figure <ref type="figure">7</ref> clearly demonstrates that the neural nets are able to learn a very accurate intermodal mapping from the human participant's expression to Leonardo's joint space, this raw mapping still occasionally produces impossible joint configurations due to noise in the tracking data. However, by using Leo's closest basis pose as the starting point for the search for the best matching pose to the human's expression, Leonardo avoids attempting to execute impossible or unnatural joint configurations.</p><p>Figure <ref type="figure" target="#fig_7">9</ref> shows some of Leo's intermediate approximations of the model's expression, generated while searching its blend space. As can be seen in this figure, Leonardo is able to produce visually Table <ref type="table">2</ref> . Pseudocode for hill-climbing algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Define:</head><p>. D as the distance metric defined in equation <ref type="formula" target="#formula_1">2</ref>. P ({x 1 , x 2 , x 3 . . .}) is the blend of facial poses with given weights x 1 , x 2 , x 3 . . .</p><p>. BW (k, W ) = {w 1 , w 2 . . ., w SP À c,. . .,w k + c,. . .}</p><p>(that is, the array W with c subtracted from the value at index SP and added to the value at index k)</p><p>W is the array of blend weights, which is updated so that P(W)iteratively approaches I, the target intermodal pose.</p><p>Initialized W to all 0's with a 1 at index SP, where SP is the index of the initial basis pose.</p><p>Repeat until W converges: successful matches to a wide variety of human facial expressions via interpolation of its movement primitives. Finally, Figure <ref type="figure" target="#fig_8">10</ref> shows that Leonardo is able to superimpose its motor subsystems corresponding to different facial regions to represent and generate novel facial poses, such as a cocked eyebrow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">From Facial Imitation to Social Referencing</head><p>Social referencing is an important form of socially guided learning in which one person utilizes another person's interpretation of a given situation to formulate his or her own interpretation of it and to determine how to interact with it <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b50">51]</ref>. Given the large number of novel situations, objects, and people that infants (as well as robots) encounter, social referencing is extremely useful in forming early appraisals and coping responses to unfamiliar stimuli with the help of others.</p><p>Referencing behavior operates primarily under conditions of uncertainty -if the situation has low ambiguity then intrinsic appraisal processes are used <ref type="bibr" target="#b20">[21]</ref>. Further, social referencing can take multiple forms. For instance, emotional referencing is viewed as a process of emotional communication whereby the infant learns how to feel about a given situation, and then responds to the situation according to his or her emotional state <ref type="bibr" target="#b31">[32]</ref>. For example, the infant may approach a toy and kiss it  upon receiving a joy message from the adult, or swat the toy aside upon receiving a fear message <ref type="bibr" target="#b43">[44]</ref>. In instrumental referencing, the infant looks to the adult to determine what to do in a particular situation or how to interact with a stimulus <ref type="bibr" target="#b81">[81]</ref>. Clearly, instrumental and emotional factors interacta certain emotional state biases the child to have certain kinds of interactions with the stimulus, and interacting with a stimulus in a particular way can influence how the child feels about it.</p><p>This section presents ongoing work in developing a model of social (emotional) referencing for Leonardo. Due to space constraints, we will not present the model in detail, and only briefly describe the associated shared attention and emotion systems. We focus our discussion on the role facial imitation can play in bootstrapping the social referencing competence of Leonardo. Furthermore, we present a scenario to illustrate how early facial imitation can play an important role in the development of social understanding. For instance, much of the excitement over mirror neurons stems from their potential as a mechanism for the simulation of others' behavior and their mental states by using an individual's already existing machinery for generating those states within themselves. Similarly, we are developing a model that uses the perception-production coupling of Leonardo's imitative abilities to allow the robot to make simple inferences about the emotional state of others, and to apply their affective appraisals to help the robot evaluate novel external situations via the robot's joint attention and emotion-based mechanisms. This should allow Leonardo to use the emotionally communicated assessment of others to form its own appraisals of the same situations, and use these appraisals to guide its own subsequent responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Social Referencing in Infants</head><p>In human infants, social referencing first appears as a secondary appraisal process at the end of the first year of development. Baldwin and Moses <ref type="bibr" target="#b3">[4]</ref> argue that the appearance of social referencing demonstrates a simple but genuinely mentalistic understanding of other people. It is therefore a significant milestone in the development of social understanding in humans. In particular, social referencing indicates that infants understand the attention of others as mental states -they understand that the other is interested in some external object or event and that they have some sort of positive or negative evaluation of it. Thus, the infant has begun to understand that emotions have an intentional or referential quality. One usually feels happy, sad, and so on, about thingsobjects, events, people, outcomes, and the like.</p><p>A variety of experiments have explored the social referencing behavior of infants for a range of stimuli including unknown situations such as a visual cliff, unfamiliar persons, or novel toys (see <ref type="bibr" target="#b31">[32]</ref> for a review). For instance, a 12-month-old infant confronted by a novel stimulus will deliberately look to his or her mother (or other trusted adult) to witness the adult's emotional reaction to the thing in question. The infant uses the adult's emotional assessment as a basis to form his or her own affective appraisal of the novel entity, and then uses this assessment to regulate his or her own subsequent behavior towards it. For example, if the caregiver responds positively and enthusiastically to the unknown stimulus, the infant will be more inclined to explore or engage it. Conversely, if the caregiver displays a fearful reaction to the unknown stimulus, the infant will tend to avoid it.</p><p>To perform social referencing, the infant must be able to accomplish at least four distinct socialcognitive prerequisites <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b50">51]</ref>. First, the infant must understand the content of the message. At around 2 to 3 months of age, infants begin to discriminate the facial expressions of others and respond to them systematically with smiles and frowns of their own <ref type="bibr" target="#b79">[80]</ref>. By 6 months of age, infants are able to respond appropriately to the expressed emotions of others. For instance, emotion contagion is a process by which the caregiver's emotional expression influences the infant's own emotional state and subsequent behavior <ref type="bibr" target="#b30">[31]</ref>. Second, the infant must be able to actively appraise incoming information about environmental events, rather than simply respond to them in a prewired fashion. By around 9 months, infants exhibit the ability to evaluate the consequences of predicted outcomes before responding <ref type="bibr" target="#b30">[31]</ref>. Further, these appraisals persist to regulate how the infant interacts with the stimulus in the future and in different contexts. Third, the infant must have referential skills. Specifically, he or she must be able to identify the particular referent that is the topic of the adult's communication. Infants first demonstrate the ability to share attention with others, as in following the adult's gaze or pointing gestures to the object that they refer to, at around 9 to 12 months of age <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20]</ref>. Finally, the infant must have inferential skills to extract the intentional nature of the affective information from the adult's expression and associate this appraisal with the specific referent. Namely, the infant begins to understand that the expressed emotion is about something in particular <ref type="bibr" target="#b3">[4]</ref>. This ability also appears near the end of the first year, when social referencing behavior can be observed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">A Computational Model for Social Referencing</head><p>In our computational model of social referencing, three systems and their associated mechanisms interact to give rise to social referencing behavior. These skills include the ability to imitate facial expression, the ability to share attention with others, and the ability to engage in emotional communication. We have already presented the facial imitation capabilities of Leonardo in detail. We briefly describe the emotion system and shared attention system below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.1">Model of Basic Emotions</head><p>The robot's emotion system is based on computational models of basic emotions as described in <ref type="bibr" target="#b16">[17]</ref>. Emotions are an important motivation system for complex organisms, as they can also be for robots. Emotions seem to be centrally involved in determining the behavioral reaction to environmental (often social) and internal events of major significance for the needs and goals of a creature <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b45">46]</ref>. Several theorists argue that a few select emotions are basic or primarythey are provided by evolution because of their proven ability to facilitate adaptive responses to the vast array of demands and opportunities a creature faces in its daily life <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b46">47]</ref>. In particular, the emotions of anger, disgust, fear, joy, sorrow, and surprise are often supported as being basic by evolutionary, developmental, and cross-cultural studies <ref type="bibr" target="#b29">[30]</ref>. Models for these basic emotions have been implemented in robots <ref type="bibr" target="#b16">[17]</ref>.</p><p>Each basic emotion is designed to serve a particular function (often biological or social), arising in particular contexts, to prepare and motivate the robot to respond in adaptive ways. Several emotion theorists posit an appraisal system that assesses the perceived antecedent conditions with respect to the organism's well-being, its plans, and its goals <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b34">35]</ref>. Scherer <ref type="bibr" target="#b77">[78]</ref> has studied this assessment process in humans and suggests that people affectively appraise events with respect to novelty, intrinsic pleasantness, goal/need significance, coping, and norm/self compatibility. Our model of basic emotions includes a simple appraisal process based on Damasio's theory of somatic markers <ref type="bibr" target="#b22">[23]</ref>, that tags the robot's incoming perceptual and internal states with affective information, such as valence (positive or negative) and novelty.</p><p>These appraisals, along with other internal factors, evoke a particular emotive state that recruits response tendencies within multiple systems, including eliciting specific kinds of expressive and behavioral responses for coping with the demands of the original antecedent conditions. Plutchik <ref type="bibr" target="#b66">[67]</ref> calls this stabilizing feedback process behavioral homeostasis. Through this process, the robot's models of basic emotions establish a desired relation between the robot and the environment that pulls the robot toward beneficial stimuli and events and pushes it away from others that are not. The relational activity can be social or instrumental in nature, motivating the robot's behaviors for exploration and information gathering, seeking comfort, engagement and interaction, avoidance, or escape <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.2">Model of Shared Attention</head><p>Leonardo's attentional system determines the robot's focus of attention, monitors the attentional focus of the human, and uses both to keep track of the referential focus held by both. Therefore, the robot not only has a model for its own attentional state, but models that of the human as well. Previous computational models have focused on developing robots that can engage in deictic gaze or joint visual attention, defined by Butterworth <ref type="bibr" target="#b19">[20]</ref> as ''looking where someone else is looking'' <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b73">74]</ref>. In contrast, our approach follows that of Baron-Cohen <ref type="bibr" target="#b4">[5]</ref>, where joint attention is explicitly represented as a mental state. This turns out to be very important for social referencing as described in Section 8.2.3.</p><p>Leonardo's attentional system computes the level of saliency (a measure of interest) for objects and events in the robot's perceivable space. The 3D space around the robot, and the objects and events within this space, are represented by the vision system. The attention system operates on this 3D spatial representation to assign saliency values to the items therein. There are three kinds of factors that contribute to the overall saliency of something: its perceptual properties (its proximity to the robot, its color, whether it is moving, etc.), the internal state of the robot (including what the robot is searching for and other goals), and socially directed reference ( pointing to, looking at, or talking about something to bring it selectively to the robot's attention). For each item in the 3D spatial representation, the overall saliency at each time step is the result of the weighted sum for each of these factors <ref type="bibr" target="#b12">[13]</ref>. The item with the highest saliency becomes the current attentional focus of the robot and determines where the robot's gaze is directed <ref type="bibr" target="#b13">[14]</ref>. The referential focus is determined as the last object that was the subject of shared attention between robot and human (what they were both looking at).</p><p>Using the same 3D spatial map, the robot also monitors what objects the human looks at, points to, and talks about over time. These items are assigned a tag with a value that indicates which objects have been the human's focus of attention and therefore have been salient (of interest) to her. This allows the robot to keep track of items that both the human and robot are mutually aware of. The human's current attentional focus is defined as what she is currently looking at. The human's referential focus is determined by the last object that was the object of shared attention with the robot. For instance, Figure <ref type="figure" target="#fig_9">11</ref> shows the robot and human sharing joint visual attention (represented in 3D); the robot has tracked the human's head pose and pointing gesture to the object referent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.3">Bootstrapping Social Referencing</head><p>This section presents a scenario (currently under development) where the robot's imitative capability, its attentional system, and its emotion system interact to bootstrap its ability to engage in social referencing. In Section 8.1 we summarized four capabilities that are important for social referencing and at what ages they begin to appear in human infants. In our model, the mechanisms associated with these three systems interact with simple associative learning mechanisms to achieve each equivalent developmental stage for the robot. Figure <ref type="figure" target="#fig_1">12</ref> shows the model of social referencing behavior as represented within the cognitive-affective architecture for the final stage.</p><p>In the first stage, the robot has the ability to discriminate human expressions and to respond with its own appropriate emotional response. To achieve this capability, the facial imitation system interacts with the emotion system to help the robot to recognize these expressions and respond in an emotionally appropriate manner. As discussed earlier, the intermodal representation within the imitation system can be used to help the robot to distinguish different facial expressions of the human. Furthermore, experiments with human subjects have shown that producing a facial expression generally associated with a particular emotion is sufficient for eliciting that emotion <ref type="bibr" target="#b78">[79]</ref>. The robot has a similar (innate) mechanism, so that the act of having the robot mimic the human's facial expression will induce the corresponding emotional state within the robot. Once the emotion is activated, the robot responds in a characteristic manner: Positive affect is accompanied with exploration and interaction behaviors, whereas negative affect is accompanied with avoidance or comfort seeking behaviors. In the second stage, the robot learns to form its own affective appraisals. This is accomplished via simple associative learning mechanisms within the affective appraisal system (a component of the emotion system). Given a novel stimulus (one that the robot does not yet know how to tag affectively), the robot uses its own current emotive state as the affective tag for the novel stimulus via simple associative learning. Once the human's expressions can be reliably recognized (in the first stage), this ability allows the robot to learn what these expressions mean in affective terms. The robot can learn the affective meaning of the observed facial expression during the facial imitation game. Specifically, this is accomplished within the affective appraisal system where the robot learns via simple association how to affectively tag a visually observed facial expression with the emotion that is induced within the robot when it imitates that expression via the mechanism proposed by Strack et al. <ref type="bibr" target="#b78">[79]</ref>.</p><p>In the third stage, the robot's reference skills are exercised by its shared attention system (as discussed in Section 8.2.2). Leonardo's attentional system determines the robot's focus of attention, monitors the attentional focus of the human, and uses both to keep track of the referential focus held by both. This allows the robot to shift its gaze and attentional focus to gather Figure <ref type="figure" target="#fig_1">12</ref> . Model of social referencing. This schematic shows how social referencing is implemented within Leonardo's extended cognitive-affective architecture. Significant additions to the imitation architecture include the attention system that models the attentional and referential state of the human and the robot, a belief system that bundles visual features with attentional states to represent coherent entities in the 3D space around the robot, an affective appraisal process (associated with the emotion system) that operates on the current set of beliefs, and the emotion system with its accompanying behavioral and expressive counterparts. The social referencing behavior executes in three passes through the architecture, each pass shown by a different shaded band. The numbers represent steps in processing as information flows through the architecture. In the first pass, the robot encounters a novel object. In the second pass, the robot references the human to see his or her reaction to the novel object. On the third pass, the robot uses the human's assessment as a basis to form its own affective appraisal of the object (step 15) and interacts with the object accordingly (step 18).</p><p>information (e.g., to look to the human's face for evidence of emotional response, or to look back to the novel toy to establish joint attention), while maintaining the correct referential focus. Keeping the attentional focus and referential focus as distinct states is critical, because it allows the acquired information (from shifting the attentional focus) to be associated with the novel object (the referential focus), rather than with what the robot happens to be visually attending to at a particular time.</p><p>In the final stage, the robot uses its shared attention and affective appraisal mechanisms to associate an emotionally communicated appraisal (provided by the human) with a novel object (the referential focus). The presence of a novel object gives rise to an internal state of uncertainty within the robot that triggers its information seeking behavior. This causes the robot to look to the human's face to see how he is reacting to the novel stimulus. The robot reads the human's expression (which the robot has already learned how to affectively appraise in the second stage). The affective appraisal system tags the object referent with this socially communicated affective information.</p><p>Once the robot knows how to affectively appraise the toy, that appraisal gives rise to the corresponding emotive state and behavioral response. If the novel toy is associated with positive affect, the robot enters into a positive emotive state and tends to explore or interact with the toy. If the novel toy is associated with negative affect, the robot enters into a negative emotive state and tends to avoid or reject the toy. The robot's emotive response towards that toy will persist to future interactions with it, because the robot knows how to appraise it affectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Summary</head><p>This discussion (as well as our ongoing efforts in developing a model of social referencing for Leonardo) has focused on emotional referencing. As outlined in Section 8.2, the robot's facial imitation capabilities play an important role in bootstrapping the first two stages of the social referencing skill. Mechanisms and interactions associated with the robot's imitative behavior can be used to help the robot recognize the human's emotive facial expressions and to learn their affective meaning. This allows the robot to participate in early forms of emotional communication (such as emotion contagion). The addition of joint attention mechanisms allows the robot to associate the affective messages of others with things in the world (stages 3 and 4). Thus facial imitation, in concert with shared attention and the emotion system, helps to bootstrap early forms of emotional understanding for the robot. This is an important milestone on the way to building robots capable of social understanding in the affective and referential realms.</p><p>In the broader picture of social referencing, instrumental referencing (discussed in the beginning of Section 8) can also bootstrap from imitative learning to help a child (or robot) learn how to interact with a novel stimulus-what to do rather than how to feel <ref type="bibr" target="#b81">[81]</ref>. This shall be the subject of future work as we extend Leonardo's imitative skills to the rest of its body so that it may learn new skills via imitation (see the following section).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Discussion and Related Work</head><p>Whereas the majority of work in robot imitation has focused on imitation-inspired mechanisms as a way to easily program a robot with new skills, ours has focused on imitation as a social process <ref type="bibr" target="#b14">[15]</ref> and a means to bootstrap further social understanding of others as described in Section 8 <ref type="bibr" target="#b15">[16]</ref>. In related work, Scassellati <ref type="bibr" target="#b74">[75]</ref> has explored social understanding by robots in the context of joint visual attention and developing a robot that imitates only the movement of entities that it deems to be animate. Dautenhahn <ref type="bibr" target="#b23">[24]</ref>, Billard and Dautenhahn <ref type="bibr" target="#b5">[6]</ref>, and Billard <ref type="bibr" target="#b7">[8]</ref> have explored an ''empathic'' style of social understanding in robots where the learner robot acquires a shared protocol with the model from an imitation/following context (see Section 9.3). In contrast, our work explores social understanding in the emotional and attentional realms, where the robot explicitly represents the mental states of the human as distinct from its own. This is critical for more sophisticated social behavior such as social referencing (as described in this article), or teamwork where the mental states of the human and robot must be shared and brought into alignment whenever there is a discrepancy <ref type="bibr" target="#b17">[18]</ref>.</p><p>Although a number of computational approaches for imitative behavior have also been inspired by the AIM model <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b75">76]</ref>, these have not been applied to the domain of early facial imitation. In fact, surprisingly little computational work has focused on facial imitation given the rich scientific literature on it. Instead, most robotic efforts have focused on imitating arm gestures, dexterity skills, or head movements. The majority of work in building systems that mimic facial expressions are designed to be puppeteering interfaces where a person can drive the expressions of an animated character or a robot using the movements of her own face <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b21">22]</ref>. Such efforts focus on technical issues relating to tracking facial features and facial expression recognition, rather than modeling facial imitation.</p><p>A number of different imitation paradigms have been explored in robotics to give robots the capability to learn from each other, from people, and about people. A couple of reviews on robot imitation can be found <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b76">77]</ref>. This section discusses how our particular interest in imitative behavior relates to and is different from these other efforts to build robots that imitate either robots or humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Learning by Demonstration</head><p>Some of the earliest work in this area is called learning by demonstration. In this approach, the robot (often a robotic manipulator) learns how to perform a new task by watching a human perform the same task. This may or may not involve imitative behavior. In the case where it does not, called tasklevel imitation, the robot learns how to perform the physical task of the demonstrator -such as stacking blocks <ref type="bibr" target="#b51">[52]</ref> or inserting pegs <ref type="bibr" target="#b44">[45]</ref> -without imitating the behavior of the demonstrator. Instead, the robot acquires a high-level task model, such as a hierarchy of goal states and the actions to achieve them, from observing the effects of human movements on objects in the environment.</p><p>In other work with highly articulated humanoid robots, learning by demonstration has been explored as a way to achieve efficient learning of dexterous motor skills <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b75">76]</ref>. The state-action space for such robots is too large to search for a solution in a reasonable time. Instead, the robot observes the human's performance, using both object and human movement information to estimate a control policy for the desired task. The human's demonstration helps to guide the robot's search through the space, providing it with a good region to initiate its own search. If given knowledge of the task goal (in the form of an evaluation function), robots have learned to perform a variety of physical tasks -for example, learning the game of ''ball in cup'' or a tennis forehand <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b64">65]</ref> by utilizing both the demonstrator's movement and that of the object.</p><p>Another way to accelerate learning is to encode the state-action space using a more compact representation. This makes the overall state-action space more compact and therefore faster to explore. Researchers have used biologically inspired representations of movement, such as movement primitives <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b52">53]</ref>, to encode movements in terms of goal-directed behaviors rather than discrete joint angles. Primitives allow movement trajectories to be encoded using fewer parameters and are combined to produce the entire movement repertoire. The tradeoff for this compact representation is loss of granularity and/or generality of the movement space. As a result, more recent work has focused on using imitation as a way of acquiring new primitives (as new sequences or combinations of existing primitives) that can be added to the repertoire <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>As discussed in Section 5.2.3, our approach also incorporates the notion of movement primitives. Facial configurations are represented as poses in the posegraph of each motor for the face. They can be sequenced, layered, or superimposed within the separate motor systems to generate novel facial expressions. For instance, the robot can learn how to produce a cocked eyebrow expression by making a goal-directed search over blending weights and poses within each motor system, and then layering these results to produce the novel expression. Our motor rep-resentation is very similar to that proposed by Meltzoff, encoding a ''directory of body configurations'' within the motor system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Learning to Imitate</head><p>In learning to imitate, the robot learns how to solve the correspondence problem through experience (i.e., how to map the observed movement of another onto the robot's own movement repertoire). One strategy for solving the correspondence problem is to represent the demonstrator's movement trajectory in the coordinate frame of the imitator's own motor coordinates. This approach was explored by Billard and Schaal, who recorded human arm movement data using a Sarcos SenSuit and then projected that data into an intrinsic frame of reference for a 41 DoF humanoid simulation.</p><p>Another approach, the use of perceptual-motor primitives <ref type="bibr" target="#b83">[83,</ref><ref type="bibr" target="#b48">49]</ref>, is inspired by the discovery of ''mirror neurons'' in primates. These neurons are active both when a goal-oriented action is observed and when the same action is performed (recall Section 3.3). <ref type="bibr">Mataric</ref> ´ <ref type="bibr" target="#b53">[54]</ref> implements this idea as an online encoding process that maps observed joint angles onto movement primitives to allow a simulated upper torso humanoid to learn to imitate a sequence of arm trajectories. Others have adapted the notion of mirror neurons to predictive forward models <ref type="bibr" target="#b86">[86]</ref>. For instance, Demiris and Hayes <ref type="bibr" target="#b26">[27]</ref> present a technique that emphasizes the bidirectional interaction between perception and action where movement recognition is directly accomplished by the movement-generating mechanisms. To accomplish this, a forward model for a behavior is built directly into the behavior module responsible for producing that movement. In model-based imitation learning, the imitator's motor acts are represented in task space, where they can be directly compared with the observed trajectory. Using this approach, Atkeson and Schaal <ref type="bibr" target="#b1">[2]</ref> show how a forward model and a priori knowledge of the task goal can be used to acquire a task-level policy from reinforcement learning in very few trials. They demonstrated an anthropomorphic robot learning how to perform a pole-balancing task in a single trial and a pendulum swing up task in three or four trials <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>As discussed in Section 3.3 and Section 6.1.2, our implementation is also inspired by the possible role that mirror neurons play in imitative behavior. In the approaches described above, mirrorneuron-inspired mechanisms are used as an online process either for mapping perceived movements to another coordinate frame or as forward models that are directly involved in generating the observed action. In contrast, our implementation is consistent with that discussed in <ref type="bibr" target="#b65">[66]</ref> and <ref type="bibr" target="#b58">[59]</ref>, where mirror neurons are believed to represent observed movement in terms of the creature's own motor coordinates (i.e., the intermodal representation). This concept of explicit representation (i.e., memory) is important in order to capture the goal-directed match-to-target search that characterizes exploratory imitative behavior of infants <ref type="bibr" target="#b61">[62]</ref>. It is also important in order to account for the ability of young infants to imitate deferred actions after a substantial time delay (on the order of hours and even days) that Meltzoff has observed <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b62">63]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Learning by Imitation</head><p>Imitative behavior can either be learned or be specified a priori. In learning by imitation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, the robot is given the ability to engage in imitative behavior. This serves as a mechanism that bootstraps further learning and understanding from guided exploration by following a model. Initial studies of this style of social learning in robotics focused on allowing one robot to learn reactive control policies to navigate through mazes <ref type="bibr" target="#b40">[41]</ref> or an unknown landscape <ref type="bibr" target="#b23">[24]</ref> by using simple perception (proximity and infrared sensors) to follow another robot that was adept at maneuvering in the environment. This approach has also been applied to allow a robot to learn interpersonal communication protocols between similar robots, between robots with similar morphology but that differ in scale <ref type="bibr" target="#b5">[6]</ref>, and with a human instructor <ref type="bibr" target="#b7">[8]</ref>.</p><p>Learning by imitation advocates an ''empathic'' or direct experiential approach to social understanding whereby a robot uses its internal mechanisms to assimilate or adopt the internal state of the other as its own <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b49">50]</ref>. Given our discussion of Section 3.2, we also advocate a simulation theoretic approach to achieve social understanding of people by robots.</p><p>However, this pure empathic understanding where the robot simply ''absorbs'' the experience and does not distinguish it as arising from self or being communicated by others is not sufficient for human-style cooperation. The reason is that the robot must be able to determine what is held in common, what is not, and therefore what must be communicated and agreed upon so that coordinated joint activity can be established and maintained. Hence, capturing this representational aspect of the ToM to allow the robot to maintain information about its own internal states as well as those of others is very important for building robots that can cooperate with people in a humanlike way.</p><p>Therefore, in our approach, the robot can use its own cognitive and affective mechanisms as a simulator for inferring the other's internal states. However, it is critical that they be represented as distinct from the robot's own states. For instance, our robot could not engage in social referencing if it could not attribute affective states to entities external to itself. Although the robot's understanding of how facial expression relates to internal affective states is bootstrapped by an empathic or simulation-theoretic approach, these affective states have a representational aspect that allows them to be attributed to novel stimuli.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.4">Imitation as Social Interaction</head><p>Imitative exchanges are among the earliest forms of interaction and communication that take place between infants and adults. The approaches to robot imitation presented above view the interaction in only one direction: from human demonstrator to robot learner. This relationship is hard coded into the robot in the learning-by-imitation work -the learner is programmed to follow the model. In learning by demonstration, the human performs the task while the robot passively observes the demonstration. In contrast, Leonardo learns how to imitate within a mixed-initiative interaction. When the robot leads the imitation game (human imitates robot), the robot learns its intermodal representation from this experience. Once this map has been acquired, the human can lead the game, and the robot will imitate his or her facial expressions.</p><p>Additionally, Leonardo must decide when to lead the imitative game, when to learn from the interaction, and when to follow. The robot's contingency metrics play an important role in allowing the robot to determine whether the human is playing the imitation game with it or not. This is very important, given that the robot must collect its own training instances to learn its intermodal representation. This is in contrast to the imitative approaches described above, where the robot cannot choose for itself when is the right or wrong time to engage in imitative behavior, to lead or to follow, or to learn from the interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Summary and Conclusion</head><p>Taken as a whole, Meltzoff 's work articulates a compelling story of the possible role imitation plays in the ultimate development of the ToM. The ability to understand human behavior in terms of the mental states responsible for producing it is very important for human-style collaboration (as argued in Section 1). For this reason, we are particularly interested in exploring imitation as a way to bootstrap further social understanding of robots so that they might someday cooperate with humans as capable teammates <ref type="bibr" target="#b17">[18]</ref>. Therefore, although other models have been proposed to explain neonatal facial imitation (e.g., positing that this early ability is based on innate fixed action patterns) such models do not serve our purposes because they do not allow for this ontogenetic trajectory that could ultimately lead to a ToM. This article presents a detailed computational model of early facial imitation that tries to capture some of its key characteristics. We have based our approach on the AIM model, in part because its mechanistic description affords implementation, but more importantly because it tries to account fundamentally for a multitude of aspects and abilities (e.g., innate endowments, early imitative behavior, the importance of the social context, its goal-directed quality, its representation aspects) that are important for explaining the development of facial imitation into more sophisticated Learning From and About Others C. Breazeal, D. Buchsbaum, J. Gray, D. Gatenby, and B. Blumberg imitative abilities -such as the ability to imitate deferred acts, and ultimately to imitate intended acts. Correspondingly, we have taken care to incorporate these aspects into our own implementation.</p><p>Finally, in Section 8 we have described how this work can be extended to implement social referencing whereby the robot can infer the affective reaction of others to a novel object and then apply this assessment to that object. This is considered to be a key milestone in the social development of human infants, for it presents one of the earliest cases where infants begin to understand others in terms of mental states. Furthermore, it is one of the earliest cases where infants begin to understand that such mental states are often referential -that they are about external things and events in the world. Thus, inspired by the social development of human infants, our key interest in pursuing models of imitation with robots is to explore its posited role in bootstrapping more sophisticated competencies for understanding the minds of others.</p><p>This ability is key for developing robots that understand humans as social beings. As argued in Sections 1 through 3, this capability should allow us to design socially intelligent robots that appear intelligent and capable in their interactions with humans, are able to learn from natural human instruction, are able to cooperate with people as capable partners, and are intuitive and engaging for humans to communicate and interact with socially. These skills represent a solid foundation for future applications where sociable robots will play a useful, helpful, and enjoyable role in the daily lives of ordinary people.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure1. Schematic of the active intermodal mapping hypothesis(Meltzoff and Moore [62]). AIM models the mechanisms necessary for infant facial imitation (see Section 4). This figure depicts the flow of data between the external world, the infant's internal representation of perceived expressions (the adult's expressions and his own), and the infant's motor system. Representations of the adult expression and the infant's own expression are compared in terms of organ relations. If the infant's current expression is not a good match for the adult's, the movement-end state directory (previously generated by the infant through motor babbling) is searched for a better match, which is then executed by the motor system. If subsequent comparisons still find the match between perceived and produced expressions to be inadequate, the motor system may execute a localized search of the motor space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2 . Leonardo, the robot and virtual simulator. Cosmetically finished (left), with mechanics exposed (center), and the animated model (right). Character design copyright Stan Winston Studio. Images n MIT Media Lab (left and right image) and Sam Ogden (center image).</figDesc><graphic coords="8,33.16,493.50,355.68,108.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3 . The Axiom ff T software. The picture on the left shows the camera input to the Axiom ff T software, with a human participant's face in the field of view. The white points on the person's face are the 22 points tracked by the Axiom ff T software (see Section 5.1). The picture on the right shows the Axiom ff T's representation of the person's face, with coordinates for each of the 22 points being tracked.</figDesc><graphic coords="9,61.09,36.98,300.00,112.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4 . Leonardo's architecture.Leonardo learns how to map perceived facial expressions into its intermodal space (its own joint space) by having the human participant imitate the robot. Leonardo generates a variety of poses by motor babbling. When the human's movements are contingent on its own, the robot decides it is being imitated, and uses the human's current expression and its own current expression to train a set of neural nets that it uses for mapping the human's expression into the intermodal space. Once these nets are trained to encode this mapping, Leonardo can convert data into its intermodal representation and classify the pose as one of its own. This allows the robot to produce a similar pose, thereby imitating the human. This diagram shows an overview of how these steps are accomplished within the robot's cognitive architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5 . Leonardo's basis facial poses. The poses are broken up by organ into three groups. Each group of facial poses makes up a posegraph for that organ (see Section 5.2.3). For each group, these poses represent the convex hull of all the possible poses for that organ; the basis poses can be blended together using different blend weights to create other possible configurations.</figDesc><graphic coords="11,19.85,44.76,383.76,172.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>{ 2 Figure 7 .</head><label>27</label><figDesc>Figure 7 . Leonardo imitating three human participants. This figure shows Leo imitating a number of facial expressions presented by three different human participants. The first row shows the camera's view of the human expression.In each user grouping, the second row shows the intermodal representation of the human expression, that is, the human's expression mapped onto Leo's own joint space. The third row shows Leonardo's best approximation of the intermodal representation of the human pose after the search-to-match process. As can be seen, Leonardo is able to use a goaldirected search of its blend space to find very close approximations of the human's pose. The intermodal representation was trained by one person and then tested by several different people.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8 . Noise in the neural network output and correction by Leo's motor system. The figure shows the human's expression, the neural network's direct mapping of this expression onto Leo's joint space, and the initial closest pose to this mapping in Leonardo's posegraph. The areas circled indicate joint positions in the direct mapping that are not possible for the physical robot to achieve. By using the robot's closest basis pose as the starting point for the search-tomatch process, Leonardo avoids attempting to execute impossible joint configurations.</figDesc><graphic coords="18,152.38,38.26,160.82,113.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9 . Goal-directed search towards target pose. Once Leonardo has mapped the human's pose onto its own joint space, creating a target pose, the robot executes a goal-directed search of its possible facial expressions to find the best match to this target. In this figure, the intermediate stages of Leonardo's goal-directed search for two target poses (shown on the right) are presented.</figDesc><graphic coords="18,20.37,469.64,383.96,122.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .</head><label>10</label><figDesc>Figure10. The training set that Virtual Leonardo uses to train its intermodal representation (Human Imitates Leo). As can be seen (Leo Imitates Human), Leonardo can then imitate facial configurations that involve combining intermodal representations for different regions of the face. By searching each of its motor systems (left eye region, right eye region, and mouth) for the closest match in the overall pose, Leo can successfully imitate a novel cocked eyebrow configuration where one brow is elevated and the other is lowered.</figDesc><graphic coords="19,19.75,50.52,383.99,332.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11 . Leonardo's shared attention representation in 3D. The robot's visualizer shows the robot and a human sharing joint visual attention on the same object. The right image shows the visual input of a person looking at and pointing to the center button. The left image shows the visualization of the robot's internal model. In this visualization, the human's gaze is shown as the vector starting at the human's head, and his pointing gesture is shown as the line from his torso. The robot looks at the same button (arrow from robot's head) to engage in deictic gaze. The attentional state of robot and human are explicitly represented, as is the referent focus (see Section 8.2.2).</figDesc><graphic coords="22,19.68,421.69,384.15,153.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="17,113.69,70.61,250.63,494.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>An overall comparison of Meltzoff's AIM model of infant imitation and our robotic imitation architecture. This table summarizes how our approach and AIM's address a variety of tasks necessary for imitating facial expressions. The tasks are listed in the leftmost column. For a more detailed explanation of the steps of AIM see Section 4. For a full explanation of our imitation architecture see Sections 5 and 6.</figDesc><table><row><cell>Task</cell><cell>AIM</cell><cell>Our implementation</cell></row><row><cell>Locate and recognize</cell><cell>Organ</cell><cell>Axiom ffT software,</cell></row><row><cell>model's facial</cell><cell>identification</cell><cell>movement and</cell></row><row><cell>features and</cell><cell></cell><cell>contingency detection</cell></row><row><cell>movements</cell><cell></cell><cell></cell></row><row><cell>Find correspondence</cell><cell>Organ</cell><cell>Trained neural nets</cell></row><row><cell>between perceived</cell><cell>identification</cell><cell></cell></row><row><cell>features and own</cell><cell></cell><cell></cell></row><row><cell>features</cell><cell></cell><cell></cell></row><row><cell>Use correspondence</cell><cell>Map perceived</cell><cell>Map perceived</cell></row><row><cell>between model's</cell><cell>expression into</cell><cell>expression into</cell></row><row><cell>face and own to</cell><cell>intermodal space,</cell><cell>intermodal space,</cell></row><row><cell>identify an</cell><cell>using organ</cell><cell>using Leo's</cell></row><row><cell>expression to be</cell><cell>relations as the</cell><cell>joint space as the</cell></row><row><cell>produced</cell><cell>universal</cell><cell>universal</cell></row><row><cell></cell><cell>representation.</cell><cell>representation.</cell></row><row><cell></cell><cell>Search the</cell><cell>Search the posegraph</cell></row><row><cell></cell><cell>movement -end</cell><cell>for the closest</cell></row><row><cell></cell><cell>state directory for</cell><cell>matching basis pose.</cell></row><row><cell></cell><cell>the closest end</cell><cell></cell></row><row><cell></cell><cell>state.</cell><cell></cell></row><row><cell>Discover motor</cell><cell>Motor babbling</cell><cell>Posegraph contains</cell></row><row><cell>commands/</cell><cell>builds up</cell><cell>routes between</cell></row><row><cell>movements</cell><cell>knowledge</cell><cell>poses. Motor</cell></row><row><cell>necessary to</cell><cell>of how to achieve</cell><cell>programs know</cell></row><row><cell>generate desired</cell><cell>various organ</cell><cell>how to move</cell></row><row><cell>expression</cell><cell>relations; adds</cell><cell>the body along</cell></row><row><cell></cell><cell>this knowledge to</cell><cell>these routes.</cell></row><row><cell></cell><cell>the movement -end</cell><cell></cell></row><row><cell></cell><cell>state directory.</cell><cell></cell></row><row><cell>Judge success of</cell><cell>Use proprioceptive</cell><cell>Compare closest</cell></row><row><cell>imitation, and</cell><cell>feedback to</cell><cell>basis pose with</cell></row><row><cell>improve</cell><cell>compare</cell><cell>intermodal</cell></row><row><cell></cell><cell>achieved organ</cell><cell>representation</cell></row><row><cell></cell><cell>relations with</cell><cell>of perceived pose.</cell></row><row><cell></cell><cell>perceived organ</cell><cell>Locally explore</cell></row><row><cell></cell><cell>relations. Locally</cell><cell>blend space to</cell></row><row><cell></cell><cell>explore motor</cell><cell>find a better</cell></row><row><cell></cell><cell>space to find a</cell><cell>match. Repeat</cell></row><row><cell></cell><cell>better match.</cell><cell>until no</cell></row><row><cell></cell><cell>Repeat until</cell><cell>better match</cell></row><row><cell></cell><cell>satisfied.</cell><cell>can be found.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Artificial Life Volume 11, Number 1 -2</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Learning From and About Others C. Breazeal, D. Buchsbaum, J. Gray, D. Gatenby, and B. Blumberg</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work would not be possible but for the contributions of many others. The facial feature tracking software is provided by Nevengineering Inc. Stan Winston Studio provided the physical Leonardo robot, and Geoff Beatty and Ryan Kavanaugh provided the virtual model and facial animations. Special thanks to Marc Downie for providing us with advice while developing the motor system, and to Andrew Brooks for his assistance with the facial tracking software. This work is funded in part by a DARPA MARS grant and in part by the Digital Life and Things that Think consortia.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The automation of an astronaut&apos;s humanoid assistant</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ambrose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Savely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bluethmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kortenkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/RAS International Conference on Humanoid Robots ( Humanoids &apos;03)</title>
		<meeting>the IEEE/RAS International Conference on Humanoid Robots ( Humanoids &apos;03)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning tasks from single demonstration</title>
		<author>
			<persName><forename type="first">C</forename><surname>Atkeson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<meeting><address><addrLine>Piscataway, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="1706" to="1712" />
		</imprint>
	</monogr>
	<note>ICRA 97</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robot learning from demonstration</title>
		<author>
			<persName><forename type="first">C</forename><surname>Atkeson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<meeting><address><addrLine>San Francicsco</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="12" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Early understanding of referential intent and attentional focus: Evidence from language and emotion</title>
		<author>
			<persName><forename type="first">D</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Moses</surname></persName>
		</author>
		<editor>C. Lewis &amp; P. Mitchell</editor>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Erlbaum</publisher>
			<biblScope unit="page" from="133" to="156" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>Children&apos;s early understanding of mind</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Precursors to a theory of mind: Understanding attention in others</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baron-Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Theories of Mind</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Whiten</surname></persName>
		</editor>
		<meeting><address><addrLine>Oxford, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Blackwell Press</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="233" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Grounding communication in autonomous robots: An experimental study</title>
		<author>
			<persName><forename type="first">A</forename><surname>Billard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dautenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1 -2</biblScope>
			<biblScope unit="page" from="71" to="81" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Experiments on human-robot communication with Robota, an imitative learning and communicating doll robot</title>
		<author>
			<persName><forename type="first">A</forename><surname>Billard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dautenhahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hayes</surname></persName>
		</author>
		<idno>CPM-98-38</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Socially Situated Intelligence Workshop as part of the Fifth Conference of the Simulation of Adaptive Behavior. Center for Policy Modeling technical report series</title>
		<meeting>Socially Situated Intelligence Workshop as part of the Fifth Conference of the Simulation of Adaptive Behavior. Center for Policy Modeling technical report series</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imitation: A means to enhance learning of a synthetic proto-language in an autonomous robot</title>
		<author>
			<persName><forename type="first">A</forename><surname>Billard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Imitation in animals and artifacts</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Dautenhahn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Nehaniv</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="281" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A connectionist model for on-line learning by imitation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Billard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2001 IEEE-RSJ International Conference on Intelligent Robots and Systems</title>
		<meeting>the 2001 IEEE-RSJ International Conference on Intelligent Robots and Systems<address><addrLine>Piscataway, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Computations underlying the execution of movement: A biological perspective</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bizzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Mussa-Ivaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Giszter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">253</biblScope>
			<biblScope unit="page" from="287" to="291" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Integrated learning for interactive synthetic characters</title>
		<author>
			<persName><forename type="first">B</forename><surname>Blumberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Downie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tomlinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH &apos;02)</title>
		<meeting>the 29th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH &apos;02)<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="417" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robots that imitate humans</title>
		<author>
			<persName><forename type="first">C</forename><surname>Breazeal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scassellati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="481" to="487" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A context-dependent attention system for a social robot</title>
		<author>
			<persName><forename type="first">C</forename><surname>Breazeal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scassellati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Sixteenth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="1146" to="1151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Active vision systems for sociable robots</title>
		<author>
			<persName><forename type="first">C</forename><surname>Breazeal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Edsinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fitzpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scassellati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part A</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="443" to="453" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imitation as social exchange between humans and robots</title>
		<author>
			<persName><forename type="first">C</forename><surname>Breazeal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AISB-99 Workshop on Imitation in Animals and Artifacts</title>
		<meeting>AISB-99 Workshop on Imitation in Animals and Artifacts</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Breazeal</surname></persName>
		</author>
		<title level="m">Designing sociable robots</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Emotion and sociable humanoid robots</title>
		<author>
			<persName><forename type="first">C</forename><surname>Breazeal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human Computer Studies</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="119" to="155" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tutelage and collaboration for humanoid robots</title>
		<author>
			<persName><forename type="first">C</forename><surname>Breazeal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lieberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lockerd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chilongo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Humanoid Robotics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="315" to="348" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Creature smarts: The art and architecture of a virtual brain</title>
		<author>
			<persName><forename type="first">R</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Isla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Downie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Blumberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2001 Computer Game Developers Conference</title>
		<meeting>the 2001 Computer Game Developers Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The ontogeny and phylogeny of joint visual attention</title>
		<author>
			<persName><forename type="first">G</forename><surname>Butterworth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural theories of mind</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Whiten</surname></persName>
		</editor>
		<meeting><address><addrLine>Oxford, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Blackwell</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="223" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Perception, appraisal, and emotion: The onset of social referencing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Infant social cognition</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Lamb</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Sherrod</surname></persName>
		</editor>
		<meeting><address><addrLine>Hillsdale, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Erlbaum</publisher>
			<date type="published" when="1981">1981</date>
			<biblScope unit="page" from="273" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Real time tracking and imitation of facial expression</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPIE International Conference on Imaging and Graphics</title>
		<meeting>SPIE International Conference on Imaging and Graphics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Descartes&apos; error: Emotion, reason, and the human brain</title>
		<author>
			<persName><forename type="first">A</forename><surname>Damasio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Putnam</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Getting to know each other -Artificial social intelligence for autonomous robots</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dautenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="333" to="356" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Mental simulation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Blackwell</publisher>
			<pubPlace>Oxford, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imitative learning mechanisms in robots and humans</title>
		<author>
			<persName><forename type="first">J</forename><surname>Demiris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hayes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth European Workshop on Learning Robots</title>
		<meeting>the Fifth European Workshop on Learning Robots</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imitation as a dual-route process featuring predictive and learning components: A biologically plausible computational model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Demiris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hayes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Imitation in animals and artifacts</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Dautenhahn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Nehaniv</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="321" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Behavior, animation, and music: The music and movement of synthetic characters</title>
		<author>
			<persName><forename type="first">M</forename><surname>Downie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">MIT Program in Media Arts and Sciences</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Are there basic emotions?</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="550" to="553" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Review of research, 1970 to 1980</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Oster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Emotion in the human face</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1982">1982</date>
			<biblScope unit="page" from="147" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Social referencing in infancy</title>
		<author>
			<persName><forename type="first">S</forename><surname>Feinman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Merrill-Palmer Quarterly</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="445" to="470" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A critical review of social referencing in infancy</title>
		<author>
			<persName><forename type="first">S</forename><surname>Feinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-F</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sawyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Social referencing and the social construction of reality in infancy</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Feinman</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Plenum Press</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="15" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Automated derivation of primitives for movement classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">´</forename><surname>Mataric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Robots</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="54" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A survey of social robots</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Nourbakshsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dautenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="143" to="166" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Emotions require cognitions, even in simple ones</title>
		<author>
			<persName><forename type="first">N</forename><surname>Frijda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The nature of emotion</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">&amp;</forename><forename type="middle">R</forename><surname>Davidson</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="197" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mirror neurons and the simulation theory of mind-reading</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gallese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="493" to="501" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Action recognition in the premotor cortex</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gallese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fadiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fogassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rizzolatti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="593" to="609" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Desire, intention, and the simulation theory</title>
		<author>
			<persName><forename type="first">A</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intention and intentionality</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Malle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Moses</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">&amp;</forename><forename type="middle">D</forename><surname>Baldwin</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="207" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Folk psychology as simulation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mind and Language</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="158" to="171" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A face robot able to recognize and produce facial expression</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Intelligent Robots and Systems</title>
		<meeting>the International Conference on Intelligent Robots and Systems</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="1600" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A robot controller using learning by imitation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Demiris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Symposium on Intelligent Robots and Systems</title>
		<meeting>the Second International Symposium on Intelligent Robots and Systems</meeting>
		<imprint>
			<publisher>LIFTA-IMAG</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="198" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Understanding other minds from the inside</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mind, reason and imagination</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Sosa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Dancy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Haldane</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Harman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Jackson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Lucan</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="28" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The effects of maternal positive, neutral, and negative affective communications on infant responses to new toys</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Risenhoover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gunnar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Child Development</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="937" to="944" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A descriptive analysis of infant social referencing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gunnar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Child Development</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="626" to="634" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Skill acquisition from human demonstration using a hidden Markov model</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hovland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mc Carragher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation (ICRA &apos;96)</title>
		<meeting>the IEEE International Conference on Robotics and Automation (ICRA &apos;96)<address><addrLine>Piscataway, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="2706" to="2711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Human emotions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Izard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977">1977</date>
			<publisher>Plenum Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Four systems for emotion activation: Cognitive and noncognitive processes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Izard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="68" to="90" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Cognition is one of four types of emotion activating systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Izard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The nature of emotion</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">&amp;</forename><forename type="middle">R</forename><surname>Davidson</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="203" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Primitive-based movement classification for humanoid imitation</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">C</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matari</surname></persName>
		</author>
		<idno>IRIS-00-385</idno>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
		<respStmt>
			<orgName>University of Southern California, Institute for Robotics and Intelligent Systems</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Report</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Attention-sharing and behavior-sharing in human-robot communication</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kozima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Robot and Human Communication</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="9" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Emotions as behavior regulators: Social referencing in infancy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Klinnert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Source</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Emde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Svejda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The emotions</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Plutchik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Kellerman</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1983">1983</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="57" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning by watching: Extracting reuseable task knowledge from visual observation of human performance</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kuniyoshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Inaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Inoue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics and Automation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="799" to="822" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Movement control methods for complex, dynamically simulated agents: Adonis dances the Macarena</title>
		<author>
			<persName><forename type="first">´</forename><surname>Mataric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">B</forename><surname>Masson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Autonomous Agents</title>
		<meeting>the Second International Conference on Autonomous Agents<address><addrLine>Minneapolis, MN</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="317" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Sensory-motor primitives as a basis for imitation: Linking perception to action and biology to robotics</title>
		<author>
			<persName><forename type="first">´</forename><surname>Mataric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Imitation in animals and artifacts</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Dautenhahn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Nehaniv</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="391" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Neonatal synesthesia: Implications for the processing of speech and faces</title>
		<author>
			<persName><forename type="first">D</forename><surname>Maurer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Developmental neurocognition: Speech and face processing in the first year of life</title>
		<editor>
			<persName><forename type="first">B</forename><surname>De Boysson-Bardies</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>De Schonen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Jusczik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Macneilage</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">&amp;</forename><forename type="middle">J</forename><surname>Morton</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="109" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The human infant as imitative generalist: A 20-year progress report on infant imitation with implications for comparative psychology</title>
		<author>
			<persName><forename type="first">A</forename><surname>Meltzoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Social learning in animals: The roots of culture</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Galef</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Heyes</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="347" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Understanding the intensions of others: Re-enactment of intended acts by 18-month-old children</title>
		<author>
			<persName><forename type="first">A</forename><surname>Meltzoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Developmental Psychology</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="838" to="850" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Infant imitation after a 1-week delay: Long-term memory for novel acts and multiple stimuli</title>
		<author>
			<persName><forename type="first">A</forename><surname>Meltzoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Developmental Psychology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="470" to="476" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">What imitation tells us about social cognition: A rapprochement between developmental psychology and cognitive neuroscience</title>
		<author>
			<persName><forename type="first">A</forename><surname>Meltzoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Decety</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Philosophical Transactions of the Royal Society of London B</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">358</biblScope>
			<biblScope unit="page" from="491" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The role of imitation in understanding persons and developing a theory of mind</title>
		<author>
			<persName><forename type="first">A</forename><surname>Meltzoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gopnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Understanding other minds, perspectives from autism</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Baron-Cohen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Tager-Flusberg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Cohen</surname></persName>
		</editor>
		<meeting><address><addrLine>Oxford, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="335" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Like me&apos;&apos; as a building block for understanding other minds: Bodily acts, attention, and intention</title>
		<author>
			<persName><forename type="first">A</forename><surname>Meltzoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intention and intentionality</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Malle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Moses</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">&amp;</forename><forename type="middle">D</forename><surname>Baldwin</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="171" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Explaining facial imitation: A theoretical model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Meltzoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Early Development and Parenting</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="179" to="192" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Imitation, memory and the representation of persons</title>
		<author>
			<persName><forename type="first">A</forename><surname>Meltzoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infant Behavior and Development</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="83" to="99" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A tennis serve and upswing learning robot based on bi-directional theory</title>
		<author>
			<persName><forename type="first">H</forename><surname>Miyamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kawato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1131" to="1344" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A Kendama learning robot based on bi-directional theory</title>
		<author>
			<persName><forename type="first">H</forename><surname>Miyamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gandolfo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Koike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Osu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kawato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1181" to="1302" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Schema design and implementation of the grasp-related mirror neuron system</title>
		<author>
			<persName><forename type="first">E</forename><surname>Oztop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arbib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="116" to="140" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Emotions: A general psychoevolutionary theory</title>
		<author>
			<persName><forename type="first">R</forename><surname>Plutchik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Approaches to emotion</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Sherer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Elkman</surname></persName>
		</editor>
		<meeting><address><addrLine>Hillsdale, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Erlbaum</publisher>
			<date type="published" when="1984">1984</date>
			<biblScope unit="page" from="197" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">The emotions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Plutchik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>University Press of America</publisher>
			<pubPlace>Lanham, MD</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Does the chimpanzee have a theory of mind?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Premack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Woodruff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="515" to="526" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A common coding approach to perception and action</title>
		<author>
			<persName><forename type="first">W</forename><surname>Prinz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Relationships between perception and action</title>
		<editor>
			<persName><forename type="first">O</forename><surname>Neumann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">&amp;</forename><forename type="middle">W</forename><surname>Prinz</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="167" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Imitation learning in infants and robots: Towards probabilistic computational models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Meltzoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Artificial Intelligence and Simulation of Behavior (AISB): Cognition in machines and animals</title>
		<meeting>Artificial Intelligence and Simulation of Behavior (AISB): Cognition in machines and animals</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>Aberystwith: SSAISB</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Premotor cortex and the recognition of motor actions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rizzolatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fadiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gallese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fogassi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Brain Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="131" to="141" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Imitation and mechanisms of joint attention: A developmental structure for building social skills on a humanoid robot</title>
		<author>
			<persName><forename type="first">B</forename><surname>Scassellati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computation for metaphors, analogy and agents</title>
		<title level="s">Springer Lecture Notes in Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Nehaniv</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">1562</biblScope>
			<biblScope unit="page" from="176" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Foundations for a theory of mind for a humanoid robot</title>
		<author>
			<persName><forename type="first">B</forename><surname>Scassellati</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Electrical Engineering and Computer Science, MIT</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Theory of mind for a humanoid robot</title>
		<author>
			<persName><forename type="first">B</forename><surname>Scassellati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Robots</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="24" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Learning from demonstration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Mozer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Petsche</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1040" to="1046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Is imitation learning the route to humanoid robots?</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="233" to="242" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Evidence for both universality and cultural specificity of emotion elicitation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The nature of emotion</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">&amp;</forename><forename type="middle">R</forename><surname>Davidson</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Erlbaum</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="172" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Inhibiting and facilitating conditions of the human smile: A nonobtrusive test of the facial feedback hypothesis</title>
		<author>
			<persName><forename type="first">F</forename><surname>Strack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stepper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="768" to="777" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Communication and cooperation in early infancy: A description of primary</title>
		<author>
			<persName><forename type="first">C</forename><surname>Trevarthen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Life</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">61</biblScope>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<author>
			<persName><forename type="first">Learning</forename><surname>From</surname></persName>
		</author>
		<author>
			<persName><forename type="first">About</forename><surname>Others</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Breazeal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Buchsbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gatenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Blumberg</surname></persName>
		</author>
		<title level="m">Before speech: The beginning of interpersonal communication</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Bullowa</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="page" from="321" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">The links between imitation and social referencing</title>
		<author>
			<persName><forename type="first">I</forename><surname>Uzgiris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kruper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Social referencing and the social construction of reality</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Feinman</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Plenum Press</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="115" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">The development of social referencing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Walden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ogan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Child Development</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="1230" to="1240" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Experiments in imitation using perceptuo-motor primitives</title>
		<author>
			<persName><forename type="first">S</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">´</forename><surname>Mataric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Autonomous Agents</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Sierra</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Gini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Rosenschein</surname></persName>
		</editor>
		<meeting>the Fourth International Conference on Autonomous Agents<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="136" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Machiavellian intelligence II: Extensions and evaluations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Whiten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Byrne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Imitation, mirror neurons and autism</title>
		<author>
			<persName><forename type="first">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Whiten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Suddendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Perrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroscience and Biobehavioral Reviews</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="285" to="295" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Multiple paired forward and inverse models for motor control</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wolpert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kawato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1317" to="1329" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
