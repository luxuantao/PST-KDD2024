<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">People Detection and Tracking from Aerial Thermal Views</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jan</forename><surname>Portmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Autonomous Systems Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Simon</forename><surname>Lynen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Autonomous Systems Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Margarita</forename><surname>Chli</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Autonomous Systems Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Roland</forename><surname>Siegwart</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Autonomous Systems Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">People Detection and Tracking from Aerial Thermal Views</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9277003A6FEB811F74E91A9E82295CC7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Detection and tracking of people in visible-light images has been subject to extensive research in the past decades with applications ranging from surveillance to searchand-rescue. Following the growing availability of thermal cameras and the distinctive thermal signature of humans, research effort has been focusing on developing people detection and tracking methodologies applicable to this sensing modality. However, a plethora of challenges arise on the transition from visible-light to thermal images, especially with the recent trend of employing thermal cameras onboard aerial platforms (e.g. in search-and-rescue research) capturing oblique views of the scenery. This paper presents a new, publicly available dataset of annotated thermal image sequences, posing a multitude of challenges for people detection and tracking. Moreover, we propose a new particle filter based framework for tracking people in aerial thermal images. Finally, we evaluate the performance of this pipeline on our dataset, incorporating a selection of relevant, state-of-the-art methods and present a comprehensive discussion of the merits spawning from our study.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>With the relaxation of the almost exclusive use of thermal cameras on military applications, the research community has been experiencing growing interest in their use in applications such as search-and-rescue. While people tracking in visible-light images has been studied for decades, the application of the developed methodologies on thermal images is far from straight-forward. As visual appearance cues in form of color and texture are no longer available, optaining meaningful segmentation results becomes challenging, especially given variable environmental conditions (e.g., weather, type of scenery). Beause texture information from color is rare, associating detections of humans before and after their paths cross is no longer possible unless tracking is employed on a sequence of consecutive images.</p><p>With the increasing resolution and decreasing size, weight, cost and power consumption of thermal cameras, mounting them onboard aerial platforms for search-and-rescue scenarios has become increasingly popular where victims often need to be searched for within a potentially large area. In order to boost effectiveness of rescue missions, we aim for employing Unmanned Aerial Vehicles (UAVs) to gain an overview of the scene. However, people detection from such oblique, top-down views is a real challenge and little work exists in the literature addressing this problem, especially in the context of thermal imaging.</p><p>The research leading to these results has received funding from the European Community's Seventh Framework Programme (FP7) under grant agreements n.285417 (ICARUS) and n.600958 (SHERPA). We propose a detection and tracking system processing thermal image sequences viewing the scene from viewpoints resembling those captured from of a UAV. As a novelty, we introduce a pipeline containing a robust background subtraction method and a particle filter guided detector. We show that our tracking framework outperforms all implemented detectors in terms of recall rates at high precision even if the camera setup experiences large, unsteady motion. Our framework achieves dramatic improvement on recall (of about 7× the recall of the best detector, on occasions), while reaching real-time performance of 16 Hz. Fig. <ref type="figure" target="#fig_0">1</ref> shows intermediate steps within our tracking pipeline when operating on both frontal and oblique views of humans present in our manually annotated and publicly available dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. People Detection &amp; Tracking in visible-light images</head><p>Detecting and tracking people, and more generally, objects, has been a very active area of research over the last couple of decades. The works in <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b1">[2]</ref> present a thorough study of state-of-the-art people detectors. When an image sequence is available, tracking on top of pure detection can provide better performance as discussed in <ref type="bibr" target="#b2">[3]</ref>. One of the first object detection approaches was proposed by Viola and Jones <ref type="bibr" target="#b3">[4]</ref>, initially for face detection. Inspired by <ref type="bibr" target="#b3">[4]</ref> in this paper we adapt and extend this methodology to construct a body part detector in the people detection paradigm, as discussed in Section II-B. Another extremely popular detection algorithm is based on Histograms of Oriented Gradients (HOG) <ref type="bibr" target="#b4">[5]</ref> which allows the comparison of an object shape with a pre-trained model. An extension has been proposed in <ref type="bibr" target="#b5">[6]</ref>, which takes body parts and their relative position into account, however at the cost of lower processing speed <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. From visible-light to thermal image sequences</head><p>The application of such detectors on thermal imagery however poses several challenges: Thermal cameras still have significantly lower resolution than their visible-light counterparts and are commonly corrupted by thermal noise. Additionally, reasoning between different, correctly classified objects becomes more difficult since texture information is rare. In <ref type="bibr" target="#b6">[7]</ref>, thermal as well as visible-light images are combined to detect cars and humans from a UAV viewpoint. In <ref type="bibr" target="#b7">[8]</ref>, thermal cameras have been applied to track pedestrians from an upfront viewpoint for night driving using a fusion of the hyper permutation network, a hierarchical contour matching algorithm and a cascaded classifier, respectively. A detector based on SURF features aided by a Kalman filter to predict the motion of individual features has been proposed in <ref type="bibr" target="#b8">[9]</ref>.</p><p>Since humans most often have a different body temperature than the surrounding background, background subtraction method offers a first and fast selection method to truncate the detector search space to image regions containing humans. In <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b10">[11]</ref>, a background subtraction method has been applied, which requires a static thermal camera for surveillance scenarios. In this work, we employ the ViBe <ref type="bibr" target="#b11">[12]</ref> background estimation, which is capable of segmenting regions that are both hotter and colder than the environment and can deal with situations where the camera is moving, which is essential in our UAV application. Our algorithm processes thermal (long-infrared) wavelengths only, enabling tracking during night and in other situations where the image quality of visible-light cameras is limited. As the framework is intended to be carried by a UAV, we focus on camera scenes from an elevated platform, where humans appear and move differently in image space than when observed from a ground based viewpoint. Our particle filter follows the approach of <ref type="bibr" target="#b12">[13]</ref> with several necessary adjustments described in section Section II-C to enable tracking in thermal rather than visible imagery and on the limited computational power that can be carried by a small-sized UAV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHODOLOGY</head><p>Our approach comprises of three steps. Background subtraction is used to generate candidate foreground regions for accelerated detection and improved tracking performance. We employ several detectors in our pipeline and evaluate their performance. Finally, the tracker uses both detections as well as foreground regions classified by the first step as guidance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Background Subtraction</head><p>Background subtraction effectively reduces the search space for the people detector, which is otherwise the most time consuming part of the tracking process, as evident in Table <ref type="table" target="#tab_0">I</ref>. The employed background subtraction method ViBe <ref type="bibr" target="#b11">[12]</ref> stores a background model by randomly selecting image  values at the current pixel location from past frames and image values at the location of neighbouring pixels in the current image. Every pixel of a new image is classified as background if its intensity value is within a predefined threshold of the background model pixels. Connected foreground regions of a number of pixels above a threshold, form ROIs. We adapt our implementation to allow processing of 16-bit images and introduce a variable threshold which is based on the standard deviation over the image instead of a fixed threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Detector</head><p>Within our tracker framework, we employ different detectors publicly available in OpenCV and propose a part based detector. For the high performance <ref type="bibr" target="#b0">[1]</ref> HOG detector a descriptor is obtained by calculating histograms of image gradients followed by ordering and normalizing them in blocks followed by e.g. a SVM based training. A sliding window approach, is then commonly used for object detection. We trained our own HOG classifier using a linear SVM and our training dataset described in Section III with a cell size of 8×8, a block size of 16×16 px and 128×64 px sized training images. Additionally we implemented a HOG classifier trained on the well-known INRIA <ref type="bibr" target="#b4">[5]</ref> dataset, which contains visible-light images only. An extension to HOG, LatentSVM <ref type="bibr" target="#b13">[14]</ref> detects not only objects in the exact shape it has been trained for, but also in different body configurations in the case of articulated objects such as humans. Instead of classifying an object as a whole, LatentSVM searches for distinctive parts and returns a confidence value, taking into account the positions of the detected parts. As for HOG, we trained a detector with our own thermal dataset as well as the INRIA dataset using the parameters suggested by the authors.</p><p>Another detector concept is based on cascades of Local Binary Patterns (LBP) <ref type="bibr" target="#b14">[15]</ref>. Here the image intensity of points around a center pixel are compared to the intensity of the center pixel itself the binary test results are accumulated in a histogram within a cell of pixels. In the training step, boosting techniques are applied to select the most representative features. Combining these tests in a cascade allow high detection rates by gradually filtering out non matching objects. Object classification of input images is obtained by using a sliding window approach and a pyramid representation of the image. Here, the LBP cascade is trained with our thermal training set.</p><p>To render cascades of simple features more competitive, we extend a Haar feature based cascade to what we coin as the "Body Part based Detector (BPD)". Haar features are composed of rectangular windows containing two image regions, over which the pixel intensities are added; the difference of these sums correspond to the feature we use. As in <ref type="bibr" target="#b3">[4]</ref>, distinctive features are selected from positive training images using AdaBoost <ref type="bibr" target="#b15">[16]</ref>, tested against negative training images and stored in a cascade. To cope with the varying sizes and poses of humans observed from a UAV perspective, we only choose to train on head, upper body and legs as the most characteristic parts potentially observable.</p><p>After finding head candidates by applying a sliding window approach, new image scan windows are constructed by rotating rectangles around the candidate followed by applying a second detector trained for upper bodies. If the response is above a threshold, the procedure is repeated to search for a single leg. Thus, the detector becomes rotation-invariant and able to classify humans at different angles. Furthermore the employed individual detectors remain simple and can be trained very quickly, as opposed to diversifying the fundamental features. The overall detection speed outperforms other detectors such as HOG (see Table <ref type="table" target="#tab_0">I</ref>). Fig. <ref type="figure" target="#fig_2">3</ref> illustrates training examples, while Fig. <ref type="figure" target="#fig_1">2</ref> shows example detections of our BPD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Tracker</head><p>A tracker increases considerably the identification and location accuracy of an object making use of both detection results and temporal constraints. Our approach is based on a particle filter, such that each identified object is assigned a number of particles. Following the assignment of detections in a new image to the corresponding particle clouds of the tracker, the particles are attributed a weight based on how well they represent the actual object. These weights are then sampled from a probability distribution used to determine the future location of the object.</p><p>Each tracker T L is composed of a label L, K particles P, the last associated guidance candidate C defined later in Eq. ( <ref type="formula" target="#formula_1">2</ref>), a rectangle r of dimensions corresponding to the average of the last N r associated guidance candidates, as well as the position m and velocity u of the tracker (mean of the particles):</p><formula xml:id="formula_0">T L : {L, P = {P 0 , P 1 , ..., P K }, m, u, C, r}<label>(1)</label></formula><p>1) Update: We chose a constant velocity model to describe the propagation of the particles P for each tracker where the process noise for the position and velocity are drawn from a zero-mean normal distribution N (0, σ 2</p><p>x,v κ ), where κ is the number of successfully associated frames up to a threshold. This allows the tracker to close in to the object movement behaviour. Additionally, each particle carries a weight w to indicate the likelihood of the particle correctly tracking the object. To account for rough camera movement and the associated error in velocity estimation, particle prediction is driven by a homography obtained from frame to frame optical flow tracking.</p><p>2) Association: Detections, as well as ROIs retrieved from the detectors in Section II-B and the background subtraction in Section II-A are merged into tracker guidance candidates C i containing a position m, a rectangle r and a weight obtained by the detector.</p><formula xml:id="formula_1">C i : {m, r, weight}<label>(2)</label></formula><p>We divide large foreground regions containing smaller detections into two separate candidates and replace similarly sized regions with their corresponding overlaying detections. Then, a matching matrix can be constructed with matching scores S iL for each guidance candidate C i to every current tracker. The score of a C i is calculated by taking into account the probability distributions on the difference in rectangle sizes p(∆r iL ), the distance to the last associated guidance p(∆m iL ), the distances to the tracker particles p(∆d iLk ), a score indicating how well the candidate position is in accordance to the tracker velocity g(C i (m), T L (m, u)), the result of ViBe and a weighting factor on detections det , according to:</p><formula xml:id="formula_2">S iL = p(∆r iL ) • p(∆m iL ) • g(C i (m), T L (m, u)) • det • K k∈T L (P) p(∆d iLk ) ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">∆r iL = C i (r) -T L (r) , p(∆r iL ) ∼ N (0, σ 2 r ) ∆m iL = C i (m) -T L (C(m)) , p(∆m iL ) ∼ N (0, σ 2 m ) ∆d iLk = C i (m) -T L (P (x) k ) , p(∆d iLk ) ∼ N (0, σ 2 d ).<label>(4)</label></formula><p>It should be noted that ∆r iL is calculated by comparing the height and width of the detection and the past associated detections and ∆m iL by subtracting the distance from the position of the guidance candidate to the position of last successfully associated guidance of the tracker. The distance of the position x of the individual tracker particle P k to the position of the candidate is named ∆d iLk . The weighting factor det is implemented to favor detector to background subtraction output and is defined by:</p><formula xml:id="formula_4">det = if C i ∈ detections 1 if C i ∈ foreground<label>(5)</label></formula><p>As with all the thresholds we use in this work, the value of has been determined using nonlinear optimization on a seperate training dataset ("Sempach-11"), as described in Section III-B.</p><p>Inspired by <ref type="bibr" target="#b12">[13]</ref>, g(C i (m), T L (m, u)) takes into account the velocity of the tracker according to:</p><formula xml:id="formula_5">g(C i (m), T L (m, u)) = p(∆s iL ) dist if T L (u) &lt; τ q(C i (m), T L (m, u)) otherwise,<label>(6)</label></formula><p>where</p><formula xml:id="formula_6">∆s iL = C i (m) -T L (m) , p(∆s iL ) ∼ N (0, σ 2 s ) ,</formula><p>such that ∆s iL represents the distance between the guidance candidate and the position of the tracker and τ is a threshold on the tracker speed. Function q ∼ N (0, σ 2 q ) assigns a weight based on a zero-mean Gaussian distribution by calculating the distance of the candidate location C i (m) to the line defined by the tracker location T L (m) and the velocity vector T L (u), divided by ∆m iL (Eq. ( <ref type="formula" target="#formula_3">4</ref>)) and weighted by the norm of the difference of the normalized association vector to the candidate location and the normalized velocity vector. Only trackers that have a speed more than τ benefit from this function to suppress noise.</p><p>Having obtained the matching score matrix, the particle filter associates highest matching guidance candidates to the corresponding trackers, forming a new T L (C).</p><p>3) Particle Weighting: The next step in particle filtering involves the calculation of the individual particle weights w P k for each tracker T L :</p><formula xml:id="formula_7">w P k = p( (P k (x)) -C(m) ) • ϕ(P k (x)) + ρ,<label>(7)</label></formula><p>with ρ ∼ N (0, σ 2 ρ ) to prevent particle starvation due to similar w, the distance of the particle to the guidance C (zero if none associated) taking values in p( (P k (x))-C(m)) ) ∼ N (0, σ 2 w ) and the weighting factor ϕ defined as:</p><formula xml:id="formula_8">ϕ(P k (x)) = θ &gt; 1 if x i ∈ foreground 1 otherwise,<label>(8)</label></formula><p>to increase the weight if the particle is located on foreground.</p><p>4) Resampling: Finally, a random number drawn from the uniform distribution from zero to the sum of all the particle weights is taken. New particles are then selected by summing up their associated weights until their sum is higher or equal to the random number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Initialization:</head><p>A new cloud of normally distributed particles around the detection center is initialized for a new tracker if an unassociated candidate detection is above a threshold, fully inside the frame and positively classified by the detector in two consecutive frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6) Deletion:</head><p>To avoid ghost trackers with no associated detections or ROIs, each tracker is examined at every iteration to reside within the frame and have an association with a ROI not further further back than a fixed number of frames n ROI &lt; n detection and a fixed number of iterations n detection . Association vectors ν L between tracker T L and the corresponding detection or ROI have to be steady in both direction and length, since heavily fluctuating associations are most likely caused by misguided trackers. This is decided by setting a threshold γ, which needs to be larger than the averaged dot product ψ(ν L ) of the association vectors divided by the average vector length defined as bias(ν L ). Namely, γ is defined as:</p><formula xml:id="formula_9">γ &gt; ψ(ν L ) bias(ν L )<label>(9)</label></formula><formula xml:id="formula_10">ψ(ν L ) = 1 2(N -1) 2 N j∈V L N k∈V L ν L,j • ν L,k if j &lt; k 0 otherwise, (<label>10</label></formula><formula xml:id="formula_11">) where V L = {ν t L , ν t-1 L , ..., ν t-N L</formula><p>} are the past N association vectors of tracker T L , and bias(ν L ) = N j∈V L ν L,j N . If one of these rules applies, the corresponding tracker will be deleted. Since texture information in thermal wavelength images is sparse and therefore inter-object distinction is challenging even for the human eye, we do not store information about deleted trackers for future re-initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>To the best of our knowledge, the only publicly available high quality dataset containing thermal images is OTCBVS <ref type="bibr" target="#b9">[10]</ref>. However, the images are highly non-uniformly sampled in time and thus, not suited for tracking applications.</p><p>We introduce a new dataset<ref type="foot" target="#foot_1">1</ref> including 4381 manually annotated images containing humans and animals (e.g., cat, horse), as well as background scenery. The dataset is composed of 9 outdoor sequences captured at a uniform sampling rate (20 Hz) from different viewing angles and at varying temperatures. For the recordings, a FLIR Tau 320 thermal camera was used (visible in Fig. <ref type="figure" target="#fig_4">4(f)</ref>) with a 324×256 resolution, which was handheld on an elevated platform (roughly between 10-30m above ground) to replicate the topdown viewpoints from a flying UAV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Methodology</head><p>For our evaluation, we use the three sequences 'ETHZ-CLA", "Sempach-7"' and "Sempach-10" with a total of 1282 frames.</p><p>In order to train the detectors, we used a training set containing 5578 true instances of humans with different body and outdoor temperatures, background and poses taken from the "Sempach-8", "Sempach-12" and other images we recorded at different weather conditions. The whole training set is provided online. With A anno denoting the area of the annotation (groundtruth) rectangle surrounding a human in the scene, and A det the area of the detection, a detection is classified as true positive when the following holds:</p><formula xml:id="formula_12">A anno ∩ A det min{A anno , A det } ≥ 0.5.<label>(11)</label></formula><p>Likewise, an annotation is regarded as successfully tracked if the position of the tracker x mean , defined by the mean of all the tracker particles T L (m) satisfies x mean ∈ A anno .</p><p>The various particle filter parameters h described in Section II-C are trained on the "Sempach-11" sequence. To achieve high recall rates at reasonable precisions, the desirable performance metric of rescue personal for people search scenarios, we formulate the optimization criteria, such that h max = max{recall(h) + 0.2 • precision(h)}. Finally, we produce Recall vs. 1-Precision plots by averaging the results over five passes of a sequence to limit the influence of random particle initialization and propagation addition in the particle filter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS</head><p>To decouple the detector evaluation from the performance of the tracker framework, we first look at the performance of the detectors trained on different training sets. Fig. <ref type="figure" target="#fig_3">5</ref> illustrates the relative performance of the different detectors on the "Sempach-7" sequence. Both HOG <ref type="bibr" target="#b4">[5]</ref> and LatentSVM <ref type="bibr" target="#b5">[6]</ref> perform significantly better when trained on the INRIA dataset <ref type="bibr" target="#b4">[5]</ref> rather than on our thermal training set. This can be explained by the fact that the INRIA dataset only contains pictures of upright, standing people and with the typical people's height exceeding 100 px, it allows very clear training examples on the human figure. In contrast, our training set is far more complex due to the viewpoints resembling those of a UAV, containing images of people from highly oblique angles with people-heights as low as 12 px. The training stage of the detectors requires images of the same size, which necessitates up-and down-sampling for examples from our dataset. The segmentation of humans from background therefore becomes less shared and hence the magnitude of local gradients is reduced, deteriorating the discriminanility of the SVM decision boundary. Similar effects are caused by different viewpoint angles, where boundaries between human and background occur at variable pixel locations depending on which side of the camera they appear. However, the authors of <ref type="bibr" target="#b16">[17]</ref> suggest that training with low resolution thermal images can still result in a robust classifier, which leads to the conclusion that the viewpoint angle differences are more substantial to the performance decrease.</p><p>As a result, HOG and LatentSVM are trained best on samples as they appear in the INRIA dataset, while experiencing significant decline in performance when humans are not seen from a frontal view. Even from a UAV's perspective, humans can often be captured in a nearly frontal view, e.g. when observed far away from a front-looking camera or if the ground surface is significantly inclined (e.g. in alpine rescue scenarios). Hence, detectors trained on frontal views remain a valid choice.</p><p>Since the LatentSVM detector is built from HOG detections of different body parts and therefore the resulting windows smaller than the whole body, the result is an even stronger blur on the boundaries between body and background and thus, worse performance (See Fig. <ref type="figure" target="#fig_3">5</ref>). As in the test sequence humans are observed from various distances, the detectors experience degradation in performance with larger distance of humans from the camera. If the humans are close to the camera, the LatentSVM outperforms HOG, because the necessity of appearance similarity with the training images is reduced. However, with smaller sizes, the human form as a whole is simpler to detect than the individual body parts, which explains HOG overtaking LatentSVM in terms of recall at lower precisions, hence exposing the problem of interpolating smaller images on the detection performance of LatentSVM.</p><p>Fig. <ref type="figure" target="#fig_3">5</ref> shows that the standard LBP cascade <ref type="bibr" target="#b14">[15]</ref> cannot compete with the INRIA-trained HOG, LatentSVM or our BPD, reaching at most half the recall rates at any given precision. Consequently, we focus on the integration of the better performing INRIA-trained detectors in our particle filter framework, dropping LBP cascade from further analysis.</p><p>Since processing time analysis in Table <ref type="table" target="#tab_0">I</ref> shows that  LatentSVM is far from able to process video frames in real-time, our further analysis focuses on the two remaining detectors HOG and BPD. The full processing pipeline, e.g. combining the BPD with our tracker, runs in real-time requiring about 65 ms per image.</p><p>The "Sempach-7" sequence features humans both far away and close to the camera (See Fig. <ref type="figure" target="#fig_5">6</ref>). At higher precisions, the HOG-Tracker shows far superior performance than the other methods, roughly doubling the recall of the best detector (See Fig. <ref type="figure" target="#fig_6">7</ref>). Furthermore, the BPD-Tracker can increase precision rates by a factor of two compared to the standalone BPD. The background subtraction implemented in the tracking framework filters out many potential false positives, thus increasing precision. The sequence "Sempach-7" highlights a set of challenges for our framework: with low temperature difference between humans and their environment, the background subtraction algorithm does not segment the humans, preventing the detector to label the region as a human and thereby lowering the recall value. This can be observed at lower precisions, where the detectors eventually classify barely visible humans such as the one shown in Fig. <ref type="figure" target="#fig_5">6</ref>(a) as positive, while the HOG-Tracker does not. In sequence "ETHZ-CLA", people are clearly distinguishable from the background (a typical image shown in Fig. <ref type="figure" target="#fig_4">4(a)</ref>), therefore the tracker can exploit the full potential of the background subtraction method (Fig. <ref type="figure" target="#fig_4">4(b)</ref>) as a guidance for the particles. As expected and evident from Fig. <ref type="figure">8</ref>, the HOG-Tracker significantly outperforms the standalone detector in terms of recall by at least a factor of seven at higher precisions. The BPD-Tracker does not reach the performance of the HOG-Tracker, but still obtains recall rates at least twice as good as the best detector. None of the detectors reach the performance they achieve on "Sempach-7" illustrated in Fig. <ref type="figure" target="#fig_6">7</ref> since people in this sequence are further away from the camera, hence the body shapes become less detailed in the images, which has the greatest impact on the LatentSVM detector.</p><p>In sequence "Sempach-10" pictured in Fig. <ref type="figure">9</ref>, constant disappearance and reappearance of humans due to the heavy swifts of the camera require regular tracker initialization and deletion and the sequence is anyway challenging in terms of particle location stabilization, thus lowering the tracker performance. Here, the humans appear larger and LatentSVM can profit from detecting single body parts, thereby obtaining better recall values than the other detectors. Otherwise, HOG and BPD appear to perform roughly similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this paper, we propose a tracking algorithm based on a particle filter combined with background subtraction for people tracking in thermal-infrared images. We show that the tracker is capable of locating people in different scenarios from viewpoints such as the ones experienced by aerial platforms used in search and rescue. Using background segmentation to both reduce the detection space and to serve as guidance, the tracker substantially increases people locating accuracy and detection speed. We present a comprehensive study of existing algorithms as they are applied on our dataset. This dataset, containing thermal image sequences of urban scenery observing humans and animals from oblique, top-down viewpoints complete with annotations, is made publicly available. Future work will focus on employing a higher resolution thermal camera to aid people detection, which remains the bottleneck in tracking algorithms. In situations where the contrast between people and background is low, true detections can be disregarded by the background subtraction. Consequently a combination with additional sensing modalities (e.g. visible-light cameras) would be necessary to disambiguate in these cases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Intermediate results within our framework, operating on both frontal (top row) and oblique (bottom row) views of humans. The raw image (in (a) and (d)) is background-segmented (in (b) and (e)), followed by the application of the proposed Body Part Detector (BPD) and the particle filter tracker (in (c) and (f), respectively). Black: background, white: foreground, brown: foreground blob, blue: tracker, green: particles, red: detection.</figDesc><graphic coords="1,313.20,157.68,244.81,145.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Example of pairs of raw images with the obtained detections using the Body Part Detector; in (a) is a human in oblique view, in (b) the human occupies a small image region, and in (c) is a true negative detection. Colorcoding of rectangles: red -head, green -upper body, blue -legs, pink -whole human.</figDesc><graphic coords="2,313.20,50.08,244.80,52.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Examples of training images for the different body parts in the BPD, enlarged for illustration without interpolation. In (a) are training images for heads, (b) for upper bodies, and (c) for legs.</figDesc><graphic coords="2,313.20,156.86,244.80,102.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Sequence: "Sempach-7". Comparison INRIA and our thermal training sets. BPD, INRIA trained HOG and LatentSVM perform best (best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Fig.4: The raw input image from "ETHZ-CLA" shown in (a) is fed through ViBe background segmentation to obtain (b), while (c) depicts the results of the tracker (in brown: ROI rectangles, red: detections, green: particles of the tracker, white: velocity vector). In (d) is a typical image taken from "Sempach-7", while (e) is taken from "Sempach-10". Finally, in (f) is the TAU FLIR 320 thermal camera used in our setup. The body temperature of the people w.r.t. the environment is higher in (a), while being lower in (d) and (e) with an exception of a single person in (d) in the top right corner.</figDesc><graphic coords="6,54.00,50.08,504.01,226.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: From left to right: (a) human barely visible in raw image, (b) ViBe background segmentation on (a), (c) raw image of human with lower body temperature, and (c) raw image of human with higher body temperature. All snippets are taken from a single frame inside the "Sempach-7" sequence.</figDesc><graphic coords="6,54.00,324.42,244.80,91.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Sequence: "Sempach-7". Comparison of detection and tracking methods. Our HOG-Tracker outperforms the detectors at high precisions by a factor of two. Meandering can appear since the pipeline contains stochastic elements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :Fig. 9 :</head><label>89</label><figDesc>Fig. 8: Sequence: "ETHZ-CLA". Comparison of detection and tracking methods. Both trackers heavily outperform the detectors by a factor of three respectively seven, exploiting the background subtraction algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Comparison of detection time per frame integrated in our particle filter on an Intel i5 @ 3.3 GHz without substantial code optimization on sequence "ETHZ-CLA". The whole tracker needs the particle filter framework and a choice of any detector.</figDesc><table><row><cell>Detector</cell><cell>Time per Frame (sec)</cell></row><row><cell>HOG</cell><cell>0.1054</cell></row><row><cell>BPD</cell><cell>0.0384</cell></row><row><cell>LatentSVM</cell><cell>9.6057</cell></row><row><cell>LBP</cell><cell>0.074</cell></row><row><cell>Our Particle Filter Framework</cell><cell>Time per Frame (sec)</cell></row><row><cell>ViBe</cell><cell>0.0091</cell></row><row><cell>Particle Filter</cell><cell>0.0126</cell></row><row><cell>Rest (resizing, blobbing, etc.)</cell><cell>0.0023</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2014" xml:id="foot_0"><p>IEEE International Conference on Robotics &amp; Automation (ICRA) Hong Kong Convention and Exhibition Center May 31 -June 7, 2014. Hong Kong, China 978-1-4799-3685-4/14/$31.00 ©2014 IEEE</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>Our publicly available dataset can be accessed on http://projects.asl.ethz.ch/datasets/doku.php?id= ir:iricra2014.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ACKNOWLEDGMENTS</head><p>The authors are grateful to the help of Michael Burri, Alexandra and Bruno Portmann and Carmen Schwizer for capturing datasets.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Monocular pedestrian detection: Survey and experiments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">People-tracking-by-detection and people-detection-by-tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision &amp; Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part based models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Real-time people and vehicle detection from UAV imagery</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gaszczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SPIE Conference Intelligent Robots and Computer Vision XXVIII: Algorithms and Techniques</title>
		<meeting>the SPIE Conference Intelligent Robots and Computer Vision XXVIII: Algorithms and Techniques</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A multiple detector approach to low-resolution fir pedestrian recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mählisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oberlander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Lohlein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gavrila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Intelligent Vehicles Symposium (IV)</title>
		<meeting>the Intelligent Vehicles Symposium (IV)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Local feature based person detection and tracking beyond the visible spectrum</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jüngling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Vision Beyond Visible Spectrum</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="978" to="981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A two-stage template approach to person detection in thermal imagery</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Keck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improved human detection and classification in thermal images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing (ICIP)</title>
		<meeting>the IEEE International Conference on Image Processing (ICIP)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ViBe: a powerful random technique to estimate the background in video sequences</title>
		<author>
			<persName><forename type="first">O</forename><surname>Barnich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Droogenbroeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Online multiperson tracking-by-detection from a single, uncalibrated camera</title>
		<author>
			<persName><forename type="first">M</forename><surname>Breitenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Reichlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Koller-Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Discriminatively trained deformable part models, release 4</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<ptr target="http://people.cs.uchicago.edu/∼pff/latent-release4/" />
		<imprint/>
	</monogr>
	<note>implemented as LatentSVM</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning multi-scale block local binary patterns for face recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Advances in Biometrics (ICB)</title>
		<meeting>the International Conference on Advances in Biometrics (ICB)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A decision-theoretic generalization of on-line learning and an application to boosting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pedestrian detection using infrared images and histograms of oriented gradients</title>
		<author>
			<persName><forename type="first">F</forename><surname>Suard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rakotomamonjy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bensrhair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Broggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Intelligent Vehicles Symposium (IV)</title>
		<meeting>the Intelligent Vehicles Symposium (IV)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
