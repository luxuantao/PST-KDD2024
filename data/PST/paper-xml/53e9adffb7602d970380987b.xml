<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Clustering using a random walk based distance measure</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Luh</forename><surname>Yen</surname></persName>
							<email>yen@isys.ucl.ac.be</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université catholique de Louvain</orgName>
								<orgName type="institution" key="instit2">ISYS</orgName>
								<address>
									<addrLine>IAG Place des Doyens 1</addrLine>
									<postCode>B-1348</postCode>
									<settlement>Louvain-la-Neuve</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Denis</forename><surname>Vanvyve</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fabien</forename><surname>Wouters</surname></persName>
						</author>
						<author>
							<persName><forename type="first">François</forename><surname>Fouss</surname></persName>
							<email>fouss@isys.ucl.ac.be</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université catholique de Louvain</orgName>
								<orgName type="institution" key="instit2">ISYS</orgName>
								<address>
									<addrLine>IAG Place des Doyens 1</addrLine>
									<postCode>B-1348</postCode>
									<settlement>Louvain-la-Neuve</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michel</forename><surname>Verleysen</surname></persName>
							<email>verleysen@dice.ucl.ac.be</email>
							<affiliation key="aff1">
								<orgName type="department">DICE</orgName>
								<orgName type="institution">Université catholique de Louvain</orgName>
								<address>
									<addrLine>FSA Place de Levant 3</addrLine>
									<postCode>B-1348</postCode>
									<settlement>Louvain-la-Neuve</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marco</forename><surname>Saerens</surname></persName>
							<email>saerens@isys.ucl.ac.be</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université catholique de Louvain</orgName>
								<orgName type="institution" key="instit2">ISYS</orgName>
								<address>
									<addrLine>IAG Place des Doyens 1</addrLine>
									<postCode>B-1348</postCode>
									<settlement>Louvain-la-Neuve</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Clustering using a random walk based distance measure</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">490F94DF24B2577C3BF3EAA4CBD31760</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work proposes a simple way to improve a clustering algorithm. The idea is to exploit a new distance metric called the "Euclidian Commute Time" (ECT) distance, based on a random walk model on a graph derived from the data. Using this distance measure instead of the usual Euclidean distance in a k-means algorithm allows to retrieve wellseparated clusters of arbitrary shape, without working hypothesis about their data distribution. Experimental results show that the use of this new distance measure significantly improves the quality of the clustering on the tested data sets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In clustering, the data distribution has an important impact on the classification results. However, in most clustering problems, there is few prior information available about the underlying statistical model, and the decision maker must make some arbitrary assumptions. For instance, the k-means algorithm, in its basic form, can fail on data sets containing clusters of arbitrary or even nonconvex shape, even if they are well-separated.</p><p>In this work, we propose the use of a new distance measure, the Euclidean Commute Time distance (ECT distance, see reference <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b11">[12]</ref>), in order to improve the clustering performance. The ECT distance is based on a random walk model on a graph derived from the data. More precisely, the ECT distance is a distance measure between the nodes of a weighted graph and presents the interesting property of decreasing when the number of paths connecting two nodes increases or when the "length" of any path decreases, which makes it well-suited for clustering tasks.</p><p>At first sight, the proposed method seems similar to the classical "shortest path" distance on a graph (also called Dijkstra or geodesic distance <ref type="bibr" target="#b1">[2]</ref>). Actually our distance metric differs about the fact that it takes the connectivity between nodes into account: Two nodes are "close" according to this distance if they are highly connected. Notice that the idea of exploiting random walks concept for clustering has already been proposed by Koren and Harel <ref type="bibr" target="#b6">[7]</ref>, by using the notion of escape probabilities to find separating edges of a graph. The difference between the two works is that our method is based on a distance measure and has a nice geometric interpretation in terms of a Mahalanobis distance (see Equation <ref type="formula" target="#formula_3">2</ref>).</p><p>The paper is organized as follows. An introduction to the ECT distance is provided in Section 2. Section 3 shows how the ECT distance can be computed from the Laplacian matrix of the graph derived from the data. Section 4 presents the clustering algorithm based on ECT distance. Section 5 provides experimental results on an artificial data set and on a digital characters clustering problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Distance measure based on a random walk model</head><p>The essential of the theory justifying the defined distance is developed in papers <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b11">[12]</ref>. Only a short overview is provided here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">A random walk model on a weighted graph</head><p>In a first step, the data (N observations in total) are linked to form a connected graph in the following way: Each observation is represented by a node of the graph and is connected to his k nearest neighbors, according to the Euclidean distance. In addition, the minimum spanning tree <ref type="bibr" target="#b2">[3]</ref> (minimizing the sum of the Euclidian distances) is computed and its edges are added to the graph in order to obtain a connected graph : each node can be reached from any other node of the graph through at least one path. Following the definition of this graph, we expect that two points in the same cohesive cluster are connected by a large number of short paths.</p><p>The weight w ij ≥ 0 of the edge connecting node i and node j is set to some meaningful value, representing the closeness of observations i and j. It is chosen here to be inversely proportional to the Euclidean distance between the two observations.</p><p>Based on the constructed graph it is possible to compute the associated adjacency matrix A in the standard way, with elements a ij = w ij if node i is connected to node j, and 0 otherwise.</p><p>Then we associate the state of a Markov chain to every node of the graph (N in total). To any state or node i, we associate a probability of jumping to an adjacent node (a nearest neighbor) :</p><formula xml:id="formula_0">p ij = a ij ai. , with a i. = N j=1 a ij .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The average commute time</head><p>Based on this Markov chain, two important quantities are defined : the average first-passage time and the average commute time.</p><p>The average first-passage time m(k|i) is defined as the average number of steps a random walker, starting in state i = k, will take to enter state k for the first time. Formally, m(k|i) is defined as (see for instance <ref type="bibr" target="#b9">[10]</ref>) :</p><formula xml:id="formula_1">       m(k|k) = 0 m(k|i) = 1 + N j=1 j =k p ij m(k|j), for i = k. (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>These equations can be used in order to iteratively compute the first-passage times.</p><p>The second quantity is the average commute time, n(i, j), which is defined as the average number of steps a random walker, starting in state i = j, will take before entering a given state j for the first time, and go back to i. That is, n(i, j) = m(j|i) + m(i|j). It was shown by several authors <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref> that the average commute time is a distance measure between any nodes of the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Computation of the basic quantities by means of L +</head><p>The Laplacian matrix of the graph is defined by L = D -A, where A is the adjacency matrix of the graph and D = diag(a i. ) (with a i. = N j=1 a ij ) is the degree matrix. It is shown in <ref type="bibr" target="#b10">[11]</ref> that the computation of the average commute time can be obtained from the Moore-Penrose pseudoinverse <ref type="bibr" target="#b0">[1]</ref> of L, denoted by</p><formula xml:id="formula_3">L + : n(i, j) = V G (e i -e j ) T L + (e i -e j ),<label>(2)</label></formula><p>where</p><formula xml:id="formula_4">e i = [0 1 , . . . , 0 i-1 , 1 i , 0 i+1 , . . . , 0 N</formula><p>] T is a basis vector and where</p><formula xml:id="formula_5">V G = i,j a ij</formula><p>is the volume of the graph. We easily observe from Equation 2 that [n(i, j)] 1/2 is a distance, since it can be shown <ref type="bibr" target="#b10">[11]</ref> that L + is symmetric and positive semidefinite. It is therefore called the Euclidean Commute Time (ECT) distance.</p><p>If the matrices are too large, the computation by pseudoinverse becomes cumbersome; in this case, it is still possible to compute the ECT distance iteratively using Equation 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">K-means based on ECT distances</head><p>Of course, any clustering algorithm (hierarchical clustering, k-means, etc) could be used in conjunction with the ECT distance. In this work, we illustrate its potential usefulness by using a k-means algorithm. To this end, we implemented a k-means method working directly on the distance matrix (see for instance <ref type="bibr" target="#b13">[14]</ref>).</p><p>Let us denote as {x k }, k = 1, ..., N , the set of observations to be clustered into c different clusters. We define the ECT distance matrix, ∆, where element [∆] ij = δ(x i , x j ) = n(i, j) is the squared ECT distance between observations x i and x j .</p><p>Each cluster C l , l = 1, ..., c, is represented by one prototype, p l , which is chosen among the observations (it is therefore not the centroid, as it is usually the case with the k-means algorithm). The distance between an observation x k and a cluster C l is defined as the distance to the prototype : dist</p><formula xml:id="formula_6">[x k , C l ] = δ(x k , p l )</formula><p>The within-cluster variance for cluster C l is defined by</p><formula xml:id="formula_7">J l = x k ∈C l dist 2 [x k , C l ].<label>(3)</label></formula><p>The optimization criterion J is simply the sum of the within-cluster variances J l of each cluster C l :</p><formula xml:id="formula_8">J = c l=1 J l = c l=1 x k ∈C l dist 2 [x k , C l ].<label>(4)</label></formula><p>Criterion J depends on two elements: the allocation of the observations to a cluster and the position of the prototypes. It is quite difficult in terms of computing time to find the best, global, minimum of J. Most of the algorithms only compute a local minimum of J; this is the case for our ECT distance kmeans algorithm, which iterates the two basics steps:</p><p>(1) Allocation of the observations. The prototypes are fixed. Each observation x k is allocated to its nearest cluster; that is, x k is assigned to cluster</p><formula xml:id="formula_9">C l such that l = arg min j dist 2 [x k , C j ] = arg min j δ 2 (x k , p j );<label>(5)</label></formula><p>(2) Computation of the prototypes. We now consider that the allocation of the observations is fixed (each x k is assigned to a cluster). For each cluster C l , we choose a new prototype, p l , among the observations so that it minimize the within-cluster variance (3) of this cluster. More precisely, the prototype of each cluster C l is chosen according to:</p><formula xml:id="formula_10">p l = arg min x j x k ∈C l δ 2 (x k , x j ) . (<label>6</label></formula><formula xml:id="formula_11">)</formula><p>The clustering algorithm aims to repeat steps (1) and (2) until convergence of J to a local minimum. It can be shown that J decreases at each such step <ref type="bibr" target="#b13">[14]</ref>. This clustering procedure based on the ECT distance will be called the ECT distance k-means.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In order to evaluate the ECT distance k-means algorithm, it is applied to two clustering problems, and compared to the classical k-means based on the Euclidean distance. Five artificial data sets (inspired by <ref type="bibr" target="#b8">[9]</ref>) are used to illustrate the ability to detect clusters with arbitrary shapes. We also compare our method to the normalized cuts <ref type="bibr" target="#b12">[13]</ref>, since we established in <ref type="bibr" target="#b11">[12]</ref> several similarities between the normalized cuts and the ECT distance. The second experiment aims to cluster digital characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ESANN'2005 proceedings -European Symposium on Artificial Neural Networks</head><p>Bruges (Belgium), 27-29 April 2005, d-side publi., ISBN 2-930307-05-6. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiments on artificial data sets</head><p>Figure <ref type="figure" target="#fig_2">1a</ref> shows an example of graph construction. We made the arbitrary choice for every experiments of this paper to link each observation (node) of the data set to its three nearest neighbors, in addition to the links provided by the computation of the minimum spanning tree. Actually we observed that three neighbors are enough to get satisfactory results, in addition to reduce the computation complexity.</p><p>For illustration, the multidimensional scaling projection of the ECT distance matrix on the two first principal axis is shown in Figure <ref type="figure" target="#fig_2">1b</ref>. We observe that the two clusters are well separated with the ECT distance metric.</p><p>The resulting partition obtained by using the ECT distance and the Euclidean distance are shown respectively in Figure <ref type="figure" target="#fig_2">1c</ref> and Figure <ref type="figure" target="#fig_2">1d</ref>. Both clustering algorithms are run twenty times with two prototypes (two clusters) and various random seeds; only the clustering with the minimal total within-class variance J is retained.</p><p>The same experiment is realized with four other artificial data sets (Figures <ref type="figure" target="#fig_2">1e, 1f, 1g</ref> and<ref type="figure" target="#fig_2">1h</ref>). Figure <ref type="figure" target="#fig_2">1i</ref> shows an example of the clustering result obtained by using Shi and Malik's spectral clustering algorithm <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Digital characters clustering</head><p>The second experiment concerns a digital character clustering problem where the word "DENIS" is digitalized; the objective here is to retrieve the letters from the two-dimensional image.</p><p>Three data sets are constructed from the digitalized "DENIS", with various letter interspaces (see Figure <ref type="figure" target="#fig_3">2a</ref>). An example of clustering on medium interspace set, obtained by ECT distance k-means, is shown in Figure <ref type="figure" target="#fig_3">2b</ref>.</p><p>For each of the three data sets the ECT distance k-means and the classical kmeans are respectively repeated twenty times. For each of the twenty clusterings, the quality of the obtained partition is assessed by comparing it to the optimal partition where each letter is a cluster (in this case, there are five clusters: the five letters of "DENIS"). Therefore, the adjusted rand index is computed, measuring the quality of the clustering (see for instance <ref type="bibr" target="#b4">[5]</ref>). Then the adjusted rand indexes obtained by the twenty clusterings are averaged, in order to obtain the averaged adjusted rand index.</p><p>Figure <ref type="figure" target="#fig_3">2c</ref> shows the values of the averaged adjusted rand index for the three "DENIS" data sets and the two k-means procedures, based on ECT and Euclidean distances. The first data set (label 1 in Figure <ref type="figure" target="#fig_3">2a</ref>) contains small letter interspaces; the second data set (label 2) contains medium letter interspaces, and the third data set (label 3) contains large letter interspaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Discussion of the results</head><p>We observe that the algorithm based on the ECT distances provides good clustering results, both for the artificial data and the character clustering problems. The classical k-means usually fails to cluster properly when the separation border between clusters is not trivial. On the contrary, the ECT distance k-means algorithm overcomes the difficulty and manages to separate the different clusters for the non-linearly separable, but nevertheless well separated, data sets. The visualization of the ECT distance matrix projected in a two-dimensional space by multidimensional scaling (Figure <ref type="figure" target="#fig_2">1b</ref>) shows a interesting characteristic of the ECT distance metric : observations with strong internal cohesion move closer to their nearest neighbors. On the contrary, observations with few connections between them tend to be drawn aside. But what happens if the subgroups are really close ? In this case, many connections can be built between close observations of different groups and can alter the performances. Indeed, as expected, the clustering performances decrease in the second experiment when the interspaces between letters get smaller (Figure <ref type="figure" target="#fig_3">2c</ref>). Actually, this experiment illustrates one advantage of using the ECT distance compared to Euclidean distance: two points, which are close in the Euclidian space, can nevertheless have a large ECT distance if there are few paths connecting them. On the other hand, two points that are distant in the Euclidean space can nevertheless be close in terms of ECT distance if there are many paths connecting them.</p><p>Notice that the application of the normalized cuts proposed by Shi and Malik on our data sets gives slightly worse results when clusters are close (e.g., Figure <ref type="figure" target="#fig_2">1i</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and further work</head><p>We introduced a new distance measure, called the Euclidean commute time distance, which allows to retrieve well-separated clusters of arbitrary shapes. Experiments show that the ECT distance k-means is less sensitive to the shape of the cluster than the standard k-means based on the Euclidean distance. It is also interesting to notice that the ECT distance k-means is easy to use since there is no need to make assumption on the data distribution nor to fix some parameter values.</p><p>The main drawback of this method is that it does not scale well for large data sets. The distance matrix size is determined by the number of data and its estimation can be time consuming. However, the Laplacian matrix is usually sparse: only the information about links between nearest neighbors is kept.</p><p>Further work will extend the application of the ECT distance k-means to more sophisticated clustering problems. We will also continue our comparisons and investigations of the links between ECT distance k-means and spectral clustering (see <ref type="bibr" target="#b11">[12]</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>ESANN'2005 proceedings -European Symposium on Artificial Neural Networks Bruges (Belgium), 27-29 April 2005, d-side publi., ISBN 2-930307-05-6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>ESANN'2005 proceedings -European Symposium on Artificial Neural Networks Bruges (Belgium), 27-29 April 2005, d-side publi., ISBN 2-930307-05-6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Clustering using ECT distance k-means. (a) Rings data set and its associated connected graph. (b) The multidimensional scaling projection of the ECT distance matrix on the two first principal axis. (c) Clustering results using the ECT distance k-means. Clusters are indicated by different symbols and prototypes by stars. (d) Clustering results using the Euclidean distance k-means. (e) -(h) Other clustering examples using ECT distance k-means on artificial data sets. (i) Clustering results using Shi and Malik's algorithm.</figDesc><graphic coords="5,250.18,462.69,113.30,88.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Digital characters clustering. (a) Three "DENIS" sets with various interspace between letters. (b) Clustering results using the ECT distance kmeans for medium interspace. (c) Comparisons of the averaged adjusted rand index for the three "DENIS" sets and the two clustering methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>ESANN'2005 proceedings -European Symposium on Artificial Neural Networks Bruges (Belgium), 27-29 April 2005, d-side publi., ISBN 2-930307-05-6.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Matrices: Methods and Applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Barnett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Distance in graphs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Harary</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Addison-Wesley Publishing Company</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Cormen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Rivest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
		<title level="m">Introduction to Algorithms</title>
		<imprint>
			<date type="published" when="2001-09">September 2001</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Random Walks and Electric Networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Snell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>The Mathematical Association of America</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Everitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Landau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Leese</surname></persName>
		</author>
		<title level="m">Cluster Analysis</title>
		<meeting><address><addrLine>Edward Arnold, London</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Random walks on graphs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gobel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jagers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stochastic Processes and their Applications</title>
		<imprint>
			<date type="published" when="1974">1974</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="311" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On clustering using random walks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">2245</biblScope>
			<biblScope unit="page" from="18" to="41" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Resistance distance</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Randic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Chemistry</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="81" to="95" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Markov Chains</title>
		<author>
			<persName><forename type="first">J</forename><surname>Norris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Computing similarities between nodes of a graph: Application to collaborative filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Saerens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fouss</surname></persName>
		</author>
		<ptr target="http://www.isys.ucl.ac.be/staff/marco/Publications/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The principal components analysis of a graph, and its relationships to spectral clustering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Saerens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fouss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dupont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 15th European Conference on Machine Learning (ECML)</title>
		<title level="s">Lecture Notes in Artificial Intelligence</title>
		<meeting>eeding of the 15th European Conference on Machine Learning (ECML)<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="volume">3201</biblScope>
			<biblScope unit="page" from="371" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Normalised cuts and image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Matching and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000-08">August 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Cluster analysis algorithms for data reduction and classification of objects</title>
		<author>
			<persName><forename type="first">H</forename><surname>Spath</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
			<publisher>Ellis Horwood</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m">ESANN&apos;2005 proceedings -European Symposium on Artificial Neural Networks Bruges (Belgium)</title>
		<imprint>
			<date type="published" when="2005-04">April 2005</date>
			<biblScope unit="page" from="27" to="29" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
