<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowledge Transfer for Out-of-Knowledge-Base Entities: Improving Graph-Neural-Network-Based Embedding Using Convolutional Layers</title>
				<funder ref="#_4P3YTpg #_dMbgwnN">
					<orgName type="full">National Nature Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhongqin</forename><surname>Bi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Shanghai University of Electric Power</orgName>
								<address>
									<postCode>200090</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Tianchen</forename><surname>Zhang</surname></persName>
							<email>zhangtianchen@gmail.com</email>
							<idno type="ORCID">0000-0002-5044-6065</idno>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Shanghai University of Electric Power</orgName>
								<address>
									<postCode>200090</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Shanghai University of Electric Power</orgName>
								<address>
									<postCode>200090</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>AND</roleName><forename type="first">Ping</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Shanghai University of Electric Power</orgName>
								<address>
									<postCode>200090</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yongbin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Shanghai University of Electric Power</orgName>
								<address>
									<postCode>200090</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Knowledge Transfer for Out-of-Knowledge-Base Entities: Improving Graph-Neural-Network-Based Embedding Using Convolutional Layers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/ACCESS.2020.3019592</idno>
					<note type="submission">Received July 11, 2020, accepted August 24, 2020, date of publication August 26, 2020, date of current version September 11, 2020.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge base completion (KBC) aims to predict missing information in a knowledge base. Most existing embedding-based KBC models assume that all test entities are available at training time. Thus, a question arises-that is, how to answer queries concerning test entities not observed at training time, which is called the out-of-knowledge-base (OOKB) entity problem. In this article, we propose a parameter-efficient embedding model that combines the benefits of a graph neural network (GNN) and a convolutional neural network (CNN) to solve the KBC task with OOKB entities. First, we apply the GNN architecture to learn the information between nodes in the graph. Second, convolution layers are used as a transition matrix in GNN to learn more expressive embeddings with fewer parameters. Finally, we use a transition-based knowledge graph embedding model to solve the KBC task. The model has learnable weights that adapt based on information from neighbors and can exploit auxiliary knowledge for OOKB entities to compute their embedding while remaining parameter efficient. We demonstrate the effectiveness of the proposed model on OOKB datasets, and the code is available at https://github.com/Tianchen627/Knowledge-Transfer-for-Outof-Knowledge-Base-Entities.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Knowledge bases, such as WordNet <ref type="bibr" target="#b0">[1]</ref> and Freebase <ref type="bibr" target="#b1">[2]</ref>, which store complex structured and unstructured information, have important applications in semantic search <ref type="bibr" target="#b2">[3]</ref>, dialog generation <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> and question answering <ref type="bibr" target="#b5">[6]</ref>. These knowledge bases can be viewed as a set of relation triplets, i.e., triplets of the form (h, r, t) with an entity h called the head entity, a relation r, and an entity t called the tail entity <ref type="bibr" target="#b6">[7]</ref>. Some examples of these triplets are (Forrest -Gump, hasdirector, Robert -Zemeckis) and (Forrest -Gump, isa, film). However, these knowledge bases suffer from incompleteness <ref type="bibr" target="#b7">[8]</ref>, which means some facts are missing. This problem gives rise to the task of knowledge base completion (KBC), which entails predicting missing facts and whether a given triplet is valid.</p><p>Starting with TransE <ref type="bibr" target="#b6">[7]</ref>, embedding-based KBC models have been successfully applied to large-scale knowledge</p><p>The associate editor coordinating the review of this manuscript and approving it for publication was Ananya Sen Gupta .</p><p>bases. Since then, some works <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b11">[12]</ref> have focused on the extension of TransE, whereas others consider semantic matching methods, such as RESCAL <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> and its extensions <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b17">[18]</ref>. Some works also make use of additional information to improve task performance, e.g., entity types <ref type="bibr" target="#b18">[19]</ref>, relation paths <ref type="bibr" target="#b19">[20]</ref>, textual descriptions <ref type="bibr" target="#b20">[21]</ref>, as well as logical rules <ref type="bibr" target="#b21">[22]</ref>. All these models build distributed representations of entities and relations observed in the training data and use various vector operations over the embeddings to predict triplets. Although there are some different solutions, such as the collaborative filtering framework <ref type="bibr" target="#b22">[23]</ref>, embedding-based models have received the most attention because this method stands out in various KBC tasks. Moreover,increasingly more new technology in deep learning and representation learning can be applied to the KBC embedding-based model.</p><p>The state-of-the-art KBC methods are primarily embedding-based models. These popular methods can be broadly classified as graph neural network (GNN)based models <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b25">[26]</ref> and convolutional neural network  <ref type="bibr" target="#b1">(2)</ref>. Case <ref type="bibr" target="#b2">(3)</ref> is the KBC task with out-of-knowledge-base (OOKB) entities. (1): During training, triplets that represent facts in the knowledge base are fed to the model, for example, (Forrest -Gump, hasdirector , Robert -Zemeckis), which contains the head entity ''Forrest-Gump'', the relation ''has-director'' and the tail entity ''Robert-Zemeckis''. In this stage, the model performs knowledge base embedding by building distributed representations of entities and relations observed in the training data. (2): During testing, the model applies various vector operations over the embeddings to predict missing relation triplets. Suppose the fact ''Forrest-Gump is a film'', which can be depicted as the triplet (Forrest -Gump, isa, film), is missing from the current knowledge base. The model can find this missing fact through vector operations. For example, if we want to answer the question ''What is Forrest Gump?''. We can obtain the answer ''film'' by following the green dashed arrow in the figure . (3): OOKB entity problem. The difference between the traditional KBC task and the OOKB KBC task is that the OOKB test triplet contains an entity that is not observed in the training step, which means the model does not have access to its vector embedding and vector operations cannot work. In the traditional KBC task, all the entities are observed in the knowledge base (shown as the shaded box). In this case, we use the new triplet (To -Walktheclouds, hasdirector , Robert -Zemeckis), which is called auxiliary knowledge in the OOKB situation, to obtain the embedding of the new entity. Thus, we can answer the question ''What is to walk the clouds?'' by predicting the triplet (To -Walktheclouds, isa, film).</p><p>(CNN)-based models <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. The former are known for the ability to incorporate the connectivity structure in the graph since the knowledge base can be seen as a graph, and the latter are known for their highly parameter efficiency.</p><p>The out-of-knowledge-base (OOKB) entity problem in the KBC task was first introduced in <ref type="bibr" target="#b28">[29]</ref>. Before that, there were many studies focusing on OOKB in different knowledge base applications and tasks <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b32">[33]</ref>. The OOKB entity problem arises when new entities (OOKB entities) occur in the triplets that are given to the system after training. Although we can retrain the model with the new triplets containing the OOKB entities, a method to avoid costly retraining is desirable. Fig. <ref type="bibr" target="#b0">(1)</ref> illustrates the KBC task with OOKB entities schematically, and a more detailed definition can be found in Section III.</p><p>As these entities were unknown to the system at training time, the system does not have their embeddings and, hence, does not have a means to predict relations for these entities. Therefore, traditional KBC methods cannot be applied directly to the OOKB KBC task. A GNN method was used to solve the OOKB entity problem in <ref type="bibr" target="#b28">[29]</ref>. The method embodies knowledge transfer,which is a process included in knowledge management domain. However, GNN models suffer from potentially prohibitive memory requirements <ref type="bibr" target="#b23">[24]</ref> and generally do not outperform CNN-based models <ref type="bibr" target="#b27">[28]</ref>.</p><p>The OOKB problem is of practical importance because OOKB entities can occur whenever new entities, such as events and products, are produced, which happens everyday, and in domain-specific knowledge base where the range of entities is limited. Moreover, we often want to infer more facts (triplets) from the knowledge (triplets) we already have.</p><p>In this article, we propose a parameter-efficient embedding model that combines the benefits of GNN and CNN by replacing the transition weight matrix in GNN, which represents the relations, with a multilayer convolutional network. The model has learnable weights that adapt to the amount of information from neighbors and can exploit auxiliary knowledge for OOKB entities to compute their embeddings while remaining parameter efficient.</p><p>Our contributions are summarized as follows:</p><p>1. We propose an end-to-end model for the KBC task with OOKB entities. The model combines the advantages of the GNN structure and the CNN structure.</p><p>2. We develop a new method to transfer knowledge for OOKB entities. In contrast to using a vector or weight matrix to represent relation embeddings in the KBC model, we use a convolution kernel to learn expressive features from the auxiliary knowledge of OOKB entities.</p><p>3. We verify the effectiveness of our model in OOKB datasets. The model has good accuracy and parameter efficiency. Since the OOKB problem often occurs in realistic cases, our work is a successful attempt to combine the KBC task with practical scenarios.</p><p>The remainder of this article is organized as follows. We discuss related work in Section II and introduce the background of KBC and OOKB in Section III. We describe our approach in detail in Section IV. Then, we elaborate our experimental study in Section V and compared the results with the original GNN-for-KBC. The conclusion is presented in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The knowledge graph embedding model has been an active research topic for KBC since TransE <ref type="bibr" target="#b6">[7]</ref> addressed the task by projecting both entities and relations into the same embedding vector space with a translational constraint of h + r ? t. Later works introduced new representations of relational translations and thus improved the performance and increased the model complexity. Recent research incorporates GNN <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b25">[26]</ref> and CNN structures <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> into the model.</p><p>GNN-based models are famous for taking the graph structure into consideration. Since previous works consider each triplet independently without taking into account the relationships between triplets, GNN-based models aggregate local information in the graph neighborhood for each node. Graph convolutional networks (GCNs) have been an effective tool to create node embeddings and were first applied to the KBC task in <ref type="bibr" target="#b23">[24]</ref>. Relational GCN (R-GCN) <ref type="bibr" target="#b24">[25]</ref> is an extension that performs well for highly multirelational data. Furthermore, in <ref type="bibr" target="#b25">[26]</ref>, graph attention was applied to the GNN. However, these models have been criticized for their huge memory requirements and failure to outperform CNN-based models <ref type="bibr" target="#b26">[27]</ref>.</p><p>GCN approaches define convolutions directly on a graph and sum node features over all spatial neighbors using an adjacency matrix, whereas our method replaces the transition weight matrix in the GNN with a multilayer convolutional network. GCN suffers from prohibitive memory requirements, while our work maintains the parameter efficiency of CNN-based models.</p><p>CNN-based models use convolutional layers to learn embeddings because the performance in previous work was limited by feature interactions. ConvE <ref type="bibr" target="#b26">[27]</ref> was the first model to use 2D convolutions over embeddings of different embedding dimensions to extract more feature interactions. InteractE <ref type="bibr" target="#b27">[28]</ref> later extended ConvE through more feature interactions via feature permutation, checkered reshaping and circular convolution. These models have good parameter efficiency: ConvE achieves better scores than R-GCNs on FB15k-237 with 17? fewer parameters. The reason why CNN-based models have high parameter efficiency is that the convolutional layer has more feature interactions that can make the model learn more expressive features in low embedding size with fewer parameters. More details about CNN-based models and parameter efficiency can be found in Section IV-C. However, CNN-based models do not use the relationships between triplets: every triplet is treated independently.</p><p>The traditional KBC supposes that all the test entities are observed in the training data, so every entity's embedding is available. The situation changes when a new entity occurs in the test triplets, and the traditional KBC models cannot be applied in this scenario because they do not have the embeddings of these entities. This situation is called the OOKB entity problem. We focus on handling the KBC task with the OOKB entity problem in this article.</p><p>The most closely related work to ours is GNN-for-OOKB <ref type="bibr" target="#b28">[29]</ref>, which proposed a GNN model for the KBC task with OOKB entities. We improve the model by replacing the transition weight matrix in the GNN with a multilayer convolutional network to take advantage of the high computational efficiency of the convolutional network and adapt to information from neighbors with the GNN structure.</p><p>Another similar work <ref type="bibr" target="#b33">[34]</ref> proposed an end-to-end graph structure-aware convolutional network (SACN) that combines the benefits of GCN and ConvE. However, their work is not designed for the OOKB entity problem, and the SACN cannot handle OOKB entities. Moreover, they focus on link prediction, while we focus on triplet classification, which are both typical KBC tasks <ref type="bibr" target="#b34">[35]</ref>.</p><p>There are also some recent works focusing on the OOKB problem in the KBC task recently. Some researchers focus on different type of tasks like link prediction <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref> in few-shot learning and entity detection <ref type="bibr" target="#b37">[38]</ref> while we focus on triplet classification. Some researchers used the jointly embedding method <ref type="bibr" target="#b38">[39]</ref> and multimodal data enhanced representation <ref type="bibr" target="#b39">[40]</ref> to achieve OOKB entity embedding while in our work these external data are not considered. Reference <ref type="bibr" target="#b40">[41]</ref> used attention-based aggregation to solve the new OOKB relation problem. Their idea and method are exciting and we want to extend our work to the OOKB relation problem in the future. Many studies assumed a specific scenario while our work considers only the standard scenario of the OOKB entity problem in which all the knowledge we have is from the current knowledge base.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. BACKGROUND A. KNOWLEDGE GRAPH</head><p>A knowledge base G contains many facts that can be represented as a triplet (h, r, t) with head entity h, relation r, and tail entity t. A knowledge base can also be called a knowledge graph because each triplet in G can be regarded as a labeled edge in a graph. The entities in a triplet correspond to nodes, and the relations are the edges. Therefore, some algorithms that are applied to graphs can also be applied to the knowledge base as a graph-structured knowledge graph. Now, we define E as a set of entities and R as a set of relations. The fact, or triplet (h, r, t), has h, t ? E and r ? R. Let G gold ? E ? R ? E be the set of gold facts, that is, ground truth facts in knowledge base G. If a triplet is in G gold , we say it is a positive triplet; otherwise, it is a negative triplet. Suppose we have an incomplete knowledge base G ? G gold ; the goal of KBC is to identify G gold by finding all the missing facts in knowledge base G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. OOKB ENTITY PROBLEM</head><p>In the OOKB situation,we have OOKB entities that are not observed in the training step.</p><p>With knowledge base G observed at training time, new triplets G aux are provided at test time, with E(G aux ) ? E(G) and R(G aux ) ? R(G). The aux contains new entities, but no new relations are involved. We call these new entities OOKB entities, as E OOKB = E(G aux )\E(G). Then, we have</p><formula xml:id="formula_0">E = E(G) ? E OOKB = E(G).</formula><p>In this situation, the KBC task is to correctly identify missing relation triplets that involve the OOKB entities E OOKB .,The difference from the traditional KBC task is that the embeddings for these entities are missing: they must be computed with the help of G aux , every triplet of which contains exactly one OOKB entity in E OOKB and one entity in E(G).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. KBC: TRIPLET CLASSIFICATION</head><p>Triplet classification, a typical KBC task first introduced in <ref type="bibr" target="#b6">[7]</ref> and regarded as a standard benchmark for KBC methods, aims to verify whether an unseen triplet (h, r, t) is true, e.g., (Forrest -Gump, hasdirector, Robert -Zemeckis) should be classified as a true fact while (Forrest -Gump, hasdirector, James-Cameron) should be classified as false <ref type="bibr" target="#b34">[35]</ref>.</p><p>Viewed as a machine learning problem, triplet classification is a classifier induction task in which E and R are given For the KBC task, the existing knowledge base G is assumed to be incomplete, which means that some triplets that must be present in G are missing; i.e., G = G gold .</p><p>Here, we define H = (E ?R?E)\G as the set of triplets not presented in G. The set does not appear during training or in G. We assume that G is incomplete, so two cases are possible for each triplet x ? H; that is, x ? G gold : positive triplet and x / ? G gold : negative triplet. For the former case, the triplet x is not contained in knowledge base G because of incompleteness. Thus, we encounter the problem of determining which of the above two possible cases holds for each triplet not present in G. This is the approach used for triplet classification in the KBC task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. OUR APPROACH A. OVERVIEW</head><p>In this article, we use an embedding-based model to solve the KBC task. We propose a parameter-efficient embedding model that combines the benefits of GNN and CNN by replacing the transition weight matrix in GNN, which represents the relation, with a multilayer convolutional network. The overall structure is based on a GNN.</p><p>A GNN structure consists of two models, the propagation model and the output model <ref type="bibr" target="#b41">[42]</ref>. In some papers <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b33">[34]</ref>, these two components are called the encoder and decoder. The propagation model determines how to propagate information between nodes in a graph while learning information from the graph structure and the neighbor nodes. The output model defines an objective function according to given tasks using vector-represented nodes and edges. In this article, we focus on the propagation component, modifying the structure to improve its parameter efficiency. For the output model, we retain the settings in <ref type="bibr" target="#b28">[29]</ref>. A simple illustration of our model is given in Fig. <ref type="figure" target="#fig_3">2</ref>, and the detailed procedure is described in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. GNN FOR KBC</head><p>GNN is a popular embedding-based model for KBC tasks. In contrast to some existing GNN structures that encode the entire graph into a vector, GNNs for KBC models encode nodes and edges into vectors. Let G be a knowledge graph and e ? E(G) be an entity. The head neighborhood N head and tail neighborhood N tail of the entity e are: </p><formula xml:id="formula_1">N head (e) = {(h, r, e)|(h, r, e) ? G}<label>(</label></formula><p>Here, S head (e) and S tail (e) are sets of vectors. S head (e) contains the representation vectors of neighborhood N head (e), and S tail (e) contains the representation vectors of neighborhood N tail (e). P is a pooling function that maps a set of vectors into a vector, i.e., P :</p><formula xml:id="formula_3">2 R d ? R d .</formula><p>The objective is to extract shared aspects from a set of vectors. For S =</p><p>x i ? R d N i=1 , some common pooling function are as follows:</p><formula xml:id="formula_4">P(S) = N n=1</formula><p>x i (4)</p><formula xml:id="formula_5">P(S) = 1 N N n=1</formula><p>x i (5)</p><formula xml:id="formula_6">P(S) = max({x i } N i=1 )<label>(6)</label></formula><p>Equation (4)(5)(6) are sum pooling, average pooling, and max pooling, respectively. In <ref type="bibr" target="#b41">[42]</ref>, sum pooling was used in the propagation model, while in <ref type="bibr" target="#b28">[29]</ref>, average pooling was used and proved to be the best choice for KBC. In this article, we use average pooling based on previous experience. The sets S head (e) and S tail (e) can be represented as follows:</p><p>S head (e) = {T head (v h ; h, r, e)|(h, r, e) ? N head (e)} (7) S tail (e) = {T tail (v t ; e, r, t)|(e, r, t) ? N tail (e)} ( <ref type="formula">8</ref>)</p><formula xml:id="formula_7">T head , T tail : R d ? E(G) ? R(G) ? E(G) ? R d</formula><p>are called transition functions, and they are used to transform the vector of the head neighborhood entity and tail neighborhood entity of e to the vector v e , depending on the edge between them. Various transition functions have been proposed, and many neural network techniques can be used, such as batch-normalization, residual connection, and long short-term memory. Here, we introduce the transition function that is most closely related to our work, which was proposed in GNN for OOKB <ref type="bibr" target="#b28">[29]</ref>:</p><formula xml:id="formula_8">T head (v h ; h, r, e) = ReLu(BN(A head r v h )<label>(9)</label></formula><p>T tail (v t ; e, r, t) = ReLu(BN(A tail r v t ) <ref type="bibr" target="#b9">(10)</ref> where BN indicates batch normalization. The above equation makes the transition function dependent on the relation between the current node (entity) and the neighbor. Note that the parameter matrices are defined individually for the relation r. The reverse relationship is considered as in different situations v e is either a head entity or tail entity. Therefore, every relation in the knowledge base has two parameter matrices, one for transforming the head entity and the other for transforming the tail entity.</p><p>Overall, the transition function can be written as:</p><formula xml:id="formula_9">v e = T (x) = ReLu(BN(A head r v h ), if x = (h, r, e), ReLu(BN(A tail r v t ), if x = (e, r, t),<label>(11)</label></formula><p>Here, v e is the candidate embedding vector of entity e, which is obtained by a single triplet, while S head (e) and S tail (e) always contain multiple of triplets. By means of (3), we can obtain the final embedding v e of e.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. PARAMETER EFFICIENCY AND CONVOLUTIONAL LAYER</head><p>The efficiency of an algorithm can be measured based on the usage of different resources. In deep learning, a deeper architecture and higher embedding dimension always lead to better performance. However, the cost of such a model is enormous, with higher capacity of calculation and memory usage, and the number of parameters in the model will be astronomical. Therefore, many works focus on the parameter efficiency of  end if 23: end for the model and design the model to have acceptable performance with fewer parameters. Various model compression techniques, such as distillation, pruning and quantization, can be used to reduce the number of network parameters by removing redundant parameters with minimal loss in performance.</p><formula xml:id="formula_10">S head (h) = {T (h 1 , r, h)|(h 1 , r, h) ? N head (h)} 6: S tail (h) = {T (h,</formula><p>In the KBC task, ConvE <ref type="bibr" target="#b26">[27]</ref> is known for its high parameter efficiency and robust performance on different datasets, achieving better scores than DistMult and R-GCNs on FB15k-237 with 8? and 17? fewer parameters. When the embedding size is 200, the ConvE model contains nearly 5M parameters. The embedding layer, convolutional layer, and projection layer account for 2.96M, 320, and 1.96M parameters, respectively. The convolutional layer needs few parameters for the filter, and the embedding layer accounts for a large proportion of the total number of parameters. The number 2.96M is obtained from (14541 + 237) * 200 = 2, 955, 600, where 14541 is the entity number in FB15k-237 and 237 stands for the relation number. A larger knowledge base would result in more parameters, especially in the embedding layer, so controlling the embedding layer size is very important. Moreover, a model that achieves robust performance with a small embedding size, such as ConvE, is highly desirable. In the GNN framework, the design of the transition function is key to improving the parameter efficiency because when the embedding size is fixed, the transition function determines the performance of the model and the number of of parameters.</p><p>The scoring function of ConvE is defined as follows:</p><p>? r (e s , e o ) = f (vec(f ([e s ; r r ] * w))W)e o <ref type="bibr" target="#b11">(12)</ref> e s represents the subject entity and e o is the object entity. A higher score represents a higher likelihood the triplet is true. We believe ConvE can be used as the transition function in the propagation model, so we reorganize the equation as <ref type="bibr" target="#b12">(13)</ref>.</p><formula xml:id="formula_11">v e = T (x) = ReLu(BN(vec([v h ; v r ] * w)W)), if x = (h, r, e), ReLu(BN(vec([v t ; v r ] * w)W)), if x = (e, r, t), (<label>13</label></formula><formula xml:id="formula_12">)</formula><p>where vec() is the flatten operation, w is the convolution operation and W is the parameter matrix for projection. [e; r] denotes 2D reshaping of vectors e and r.</p><p>The main difference between ( <ref type="formula" target="#formula_9">11</ref>) and ( <ref type="formula" target="#formula_11">13</ref>) is the treatment of the relation embedding. In GNN <ref type="bibr" target="#b10">(11)</ref>, the embedding of relations is achieve via weight matrix A used for projection, and every relation has two corresponding matrices for the sake of the alternative relation direction, which means the reverse relationship has a separate embedding. For ConvE <ref type="bibr" target="#b12">(13)</ref>, every relation is embedded as a unique vector v r . Convolutional filter w and projection matrix W are applied for the transition, and all these components share parameters.</p><p>Here, we represent the relation as a convolution kernel, which can learn expressive features from the auxiliary knowledge of OOKB entities. In contrast to ConvE, we discard the vector representation in our propagation model. The alternative is to directly convolve v h or v t with the specific convolution kernel w head r or w tail r representing the corresponding relation. The convolution kernels no longer share parameters, and every relation has two corresponding convolution kernels for whether the direction is head to tail or tail to head. The convolution operation can either be 1D convolution, which is always used in NLP work, or 2D convolution, commonly used in CV work (ConvE is the first KBC model to use 2D convolutional layers). In addition, if 2D convolutions is applied, the embedding vector of a given entity v h or v t should be reshaped to a matrix for the 2D convolutional layer. In a later experiment, we found the performance of 2D convolution to be slightly better. For our new transition function, see <ref type="bibr" target="#b13">(14)</ref>.</p><formula xml:id="formula_13">v e = T (x) = ReLu(BN(vec[v h ] * w head r )W head r )), if x = (h, r, e), ReLu(BN(vec[v r ] * w tail r )W tail r )), if x = (e, r, t),<label>(14)</label></formula><p>The formula retains a different projection weight matrix for each relation. The number of parameters appears to increase, TABLE 1. Given a triplet x, whether x = (h, r , e) or x = (e, r , t ), transition function T (x) is used to obtain the embedding v e of entity e. We modify the score function from ConvE(12) into a transition function suitable for the propagation model.</p><p>but we can achieve better performance with lower embedding size, which means the number decrease overall. The final performance is shown in Table <ref type="table" target="#tab_4">6</ref>, and Table <ref type="table">1</ref> summarizes all the transition functions mentioned above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. OBJECTIVE FUNCTION</head><p>The objective function is designed to guide the training for the task. We use TransE <ref type="bibr" target="#b6">[7]</ref> as the output model because TransE is the most representative translational distance model and the translational property of TransE is highly regarded <ref type="bibr" target="#b33">[34]</ref>. What is different is that we use an absolute-margin objective function instead of the pairwise-margin objective function in original TransE.</p><p>The objective function is as follows:</p><formula xml:id="formula_14">L = N i=1 f (h i , r i , t i ) + [? -f (h i , r i , t i )] +<label>(15)</label></formula><p>Here, [x] + is the hinge function max(0,x). (h i , r i , t i ) denotes a positive triplet in the training set, and (h i , r i , t i ) denotes a negative triplet generated by negative sampling. ? ? R is a threshold, also called the margin. The objective function requires the score f (h i , r i , t i ) to be greater than the score (h i , r i , t i ) by at least ? . The score function f evaluates the implausibility of a triplet (h, r, t): smaller scores indicate that the triplet is more likely to be positive. In TransE <ref type="bibr" target="#b6">[7]</ref>, the score function is defined by</p><formula xml:id="formula_15">f (h, r, t) = v h + v r -v t .</formula><p>The objective function is used to optimize the scores for the positive triplets towards zero, whereas the scores of the negative triplets are going to be at least ? . Some recent research used CNN-based models as the output model for GNN <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b33">[34]</ref>. However, these works studied link prediction, another KBC task. We attempted to adapt the CNN structure to triplet classification, but the results were not satisfactory. Therefore, we chose TransE, a parameterefficient model whose parameters consist of only embedding parameters, as the output model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENT A. DATASETS</head><p>The OOKB datasets were generated through WordNet11 <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b28">[29]</ref>. The data files can be downloaded from https://github.com/takuo-h/GNN-for-OOKB. The specifications of these datasets are shown in Table <ref type="table">2</ref> and Table <ref type="table" target="#tab_1">3</ref>. The datasets include nine independent datasets obtained by different filtering and splitting methods that can be denoted by Head,Tail,Both-1,000,3,000,5,000, respectively, where the first part represents the position of the OOKB entities and the second part represents the number of triplets used for generating the OOKB entities. More details about the datasets can be found in the original paper <ref type="bibr" target="#b28">[29]</ref>. All entities and relations come from WordNet11, and every dataset contains training, validation, and test sets. In addition, the validation and test sets include positive and negative triplets, whereas the training set does not contain negative triplets.</p><p>Here, we use Both-1000 and Both-5000 for our experiment, as the OOKB entities can be either head or tail entities, which is a more general situation in practice. Both-1000 contains 93,364 training triplets and 1,238 OOKB entities, while Both-5000 contains 57,601 training triplets and 4,963 OOKB entities. Thus, the difficulty of Both-1000 is relatively low, and Both-5000 is more difficult: we want to evaluate our model in both easy and difficult scenarios. Details of the datasets are shown in Table <ref type="table" target="#tab_2">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. IMPLEMENTATION AND HYPERPARAMETERS</head><p>Recently, many state-of-the-art empirical results have been challenged for whether they were achieved due to a better model/algorithm or simply by means of a more extensive hyperparameter search <ref type="bibr" target="#b42">[43]</ref>. Since we want to compare our work with GNN-for-OOKB <ref type="bibr" target="#b28">[29]</ref>, we use the same hyperparameters including training epoch,batch size,and learning rate provided in the previous work for the sake of fairness.</p><p>All networks were trained by stochastic gradient descent with the Adam optimizer <ref type="bibr" target="#b43">[44]</ref>. The step size of Adam was ? 1 /(? 2 ? k + 1.0), where k indicates the current epoch in training, ? 1 = 0.01, and ? 2 = 0.0001. The batch size was 5000, and the threshold ? = 300. The embedding size was 200 in GNN-for-OOKB, and the number of training epochs was 500 in every experiment. We trained our model from the beginning and we did not use pretraining.</p><p>To generate corrupted triplets, we used the ''Bernoulli'' trick, a technique also used in <ref type="bibr" target="#b8">[9]</ref>. When corrupting triplets, the Bernoulli trick set different probabilities for replacing the head or tail entity to reduce the chance of generating false negative labels.</p><p>For the hyperparameters in the transition function, we set the kernel size to 3 ? 3 and the number of channels to 10. These settings are from the original ConvE paper. The dimension of the embedding space was 36 in the proposed model; thus, in the feed-forward pass, the input vector is reshaped as R 6?6 and then the convolution operation is applied to the R 10?3?3 filter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. RESULTS AND ANALYSIS</head><p>The performance of the model is shown in Table <ref type="table" target="#tab_4">6</ref>. From Section II, we know that most popular KBC models cannot handle OOKB entities because they are not designed for such entities. Here, we choose GNN-for-OOKB <ref type="bibr" target="#b28">[29]</ref> as the baseline and run both models with the settings in Section V-B. The proposed model performs better with fewer parameters, which means the model has higher parameter efficiency.</p><p>As discussed in Section IV-C, the embedding results in more parameters, and ConvE <ref type="bibr" target="#b26">[27]</ref> is highly parameter efficient because it performs well with a low embedding size. The proposed model is similar, and its high parameter efficiency is a result of the robust performance at low embedding size. Table <ref type="table" target="#tab_5">7</ref> shows the results from our ablation study where we evaluate the performance of both models on the Both-1000 dataset with different embedding sizes. The proposed model always outperforms the previous model for a given parameter scale. The proposed model can learn more expressive features in low embedding size with fewer parameters.</p><p>We investigate the training process in the Fig. <ref type="figure" target="#fig_6">3</ref>. The image has 12 subcharts: the former 11 contain the triplet scores of 11 different relations in the both-1000 dataset, showing how the scores change with learning, and the last is the overall performance, that is, the triplet classification accuracy on the test set. The x label means epoch. In the first 11 charts, we sampled the triplet score during training, and every red and blue line indicates an individual negative or positive triplet's score, the black line is the threshold, which is 300 in this article, and the green line is the accuracy using this threshold. The ''simalar-to'' relation only has two triplets in the test set, so there is only a single red line and single green line in the chart. The information about triplet number in the training set can be found in Fig. <ref type="figure" target="#fig_7">4</ref> and Table <ref type="table" target="#tab_3">5</ref>.</p><p>Our model classified triplets with a score less than 300 as positive, so in Fig. <ref type="figure" target="#fig_6">3</ref>, the score of positive triplets (blue line) decreases as training proceeds, while the score of negative triplets (red line) is increasing. The accuracy (green line) increases as more red lines go above the threshold and blue lines descend under the threshold, which means the model can distinguish the triplets accurately. The red dots in the charts  Fig. <ref type="figure" target="#fig_6">3</ref> shows that most triplets can be optimized towards the correct score, but some triplets are difficult to optimized, that is, the red lines and blue lines are too close to be divided by the black line. This situation is substantial, especially for the ''type-of'' and ''has-instance'' relations. Fig. <ref type="figure" target="#fig_7">4</ref> shows that these two relations have the largest number of triplets, which is interesting because more data always means better performance in representation learning. Combining Fig. <ref type="figure" target="#fig_6">3</ref> and Fig. <ref type="figure" target="#fig_7">4</ref>, we can also see that the accuracy of these two relations rises steadily, which may indicate that relations with a large number of triplets need more training time. This phenomenon may also be related to the knowledge base dataset. There are some discussions <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b44">[45]</ref> of the available KBC datasets, in which the researchers believe the existing dataset and the evaluation protocol are deficient. In the future, we will attempt to generalize the OOKB problem by applying a newly proposed dataset and evaluation method.</p><p>On the basis of the evidence and analysis above, we conclude that the proposed model has advantages in OOKB entity problem. We believe the model's steady convergence in training and high parameter efficiency make it worthy to extend to more KBC scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this article, we focus on the KBC task in the OOKB entity problem. The OOKB entity problem means that entities are unobserved at training time. We propose a parameter-efficient embedding model that benefits from both a GNN and a CNN to handle the OOKB KBC task. The effectiveness of our model in terms of improved accuracy and parameter efficiency is verified in OOKB datasets. Our model achieved better accuracy with approximately one-fifth of the parameter count in the previous work.</p><p>Recently increasing studies have focused on the KBC task in specific scenario. These studies have made KBC task more practical because in real-world cases there are always complicated situations. The OOKB entity problem is one of the common scenarios in real-world cases. Our work is a successful attempt to combine the KBC task with practical scenarios. In the future we would like to extend our model to be scalable to larger datasets and make the model adaptable to different scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 1 .</head><label>1</label><figDesc>FIGURE 1. The knowledge base completion (KBC) models can be divided into 2 stages: training stage (1) and testing stage (2). Case (3) is the KBC task with out-of-knowledge-base (OOKB) entities. (1): During training, triplets that represent facts in the knowledge base are fed to the model, for example, (Forrest -Gump, hasdirector , Robert -Zemeckis), which contains the head entity ''Forrest-Gump'', the relation ''has-director'' and the tail entity ''Robert-Zemeckis''. In this stage, the model performs knowledge base embedding by building distributed representations of entities and relations observed in the training data. (2): During testing, the model applies various vector operations over the embeddings to predict missing relation triplets. Suppose the fact ''Forrest-Gump is a film'', which can be depicted as the triplet (Forrest -Gump, isa, film), is missing from the current knowledge base. The model can find this missing fact through vector operations. For example, if we want to answer the question ''What is Forrest Gump?''. We can obtain the answer ''film'' by following the green dashed arrow in the figure.(3): OOKB entity problem. The difference between the traditional KBC task and the OOKB KBC task is that the OOKB test triplet contains an entity that is not observed in the training step, which means the model does not have access to its vector embedding and vector operations cannot work. In the traditional KBC task, all the entities are observed in the knowledge base (shown as the shaded box). In this case, we use the new triplet (To -Walktheclouds, hasdirector , Robert -Zemeckis), which is called auxiliary knowledge in the OOKB situation, to obtain the embedding of the new entity. Thus, we can answer the question ''What is to walk the clouds?'' by predicting the triplet (To -Walktheclouds, isa, film).</figDesc><graphic url="image-3.png" coords="2,143.23,66.06,288.00,262.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>, and knowledge base G forms the training set (with only positive examples), with G test being the test set. The set G test can be divided into the set of positive test examples H ? (G gold ) = (G gold )\G and the set of negative test examples G test \G gold .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 )</head><label>1</label><figDesc>N tail (e) = {(e, r, t)|(e, r, t) ? G} (2) v e ? R d is a d-dimensional representation vector of e. Our goal is to obtain v e through the propagation model by means of the following equation: v e = P(S head (e) ? S tail (e))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FIGURE 2 .</head><label>2</label><figDesc>FIGURE 2. An illustration of OOKB knowledge base completion. Here, the different colors of the nodes and edges represent different entities and relations, and the blocks are their embeddings. Notably, a relation in a different direction has a different embedding, such as the purple relation in the figure, which has a different embedding when it goes in or goes out blue entity. These embedding are used as convolution kernels in the model. The figure is only a sketch; more details can be found in Algorithm 1.</figDesc><graphic url="image-7.png" coords="5,36.17,66.06,504.80,243.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>VOLUME 8, 2020 Algorithm 1 4 :</head><label>202014</label><figDesc>Test Stage of Our Model Input: Test triplets G test , auxiliary triplets G aux , transition function T , pooling function P, threshold ? , entity embedding dictionary E, relation embedding dictionary R Output: result dictionary Result 1: N head (e) = {(h, r, e)|(h, r, e) ? G aux } //head neighborhood entity set 2: N tail (e) = {(e, r, t)|(e, r, t) ? G aux } //tail neighborhood entity set 3: for each (h, r, t) ? G test do if h / ? E then //head entity is the OOKB entity 5:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>TABLE 2 .</head><label>2</label><figDesc>Specifications of the WordNet11. All the training triplets are positive. Half of the validation and test sets are negative triplets, and these are included in the numbers of validation triplets and test triplets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FIGURE 3 .</head><label>3</label><figDesc>FIGURE 3. Changes in the scores of triplets in the both-1000 dataset during training.</figDesc><graphic url="image-12.png" coords="9,32.20,58.82,504.60,268.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>FIGURE 4 .</head><label>4</label><figDesc>FIGURE 4. Triplet number of a given relation in training set of both-1000.</figDesc><graphic url="image-13.png" coords="9,36.23,352.36,502.00,205.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>r, t 1 )|(h, r, t 1 ) ? N tail (h)}</figDesc><table><row><cell>7:</cell><cell>v h = P(S head (e) ? S tail (e))</cell></row><row><cell>8:</cell><cell>v r , v t = R(r), E(t)</cell></row><row><cell>9:</cell><cell>else if t / ? E(G) then //tail entity is the OOKB entity</cell></row><row><cell>10:</cell><cell></cell></row><row><cell>14:</cell><cell>else//no OOKB entity</cell></row><row><cell cols="2">15: v 16: end if</cell></row><row><cell cols="2">17: score= v 20: else</cell></row><row><cell>21:</cell><cell>Result((h, r, t)) = False</cell></row><row><cell>22:</cell><cell></cell></row></table><note><p><p>S head (t) = {T (h 1 , r, t)|(h 1 , r, t) ? N head (t)} 11: S tail (t) = {T (t, r, t 1 )|(t, r, t 1 ) ? N tail (t)} 12: v t = P(S head (e) ? S tail (e)) 13: v r , v h = E(r), E(h) h , v r , v t = E(h), R(r), E(h) h + v rv t 18: if score &lt; ? then 19:</p>Result((h, r, t)) = True</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 3 .</head><label>3</label><figDesc>The OOKB datasets. The numbers of triplets in the validation and test sets include negative triplets. All relations and entities come from WordNet11.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 4 .</head><label>4</label><figDesc>''Both'' means both head and tail entities can be OOKB entities. ''1000'' and ''5000'' denote the number of OOKB entities.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 5 .</head><label>5</label><figDesc>Triplet number of a given relation in training set of both-1000.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 6 .</head><label>6</label><figDesc>Results. The accuracy of triplet classification in different datasets.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 7 .</head><label>7</label><figDesc>Performance with different embedding size in Both-1000 dataset.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="159040" xml:id="foot_0"><p>  VOLUME 8, 2020   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>VOLUME 8, 2020   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="159044" xml:id="foot_2"><p>  VOLUME 8, 2020   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="159046" xml:id="foot_3"><p>  VOLUME 8, 2020   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="159048" xml:id="foot_4"><p>  VOLUME 8, 2020   </p></note>
		</body>
		<back>

			<div type="funding">
<div><p>This work was supported by the <rs type="funder">National Nature Science Foundation of China</rs> under Grant <rs type="grantNumber">61972357</rs> and Grant <rs type="grantNumber">61672337</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_4P3YTpg">
					<idno type="grant-number">61972357</idno>
				</org>
				<org type="funding" xml:id="_dMbgwnN">
					<idno type="grant-number">61672337</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">WordNet: A lexical database for English</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Freebase: A collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGMOD Int. Conf. Manage. Data</title>
		<meeting>ACM SIGMOD Int. Conf. Manage. Data</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic parsing via paraphrasing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 52nd Annu. Meeting Assoc</title>
		<meeting>52nd Annu. Meeting Assoc</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1415" to="1425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning symmetric collaborative dialogue agents with dynamic knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 55th Annu</title>
		<meeting>55th Annu</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1766" to="1776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Evaluating persuasion strategies and deep reinforcement learning methods for negotiation dialogue agents</title>
		<author>
			<persName><forename type="first">S</forename><surname>Keizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cuayahuitl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Efstathiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-P</forename><surname>Engelbrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dobre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lascarides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Lemon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th Conf</title>
		<meeting>15th Conf</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="480" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">WDAqua-core1: A question answering service for RDF knowledge bases</title>
		<author>
			<persName><forename type="first">D</forename><surname>Diefenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Maret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Companion Web Conf. Web Conf. (WWW)</title>
		<meeting>Companion Web Conf. Web Conf. (WWW)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1087" to="1091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data,&apos;&apos; in Proc</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="2787" to="2795" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A review of relational machine learning for knowledge graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="33" />
			<date type="published" when="2016-01">Jan. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 28th AAAI Conf</title>
		<meeting>28th AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 29th AAAI Conf</title>
		<meeting>29th AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding via dynamic mapping matrix</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 53rd Annu. Meeting Assoc. Comput. Linguistics, 7th Int. Joint Conf. Natural Lang. Process</title>
		<meeting>53rd Annu. Meeting Assoc. Comput. Linguistics, 7th Int. Joint Conf. Natural Lang. ess</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="687" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Knowledge graph completion with adaptive sparse transfer matrix</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 30th AAAI Conf</title>
		<meeting>30th AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="985" to="991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Effective blending of two and three-way interactions for modeling multi-relational data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Garc?a-Dur?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-662-44848-9_28</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Joint Eur. Conf. Mach. Learn. Knowl. Discovery Databases</title>
		<meeting>Joint Eur. Conf. Mach. Learn. Knowl. Discovery Databases<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="434" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Holographic embeddings of knowledge graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>-T. Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6575</idno>
		<ptr target="http://arxiv.org/abs/1412.6575" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn. (ICML)</title>
		<meeting>Int. Conf. Mach. Learn. (ICML)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Analogical inference for multi-relational embeddings</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 34th Int. Conf. Mach. Learn</title>
		<meeting>34th Int. Conf. Mach. Learn<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2168" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Representation learning of knowledge graphs with hierarchical types</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2965" to="2971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modeling relation paths for representation learning of knowledge bases</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Lang. Process</title>
		<meeting>Conf. Empirical Methods Natural Lang. ess</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="705" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Representation learning of knowledge graphs with entity descriptions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th AAAI Conf</title>
		<meeting>13th AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Jointly embedding knowledge graphs and logical rules</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Lang. Process</title>
		<meeting>Conf. Empirical Methods Natural Lang. ess</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="192" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Probase+: Inferring missing links in conceptual taxonomies</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1281" to="1295" />
			<date type="published" when="2017-06">Jun. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<ptr target="http://arxiv.org/abs/1609.02907" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Semantic Web Conf</title>
		<meeting>Eur. Semantic Web Conf</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning attentionbased embeddings for relation prediction in knowledge graphs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nathani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 57th Annu. Meeting Assoc. Comput. Linguistics</title>
		<meeting>57th Annu. Meeting Assoc. Comput. Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4710" to="4723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Convolutional 2D knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 32nd AAAI Conf</title>
		<meeting>32nd AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Interacte: Improving convolution-based knowledge graph embeddings by increasing feature interactions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nitin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3009" to="3016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Knowledge transfer for out-of-knowledge-base entities : A graph neural network approach</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Oiwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shimbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. 26th Int. Joint Conf. Artif. Intell.</title>
		<imprint>
			<biblScope unit="page" from="1802" to="1808" />
			<date type="published" when="2017-08">Aug. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Real-time population of knowledge bases: Opportunities and challenges</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nakashole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Joint Workshop Autom</title>
		<meeting>Joint Workshop Autom</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="41" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fine-grained semantic typing of emerging entities</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nakashole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tylenda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 51st Annu. Meeting Assoc. Comput. Linguistics</title>
		<meeting>51st Annu. Meeting Assoc. Comput. Linguistics</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1488" to="1497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Entity linking at the tail: Sparse signals, unknown entities, and phrase models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>K?c?man</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Loynd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th ACM Int. Conf. Web Search Data Mining</title>
		<meeting>7th ACM Int. Conf. Web Search Data Mining</meeting>
		<imprint>
			<publisher>WSDM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="453" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Entity linking and knowledge discovery in microblogs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Manchanda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISWC-DC ISWC Doctoral Consortium</title>
		<meeting>ISWC-DC ISWC Doctoral Consortium</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">End-to-end structure-aware convolutional networks for knowledge base completion</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. AAAI Conf. Artif. Intell.</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3060" to="3067" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017-12">Dec. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning to extrapolate knowledge: Transductive few-shot out-of-graph link prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Bok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06648</idno>
		<ptr target="http://arxiv.org/abs/2006.06648" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A neural bag-of-words modelling framework for link prediction in knowledge bases with sparse connectivity</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mensah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. World Wide Web Conf. (WWW)</title>
		<meeting>World Wide Web Conf. (WWW)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2929" to="2935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">VLX-stories: Building an online event knowledge base with emerging entity detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fern?ndez-Ca?ellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Espadaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Garolera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Canet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Colom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rimmek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Giro-I Nieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Riveiro</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-30796-7_24</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Semantic Web Conf</title>
		<meeting>Int. Semantic Web Conf<address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="382" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Jointly modeling structural and textual representation for knowledge graph completion in zero-shot scenario</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-96890-2_31</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Asia-Pacific Web (APWeb) Web-Age Inf. Manage. (WAIM) Joint Int. Conf. Web Big Data</title>
		<meeting>Asia-Pacific Web (APWeb) Web-Age Inf. Manage. (WAIM) Joint Int. Conf. Web Big Data<address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="369" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multimodal data enhanced representation learning for knowledge graphs</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Neural Netw. (IJCNN)</title>
		<meeting>Int. Joint Conf. Neural Netw. (IJCNN)</meeting>
		<imprint>
			<date type="published" when="2019-07">Jul. 2019</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attention-based aggregation graph networks for knowledge graph information transfer</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-47436-2_41</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Pacific-Asia Conf. Knowl. Discovery Data Mining</title>
		<meeting>Pacific-Asia Conf. Knowl. Discovery Data Mining<address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="542" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<ptr target="http://arxiv.org/abs/1511.05493" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Knowledge base completion: Baselines strike back</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bajgar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleindienst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Workshop Represent. Learn</title>
		<meeting>2nd Workshop Represent. Learn</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="69" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<ptr target="http://arxiv.org/abs/1412.6980" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A re-evaluation of knowledge graph completion methods</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03903</idno>
		<ptr target="http://arxiv.org/abs/1911.03903" />
	</analytic>
	<monogr>
		<title level="m">ZHONGQIN BI received the Ph.D. degree in system analysis</title>
		<meeting><address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2019. 2006</date>
		</imprint>
		<respStmt>
			<orgName>East China Normal University</orgName>
		</respStmt>
	</monogr>
	<note>He is currently an</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
