<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SimpleX: A Simple and Strong Baseline for Collaborative Filtering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-09-26">26 Sep 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kelong</forename><surname>Mao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of AI</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jieming</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinpeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Tsinghua Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Quanyu</forename><surname>Dai</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Zhenhua</forename><surname>Dong</surname></persName>
							<email>&lt;dongzhenhua@huawei.com&gt;&lt;xiaox@sz.tsinghua.edu.cn&gt;&lt;hexiuqiang1@huawei.com&gt;</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xi</forename><forename type="middle">Xiao</forename><surname>3★</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Tsinghua Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SimpleX: A Simple and Strong Baseline for Collaborative Filtering</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-09-26">26 Sep 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3459637</idno>
					<idno type="arXiv">arXiv:2109.12613v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Information systems → Recommender systems</term>
					<term>Collaborative filtering Recommender systems</term>
					<term>collaborative filtering</term>
					<term>graph neural networks</term>
					<term>contrastive loss</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Collaborative filtering (CF) is a widely studied research topic in recommender systems. The learning of a CF model generally depends on three major components, namely interaction encoder, loss function, and negative sampling. While many existing studies focus on the design of more powerful interaction encoders, the impacts of loss functions and negative sampling ratios have not yet been well explored. In this work, we show that the choice of loss function as well as negative sampling ratio is equivalently important. More specifically, we propose the cosine contrastive loss (CCL) and further incorporate it to a simple unified CF model, dubbed SimpleX. Extensive experiments have been conducted on 11 benchmark datasets and compared with 29 existing CF models in total. Surprisingly, the results show that, under our CCL loss and a large negative sampling ratio, SimpleX can surpass most sophisticated state-of-the-art models by a large margin (e.g., max 48.5% improvement in NDCG@20 over LightGCN). We believe that SimpleX could not only serve as a simple strong baseline to foster future research on CF, but also shed light on the potential research direction towards improving loss function and negative sampling.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Nowadays, personalized recommendation is ubiquitous in various applications, such as video recommendation in YouTube <ref type="bibr" target="#b4">[5]</ref>, product recommendation in Amazon <ref type="bibr" target="#b23">[24]</ref>, and news recommendation in Bing <ref type="bibr" target="#b33">[34]</ref>. The goal of recommendation is to predict whether a user will interact (e.g., click or purchase) with an item and thus help users discover potential items of interests. Collaborative filtering (CF) <ref type="bibr" target="#b26">[27]</ref> is a fundamental task in recommendation that leverages the collaborative information among users and items to predict users' preferences on candidate items. The simplicity and effectiveness make it one of the most popular techniques in recommender systems.</p><p>Generally, the learning process of a CF model can be separated to three major components, including interaction encoder, loss function, and the negative sampling strategy used when only positive (i.e.,implicit) feedbacks are available. Most existing studies focus on the design of more powerful interaction encoders to capture collaborative signals among users and items. Especially, the prevalence of deep learning motivates a rich line of work that applies various neural networks to CF, including multi-layer perceptrons (MLPs) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11]</ref>, auto-encoders <ref type="bibr" target="#b15">[16]</ref>, attention networks <ref type="bibr" target="#b2">[3]</ref>, transformers <ref type="bibr" target="#b27">[28]</ref>, graph neural networks (GNNs) <ref type="bibr" target="#b9">[10]</ref>, and so on. Nevertheless, these models tend to become more and more complex to show performance improvements. This somehow limits their practical applicability in industrial recommender systems that demand high efficiency.</p><p>On the contrary, few research efforts have been devoted to investigating the impacts of the latter two components. Specifically, while multiple loss functions have been used in CF, such as Bayesian personalized ranking (BPR) loss <ref type="bibr" target="#b21">[22]</ref>, binary cross-entropy loss <ref type="bibr" target="#b10">[11]</ref>, softmax cross-entropy loss <ref type="bibr" target="#b4">[5]</ref>, pairwise hinge loss <ref type="bibr" target="#b11">[12]</ref>, and mean square error loss <ref type="bibr" target="#b1">[2]</ref>, there is still a lack of systematic evaluation and comparisons among different loss functions. Furthermore, many recent GNN-based studies <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref> experiment with the BPR loss <ref type="bibr" target="#b21">[22]</ref> and simply set the negative sampling ratio to a small value (i.e., sampling 1 or 10 negative samples per positive useritem pair). In this way, they can justify the superiority of their proposed interaction encoders, but they neglect the importance of loss functions and negative sampling in the learning of CF models.</p><p>In fact, we empirically observed that training with the BPR loss and a small negative sampling ratio results in inferior results for many CF models. In this paper, we show that choosing a suitable loss function and a proper number of negative samples plays an equal or more important role than an interaction encoder. Towards this goal, we systematically compare multiple commonly-used loss functions and also investigate the impact of negative sampling ratio on each loss function. Moreover, inspired by the widely used contrastive loss <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b37">38]</ref> in computer vision, we propose a cosine contrastive loss (CCL) tailored for CF. Our CCL loss optimizes the embedding by maximizing the cosine similarity of a positive useritem pair, while minimizing the similarity of a negative pair to a certain margin. Surprisingly, we found that even a simple model (e.g., MF), if paired with our proposed CCL loss, is sufficient to surpass many sophisticated state-of-the-art models.</p><p>These findings raise questions about whether the current baselines are strong enough to verify the performance improvements of the state-of-the-art CF models, and how much these sophisticated models have really improved. Our work aims to answer these questions. We argue that the current baselines might not be strong enough, which could mislead us to overestimate the real improvements of many new CF models. Instead of criticizing the contributions of any existing work, the main goal of our work is to build a simple and strong baseline model to foster future research on CF.</p><p>In the design of SimpleX, we keep simplicity in mind and borrow ideas from several existing studies (e.g., average pooling in YouTubeNet <ref type="bibr" target="#b4">[5]</ref>, attention in ACF <ref type="bibr" target="#b2">[3]</ref>). We build Simplex as a unified model that integrates matrix factorization and user behaviour modeling. Specifically, it comprises a behavior aggregation layer (e.g., average pooling) to obtain a user's preference vector from the historically interacted items, and then fuses with the user embedding vector via a weighted sum. More importantly, SimpleX is optimized with our CCL loss and a large negative sampling ratio. Although the interaction encoder of SimpleX seems quite simple and might not be novel at all, we show that it could serve as a super-strong baseline model and have great potential for industrial applications because of its high efficiency.</p><p>For evaluation, we conduct comprehensive experiments on 11 benchmark datasets in total and compare with a total of 29 popular CF models of different types. The results show that SimpleX outperforms most sophisticated state-of-the-art methods by a large margin (up to 48.5% improvement in NDCG@20 over LightGCN <ref type="bibr" target="#b9">[10]</ref> on Amazon-Books). We also empirically compare the performance of six representative loss functions and investigate the impact of different negative sampling ratios on each loss function, which demonstrates the superiority of our proposed CCL loss for CF tasks. Furthermore, we evaluate the efficiency of SimpleX, which shows more than 10x speedup over the simplified GNN-based CF model, LightGCN <ref type="bibr" target="#b9">[10]</ref>. We hope that our work could not only serve as a simple and strong baseline to foster future research on CF, but also attract more research efforts towards the co-design of interaction encoders, loss functions, and negative sampling strategies.</p><p>The main contributions of our work are summarized as follows:</p><p>• We highlight the importances of loss functions and negative sampling in CF, and propose the cosine contrastive loss accordingly. • We present a simple and strong baseline model, SimpleX, which could even attain much better performance than most sophisticated state-of-the-art models. • We perform experiments on 11 benchmark datasets and compare SimpleX with 29 existing CF models to show its superiority in terms of both effectiveness and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND RELATED WORK</head><p>In this section, we first give a formulation of collaborative filtering and point out three important aspects in CF modeling. We then summarize different categories of CF models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Formulation of CF</head><p>The research of collaborative filtering includes implicit CF and explicit CF. Implicit CF models learn from implicit feedback data, e.g., click, visit, and purchase, while explicit CF models learn from explicit feedbacks such as ratings. In this work, we focus on implicit CF since it is more common in real recommendation scenarios. Besides, it is also easy to transform explicit feedback to implicit feedback via binarization. In implicit CF, a matrix Y is used to denote the user-item interactions, where 𝑦 𝑢𝑖 = 1 if user u has observed interaction with item i and 𝑦 𝑢𝑖 = 0 otherwise. As mentioned in Section 1, we highlight three vital aspects that have a large impact to the learning process of CF models:</p><p>(1) Interaction Encoder. The function of the interaction encoder is to learn embeddings for each user and each item, which capture collaborative signals in the interaction matrix that reflect the behavioral similarity between users (or items). It is undoubtedly the core of CF models and has been well studied. We give a brief summary of interaction encoders in section 2.2.</p><p>(2) Loss Function. In general, there are two common types of loss functions in CF. Pointwise loss functions such as binary crossentropy (BCE) and mean square error (MSE) treat the learning process as a binary classification or a regression task. Pairwise loss such as Bayesian personalized ranking loss (BPR) is optimized to make the similarities of positive user-item pairs larger than the negative ones.</p><p>(3) Negative Sampling. Since there are a lot of unobserved entries, in most cases we need to perform negative sampling to improve training efficiency. A few studies have been made to improve the uniform random sampling for recommendation, including mining informative negative samples (e.g., RNS <ref type="bibr" target="#b5">[6]</ref>, and NBPO <ref type="bibr" target="#b40">[41]</ref>), tackling the selection bias of implicit user feedback (e.g., MSN <ref type="bibr" target="#b35">[36]</ref>) and so on. In this work, we mainly investigate the influence of the negative sampling ratio. The existing studies are complementary to our work and potential to be applied to our SimpleX model for further improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Summary of representative CF methods</head><p>We summarize representative CF methods into four categories:</p><p>(1) MF-based methods. Matrix factorization (MF) based algorithms decompose the user-item interaction matrix into two lowdimensional latent matrices for user and item representation. Due to its effectiveness, MF has been wildly studied in CF. Manotumruksa et al. proposed GRMF <ref type="bibr" target="#b18">[19]</ref> that smoothed MF through adding the graph Laplacian regularizer to introduce graph information. Yang et al. devised a unified and efficient method called HOP-Rec <ref type="bibr" target="#b36">[37]</ref> that incorporated both MF and graph-based models for implicit CF. Chen et al. designed ENMF <ref type="bibr" target="#b1">[2]</ref>, which is an efficient MF-based CF model with modified MSE loss function. It can be optimized efficiently without negative sampling for implicit feedback.</p><p>(2) Autoencoder-based methods. Autoencoder-based CF methods leverage the autoencoder network architectures to learn item embeddings. Such models are suitable to perform inductive recommendation, i.e., learning from one group of users while performing recommendation for another group of users with the same candidate items. For example, Liang et al. proposed Mult-VAE <ref type="bibr" target="#b15">[16]</ref>, which applied variational autoencoder (VAE) for CF. Ma et al. proposed MacridVAE <ref type="bibr" target="#b17">[18]</ref> by disentangling user intents behind user-item and leveraging 𝛽-VAE to simulate the generative process of a user's personal history interactions. Steck et al. designed a linear model called EASE R <ref type="bibr" target="#b25">[26]</ref> that is geared toward sparse data, in particular implicit feedback data, for the recommendation.</p><p>(3) GNN-based methods. Since the interaction data can be naturally modelled as a user-item bipartite graph, recent studies propose graph neural network (GNN) based CF models and report state-of-the-art performance. GNN-based methods model the recommendation as the link prediction task between user nodes and item nodes, where the higher-order collaborative signals can be effectively captured through multi-layers message passing. Ying et al.</p><p>proposed PinSage <ref type="bibr" target="#b38">[39]</ref> that improved GraphSage <ref type="bibr" target="#b8">[9]</ref> to model the item-item relationships for Pinterest. Wang et al. devised NGCF <ref type="bibr" target="#b31">[32]</ref> that explicitly encoded the collaborative signals as high-order connectivities by performing embedding propagation. He et al. proposed LightGCN <ref type="bibr" target="#b9">[10]</ref>, which removed the feature transformation and non-linear activation in NGCF and improved both performance and efficiency. These successful applications of GNN in recommendation further inspire many good studies, including BGCF <ref type="bibr" target="#b28">[29]</ref> which models the uncertainty in the user-item graph with bayesian graph neural networks, DGCF <ref type="bibr" target="#b32">[33]</ref> which models a distribution over intents for each user-item interaction, NIA-GCN <ref type="bibr" target="#b29">[30]</ref> and NGAT4Rec <ref type="bibr" target="#b24">[25]</ref> that learn neighborhood relationships, and SGL-ED <ref type="bibr" target="#b34">[35]</ref>, DHCF <ref type="bibr" target="#b13">[14]</ref>, LCFN <ref type="bibr" target="#b39">[40]</ref>, and so on.</p><p>(4) Others. We put methods that do not fall into the first three categories into this "Others" category. Here we list some representative models such as SLIM <ref type="bibr" target="#b20">[21]</ref> which is a simple linear model that combines the advantages of neighborhood-and model-based CF approaches, MLPs-based NeuMF <ref type="bibr" target="#b10">[11]</ref> and YouTubet <ref type="bibr" target="#b4">[5]</ref>, memory network-based CMN <ref type="bibr" target="#b6">[7]</ref>, metric learning-based CML <ref type="bibr" target="#b11">[12]</ref>, and NBPO <ref type="bibr" target="#b40">[41]</ref> that leverages noisy-label robust learning techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SIMPLEX</head><p>In this section, we first present our cosine contrastive loss and the SimpleX model architecture for CF. We then analyze its connections to other existing models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Cosine Contrastive Loss</head><p>In the CF literature, many different loss functions have been employed, including BPR loss <ref type="bibr" target="#b21">[22]</ref>, binary cross-entropy <ref type="bibr" target="#b10">[11]</ref>, softmax cross-entropy <ref type="bibr" target="#b4">[5]</ref>, pairwise hinge loss <ref type="bibr" target="#b11">[12]</ref>, etc. However, there is still a lack of a systematic comparison among them, leaving their effects on model performance not well understood. In this work, we not only make such a comparison, but also propose a new loss function for CF, namely cosine contrastive loss (CCL). Given a positive user-item pair (𝑢, 𝑖) and a set of randomly sampled negative samples (i.e., N ), the CCL loss is expressed as follows:</p><formula xml:id="formula_0">L 𝐶𝐶𝐿 (𝑢, 𝑖) = (1 − ŷ𝑢𝑖 ) + 𝑤 |N | ∑︁ 𝑗 ∈N max(0, ŷ𝑢 𝑗 − 𝑚)<label>(1)</label></formula><p>where ŷ𝑢𝑖 calculates the cosine similarity between the representation vectors of user 𝑢 and item 𝑖. |N | denotes the number of negative samples. 𝑚 is the margin to filter negative samples, which is usually set to 0∼1. Intuitively, CCL is optimized to maximize the similarity between positive pairs and minimize the similarity of negative pairs below the margin 𝑚. 𝑤 is a hyper-parameter to control the relative weights of positive-sample loss and negative-sample loss. Design Choices. The formulation of CCL is simple and largely inspired by the widely used contrastive loss <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b37">38]</ref> in the computer vision tasks, such as face recognition and image retrieval. But we make several design choices that differ from most widely-used loss functions in CF and greatly facilitate model training. First, instead of applying dot product (e.g., in LightGCN <ref type="bibr" target="#b9">[10]</ref>) or Euclidean distance (e.g., in CML <ref type="bibr" target="#b11">[12]</ref>) to measure the similarity (or distance) between a user-item pair, we choose to compute the cosine similarity between them. By applying L2 normalization on both representation vectors, cosine similarity only calculates the angle difference and thus avoid the effect of representation magnitude. This is favorable since the magnitude of a user/item representation could be strongly biased by its popularity in CF tasks. This is also similar to the calculation of word similarity in Word2Vec <ref type="bibr" target="#b19">[20]</ref>, where cosine similarity is usually used.</p><p>Second, when the number of negative samples becomes large, there usually exist many redundant yet uninformative samples. But existing loss functions (e.g., BPR <ref type="bibr" target="#b21">[22]</ref>) treat every negative sample equivalently. As such, model training could be overwhelmed by these uninformative samples, which significantly degrade the model performance and also slows the convergence. In contrast, CCL alleviates this problem by using a proper margin 𝑚 to filter uninformative negative samples. Intuitively, uninformative negative samples will get zero loss in CCL when they have a small cosine similarity below the margin 𝑚. As a result, it helps automatically identify those hard negative samples with cosine similarity larger than 𝑚, and thus facilitates better training of the model.</p><p>Third, we found that directly summing or averaging the loss terms of all negative samples could degrade the model performance, especially when the number of negative samples is large. This is partially due to the high imbalance between positive and negative samples (e.g., 1:1000 when |N | = 1000). We thus introduce a datadependent weight 𝑤 to control the balance between positive loss and negative loss. We emphasize that it also achieves a similar effect to the confidence weight imposed on negative samples in weighted matrix factorization <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Architecture</head><p>To leverage the advantages of CCL, we further propose a simple CF model, dubbed SimpleX. In the design of SimpleX, we keep simplicity in mind and borrow ideas from several successful models such as YouTubeNet <ref type="bibr" target="#b4">[5]</ref>, ACF <ref type="bibr" target="#b2">[3]</ref>, and PinSage <ref type="bibr" target="#b38">[39]</ref>.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> illustrates the overall architecture of SimpleX. It largely follows the mechanism of MF, which factorizes users and items into a common latent space. Yet, SimpleX also takes the interacted item sequence of each user as additional input to better model user behaviors. This also has been shown effective in many existing studies, such as YouTubeNet <ref type="bibr" target="#b4">[5]</ref> and ACF <ref type="bibr" target="#b2">[3]</ref>. The key part of Sim-pleX lies in its aggregation layer for behavior sequence aggregation. Here we introduce three common aggregation choices, including average pooling, self-attention, and user-attention, but Simplex is a unified architecture that any other aggregation method should also be applicable. Suppose the historically interacted item set of user 𝑢 as H 𝑢 , and we set its maximal size to 𝐾. For users with a different size of interacted items, either padding or chunking can be applied accordingly. As such, the aggregated vector can be obtained as follows:</p><formula xml:id="formula_1">𝑝 𝑢 = ∑︁ 𝑘 ∈H 𝑢 𝐼 𝑘 • 𝛼 𝑘 𝒆 𝑘 ,<label>(2)</label></formula><p>where 𝑒 𝑘 ∈ R 𝑑×1 is the 𝑑-dimensional embedding vector of item 𝑘. 𝐼 𝑘 denotes the mask indices to H 𝑢 during padding, where 𝐼 𝑘 = 0 indicates a padding token; otherwise 𝐼 𝑘 = 1. 𝛼 𝑘 denotes the aggregation weight, which can be computed according to different aggregation types as follows.</p><formula xml:id="formula_2">𝛼 𝑘 =        𝐼 𝑘 𝑘 ∈H𝑢 𝐼 𝑘 , if average pooling, 𝐼 𝑘 •𝑒𝑥𝑝 (𝛽 𝑘 )</formula><p>𝑗 ∈H𝑢 𝐼 𝑘 •𝑒𝑥𝑝 (𝛽 𝑗 ) , if self-attention or user-attention.</p><p>(</p><formula xml:id="formula_3">)<label>3</label></formula><p>Average pooling provides a straightforward way to aggregate the interacted items, which has been successfully applied in YouTubeNet <ref type="bibr" target="#b4">[5]</ref>. But it treats each item equally and fails to account for the relative importances of different items as well as a user's preference on each item. The attention mechanism, such as self-attention and user-attention, can be applied in such cases as calculated in the lower part of Equation <ref type="formula" target="#formula_3">3</ref>. The difference between them lies in the computation of 𝛽 𝑘 , which is:</p><formula xml:id="formula_4">𝛽 𝑘 = 𝑞 𝑇 𝑡𝑎𝑛ℎ(𝑾 1 𝑒 𝑘 + 𝑏 1 ) , if self-attention, 𝑒 𝑇 𝑢 𝑡𝑎𝑛ℎ(𝑾 2 × 𝑒 𝑘 + 𝑏 2 ) , if user-attention. (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>where 𝑞 ∈ R 𝑑×1 is a learnable global query vector for self-attention and 𝑒 𝑢 is the user-specific query vector for user 𝑢 in user-attention. 𝑾 1 , 𝑾 2 ∈ R 𝑑×𝑑 and 𝑏 1 , 𝑏 2 ∈ R 𝑑×1 are learnable parameters. Note that similar attention mechanisms can be found in some existing work <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b33">34]</ref>. However, after behavior aggregation via Equation 2, the pooling vector 𝑝 𝑢 may lie in a different latent space with user vector 𝑒 𝑢 . We further fuse both parts to get the final user representation ℎ 𝑢 :</p><formula xml:id="formula_6">ℎ 𝑢 = 𝑔 • 𝑒 𝑢 + (1 − 𝑔) • 𝑉 𝑝 𝑢<label>(5)</label></formula><p>where 𝑽 ∈ R 𝑑×𝑑 is a learnable parameter and 𝑔 is a hyperparameter weight. Finally, we measure the cosine similarity ŷ𝑢 𝑖 between user 𝑢 and item 𝑖 as the input to our CCL loss.</p><formula xml:id="formula_7">ŷ𝑢𝑖 = 𝑐𝑜𝑠 (ℎ 𝑢 , 𝑒 𝑖 )<label>(6)</label></formula><p>The above three aggregation layers provide different views for aggregation, including global-average view, global-weighed view and user-specific weighted view. The choice among them is quite data-dependent. In our experiment, we show that average-pooling is a robust aggregation method that always demands a first attempt when applying SimpleX. The other two usually needs more efforts to tune and in some cases brings marginal improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Relationships to Existing Models</head><p>SimpleX is also related to multiple popular CF models.</p><p>• MF. MF is the most common model for CF. SimpleX follows the similar mechanism of MF. When setting g = 1 in SimpleX, it reduces to a MF model trained with CCL (i.e., MF-CCL). • YouTubeNet. YouTubeNet is a successful model that has been widely used in industry. SimpleX can be also seen as a simplified YouTubeNet model (without using side features) when average pooling is employed. The only difference is that YouTubeNet employs concatenation instead of weighted sum to fuse 𝑒 𝑢 and 𝑝 𝑢 . But the latter performs better in our experiments. • GNN-based models. Simplex is also similar to GNN-based models. For instance, when choosing the user-attention aggregation layer, it almost equals to a graph attention (GAT) layer applied on user nodes only. If using the self-attention aggregation layer, it works like the neighbor interaction in NIA-GCN <ref type="bibr" target="#b29">[30]</ref> as well.</p><p>We emphasize that although the design of SimpleX is simple and might not be novel to some extent, it unifies several key components in existing CF models. Surprisingly, such a simple model is sufficient to surpass most state-of-the-art CF models by a large margin, which could serve as simple and strong baseline for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we conduct comprehensive experiments to evaluate SimpleX, including: 1) studying the impacts of loss functions and negative sampling ratios, 2) making performance comparisons to existing models on three main datasets, 3) incorporating CCL to other models, 4) performing parameter analysis and efficiency evaluation, 5) further validating SimpleX on some other datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>4.1.1 Dataset. We use 11 benchmark datasets in our study. For fairness and ease of comparison, we choose those open datasets that have been already split and preprocessed. Specifically:</p><p>(1) We employ three main datasets Amazon-Books, Yelp2018, and Gowalla, which are commonly used in recent GNN-based CF models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35]</ref>. We perform most of our experiments on them and further make comparisons to these GNN-based models. (2) To demonstrate the universality of SimpleX, we further test SimpleX on some other datasets adopted by studies published in toptier conferences. Three of them, Amazon-CDs, Amazon-Movies, Amazon-Beauty, are adopted by the work NIA-GCN <ref type="bibr" target="#b29">[30]</ref> and BGCF <ref type="bibr" target="#b28">[29]</ref>. The other three, Amazon-Electronics, CiteUlike-A, and Movielens-1M, are provided by NBPO <ref type="bibr" target="#b40">[41]</ref>, DHCF <ref type="bibr" target="#b13">[14]</ref>, and LCFN <ref type="bibr" target="#b39">[40]</ref>, respectively. Specifically, we compare SimpleX with the corresponding models on the corresponding datasets that adopted in their original papers. For example, we will compare with DHCF <ref type="bibr" target="#b13">[14]</ref> on CiteUlike-A dataset because DHCF adopts this dataset in their original paper.</p><p>(3) The last two are Movielens-20M and MillionSongData, which are commonly used by autoencoder-based CF models, such as Mult-VAE <ref type="bibr" target="#b15">[16]</ref> and RecVAE <ref type="bibr" target="#b22">[23]</ref>. We follow the strong generalization setting, which split train/validation/test sets with different sets of users, and specially make comparison with those autoencoderbased CF models to further demonstrate the effectiveness of Sim-pleX.</p><p>4.1.2 Compared Methods. We compare SimpleX with 29 existing CF models of different types:</p><p>• Five methods based on MF and its variants, including MF-BPR <ref type="bibr" target="#b14">[15]</ref>, GRMF <ref type="bibr" target="#b18">[19]</ref>, HOP-Rec <ref type="bibr" target="#b36">[37]</ref>, NeuMF <ref type="bibr" target="#b10">[11]</ref>, and ENMF <ref type="bibr" target="#b1">[2]</ref>; • Four autoencoder-based methods, including Mult-VAE <ref type="bibr" target="#b15">[16]</ref>,</p><p>MacridVAE <ref type="bibr" target="#b17">[18]</ref>, EASE R [26], and RecVAE <ref type="bibr" target="#b22">[23]</ref>; • Fourteen GNN-based methods, including GC-MC <ref type="bibr" target="#b0">[1]</ref>, Pinsage <ref type="bibr" target="#b38">[39]</ref>, GAT <ref type="bibr" target="#b30">[31]</ref>, NGCF <ref type="bibr" target="#b31">[32]</ref>, DisenGCN <ref type="bibr" target="#b16">[17]</ref>, LR-GCCF <ref type="bibr" target="#b3">[4]</ref>, NIA-GCN <ref type="bibr" target="#b29">[30]</ref>, LightGCN <ref type="bibr" target="#b9">[10]</ref>, DGCF <ref type="bibr" target="#b32">[33]</ref>, NGAT4Rec <ref type="bibr" target="#b24">[25]</ref>, SGL-ED <ref type="bibr" target="#b34">[35]</ref>, BGCF <ref type="bibr" target="#b28">[29]</ref>, DHCF <ref type="bibr" target="#b13">[14]</ref>, and LCFN <ref type="bibr" target="#b39">[40]</ref>; • Six methods of other types, including ItemPop, SLIM <ref type="bibr" target="#b20">[21]</ref>,</p><p>CML <ref type="bibr" target="#b11">[12]</ref>, YouTubeNet <ref type="bibr" target="#b4">[5]</ref>, CMN <ref type="bibr" target="#b6">[7]</ref>, and NBPO <ref type="bibr" target="#b40">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Implementation Details.</head><p>We implement SimpleX in PyTorch. Specifically, we set the batch size to 1024 by default. We use the Adam optimizer and tune the learning rate among [1e-3, 5e-4, 1e-4]. We also employ 𝐿 2 regularization on the embedding parameters and search the regularization weight between 1e-9∼1e-2 with an increase ratio of 5. For cosine contrastive loss, we search the number of negative samples from 1 to 2000. In many cases, we pick 100, 500, or 1000. The margin 𝑚 is tuned among 0∼1 at an interval of 0.1, for example, we set 0.4, 0.9, and 0.9 on Amazon-Books, Yelp2018, and Gowalla, respectively. Meanwhile, we use the same embedding size with the compared model, for example, 64 in LightGCN and 128 in LCFN. For fairness of comparison with existing models, we report the results using the same evaluation metrics (e.g., Recall@20 and NDCG@20) and duplicate the reported results on their papers for consistency. To facilitate reproducible research in the community, we have contributed our source code and detailed benchmark settings to the public Open-CF-Benchmark<ref type="foot" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Impact of Different Loss Functions</head><p>While most studies focus on the interaction encoder design, they neglect the importance of loss functions in the learning of a CF model. We make a systematic comparison on the impacts of different loss functions. For this purpose, we choose one of the simplest baseline CF models, i.e., MF, as the backbone to perform the experiments, since simple models tend to be more illustrative. In addition to our CCL loss, we evaluate MF on the following representative loss functions:</p><p>• Bayesian personalized ranking (BPR) loss encourages the similarity of a positive user-item pair to be higher than that of each negative user-item pair. It is one of the most commonly used loss function for CF research <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33</ref>]. • Pairwise hinge loss (PHL), is also known as max-margin objective, which has been used in CML <ref type="bibr" target="#b11">[12]</ref>. PHL forces the distance of a negative user-item pair to be larger than a positive one by at least the marginal distance. • Binary cross-entropy (BCE) loss is commonly used for binary classification, which has been adopted in the early work NeuMF <ref type="bibr" target="#b10">[11]</ref>. • Softmax cross-entropy (SCE) loss is widely used for multiclass classification. YouTubeNet <ref type="bibr" target="#b4">[5]</ref> cast item prediction as a multi-class classification task through the SCE loss. • Mean square error (MSE) has been widely used for CF, such as WMF <ref type="bibr" target="#b12">[13]</ref> and ENMF <ref type="bibr" target="#b1">[2]</ref>. Table <ref type="table" target="#tab_0">1</ref> shows the results of training MF with different loss functions on Amazon-Books, Yelp2018, and Gowalla. Note that every model has been trained with enough epochs to reach convergence and the best results are reported. From the results, we have the following observations:</p><p>1) CCL consistently achieves the best performance on all the three datasets, outperforming the other loss functions by at least 16.7%, 9.2% and 13.7% w.r.t. Recall@20 on Amazon-Books, Yelp2018 and Gowalla, respectively.</p><p>2) BPR only appears to be strong on Gowalla and performs not well on both Amazon-Books and Yelp2018. This demonstrates that using BPR for training is probably sub-optimal, and thus the results reported by many previous papers may need careful re-examination and are likely to be further improved with our CCL loss. Why CCL performs better than the other loss functions? addition to the design choices analyzed in Section 3.1, we further highlight the advantages of CCL with some concrete comparisons. First, in contrast to BPR, BCE, SCE, and MSE, CCL can automatically filter out hard negative samples that are hard to distinguish (i.e., large cosine similarity) by the model via its margin mechanism. For example, if we set 𝑚 = 0.8, only those negative pairs with ŷ𝑢𝑖 &gt; 0.8 will contribute to the loss. Different from the above loss functions that treat each negative sample equally, CCL allows the model to emphasize on the learning of hard negative samples and thus generate more discriminative representations. Second, compared with PHL that also applies a margin mechanism, CCL is more effective for CF. The PHL loss is determined by the relative distance between positive samples and negative samples. Even if a negative sample is actually hard to be distinguished (e.g., ŷ𝑛𝑒𝑔 𝑢𝑖 = 0.8), it will not contribute to learning if the corresponding positive sample has ŷ𝑝𝑜𝑠 𝑢𝑖 = 0.9. CCL avoids such ambiguity by penalizing the absolute similarity ŷ𝑢𝑖 of each negative sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Impact of Negative Sampling Ratio</head><p>We argue that negative sampling ratio is also important in the learning of CF models, which has been largely ignored by existing studies. To support our claims, we compare the performance of MF trained with 1∼2000 negative samples on Amazon-Books. We also repeat the experiment on different loss functions. We train each model until convergence and report the best results, as shown in Figure <ref type="figure" target="#fig_1">2</ref>. We have the following observations from the results:</p><p>1) The number of negative samples does matter for CF model training. Generally, increasing it within a certain range leads to improvements. This suggests that we should carefully consider the impact of the number of negative samples in the evaluation.</p><p>2) MF trained with CCL is consistently better than training with the other loss functions under different negative sampling ratios, further demonstrating the superiority of our CCL.</p><p>3) The performances of PHL, MSE, and BPR become stable when the number of negative samples increases to 50. In contrast, CCL, BCE, and SCE can keep performance gains with the increase of number of negative samples, even when it reaches to 1000.</p><p>In summary, our experimental results show that both loss functions and negative sampling ratios can have a large impact on model performance. Training with the CCL loss and a large negative sampling ratio appears to be a promising setting for CF methods to gain higher performance. We therefore call for more future research towards this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Performance Comparison to SOTA Models</head><p>In this section, we provide a comprehensive comparison results of SimpleX and other 23 CF models on three main datasets, i.e., Amazon-Books, Yelp2018, and Gowalla, which are very commonly adopted in CF studies (especially in GNN-based CF), to demonstrate the superiority of SimpleX. Table <ref type="table">2</ref> shows our performance comparisons on Amazon-Books, Yelp2018, and Gowalla under the same evaluation protocol, and we have the following observations:</p><p>1) Our SimpleX achieves the best overall performance on all the three datasets. In particular, compared with the most recent Light-GCN, SimpleX makes 41.9%, 8.0%, and 2.3% performance improvements on Recall@20 for Amazon-Books, Yelp2018, and Gowalla, respectively, demonstrating the high effectiveness of SimpleX. Besides, note that we do not report the results of SGL-ED <ref type="bibr" target="#b34">[35]</ref> and NGAT4Rec <ref type="bibr" target="#b24">[25]</ref> on Gowalla since they are not evaluated on Gowalla but only evaluated on the other two datasets in their original papers too, and the authors have not released their code. As the experimental settings of SGL-ED and NGAT4Rec are exactly same as us, we just report their results on Amazon-Books and Yelp2018.</p><p>2) The performance of MF-CCL is surprising. When using CCL as the loss function, the performance of MF is not only much better than the results of MF-BPR reported in the previous paper, but also reaches a new state-of-the-art performance (if leaving out our SimpleX) on Amazon-Books and Yelp2018. On Gowalla, it also achieves comparable performance compared to the previous best model DGCF. Such results strongly suggest that loss functions can make a big difference and should be carefully chosen and studied.</p><p>3) YouTubeNet, CML, and SLIM are three models that we added and have not been tested on these three datasets before by the existing work. We found that they achieve pretty good performance. Specifically, these three models can averagely outperform a representative GNN-based CF model -NGCF, by more than 24% and 28% w.r.t. Recall@20 and NDCG@20, respectively. This implies that the current baselines are relatively weak, which may lead us to overestimate how much real progress we have made in CF.</p><p>4) In CF tasks, more complex models not always lead to better performance. The designs of SLIM, YouTubeNet, CML, MF-CCL, and our SimpleX are all much more concise than most of autoencoderbased (e.g., Mult-VAE and MacridVAE) and GNN-based models (e.g., NGCF, NIA-GCN, and DGCF), but they can achieves better performance. This also reveals that the current trend in CF research, which pays too much attention to the design of sophisticated interaction encoders while ignoring the impacts of loss functions and negative sampling, needs to be improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Incorporating CCL to Other Models</head><p>In Table <ref type="table">2</ref>, we have shown that one of the simplest models, i.e., MF, can even largely outperforms most of state-of-the-art models if training with CCL. We are curious about how other models will perform if incorporated with CCL instead of their original losses. Therefore, in this part, we take experiments with two effective CF models in addition, i.e., YouTubeNet and LightGCN with CCL, and report the results on Amazon-Books and Yelp2018 in Table <ref type="table" target="#tab_2">3</ref>.</p><p>Table <ref type="table">2</ref>: Performance comparison to popular CF models. We highlight the top-5 best results in each column. RI stands for relative improvement. We also report the average RI over NGCF for each model if applicable. For models marked with * , part of the results are duplicated from existing papers for consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Amazon-Books</head><p>Yelp2018 Gowalla Avg RI over NGCF Publication Model Recall@20 NDCG@20 Recall@20 NDCG@20 Recall@20 NDCG@20 Recall@20 NDCG@20 From the results, we find that training YouTubeNet and Light-GCN with CCL instead of their original loss functions, i.e, SCE and BPR respectively, can bring good improvements. This demonstrates that CCL is likely to be a more promising loss function to help CF models achieve better performance. Besides, we observe that the improvements brought by CCL on YouTubeNet and LightGCN are not as significant as those on MF. CCL seems to improve these models to a similar level of performance. This may be because of the following reason: Generally, valuable collaborative information can be captured by both the interaction encoder and the loss function. As the encoders of YouTubeNet and LightGCN are sophisticated and stronger to learn biased collaborative signals, by contrast, the impact of the loss function to them appears relatively small.</p><p>In addition, it is worth noting that our main focus is to question the value of sophisticated encoders and provide a simple strong baseline, but not to improve current state-of-the-art CF models by exhaustingly trying of various loss functions. Based on the experiments with MF, YouTubeNet, and LightGCN, we demonstrate and highlight that the loss function is a large bottleneck in CF models. We expect our work could inspire more research to study </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Parameter Analysis on SimpleX</head><p>We investigate the performance of three different behavior aggregation layers, the fusing weight 𝑔, and the negative loss weight 𝑤. Results on Amazon-Books and Yelp2018 are shown in Table <ref type="table" target="#tab_3">4</ref>. We can make the following observations: 1) Average pooling, self-attention, and user-attention obtain very similar results on Amazon-Books and Yelp2018, respectively. This shows the robustness of apply average pooling for behavior aggregation in practice. SimpleX with 𝑔 = 0.5 reaches higher performance compared with the other two settings on Amazon-Books, which shows that importance of fusing user embedding with user behavior aggregation. 2) The negative weight 𝑤 which adjusts the ratio of positive and negative losses is vital to model's performance. In general, too small (𝑤 = 1) too large (𝑤 = 1000) difference between positive and negative losses leads to performance reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Efficiency Comparison</head><p>Our SimpleX has high efficiency due to its simple design. We numerically compare the training time of SimpleX with two state-of-the-art CF models, i.e., ENMF and LightGCN, which are relatively efficient in their respective categories, on Amazon-Books. The efficiency experiments are conducted on the same Intel(R) Xeon(R) Silver 4210 CPU @2.20GHz machine with one GeForce RTX 2080 GPU. We compare them under the same implementation framework, using the same acceleration methods (e.g., implementing the sampling with C++) to ensure fairness. Specifically, we present the averaged training time per epoch, the number of epochs that the model needs to reach the level of performance reported in the original paper, and the total training time (test time is not included), in Table <ref type="table" target="#tab_4">5</ref>. It turns out that SimpleX is much more efficient than ENMF and LightGCN overall. Specifically, SimpleX only needs around 30 epochs to converge in training, which is more convenient for real application. The total training time of SimpleX with a 1000:1 negative sampling ratio has around 2x and 10x speedup compared with ENMF and LightGCN respectively. Moreover, if we decrease the negative sampling ratio to 100:1, the training time for one epoch of SimpleX can be optimized to 40s, finally resulting in only 19 minutes total training time. Certainly, the performance slightly drops compared with using a 1000:1 negative sampling ratio, but it still maintains a pretty good level (much better than ENMF and LightGCN). Such high efficiency makes our model promising to be applied in large-scale real recommender systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Evaluating SimpleX on More Datasets</head><p>In addition to the three main datasets used in the above sub-sections, we additionally evaluate SimpleX on 8 more datasets to further demonstrate the generability of SimpleX.</p><p>Table <ref type="table">6</ref> shows the comparison results to some state-of-the-art CF models published in 2020. For fairness of comparison, we use the same data preprocessing and experimental settings (embedding dimensions and evaluation metrics) provided by the corresponding papers. We observed that SimpleX consistently outperforms all the compared models on different datasets. The performance improvements are especially large (12.8% to 33.6% improvement in NDCG@20) on Amazon-CDs, Amazon-Movies and Amazon-Beauty compared to BGCF, a recent GNN-based model. This again strongly verifies the effectiveness and robustness of SimpleX to serve as a strong baseline in future work.</p><p>Moreover, we also make a comparison to some autoencoderbased models, including SLIM, Mult-VAE, EASE R , and RecVAE. It is worth noting that our experiment also follows the same setting with them. In particular, we adopt the strong generalization protocol, where the training, validation and test sets are disjoint in terms of users. This requires the model to perform inductive learning during inference. That is, only item embeddings can be learned during training and then transferred to the validation and test sets for prediction. To achieve this, we simplify SimpleX by setting 𝑔 = 0 in this experiment and only learn user representations from their historically interacted items.</p><p>Table <ref type="table" target="#tab_5">7</ref> presents the evaluation results on Movielens-20M and MillionSongData. We can see that SimpleX obtains better performance than SLIM, which is a well-known strong baseline for CF. But it does not surpass Mult-VAE, EASE R and RecVAE given their complete forms. This is reasonable because all of them use many more parameters (O (|𝐼 | * 600) for Mult-VAE and RecVAE, O (|𝐼 | * |𝐼 |) for EASE R ) than SimpleX, as shown in the "#Params" columns. Note that both Mult-VAE and RecVAE use 600 as the dimension of the first hidden layer. As the number of items (|𝐼 |) easily reaches millions to billions in industrial recommender systems, we choose a small embedding dimension (i.e., 64) and results in parameters in the scale of O (|𝐼 | * 64). To make the comparison more fair, we reduce the embedding dimensions of baseline models accordingly. Specifically, for Mult-VAE and RecVAE, we set its encoder and decoder as a single (|𝐼 | * 64)-dimensional dense layer. For EASE R , we decompose its item similarity matrix (denoted as B) to two (|𝐼 | * 64)-dimensional sub-matrices by truncated SVD, and multiply the two sub-matrices to approximate the item similarity matrix to perform predictions. In this setting, SimpleX clearly outperform these autoencoder based CF models.</p><p>Overall, our comprehensive experimental results on various datasets show that our SimpleX is simple and strong to serve as a Table <ref type="table">6</ref>: Performance comparison to some more models published in 2020. We use the same dataset settings and report the same metrics with the original papers. We also duplicate their reported results for consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Amazon-CDs</head><p>Amazon-Movies Amazon-Beauty Model Recall@20 NDCG@20 Model Recall@20 NDCG@20 Model Recall@20 NDCG@20 NGCF 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we study the progress made in CF research and identify three key aspects for CF modeling. While most research focuses on interaction encoders, the impacts of loss functions and negative sampling on CF models have been largely neglected. In this work, we highlight their impacts and further propose the cosine contrastive loss together with a simple and strong baseline for CF, dubbed SimpleX. It outperforms most state-of-the-art CF models by a large margin. Our work released the simple and strong baseline model and the whole benchmarking results for foster future research on CF. We conduct extensive experiments to validate the effectiveness and efficiency of SimpleX. We suggest that the CF community should pay more attention to other key components in addition to interaction encoders and encourage researchers to conduct more robust empirical evaluation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The Simple Model Architecture of SimpleX.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The effect of number of negative samples across different loss functions on Amazon-Books.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance of MF under different loss functions. The best result in each column is marked in bold.</figDesc><table><row><cell>Loss</cell><cell cols="6">AmazonBooks Recall@20 NDCG@20 Recall@20 NDCG@20 Recall@20 NDCG@20 Yelp18 Gowalla</cell></row><row><cell>BPR Loss</cell><cell>0.0338</cell><cell>0.0261</cell><cell>0.0549</cell><cell>0.0445</cell><cell>0.1616</cell><cell>0.1366</cell></row><row><cell>Pairwise Hinge Loss</cell><cell>0.0352</cell><cell>0.0267</cell><cell>0.0562</cell><cell>0.0453</cell><cell>0.1318</cell><cell>0.0996</cell></row><row><cell>Binary Cross-Entropy</cell><cell>0.0479</cell><cell>0.0371</cell><cell>0.0617</cell><cell>0.0503</cell><cell>0.1321</cell><cell>0.1159</cell></row><row><cell>Softmax Cross-Entropy</cell><cell>0.0478</cell><cell>0.0367</cell><cell>0.0639</cell><cell>0.0522</cell><cell>0.1545</cell><cell>0.1276</cell></row><row><cell>Mean Square Error</cell><cell>0.0337</cell><cell>0.0267</cell><cell>0.0624</cell><cell>0.0513</cell><cell>0.1528</cell><cell>0.1315</cell></row><row><cell>Cosine Contrastive Loss</cell><cell>0.0559</cell><cell>0.0447</cell><cell>0.0698</cell><cell>0.0572</cell><cell>0.1837</cell><cell>0.1493</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance of different models trained with CCL v.s. their original losses.</figDesc><table><row><cell>Model</cell><cell cols="4">Amazon-Books Recall@20 NDCG@20 Recall@20 NDCG@20 Yelp2018</cell></row><row><cell>MF-BPR</cell><cell>0.0338</cell><cell>0.0261</cell><cell>0.0549</cell><cell>0.0445</cell></row><row><cell>MF-CCL</cell><cell>0.0559</cell><cell>0.0447</cell><cell>0.0698</cell><cell>0.0572</cell></row><row><cell>YouTubeNet</cell><cell>0.0502</cell><cell>0.0388</cell><cell>0.0655</cell><cell>0.0537</cell></row><row><cell cols="2">YouTubeNet-CCL 0.0544</cell><cell>0.0430</cell><cell>0.0685</cell><cell>0.0563</cell></row><row><cell>LightGCN</cell><cell>0.0411</cell><cell>0.0315</cell><cell>0.0649</cell><cell>0.0530</cell></row><row><cell>LightGCN-CCL</cell><cell>0.0528</cell><cell>0.0416</cell><cell>0.0669</cell><cell>0.0554</cell></row><row><cell cols="5">the co-design of the interaction encoder, loss function, and negative</cell></row><row><cell>sampling.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Parameter analysis results on SimpleX.</figDesc><table><row><cell>Ablations</cell><cell cols="4">Amazon-Books Recall@20 NDCG@20 Recall@20 NDCG@20 Yelp2018</cell></row><row><cell>avg_pooling</cell><cell>0.0583</cell><cell>0.0468</cell><cell>0.0701</cell><cell>0.0575</cell></row><row><cell>self_attn.</cell><cell>0.0580</cell><cell>0.0462</cell><cell>0.0698</cell><cell>0.0576</cell></row><row><cell>user_attn.</cell><cell>0.0551</cell><cell>0.0436</cell><cell>0.0698</cell><cell>0.0574</cell></row><row><cell>g = 0</cell><cell>0.0534</cell><cell>0.0429</cell><cell>0.0679</cell><cell>0.0555</cell></row><row><cell>g = 0.5</cell><cell>0.0583</cell><cell>0.0468</cell><cell>0.0688</cell><cell>0.0565</cell></row><row><cell>g = 1</cell><cell>0.0540</cell><cell>0.0432</cell><cell>0.0701</cell><cell>0.0575</cell></row><row><cell>𝑤 = 1</cell><cell>0.0163</cell><cell>0.0128</cell><cell>0.0238</cell><cell>0.0189</cell></row><row><cell>𝑤 = 150</cell><cell>0.0542</cell><cell>0.0429</cell><cell>0.0701</cell><cell>0.0575</cell></row><row><cell>𝑤 = 300</cell><cell>0.0583</cell><cell>0.0468</cell><cell>0.0666</cell><cell>0.0549</cell></row><row><cell>𝑤 = 1000</cell><cell>0.0481</cell><cell>0.0379</cell><cell>0.0568</cell><cell>0.0463</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Efficiency comparison on Amazon-Books, w.r.t. the average training time per epoch, the number of epochs to converge, and the total training time.</figDesc><table><row><cell>Model</cell><cell cols="3">Time/Epoch #Epochs Training Time</cell></row><row><cell>ENMF</cell><cell>129s</cell><cell>81</cell><cell>2h54m</cell></row><row><cell>LightGCN</cell><cell>51s</cell><cell>780</cell><cell>11h3m</cell></row><row><cell>SimpleX (|N |=100)</cell><cell>40s</cell><cell>28</cell><cell>19m</cell></row><row><cell>SimpleX (|N |=1000)</cell><cell>131s</cell><cell>35</cell><cell>1h16m</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Performence comparison to autoenoder-based models on Movielens-20M and MillionSongData. to facilitate future research on CF. The availability of this baseline would allow for more solid experimental evaluations and more fair comparisons among CF models.</figDesc><table><row><cell></cell><cell>1258</cell><cell>0.0792</cell><cell></cell><cell>NGCF</cell><cell>0.0866</cell><cell>0.0555</cell><cell></cell><cell>MF-BPR</cell><cell>0.1312</cell><cell>0.0778</cell></row><row><cell>NIA-GCN</cell><cell>0.1487</cell><cell>0.0932</cell><cell cols="2">NIA-GCN</cell><cell>0.1058</cell><cell>0.0683</cell><cell></cell><cell>NGCF</cell><cell>0.1513</cell><cell>0.0917</cell></row><row><cell>BGCF</cell><cell>0.1506</cell><cell>0.0948</cell><cell></cell><cell>BGCF</cell><cell>0.1066</cell><cell>0.0693</cell><cell></cell><cell>BGCF</cell><cell>0.1534</cell><cell>0.0912</cell></row><row><cell>SimpleX</cell><cell>0.1763</cell><cell>0.1145</cell><cell cols="2">SimpleX</cell><cell>0.1342</cell><cell>0.0926</cell><cell></cell><cell>SimpleX</cell><cell>0.1721</cell><cell>0.1028</cell></row><row><cell>RI over NIA-GCN</cell><cell>18.6%</cell><cell>22.9%</cell><cell cols="2">RI over NIA-GCN</cell><cell>26.8%</cell><cell>35.5%</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RI over BGCF</cell><cell>17.1%</cell><cell>20.8%</cell><cell cols="2">RI over BGCF</cell><cell>25.9%</cell><cell>33.6%</cell><cell cols="2">RI over BGCF</cell><cell>12.2%</cell><cell>12.8%</cell></row><row><cell cols="3">Amazon-Electronics</cell><cell></cell><cell cols="2">CiteUlike-A</cell><cell></cell><cell></cell><cell cols="2">Movielens-1M</cell></row><row><cell>Model</cell><cell cols="2">F1@20 NDCG@20</cell><cell></cell><cell>Model</cell><cell cols="3">Precision@20 Recall@20</cell><cell>Model</cell><cell>F1@20 NDCG@20</cell></row><row><cell>MF-BPR</cell><cell>0.0275</cell><cell>0.0680</cell><cell></cell><cell>NGCF</cell><cell>0.0517</cell><cell>0.0193</cell><cell></cell><cell>NGCF</cell><cell>0.1582</cell><cell>0.2511</cell></row><row><cell>NBPO</cell><cell>0.0313</cell><cell>0.0810</cell><cell></cell><cell>DHCF</cell><cell>0.0635</cell><cell>0.0249</cell><cell></cell><cell>LCFN</cell><cell>0.1625</cell><cell>0.2603</cell></row><row><cell>SimpleX</cell><cell>0.0338</cell><cell>0.0842</cell><cell cols="2">SimpleX</cell><cell>0.0754</cell><cell>0.0269</cell><cell></cell><cell>SimpleX</cell><cell>0.1658</cell><cell>0.2670</cell></row><row><cell>RI over NBPO</cell><cell>8.0%</cell><cell>4.0%</cell><cell cols="2">RI over DHCF</cell><cell>18.7%</cell><cell>8.2%</cell><cell cols="2">RI over LCFN</cell><cell>2.0%</cell><cell>2.6%</cell></row><row><cell cols="2">Model</cell><cell cols="8">Movielens-20M Recall@20 Recall@50 NDCG@100 #Params Recall@20 Recall@50 NDCG@100 #Params MillionSongData</cell></row><row><cell cols="2">SLIM</cell><cell>0.370</cell><cell>0.495</cell><cell>0.401</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Mult-VAE</cell><cell>0.395</cell><cell>0.537</cell><cell>0.426</cell><cell>24.5M</cell><cell>0.266</cell><cell>0.363</cell><cell>0.313</cell><cell>49.7M</cell></row><row><cell cols="2">EASE R</cell><cell>0.391</cell><cell>0.521</cell><cell>0.420</cell><cell>404.3M</cell><cell>0.333</cell><cell>0.428</cell><cell>0.389</cell><cell>1,692M</cell></row><row><cell cols="2">RecVAE</cell><cell>0.414</cell><cell>0.553</cell><cell>0.442</cell><cell>16.5 M</cell><cell>0.276</cell><cell>0.374</cell><cell>0.326</cell><cell>33.3M</cell></row><row><cell cols="2">Mult-VAE (d=64)</cell><cell>0.375</cell><cell>0.514</cell><cell>0.407</cell><cell>2.6M</cell><cell>0.230</cell><cell>0.319</cell><cell>0.280</cell><cell>5.3M</cell></row><row><cell cols="2">EASE R (d=64)</cell><cell>0.361</cell><cell>0.487</cell><cell>0.392</cell><cell>2.6M</cell><cell>0.170</cell><cell>0.235</cell><cell>0.205</cell><cell>5.3M</cell></row><row><cell cols="2">RecVAE (d=64)</cell><cell>0.385</cell><cell>0.520</cell><cell>0.412</cell><cell>2.6M</cell><cell>0.232</cell><cell>0.319</cell><cell>0.280</cell><cell>5.3M</cell></row><row><cell cols="2">SimpleX (d=64)</cell><cell>0.389</cell><cell>0.523</cell><cell>0.416</cell><cell>1.3M</cell><cell>0.245</cell><cell>0.329</cell><cell>0.293</cell><cell>2.6M</cell></row><row><cell cols="2">RI over Mult-VAE</cell><cell>3.8%</cell><cell>1.7%</cell><cell>2.3%</cell><cell></cell><cell>6.5%</cell><cell>3.2%</cell><cell>4.7%</cell></row><row><cell cols="2">RI over EASE R</cell><cell>7.8%</cell><cell>7.4%</cell><cell>6.2%</cell><cell></cell><cell>44.0%</cell><cell>40.3%</cell><cell>43.3%</cell></row><row><cell cols="2">RI over RecVAE</cell><cell>1.1%</cell><cell>0.6%</cell><cell>1.1%</cell><cell></cell><cell>5.8%</cell><cell>3.2%</cell><cell>4.8%</cell></row><row><cell>new baseline model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://openbenchmark.github.io/collaborative-filtering</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGEMENTS</head><p>This work was supported in part by the National Natural Science Foundation of China (61972219), the Research and Development Program of Shenzhen (JCYJ20190813174403598, SGDX20190918101201-696), the National Key Research and Development Program of China (2018YFB1800601), and the Overseas Research Cooperation Fund of Tsinghua Shenzhen International Graduate School (HW2021013).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Graph Convolutional Matrix Completion</title>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;18 Deep Learning Day</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient Neural Matrix Factorization without Sampling for Recommendation</title>
		<author>
			<persName><forename type="first">Chong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attentive Collaborative Filtering: Multimedia Recommendation with Item-and Component-Level Attention</title>
		<author>
			<persName><forename type="first">Jingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</title>
				<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="335" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Revisiting Graph Based Collaborative Filtering: A Linear Residual Graph Convolutional Network Approach</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
				<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep Neural Networks for YouTube Recommendations</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Sargin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM conference on Recommender Systems (RecSys)</title>
				<meeting>the 10th ACM conference on Recommender Systems (RecSys)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reinforced Negative Sampling for Recommendation with Exposure Data</title>
		<author>
			<persName><forename type="first">Jingtao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhan</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Depeng</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI)</title>
				<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2230" to="2236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Collaborative Memory Network for Recommendation Systems</title>
		<author>
			<persName><forename type="first">Travis</forename><surname>Ebesu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval (SIGIR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="515" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dimensionality Reduction by Learning an Invariant Mapping</title>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong-Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval (SIGIR)</title>
				<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web (WWW)</title>
				<meeting>the 26th International Conference on World Wide Web (WWW)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Collaborative Metric Learning</title>
		<author>
			<persName><forename type="first">Cheng-Kang</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deborah</forename><surname>Estrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web (WWW)</title>
				<meeting>the 26th International Conference on World Wide Web (WWW)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="193" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Collaborative Filtering for Implicit Feedback Datasets</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th IEEE International Conference on Data Mining (ICDM)</title>
				<meeting>the 8th IEEE International Conference on Data Mining (ICDM)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dual Channel Hypergraph Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Shuyi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xibin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanwan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD)</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2020" to="2029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Matrix Factorization Techniques for Recommender Systems</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Variational Autoencoders for Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Rahul G Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><surname>Jebara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference</title>
				<meeting>the 2018 World Wide Web Conference</meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="689" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Disentangled Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4212" to="4221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning Disentangled Representations for Recommendation</title>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5711" to="5722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Deep Recurrent Collaborative Filtering Framework for Venue Recommendation</title>
		<author>
			<persName><forename type="first">Jarana</forename><surname>Manotumruksa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management (CIKM)</title>
				<meeting>the 2017 ACM on Conference on Information and Knowledge Management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1429" to="1438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SLIM: Sparse Linear Methods for Top-N Recommender Systems</title>
		<author>
			<persName><forename type="first">Xia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th International Conference on Data Mining (ICDM)</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="497" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">BPR: Bayesian Personalized Ranking from Implicit Feedback</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence (UAI)</title>
				<meeting>the 25th Conference on Uncertainty in Artificial Intelligence (UAI)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">RecVAE: A New Variational Autoencoder for Top-N Recommendations with Implicit Feedback</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Shenbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Alekseev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Tutubalina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Malykh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><forename type="middle">I</forename><surname>Nikolenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirteenth ACM International Conference on Web Search and Data Mining (WSDM)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="528" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Two Decades of Recommender Systems at Amazon.com</title>
		<author>
			<persName><forename type="first">Brent</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Linden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Comput</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="12" to="18" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Jinbo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinbo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12256</idno>
		<title level="m">NGAT4Rec: Neighbor-Aware Graph Attention Network For Recommendation</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Embarrassingly Shallow Autoencoders for Sparse Data</title>
		<author>
			<persName><forename type="first">Harald</forename><surname>Steck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference (WWW)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3251" to="3257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Survey of Collaborative Filtering Techniques</title>
		<author>
			<persName><forename type="first">Xiaoyuan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taghi</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<idno>421425:1-421425:19</idno>
	</analytic>
	<monogr>
		<title level="j">Adv. Artif. Intell</title>
		<imprint>
			<date type="published" when="2009">2009. 2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhua</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management (CIKM)</title>
				<meeting>the 28th ACM International Conference on Information and Knowledge Management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1441" to="1450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A Framework for Recommending Accurate and Diverse Items Using Bayesian Graph Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Jianing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dengcheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingxue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florence</forename><surname>Regol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaochen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD)</title>
				<meeting>ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2030" to="2039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neighbor Interaction Aware Graph Convolution Networks for Recommendation</title>
		<author>
			<persName><forename type="first">Jianing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingxue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Coates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</title>
				<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1289" to="1298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural Graph Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</title>
				<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Disentangled Graph Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongye</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</title>
				<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1001" to="1010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">NPA: Neural News Recommendation with Personalized Attention</title>
		<author>
			<persName><forename type="first">Chuhan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxiao</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD)</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2576" to="2584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Self-supervised Graph Learning for Recommendation</title>
		<author>
			<persName><forename type="first">Jiancan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</title>
				<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="726" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mixed Negative Sampling for Learning Two-tower Neural Networks in Recommendations</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">Zhiyuan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">Xiaoming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taibai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Proceedings of the Web Conference (WWW)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="441" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">HOP-Rec: High-order Proximity for Implicit Recommendation</title>
		<author>
			<persName><forename type="first">Jheng-Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan-Ju</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Feng</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM Conference on Recommender Systems (RecSys)</title>
				<meeting>the 12th ACM Conference on Recommender Systems (RecSys)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="140" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adaptive Semantic-Visual Tree for Hierarchical Embeddings</title>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia (MM)</title>
				<meeting>the 27th ACM International Conference on Multimedia (MM)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2097" to="2105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graph Convolutional Neural Networks for Web-scale Recommender Systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD)</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graph Convolutional Network for Recommendation with Low-pass Collaborative Filters</title>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10936" to="10945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sampler Design for Implicit Feedback Data by Noisy-label Robust Learning</title>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</title>
				<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="861" to="870" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
