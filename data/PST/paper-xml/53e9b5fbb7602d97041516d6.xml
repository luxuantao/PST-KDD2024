<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A SPARSE GRID STOCHASTIC COLLOCATION METHOD FOR PARTIAL DIFFERENTIAL EQUATIONS WITH RANDOM INPUT DATA *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">F</forename><surname>Nobile</surname></persName>
							<email>fabio.nobile@polimi.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Matematica &quot;F. Brioschi</orgName>
								<orgName type="laboratory">MOX</orgName>
								<orgName type="institution">Politecnico di Milano</orgName>
								<address>
									<postCode>20133</postCode>
									<settlement>Milano</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">R</forename><surname>Tempone</surname></persName>
							<email>rtempone@scs.fsu.edu</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Mathematics and School of Computational Science</orgName>
								<orgName type="department" key="dep2">School of Computer Sciences and Commu-nication</orgName>
								<orgName type="institution">Florida State University</orgName>
								<address>
									<addrLine>400 Dirac Science Library</addrLine>
									<postCode>32306-4120</postCode>
									<settlement>Tallahassee</settlement>
									<region>FL</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">KTH</orgName>
								<address>
									<postCode>S-100 44</postCode>
									<settlement>Stockholm</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Webster</surname></persName>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Optimization and Uncertainty Quantification Department</orgName>
								<orgName type="department" key="dep2">Sandia National Laboratories</orgName>
								<address>
									<postBox>P.O. Box 5800</postBox>
									<postCode>87185-1318</postCode>
									<settlement>Albuquerque</settlement>
									<region>NM</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A SPARSE GRID STOCHASTIC COLLOCATION METHOD FOR PARTIAL DIFFERENTIAL EQUATIONS WITH RANDOM INPUT DATA *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A168B856082FB38E458E90138D638512</idno>
					<idno type="DOI">10.1137/060663660</idno>
					<note type="submission">Received by the editors June 26, 2006; accepted for publication (in revised form) February 4, 2008;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>collocation techniques</term>
					<term>stochastic PDEs</term>
					<term>finite elements</term>
					<term>uncertainty quantification</term>
					<term>sparse grids</term>
					<term>Smolyak approximation</term>
					<term>multivariate polynomial approximation AMS subject classifications. 65N30</term>
					<term>65N35</term>
					<term>65N12</term>
					<term>65N15</term>
					<term>65C20</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work proposes and analyzes a Smolyak-type sparse grid stochastic collocation method for the approximation of statistical quantities related to the solution of partial differential equations with random coefficients and forcing terms (input data of the model). To compute solution statistics, the sparse grid stochastic collocation method uses approximate solutions, produced here by finite elements, corresponding to a deterministic set of points in the random input space. This naturally requires solving uncoupled deterministic problems as in the Monte Carlo method. If the number of random variables needed to describe the input data is moderately large, full tensor product spaces are computationally expensive to use due to the curse of dimensionality. In this case the sparse grid approach is still expected to be competitive with the classical Monte Carlo method. Therefore, it is of major practical relevance to understand in which situations the sparse grid stochastic collocation method is more efficient than Monte Carlo. This work provides error estimates for the fully discrete solution using L q norms and analyzes the computational efficiency of the proposed method. In particular, it demonstrates algebraic convergence with respect to the total number of collocation points and quantifies the effect of the dimension of the problem (number of input random variables) in the final estimates. The derived estimates are then used to compare the method with Monte Carlo, indicating for which problems the former is more efficient than the latter. Computational evidence complements the present theory and shows the effectiveness of the sparse grid stochastic collocation method compared to full tensor and Monte Carlo approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Introduction. Mathematical modeling and computer simulations are nowadays widely used tools to predict the behavior of physical and engineering problems. Whenever a particular application is considered, the mathematical models need to be equipped with input data, such as coefficients, forcing terms, boundary conditions, geometry, etc. However, in many applications, such input data may be affected by a relatively large amount of uncertainty. This can be due to an intrinsic variability in the physical system, as, for instance, in the mechanical properties of many biomaterials, polymeric fluids, and composite materials or the action of wind or seismic vibrations on civil structures.</p><p>In other situations, uncertainty may come from our difficulty in characterizing accurately the physical system under investigation, as in the study of groundwater flows, where the subsurface properties such as porosity and permeability in an aquifer have to be extrapolated from measurements taken only in a few spatial locations.</p><p>Such uncertainties can be included in the mathematical model adopting a probabilistic setting, provided enough information is available for a complete statistical characterization of the physical system. In this framework, the input data are modeled as random variables or, more generally, as random fields with a given spatial (or temporal) correlation structure.</p><p>Therefore, the goal of the mathematical and computational analysis becomes the prediction of statistical moments of the solution (mean value, variance, covariance, etc.) or statistics of some given responses of the system (sometimes also called quantities of physical interest which are real valued functionals of the solution), given the probability distribution of the input random data. Examples of quantities of interest could be the solution values in a given region, fluxes across given boundaries, etc.</p><p>In order to parametrize the input data for a given partial differential equation (PDE), random fields that are either coefficients or loads can often be expanded as an infinite combination of random variables by, for instance, the so-called Karhunen-Loève <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> or polynomial chaos (PC) expansions <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b38">38]</ref>. Although such random fields are properly described only by means of an infinite number of random variables, whenever the realizations are slowly varying in space, with a correlation length comparable to the size of the domain, only a few terms in the above-mentioned expansions are typically needed to describe the random field with sufficient accuracy. Therefore, in this case, it is reasonable to limit the analysis to just a few random variables in the expansion (see, e.g., <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref>).</p><p>In this work we focus on PDEs whose coefficients and forcing terms are described by a finite-dimensional random vector. This is often called finite-dimensional noise assumption and may hold either because the problem itself can be described by a finite number of random variables or because the input coefficients are modeled as truncated random fields.</p><p>The most popular approach to solving mathematical problems in a probabilistic setting is the Monte Carlo method (see, e.g., <ref type="bibr" target="#b14">[15]</ref> and the references therein). The Monte Carlo method is easy to implement and allows one to reuse available deterministic codes. However, the convergence rate is typically very slow, although with a mild dependence on the number on sampled random variables.</p><p>In the last few years, other approaches have been proposed, which in certain situations feature a much faster convergence rate. We mention, among others, the spectral Galerkin method <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28]</ref>, stochastic collocation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b36">36]</ref>, and perturbation methods or Neumann expansions <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b31">31]</ref>.</p><p>For certain classes of problems, the solution may have a very regular dependence on the input random variables. For instance, it was shown in <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b3">[4]</ref> that the solution of a linear elliptic PDE with diffusivity coefficient and/or forcing term described as truncated expansions of random fields is analytic in the input random variables. In such situations, spectral Galerkin or stochastic collocation methods based on orthogonal tensor product polynomials feature a very fast convergence rate.</p><p>In particular, our earlier work <ref type="bibr" target="#b2">[3]</ref> proposed a stochastic collocation/finite element Downloaded 12/30/12 to 139.184. <ref type="bibr">30.136</ref>. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php method based on standard finite element approximations in space and a collocation on a tensor grid built upon the zeros of orthogonal polynomials with respect to the joint probability density function of the input random variables. It was shown that for an elliptic PDE the error converges exponentially fast with respect to the number of points employed for each random input variable.</p><p>The stochastic collocation method can be easily implemented and leads naturally to the solution of uncoupled deterministic problems as in the Monte Carlo method, even in the presence of input data which depend nonlinearly on the driving random variables. It can also treat efficiently the case of nonindependent random variables with the introduction of an auxiliary density and can handle, for instance, cases with log-normal diffusivity coefficient, which is not bounded in Ω×D but has bounded realizations. When the number of input random variables is small, stochastic collocation is a very effective numerical tool.</p><p>On the other hand, approximations based on tensor product grids suffer from the curse of dimensionality since the number of collocation points in a tensor grid grows exponentially fast in the number of input random variables.</p><p>If the number of random variables is moderately large, one should rather consider sparse tensor product spaces as first proposed by Smolyak <ref type="bibr" target="#b28">[29]</ref> and further investigated by, e.g., <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b36">36]</ref>, which will be the primary focus of this paper. It is natural to expect that the use of sparse grids will reduce dramatically the number of collocation points, while preserving a high level of accuracy and thus being able to successfully compete with Monte Carlo. Our main purpose is to clarify the limitations of the previous statement and to understand in which situations the sparse grid stochastic collocation method is more efficient than Monte Carlo.</p><p>Motivated by the above, this work proposes and analyzes a Smolyak-type sparse grid stochastic collocation method for the approximation of statistical quantities related to the solution of PDEs whose input data are described through a finite number of random variables. The sparse tensor product grids are built upon either Clenshaw-Curtis <ref type="bibr" target="#b10">[11]</ref> or Gaussian abscissas. After outlining the method, this work provides strong error estimates for the fully discrete solution and analyzes its computational efficiency. In particular, it proves algebraic convergence with respect to the total number of collocation points or, equivalently, the total computational work which is directly proportional to the number of collocation points. The exponent of such algebraic convergence is connected to both the regularity of the solution and the number of input random variables, N , and essentially deteriorates with N by a 1/ log(N ) factor. Then these error estimates are used to compare the method with the standard Monte Carlo method, indicating for which problems the former is more efficient than the latter.</p><p>Moreover, this work addresses the case where the input random variables come from suitably truncated expansions of random fields. There it discusses how to relate the number of points in the sparse grid to the number of random variables retained in the truncated expansion in order to balance discretization error with truncation error in the input random fields. Computational evidence complements the present theory and shows the effectiveness of the sparse grid stochastic collocation method. It also includes a comparison with full tensor and Monte Carlo methods.</p><p>The outline of the work is the following: Section 1 introduces the mathematical problem and basic notation and states a regularity assumption to be used later in the error analysis. Clenshaw-Curtis and Gaussian, that will be employed in the sparse approximation method.</p><p>Section 3 is the core of the work. We first develop strong error estimates for the fully discrete solution using L ∞ P and L 2 P norms for Clenshaw-Curtis and Gaussian abscissas, respectively (P being the probability measure considered). These norms control the error in the approximation of expected values of smooth functionals of the solution. Then in section 3.2 these error estimates are used to compare the method with the standard Monte Carlo method, explaining cases where the former is more efficient than the latter.</p><p>Sections 4 and 5 focus on applications to linear elliptic PDEs with random input data. In section 4 we verify that the assumptions under which our general theory works hold in this particular case. Then we present in section 5 some numerical results showing the effectiveness of the proposed method when compared to the full tensor and Monte Carlo methods.</p><p>1. Problem setting. We begin by focusing our attention on a differential operator L, linear or nonlinear, on a domain D ⊂ R d , which depends on some coefficients a(ω, x) with x ∈ D, ω ∈ Ω, where (Ω, F, P ) is a complete probability space. Here Ω is the set of outcomes, F ⊂ 2 Ω is the σ-algebra of events, and P : F → [0, 1] is a probability measure. Similarly the forcing term f = f (ω, x) can be assumed random as well.</p><p>Consider the stochastic boundary value problem: find a random function, u : Ω × D → R, such that P -a.e. in Ω, or in other words almost surely (a.s.), the following equation holds:</p><formula xml:id="formula_0">(1.1) L(a)(u) = f in D</formula><p>equipped with suitable boundary conditions. Before introducing some assumptions we denote by W (D) a Banach space of functions v : D → R and define, for q ∈ [1, ∞], the stochastic Banach spaces</p><formula xml:id="formula_1">L q P (Ω; W (D)) = v : Ω → W (D) | v is strongly measurable and Ω v(ω, •) q W (D) dP (ω) &lt; +∞ and L ∞ P (Ω; W (D)) = v : Ω → W (D) | v is strongly measurable and P -ess sup ω∈Ω v(ω, •) 2 W (D) &lt; +∞ .</formula><p>Of particular interest is the space L 2 P (Ω; W (D)), consisting of Banach valued functions that have finite second moments.</p><p>We will now make the following assumptions: (A 1 ) The solution to (1.1) has realizations in the Banach space W (D), i.e., u(•, ω) ∈ W (D) a.s. and ∀ω ∈ Ω </p><formula xml:id="formula_2">u(•, ω) W (D) ≤ C f (•, ω) W * (D) ,</formula><formula xml:id="formula_3">-∇ • (a(ω, •)∇u(ω, •)) = f (ω, •) in Ω× D, u(ω, •) = 0 on Ω × ∂D,</formula><p>with a(ω, •) uniformly bounded and coercive, i.e., there exists a min , a max ∈ (0, +∞) such that</p><formula xml:id="formula_4">P (ω ∈ Ω : a(ω, x) ∈ [a min , a max ] ∀x ∈ D) = 1</formula><p>and f (ω, •) square integrable with respect to P , satisfies assumptions A 1 and A 2 with W (D) = H 1 0 (D) (see <ref type="bibr" target="#b2">[3]</ref>). Example 1.2. Similarly, for k ∈ N + , the nonlinear problem</p><formula xml:id="formula_5">(1.3) -∇ • (a(ω, •)∇u(ω, •)) + u(ω, •) 2k+1 = f (ω, •) in Ω× D, u(ω, •) = 0 on Ω × ∂D,</formula><p>with a(ω, •) uniformly bounded and coercive and f (ω, •) square integrable with respect to P , satisfies assumptions A 1 and A 2 with W (D) = H 1 0 (D) ∩ L 2k+2 (D). Remark 1.3 (goals of the computation). As said in the introduction, the goal of the mathematical and computational analysis is the prediction of statistical moments of the solution u to (1.1) (mean value, variance, covariance, etc.) or statistics of some given quantities of physical interest ψ(u). Examples of quantities of interest could be the average value of the solution in a given region</p><formula xml:id="formula_6">D c ⊂ D, ψ(u) = 1 |D c | Dc udx,</formula><p>and similarly average fluxes on a given direction n ∈ R d . In the case of Examples 1.1 and 1.2 these fluxes can be written as</p><formula xml:id="formula_7">ψ(u) = 1 |D c | Dc a ∂u ∂n dx.</formula><p>1.1. On finite-dimensional noise. In some applications, the coefficient a and the forcing term f appearing in (1.1) can be described by a random vector [Y 1 , . . . , Y N ] : Ω → R N , as in the following examples. In such cases, we will emphasize such dependence by writing a N and f N .</p><p>Example 1.4 (piecewise constant random fields). Let us consider again problem (1.2), where the physical domain D is the union of nonoverlapping subdomains D i , i = 1, . . . , N. We consider a diffusion coefficient that is piecewise constant and random on each subdomain, i.e., In other applications the coefficients and forcing terms in (1.1) may have another type of spatial variation that is amenable to being described by an expansion. Depending on the decay of such an expansion and the desired accuracy in our computations we may retain just the first N terms.</p><formula xml:id="formula_8">a N (ω, x) = a min + N i=1 σ i Y i (ω)1 Di (x).</formula><p>Example 1.5 (Karhunen-Loève expansion). We recall that any second order random field g(ω, x), with continuous covariance function cov[g] : D × D → R, can be represented as an infinite sum of random variables by means, for instance, of a Karhunen-Loève expansion <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. To this end, introduce the compact and selfadjoint operator T g : L 2 (D) → L 2 (D), which is defined by</p><formula xml:id="formula_9">T g v(•) := D cov[g](x, •) v(x) dx ∀v ∈ L 2 (D).</formula><p>Then consider the sequence of nonnegative decreasing eigenvalues of T g , {λ i } ∞ i=1 , and the corresponding sequence of orthonormal eigenfunctions, {b i } ∞ i=1 , satisfying</p><formula xml:id="formula_10">T g b i = λ i b i , (b i , b j ) L 2 (D) = δ ij for i, j ∈ N + .</formula><p>In addition, define mutually uncorrelated real random variables</p><formula xml:id="formula_11">Y i (ω) := 1 √ λ i D (g(ω, x) -E[g](x)) b i (x)dx, i = 1, . . . ,</formula><p>with zero mean and unit variance, i.e.,</p><formula xml:id="formula_12">E[Y i ] = 0 and E[Y i Y j ] = δ ij for i, j ∈ N + .</formula><p>The truncated Karhunen-Loève expansion g N , of the stochastic function g, is defined by</p><formula xml:id="formula_13">g N (ω, x) := E[g](x) + N i=1 λ i b i (x) Y i (ω) ∀N ∈ N + .</formula><p>Then by Mercer's theorem (cf. <ref type="bibr">[27, p. 245]</ref>), it follows that lim</p><formula xml:id="formula_14">N →∞ sup D E (g -g N ) 2 = lim N →∞ sup D ∞ i=N +1 λ i b 2 i = 0.</formula><p>Observe that the N random variables in (1.5), describing the random data, are then weighted differently due to the decay of the eigenpairs of the Karhunen-Loève expansion. The decay of eigenvalues and eigenvectors has been investigated, e.g., in the works <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b31">[31]</ref>.</p><p>The above examples motivate us to consider problems whose coefficients are described by finitely many random variables. Thus, we will seek a random field u N : Ω × D → R, such that a.s. the following equation holds:</p><formula xml:id="formula_15">(1.4) L(a N )(u N ) = f N in D.</formula><p>We assume that (1.4) admits a unique solution u N ∈ L 2 P (Ω; W (D)). Therefore, following the same argument as in <ref type="bibr">[3, p. 1010</ref>] yields that the solution u N of the stochastic boundary value problem (1.4) can be described by the [Y 1 , . . . , Y N ] random variables, i.e., u</p><formula xml:id="formula_16">N = u N (ω, x) = u N (Y 1 (ω), . . . , Y N (ω), x).</formula><p>We underline that the coefficients a N and f N in (1.4) may be an exact representation of the input data as in Example 1.4 or a suitable truncation of the input data Downloaded 12/30/12 to 139.184.30.136. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php as in Example 1.5. In the latter case, the solution u N will also be an approximation of the exact solution u in (1.1) and the truncation error uu N has to be properly estimated (see section 3.2).</p><p>Remark 1.6 (nonlinear coefficients). In certain cases, one may need to ensure qualitative properties on the coefficients a N and f N , and it may be worthwhile describing them as nonlinear functions of Y . For instance, in Example 1.1 one is required to enforce positiveness on the coefficient a N (ω, x), say a N (ω, x) ≥ a min ∀x ∈ D, a.s. in Ω. Then a better choice is to expand log(a Na min ). The following standard transformation guarantees that the diffusivity coefficient is bounded away from zero a.s.:</p><formula xml:id="formula_17">(1.5) log(a N -a min )(ω, x) = b 0 (x) + 1≤n≤N λ n b n (x)Y n (ω),</formula><p>i.e., one performs a Karhunen-Loève expansion for log(a Na min ), assuming that a N &gt; a min a.s. On the other hand, the right-hand side of (1.4) can be represented as a truncated Karhunen-Loève expansion</p><formula xml:id="formula_18">f N (ω, x) = c 0 (x) + 1≤n≤N √ μ n c n (x)Y n (ω).</formula><p>For </p><formula xml:id="formula_19">ρ : Γ N → R + , with ρ ∈ L ∞ (Γ N ).</formula><p>Thus, the plan is to approximate the function u N = u N (y, x) for any y ∈ Γ N and x ∈ D (see <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>). Remark 1.7 (unbounded random variables). By using a similar approach to the work <ref type="bibr" target="#b2">[3]</ref> we can easily deal with unbounded random variables, such as Gaussian or exponential ones. For the sake of simplicity in the presentation we focus our study on bounded random variables only.</p><p>The convergence properties of the collocation techniques that will be developed in the next section depend on the regularity that the solution u N has with respect to y. Denote Γ * n = N j=1 j =n Γ j , and let y * n be an arbitrary element of Γ * n . Here we require the solution to problem (1.1) to satisfy the following assumption.</p><p>Assumption 1.8 (regularity). For each y n ∈ Γ n , there exists τ n &gt; 0 such that the function u N (y n , y * n , x) as a function of y n , u N : Γ n → C 0 (Γ * n ; W (D)) admits an analytic extension u(z, y * n , x), z ∈ C, in the region of the complex plane</p><formula xml:id="formula_20">(1.7) Σ(Γ n ; τ n ) ≡ {z ∈ C, dist(z, Γ n ) ≤ τ n }. Moreover, ∀z ∈ Σ(Γ n ; τ n ), (1.8) u N (z) C 0 (Γ * n ;W (D)) ≤ λ with λ a constant independent of n.</formula><p>This assumption is sound in several problems; in particular, it can be verified for the linear problem that will be analyzed in section 4. In the more general case, this assumption should be verified for each particular application and will have implications on the allowed regularity of the input data, e.g., coefficients, loads, etc., of the stochastic PDE under study. See also Remark 3.14 for related results based on fewer regularity requirements. Downloaded 12/30/12 to 139.184.30.136. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php 2. Collocation techniques. We seek a numerical approximation to the exact solution of (1.4) in a suitable finite-dimensional subspace. To describe such a subspace properly, we introduce some standard approximation subspaces:</p><p>• W h (D) ⊂ W (D) is a standard finite element space of dimension N h , which contains continuous piecewise polynomials defined on regular triangulations T h that have a maximum mesh-spacing parameter h &gt; 0. We suppose that W h has the following deterministic approximation property: for a given function ϕ ∈ W (D),</p><p>(2.1) min</p><formula xml:id="formula_21">v∈W h (D) ϕ -v W (D) ≤ C(s; ϕ) h s ,</formula><p>where s is a positive integer determined by the smoothness of ϕ and the degree of the approximating finite element subspace, and C(s; ϕ) is independent of h. Example 2.1. Let D be a convex polygonal domain and W (D) = H 1 0 (D). For piecewise linear finite element subspaces we have min</p><formula xml:id="formula_22">v∈W h (D) ϕ -v H 1 0 (D) ≤ c h ϕ H 2 (D) .</formula><p>That is, s = 1 and C(s; ϕ) = c ϕ H 2 (D) ; see, for example, <ref type="bibr" target="#b6">[7]</ref>.</p><p>We will also assume that there exists a finite element operator</p><formula xml:id="formula_23">π h : W (D) → W h (D) with the optimality condition (2.2) ϕ -π h ϕ W (D) ≤ C π min v∈W h (D) ϕ -v W (D) ∀ϕ ∈ W (D),</formula><p>where the constant C π is independent of the mesh size h. It is worth noticing that in general the operator π h will depend on the specific problem, as well as on y, i.e., π h = π h (y).</p><formula xml:id="formula_24">• P p (Γ N ) ⊂ L 2 ρ (Γ N</formula><p>) is the span of tensor product polynomials with degree at most p = (p 1 , . . . , p N ), i.e., P p (Γ N ) =</p><formula xml:id="formula_25">N n=1 P pn (Γ n ), with P pn (Γ n ) = span(y k n , k = 0, . . . , p n ), n = 1, . . . , N.</formula><p>Hence the dimension of</p><formula xml:id="formula_26">P p (Γ N ) is N p = N n=1 (p n + 1).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stochastic collocation entails the evaluation of approximate values π</head><formula xml:id="formula_27">h u N (y k ) = u N h (y k ) ∈ W h (D) to the solution u N of (1.4) on a suitable set of points y k ∈ Γ N . Then the fully discrete solution u N h,p ∈ C 0 (Γ N ; W h (D)</formula><p>) is a global approximation (sometimes an interpolation) constructed by linear combinations of the point values. That is,</p><formula xml:id="formula_28">(2.3) u N h,p (y, •) = k∈K u N h (y k , •)l p k (y),</formula><p>where, for instance, the functions l p k can be taken as the Lagrange polynomials (see sections 2.1 and 2.2). This formulation can be used to compute the mean value or variance of u, as described in [3, section 2], or to approximate expected values of functionals ψ(u) (cf. Remark 1.3) by</p><formula xml:id="formula_29">E[ψ(u)] ≈ E[ψ(u N h,p )] ≈ k∈K ψ(u N h (y k )) E[l p k ].</formula><p>In the next sections we consider different choices of the evaluation points y k and corresponding weights E[l p k ] in the associated quadrature formula. Downloaded 12/30/12 to 139.184.30.136. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php 2.1. Full tensor product interpolation. In this section we briefly recall interpolation based on Lagrange polynomials. We first introduce an index i ∈ N + , i ≥ 1. Then, for each value of i, let {y i 1 , . . . , y i mi } ⊂ [-1, 1] be a sequence of abscissas for Lagrange interpolation on [-1, 1].</p><p>For u ∈ C 0 (Γ 1 ; W (D)) and N = 1 we introduce a sequence of one-dimensional Lagrange interpolation operators</p><formula xml:id="formula_30">U i : C 0 (Γ 1 ; W (D)) → V mi (Γ 1 ; W (D)), (2.4) U i (u)(y) = mi j=1 u(y i j ) l i j (y) ∀u ∈ C 0 (Γ 1 ; W (D)),</formula><p>where</p><formula xml:id="formula_31">l i j ∈ P mi-1 (Γ 1 ) are the Lagrange polynomials of degree m i -1, i.e., l i j (y) = mi k=1 k =j (y-y i k ) (y i j -y i k )</formula><p>, and</p><formula xml:id="formula_32">V m (Γ 1 ; W (D)) = v ∈ C 0 (Γ 1 ; W (D)) : v(y, x) = m k=1 v k (x)l k (y), { v k } m k=1 ∈ W (D) .</formula><p>Formula (2.4) reproduces exactly all polynomials of degree less than m i . Now, in the multivariate case N &gt; 1, for each u ∈ C 0 (Γ N ; W (D)) and the multi-index i = (i 1 , . . . , i N ) ∈ N N + we define the full tensor product interpolation formulas (2.5)</p><formula xml:id="formula_33">I N i u(y) = U i1 ⊗ • • • ⊗ U i N (u)(y) = mi 1 j1=1 • • • mi N j N =1 u y i1 j1 , . . . , y i N j N l i1 j1 ⊗ • • • ⊗ l i N j N .</formula><p>Clearly, the above product needs N n=1 m in function evaluations. These formulas will also be used as the building blocks for the Smolyak method, described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Smolyak approximation.</head><p>Here we follow closely the work <ref type="bibr" target="#b5">[6]</ref> and describe the Smolyak isotropic formulas A (w, N). The Smolyak formulas are just linear combinations of product formulas (2.5) with the following key properties: only products with a relatively small number of points are used. With U 0 = 0 and for i ∈ N + define (2.6)</p><formula xml:id="formula_34">Δ i := U i -U i-1 .</formula><p>Moreover, given an integer w ∈ N + , hereafter called the level, we define the sets</p><formula xml:id="formula_35">X(w, N) := i ∈ N N + , i ≥ 1 : N n=1 (i n -1) ≤ w , (2.7a) X(w, N) := i ∈ N N + , i ≥ 1 : N n=1 (i n -1) = w , (2.7b) Y (w, N) := i ∈ N N + , i ≥ 1 : w -N + 1 ≤ N n=1 (i n -1) ≤ w , (2.7c) and for i ∈ N N + we set |i| = i 1 + • • • + i N .</formula><p>Then the isotropic Smolyak formula is given by Equivalently, formula (2.8) can be written as (see <ref type="bibr" target="#b33">[33]</ref>)</p><formula xml:id="formula_36">(2.8) A (w, N) = i∈X(w,N ) Δ i1 ⊗ • • • ⊗ Δ i N . Downloaded 12/</formula><formula xml:id="formula_37">(2.9) A (w, N) = i∈Y (w,N ) (-1) w+N -|i| N -1 w + N -|i| • U i1 ⊗ • • • ⊗ U i N .</formula><p>To compute A (w, N)(u), one only needs to know function values on the "sparse grid"</p><p>(2.10)</p><formula xml:id="formula_38">H (w, N) = i∈Y (w,N ) ϑ i1 × • • • × ϑ i N ⊂ [-1, 1] N ,</formula><p>where</p><formula xml:id="formula_39">ϑ i = y i 1 , . . . , y i mi ⊂ [-1, 1] denotes the set of abscissas used by U i . If the sets are nested, i.e., ϑ i ⊂ ϑ i+1 , then H (w, N) ⊂ H (w + 1, N) and (2.11) H (w, N) = i∈ X(w,N ) ϑ i1 × • • • × ϑ i N .</formula><p>The Smolyak formula is actually interpolatory whenever nested points are used. This result has been proved in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr">Proposition 6,</ref><ref type="bibr">p. 277]</ref>. By comparing (2.11) and (2.10), we observe that the Smolyak approximation that employs nested points requires fewer function evaluations than the corresponding formula with nonnested points. In the next section we introduce two particular sets of abscissas, nested and nonnested, respectively. Also, Figure <ref type="figure" target="#fig_0">1</ref> shows, as an example, the sparse grid H (5, 2) obtained in those two cases. Note that the Smolyak approximation formula, as presented in this section, is isotropic, since all directions are treated equally. This can be seen from (2.8) observing that if a multi-index i = (i 1 , i 2 , . . . , i N ) belongs to the set X(w, N), then any permutation of i also belongs to X(w, N) and contributes to the construction of the Smolyak approximation A (w, N). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Choice of interpolation abscissas.</head><p>Clenshaw-Curtis abscissas. We first suggest using Clenshaw-Curtis abscissas (see <ref type="bibr" target="#b10">[11]</ref>) in the construction of the Smolyak formula. These abscissas are the extrema of Chebyshev polynomials and, for any choice of m i &gt; 1, are given by (2.12)</p><formula xml:id="formula_40">y i j = -cos π(j -1) m i -1 , j = 1, .</formula><p>. . , m i . Downloaded 12/30/12 to 139.184.30.136. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p><p>In addition, one sets y i 1 = 0 if m i = 1 and lets the number of abscissas m i in each level grow according to the following formula:</p><p>(2.13)</p><formula xml:id="formula_41">m 1 = 1 and m i = 2 i-1 + 1 for i &gt; 1.</formula><p>With this particular choice, one obtains nested sets of abscissas, i.e., ϑ i ⊂ ϑ i+1 and thereby H (w, N) ⊂ H (w + 1, N). It is important to choose m 1 = 1 if we are interested in optimal approximation in relatively large N , because in all other cases the number of points used by A (w, N) increases too fast with N .</p><p>Gaussian abscissas. We also propose using Gaussian abscissas, i.e., the zeros of the orthogonal polynomials with respect to some positive weight. However, these Gaussian abscissas are in general not nested. Regardless, as in the Clenshaw-Curtis case, we choose the number m i of abscissas that are used by U i as in <ref type="bibr">(2.13)</ref>. See the work <ref type="bibr" target="#b32">[32]</ref> for an insightful comparison of quadrature formulas based on Clenshaw-Curtis and Gaussian abscissas. The natural choice of the weight should be the probability density function ρ of the random variables Y i (ω), i = 1, . . . , N. Yet, in the general multivariate case, if the random variables Y i are not independent, the density ρ does not factorize, i.e., ρ(y 1 , . . . , y n ) = N n=1 ρ n (y n ). To this end, we first introduce an auxiliary probability density function ρ : Γ N → R + that can be seen as the joint probability of N independent random variables, i.e., it factorizes as</p><formula xml:id="formula_42">(2.14) ρ(y 1 , . . . , y n ) = N n=1 ρn (y n ) ∀y ∈ Γ N and is such that ρ ρ L ∞ (Γ N ) &lt; ∞.</formula><p>For each dimension n = 1, . . . , N, let the m n Gaussian abscissas be the roots of the m n degree polynomial that is ρn -orthogonal to all polynomials of degree m n -1 on the interval [-1, 1]. The auxiliary density ρ should be chosen as close as possible to the true density ρ, so as to have the quotient ρ/ρ not too large. Indeed, such a quotient will appear in the final error estimate (see section 3.1.2).</p><p>Examples of isotropic sparse grids, constructed from the nested Clenshaw-Curtis abscissas and the nonnested Gaussian abscissas, are shown in Figure <ref type="figure" target="#fig_0">1</ref>. There, we consider a two-dimensional parameter space and a maximum level w = 5 (sparse grid H (5, 2)). To see the reduction in function evaluations with respect to full tensor product grids, we also include a plot of the corresponding Clenshaw-Curtis isotropic full tensor grid having the same maximum number of points in each direction, namely, 2 w +1 = 33. Observe that if we take m points in each direction, the isotropic full tensor grid will contain m N points, while the analogous isotropic Smolyak grid H (w, N) will contain much fewer points. Figure <ref type="figure">2</ref> shows the total number of points contained in the full tensor grid and in the Smolyak sparse grid as a function of the level w (or the corresponding maximum number m of points in each direction) for dimensions N = 5, 11, 21.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Error analysis.</head><p>In this section we develop error estimates that will help us compare the efficiency of the isotropic Smolyak approximation with other alternatives, for instance, the Monte Carlo method as explained in section 3.2. Much about this has been claimed in the existing literature based on particular numerical examples. Our main goal is therefore to understand in which situations the sparse grid stochastic collocation method is more efficient than Monte Carlo.</p><p>As explained in section 2, it is possible to use collocation methods to approximate the solution u N ∈ C 0 (Γ N ; W (D)) using finitely many function values, each of them Downloaded 12/30/12 to 139.184.30.136. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php F. NOBILE, R. TEMPONE, AND C. G. WEBSTER Fig. <ref type="figure">2</ref>. For a finite-dimensional Γ N with N = 5, 11, and 21 we plot the log of the number of distinct Clenshaw-Curtis collocation points used by the isotropic Smolyak method and the corresponding isotropic full tensor product method versus the level w (or the maximum number of points m employed in each direction).</p><p>computed by finite elements. Recall that by Assumption 1.8, u N admits an analytic extension. Let the fully discrete numerical approximation be A (w, N)π h u N . The aim of this section is to give a priori estimates for the total error</p><formula xml:id="formula_43">e = u -A (w, N)π h u N ,</formula><p>where the operator A (w, N) is described by (2.8) and π h is the finite element projection operator described by (2.2). We will investigate the error</p><formula xml:id="formula_44">(3.1) u -A (w, N)π h u N ≤ u -u N (I) + u N -π h u N (II) + π h u N -A (w, N)π h u N (III)</formula><p>evaluated in the norm L q P (Ω; W (D)) with either q = 2 or q = ∞. This yields also control of the error in the expected value of u, E[e] W (D) ≤ E e W (D) ≤ e L q P (Ω;W (D)) , and the error in the approximation of E[ψ(u)], with ψ being a smooth functional of u. In such a case we have</p><formula xml:id="formula_45">|E[ψ(u) -ψ(A (w, N)π h u N )]| ≤ 1 0 δ e ψ(u + θe) L q * P (Ω;W * (D)) dθ e L q P (Ω;W (D)) ,</formula><p>with 1/q + 1/q * = 1 and δ e ψ(u + θe) denoting the Fréchet derivative of ψ at u + θe.</p><p>Although in the previous inequalities it is suboptimal to estimate expectations of Sobolev norms rather than a Sobolev norm of expectations, we will content ourselves in this work with providing estimates for e L q P (Ω;W (D)) . Sharper estimates could be obtained generalizing the ideas given in [4, Theorem 5.1]. Downloaded 12/30/12 to 139.184.30.136. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p><p>The quantity (I) controls the truncation error for the case where the input data a N and f N are suitable truncations of random fields. This contribution to the total error will be considered in section 3.2. The quantity (I) is otherwise zero if the representation of a N and f N is exact, as in Example 1.4. The second term, (II), controls the convergence with respect to h, i.e., the finite element error, which will be dictated by standard approximability properties of the finite element space W h (D), given by (2.1), and the regularity in space of the solution u (see, e.g., <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b6">7]</ref>). For example, if we let q = 2, we have</p><formula xml:id="formula_46">u N -π h u N L 2 ρ (Γ N ;W (D)) ≤ h s Γ N (C π (y)C(s; u(y))) 2 ρ(y) dy 1/2 .</formula><p>The full tensor product convergence results are given by [3, Theorem 1], and therefore we will concern ourselves only with the convergence results when implementing the Smolyak approximation formula described in section 2.2. Namely, our primary concern will be to analyze the Smolyak approximation error</p><formula xml:id="formula_47">(III) = π h u N -A (w, N)π h u N L q ρ (Γ N ;W (D))</formula><p>for both the Clenshaw-Curtis and Gaussian versions of the Smolyak formula.</p><p>Under the very reasonable assumption that the semidiscrete finite element solution π h u N admits an analytic extension as described in Assumption 1.8 with the same analyticity region as for u N , the behavior of the error (III) will be analogous to</p><formula xml:id="formula_48">u N -A (w, N)u N L q ρ (Γ N ;W (D))</formula><p>. For this reason in the next sections we will analyze the latter.</p><p>Remark 3.1 (on the randomness of approximate and exact solutions). It is important to emphasize that the exact solution u and its numerical approximation ū := A (w, N)π h u N do not depend on the same randomness. Therefore, when considering both solutions we should in principle work with a product of probability spaces. For instance, assume now that μ u,ū is a joint distribution for u and ū. It is key to recall that we are interested only in the approximation of statistics of u, which depend only on the marginal distribution μ u , by the corresponding statistics of ū, which depend only on the marginal distribution μ ū. Let us introduce an auxiliary approximate solution, û, with the same law as ū but different joint distribution, μ u,û . We have</p><formula xml:id="formula_49">|E μu [ψ(u)] -E μū [ψ(ū)]| = |E μu [ψ(u)] -E μ û [ψ(û)]| = |E μ u, û [ψ(u) -ψ(û)]| ≤ E μ u, û [(ψ(u) -ψ(û)) 2 ] 1/2 .</formula><p>Observe now that for the purpose of estimating the difference |E μu [ψ(u)]-E μū [ψ(ū)]| the choice of the auxiliary joint distribution μ u,û is arbitrary. To this end, for the remainder of the work we select the joint distribution μ u,û such that u and û are driven by the same random variables Y . We will also avoid making any distinction in the notation between û and ū.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Analysis of the approximation error.</head><p>In this work the technique to develop error bounds for multidimensional Smolyak approximation is based on onedimensional results. Therefore, we first address the case N = 1. Let us recall the best approximation error for a function v : Γ 1 → W (D) which admits an analytic extension in the region Σ(Γ 1 ; τ ) = {z ∈ C, dist(z, Γ 1 ) &lt; τ} of the complex plane for Downloaded 12/30/12 to 139.184.30.136. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php F. NOBILE, R. TEMPONE, AND C. G. WEBSTER some τ &gt; 0. We will still denote the extension by v; in this case, τ is smaller than the distance between Γ 1 ⊂ R and the nearest singularity of v(z) in the complex plane. Since we are considering only the case of bounded random variables, we recall the following result, whose proof can be found in [3, <ref type="bibr">Lemma 7]</ref> and which is an immediate extension of the result given in [12, Chapter 7, section 8].</p><p>Lemma 3.2. Given a function v ∈ C 0 (Γ 1 ; W (D)) which admits an analytic extension in the region of the complex plane Σ(Γ 1 ; τ ) = {z ∈ C, dist(z, Γ 1 ) ≤ τ } for some τ &gt; 0, there holds</p><formula xml:id="formula_50">(3.2) E mi ≡ min w∈Vm i v -w C 0 (Γ 1 ;W (D)) ≤ 2 e σ -1 e -σ mi max z∈Σ(Γ 1 ;τ ) v(z) W (D) ,</formula><p>where</p><formula xml:id="formula_51">0 &lt; σ = log 2τ |Γ 1 | + 1 + 4τ 2 |Γ 1 | 2 . Remark 3.</formula><p>3 (approximation with unbounded random variables). A related result with weighted norms holds for unbounded random variables whose probability density decays as the Gaussian density at infinity (see <ref type="bibr" target="#b2">[3]</ref>).</p><p>In the multidimensional case, the size of the analyticity region, τ n (see, e.g., the problem considered in section 4), will depend, in general, on the direction n and on the (N -1) values of the remainder y components, denoted by y * n in Assumption 1.8. The same holds for the decay coefficient σn . In what follows, we set</p><formula xml:id="formula_52">(3.3) σ ≡ min 1≤n≤N min y * n ∈Γ * n σn</formula><p>and assume that σ is strictly positive. Similarly to that, the norm max z∈Σ(Γ 1 ;τ ) v(z) W (D) appearing in Lemma 3.2 also depends on the direction n and the remainder vector y * n and should be replaced, in the multidimensional case, by</p><formula xml:id="formula_53">M (v) = max 1≤n≤N max y * n ∈Γ * n max z∈Σ(Γn;τn) v(z) W (D) ,</formula><p>which will be assumed bounded hereafter. Both assumptions on σ and M (v) have to be verified for each problem under consideration. In particular, they are satisfied for the application described in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Interpolation estimates for the Clenshaw-Curtis abscissas.</head><p>In this section we develop L ∞ error estimates for the Smolyak interpolant based on Clenshaw-Curtis abscissas (cf. (2.12) and (2.13)) applied to analytic functions u ∈ C 0 (Γ N ; W (D)) that satisfy Assumption 1.8. We remind the reader that even though in the global estimate (3.1) it is enough to bound the approximation error (III) in the L 2 ρ (Γ N ; W (D)) norm, we will still work with the more stringent L ∞ (Γ N ; W (D)) norm.</p><p>In our notation the norm</p><formula xml:id="formula_54">• ∞,N is shorthand for • L ∞ (Γ N ;W (D))</formula><p>and will be used henceforth. We also define I N : Γ N → Γ N as the identity operator on an Ndimensional space. We begin by letting E m be the error of the best approximation to functions u ∈ C 0 (Γ 1 ; W (D)) by functions w ∈ V m . Similarly to <ref type="bibr" target="#b5">[6]</ref>, since the interpolation U i is exact on the subspace V mi-1 we can apply the general formula</p><formula xml:id="formula_55">(3.4) u -U i (u) ∞,1 ≤ E mi-1 (u) • (1 + Λ mi ) ,</formula><p>where Λ m is the Lebesgue constant for our choice (2.12). It is known that</p><formula xml:id="formula_56">(3.5) Λ m ≤ 2 π</formula><p>log(m -1) + 1 Downloaded 12/30/12 to 139.184.30.136. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php for m ≥ 2; see <ref type="bibr" target="#b12">[13]</ref>. Using Lemma 3.2, the best approximation E mi (u) to functions u ∈ C 0 (Γ 1 ; W (D)) that admit an analytic extension as described by Assumption 1.8 is bounded by (3.2). Moreover, for the choice of m i given in (2.13), it is easy to show that (1 + Λ mi ) ≤ 2i ∀i &gt; 0. Hence (3.2)-(3.4) implies</p><formula xml:id="formula_57">(I 1 -U i )(u) ∞,1 ≤ C ie -σ2 i M (u), Δ i (u) ∞,1 = (U i -U i-1 )(u) ∞,1 ≤ (I 1 -U i )(u) ∞,1 + (I 1 -U i-1 )(u) ∞,1 ≤ 2C ie -σ2 i-1 M (u)</formula><p>∀i ∈ N + , with positive constants</p><formula xml:id="formula_58">C = 4 e 2σ -1</formula><p>and σ = σ 2 (with σ as in Lemma 3.2).</p><p>Since the value M (u) affects the error estimates as a multiplicative constant, from now on we assume it to be one without loss of generality.</p><p>The convergence proof will be split into several steps, the main results being given in Theorems 3.7 and 3.10, which state the convergence rates in terms of the level w and the total number of collocation points, respectively. We denote by I d the identity operator applicable to functions which depend on the first d variables y 1 , . . . , y d . Then the following result holds.</p><p>Lemma 3.4. For functions u ∈ C 0 (Γ N ; W (D)) satisfying the assumption of Lemma 3.2 the isotropic Smolyak formula (2.8) based on Clenshaw-Curtis abscissas satisfies</p><formula xml:id="formula_59">(3.6) (I N -A (w, N)) (u) ∞,N ≤ N d=1 R(w, d) with (3.7) R(w, d) := 1 2 i∈ X(w,d) (2C) d d n=1</formula><p>i n e -σh (i,d)   and</p><formula xml:id="formula_60">(3.8) h(i, d) = d n=1 2 in-1 .</formula><p>Proof. We start by providing an equivalent representation of the isotropic Smolyak formula: : Γ n → Γ n for n = 1, . . . , N, we can compute the error estimate recursively using the previous representation, namely, (3.9)</p><formula xml:id="formula_61">A (w, N) = i∈X(w,N ) N n=1 Δ in = i∈X(w,N -1) N -1 n=1 Δ in ⊗ 1+w-N -1 n=1 (in-1) j=1 Δ j = i∈X(w,N -1) N -1 n=1 Δ in ⊗ U 1+w-N -1 n=1 (in-</formula><formula xml:id="formula_62">I N -A (w, N) = I N - i∈X(w,N -1) N -1 n=1 Δ in ⊗ U 1+w-N -1 n=1 (in-1) -I (N ) 1 - i∈X(w,N -1) N -1 n=1 Δ in ⊗ I (N ) 1 = i∈X(w,N -1) N -1 n=1 Δ in ⊗ I (N ) 1 -U 1+w-N -1 n=1 (in-1)</formula><formula xml:id="formula_63">+ (I N -1 -A (w, N -1)) ⊗ I (N ) 1 = N d=2 R(w, d) N n=d+1 I (n) 1 + I (1)</formula><p>1 -A (w, 1)</p><formula xml:id="formula_64">N n=2 I (n) 1 ,</formula><p>where, for a general dimension d, we define</p><formula xml:id="formula_65">R(w, d) = i∈X(w,d-1) d-1 n=1 Δ in ⊗ I (d)</formula><p>1 -U îd and, for any (i 1 , . . . , i d-1 ) ∈ X(w, d -1), we have set îd = 1 + w -</p><formula xml:id="formula_66">d-1</formula><p>n=1 (i n -1). Observe that with this definition, the d-dimensional vector j = (i 1 , . . . , i d-1 , îd ) belongs to the set X(w, d), defined in (2.7), and the term R(w, d) can now be bounded as follows:</p><formula xml:id="formula_67">R(w, d)(u) ∞,d ≤ i∈X(w,d-1) d-1 n=1 (Δ in )(u) ∞,d I (d) 1 -U îd (u) ∞,d ≤ 1 2 i∈X(w,d-1) (2C) d d-1 n=1 i n îd e -σ( d-1 n=1 2 in-1 +2 îd ) ≤ (2C) d 2 i∈ X(w,d) d n=1</formula><p>i n e -σ h(i,d) =: R(w, d).</p><p>Hence, the Smolyak approximation error satisfies</p><formula xml:id="formula_68">(I N -A (w, N))(u) ∞,N ≤ N d=2 R(w, d) + (I (1) 1 -A (w, 1))(u) ∞,1</formula><p>.</p><p>Observe that the last term in the previous equation can also be bounded by R(w, 1) Downloaded 12/30/12 to 139.184.30.136. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php defined in (3.7). Indeed, the set X(w, 1) contains only the point i 1 = 1 + w and</p><formula xml:id="formula_69">I (1) 1 -A (w, 1) (u) ∞,1 = I (1) 1 -U 1+w (u) ∞,1 ≤ C (1 + w)e -σ 2 1+w ≤ i1∈ X(w,1) C i 1 e -σ 2 i 1 -1</formula><p>=: R(w, 1), and this concludes the proof. Lemma 3.5. Let δ &gt; 0. Under the assumptions of Lemma 3.4 the following bound holds for the term R(w, d), d = 1, . . . , N:</p><formula xml:id="formula_70">(3.10) R(w, d) ≤ C 1 (σ, δ) d 2 exp -σd 2 w/d -δ C2 (σ)w ,</formula><p>where</p><formula xml:id="formula_71">(3.11) C2 (σ) := 1 + 1 log(2) π 2σ and (3.12) C 1 (σ, δ) := 4C eδσ exp δσ 1 σ log 2 (2) + 1 log(2) √ 2σ + 2 1 + 1 log(2) π 2σ .</formula><p>Proof. The proof is divided into several steps. </p><formula xml:id="formula_72">C 1 (σ, d, w) ≤ 1 2 4C δσ d exp dδσ 1 σ log 2 (2) + 1 log(2) √ 2σ + (int {w/d} + 2) 1 + 1 log(2) π 2σ -d ≤ C 1 (σ, δ) d 2 exp δσ 1 + 1 log(2) π 2σ w , with C 1 (σ, δ) := 4C eδσ exp δσ 1 σ log 2 (2) + 1 log(2) √ 2σ +2 1+ 1 log(2)</formula><p>π 2σ defined as in (3.12). Estimate (3.10) follows from (3.16) and (3.17). The proof is now complete.</p><p>Remark 3.6 (alternative estimate). Observe that an alternative upper bound for</p><formula xml:id="formula_73">T 1 in (3.15) is (3.18) T 1 ≤ exp σ log 2 (2) 1 + w d (2 + w) 2 2 ,</formula><p>which remains bounded as σ → 0. This does not happen with the bound C 1 (σ, d) (cf. (3.12)), which blows up as σ → 0. As an implication of (3.18), we have</p><formula xml:id="formula_74">R(w, d) ≤ (C(2 + w) 2 ) d 2 e -σd(2 w/d -w d log 2 (2)) ,</formula><p>which is an alternative to the estimate (3.10) that has an extra polynomial growth in w but remains bounded for small values of σ. Theorem 3.7. For functions u ∈ C 0 (Γ N ; W (D)) satisfying the assumption of Lemma 3.2 the isotropic Smolyak formula (2.8) based on Clenshaw-Curtis abscissas satisfies</p><formula xml:id="formula_75">(I N -A (w, N)) (u) ∞,N ≤ inf δ∈(0, χ √ π ) Ĉ(σ, δ, N ) (3.19) × ⎧ ⎪ ⎨ ⎪ ⎩ e -σw(e log(2)-δ C2(σ)) if 0 ≤ w ≤ N log(2) , e -σw( N w 2 w/N -δ C2(σ))</formula><p>otherwise,</p><formula xml:id="formula_76">where function Ĉ(σ, δ, N ) = C1(σ,δ) 2 1-C1(σ,δ) N 1-C1(σ,δ)</formula><p>. The values of C2 (σ) and C 1 (σ, δ) have been defined in <ref type="bibr">(3.11</ref>) and (3.12), respectively.</p><p>Proof. From Lemmas 3.4 and 3.5 we obtain the following bound for the approximation error:   <ref type="bibr">(3.12)</ref>. Then</p><formula xml:id="formula_77">(I N -A (w, N)) (u) ∞,N ≤ 1 2 N d=1 C 1 (σ, δ) d e -σd(</formula><formula xml:id="formula_78">(I N -A (w, N)) (u) ∞,N ≤ 1 2 max 1≤d≤N e -σw( d w 2 w/d -δ C2(σ)) N d=1 C 1 (σ, δ) d ≤ Ĉ(σ, δ, N )e σwδ C2(σ) max 1≤d≤N e -σw( d w 2 w/d ) , with (3.20) Ĉ(σ, δ, N ) := 1 2 N d=1 C 1 (σ, δ) d = C 1 (σ, δ) 2 1 -C 1 (σ, δ) N 1 -C 1 (σ, δ) .</formula><p>To finish the proof we further bound</p><formula xml:id="formula_79">(I N -A (w, N)) (u) ∞,N ≤ Ĉ(σ, δ, N ) e σwδ C2(σ) e -σw(min 1≤d≤N d w 2 w/d ) ≤ Ĉ(σ, δ, N ) e σwδ C2(σ) e -σw(min s∈[w/N,w] 1 s 2 s )</formula><p>and observe that min</p><formula xml:id="formula_80">s∈[w/N,w] 1 s 2 s = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ e log(2) if 0 ≤ w ≤ N log(2)</formula><p>, N w 2 w/N otherwise.</p><p>Remark 3.8 (alternative estimate). Following Remark 3.6 we have an alternative to <ref type="bibr">(3.19)</ref> in the estimate</p><formula xml:id="formula_81">(I N -A (w, N)) (u) ∞,N ≤ C(2 + w) 2 2 (C(2 + w) 2 ) N -1 C(2 + w) 2 -1 (3.21) × ⎧ ⎪ ⎨ ⎪ ⎩ e -σwχ if 0 ≤ w ≤ N log(2) , e -σw( N w 2 w/N -log 2 (2))</formula><p>otherwise.</p><p>Here we used the notation χ = log(2) (elog(2)) ≈ 1.4037. The previous estimate can be used to produce estimates like those in Theorems 3.10 and 3.11. The alternative estimates have constants which do not blow up as σ → 0 but have the drawback of exhibiting additional multiplicative powers of log(η), η being the total number of collocation points. A completely identical discussion applies to the estimates based on Gaussian abscissas (see section 3.1.2) and will not be repeated there. Now we relate the number of collocation points, η = η(w, N) = #H (w, N), to the level w of the isotropic Smolyak formula. We state the result in the following lemma. Lemma 3.9. Using the isotropic Smolyak interpolant described by (2.8) with Clenshaw-Curtis abscissas, the total number of points required at level w satisfies the following bounds: Proof. By using formula (2.8) and exploiting the nested structure of the Clenshaw-Curtis abscissas, the number of points η = η(w, N) = #H (w, N) can be counted in the following way:</p><formula xml:id="formula_82">(3.22) N (2 w -1) ≤ η ≤ (2eN ) w min{w + 1,</formula><formula xml:id="formula_83">(3.24) η = i∈X(w,N ) N n=1 r(i n ), where r(i) := ⎧ ⎪ ⎨ ⎪ ⎩ 1 ifi = 1, 2 ifi = 2, 2 i-2 if i &gt; 2.</formula><p>Now notice that ∀n = 1, 2, . . . , N the following bound holds:</p><formula xml:id="formula_84">(3.25) 2 in-2 ≤ r(i n ) ≤ 2 in-1 .</formula><p>We now produce a lower bound and an upper bound for η.</p><p>A lower bound on the number η of points can be obtained considering only the contribution from certain tensor grids. Indeed, for a fixed value of w = 1, . . . , w, let us consider the N grids with indices i n = 1 for n = m and i m = w + 1, m = 1, . . . , N. Since each of those N grids has 2 w-1 points, we have</p><formula xml:id="formula_85">η ≥ w w=1 N 2 w-1 = N (2 w -1).</formula><p>On the other hand, to produce an upper bound for η, we recall that |i -1| = N n=1 (i n -1) ≤ w, so the following bounds hold:</p><formula xml:id="formula_86">η = i∈X(w,N ) N n=1 r(i n ) ≤ i∈X(w,N ) 2 |i-1| ≤ w j=0 |i-1|=j 2 j ≤ w j=0 2 j N -1 + j N -1 ≤ w j=0 2 j N -1 s=1 1 + j s ≤ w j=0 2 j exp N -1 s=1 j s ≤ w j=0 2 j exp (j(1 + log(N ))) ≤ w j=0 (2eN ) j ≤ min{(w + 1)(2eN ) w , (2eN ) w+1 },</formula><p>and this finishes the proof.</p><p>The next theorem provides an error bound in terms of the total number η of collocation points. The proof follows directly from the results in Theorem 3.7 (taking δ = (e log(2) -1)/ C2 (σ)) and Lemma 3.9; it is therefore omitted.</p><p>Theorem 3.10 (algebraic convergence). For functions u ∈ C 0 (Γ N ; W (D)) satisfying the assumption of Lemma 3.2 the isotropic Smolyak formula (2.8) based on Clenshaw-Curtis abscissas satisfies</p><formula xml:id="formula_87">(3.26) (I N -A (w, N)) (u) ∞,N ≤ C 1 (σ, δ * )e σ |1 -C 1 (σ, δ * )| max{1, C 1 (σ, δ * )} N η -μ1 , with μ 1 = σ 1 + log(2N )</formula><p>. Downloaded 12/30/12 to 139.184.30.136. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Here δ * = (e log(2) -1)/ C2 (σ) and the constants C2 (σ) and C 1 (σ, δ * ), defined in <ref type="bibr">(3.11</ref>) and (3.12), respectively, do not depend on η.</p><p>Observe that the previous result indicates at least algebraic convergence with respect to the number of collocation points η. Under the same assumptions as those of the previous theorem and with a completely similar derivation, for large values of w we have the following sharper estimate.</p><p>Theorem 3.11 (subexponential convergence). Under the same assumptions as those of Theorem 3.10 and for w &gt; N log(2) it holds that</p><formula xml:id="formula_88">(3.27) (I N -A (w, N)) (u) ∞,N ≤ C 1 (σ, δ * ) e σδ * C2(σ) max{1, C 1 (σ, δ * )} N |1 -C 1 (σ, δ * )| η μ3 e -Nσ 2 1/N η μ 2 , with μ 2 = log(2) N (1 + log(2N ))</formula><p>and</p><formula xml:id="formula_89">μ 3 = σδ * C2 (σ) 1 + log(2N ) ,</formula><p>and with constant C 1 (σ, δ * ) defined in (3.12) and independent of η.</p><p>Proof. We start from the result stated in Theorem 3.7 and observe that for w &gt; N/ log(2) the function</p><formula xml:id="formula_90">g(w) = σ(N 2 w/N -wδ C2 (σ))</formula><p>is increasing in w for all values of δ &lt; log(2)e/ C2 (σ). Hence, combining <ref type="bibr">(3.19)</ref> with the lower bound (3.23) we obtain the desired result.</p><p>The previous theorem indicates at least asymptotic subexponential convergence with respect to the number of collocation points η. It should be pointed out, however, that the large values of w &gt; N/ log(2) under which the bound holds are seldom used in practical computations. Therefore, from this point of view estimate <ref type="bibr">(3.27)</ref> is less useful than <ref type="bibr">(3.26)</ref>.</p><p>Remark 3.12 (deterioration of the estimates with respect to the dimension N ). Depending on the distance to the singularities of the solution, related to the parameter τ introduced in Lemma 3.2, the constant C 1 (σ, δ * ) may be less than 1. In such a case the only dependence of the error bounds for (I N -A (w, N)) (u) ∞,N is in the exponent, whose denominator slowly grows like log(2N ).</p><p>Remark 3.13 (full tensor versus Smolyak). An isotropic full tensor product interpolation converges roughly like C(σ, N ) exp(-σp), where p is the order of the polynomial space. Since the number of collocation points relates to p in this case as η = (1 + p) N then log(η) = N log(1 + p) ≤ Np and with respect to η the convergence rate can be bounded as C(σ, N )η -σ/N . The slowdown effect that the dimension N has on the last convergence is known as the curse of dimensionality, and it is the reason for not using isotropic full tensor interpolation for large values of N . On the other hand, the isotropic Smolyak approximation seems to be better suited for this case. Indeed, from the estimate (3.26) we see that the Smolyak algebraic convergence has the faster exponent O( σ log(2N ) ). This is a clear advantage of the isotropic Smolyak method with respect to the full tensor and justifies our claim that the use of Smolyak approximation greatly reduces the curse of dimensionality. In section 5 numerical results will give computational ground to this claim.</p><p>Remark 3.14 (estimates based on bounded mixed derivatives). We can proceed in a similar way to analyze the approximation error for functions that have a bounded mixed derivative of order (k, . . . , k). In that case, the one-dimensional best approximation error is u on m i , and using again the recursion (3.9) yields</p><formula xml:id="formula_91">-U i (u) ≤ Cm -k i (1 + Λ mi ),</formula><formula xml:id="formula_92">(3.28) (I N -A (w, N)) (u) ∞,N ≤ C |C(1 + 2 k ) -1| (C(1 + 2 k )) N (w + 1) 2N 2 -kw .</formula><p>Finally, the combination of (3.28) with the counting estimates in Lemma 3.9 yields</p><formula xml:id="formula_93">(I N -A (w, N)) (u) ∞,N ≤ (C(1 + 2 k )) N |1 + 2 k -1/C| 1 + log 2 1 + η N 2N min 2 k η -k log(2) 1+log(2N ) , η -k 1 + log 2 1 + η N Nk .</formula><p>This estimate improves the one derived in <ref type="bibr" target="#b5">[6]</ref>. Analogous results can be derived for Gaussian abscissas and L 2 norms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Approximation estimates for Gaussian abscissas.</head><p>Similarly to the previous section, we now develop error estimates for Smolyak approximation using Gaussian abscissas (cf. section 2.3) of C 0 (Γ N ; W (D)) analytic functions described by Assumption 1.8. As before, we remind the reader that in the global estimate (3.1) we need to bound the approximation error (III) in the norm L 2 ρ (Γ N ; W (D)). Yet, the Gaussian abscissas defined in section 2.3 are constructed for the auxiliary density ρ = N n=1 ρn , still yielding control of the desired norm</p><formula xml:id="formula_94">v L 2 ρ (Γ N ;W (D)) ≤ ρ ρ 1/2 L ∞ (Γ N ) v L 2 ρ (Γ N ;W (D)) ∀v ∈ C 0 (Γ N ; W (D)).</formula><p>In what follows we will use the shorthand notation • ρ,N for • L 2 ρ (Γ N ;W (D)) and restrict out analysis to the case where W (D) is a Hilbert space. We now quote a useful result from Erdös and Turán <ref type="bibr" target="#b13">[14]</ref>.</p><p>Lemma 3.15. For every function u ∈ C 0 (Γ 1 ; W (D)) the interpolation error with Lagrange polynomials based on Gaussian abscissas satisfies</p><formula xml:id="formula_95">(3.29) u -U i (u) ρ,1 ≤ 2 Γ 1 ρ(y) dy inf v∈Vm i u -v ∞,1 .</formula><p>Similarly to section 3.1.1, the combination of (3.2) with (3.29) yields</p><formula xml:id="formula_96">(I 1 -U i )(u) ρ,1 ≤ C e -σ2 i M (u), with C = 8 e 2σ -1 Γ 1 ρ(y) dy, Δ i (u) ρ,1 = (U i -U i-1 )(u) ρ,1 ≤ (I 1 -U i )(u) ρ,1 + (I 1 -U i-1 )(u) ρ,1 ≤ 2 C e -σ2 i-1 M (u)</formula><p>∀i ∈ N + . As in section 3.1.1 we assume the value M (u) to be one without loss of generality. We then present the following lemma and theorem whose proofs follow, with minor changes, those given in Lemma 3.5 and Theorem 3.10, respectively. For instance, we apply (A.  <ref type="formula">2</ref>) , e -σN 2 w/N otherwise.</p><formula xml:id="formula_97">(I N -A (w, N)) (u) ρ,N ≤ ρ/ρ L ∞ (Γ N ) C1 (σ) 2 1 -C1 (σ) N 1 -C1 (σ) (3.30) × ⎧ ⎪ ⎨ ⎪ ⎩ e -σe log(2) w if 0 ≤ w ≤ N log(</formula><p>Here we have</p><formula xml:id="formula_98">(3.31) C1 (σ) := 4 C 1 + 1 log(2) π 2σ</formula><p>.</p><p>Now we relate the number of collocation points η = η(w, N) = #H (w, N) to the level w of the Smolyak formula. We state the result in the following lemma.</p><p>Lemma 3.17. Using the Smolyak formula described by (2.9) with Gaussian abscissas, the total number of points required at level w satisfies the following bounds:</p><formula xml:id="formula_99">(3.32) N (2 w + 1) ≤ η ≤ (e 2 1+log 2 (1.5) N ) w min{(w + 1), e 2 1+log 2 (1.5) N }, which implies log(η) ζ + log(N ) -1 ≤ w, with ζ := 1 + (1 + log 2 (1.5)) log(2) ≈ 2.1.</formula><p>Proof. By using formula (2.9), where we collocate using the Gaussian abscissas, the number of points η = η(w, N) = #H (w, N) can be counted in the following way:</p><formula xml:id="formula_100">(3.33) η = i∈Y (w,N ) N n=1 r(i n ), where 2 i-1 ≤ r(i) := 1 i fi = 1, 2 i-1 + 1 if i ≥ 2.</formula><p>Proceeding in a similar way as for the proof of Lemma 3.9, we obtain the following lower bound on the number of points η:</p><formula xml:id="formula_101">η ≥ N w w=w-N +1 2 w + 1 ≥ N (2 w + 1).</formula><p>On the other hand, an upper bound on η can be obtained following along the same lines as in the proof of Lemma 3.9 and observing that 2 i-1 ≤ r(i) ≤ 2 (1+ )(i-1) , with = log 2 (1.5) ≈ 0.585.</p><p>Finally, the next theorem relates the error bound (3.30) to the number of collocation points η = η(w, N) = #H (w, N), described by Lemma 3.17.</p><p>Theorem 3.18 (algebraic convergence). For functions u ∈ C 0 (Γ N ; W (D)) satisfying the assumption of Lemma 3.2 the isotropic Smolyak formula (2.8) based on Gaussian abscissas satisfies (3.34) </p><formula xml:id="formula_102">(I N -A (w, N)) (u) ρ,N ≤ ρ/ρ L ∞ (Γ N ) e σ e log(2) C1 (σ) max{1, C1 (σ)} N |1 -C1 (σ)| η -μ1 , μ1 = σ e</formula><formula xml:id="formula_103">ζ := 1 + (1 + log 2 (1.5)) log(2) ≈ 2.1.</formula><p>The constant C1 (σ) was defined in <ref type="bibr">(3.31)</ref>.</p><p>Similarly to section 3.1.1 and with the same assumptions as those of the previous theorem, for large values of w we have the following sharper estimate.</p><p>Theorem 3.19 (subexponential convergence).</p><formula xml:id="formula_104">If w &gt; N log(2) , then (3.35) (I N -A (w, N)) (u) ρ,N ≤ ρ/ρ L ∞ (Γ N ) C1 (σ) max{1, C1 (σ)} N |1 -C1 (σ)| e -Nσ 2 1/N η μ2 , with μ2 = log(2) N (ζ + log(N )) and ζ := 1 + (1 + log 2 (1.5)) log(2) ≈ 2.1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Influence of truncation errors.</head><p>In this section we consider the case where the coefficients a N and f N from (1.4) are suitably truncated random fields. Therefore, the truncation error uu N is nonzero and contributes to the total error. Such a contribution should be considered as well in the error analysis. In particular, understanding the relationship of this error with the discretization error allows us to compare the efficiency of the isotropic Smolyak method with other computational alternatives, for instance, the Monte Carlo method.</p><p>To this end, we make the assumption that the truncation error uu N decays as</p><formula xml:id="formula_105">(3.36) u -u N L 2 P (Ω;W (D)) ≤ ζ(N ) for some monotonic decreasing function ζ(N ) such that ζ(N ) → 0 as N → ∞.</formula><p>For example, if one truncates the input random fields with a Karhunen-Loève expansion (see <ref type="bibr" target="#b15">[16]</ref>), the function ζ(N ) is typically related to the decay of the eigenpairs of their covariance operators. Now, given a desired computational accuracy to achieve, tol &gt; 0, our aim is to choose the dimension N = N (tol) and the level w = w(tol) (or equivalently η = η(tol), the number of collocation points) such that</p><formula xml:id="formula_106">u -A (w, N)(u N ) L 2 P (Ω;W (D)) ≤ ζ(N ) + u N -A (w, N)(u N ) L 2 P (Ω;W (D)) ≈ tol.</formula><p>More precisely, we will impose that both error contributions should be of size tol, i.e., <ref type="bibr">(3.37)</ref> ζ(N ) ≈ tol and</p><formula xml:id="formula_107">(3.38) u N -A (w, N)(u N ) L 2 P (Ω;W (D))</formula><p>≈ tol. Condition (3.37) determines the dimension N (tol), while <ref type="bibr">(3.38)</ref> determines the necessary number of collocation points in the isotropic Smolyak approximation. Then this number of collocation points is compared to the number of samples required in the standard Monte Carlo method to approximate a statistical quantity of interest with accuracy tol. The latter is O(tol -2 ).</p><p>We detail only the procedure for the choice of Clenshaw-Curtis abscissas, since the discussion for Gaussian abscissas is identical. To impose condition (3.38), we apply Theorem 3.7, with the choice δ * = (e log(2) -1)/ C2 (σ) and C2 (σ) as in <ref type="bibr">(3.11)</ref>, yielding</p><formula xml:id="formula_108">u N -A (w, N)(u N ) L 2 ρ (Γ N ;W (D)) ≤ C 1 (σ, δ * ) |1 -C 1 (σ, δ * )| max{1, C 1 (σ, δ * )</formula><p>} N e -σw , Downloaded 12/30/12 to 139.184.30.136. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php where the constant C 1 (σ, δ * ) is defined in <ref type="bibr">(3.12)</ref>. Now define the constants C = C1(σ,δ * ) |1-C1(σ,δ * )| and F = max{1, C 1 (σ, δ * )}. With this notation and using (3.23), we have an upper bound in terms of the number of collocation points,</p><formula xml:id="formula_109">u N -A (w, N)(u N ) L 2 ρ (Γ N ;W (D)) ≤ C F N e -σw ≤ C F N e σ η - σ 1+log(2N ) ≈ tol.</formula><p>Then, given the value of N (tol), we can find</p><formula xml:id="formula_110">(3.39) η(tol) ≈ CF N e σ tol 1+log(2N ) σ</formula><p>and compare with the number of samples needed to achieve accuracy tol with Monte Carlo, which is η MC ≈ tol -2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exponential truncation error.</head><p>Here we have ζ(N ) = θe -γN , with θ and γ positive constants. Therefore the dimension depends on the required accuracy like</p><formula xml:id="formula_111">N (tol) = 1 γ log θ tol ,</formula><p>and the number of corresponding collocation points, following (3.39), is</p><formula xml:id="formula_112">η(tol) ≈ Ce σ F 1 γ log(θ) log(2e/γ)+log(log(θ/tol)) σ tol -(1+log(F )/γ)( log(2e/γ)+log(log(θ/tol)) σ</formula><p>) .</p><p>From here we can see roughly that for the exponential truncation error case the isotropic Smolyak method would be more efficient than Monte Carlo only if</p><formula xml:id="formula_113">1 + log(F ) γ log(2e/γ) + log(log(θ/tol)) σ &lt; 2.</formula><p>Observe that for sufficiently stringent accuracy requirements, i.e., tol sufficiently small, the Monte Carlo method will have a better convergence rate. On the other hand, due to the very slow growth of the log(log(θ/tol)) term above, these values of tol may be much smaller than the ones we need in practice. Thus, the range of parameters for which the isotropic Smolyak approximation gives a better convergence rate than Monte Carlo can still be relevant in many practical problems with truncated coefficients.</p><p>Observe, moreover, that whenever the parameter γ is large, the behavior of the one-dimensional interpolation error varies widely with respect to the different y directions. In such a case, it is likely that the isotropic Smolyak method uses too many points in the directions with fastest decay. For such a case, the isotropic Smolyak method may still be better than Monte Carlo, yet we recommend the use of an anisotropic version of the Smolyak method to obtain faster convergence. For instance, see <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b7">8]</ref>, where anisotropic Smolyak formulas have been proposed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algebraic truncation error.</head><p>Here we have ζ(N ) = θN -r , with θ and r positive constants. Therefore the dimension is N (tol) = (tol/θ) -1 r and we have tol F (θ/tol) After denoting tol = tol F (θ/tol) 1/r ≤ tol, the corresponding number of collocation points is</p><formula xml:id="formula_114">(3.40) η(tol) ≈ Ce σ tol - log(2e)+ 1 r log(θ/tol) σ .</formula><p>Observe that even for the case where F = 1 we now have an asymptotically faster growth of η(tol) than in the exponential truncation case. In fact, for such a case we need to have</p><formula xml:id="formula_115">log(2e) + 1 r log(θ/tol) σ &lt; 2</formula><p>for the isotropic Smolyak method to be more efficient than Monte Carlo. If F &gt; 1, then tol &lt; tol and this makes, as tol gets smaller, the comparison even more favorable to Monte Carlo; cf. (3.40).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Application to linear elliptic PDEs with random input data.</head><p>In this section we apply the theory developed so far to the particular linear problem described in Example 1.1. Problem (1.2) can be written in a weak form as follows: Find</p><formula xml:id="formula_116">u ∈ L 2 P (Ω; H 1 0 (D)) such that (4.1) D E[a∇u • ∇v] dx = D E[fv] dx ∀ v ∈ L 2 P (Ω; H 1 0 (D)).</formula><p>A straightforward application of the Lax-Milgram theorem allows one to state the well-posedness of problem (4.1) and yields</p><formula xml:id="formula_117">u(ω) H 1 0 (D) ≤ C P a min f (ω, •) L 2 (D) a.s., u L 2 P (Ω;H 1 0 (D)) ≤ C P a min D E[f 2 ] dx 1/2</formula><p>, where C P denotes the constant appearing in the Poincaré inequality:</p><formula xml:id="formula_118">v L 2 (D) ≤ C P ∇v L 2 (D) ∀v ∈ H 1 0 (D).</formula><p>Once we have the input random fields described by a finite set of random variables, i.e., a(ω, x) = a N (Y 1 (ω), . . . , Y N (ω), x), and similarly for f (ω, x), the "finitedimensional" version of the stochastic variational formulation (4.1) has a "deterministic" equivalent, which is the following: Find</p><formula xml:id="formula_119">u N ∈ L 2 ρ (Γ N ; H 1 0 (D)) such that Γ N (a N ∇u N , ∇v) L 2 (D) ρ(y)dy (4.2) = Γ N (f N , v) L 2 (D) ρ(y)dy ∀ v ∈ L 2 ρ (Γ N ; H 1 0 (D)),</formula><p>where ρ(y) is the joint probability density function defined by <ref type="bibr">(1.6)</ref> For our convenience, we will suppose that the coefficient a N and the forcing term f N admit a smooth extension on the ρ-zero measure sets. Then (4.3) can be extended a.e. in Γ N with respect to the Lebesgue measure (instead of the measure ρdy).</p><p>It has been proved in <ref type="bibr" target="#b2">[3]</ref> that problem (4.3) satisfies the analyticity result stated in Assumption 1.8. For instance, if we take the diffusivity coefficient as in Example 1.4 and a deterministic load, the size of the analyticity region is given by <ref type="bibr">(4.4)</ref> τ n = a min 4σ n .</p><p>On the other hand, if we take the diffusivity coefficient as a truncated expansion as in Remark 1.6, then the analyticity region Σ(Γ n ; τ n ) is given by (4.5)</p><formula xml:id="formula_120">τ n = 1 4 √ λ n b n L ∞ (D) .</formula><p>Observe that, in the latter case, as √ λ n b n L ∞ (D) → 0 for a regular enough covariance function (see <ref type="bibr" target="#b15">[16]</ref>), the analyticity region increases as n increases. This fact introduces, naturally, an anisotropic behavior with respect to the "direction" n. This effect will not be exploited in the numerical methods proposed in the next sections but is the subject of ongoing research.</p><p>The finite element operator π h can be introduced for this problem by projecting (4.3) onto the subspace W h (D) for each y ∈ Γ N , i.e., u N h (y) = π h u N (y) satisfies</p><formula xml:id="formula_121">(4.6) D a N (y)∇u N h (y) • ∇φ h dx = D f N (y)φ h dx ∀φ h ∈ W h (D) for a.e. y ∈ Γ N .</formula><p>Notice that the finite element functions u N h (y) satisfy the optimality condition (2.2) ∀y ∈ Γ N . Finally, the Smolyak formula (2.8) can be applied to u N h to obtain the fully discrete solution. The error estimates for the Smolyak approximation, stated in Theorems 3.10 and 3.11 for Clenshaw-Curtis abscissas and Theorems 3.18 and 3.19 for Gaussian abscissas, hold in this case, with parameter</p><formula xml:id="formula_122">(4.7) σ = 1 2 min n=1,...,N log 2τ n |Γ n | + 1 + 4τ n 2 |Γ n | 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Numerical examples.</head><p>This section illustrates the convergence of the sparse collocation method for the stochastic linear elliptic problem in two spatial dimensions, as described in section 4. The computational results are in accordance with the convergence rates predicted by the theory. Actually, we observe a faster convergence than stated in Theorems 3.10 and 3.18, which hints that the current estimates may be improved.</p><p>We will also use this section to compare the convergence of the isotropic Smolyak approximation, described and analyzed in sections 2.2 and 3.1, respectively, with other ensemble-based methods such as the anisotropic adaptive full tensor product method described in the work [5, section 9] and the Monte Carlo method. The problem is to solve </p><formula xml:id="formula_123">(5.1) -∇ • (a(ω, •)∇u(ω, •)) = f (ω, •) in Ω× D, u(ω, •) = 0 on Ω × ∂D, with D = [0, d]</formula><formula xml:id="formula_124">-0.5)](x 1 , x 2 ) = E (log(a)(x 1 ) -E[log(a)](x 1 )) (log(a)(x 2 ) -E[log(a)](x 2 )) = exp -(x 1 -x 2 ) 2 L 2 c .</formula><p>For x ∈ [0, d], let L c be a desired physical correlation length for the coefficient a, meaning that the random variables a(x) and a(y) become essentially uncorrelated for |x -y| L c . Then the parameter L p in (5.4) is L p = max{d, 2L c } and the parameter L in (5.2) and <ref type="bibr">(5.3)</ref> </p><formula xml:id="formula_125">is L = L c /L p .</formula><p>The rate of convergence of the isotropic Smolyak method is dictated by the decay coefficient σ defined by (4.7), which in this case can be bounded as <ref type="bibr">(5.5)</ref> σ ≥ 1 2 log 1 + 1 24 √ πL .</p><p>From (5.5) we notice that larger correlation lengths will have negative effects on the rate of convergence, i.e., the coefficient σ appearing in the estimates (3.26)-(3.27) and (3.34)-(3.35) is approaching 1 as L c becomes large. Hence, the effect of increasing L c is a deterioration of the rate of convergence. Recall from section 2.3 that the Clenshaw-Curtis abscissas are nested and therefore, in practice, we exploit this fact and construct the isotropic Smolyak interpolant using formula <ref type="bibr">(2.8)</ref>. Hence, the number of points η = η(w, N) = #H (w, N) can be counted as in formula <ref type="bibr">(3.24)</ref>. On the other hand, the Gaussian abscissas, which in this case are the roots of the Legendre polynomials, are not nested, and to reduce the number of points necessary to build the isotropic Smolyak formula one utilizes the variant of (2.8), given by (2.9). Consequently, we can count the number of points η used by the Smolyak formula as in <ref type="bibr">(3.33)</ref>. Downloaded 12/30/12 to 139.184.30.136. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php The finite element space for the spatial discretization is the span of continuous functions that are piecewise polynomials with degree two over a uniform triangulation of D with 4225 unknowns.</p><p>Observe that the collocation method requires only the solution of uncoupled deterministic problems over the set of collocation points, even in the presence of a diffusivity coefficient which depends nonlinearly on the random variables as in <ref type="bibr">(5.2)</ref>. This is a significant advantage that the collocation method offers compared to the classical stochastic-Galerkin finite element method as considered, for instance, in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">37]</ref>.</p><p>To study the convergence of the isotropic Smolyak approximation we consider a problem with a fixed dimension N and investigate the behavior when the level w in the Smolyak formula is increased linearly.</p><p>The computational results for the L 2 (D) error to the expected value, E[u], using the isotropic Smolyak approximation, are shown in Figure <ref type="figure" target="#fig_1">3</ref>. Here we consider the truncated probability space to have dimensions N = 5 and N = 11. To estimate the computational error in the level w we approximate</p><formula xml:id="formula_126">E[ ] ≈ E[A (w, N)π h u N - A (w + 1, N)π h u N ] .</formula><p>The results reveal, as expected, that for a small nondegenerate correlation length, i.e., L c = 1/64, the error decreases (sub)exponentially, as the level w increases. We also observe that the convergence rate is dimension dependent and slightly deteriorates as N increases.</p><p>To investigate the performance of the isotropic Smolyak approximation by varying the correlation length L c we also include the cases where L c = 1/16, L c = 1/4, and L c = 1/2 for both N = 5 and N = 11, as seen in Figure <ref type="figure">4</ref>. As predicted by (5.5), we observe that the larger correlation lengths do indeed slow down the rate of convergence. Our final interest, then, is to compare our isotropic sparse tensor product method with the Monte Carlo approach, as well as with the anisotropic full tensor product method, proposed in <ref type="bibr" target="#b4">[5]</ref>.</p><p>The anisotropic full tensor product algorithm can be described in the following way: Given a tolerance tol the method computes a multi-index p = (p 1 , p 2 , . . . , p N ), corresponding to the order of the approximating polynomial spaces P p (Γ N ). This Downloaded 12/30/12 to 139.184.30.136. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php adaptive algorithm increases the tensor polynomial degree with an anisotropic strategy: it increases the order of approximation in one direction as much as possible before considering the next direction. Tables <ref type="table">1</ref> and<ref type="table">2</ref> show the values of components of the 11-dimensional multi-index p for different values of tol, corresponding to L c = 1/2 and L c = 1/64, respectively. These tables also give insight into the anisotropic behavior of each particular problem. Notice, in particular, that for the case L c = 1/64 the algorithm predicts a multi-index p which is equal in all directions, i.e., an isotropic tensor product space. A convergence plot for L c = 1/2 and L c = 1/64 can be constructed by examining each row of Table <ref type="table">1</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>For N = 11, we compare the number of function evaluations required by the anisotropic full tensor product method (AF) using Gaussian abscissas, isotropic smolyak (IS) using Clenshaw-Curtis abscissas, and the Monte Carlo (MC) method using random abscissas to reduce the original error of problem <ref type="bibr">(5.1)</ref> large amount of function evaluations are required. This can been seen from Figure <ref type="figure" target="#fig_3">5</ref>, where we include reference lines with slopes -1/2 and -1, respectively, or in Table <ref type="table">3</ref>, where, for N = 11, we compare the work, proportional to the number of samples, which is the number of collocation points, required by each method to decrease the original error by a factor of 10 4 for all four correlation lengths L c = 1/2, 1/4, 1/16, and 1/64. Downloaded 12/30/12 to 139.184.30.136. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php 6. Conclusions. In this work we proposed and analyzed a sparse grid stochastic collocation method for solving PDEs whose coefficients and forcing terms depend on a finite number of random variables. The sparse grids are constructed from the Smolyak formula, utilizing either Clenshaw-Curtis or Gaussian abscissas. The method leads to the solution of uncoupled deterministic problems and, as such, it is simple to implement, allows for the use of legacy codes, and is fully parallelizable like a Monte Carlo method.</p><p>This method is an improvement of the stochastic collocation method on tensor product grids proposed in <ref type="bibr" target="#b2">[3]</ref>. The use of sparse grids considered in the present work (as opposed to full tensor grids) reduces considerably the curse of dimensionality and allows us to treat effectively problems that depend on a moderately large number of random variables, while keeping a high level of accuracy.</p><p>Upon assumption that the solution depends analytically on each random variable (which is a reasonable assumption for a certain class of applications; see <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3]</ref>), we derived strong error estimates for the fully discrete sparse grid stochastic collocation solution and analyzed its computational efficiency. In particular, the main result is the algebraic convergence with respect to the total number of collocation points; cf. Theorems 3.10 and 3.18. The exponent of such algebraic convergence depends on both the regularity of the solution and the number of input random variables, N . The exponent essentially deteriorates with N by a factor of 1/ log(N ). The theory is confirmed numerically by the examples presented in section 5. We also utilized the error estimates to compare the method with Monte Carlo in terms of computational work to achieve a given accuracy, indicating for which problems the former is more efficient than the latter. To this effect, in section 3.2 we considered a case where the input random variables come from suitably truncated expansions of random fields and related the number of collocation points in the sparse grid to the number of random variables retained in the truncated expansion. We also developed error estimates with fewer regularity requirements in Remark 3.14.</p><p>The sparse grid method is very effective for problems whose input data depend on a moderate number of random variables, which "weigh equally" in the solution.</p><p>For such an isotropic situation the displayed convergence is faster than standard collocation techniques built upon full tensor product spaces.</p><p>On the other hand, the convergence rate deteriorates when we attempt to solve highly anisotropic problems, such as those appearing when the input random variables come, e.g., from Karhunen-Loève truncated expansions of "smooth" random fields. In such cases, a full anisotropic tensor product approximation, as proposed in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b2">3]</ref>, may still be more effective for a small or moderate number of random variables.</p><p>Future directions of this research will include the development and analysis of an anisotropic version of the sparse grid stochastic collocation method, which will combine an optimal treatment of the anisotropy of the problem while reducing the curse of dimensionality via the use of sparse grids.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix. Additional estimates.</head><p>Here we present auxiliary results that are used in section 3. Let us recall the definition for the integer and fractional parts of a nonnegative real number x that satisfy x = frac {x} + int {x} ∀x ∈ R + , with int {x} being the largest natural number that is smaller than or equal to x.</p><p>Lemma A.1. Given w ∈ N + , for any α &gt; 0 and 0 ≤ β &lt; w, we have w i=0 e -α(i-β) 2 ≤ 2e -α(min{frac{β},1-frac{β}})  + (int {β} + 1) 2e -α(min{frac{β},1-frac{β}}) </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. For a two-dimensional parameter space (N = 2) and maximum level w = 5, we plot the full tensor product grid using the Clenshaw-Curtis abscissas (left) and isotropic Smolyak sparse grids H (5, 2), utilizing the Clenshaw-Curtis abscissas (middle) and the Gaussian abscissas (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The rate of convergence of the isotropic Smolyak approximation for solving problem (5.1) with correlation length Lc = 1/64 using both the Gaussian and Clenshaw-Curtis abscissas. For a finite-dimensional probability space Γ N with N = 5 and N = 11 we plot the L 2 (D) approximation error in the expected value in the log-linear scale (left) and log-log scale (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>F 4 N = 5 , 11 &amp; L = 1/ 16 Fig. 4 .</head><label>4511164</label><figDesc>Fig. 4. The convergence of the isotropic Smolyak approximation for solving problem (5.1) with given correlation lengths Lc = 1/2, 1/4, 1/16, and 1/64 using both the Gaussian and Clenshaw-Curtis abscissas. For a finite-dimensional probability space Γ N with N = 5 and N = 11 we plot the L 2 (D) approximation error in the expected value versus the number of collocation points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. An 11-dimensional comparison of the isotropic Smolyak method, the anisotropic full tensor product algorithm, and Monte Carlo approach for solving problem (5.1) with correlation lengths Lc = 1/2, 1/4, 1/16, and 1/64. We plot the L 2 (D) approximation error in the expected value versus the number of collocation points (or samples of the Monte Carlo method).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fe 2 ∞e -αj 2 ≤ 2 ⎛ 1 √ 1 √Finally, choosing k 0 2 + (int {β} + 1 )</head><label>22211021</label><figDesc>. NOBILE, R. TEMPONE, AND C. G. WEBSTER Proof. Let us write β = int {β} + frac {β} . Thenw i=0 e -α(i-β) 2 = -α(i-β) 2 ≤ 2e -α(min{frac{β},1-frac{β}}) j=0 2e -α(min{frac{β},1-frac{β}})Now we state and prove an auxiliary estimate, to be used later on in the proof of Lemma A.3.Lemma A.2. If α &gt; 0, we have Observe first that xe -αx 2 ≤ 2eα ∀x ≥ 0 and that the bound is attained at x * = 2α . Then, for any integer k 0 ≥ x * , we can estimate = int {x * }+1 = int 1/ √ 2α +1, the desired result follows. Lemma A.3. Given w ∈ N + , for any α &gt; 0 and 0 ≤ β &lt; w, we havew i=1 i e -α(i-β) 2 ≤ 2 e -α(1-frac{β}) {β} + 1) 2 e -α(min{frac{β},1-frac{β}}) 30/12 to 139.184.30.136. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpProof. Write w i=0 i e -α(i-β) 2 = w-int{β} j=-int{β} (j + int {β})e -α(j-frac{β}) 2 = w-int{β} j=-int{β} (j -1) e -α(j-frac{β}) α(i-β) 2 ≤ w-int{β}j=1(j -1) e -α(j-frac{β}) 2 + (int {β} + 1) 2e -α(min{frac{β},1-frac{β}}) 1) e -α(j-1) 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Section 2 summarizes various collocation techniques and describes the sparse approximation method under study. It also describes two types of abscissas, Downloaded 12/30/12 to 139.184.30.136. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php F. NOBILE, R. TEMPONE, AND C. G. WEBSTER</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Downloaded 12/30/12 to 139.184.30.136. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php where we denote W * (D) to be the dual space of W (D), and C is a constant independent of the realization ω ∈ Ω.(A 2 ) The forcing term f ∈ L 2 P (Ω; W * (D)) is such that the solution u is unique and bounded in L 2 P (Ω; W (D)). Here we give two example problems that are posed in this setting.</figDesc><table><row><cell>Example 1.1. The linear problem</cell></row><row><cell>(1.2)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Downloaded 12/30/12 to 139.184.30.136. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Here 1 Di is the indicator function of the set D i , σ i , a min are positive constants, and the random variables Y i are nonnegative with unit variance.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>this work we denote by Γ n ≡ Y n (Ω) the image of Y n , where we assume Y n (ω) to be bounded. Without loss of generality we can assume Γ n = [-1, 1]. We also let Γ N =</figDesc><table><row><cell>N n=1 Γ n and assume that the random variables [Y 1 , Y 2 , . . . , Y N ] have a joint</cell></row><row><cell>probability density function</cell></row><row><cell>(1.6)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>30/12 to 139.184.30.136. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php F. NOBILE, R. TEMPONE, AND C. G. WEBSTER</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>1) . Downloaded 12/30/12 to 139.184.30.136. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php F. NOBILE, R. TEMPONE, AND C. G. WEBSTER Introducing the one-dimensional identity operator I</figDesc><table><row><cell>(n)</cell></row><row><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Downloaded 12/30/12 to 139.184.30.136. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table><row><cell></cell><cell cols="9">F. NOBILE, R. TEMPONE, AND C. G. WEBSTER</cell></row><row><cell>with</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">C 1 (σ, d, w) ≤</cell><cell cols="2">1 2</cell><cell cols="3">(4C) d</cell><cell></cell><cell>1 σ log 2 (2)</cell><cell>+</cell><cell>1 log(2) √</cell><cell>2σ</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">+ (int {w/d} + 2) 1 +</cell><cell>1 log(2)</cell><cell>π 2σ</cell><cell>d</cell><cell>.</cell></row><row><cell cols="10">Now let δ &gt; 0 and use the inequality x + 1 ≤ e x , x ≥ 0, to bound</cell></row><row><cell>(3.17)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(3.13)</cell><cell cols="7">h(i, d) ≥ d2 w/d +</cell><cell cols="2">log 2 (2) 2</cell></row><row><cell cols="10">2. Combining (3.7) and (3.13), estimate</cell></row><row><cell>(3.14)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">R(w, d) ≤</cell><cell>(2C) d 2</cell><cell></cell><cell cols="4">e -σd2 w/d</cell><cell cols="2">i∈ X(w,d)</cell><cell>n=1 d</cell><cell>i n e -σ log 2 (2) 2</cell><cell>d n=1 (in-(1+w/d)) 2</cell></row><row><cell></cell><cell>≤</cell><cell>(2C) d 2</cell><cell></cell><cell cols="4">e -σd2 w/d</cell><cell cols="2">i=1 w+1</cell><cell>i e -σ log 2 (2) 2</cell><cell>(i-(1+w/d)) 2</cell><cell>d</cell><cell>.</cell></row><row><cell cols="10">3. Next, use (A.2) from Corollary A.4 of the appendix to estimate the term</cell></row><row><cell>T 1 :=</cell><cell cols="6">w+1 i=1 i e -σ log 2 (2) 2</cell><cell cols="3">(i-(1+w/d)) 2 . We have</cell></row><row><cell>(3.15)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>T 1 ≤ 2</cell><cell cols="3">1 σ log 2 (2)</cell><cell cols="2">+</cell><cell cols="3">1 log(2) √</cell><cell>2σ</cell><cell>+ 2(int {w/d} + 2) 1 +</cell><cell>1 log(2)</cell><cell>π 2σ</cell><cell>,</cell></row><row><cell cols="10">where int {•} denotes the integer part of a real number. Combine (3.14) and</cell></row><row><cell cols="3">(3.15), arriving at</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(3.16)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>1. Expand the function h(i, d) up to second order around the point i * = (1 + w/d, . . . , 1 + w/d) on the subspace {x ∈ R d :, |x -1| = w}. Observe that i * is a constrained minimizer of h(i, d) and d n=1 (i n -(1 + w/d)) 2 ∀i ∈ X(w, d). R(w, d) ≤C 1 (σ, d, w)e -σd 2 w/d</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Downloaded 12/30/12 to 139.184.30.136. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php with C 1 (σ, δ) defined in</figDesc><table /><note><p>w/d -w d δ C2(σ)) ,</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>2eN }. Downloaded 12/30/12 to 139.184.30.136. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php F. NOBILE, R. TEMPONE, AND C. G. WEBSTER</figDesc><table><row><cell cols="3">Moreover, as a direct consequence of (3.22) we get that</cell></row><row><cell>(3.23)</cell><cell>log(η) 1 + log(2) + log(N )</cell><cell>-1 ≤ w.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>with C depending on u and k but not Downloaded 12/30/12 to 139.184.30.136. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php F. NOBILE, R. TEMPONE, AND C. G. WEBSTER</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Downloaded 12/30/12 to 139.184.30.136. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Lemma 3.16. For functions u ∈ C 0 (Γ N ; W (D)) satisfying the assumption of Lemma 3.2 the isotropic Smolyak formula (2.8) based on Gaussian abscissas satisfies</figDesc><table /><note><p>1) from Corollary A.4 to bound the corresponding T 1 sum in the estimate of R(w, d).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Downloaded 12/30/12 to 139.184.30.136. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php F. NOBILE, R. TEMPONE, AND C. G. WEBSTER</figDesc><table><row><cell>with</cell></row></table><note><p>log(2) ζ + log(N ) ,</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>1/r ≈ Ce σ η Downloaded 12/30/12 to 139.184.30.136. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php F. NOBILE, R. TEMPONE, AND C. G. WEBSTER</figDesc><table><row><cell cols="2">σ</cell></row><row><cell>log(2e)+ 1 r</cell><cell>log(θ/tol) .</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Downloaded 12/30/12 to 139.184.30.136. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table><row><cell>. Observe that</cell></row><row><cell>in this work the gradient notation, ∇, always means differentiation with respect to</cell></row><row><cell>x ∈ D only, unless otherwise stated. The stochastic boundary value problem (4.1)</cell></row><row><cell>now becomes a deterministic Dirichlet boundary value problem for an elliptic partial</cell></row><row><cell>differential equation with an N -dimensional parameter. Then it can be shown that</cell></row><row><cell>problem (4.1) is equivalent to</cell></row><row><cell>(4.3)</cell></row></table><note><p>D a N (y)∇u N (y) • ∇φ dx = D f N (y)φ dx ∀φ ∈ H 1 0 (D), ρ-a.e. in Γ N .</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>2 and d = 1. We consider a deterministic load f (ω, x, z) = cos(x) sin(z) and construct the random diffusion coefficient a N (ω, x) with one-dimensional (layered) Downloaded 12/30/12 to 139.184.30.136. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php F. NOBILE, R. TEMPONE, AND C. G. WEBSTER In this example, the random variables {Y n (ω)} ∞ n=1 are independent, have zero mean and unit variance, i.e., E[Y n ] = 0 and E[Y n Y m ] = δ nm for n, m ∈ N + , and are uniformly distributed in the interval [-</figDesc><table><row><cell cols="2">spatial dependence as</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(5.2)</cell><cell cols="8">log(a N (ω, x) -0.5) = 1 + Y 1 (ω)</cell><cell>√ 2 πL</cell><cell>1/2</cell><cell>+</cell><cell>N n=2</cell><cell>ζ n ϕ n (x) Y n (ω),</cell></row><row><cell>where</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(5.3)</cell><cell>ζ n :=</cell><cell>√</cell><cell>πL</cell><cell cols="4">1/2 exp</cell><cell>-n 2 πL 8</cell><cell>2</cell><cell>if n &gt; 1</cell></row><row><cell>and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(5.4)</cell><cell cols="4">ϕ n (x) :=</cell><cell>⎧ ⎨ ⎩</cell><cell cols="2">sin cos</cell><cell>n 2 πx Lp n 2 πx Lp</cell><cell>if n even, if n odd.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>√</cell><cell>3,</cell><cell>√</cell><cell>3]. Consequently, the auxiliary probability</cell></row><row><cell cols="9">density ρ defined by (2.14) can be taken equal to the joint probability density function</cell></row><row><cell cols="9">ρ defined by (1.6). Expression (5.2) represents the truncation of a one-dimensional</cell></row><row><cell cols="8">random field with stationary covariance</cell></row><row><cell></cell><cell>cov[log(a N</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>and Table2, respectively, and plotting the number of points in the tensor product grid versus the error in expectation. We estimate the error in expectation byE[ ] ≈ E[u N h,pu N h, p ] , with p = (p 1 + 1, p 2 + 1, . . . , p N + 1). This entails an additional computational cost, which is bounded by the factor exp ] and the references therein). If the aim is to compute a functional of the solution such as the expected value, one would approximate E[u] numerically by sample averages of independant and identically dis-Downloaded 12/30/12 to 139.184.<ref type="bibr" target="#b30">30</ref>.136. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php F. NOBILE, R. TEMPONE, AND C. G. WEBSTER</figDesc><table><row><cell>N n=1 1/p n times the work to compute E[u N h,p ].</cell></row><row><cell>The standard Monte Carlo finite element method is a popular choice for solving</cell></row><row><cell>stochastic problems such as (5.1) (see, e.g., [21, 9, 5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>, in expectation, by a factor of 10 4 .</figDesc><table><row><cell>Lc</cell><cell>AF</cell><cell>IS</cell><cell>MC</cell></row><row><cell>1/2</cell><cell>2.5 × 10 2</cell><cell>2.5 × 10 3</cell><cell>5.0 × 10 9</cell></row><row><cell>1/4</cell><cell>1.2 × 10 3</cell><cell>4.0 × 10 3</cell><cell>2.0 × 10 9</cell></row><row><cell>1/16</cell><cell>2.0 × 10 3</cell><cell>5.0 × 10 2</cell><cell>1.6 × 10 9</cell></row><row><cell>1/64</cell><cell>2.0 × 10 5</cell><cell>3.6 × 10 2</cell><cell>1.3 × 10 9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head></head><label></label><figDesc>Downloaded 12/30/12 to 139.184.30.136. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table><row><cell>2</cell><cell>1 +</cell><cell>1 2</cell><cell>π α</cell><cell>.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>We have, as a direct consequence of Lemmas A.1 and A.3 the following estimates.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell>1 +</cell><cell>1 2</cell><cell>π α</cell><cell>.</cell></row><row><cell cols="9">Finally, use the auxiliary Lemma A.2 to estimate</cell></row><row><cell></cell><cell></cell><cell cols="2">w-int{w/d} j=0</cell><cell cols="3">j e -αj 2 ≤</cell><cell>∞ j=1</cell><cell>j e -αj 2 ≤</cell><cell>α</cell><cell>+</cell><cell>1 √ 2α</cell><cell>.</cell></row><row><cell cols="7">Corollary A.4. There holds</cell><cell></cell></row><row><cell>(A.1)</cell><cell></cell><cell>w i=0</cell><cell cols="3">e -σ log 2 (2) 2</cell><cell cols="3">(i-w/d) 2 ≤ 2 1 +</cell><cell>1 log(2)</cell><cell>π 2σ</cell></row><row><cell>and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(A.2)</cell><cell>w i=0</cell><cell cols="3">(1 + i) e -σ log 2 (2) 2</cell><cell cols="4">(i-w/d) 2 ≤ 2</cell><cell>1 σ log 2 (2)</cell><cell>+</cell><cell>1 log(2) √</cell><cell>2σ</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+ 2(int {w/d} + 2) 1 +</cell><cell>1 log(2)</cell><cell>π 2σ</cell><cell>.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Downloaded 12/30/12 to 139.184.30.136. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. The third author would like to thank MOX, Dipartimento di Matematica, Politecnico di Milano, Italy, for hosting a visit to complete this research. Also, the third author would like to thank Prof. Max Gunzburger and Dr. John Burkardt for their insight, guidance, and many helpful discussions. The authors also would like to thank the anonymous referee for constructive comments. Downloaded 12/30/12 to 139.184.30.136. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This author's research was partially supported by M.U.R.S.T. Cofin 2005 "Numerical Modeling for Scientific Computing and Advanced Applications" and SANDIA project 523695. This author's research was partially supported by SANDIA project 523695, the Dahlquist fellowship at the Royal Institute of Technology in Stockholm, Sweden, and the UdelaR in Uruguay. This author's research was supported by the School of Computational Science (SCS) at Florida State University and by the John von Neumann Fellowship at the Computer Science Research Institute (CSRI), Sandia National Laboratories. Sandia is a multiprogram laboratory operated by Sandia Corporation, a Lockheed Martin Company, for the U.S. Department of Energy's National Nuclear Security Administration under contract DE AC04-94-AL85000.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>The N = 11 components of the multi-index p computed by the anisotropic full tensor product algorithm when solving problem (5.1) with a correlation length Lc = 1/2.  tributed (i.i.d.) realizations of the stochastic input data. Given a number of realizations, M ∈ N + , we compute the sample average as follows: For each k = 1, . . . , M, sample i.i.d. realizations of a(ω k , •) and f (ω k , •), solve problem (5.1), and construct finite element approximations u N h (ω k , •). We note that once we have fixed ω = ω k , the problem is completely deterministic, and may be solved by standard methods as in the collocation approach. Finally, approximate E[u] by the sample average:</p><p>For the cases L c = 1/2, 1/4, 1/16, and 1/64 we take M = 2 i , i = 0, 1, 2, . . . , 11, realizations and compute the approximation to the error in expectation by</p><p>, with w = 4, so that A (5, N) is a highly enriched Clenshaw-Curtis isotropic sparse solution.</p><p>To study the advantages of utilizing an isotropic sparse tensor product space as opposed to an anisotropic full tensor product space, we show in Figure <ref type="figure">5</ref> the convergence of these methods when solving problem (5.1), using correlation lengths L c = 1/2, 1/4, 1/16, and 1/64 with N = 11. We also include five ensembles of the Monte Carlo method described previously. Figure <ref type="figure">5</ref> reveals that for the isotropic case with L c = 1/64 the isotropic Smolyak method obtains a faster rate of convergence than the anisotropic full tensor product method. This is due to a slower decay of the eigenvalues expansion (5.2) and hence an almost equal weighing of all N = 11 random variables. On the contrary, opposite behavior can be observed for L c = 1/2. Since, in this case, the rate of decay of the expansion is faster, the anisotropic full tensor method weighs heavily the important modes and, therefore, achieves a faster convergence than the isotropic Smolyak method.</p><p>In all four cases we observe that the two methods outperform the Monte Carlo method. We know that the amount of work to reach the accuracy in the Monte Carlo approach can be approximated by ≈ O(M -1/2 ) times the amount of work per sample, where M is the number of samples. This is affected only by the problem dimension through the eventual increase of the work per sample. Nevertheless, the convergence rate is quite slow and a high level of accuracy is achieved only when a Downloaded 12/30/12 to 139.184.30.136. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On solving elliptic stochastic partial differential equations</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Babuška</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chatzipantelidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Appl. Mech. Engrg</title>
		<imprint>
			<biblScope unit="volume">191</biblScope>
			<biblScope unit="page" from="4093" to="4122" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Solving stochastic partial differential equations based on the experimental data</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Babuška</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tempone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Models Methods Appl. Sci</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="415" to="444" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A stochastic collocation method for elliptic partial differential equations with random input data</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Babuška</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nobile</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tempone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Numer. Anal</title>
		<imprint>
			<biblScope unit="volume">435</biblScope>
			<biblScope unit="page" from="1005" to="1034" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Galerkin finite element approximations of stochastic elliptic partial differential equations</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Babuška</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tempone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Zouraris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Numer. Anal</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="800" to="825" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Solving elliptic boundary value problems with uncertain coefficients by the finite element method: The stochastic formulation</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Babuška</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tempone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Zouraris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Appl. Mech. Engrg</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="1251" to="1294" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">High dimensional polynomial interpolation on sparse grids</title>
		<author>
			<persName><forename type="first">V</forename><surname>Barthelmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Comput. Math</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="273" to="288" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Brenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
		<title level="m">The Mathematical Theory of Finite Element Methods</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Bungartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Griebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sparse grids</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="147" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reduced order modeling of some nonlinear stochastic partial differential equations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Burkardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gunzburger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Webster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Numer. Anal. Model</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="368" to="391" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The Finite Element Method for Elliptic Problems</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Ciarlet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978">1978</date>
			<pubPlace>North-Holland, New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A method for numerical integration on an automatic computer</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Clenshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Curtis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numer. Math</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="197" to="205" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Constructive Approximation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Devore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Lorentz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Grundlehren Math. Wiss.</title>
		<imprint>
			<biblScope unit="volume">303</biblScope>
			<date type="published" when="1993">1993</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On asymptotics and estimates for the uniform norms of the Lagrange interpolation polynomials corresponding to the Chebyshev nodal points</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Dzjadyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">V</forename><surname>Ivanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Anal. Math</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="85" to="97" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On interpolation. I. Quadrature-and mean-convergence in the Lagrange-interpolation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Erdös</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Turán</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. of Math</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="142" to="155" />
			<date type="published" when="1937">1937</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Monte Carlo: Concepts, Algorithms, and Applications</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Fishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oper. Res</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Finite elements for elliptic problems with stochastic coefficients</title>
		<author>
			<persName><forename type="first">P</forename><surname>Frauenfelder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Todor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Appl. Mech. Engrg</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="205" to="228" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nonlocal and localized analysis of conditional mean steady state flow in bounded, randomly nonuniform domains. 1. Theory and computational approach. 2. Computational examples</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gaudagnini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Water Resources Res</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2999" to="3039" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Numerical integration using sparse grids</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gerstner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Griebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numer. Algorithms</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="209" to="232" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dimension-adaptive tensor-product quadrature</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gerstner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Griebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="65" to="87" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Spanos</surname></persName>
		</author>
		<title level="m">Stochastic Finite Elements: A Spectral Approach</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Grigoriu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stochastic Calculus, Appl. Sci. Engrg., Birkhäuser</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Uncertainty propagation using Wiener-Haar expansions</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">P</forename><surname>Le Maître</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Knio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">N</forename><surname>Najm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">197</biblScope>
			<biblScope unit="page" from="28" to="57" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Probability Theory, I, 4th ed</title>
		<author>
			<persName><forename type="first">M</forename><surname>Loève</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Grad. Texts in Math</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1977">1977</date>
			<biblScope unit="volume">45</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Loève</surname></persName>
		</author>
		<title level="m">Probability Theory, II, 4</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1978">1978</date>
			<biblScope unit="volume">46</biblScope>
		</imprint>
	</monogr>
	<note>Grad. Texts in Math</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stochastic approaches to uncertainty quantification in CFD simulations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mathelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Hussaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Zang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numer. Algorithms</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="209" to="236" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Galerkin methods for linear and nonlinear elliptic stochastic partial differential equations</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Matthies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Keese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Appl. Mech. Engrg</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="1295" to="1331" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">F</forename><surname>Riesz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sz-Nagy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Functional Analysis</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Dover</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stochastic Galerkin method for elliptic SPDEs: A white noise approach</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Roman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sarkis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Contin. Dyn. Syst. Ser. B</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="941" to="955" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Quadrature and interpolation formulas for tensor products of certain classes of functions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Smolyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dokl. Akad. Nauk SSSR</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="240" to="243" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Redistribution subject to SIAM license or copyright</title>
		<idno>Downloaded 12/30/12 to 139.184.30.136</idno>
		<ptr target="http://www.siam.org/journals/ojsa.php" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Tatang</surname></persName>
		</author>
		<title level="m">Direct Incorporation of Uncertainty in Chemical and Environmental Engineering Systems</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
		<respStmt>
			<orgName>MIT</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Sparse Perturbation Algorithms for Elliptic PDE&apos;s with Stochastic Data, Dissertation 16192</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Todor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>ETH</publisher>
			<pubPlace>Zurich, Switzerland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Is Gauss quadrature better than Clenshaw-Curtis?</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Trefethen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="67" to="87" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Explicit cost bounds of algorithms for multivariate tensor product problems</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Wasilkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Woźniakowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Complexity</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="56" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<author>
			<persName><forename type="first">N</forename><surname>Wiener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The homogeneous chaos</title>
		<imprint>
			<date type="published" when="1938">1938</date>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="897" to="936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Groundwater flow in heterogeneous composite aquifers</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tartakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Water Resources Res</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1148</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">High-order collocation methods for differential equations with random inputs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Hesthaven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1118" to="1139" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Modeling uncertainty in steady state diffusion problems via generalized polynomial chaos</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Appl. Mech. Engrg</title>
		<imprint>
			<biblScope unit="volume">191</biblScope>
			<biblScope unit="page" from="4927" to="4948" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The Wiener-Askey polynomial chaos for stochastic differential equations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="619" to="644" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
