<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Medusa: Simplified Graph Processing on GPUs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jianlong</forename><surname>Zhong</surname></persName>
							<email>jzhong2@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<postCode>639798</postCode>
									<settlement>Singapore</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bingsheng</forename><surname>He</surname></persName>
							<email>bshe@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<postCode>639798</postCode>
									<settlement>Singapore</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Medusa: Simplified Graph Processing on GPUs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">01CD5324548DA0C5A3BD58C04E4CF02B</idno>
					<idno type="DOI">10.1109/TPDS.2013.111</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>GPGPU</term>
					<term>GPU Programming</term>
					<term>Graph Processing</term>
					<term>Runtime Framework</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graphs are common data structures for many applications, and efficient graph processing is a must for application performance. Recently, the graphics processing unit (GPU) has been adopted to accelerate various graph processing algorithms such as BFS and shortest paths. However, it is difficult to write correct and efficient GPU programs and even more difficult for graph processing due to the irregularities of graph structures. To simplify graph processing on GPUs, we propose a programming framework called Medusa which enables developers to leverage the capabilities of GPUs by writing sequential C/C++ code. Medusa offers a small set of user-defined APIs, and embraces a runtime system to automatically execute those APIs in parallel on the GPU. We develop a series of graph-centric optimizations based on the architecture features of GPUs for efficiency. Additionally, Medusa is extended to execute on multiple GPUs within a machine. Our experiments show that (1) Medusa greatly simplifies implementation of GPGPU programs for graph processing, with many fewer lines of source code written by developers;</p><p>(2) The optimization techniques significantly improve the performance of the runtime system, making its performance comparable with or better than manually tuned GPU graph operations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>G RAPHS are common data structures in various applications such as social networks, chemistry and web link analysis. Graph processing algorithms have been the fundamental tools in various fields. Developers usually apply a series of operations on the graph edges and vertices to obtain the final result. The example operations can be breadth first search (BFS), PageRank <ref type="bibr" target="#b31">[32]</ref>, shortest paths and even their customized variants (for example, developers may apply different application logics on top of BFS). The efficiency of graph processing is a must for high performance of the entire system. On the other hand, writing every graph processing algorithm from scratch is inefficient and involves repetitive work, since different algorithms may share the same operation patterns, optimization techniques and common software components. A programming framework supporting high programmability for various graph processing applications and providing high efficiency as well can greatly improve productivity.</p><p>Recent years have witnessed the increasing adoption of GPGPU (General-Purpose computation on Graphics Processing Units) in many applications <ref type="bibr" target="#b30">[31]</ref>. The GPU has been used as an accelerator for various graph processing applications <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b34">[35]</ref>. While those GPU-based solutions have demonstrated significant performance improvement over CPU-based implementations, they are limited to spe-cific graph operations. Developers usually need to implement and optimize GPU programs from scratch for different graph processing tasks.</p><p>Writing a correct and efficient GPU program is challenging in general, and even more difficult for graph applications. First, the GPU is a many-core processor with massive thread parallelism. To fully exploit the GPU parallelism, developers need to write parallel programs that scale to hundreds of cores. Moreover, compared with CPU threads, the GPU threads are lightweight, and the tasks in the parallel algorithms should be fine grained. Second, the GPU has a memory hierarchy that is different from the CPU's. Since graph applications usually involve irregular accesses to the graph data, careful designs of data layouts and memory accesses are key factors to the efficiency of GPU acceleration. Finally, since the GPU is designed as a co-processor, developers have to explicitly perform memory management on the GPU, and deal with GPU specific programming details such as kernel configuration and invocation. All these factors make the GPU programming a difficult task.</p><p>To ease the pain of leveraging the GPU in common graph computation tasks, we propose a software framework named Medusa to simplify programming graph processing algorithms on the GPU. Inspired by the bulk synchronous parallel (BSP) model, we develop a novel graph programming model called "Edge-Message-Vertex" (EMV) for fine-grained processing on vertices and edges. EMV is specifically tailored for parallel graph processing on the GPU. Like existing programming frameworks such as MapReduce <ref type="bibr" target="#b8">[9]</ref> and its variant on the GPU <ref type="bibr" target="#b14">[15]</ref>, Medusa provides a set of APIs for developers to implement their applications. The APIs are oriented at the EMV programming model for fine-grained parallelism. Medusa embraces an efficient message passing based runtime. It automatically executes userdefined APIs in parallel on all the processor cores within the GPU and on multiple GPUs, and hides the complexity of GPU programming from developers. Thus, developers can write the same APIs, which automatically run on multiple GPUs.</p><p>Memory efficiency is often an important factor for the overall performance of graph applications <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b34">[35]</ref>. To improve the memory efficiency of Medusa, we have developed a series of memory optimizations. A novel graph layout is developed to exploit the coalesced memory feature of the GPU. A graph aware message passing mechanism is specially designed for message passing in Medusa. We also develop two multi-GPU-specific optimization techniques, including the cost model guided replication for reducing data transfer across the GPUs and overlapping between computation and data transfer.</p><p>We have evaluated the efficiency and programmability of Medusa on a machine with four NVIDIA C2050 GPUs and two Intel E5645 CPUs. To demonstrate the programmability of Medusa, we develop a set of common graph processing primitives on sparse graphs and compare Medusa-based implementations with manual implementations. The CPU-based manual implementations are based on the MultiThreaded Graph Library (MTGL) <ref type="bibr" target="#b6">[7]</ref>, and we adopt previous GPU implementations <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b29">[30]</ref> as GPU-based manual implementations.</p><p>Our experimental results show that: (1) Medusa simplifies programming GPU graph processing algorithms in terms of a significant reduction in the number of source code lines. Medusa achieves comparable or better performance than the manually tuned GPU graph operations. (2) Our optimization techniques on graph layout and message buffering significantly improve the performance of graph processing operations on the GPU. (3) Medusa executing on four GPUs is up to 1.8 and 2.6 times faster than on a single GPU for BFS and PageRank, respectively.</p><p>Organization. The remainder of this paper is organized as follows. Section 2 reviews the related work. Section 3 describes the system overview, followed by detailed design in Section 4. We present evaluation results in Section 5, and conclude in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Processing</head><p>Parallel algorithms have been a classical way to improve the performance of graph processing. On multi-core CPUs, parallel libraries such as MTGL <ref type="bibr" target="#b6">[7]</ref> have been developed for parallel graph algorithms. Similar to Medusa, MTGL offers a set of data structures and APIs for building graph algorithms. The MTGL API is modeled after the Boost Graph Library <ref type="bibr" target="#b33">[34]</ref> and optimized to leverage shared memory multithreaded machines. The SNAP framework <ref type="bibr" target="#b4">[5]</ref> provides a set of algorithms and building blocks for graph analysis, especially for small-world graphs. To facilitate developing distributed graph algorithms in the cluster/grid settings, software libraries such as Parallel BGL <ref type="bibr" target="#b12">[13]</ref> and Combinatorial BLAS <ref type="bibr" target="#b7">[8]</ref> have been developed. Cloud platforms are becoming popular for graph applications <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b28">[29]</ref>.</p><p>Previous studies <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> have observed that many common graph algorithms can be formulated using a form of the bulk synchronous parallel (BSP) model (we call it GBSP). In GBSP, local computations are performed on individual vertices. Vertices are able to exchange data with each other. The same computation and communication procedures are executed iteratively with barrier synchronization at the end of each iteration. This common algorithmic pattern is also adopted by distributed graph processing frameworks such as Pregel <ref type="bibr" target="#b28">[29]</ref> and distributed GraphLab <ref type="bibr" target="#b26">[27]</ref>. For example, Pregel applies a user-defined function Compute() on each vertex in parallel in each iteration of the GBSP execution. The communications between vertices are performed with message passing interfaces. Medusa shares the same design goal as Pregel in providing a programming framework to ease development of graph algorithms, and in hiding the complexity of the underlying runtime from developers.</p><p>Medusa differs from Pregel in the following aspects. First, the design, implementation and optimization of Medusa are specific to the hardware features of GPUs. For example, our multi-GPU Medusa adopts graph partitioning to reduce data transfer on the host-device communication link (i.e., PCI-e bus), while Pregel uses random hashing by default. Second, Medusa provides more fine-grained programming interfaces than Pregel, exposing fine-grained data parallelism on edges, vertices and messages. Finally, Medusa does not have the sophisticated design for distributed systems, such as failure handling.</p><p>More recently, the GraphLab2 project <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b23">[24]</ref> further decomposes the vertex-program abstraction into small pieces, which also offer fine-grained parallelism like our EMV model. Green-Marl <ref type="bibr" target="#b17">[18]</ref> is another recent effort on easing the difficulty of optimizing GPU graph analysis algorithms, which uses domainspecific language (DSL) to provide developers a high level language interface. In comparison with Medusa, Green-Marl processes all vertices with a foreach loop in the order of BFS or DFS, and does not use message passing mechanisms of the GBSP model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">GPGPU</head><p>In this work, we adopt NVIDIA CUDA as our development platform. The GPU consists of an array of streaming multiprocessors (SM). Inside each SM is a group of scalar cores. CUDA allows developers to write device programs, which are called kernels, to run on hundreds of GPU cores with thousands of threads. Each 32 of the massive number of threads are grouped as a warp and execute synchronously on one SM. Divergence inside a warp is supported but may introduce a severe performance penalty since different paths are executed serially. An important memory feature exposed by CUDA is called coalesced accesses. If memory requests issued by a warp fall into the same memory segment, they are coalesced into one, thus significantly improving memory bandwidth utilization. Different from common CPUs, the CUDA memory hierarchy includes a scratchpad memory called shared memory which has much lower latency than the device memory.</p><p>With massive parallelism, GPUs have been adopted to accelerate graph processing. Harish et al. <ref type="bibr" target="#b13">[14]</ref> investigated the design and implementation of several most commonly used graph algorithms on GPUs, including BFS, single source shortest paths (SSSP) and all-pair shortest paths (APSP). Hong et al. proposed a virtual warp-centric <ref type="bibr" target="#b18">[19]</ref> GPU BFS algorithm with optimization techniques such as deferring outliers to address irregularities of the graph data structure. Compared with Harish's work, the warp-centric method achieved notable speedup when the input graph is highly irregular. Luo et al. <ref type="bibr" target="#b27">[28]</ref> and Merrill et al. <ref type="bibr" target="#b29">[30]</ref> implemented BFS with queue structures to store the frontier vertices or edges in order to reduce excessive accesses. Most existing GPU graph processing studies focus on specific algorithms.</p><p>Both Medusa and our previous work Mars <ref type="bibr" target="#b14">[15]</ref> are designed as programming frameworks to simplify parallel GPU programming with sequential interfaces. Medusa is specifically designed for graph processing. We have also addressed some inefficient designs of Mars, e.g., the graph-aware message passing mechanism for Medusa avoids the costly pre-counting result output mechanism in Mars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OVERVIEW</head><p>The following two design goals guide our design to make a useful programming framework for different graph processing algorithms. Particularly, programmability is our first-class design goal, and our overall goal is to offer a highly programmable graph processing framework for different applications with reasonable performance.</p><p>We present our techniques for directed graphs, and the techniques are applicable to undirected graphs. In a directed graph, we define an edge s → t, where s is the head vertex and t is the tail vertex. We say the edge is associated with s. Each vertex in the graph has a unique ID ranging in [0, V -1], where V is the number of vertices in the graph. For the set of edges associated with the same vertex, we assign a unique local ID for each edge ranging in [0, d -1], where d is the out-degree of the vertex. d max is defined as the maximum value of the out-degrees in the graph.</p><p>In the remainder of this section, we present the programming interface and workflow of Medusa, mainly from the developers' perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Programming Interface</head><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the system architecture of Medusa. Medusa is able to run on one or multiple GPUs in the same machine. In this section, we give an overview of the entire system from the developers' perspective on how they use Medusa. The detailed designs are described in Section 4.</p><p>Previous studies <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b28">[29]</ref> have shown that the GBSP model greatly simplifies the composition of graph algorithms by offering a sequential programming interface oriented on individual vertices. This model is derived from the observation of two common access patterns in various graph applications. First, the processing of vertices and edges is often localized within neighboring vertices. Second, many graph applications have multiple iterations where many edges and vertices are accessed and updated within an iteration. Most GBSP-based systems provide a single vertex-based API.</p><p>The EMV model of Medusa enhances the current single vertex-based API design to support efficient and fine-grained graph processing on the GPU. In particular, Medusa offers the following two mechanisms for programmability and efficiency.</p><p>First, Medusa provides six device code APIs for developers to write GPU graph processing algorithms, as shown in Table <ref type="table" target="#tab_0">1</ref>. Each API is either for processing vertices (VERTEX ), edges (ELIST , EDGE ) or messages (MESSAGE , MLIST ). Using these APIs, programmers can define their computation on vertices, edges and messages. The vertex and edge APIs can also send messages to neighboring vertices. The idea of providing six APIs is mainly for efficiency (The details are presented in Section 4.1).</p><p>Second, Medusa hides the GPU-specific programming details with a small set of system  Given user-defined data structures and definitions of device code APIs, the Medusa front end automatically transforms them into compilable CUDA kernels and related device management code. The design goal of the front end is to hide GPU specific programming details. After the preprocessing using the front end, the program is compiled and linked with the Medusa libraries.</p><p>In the storage component, Medusa allows developers to initialize the graph structure by adding vertices and edges with two system provided APIs, namely AddEdge and AddVertex . After initialization, the storage component stores the graph with the optimized graph layout on the GPU (Section 4). Note, the memory management on the GPU and data transfer between the GPU memory and the main memory is managed by Medusa, which is transparent to developers.</p><p>The Medusa runtime is responsible for executing the user-defined APIs in parallel on the GPU. Medusa offers two system provided APIs for execution, Medusa :: Run(Func f ) and EMV &lt;type&gt;:: Run(Func f ). Medusa :: Run(Func f ) is the main entry of the Medusa execution, and executes function f according to the iteration control policy, where f usually consists of an execution sequence of the EMV Fig. <ref type="figure">2</ref>. User-defined functions in PageRank implemented with Medusa.</p><p>APIs. EMV &lt;type&gt;:: Run(Func f ) executes an EMV user-defined API on the graph storage according to type (type ∈ {ELIST, EDGE, MLIST, MESSAGE, MLIST}).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Medusa Workflow</head><p>There are three steps to implement a graph algorithm based on Medusa. First, the developer defines the basic data structures such as edge, message and vertex in C/C++ structs. Second, the developer implements EMV APIs according to his/her application logic. Third, the developer composes the main program, including initializing the graph structure, configuring the framework parameters and invoking the customized EMV APIs with the system provided APIs (in Table <ref type="table" target="#tab_1">2</ref>).</p><p>Many graph computation tasks require multiple iterations until convergence. To support iterations, Medusa provides two interfaces for controlling the number of iterations of the execution. Developers can use both of them for a more flexible iteration control. First, the developer can specify the maximum number of iterations, maxIteration. Medusa terminates when the number of iterations reaches the predefined limit. Second, Medusa has defined a global variable halt, which can be modified by the EMV APIs. By initializing halt as false, the framework continues the iterations until any of the API instance sets halt to be true. This is equivalent to all API instances needing to vote false to continue the iteration. This iteration control mechanism is also used in Pregel <ref type="bibr" target="#b28">[29]</ref>.</p><p>To demonstrate the usage of Medusa, we show an example of the PageRank implementation with Medusa, as shown in Figure <ref type="figure">2</ref>. Data structures (e.g., vertex) are defined. The function PageRank() is composed of three user-defined EMV API function calls: an ELIST type API (SendRank), a message Combiner and a VERTEX type API (UpdateRank). In the main function, we configure the execution parameters such as the Combiner data type and operation type, the number of GPUs to use and the maximum number of iterations. Init Device DS automatically builds the graph data structures and copies them to the GPU. Medusa::Run(PageRank) invokes the PageRank function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SYSTEM DESIGN</head><p>This section details the design and implementation of Medusa. The Medusa runtime involves advanced and complicated mechanisms and implementations in order to improve the efficiency with the constraint of preserving high programmability. Most runtime optimizations are entirely transparent to developers. Some implementations may be seemingly trivial for specific applications, but become challenging to integrate into a framework to support general graph processing operations. In particular, Medusa focuses on processing sparse graphs.</p><p>Table <ref type="table" target="#tab_2">3</ref> presents a summary of the list of optimizations in Medusa and their respective advantages. The proposed optimizations enable Medusa to better exploit massive parallelism and memory features of the GPU while preserving the simple programming interface at the same time. For multi-GPU execution, the graph is partitioned using METIS <ref type="bibr" target="#b21">[22]</ref>. Due to space limitations, we present the details on the GPUtransparent programming interface and graph layouts in Appendix A of the supplementary file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Fine-Grained Graph APIs</head><p>Most GBSP model based systems provide a single vertex centered API. Programmers use the single vertex API to access all associated edges and messages (one typical access pattern is iterating edges/messages one by one). While the single vertex-based API design of the GBSP model has achieved good performance and programmability on distributed systems like Pregel <ref type="bibr" target="#b28">[29]</ref>, such coarse-grained designs are inefficient on GPUs due to execution divergence and irregular memory access. The vertex-based API exhibits severe divergence which makes it unsuitable for GPU execution. First, different vertices may have different numbers of edges, leading to different workloads on each API instance. Second, different number of received messages is another source of divergence. As for memory efficiency, the vertex-centric API makes the memory optimizations on edges and messages a challenging task.</p><p>To address those issues, we propose the EMV model as an extension of GBSP. It decouples the single vertex API into separate APIs which target individual vertices, edges or messages. Each GPU thread executes one instance of the user-defined API. The thread configuration such as the number of threads is tuned to maximize GPU utilization. The fine-grained data parallelism exposed by the EMV model can better exploit the massive parallelism of the GPU.</p><p>In addition, Medusa supports two variants of APIs for individual and collective operations of edges and messages associated with the same vertex. The collective APIs allow developers to access the elements in each edge-list (the set of edges associated with the same head vertex) or a message-list (the set of messages sent to the same vertex) sequentially. On the other hand, the individual APIs support operations on individual edges, vertices or messages and expose more parallelism. Medusa also provides a Combiner interface, with which developers can apply an associative operator to all the elements of each edge-list and message-list. All these APIs require no parallel programming, and developers write conventional sequential code to implement those APIs.</p><p>The collective APIs forms a superset of the individual APIs in terms of expressibility. Operations which involve dependent computation (e.g., the computation on one edge depends on other edges in the same edge-list) can only be implemented by collective APIs. However, we have observed that many graph algorithms do not need dependent computation on the edge-lists or message-lists. Choosing individual graph elements yields better workload balance and more parallelism. Moreover, many dependent computations are associative operations, for example, PageRank sums the values of received messages of each vertex to update rank values. This enables us to use the Combiner interface. The Combiner interface is implemented as segmented scan, which has the load-balanced implementation on GPUs <ref type="bibr" target="#b32">[33]</ref>.</p><p>By default, Medusa applies the user-defined API on the vertices/edges on the entire graph. This may result in work-suboptimal algorithms for some  applications such as BFS and SSSP. In order to allow developers to implement work-efficient algorithms, we have added an additional device code API called SetActive(vertexID/edgeID), and developers are able to indicate whether a vertex or an edge is active in the next EMV API call. The active edges/vertices are maintained in a dynamic queue. We implement the queue structure following the previous study <ref type="bibr" target="#b29">[30]</ref> , where we do not have specific order of enqueuing vertices or edges. In subsequent API invocations, developers are able to apply the EMV APIs to the active vertices and edges only and thus implement more work efficient algorithms. With the SetActive API, we have implemented work-efficient BFS and SSSP algorithms and experimentally evaluated their performance (described in Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Graph-Aware Buffer Scheme</head><p>Messages are temporarily stored in buffers, allocated by calling the system provided API InitMessageBuffer . We first discuss two basic buffer schemes, array-and list-based buffer schemes with respect to the memory efficiency of sending and receiving messages. The array-based buffer scheme is to allocate an array for message storage. Implementing the buffer with a fixed-sized array, this buffer scheme requires the information of the buffer size as well as the output positions for each message to avoid conflicts. Even worse, if the messages to the same vertex are not stored consecutively, Medusa needs a grouping operation in order to support message processing in collective user-defined APIs. In contrast, the listbased buffer scheme relies on dynamic memory allocation. We adopt a hash table with dynamic mem-ory allocation <ref type="bibr" target="#b16">[17]</ref> to store messages. This method eliminates the pre-computation of message sizes and the grouping operation in the array-based storage scheme. However, the dynamic hash table requires atomic operations and the accesses to the hash table are minimally coalesced.</p><p>Neither of the two buffer schemes can achieve good performance on both storing and processing the messages. That motivates us to develop a buffer scheme to capture the best of both worlds. We observe that the messages are usually sent/received along the edge in the EMV model. Given the maximum number of messages that can be sent along each edge, we can compute (1) the maximum total number of messages;</p><p>(2) the maximum number of messages that each vertex can receive. The awareness of the graph structure helps us to allocate the buffer, and to obtain the write positions of the messages along each edge.</p><p>To avoid the grouping operation, we ensure that the write positions of the messages sent to the same vertex are consecutive. This is achieved with the idea of "reversed edge indexed message passing." While loading the graph, Medusa constructs a reverse graph by swapping the head and tail of each edge. The reverse graph is stored in AA format. We assign an rID (reverse ID) for each edge in the original graph, whereby the rID value of each edge equals the index of its reverse edge in the adjacency array. Figure <ref type="figure" target="#fig_1">3(b)</ref> shows the rID value for each edge in an example graph.</p><p>The rID definition has an important property: the rID values for the edges with the same tail vertex are consecutive integers. For example, the rIDs of the edges with the same tail vertex D in the original graph in Figure <ref type="figure" target="#fig_1">3</ref> are 4 and 5. We take advantage of this property to ensure that the write positions of the messages sent to the same vertex are consecutive.</p><p>The graph aware buffer scheme works as follows. First, a message buffer with (E × m) entries is allocated, where m is the maximum number of messages that can be sent via each edge. For example, m is equal to one in PageRank. Medusa allows developers to set the m value. Second, when a message is sent along an edge and the rID of that edge is k, the start position for the message generation IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication.</p><p>is (k × m) in the message buffer. Figure <ref type="figure" target="#fig_1">3(c)</ref> shows an example of the graph aware buffer scheme for PageRank (m = 1).</p><p>When sending messages, the rID values give the write locations for the message along each edge. When receiving messages, the messages for the same vertex are already stored together. Thus, all the messages are already grouped by the tail vertex. This is because of the property of the rID values. Thus, no additional grouping operation is needed. Moreover, the message buffer uses an array, and thus the memory efficiency of message processing is much higher than that of the list-based buffer scheme, as demonstrated in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Multi-GPU Execution</head><p>We first present a basic implementation of the multi-GPU extension, and then our multi-hop replication optimization to reduce the data transfer cost in the PCI-e bus. Our multi-hop replication scheme is inspired by stencil operation optimizations <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Differently, we target at partitioned graphs in multi-GPU environments.</p><p>Replication. To accommodate multi-GPU graph processing, we divide the graph into equal-sized partitions and store each partition on one GPU. We adopt the widely used graph partitioning tool METIS <ref type="bibr" target="#b21">[22]</ref> to partition the input graph. Clearly, the quality of graph partitioning has great effect on the amount of data transfer among different GPUs. It is our future work to investigate other graph partitioning algorithms.</p><p>Figure <ref type="figure" target="#fig_2">4</ref>(a) shows an example with three GPUs. A directed graph is partitioned into three parts and each part is stored on one GPU. In the design of Medusa, messages are passed along edges. Graph partitioning introduces cross-partition edges, whose head and tail vertices are in different partitions and hence stored on different GPUs.</p><p>In order to apply EMV APIs on each graph partition, we maintain replicas of the head vertices of all cross-partition edges in the partitions where the tail vertices reside (we call it the tail partition). Each cross partition edge is replicated in its tail partition, as shown in Figure <ref type="figure" target="#fig_2">4</ref>(b). Thus, messages are emitted directly from the replicas and every edge can access its head and tail vertices directly. The execution of EMV APIs is performed on each partition independently. After the execution, we update the replicas on each graph partition. The update requires the costly PCIe data transfer, which can become a bottleneck for some application such as BFS. We therefore propose a multi-hop replication scheme as well as overlapping on the computation and data transfer to alleviate the overhead of PCI-e data transfer.</p><p>Multi-hop Replication Scheme. When the inter-GPU communication time is dominant in the total execution time, reducing the time cost of communication can significantly improve the application performance. The multi-hop replication scheme presented alleviates the overhead of inter-GPU communication by reducing the number of times of replica update. Instead of only maintaining head vertices of crosspartition edges as replicas, we introduce the second hop replicas by replicating tail vertices of the first hop replicas. Similarly, more hops of replicas can be added to each partition. We call this approach as multi-hop replication scheme. Our multi-hop replication scheme is inspired by stencil operation optimizations <ref type="bibr" target="#b10">[11]</ref>. Due to the message propagation nature of the EMV model, replica update only needs to be carried out after every n iterations if there are n hops of replicas. We call n iterations as a round and one round has n stages. As the stages are carried out outer hops of replicas are marked as "outdated". That essentially uses the eventual consistency model, and the data are consistent after each round. Figure <ref type="figure" target="#fig_3">5</ref> shows an example of the same graph as in Figure <ref type="figure" target="#fig_2">4</ref>. Now Partition 2 and Partition 3 both maintain two-hop replication. The replicas need to be updated every two iterations, reducing the number of replica update by a half. In the first stage of each round, Medusa APIs are applied to all vertices in each partition. After that, the second hop replicas are outdated and are not processed in the second stage. After each round, the replicas are updated and a new round start.</p><p>As described above, increasing the number of replica hops can reduce the number of times of updating replicas. However, this scheme is not guaranteed to be beneficial compared with the basic replication scheme since more replicas and edges need to be processed. For example, maintaining multiple hops of replicas for dense graphs or small-world graphs with a small diameter can lead to explosive growth of replica vertices. However, since Medusa mainly deals with sparse graphs, multi-hop replication can be beneficial. For a given graph, we estimate the benefits of all possible hop numbers within the storage constraint and select the best one. Medusa uses a cost model to estimate the benefits of all possible hop numbers. More details can be found in Appendix A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS</head><p>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>We have conducted the evaluations on a workstation equipped with four NVIDIA Tesla C2050 GPUs, two Intel Xeon E5645 CPUs (totally 12 CPU cores at 2.4GHz) and 24GB RAM. Our workloads include a set of common graph processing operations for manipulating and visualizing a graph on top of Medusa. The graph processing operations include PageRank, breadth first search (BFS), maximal bipartite matching (MBM), and single source shortest paths (SSSP). In order to assess the queue-based design in Medusa, we have implemented two versions of BFS: BFS-N and BFS-Q for the implementations without and with the usage of SetActive APIs, respectively. Similarly, we have also implemented two versions of SSSP: SSSP-N and SSSP-Q without and with the usage of SetActive APIs, respectively. The implementation details are presented in Appendix B of the supplementary file. In the remainder of this section, we use "Medusa" to refer to the better-performing implementation of the two versions on BFS and SSSP, unless we specify "-N" and "-Q" explicitly.</p><p>Our experimental dataset includes two categories of sparse graphs: real-world and synthetic graphs. Table <ref type="table" target="#tab_3">4</ref> shows their basic characteristics. We use the GTgraph graph generator <ref type="bibr" target="#b1">[2]</ref> to generate power-law graph RMAT and Random graph. To evaluate MBM, we generate a synthetic bipartite graph (denoted as BIP), where vertex sets of two sides have one half of the vertices and the edges are randomly generated. The real world graphs are publicly available <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>.</p><p>All the experiments are executed for ten runs and the average execution time is reported. The difference among runs for the same experiment is smaller than 2%. For BFS and SSSP, we randomly choose 100 source vertices and report the average execution time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison with Manual Implementations</head><p>We first compare the Medusa BFS and SSSP implementations with manual implementations of GPU graph processing: Harish's work <ref type="bibr" target="#b13">[14]</ref> and Hong's work <ref type="bibr" target="#b18">[19]</ref>.</p><p>Harish's work provides an open-source implementation of BFS and SSSP using CUDA and we tune the thread configuration and shared memory optimizations according to the C2050 Fermi architecture. We use it as the basic implementation. We implement the virtual warp-centric BFS proposed in Hong's work <ref type="bibr" target="#b18">[19]</ref>. The underlying difference between the Medusa implementation and the warp-centric method is that Medusa applies L threads to a vertex if that vertex has L edges, while the warp-centric method applies a virtual warp to a vertex. As a result, our method incurs more memory accesses because we check the head vertex status for every edge.</p><p>Table <ref type="table" target="#tab_4">5</ref> shows the traversed edges per second (TEPS) comparison between the three implementations of BFS. Compared to the basic implementation, Medusa performs better on all graphs except KKT. Although Medusa incurs more memory access and runtime overhead than the highly optimized warpcentric method, Medusa outperforms warp-centric on some graphs and degrades the performance on other graphs. Note that the reported results of the warpcentric approach are better than those in the original paper <ref type="bibr" target="#b18">[19]</ref>, mainly because the GPU in our experiment is more powerful.</p><p>Figure <ref type="figure">6</ref> shows the performance comparison between Medusa and basic implementation of SSSP. Medusa provides comparable performance with the basic implementation except on Road and Huge. On large-diameter graphs such as Road and Huge, the performance of Medusa-based SSSP is notably worse than that of the basic implementation. This is because the Combiner API invocation in SSSP takes a large part of its execution time and that overhead is almost fixed for every iteration.</p><p>Programmability is difficult for a quantitative comparison. As a start, we show the programmability comparisons on some major implementation issues of GPU programs in Table <ref type="table" target="#tab_5">6</ref>. Medusa simplifies GPU programming for graph processing, by significantly reducing the number of GPU-related source code lines written by developers. This is because Medusa hides the GPU programming complexity by offering a small set of user-defined APIs. For example, developers only need to write 7 and 11 lines of source code for defining the APIs in BFS-Q and SSSP-Q, respectively, whereas the basic implementation <ref type="bibr" target="#b13">[14]</ref> has 56 and 59 lines of GPU-related code. Moreover, compared to manual implementations, Medusa requires no parallel or GPU specific programming.</p><p>Overall, Medusa offers reasonable performance in comparison with manual implementations. With  We present more experimental results on BFS. Table <ref type="table" target="#tab_6">7</ref> shows the comparison on BFS between Medusabased implementation and the Contract-Expand and Hybrid approaches in Merrill et al.'s paper <ref type="bibr" target="#b29">[30]</ref>. The Hybrid approach is more optimized than the Contract-Expand approach. For more details of those approaches, we refer the reader to the original paper <ref type="bibr" target="#b29">[30]</ref>. The design and implementation of Medusabased BFS is similar to the Contract-Expand approach, but targets at general graph processing. Overall, Medusa-based implementation can be slower than the Contract-Expand approach on some graphs such as Huge and KKT, and can be faster on other graphs such as Cite. On the other hand, Medusabased implementation is slower than the Hybrid approach on all the three graphs. Compared with various specific optimizations for BFS, Medusa involves Fig. <ref type="figure">6</ref>. Performance comparison between Medusa and existing GPU implementation of SSSP <ref type="bibr" target="#b13">[14]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiments on Efficiency</head><p>Overall comparisons. We implement the graph processing operations with MTGL <ref type="bibr" target="#b6">[7]</ref>, as the baseline for graph processing on multi-core CPUs.</p><p>The BFS and PageRank implementations are offered by MTGL and we implement the Bellman-Ford algorithm for single source shortest paths and a randomized maximal matching algorithm <ref type="bibr" target="#b3">[4]</ref> using the MTGL APIs. We tuned the number of threads in MTGL and report the best result obtained when the number of threads was 12 on our machine. MTGL running on 12 cores is on average 3.4 times faster than that running on one core. Due to the memory intensive nature of graph algorithms, the scalability of MTGL is limited by the memory bandwidth.</p><p>Figure <ref type="figure" target="#fig_4">7</ref> shows the speedup for Medusa over MTGL running on 12 cores. The speedup is defined as the ratio between the elapsed time of the CPU-based execution and that of Medusa-based execution. PageRank is executed with 100 iterations. Medusa is significantly faster than MTGL on most comparisons and delivers a performance speedup of 1.0-19.6 with an average of 5.5 (we report the better results of the two implementations of BFS and SSSP, respectively). On some graphs such as Road, BFS-N is notably slower than MTGL-based BFS, because the work-inefficient issue of BFS-N is exaggerated on the graphs with large diameter.</p><p>The work-efficient BFS and SSSP algorithms (BFS-Q and SSSP-Q) achieve better performance on the graphs with large diameters, and can degrade the performance in some cases (e.g., Rand, Wiki and KKT) due to the computation and memory overhead in maintaining the queue structure. This is consistent with the previous studies <ref type="bibr" target="#b18">[19]</ref>. Currently, we leave the decision on whether to use the SetActive API to the users. In the future work, we consider whether this decision can be made automatically in Medusa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>In this paper, we address the efficiency and programmability of GPU-based parallel graph processing by developing a programming framework named Medusa. Medusa embraces an optimized runtime system to hide the programming complexity of implementing parallel graph computation tasks for GPUs. Developers only need to write sequential programs to implement a small set of APIs. On an NVIDIA Tesla C2050 GPU, Medusa-based implementations are 5.5 times on average faster than the parallel MTGL based implementations on two Intel six-core CPUs. Moreover, with much less coding complexity, Medusa achieves comparable or even better performance than existing manual implementations. As for future work, we are interested in evaluating Medusa in other architectures such as Intel Xeon Phi and extending Medusa to distributed environments.</p><p>The source code of Medusa is available at http:// code.google.com/p/medusa-gpu/.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An overview of Medusa.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Graph aware buffer scheme.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Graph partitioning and replication: (a) direct partitioning; (b) replication for EMV executions (dashed circles represent the replicas).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Graph partitioning with multi-hop replication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Performance speedup of Medusa running on the GPU over MTGL [7] running on 12 cores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>User-defined APIs in the EMV model Edge-list el Collective Apply to edge-list el of each vertex v EDGE Edge e Individual Apply to each edge e MLIST Vertex v, Message-list ml Collective Apply to message-list ml of each vertex v</figDesc><table><row><cell>API Type</cell><cell>Parameters</cell><cell>Variant</cell><cell>Description</cell></row><row><cell cols="2">ELIST Vertex v, MESSAGE Message m</cell><cell cols="2">Individual Apply to each message m</cell></row><row><cell>VERTEX</cell><cell>Vertex v</cell><cell cols="2">Individual Apply to each vertex v</cell></row><row><cell>Combiner</cell><cell>Associative operation o</cell><cell cols="2">Collective Apply an associative operation to all edge-lists or message-lists</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2</head><label>2</label><figDesc>System provided APIs and parameters in Medusa</figDesc><table><row><cell>API/Parameter</cell><cell>Description</cell></row><row><cell>AddEdge (void* e), AddVertex (void* v)</cell><cell>Add an edge or a vertex into the graph</cell></row><row><cell>InitMessageBuffer (void* m) maxIteration</cell><cell>Initiate the message buffer The maximum iterations that Medusa executes (2 31 -1 by default)</cell></row><row><cell>halt</cell><cell>A flag indicating whether Medusa stops the iteration</cell></row><row><cell>Medusa :: Run(Func f )</cell><cell>Execute f iteratively according to the iteration control</cell></row><row><cell>EMV &lt;type&gt;:: Run(Func f )</cell><cell>Execute EMV API f with type on the GPU</cell></row></table><note><p><p><p>provided APIs (Table</p>2</p>). Particularly, Medusa provides EMV &lt; type &gt;:: Run() to invoke the device code API, which automatically sets up the thread block configurations and calls the corresponding EMV user-defined function. Medusa allows developers to define an iteration by running multiple EMV &lt; type &gt;:: Run() calls sequentially in one host function (invoked by Medusa :: Run()). The iteration is performed iteratively until predefined conditions are satisfied. Medusa offers a set of configuration parameters and utility functions for iteration control.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3</head><label>3</label><figDesc>Summary of techniques used in Medusa and their advantages</figDesc><table><row><cell>Problem</cell><cell>Solution</cell><cell>Advantage</cell></row><row><cell>Massive parallelism</cell><cell>EMV API</cell><cell>Fine grained parallelism for massive parallelism</cell></row><row><cell>Work efficiency</cell><cell>Queue-based implementation with our</cell><cell>Allow developing more work-efficient algorithm</cell></row><row><cell></cell><cell>SetActive API</cell><cell></cell></row><row><cell>GPU specific programming details</cell><cell>Automatic GPU specific code generation</cell><cell>Eliminate the GPGPU learning curve</cell></row><row><cell>Graph layout</cell><cell>Novel graph representation</cell><cell>Better memory bandwidth utilization</cell></row><row><cell>Message passing efficiency</cell><cell>Graph-aware buffer scheme</cell><cell>Better memory bandwidth utilization and avoid</cell></row><row><cell></cell><cell></cell><cell>message grouping overheads</cell></row><row><cell>Multi-GPU execution</cell><cell>Replication, memory transfer/computa-</cell><cell>Alleviate PCI-e overheads</cell></row><row><cell></cell><cell>tion overlapping</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4</head><label>4</label><figDesc>Characteristics of graphs used in the experiments</figDesc><table><row><cell>Graph</cell><cell>Vertices (10 6 )</cell><cell>Edges (10 6 )</cell><cell>d Max</cell><cell>d Avg</cell><cell>σ</cell></row><row><cell>RMAT</cell><cell>1.0</cell><cell>16.0</cell><cell>1742</cell><cell>16</cell><cell>32.9</cell></row><row><cell>Random (Rand)</cell><cell>1.0</cell><cell>16.0</cell><cell>38</cell><cell>16</cell><cell>4.0</cell></row><row><cell>BIP</cell><cell>4.0</cell><cell>16.0</cell><cell>40</cell><cell>4</cell><cell>5.1</cell></row><row><cell>WikiTalk (Wiki)</cell><cell>2.4</cell><cell>5.0</cell><cell cols="2">100022 2.1</cell><cell>99.9</cell></row><row><cell>RoadNet-CA (Road)</cell><cell>2.0</cell><cell>5.5</cell><cell>12</cell><cell>2.8</cell><cell>1.0</cell></row><row><cell>kkt power (KKT)</cell><cell>2.1</cell><cell>13.0</cell><cell>95</cell><cell>6.3</cell><cell>7.5</cell></row><row><cell>coPapersCiteseer</cell><cell>0.4</cell><cell>32.1</cell><cell>1188</cell><cell>73.9</cell><cell>101.3</cell></row><row><cell>(Cite)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>hugebubbles-00020</cell><cell>21.2</cell><cell>63.6</cell><cell>3</cell><cell>3.0</cell><cell>0.03</cell></row><row><cell>(Huge)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 5</head><label>5</label><figDesc>Traversed edge per second (10 6 TEPS) comparison with manual implementations<ref type="bibr" target="#b13">[14]</ref>,<ref type="bibr" target="#b18">[19]</ref>.</figDesc><table><row><cell></cell><cell>Basic</cell><cell>Warp-centric</cell><cell>Medusa</cell></row><row><cell>Wiki</cell><cell>61.4</cell><cell>152.9</cell><cell>1091.1</cell></row><row><cell>Road</cell><cell>26.2</cell><cell>45.7</cell><cell>63.5</cell></row><row><cell>RMAT</cell><cell>593.2</cell><cell>971.1</cell><cell>895.8</cell></row><row><cell>Rand</cell><cell>648.6</cell><cell>844.95</cell><cell>765.8</cell></row><row><cell>Huge</cell><cell>5.7</cell><cell>1.3</cell><cell>68.1</cell></row><row><cell>KKT</cell><cell>480.7</cell><cell>175.7</cell><cell>351.5</cell></row><row><cell>Cite</cell><cell>1460.4</cell><cell>1503.1</cell><cell>2686.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 6</head><label>6</label><figDesc>Coding complexity of Medusa implementation and manual implementations.</figDesc><table><row><cell></cell><cell>Baseline</cell><cell>Warp-centric</cell><cell>Medusa</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(N/Q)</cell></row><row><cell>GPU code lines (BFS)</cell><cell>56</cell><cell>76</cell><cell>9/7</cell></row><row><cell>GPU code lines (SSSP)</cell><cell>59</cell><cell>N.A.</cell><cell>13/11</cell></row><row><cell>GPU memory management</cell><cell>Yes</cell><cell>Yes</cell><cell>No</cell></row><row><cell>Kernel configuration</cell><cell>Yes</cell><cell>Yes</cell><cell>No</cell></row><row><cell>Parallel programming</cell><cell>Thread</cell><cell>Thread+Warp</cell><cell>No</cell></row><row><cell cols="4">different design goals, Medusa is to offer good pro-</cell></row><row><cell cols="4">grammability with reasonable performance, whereas</cell></row><row><cell cols="4">manual implementations usually do not consider</cell></row><row><cell cols="4">programmability. Some techniques that are applicable</cell></row><row><cell cols="4">to manual implementations may not be applicable to</cell></row><row><cell cols="3">Medusa, if they hurt programmability.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 7</head><label>7</label><figDesc>Traversed edge per second (10 9 TEPS) comparison with Merrill et al.'s paper<ref type="bibr" target="#b29">[30]</ref>.</figDesc><table><row><cell></cell><cell>Medusa</cell><cell>Contract-Expand [30]</cell><cell>Hybrid [30]</cell></row><row><cell>Huge</cell><cell>0.1</cell><cell>0.4</cell><cell>0.4</cell></row><row><cell>KKT</cell><cell>0.4</cell><cell>0.7</cell><cell>1.1</cell></row><row><cell>Cite</cell><cell>2.7</cell><cell>1.3</cell><cell>3.0</cell></row><row><cell cols="4">considerate runtime overhead in supporting general</cell></row><row><cell cols="4">graph processing, for example, message passing based</cell></row><row><cell cols="2">mechanisms.</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ACKNOWLEDGEMENT</head><p>The authors would like to thank the anonymous reviewers for their valuable comments, and Pawan Harish for providing the source code for CUDA-based BFS and shortest paths. This work is partly supported by a MoE AcRF Tier 2 grant (MOE2012-T2-2-067) and an NVIDIA Academic Partnership Award.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="http://www.cc.gatech.edu/dimacs10/index.shtml" />
		<title level="m">10th DIMACS implementation challenge</title>
		<imprint>
			<date type="published" when="2013-02-17">Feb 17th, 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="http://www.cse.psu.edu/∼madduri/software/GTgraph/index.html" />
		<title level="m">GTGraph generator</title>
		<imprint>
			<date type="published" when="2013-02-17">Feb 17th, 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<ptr target="http://snap.stanford.edu/data/index.html" />
		<title level="m">Stanford large network dataset collections</title>
		<imprint>
			<date type="published" when="2013-02-17">Feb 17th, 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">High-speed switch scheduling for local-area networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Owicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Thacker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="319" to="352" />
			<date type="published" when="1993-11">November 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SNAP, small-world network analysis and partitioning: An open-source parallel graph framework for the exploration of large-scale networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Madduri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPDPS</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Optimizing transformations of stencil operations for parallel objectoriented scientific frameworks on cache-based architectures</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bassetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computing in Object-Oriented Parallel Environments</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Software and algorithms for graph queries on multithreaded architectures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hendrickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Konecny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPDPS</title>
		<imprint>
			<date type="published" when="2007-03">March 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The Combinatorial BLAS: Design, implementation, and applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Buluc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. High Perform. Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2011-11">Nov. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MapReduce: Simplified data processing on large clusters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="113" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">GraphStep: A system architecture for sparse-graph algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Delorimier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kapre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rizzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Eslick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Uribe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dehon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FCCM</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cache oblivious stencil computations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Frigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Strumpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">PowerGraph: Distributed graph-parallel computation on natural graphs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Parallel BGL: A generic library for distributed graph computations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lumsdaine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel Object-Oriented Scientific Computing (POOSC)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Accelerating large graph algorithms on the GPU using CUDA</title>
		<author>
			<persName><forename type="first">P</forename><surname>Harish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HiPC</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mars: A MapReduce framework on graphics processors</title>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Govindaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACT</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Parallel SimRank computation on large graphs with iterative aggregation</title>
		<author>
			<persName><forename type="first">G</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">MapCG: Writing parallel program portable between CPU and GPU</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACT</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Green-Marl: A DSL for easy and efficient graph analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sedlar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Olukotun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<meeting><address><addrLine>London, England, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Accelerating CUDA graph algorithms at maximum warp</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Oguntebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Olukotun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PPoPP</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">HADI: Fast diameter estimation and mining in massive graphs with Hadoop</title>
		<author>
			<persName><forename type="first">U</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tsourakakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno>CMU-ML-08- 117</idno>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">PEGASUS: A peta-scale graph mining system -implementation and observations</title>
		<author>
			<persName><forename type="first">U</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Tsourakakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A fast and high quality multilevel scheme for partitioning irregular graphs</title>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="359" to="392" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">All-pairs shortest-paths for large graphs on the GPU</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kider</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graphics hardware</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">GraphChi: Large-scale graph computation on just a PC</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Blelloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Design patterns for efficient graph algorithms in MapReduce</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schatz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MLG</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">GraphLab: A new parallel framework for machine learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence (UAI)</title>
		<imprint>
			<date type="published" when="2010-07">July 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distributed GraphLab: A framework for machine learning and data mining in the cloud</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="716" to="727" />
			<date type="published" when="2012-04">April 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An effective GPU implementation of breadth-first search</title>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-M</forename><surname>Hwu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAC</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pregel: A system for large-scale graph processing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Malewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Austern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dehnert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Leiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Czajkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scalable GPU graph traversal</title>
		<author>
			<persName><forename type="first">D</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Garland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Grimshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PPoPP</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A survey of generalpurpose computation on graphics hardware</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luebke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Govindaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Lefohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Purcell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics</title>
		<title level="s">State of the Art Reports</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The PageRank citation ranking: Bringing order to the Web. Stanford InfoLab</title>
		<author>
			<persName><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficient parallel scan algorithms for GPUs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Garland</surname></persName>
		</author>
		<idno>NVR-2008-003</idno>
	</analytic>
	<monogr>
		<title level="j">NVIDIA</title>
		<imprint/>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The Boost Graph Library: User Guide and Reference Manual</title>
		<author>
			<persName><forename type="first">J</forename><surname>Siek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-Q</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lumsdaine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">CUDA cuts: Fast graph cuts on the GPU</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2008-06">June 2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
