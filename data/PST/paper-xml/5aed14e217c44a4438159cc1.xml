<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A taxonomy of task-based parallel programming technologies for high-performance computing</title>
				<funder ref="#_EKFf39R">
					<orgName type="full">AllScale EC-funded FET-HPC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-01-12">12 January 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Peter</forename><surname>Thoman</surname></persName>
							<email>petert@dps.uibk.ac.at</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Innsbruck</orgName>
								<address>
									<postCode>6020</postCode>
									<settlement>Innsbruck</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kiril</forename><surname>Dichev</surname></persName>
							<email>k.dichev@qub.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="institution">Queen&apos;s University of Belfast</orgName>
								<address>
									<postCode>BT7 1NN</postCode>
									<settlement>Belfast</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Heller</surname></persName>
							<email>thomas.heller@fau.de</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Erlangen-N?rnberg</orgName>
								<address>
									<postCode>91058</postCode>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Roman</forename><surname>Iakymchuk</surname></persName>
							<email>riakymch@kth.se</email>
							<idno type="ORCID">0000-0003-2414-700X</idno>
							<affiliation key="aff4">
								<orgName type="institution">KTH Royal Institute of Technology</orgName>
								<address>
									<addrLine>100 44 Stockholm</addrLine>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xavier</forename><surname>Aguilar</surname></persName>
							<email>xaguilar@kth.se</email>
							<affiliation key="aff4">
								<orgName type="institution">KTH Royal Institute of Technology</orgName>
								<address>
									<addrLine>100 44 Stockholm</addrLine>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Khalid</forename><surname>Hasanov</surname></persName>
							<email>khasanov@ie.ibm.com</email>
							<affiliation key="aff5">
								<orgName type="institution">IBM Ireland</orgName>
								<address>
									<addrLine>Dublin 15</addrLine>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Philipp</forename><surname>Gschwandtner</surname></persName>
							<email>philipp@dps.uibk.ac.at</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Innsbruck</orgName>
								<address>
									<postCode>6020</postCode>
									<settlement>Innsbruck</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pierre</forename><surname>Lemarinier</surname></persName>
							<email>pierrele@ie.ibm.com</email>
							<affiliation key="aff5">
								<orgName type="institution">IBM Ireland</orgName>
								<address>
									<addrLine>Dublin 15</addrLine>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stefano</forename><surname>Markidis</surname></persName>
							<email>markidis@kth.se</email>
							<affiliation key="aff4">
								<orgName type="institution">KTH Royal Institute of Technology</orgName>
								<address>
									<addrLine>100 44 Stockholm</addrLine>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Herbert</forename><surname>Jordan</surname></persName>
							<email>herbert@dps.uibk.ac.at</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Innsbruck</orgName>
								<address>
									<postCode>6020</postCode>
									<settlement>Innsbruck</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Fahringer</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Innsbruck</orgName>
								<address>
									<postCode>6020</postCode>
									<settlement>Innsbruck</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kostas</forename><surname>Katrinis</surname></persName>
							<email>katrinisk@ie.ibm.com</email>
							<affiliation key="aff5">
								<orgName type="institution">IBM Ireland</orgName>
								<address>
									<addrLine>Dublin 15</addrLine>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Erwin</forename><surname>Laure</surname></persName>
							<email>erwinl@kth.se</email>
							<affiliation key="aff4">
								<orgName type="institution">KTH Royal Institute of Technology</orgName>
								<address>
									<addrLine>100 44 Stockholm</addrLine>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dimitrios</forename><forename type="middle">S</forename><surname>Nikolopoulos</surname></persName>
							<email>d.nikolopoulos@qub.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="institution">Queen&apos;s University of Belfast</orgName>
								<address>
									<postCode>BT7 1NN</postCode>
									<settlement>Belfast</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Kiril Dichev</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A taxonomy of task-based parallel programming technologies for high-performance computing</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-01-12">12 January 2018</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1007/s11227-018-2238-4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Task-based programming models for shared memory-such as Cilk Plus and OpenMP 3-are well established and documented. However, with the increase in parallel, many-core, and heterogeneous systems, a number of research-driven projects have developed more diversified task-based support, employing various programming and runtime features. Unfortunately, despite the fact that dozens of different task-based systems exist today and are actively used for parallel and high-performance computing (HPC), no comprehensive overview or classification of task-based technologies for HPC exists. In this paper, we provide an initial task-focused taxonomy for HPC technologies, which covers both programming interfaces and runtime mechanisms. We</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Cilk language <ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b4">[5]</ref> allows task-focused parallel programming and is an early example of efficient task scheduling via work stealing. OpenMP <ref type="bibr" target="#b9">[10]</ref>, which we consider a language extension, integrates tasks into its programming interface since version 3.0. In addition to languages and extensions, industry-standard and wellsupported parallel libraries based on task parallelism have emerged, such as Intel Cilk Plus <ref type="bibr" target="#b22">[23]</ref> or Intel TBB <ref type="bibr" target="#b27">[28]</ref>. There are also runtimes specifically designed to improve shared memory performance of existing language extensions, such as Qthreads <ref type="bibr" target="#b26">[27]</ref> or Argobots <ref type="bibr" target="#b23">[24]</ref>; this topic is of significant importance, considering the increase in many-core processors in recent years and, consequently, the importance of efficient lightweight runtimes. Task-based environments for heterogeneous hardware have also naturally developed with the emergence of accelerator and GPU computing; StarPU <ref type="bibr" target="#b1">[2]</ref> is an example of such an environment.</p><p>In addition, task-based parallelism is increasingly employed on distributed memory systems, which constitute the most important target for HPC. In this context, tasks are often combined with a global address space (GAS) programming model such as GASPI <ref type="bibr" target="#b24">[25]</ref> and scheduled across multiple processes, which together form the distributed execution of a single task-parallel program. While some examples of global address space environments with task-based parallelism are specifically designed languages such as Chapel <ref type="bibr" target="#b6">[7]</ref> and X10 <ref type="bibr" target="#b7">[8]</ref>, it is also possible to implement these concepts as a library. For instance, HPX <ref type="bibr" target="#b16">[17]</ref> and Charm++ <ref type="bibr" target="#b17">[18]</ref> are asynchronous GAS runtimes.</p><p>This already very diverse landscape is made even more complex by the recent appearance of task-based runtimes using novel concepts, such as the data-centric programming language Legion <ref type="bibr" target="#b2">[3]</ref>. Many of these task-based programming environments are maintained by a dedicated community of developers and are often research oriented. As such, there might be relatively little accessible documentation of their features and inner workings.</p><p>Crucially, at this point, there is no up-to-date and comprehensive taxonomy and classification of existing common task-based environments. Consequently, researchers and developers with an interest in task-based HPC software development cannot obtain a concise picture of the alternatives to the omnipresent MPI programming model. In this work, we attempt to address this issue by first providing a taxonomy of task-based parallel programming environments. The applicability of this taxonomy is then validated by applying it to classify a number of task-based programming environments. While not all of these environments are equally mature and stable, they build an important snapshot of the task-based APIs and runtimes in use in HPC today. We consider that each task-based environment has two central components: a programming interface (API) and a runtime system; the former is the interface that a given environment provides to the programmer, while the latter encompasses the underlying implementation mechanisms.</p><p>The remainder of this article is organized as follows. In Sect. 2, we present a set of API characteristics allowing meaningful classification. For discussing the more involved topic of runtime mechanisms, in Sect. 3, we further structure our analysis into the overarching topics of scheduling (Sect. 3.1), performance monitoring (Sect. 3.2), and fault handling (Sect. <ref type="bibr">3.3)</ref>. Based on the taxonomy introduced, we classify and categorize existing APIs and runtimes in Sect. 4. Finally, Sect. 5 concludes the article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task-parallel programming interfaces</head><p>The application programming interface (API) of a given task-parallel programming environment defines the way an application developer describes parallelism, dependencies, and in many cases other more specific information such as the work mapping structure or data distribution options. As such, finding a way to concisely characterize APIs from a developer's perspective is crucial in providing an overview of task-parallel technologies.</p><p>In this work, we define a set of characterizing features for such APIs which encompasses all relevant aspects while remaining as compact as possible; Fig. <ref type="figure">1</ref> presents our API taxonomy. A subset of these features was adapted from previous work by Kasim et al. <ref type="bibr" target="#b18">[19]</ref>. To these existing characteristics, we added additional information of general importance-such as technological readiness levels-as well as features which relate to new capabilities particularly relevant for modern HPC like support for heterogeneity and resilience management.</p><p>We have grouped these aspects in four broad categories: those describing the architectural aspects targeted by an environment; those that summarize the task system We list below these four categories with the corresponding aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Architectural</head><p>Communication model Either shared memory (smem), message passing (msg), or a global address space (gas).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distributed memory</head><p>Whether targeting distributed memory systems is supported. Options are no support, explicit support, or implicit support. explicit refers to, for example, message passing between address spaces, while automatic data migration would be an example of implicit support.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Heterogeneity</head><p>This indicates whether tasks can be executed on accelerators (e.g. GPUs). Explicit support indicates that the application developer has to actively provision tasks to run on accelerators, using a distinct API.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Task system</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph structure</head><p>The type of task graph dependency structure supported by the given API. Possibilities include a tree structure, an acyclic graph (dag), or an arbitrary graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task partitioning</head><p>This feature indicates whether each task is atomic-can, thus, only be scheduled as a single unit-or can be subdivided/split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Result handling</head><p>Whether the tasking model features explicit handling of the results of task computations. For example, return types accessed as futures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task cancellation</head><p>Whether the tasking model supports cancellation of tasks.</p><p>Options are no cancellation support; cancellation is supported either cooperatively (only at task scheduling points) or preemptively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Management</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Worker management</head><p>Whether the worker threads and/or processes need to be started and maintained by the user (explicit) or are provided automatically by the environment (implicit). Resilience management This feature describes whether the API has support for task resilience management, e.g. fine-grained checkpointing and restart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Work mapping</head><p>This feature describes the way tasks are mapped to the existing hardware resources. Possibilities include explicit work mapping, implicit work mapping (e.g. stealing), or patternbased work mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synchronization</head><p>Whether tasks are synchronized in an implicit fashion, e.g. by regions or the function scope, or explicitly by the application developer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Engineering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Technology readiness</head><p>The technology readiness of the given API according to the European Commission definition. <ref type="foot" target="#foot_1">2</ref> If an API has multiple implementations, the most mature one is used to assess this metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation type</head><p>How the API is implemented and addressed from a program. A tasking API can be provided either as a library, a language extension, e.g. pragmas, or an entire language with task integration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Many-task runtime systems</head><p>Many-task runtime systems serve as the basis for implementing the APIs described in Sect. 2 and are considered a promising tool in addressing key issues associated with Exascale computing. In this section, we provide a taxonomy of many-task runtime systems, which is illustrated in Fig. <ref type="figure" target="#fig_0">2</ref>.</p><p>A crucial difference among various many-task runtime systems is their target architecture. The evolution of many-task runtime systems started from homogeneous shared memory computers with multiple cores and continued towards runtimes for heterogeneous shared memory and distributed memory platforms. Support for distributed memory architectures varies significantly across different systems: in case of implicit data distribution, data distribution is handled by the runtime, without putting any burden on the application developer; on the other hand, explicit data distribution means that distribution across nodes is explicitly specified by the programmer.</p><p>Modern HPC systems require efficiency not only in execution times, but also in power and/or energy consumption. Thus, whether the runtime provides scheduling targets (Sect. 3.1.1) other than the execution time is another important distinction between different runtimes. At the same time, the runtime can achieve its scheduling target by using different scheduling methods (Sect. 3.1.2). We divide them into three categories, namely static, dynamic, and hybrid scheduling methods. Some of them provide automatic scheduling within a single shared memory machine, while the application developer needs to handle distributed memory execution explicitly, whereas others provide uniform scheduling policies across different nodes. Many-task runtimes may require performance introspection and monitoring (Sect. 3.2) to facilitate the implementation of different scheduling policies, that is, using online performance information to assist the decision making process of the scheduler. Fault tolerance is another key factor that is important in many-task runtime systems in the context of Exascale requirements. As detailed in Sect. 3.3, a runtime may have no resilience capabilities, or it may target task faults or even process faults.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Scheduling in many-task runtime systems</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Scheduling targets</head><p>Depending on the capabilities of the underlying many-task runtime system, its scheduling domain is usually limited to a single shared memory homogeneous compute node, a heterogeneous compute node with accelerators, homogeneous distributed memory systems of interconnected compute nodes, or in a most generic form to heterogeneous distributed memory systems. By supporting different types of heterogeneous architectures, the runtime can facilitate source code portability and support transparent interaction between different types of computation units for application developers.</p><p>Traditionally, execution time has been the main objective to minimize for different scheduling policies. However, the increasing scale of HPC systems makes it necessary to take the energy and power budgeting of the target system into account as well. Therefore, some many-task runtime systems have already started providing energyaware <ref type="bibr" target="#b19">[20]</ref> scheduling policies. <ref type="foot" target="#foot_2">3</ref> In a simple scenario, it is assumed that the application can provide an energy consumption model which can be used by a scheduling policy as part of its objective function. In more advanced cases, the runtime provides offline or online profiling data. These data are used to build a look-up table that maps each frequency setting with profiling data and the number of active cores. Then, a scheduling decision is made based on this information.</p><p>In addition, recent research projects such as AllScale<ref type="foot" target="#foot_3">4</ref> focus on multi-objective scheduling policies trying to find promising trade-off solutions among conflicting optimization objectives like execution time, energy consumption, and/or resource utilization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Scheduling methods</head><p>Extensive research has been conducted in task scheduling methodologies. We simply highlight the state-of-the-art scheduling methods used in many-task runtime systems, without providing an exhaustive overview. The distribution of work at compile time is static scheduling, while the distribution of work at runtime is dynamic scheduling.</p><p>For static scheduling, depending on the decision function, either one or more of the following inputs are known in advance: execution time of each task, inter-dependencies between tasks, task precedence, resource usage of each task, location of the input data, task communications, and synchronization points. Static scheduling algorithms used in many-task runtime systems are based on distributing a global task list to different compute units; a typical example is a round-robin distribution.</p><p>On the other hand, dynamic scheduling is used if the requested information is not available before execution, or obtaining such information is too complex. Possibly, the most important choice for dynamic scheduling is load balancing; its use is a complex trade-off between its cost and its benefits. Work stealing <ref type="bibr" target="#b3">[4]</ref> can be considered the most widely used load balancing technique in task-based runtime systems. The main concept in work stealing is to distribute tasks in per-processor work queues, where each processor operates on its local queue. The processors can steal tasks from other queues to perform load balancing. Another approach to dynamic scheduling for manytask runtime systems is the work sharing strategy. Unlike work stealing, it schedules each task onto a processor when it is spawned and it is usually implemented by using a centralized task pool. In work sharing, whenever a worker spawns a new task, the scheduler migrates it to a new worker to improve load balancing. As such, migration of tasks happens more often in work sharing than in work stealing.</p><p>Combinations between static and dynamic scheduling are possible. Static scheduling can be adapted and used for dynamic scheduling by re-computing and resequencing the list of tasks. Indeed, heuristic policies based on list scheduling and performance models are employed in some many-task runtime systems <ref type="bibr" target="#b1">[2]</ref>. Additionally, hybrid policies, which integrate static and dynamic information, are possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance monitoring</head><p>The high concurrency and dynamic behaviour of upcoming Exascale systems poses a demand for performance observation and runtime introspection. This performance information is very valuable to dynamically steer many-task runtime systems in their execution and resource adaption, thereby improving application performance, resource utilization, or energy consumption.</p><p>When targeting performance observation, performance monitoring software is either generating data to be used online <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22]</ref> or offline <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14]</ref>. In other words, whether the collected data are going to be used, while the application still runs or after its execution. Furthermore, this taxonomy can be extended with respect to who is consuming data-either the end user (performance analysis) or the runtime itself (introspection and historical data). Real-time performance data (introspection and performance models from historical data) will play an important role in Exascale for runtime adaptation and optimal task scheduling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Fault tolerance</head><p>In order to detail what level of fault tolerance a runtime may have, we need to identify what types of faults we anticipate. For this topic, we extend a taxonomy <ref type="bibr" target="#b12">[13]</ref> from the HPC domain to include the concept of task faults. We retain detectability of faults as the main criterion, but distinguish three levels of the system: distributed execution, process, and task. Each of these levels may experience a fault, and each of them has a different scope. Only task faults and process faults can possibly be detected from within an application. Moreover, only for these types of faults some recovery mechanisms can be implemented at the task or process level inside a runtime system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Task faults</head><p>Tasks have the smallest scope of the three; still, a failure of a task may affect the result of a process and subsequently of a distributed run. A typical example is undetected errors in memory. The process which runs a task is generally capable of detecting task faults. There are several examples of shared memory runtimes which are capable of detecting and correcting task faults within parallel regions <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b25">26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Process faults</head><p>A process may also fail, which leads to the termination of all underlying tasks. For example, a node crash can lead to a process failure. In such a scenario, a process cannot detect its failure; however, in a distributed run, another process may detect the failure and trigger a recovery strategy across all processes. A recovery strategy in this case may rely on one of two redundancy techniques: checkpoint/restart or replication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">System faults</head><p>On the last level, a distributed system execution may fail in cases of severe faults like switch failure or power outage. While failure detection such as power outage detection can be placed at the system level, most of them cannot be detected directly by the runtime. Recovery mechanisms can take the form of a global checkpoint restart of the entire application. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Classification</head><p>Following the API taxonomy defined in Sect. 2, Table <ref type="table" target="#tab_1">1</ref> classifies existing task-parallel APIs. Note that for an API to qualify as supporting a given feature, this API must not require the user to resort to third-party libraries or implementation-specific details of the API. For instance, some APIs offer arbitrary task graphs via manual task reference counting <ref type="bibr" target="#b11">[12]</ref>. This does not qualify as support in our classification. Also note that all APIs marked as featuring task cancellation do so in a non-preemptive manner due to the absence of OS-level preemption capabilities. Some entries require additional clarification. In C++ STL, we consider the entity launched by std::async to represent a task. At the same time, HPX is an implementation of the C++ tasking API providing additional features such as distributed memory support and task dependencies. Also, while StarPU offers shared memory parallelism, it is capable of generating MPI communication from a given task graph and data distribution <ref type="bibr" target="#b1">[2]</ref>; hence, it is marked with explicit support for distributed memory using a message-based communication model. Furthermore, PaRSEC includes both a task-based runtime that works on user-specified task graph and data distribution information, as well as a compiler that accepts serial input and generates this data. As the latter is limited to loops, we only consider the runtime in this work.</p><p>Several observations can be made from the data presented in Table <ref type="table" target="#tab_1">1</ref>. First, all APIs targeting distributed memory also support heterogeneity in some form. APIs offering implicit distributed memory support generally employ a global address space and implement task partitioning. Second, among APIs lacking distributed memory, only OmpSs offers resilience (via its Nanos++ runtime), and distributed memory APIs only recently started to include resilience support <ref type="bibr" target="#b8">[9]</ref>-likely driven by the continuous increase in machine sizes and, hence, decreased mean time between failures. Finally, some form of heterogeneity support is provided in almost all modern APIs, though it often requires explicit heterogeneous task provisioning by the programmer. Table <ref type="table" target="#tab_2">2</ref> provides the corresponding classification with respect to the runtime system and its subcomponents taxonomy, introduced in Sect. 3. It is important to note that an API always translates into at least one runtime implementation, but is not always limited to one such implementation. The most diverse example is offered by the OpenMP API, which has many runtime implementations. We have grouped together most shared memory OpenMP implementations, due to them offering the same feature set in terms of our classification. However, Nanos++ is listed separately, since it goes beyond that, both in supported pragmas and in its support for distributed execution. Other runtimes with shared memory and distributed memory support (e.g. HPX) can also run as backends to OpenMP programs, but we do not detail these features in relation to OpenMP further. For the majority of cases, except for the OpenMP API and the Cilk API, there is a 1:1 mapping between API and runtime.</p><p>We refrain from using the scheduling targets in the runtime table and only include the scheduling methods. This decision is driven by the fact that energy has only recently become an important factor for scheduling, and energy-driven scheduling targets, or multi-objective scheduling targets, are only now starting to emerge. We list separately the scheduling methods for shared memory, and distributed memory, since these can be implemented in different ways. Indeed, most of the listed runtimes have similarities in scheduling within a single node; work stealing is the most common method of scheduling in this case. On the other hand, there is no established method for inter-node scheduling. For instance, ParSEC <ref type="bibr" target="#b5">[6]</ref> only provides a limited inter-node scheduling based on remote completion notifications, while Legion uses distributed work stealing. AllScale is also designed to support distributed work stealing, but as its current implementation is incomplete it is marked as supporting limited inter-node scheduling.</p><p>Again, as for the APIs, we note that there are various contributions extending runtime features, but if these contributions are not part of the main release, they are not considered in our taxonomy. For instance, recent work in X10 <ref type="bibr" target="#b20">[21]</ref> extends the X10 scheduler with distributed work stealing algorithms across nodes; however, we classify X10 as not (yet) having a distributed scheduler. The same applies to StarPU and OmpSs. New distributed memory scheduling policies are being developed for these runtimes, but they are not part of their main release yet. <ref type="foot" target="#foot_4">5</ref> Also, for Chapel, X10, and HPX, there is automatic data distribution support (runtime feature); however, these runtimes require explicit work mapping in distributed memory environments (API feature).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>The shift in HPC towards task-based parallel programming paradigms has led to a broad ecosystem of different task-based technologies. With such diversity, and some degree of isolation between individual communities of developers, there is a lack of documentation and common classification, thus hindering researchers who wish to obtain a comprehensive view of the field. In this paper, we provide an initial attempt at establishing a common taxonomy and providing the corresponding categorization for many existing task-based programming environments suitable for HPC.</p><p>We divided our taxonomy into two broad categories: APIs, which define how the programmer interacts with the system, and many-task runtime systems, covering the underlying technologies and implementation mechanisms. For the former, we identify four broad categories of features exposed to the programmer: architectural characteristics; those related to the task generation and handling; system-level management; and finally engineering aspects. For the latter, we analyse the types of scheduling policies and goals supported; online and offline performance monitoring integration; as well as the level of resilience and detection provided for task, process, and system faults.</p><p>We believe that this paper provides a useful basis to describe task-based parallel programming technologies and to select, examine, and compare APIs and runtime systems with respect to their capabilities. This serves as a foundation to both classify additional APIs and runtime systems using our definitions as well as to allow for a better overview and comparative basis for newly implemented features within the ever-expanding, diverse field of task-based parallel programming environments.</p><p>Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2</head><label>2</label><figDesc>Fig. 2 Taxonomy of many-task runtime systems</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Feature comparison of APIs for task parallelism</figDesc><table><row><cell></cell><cell cols="5">Architectural</cell><cell></cell><cell cols="2">Task System</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Management</cell><cell></cell><cell></cell><cell cols="2">Eng.</cell></row><row><cell></cell><cell>Communication</cell><cell>Model</cell><cell>Distributed</cell><cell>Memory</cell><cell>Heterogeneity</cell><cell>Graph Structure</cell><cell>Task Partitioning</cell><cell>Result Handling</cell><cell>Task Cancellation</cell><cell>Worker</cell><cell>Management</cell><cell>Resilience</cell><cell>Management</cell><cell>Work Mapping</cell><cell>Synchronization</cell><cell>Technological</cell><cell>Readiness</cell><cell>Implementation</cell><cell>Type</cell></row><row><cell cols="3">C++ STL smem</cell><cell cols="2">?</cell><cell>?</cell><cell>dag</cell><cell cols="3">? i/e ?</cell><cell>i</cell><cell></cell><cell cols="2">?</cell><cell>i/e</cell><cell>e</cell><cell>9</cell><cell></cell><cell></cell></row><row><cell>TBB HPX Legion</cell><cell cols="2">smem gas gas</cell><cell cols="2">? i i</cell><cell>? e e</cell><cell cols="2">tree ? dag tree</cell><cell>i e e</cell><cell>?</cell><cell cols="2">i i/e i</cell><cell cols="2">? ? ?</cell><cell>i i/e i/e</cell><cell>i e e</cell><cell>8 6 4</cell><cell></cell><cell cols="2">Library</cell></row><row><cell>PaRSEC</cell><cell cols="2">msg</cell><cell>e</cell><cell></cell><cell>e</cell><cell>dag</cell><cell>?</cell><cell>e</cell><cell></cell><cell>i</cell><cell></cell><cell></cell><cell></cell><cell>i/e</cell><cell>i</cell><cell>4</cell><cell></cell><cell></cell></row><row><cell>OpenMP Charm++ OmpSs AllScale StarPU</cell><cell cols="2">smem gas smem gas msg</cell><cell cols="2">? i ? i e</cell><cell>i e i i e</cell><cell>dag dag dag dag dag</cell><cell>? ?</cell><cell cols="2">i i/e ? i ? i/e ? i ?</cell><cell>e i i i i</cell><cell></cell><cell cols="2">? ?</cell><cell>i i/e i i i/e</cell><cell>i/e e i/e i/e e</cell><cell>9 6 5 3 5</cell><cell></cell><cell cols="2">Extension</cell></row><row><cell>Cilk Plus Chapel X10</cell><cell cols="2">smem gas gas</cell><cell cols="2">? i i</cell><cell>? i i</cell><cell cols="2">tree ? dag dag</cell><cell>i i i</cell><cell>? ? ?</cell><cell>i i i</cell><cell></cell><cell cols="2">? ?</cell><cell>i i/e i/e</cell><cell>e e e</cell><cell>8 5 5</cell><cell></cell><cell cols="2">Lang.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Feature comparison of runtimes for task parallelism</figDesc><table><row><cell></cell><cell>Target</cell><cell>Architecture</cell><cell>Data</cell><cell>Distribution</cell><cell>Scheduling</cell><cell>Methods (sm)</cell><cell>Scheduling</cell><cell>Methods (d)</cell><cell>Performance</cell><cell>Monitoring</cell><cell>Fault Tolerance</cell></row><row><cell>OpenMP runtimes(*)</cell><cell cols="2">sm</cell><cell cols="2">?</cell><cell cols="2">m</cell><cell cols="2">?</cell><cell cols="3">off/on ?</cell></row><row><cell>Intel TBB</cell><cell cols="2">sm</cell><cell cols="2">?</cell><cell cols="2">ws</cell><cell cols="2">?</cell><cell cols="2">off</cell><cell>?</cell><cell>i implicit</cell></row><row><cell>Intel Cilk Plus</cell><cell cols="2">sm</cell><cell cols="2">?</cell><cell cols="2">ws</cell><cell cols="2">?</cell><cell cols="2">off</cell><cell>?</cell><cell>e explicit</cell></row><row><cell>StarPU</cell><cell cols="2">sm</cell><cell>e</cell><cell></cell><cell cols="2">m</cell><cell cols="2">?</cell><cell cols="3">off/on ?</cell><cell>m multiple (incl. ws)</cell></row><row><cell>Nanos++</cell><cell>d</cell><cell></cell><cell>i</cell><cell></cell><cell cols="2">m</cell><cell cols="2">?</cell><cell cols="3">off/on tf</cell><cell>l limited</cell></row><row><cell>Charm++</cell><cell>d</cell><cell></cell><cell>i</cell><cell></cell><cell cols="2">m</cell><cell cols="2">m</cell><cell cols="3">off/on pf</cell><cell>ws work stealing</cell></row><row><cell>X10</cell><cell>d</cell><cell></cell><cell>i</cell><cell></cell><cell cols="2">ws</cell><cell cols="2">?</cell><cell cols="2">off</cell><cell>pf</cell><cell>tf task faults</cell></row><row><cell>Chapel</cell><cell>d</cell><cell></cell><cell>i</cell><cell></cell><cell cols="2">ws</cell><cell cols="2">?</cell><cell cols="2">off</cell><cell>pf</cell><cell>pf process faults</cell></row><row><cell>HPX</cell><cell>d</cell><cell></cell><cell>i</cell><cell></cell><cell cols="2">m</cell><cell cols="2">?</cell><cell cols="3">off/on ?</cell><cell>sm shared memory</cell></row><row><cell>AllScale</cell><cell>d</cell><cell></cell><cell>i</cell><cell></cell><cell cols="2">ws</cell><cell>l</cell><cell></cell><cell cols="3">off/on pf</cell><cell>d distributed memory</cell></row><row><cell>ParSEC Legion</cell><cell>d d</cell><cell></cell><cell>e i</cell><cell></cell><cell cols="2">m ws</cell><cell cols="2">l ws</cell><cell cols="3">off off/on pf tf</cell><cell>on online use off offline use</cell></row><row><cell cols="10">(*) Such as Intel OpenMP, GOMP, Qthreads, and Argobots</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Note that we use the term "language" for Cilk and Cilk Plus, even though they build on C/C++. The reasoning is that a Cilk Plus compiler is strictly required for compilation (unlike, for example, OpenMP).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://ec.europa.eu/research/participants/data/ref/h2020/wp/2014_2015/annexes/h2020-wp1415annex-g-trl_en.pdf. Accessed: 2017-12-16.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>http://starpu.gforge.inria.fr/doc/html/Scheduling.html#Energy-basedScheduling.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>The AllScale EC-funded FET-HPC project: allscale.eu.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>We received feedback from their developers.</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>This work was supported by the <rs type="funder">AllScale EC-funded FET-HPC</rs> <rs type="projectName">H2020</rs> Project (No. <rs type="grantNumber">671603</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_EKFf39R">
					<idno type="grant-number">671603</idno>
					<orgName type="project" subtype="full">H2020</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic calibration of performance models on heterogeneous multicore architectures</title>
		<author>
			<persName><forename type="first">C</forename><surname>Augonnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of</title>
		<meeting>null</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="56" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">StarPU: a unified platform for task scheduling on heterogeneous multicore architectures</title>
		<author>
			<persName><forename type="first">C</forename><surname>Augonnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concur Comput Pract Exp</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="187" to="198" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Legion: programming distributed heterogeneous architectures with logical regions</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Bauer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scheduling multithreaded computations by work stealing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Blumofe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leiserson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="720" to="748" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cilk: an efficient multithreaded runtime system</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Blumofe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Parallel Distrib Comput</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="69" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Parsec: exploiting heterogeneity to enhance scalability</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bosilca</surname></persName>
		</author>
		<idno type="DOI">10.1109/MCSE.2013.98</idno>
		<ptr target="https://doi.org/10.1109/MCSE.2013.98" />
	</analytic>
	<monogr>
		<title level="j">Comput Sci Eng</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="36" to="45" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Parallel programmability and the chapel language</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Chamberlain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J HPC Appl</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="291" to="312" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">X10: an object-oriented approach to non-uniform cluster computing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Charles</surname></persName>
		</author>
		<idno type="DOI">10.1145/1094811.1094852</idno>
		<ptr target="https://doi.org/10.1145/1094811.1094852" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of OOPSLA05</title>
		<meeting>OOPSLA05</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="519" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Resilient x10: efficient failure-aware programming</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cunningham</surname></persName>
		</author>
		<idno type="DOI">10.1145/2555243.2555248</idno>
		<ptr target="https://doi.org/10.1145/2555243.2555248" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of PPoPP14</title>
		<meeting>PPoPP14</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="67" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Openmp: an industry standard api for shared-memory programming</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dagum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Menon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Sci Eng</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="46" to="55" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ompss: a proposal for programming heterogeneous multi-core architectures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Duran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Process Lett</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">02</biblScope>
			<biblScope unit="page" from="173" to="193" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<ptr target="https://software.intel.com/en-us/node/506110" />
		<title level="m">General Acyclic Graphs of Tasks in TBB</title>
		<imprint>
			<date type="published" when="2017-11">Nov 2017</date>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fault-tolerant iterative methods via selective reliability</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hoemmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heroux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SC11</title>
		<meeting>SC11</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An early prototype of an autonomic performance environment for exascale</title>
		<author>
			<persName><forename type="first">K</forename><surname>Huck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of ROSS</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<date type="published" when="2013">2013</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An autonomic performance environment for exascale</title>
		<author>
			<persName><forename type="first">K</forename><surname>Huck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Supercomput Front Innov</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="49" to="66" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Opportunistic application-level fault detection through adaptive redundant multithreading</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hukerikar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HPCS14</title>
		<meeting>HPCS14</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="243" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hpx: a task based programming model in a global address space</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of PGAS14</title>
		<meeting>PGAS14</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Charm++: a portable concurrent object oriented system based on C++</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krishnan</surname></persName>
		</author>
		<idno type="DOI">10.1145/165854.165874</idno>
		<ptr target="https://doi.org/10.1145/165854.165874" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of OOSPLA93</title>
		<meeting>OOSPLA93</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="91" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Survey on parallel programming model</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kasim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NPC08</title>
		<meeting>NPC08</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="266" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Implementation of an energy-aware OmpSs task scheduling policy</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Meyer</surname></persName>
		</author>
		<ptr target="http://www.prace-ri.eu/IMG/pdf/wp88.pdf" />
		<imprint>
			<date type="published" when="2017-05-02">2 May 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the merits of distributed work-stealing on selective locality-aware tasks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Paudel</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICPP.2013.19</idno>
		<ptr target="https://doi.org/10.1109/ICPP.2013.19" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICPP13</title>
		<meeting>ICPP13</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="100" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Self-adaptive OMPSS tasks in heterogeneous environments</title>
		<author>
			<persName><forename type="first">J</forename><surname>Planas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IPDPS13</title>
		<meeting>IPDPS13</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="138" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Composable parallel patterns with intel cilk plus</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Robison</surname></persName>
		</author>
		<idno type="DOI">10.1109/MCSE.2013.21</idno>
		<ptr target="https://doi.org/10.1109/MCSE.2013.21" />
	</analytic>
	<monogr>
		<title level="j">Comput Sci Eng</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="66" to="71" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Argobots: a lightweight low-level threading and tasking framework</title>
		<author>
			<persName><forename type="first">S</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Parallel Distrib Syst</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The GASPI API: a failure tolerant PGAS API for asynchronous dataflow on heterogeneous architectures</title>
		<author>
			<persName><forename type="first">C</forename><surname>Simmendinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sustained Simulation Performance</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="17" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Nanocheckpoints: a task-based asynchronous dataflow framework for efficient and scalable checkpoint/restart</title>
		<author>
			<persName><forename type="first">O</forename><surname>Subasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of PDP15</title>
		<meeting>PDP15</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="99" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Qthreads: an API for programming with millions of lightweight threads</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Wheeler</surname></persName>
		</author>
		<idno type="DOI">10.1109/IPDPS.2008.4536359</idno>
		<ptr target="https://doi.org/10.1109/IPDPS.2008.4536359" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Parallel and Distributed Processing</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Putting intel threading building blocks to work</title>
		<author>
			<persName><forename type="first">T</forename><surname>Willhalm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Popovici</surname></persName>
		</author>
		<idno type="DOI">10.1145/1370082.1370085</idno>
		<ptr target="https://doi.org/10.1145/1370082.1370085" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of IWMSE08</title>
		<meeting>IWMSE08</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="3" to="4" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
