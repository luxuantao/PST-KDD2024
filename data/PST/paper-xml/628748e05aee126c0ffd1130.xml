<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Syntax-guided Contrastive Learning for Pre-trained Language Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
							<email>zhangshuai28@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lijie</forename><surname>Wang</surname></persName>
							<email>wanglijie@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
							<email>xiaoxinyan@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
							<email>wu_hua@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Syntax-guided Contrastive Learning for Pre-trained Language Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Syntactic information has been proved to be useful for transformer-based pre-trained language models. Previous studies often rely on additional syntax-guided attention components to enhance the transformer, which require more parameters and additional syntactic parsing in downstream tasks. This increase in complexity severely limits the application of syntaxenhanced language model in a wide range of scenarios. In order to inject syntactic knowledge effectively and efficiently into pre-trained language models, we propose a novel syntaxguided contrastive learning method which does not change the transformer architecture. Based on constituency and dependency structures of syntax trees, we design phrase-guided and treeguided contrastive objectives, and optimize them in the pre-training stage, so as to help the pre-trained language model to capture rich syntactic knowledge in its representations. Experimental results show that our contrastive method achieves consistent improvements in a variety of tasks, including grammatical error detection, entity tasks, structural probing and GLUE. Detailed analysis further verifies that the improvements come from the utilization of syntactic information, and the learned attention weights are more explainable in terms of linguistics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pre-trained transformer-based neural language models (LMs), such as BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> and RoBERTa <ref type="bibr" target="#b19">(Liu et al., 2019)</ref>, have achieved remarkable results in a variety of NLP tasks <ref type="bibr" target="#b31">(Wang et al., 2018)</ref>. However, many studies have found that these LMs do not encode enough syntactic knowledge in their learned representations <ref type="bibr" target="#b34">(Wang et al., 2019;</ref><ref type="bibr" target="#b22">Min et al., 2020;</ref><ref type="bibr" target="#b32">Wang et al., 2020)</ref>. As it is widely acknowledged that structural information is very important for NLP <ref type="bibr" target="#b28">(Strubell et al., 2018;</ref><ref type="bibr" target="#b23">Nguyen et al., 2019;</ref><ref type="bibr" target="#b44">Zhang et al., 2020)</ref>, there is an increasing interest in improving pre-trained LMs by using syntactic information.</p><p>Most of these works enhance pre-trained LMs by adding syntax-driven attention components to the transformer <ref type="bibr" target="#b17">(Li et al., 2020b;</ref><ref type="bibr" target="#b39">Xu et al., 2020;</ref><ref type="bibr" target="#b0">Bai et al., 2021)</ref>. They use the added components to produce a syntax-aware representation, and inject this additional representation into the original one from the vanilla transformer, so as to get a final syntax-enhanced representation. Although these works did bring improvements, the additional syntax-aware layers obviously increase application inconvenience and computation complexity, as they need to parse the input text during testing and require more neural parameters. Moreover, the performance of such explicit method depends on the parsing quality of test data <ref type="bibr" target="#b26">(Sachan et al., 2020)</ref>. There are also some efforts on incorporating syntaxrelated objectives into the pre-training stage, such as syntax head prediction <ref type="bibr" target="#b32">(Wang et al., 2020)</ref> and dependency distance prediction <ref type="bibr" target="#b39">(Xu et al., 2020)</ref>. However, these predictive pre-training tasks often fail to improve performance alone and need to work together with the additional attention components <ref type="bibr" target="#b39">(Xu et al., 2020)</ref>. Overall, it is still an open challenge to effectively and efficiently incorporate syntactic information into pre-trained LMs.</p><p>In order to address the above problems, we propose Syntax-guided Contrastive Language Model (SynCLM). Based on contrastive learning, Syn-CLM uses syntactic information to create contrastive positive and negative examples, and uses them to help the pre-trained LM to learn rich syntactic knowledge through contrastive learning method. SynCLM only adds contrastive objectives in the pre-training stage, ensuring an effective and efficient utilization of syntax.</p><p>Specifically, based on constituent and dependency structures of syntax trees, we propose phraseguided and tree-guided contrastive objectives for pre-training, as shown in Figure <ref type="figure" target="#fig_0">1</ref>. The constituent structure represents the grouping of words into phrases within an input according to constituency grammar <ref type="bibr" target="#b7">(Ford and Fox, 2002)</ref>. Inspired by recent studies <ref type="bibr" target="#b20">(Mareček and Rosa, 2019;</ref><ref type="bibr" target="#b13">Kim et al., 2020)</ref> which prove that LM's attention heads exhibit syntactic structure akin to constituency grammar, in order to better recognize phrases from attentions, we propose the phrase-guided contrastive objective to enhance attention learning by maximizing the similarity of attention distributions between words in the same phrase. The dependency structure further encodes the binary head-dependent relations between words, and the root node aggregates the semantic information of the whole structure from all its descendant words. To make the root node attend to its descendant nodes, we propose the treeguided contrastive objective to enhance word representations by maximizing the similarity between the representation obtained from all tokens and that obtained from syntactically related tokens. The two contrastive objectives are jointly optimized during pre-training, so as to inject syntactic knowledge into pre-trained LMs. In summary, our contributions are as following:</p><p>• We are the first to leverage the contrastive learning method to incorporate syntactic information into the pre-training stage. Our models can be directly applied to downstream tasks without introducing additional parameters and syntax parsing of inputs. In addition, our method is applicable to any arbitrary transformer-based pre-trained LM.</p><p>Our code<ref type="foot" target="#foot_0">1</ref> will be released.</p><p>• Based on the constituency and dependency structure, we design two novel syntax-guided learning objectives to enhance the learning of attention weight distributions and hidden representations in the transformer.</p><p>• Extensive experiments show that our SynCLM achieves consistent improvements on tasks that are often used in related works, including grammatical error detection, entity-related tasks, structural probing task, and general evaluation tasks (GLUE). Detailed analysis verifies that the performance improvements come from the use of syntactic information, and the learned attention weights are more explainable in terms of linguistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We first review studies on analyzing the linguistic knowledge learned by pre-trained LMs, and then we will introduce recent researches on incorporating linguistic knowledge into pre-trained LMs.</p><p>Linguistic Studies on Pre-trained LMs As pretrained LMs <ref type="bibr" target="#b5">(Devlin et al., 2019;</ref><ref type="bibr" target="#b19">Liu et al., 2019)</ref> continue to provide gains on NLP benchmarks, understanding what they have learned is very important, which can help us understand the reason behind their success and their limitations. Many studies aim to unveil linguistic structures from the representations learned by pre-trained LMs <ref type="bibr" target="#b11">(Jawahar et al., 2019;</ref><ref type="bibr" target="#b34">Wang et al., 2019)</ref>. Some works demonstrate that pre-trained LMs have learned syntactic information. <ref type="bibr" target="#b10">Hewitt and Manning (2019)</ref> indicate that syntax information is implicitly embedded in BERT by learning a linear transformation to predict the syntactic depth of each word based on its representation. <ref type="bibr" target="#b11">Jawahar et al. (2019)</ref> and <ref type="bibr" target="#b29">Tenney et al. (2019)</ref> show that BERT captures syntactic features at lower layers and loses some of learned syntactic information at higher layers. However, some works show that pre-trained LMs do not capture adequate syntactic knowledge. <ref type="bibr" target="#b34">Wang et al. (2019)</ref> find that certain syntactic structures may not be embedded in BERT, as the dependency weights calculated by BERT seem to be inconsistent with human intuitions of hierarchical structures. <ref type="bibr" target="#b22">Min et al. (2020)</ref> prove that BERT need to recruit syntactic representations from the generated syntactically informative examples to improve model performance on syntax-aware examples.</p><p>Based on these studies, we can find that pretrained LMs often fail to encode enough syntactic information in their representations and get poor performance on syntax-aware data.</p><p>Syntax Enhanced Pre-trained LMs On the other hand, many works try to use syntax information to further improve models <ref type="bibr" target="#b28">(Strubell et al., 2018;</ref><ref type="bibr" target="#b23">Nguyen et al., 2019;</ref><ref type="bibr" target="#b44">Zhang et al., 2020;</ref><ref type="bibr" target="#b17">Li et al., 2020b;</ref><ref type="bibr" target="#b39">Xu et al., 2020)</ref>.</p><p>Task oriented works attempt to inject syntactic knowledge into the transformer <ref type="bibr" target="#b28">(Strubell et al., 2018;</ref><ref type="bibr" target="#b23">Nguyen et al., 2019;</ref><ref type="bibr" target="#b2">Bugliarello and Okazaki, 2020;</ref><ref type="bibr" target="#b44">Zhang et al., 2020)</ref>. In the semantic role labeling task, <ref type="bibr" target="#b28">Strubell et al. (2018)</ref> restrict each token to attend to its syntactic parent in an attention head and improve the model performance. In the machine translation task, <ref type="bibr" target="#b23">Nguyen et al. (2019)</ref> incorporate a tree-structured attention into the transformer for helping encode syntactic information. <ref type="bibr" target="#b2">Bugliarello and Okazaki (2020)</ref> propose a syntaxaware self-attention mechanism to incorporate syntactic knowledge into the model. In the machine reading comprehension task, <ref type="bibr" target="#b44">Zhang et al. (2020)</ref> use syntactic information to guide the self-attention to pay no attention to the dispensable words. These works mainly inject syntactic information into attention mechanisms, and obtain performance gains. However, they confine to a certain task.</p><p>Pre-training oriented works try to integrate syntactic information in a general way that can be applied to various NLP tasks. Inspired by the above researches, some studies <ref type="bibr" target="#b39">(Xu et al., 2020;</ref><ref type="bibr" target="#b17">Li et al., 2020b;</ref><ref type="bibr" target="#b0">Bai et al., 2021)</ref> design various syntax-aware attention mechanisms. Despite different in detail, all of them use syntactic dependency relations to restrict the attention to important local regions. The syntax-aware attention can capture the information of important local regions according to syntactic structures, so as to obtain more benefits. Meanwhile, some works inject syntactic knowledge into pre-trained LMs via introducing new learning objectives, such as syntax head prediction <ref type="bibr" target="#b32">(Wang et al., 2020)</ref> and dependency distance prediction <ref type="bibr" target="#b39">(Xu et al., 2020)</ref>. However, they need to work with additional syntax-guided attention methods <ref type="bibr" target="#b39">(Xu et al., 2020)</ref>.</p><p>Notably, most of these works incorporate an explicit syntax-guided component into models during testing. This increases the computational complexity and application difficulty of the model, which may limit the application of model in broader NLP tasks. In order to address these problems, we propose a novel contrastive pre-training framework to incorporate syntactic knowledge into pre-trained LMs, without introducing computational complexity in downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we first describe the two new contrastive learning objectives in our SynCLM. Then we introduce our pre-training framework and implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Syntax-guide Contrastive Learning</head><p>In order to facilitate the learning of syntax-aware representations, we propose two learning tasks which use syntactic structures to guide the learning of attention distributions and hidden representations in the transformer. Here, we will first introduce the transformer architecture and the contrastive learning method as background. Then we will introduce our two contrastive learning objectives, and the construction of positive and negative samples, which is the main challenge of contrastive learning.</p><p>Transformer A Transformer <ref type="bibr" target="#b30">(Vaswani et al., 2017)</ref> is a stack of self-attention layers where each layer (consisting of H heads) transforms the input unit into a continuous representation. Given the input sentence S with n tokens, denoted as {t 1 , t 2 , ..., t n }, we use a (l,h) i to represent the attention distribution of the i-th token by the h-th attention head on the l-th layer, where 1 ≤ h ≤ H and 1 ≤ l ≤ L. We take the average of all heads' attention distributions on the l-th layer as the final distribution of the l-th layer, denoted as a (l, h) i . Finally, we use z l i to represent the intermediate hidden representation of token i on the l-th layer.</p><p>Contrastive Learning Method Contrastive selfsupervised learning (CSSL) <ref type="bibr" target="#b38">(Wu et al., 2018;</ref><ref type="bibr" target="#b9">He et al., 2020)</ref> is a learning paradigm which aims to capture the intrinsic patterns and properties of input data without using human-provided labels. The basic idea of CSSL is to construct auxiliary tasks solely based on the input data, which is the key to CSSL, and force the network to learn meaningful representations by performing the auxiliary tasks well. The auxiliary tasks are learned by the contrastive learning loss. In this paper, we use InfoNCE function which is a variant of Noise Contrastive Estimation (NCE) <ref type="bibr" target="#b8">(Gutmann and Hyvärinen, 2010)</ref> function for contrastive learning, as shown in Equation <ref type="formula">1</ref>.</p><formula xml:id="formula_0">L cl = −log exp( sim(q,q + ) τ ) exp( sim(q,q + ) τ ) + K i=0 exp( sim(q,q − i ) τ ) (1)</formula><p>where q is the original sample; q + and q − i are the positive and the i-th negative samples, respectively; K is the number of negative samples. The sim() function can be any similarity function, such as cosine, Jensen-Shannon Divergence <ref type="bibr" target="#b6">(Endres and Schindelin, 2003)</ref> and Hellinger distance <ref type="bibr" target="#b1">(Beran, 1977)</ref>. τ called temperature coefficient is a hyperparameter used in recent methods <ref type="bibr" target="#b12">(Khosla et al., 2020;</ref><ref type="bibr" target="#b43">Yu et al., 2021)</ref>.</p><p>Phrase-guided Contrastive Learning Objective Some phrases can be recognized by using the similarity of attention distributions over words <ref type="bibr" target="#b20">(Mareček and Rosa, 2019;</ref><ref type="bibr" target="#b13">Kim et al., 2020)</ref>. To further improve the recognition, we propose to use prior phrase structure information to further guide the learning of attention distributions by maximizing the similarity of attention distributions between words in the same phrase.</p><p>Given a sampled token t i , we randomly select a token in the same phrase<ref type="foot" target="#foot_1">2</ref> as its positive example, and select K tokens outside the phrase as the contrastive negative examples. As shown in the sampled phrases of Figure <ref type="figure" target="#fig_0">1</ref>, for the token "build", the token marked as P is the positive example, and tokens marked as N are negative examples. Then we use the contrastive learning loss (defined in Equation 1) for this learning task, and the corresponding sim() function is defined as follows:</p><formula xml:id="formula_1">simphrase = −JSD(a (l, h) i ∥ a (l, h) s ) = −(DKL(a (l, h) i ∥ m) + DKL(a (l, h) s ∥ m))/2</formula><p>where m = (a</p><formula xml:id="formula_2">(l, h) i + a (l, h) s )/2 (2)</formula><p>where JSD is short for Jensen-Shannon Divergence <ref type="bibr" target="#b6">(Endres and Schindelin, 2003)</ref>, and D KL for Kullback-Leibler Divergence (KLD) <ref type="bibr" target="#b15">(Kullback and Leibler, 1951</ref>). The index s indicates a sampled example of token t i , which may be positive or negative. Please note that there are many calculation choices for the sim() function, such as cosine, JSD and KLD. In our early-stage preliminary experiments, we have experimented with JSD and KLD, and the former performs slightly better and thus is adopted in our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tree-guided Contrastive Learning Objective</head><p>The idea that the root of a syntax tree should pay more attention to its descendant nodes has been proved to be effective in attention-based models by existing syntax-aware attention mechanisms <ref type="bibr" target="#b39">(Xu et al., 2020;</ref><ref type="bibr" target="#b17">Li et al., 2020b;</ref><ref type="bibr" target="#b0">Bai et al., 2021)</ref>. Therefore, we propose a tree-guided contrastive learning objective to maximize the similarity between the global representation based on all input tokens and the syntax-aware representation based on syntactically related tokens. Given a sampled token t i , we derive its subtree from the entire dependency tree. As described by the sampled subtrees in Figure <ref type="figure" target="#fig_0">1</ref>, the subtree of token "build" consists of all tokens dominated by token "build", and "build" is the root of the subtree. We use it as the positive tree, denoted as T + . Then we randomly replace no more than three tokens in T + with adjacent tokens to get the negative tree T − , and ensure that there is at least one same token in T + and T − , as shown by the other two subtrees in Figure <ref type="figure" target="#fig_0">1</ref>. According to the above conclusion, the representation based on the tokens in the positive subtree should be closer to the original representation given by the pre-trained LM. We also use the contrastive learning loss in Equation 1 to optimize this learning objective, and the sim() function is defined as follows:</p><formula xml:id="formula_3">simtree = cosine(z l i , t j ∈Ts eijz l j )</formula><p>where eij = exp(z l i • z l j )</p><formula xml:id="formula_4">t k ∈Ts exp(z l i • z l k )<label>(3)</label></formula><p>where T s represents a sampled subtree of token t i , which may be positive or negative. And z l i represents the intermediate hidden representation of token i on the l-th layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Syntax-guided Pre-training Framework</head><p>We then add the two contrastive learning objectives into traditional pre-training, so as to enhance vanilla pre-trained LM. The final loss for the pretraining is the summation of the training loss for masked language model (MLM) <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> and two new proposed tasks, as shown below.</p><formula xml:id="formula_5">L = L MLM + L phrase + L tree</formula><p>Data for Pre-training We use BERT's pretraining data <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> as our model's pre-training data, including documents from English Wikipedia and BookCorpus <ref type="bibr" target="#b45">(Zhu et al., 2015)</ref>. Then we use the pre-processing and BPE (Byte-Pair Encoding) tokenization from RoBERTa <ref type="bibr" target="#b19">(Liu et al., 2019)</ref> to process the training data. The maximum length of input sequence is set to 512.</p><p>To obtain syntactic structures for each sentence, we adopt a well-trained parsing model -Stanza<ref type="foot" target="#foot_2">3</ref> to automatically generate a syntax tree for each sentence. Because the pre-trained LM takes subwords as the input unit, for the word u, we take its first subword as the root, and add edges connecting non-first subwords to the first subword. Since syntactic information is pre-processed in advance, syntax parsing only needs to be performed once in the entire process. In our work, it takes about one day to parse the pre-training data with 20 P40 GPUs. Then, syntactic information is used as the additional input in the pre-training stage.</p><p>Implementation Details To accelerate the training process, we initialize parameters from RoBERTa models<ref type="foot" target="#foot_3">4</ref> released by <ref type="bibr" target="#b19">Liu et al. (2019)</ref>. We use RoBERTa-base and RoBERTa-large to initialize our base and large models respectively. RoBERTa-base contains 12 layers, each of which has 12 heads and 768 hidden states. And RoBERTalarge contains 24 layers, each of which has 16 heads and 1024 hidden states. We set l as the last hidden layer in Equation 2 and Equation <ref type="formula" target="#formula_4">3</ref>. And the number of negative examples is set to 3. As our pre-trained LMs do not introduce additional parameters, the parameter sizes of our base and large models are the same as those of RoBERTa models.</p><p>We pre-train our models with 16 32G NVIDIA V100 GPUs. The base model takes about four days and the large model takes about seven days. During the training process, in order to choose a well pretrained model, we evaluate the intermediate model per 10K steps, and terminate the training when the performance alteration (i.e., Perplexity of LMs) is below a certain threshold for five sequential evaluations. In the base setting, the batch size is 512, and the total steps are 300,000, 24,000 of which is the warm up steps. For the large model, the batch size is 256, and the total steps are 350,000, 30,000 of which is for warming up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>First, we verify the effectiveness of SynCLM on several syntax-aware tasks, including grammatical error detection task (Section 4.1) and entity tasks (Section 4.2), which are often used for testing syntax pre-training models. Then, we test the effectiveness of SynCLM on more general tasks by using GLUE benchmark (Section 4.3). At last, detailed analysis is conducted to show the impact of incorporating syntactic knowledge (Section 4.4).</p><p>Please note that ↑ in our reported results means statistically significant improvement over the baseline with p-value &lt; 0.05.  <ref type="bibr" target="#b17">Li et al. (2020b)</ref>. Reported results of CoLA are a median over 5 runs, and those of FCE are the average over 5 runs. For BLiMP, we report accuracies for "one prefix" (1P) <ref type="bibr" target="#b18">(Linzen et al., 2016)</ref> and "two prefix" (2P) <ref type="bibr" target="#b37">(Wilcox et al., 2019)</ref>.</p><p>(continuous) of RoBERTa<ref type="foot" target="#foot_4">5</ref> . The statistics of datasets adopted in this paper are summarized in Table <ref type="table">1</ref>. For datasets of GLUE, we use metrics reported in <ref type="bibr" target="#b5">Devlin et al. (2019)</ref>. For other datasets, we use popular metrics provided by dataset authors and other researchers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Grammatical Error Detection (GED)</head><p>GED task aims to evaluate the grammatical acceptability of a given sentence. We use three popular public datasets, i.e., CoLA <ref type="bibr" target="#b36">(Warstadt et al., 2019)</ref>, BLiMP <ref type="bibr" target="#b35">(Warstadt et al., 2020)</ref>, and FCE <ref type="bibr" target="#b42">(Yannakoudakis et al., 2011)</ref>, to evaluate our models. For CoLA, we use Matthews Correlation Coefficient (MCC) <ref type="bibr" target="#b21">(Matthews, 1975)</ref> as the evaluation metric. For BLiMP, we evaluate models using the overall accuracy on all input pairs, namely the proportion of pairs whose acceptable sentence is assigned a higher probability. On FCE, following <ref type="bibr" target="#b25">Rei and Søgaard (2019)</ref>, we take it as a binary classification task and a sequence labeling task, and use accuracy and F 0.5 to evaluate them respectively.</p><p>Baselines Rei and Søgaard (2019) combine objectives at different granularities (i.e., sentence and token) to learn better representations. <ref type="bibr" target="#b17">Li et al. (2020b)</ref> use dependency distance matrix to obtain a syntax-aware local attention (SLA) and achieve SOTA results on FCE. We also report the results of BERT, RoBERTa and GPT-2 <ref type="bibr">(Radford et al.)</ref>, where GPT-2 reports SOTA results on BLiMP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Results</head><p>From Table <ref type="table" target="#tab_1">2</ref>, it can be seen that SynCLM achieves consistent gains over RoBERTa on all three datasets: 2.0% higher average accuracy on BLiMP, 1.3% higher MCC on CoLA, and 0.8% higher accuracy on FCE. The results show that syntactic prior information helps SynCLM to perform much better on GED task. We believe this is because the grammatical acceptability of a sentence strongly rely on its syntactic structure. As illustrated by the first example in Figure <ref type="figure" target="#fig_1">2</ref>, which checks the morphological number agreement of the sentence, the morphological number of the word "eat" should be consistent with that of its subject "John". And the dependency syntax illustrates the subject-verb relation between them.</p><p>The tree-guided method performs better than the phrase-guided method on most metrics, as the tree structure gives the head-dependent relations between words more directly and more explicitly. Moreover, combining the two methods can achieve more gains.</p><p>Merging SLA We also test whether SynCLM can be further improved with previous syntaxenhanced attention mechanisms for fine-tuning. We implement SLA <ref type="bibr" target="#b17">(Li et al., 2020b)</ref> in the fine-tuning stage, and show the results in the third part of Table <ref type="table" target="#tab_1">2</ref>. It can be seen that merging SLA and SynCLM can achieve more gains on CoLA and FCE, which means that SynCLM can be further improved by using syntax information during fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Entity Tasks</head><p>We evaluate SynCLM on two entity related tasks: named entity recognition (NER) and entity typing (ENT), which aim to recognize entities and predict entity types respectively. We use <ref type="bibr">CoNLL-2003 (Sang and</ref><ref type="bibr" target="#b27">De Meulder, 2003)</ref> for NER task and OpenEntity <ref type="bibr" target="#b3">(Choi et al., 2018)</ref> for ENT task.</p><p>Baselines BERT-MRC <ref type="bibr" target="#b16">(Li et al., 2020a)</ref> formulates NER task as a machine reading comprehension task to handle both flat and nested NER tasks. KEPLER <ref type="bibr" target="#b33">(Wang et al., 2021)</ref> infuses knowledge into pre-trained models and jointly learns knowledge embeddings and language representations. SEPREM <ref type="bibr" target="#b39">(Xu et al., 2020)</ref> injects syntax information into pre-trained LMs by introducing two learning tasks and a syntax-aware attention layer. LUKE <ref type="bibr" target="#b40">(Yamada et al., 2020)</ref> uses a large amount of entity-annotated corpus and an entity-aware self-Models QQP MRPC STS SST CoLA RTE MNLI-m QNLI BERT-large <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> 91.3/-88.0/-90.0/-93.2 60.6 70.4 86.6 92.3 XLNet-large <ref type="bibr" target="#b41">(Yang et al., 2019)</ref> 92.3/-90.8/-92.5/-97.0 69.0 85.9 90.8 94.9 SLA-large <ref type="bibr" target="#b17">(Li et al., 2020b)</ref> -/--/--/-94.3 64.5 ---RoBERTa-base <ref type="bibr" target="#b19">(Liu et al., 2019)</ref>  Table <ref type="table">3</ref>: Performance on dev sets of GLUE tasks. The results of BERT and RoBERTa are from <ref type="bibr" target="#b19">Liu et al. (2019)</ref>.</p><p>Results with ⋆ are from our re-implementations, as some metrics are not given by <ref type="bibr" target="#b19">Liu et al. (2019)</ref>. For each task, we run model for 5 times with different random initialization seeds, and report the median result.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Results</head><p>Table <ref type="table">4</ref> shows the performances of SOTA models and our models on CoNLL-2003 and OpenEntity. On the NER task, SynCLM-large improves F1 score by 0.4% compared with RoBERTalarge. Meanwhile, phrase-guided method consistently outperforms tree-guided method both in the base and large model. We think this is because the goal of phrase-guided method matches pretty well with the goal of NER. On the ENT task, SynCLM obtains 0.9% and 0.5% precision improvements under the settings of base-size and large-size, respectively.</p><p>Comparison with SEPREM and SLA Compared with SEPREM on ENT, SynCLM achieves a smaller improvement. We suspect the reason is two-fold. First, in SynCLM, syntactic information is incorporated only in the pre-training stage. Second, the pre-training data used in SEPREM is about ten times larger than ours. In order to verify the above hypotheses, based on RoBERTabase, we implement the two pre-training tasks of SEPREM, namely dependency head prediction and dependency distance prediction, and train them on the pre-training data used in our work, resulting in SEPREM-base in Table <ref type="table">4</ref>. We observe that SEPREM trained on small-scale data does not perform well. Meanwhile, we merge SLA in the finetuning stage, resulting in SynCLM-base + SLA.</p><p>The result verifies that SynCLM can be further improved by using syntactic information during fine-tuning stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">GLUE Benchmark</head><p>Besides, we evaluate SynCLM on the GLUE benchmark <ref type="bibr" target="#b31">(Wang et al., 2018)</ref>, a collection of diverse datasets for evaluating natural language understanding models. It contains single-sentence classification tasks (CoLA and SST-2), similarity and paraphrase tasks (MRPC, QQP, and STS-B), as well as pairwise inference tasks (MNLI, RTE, and QNLI). We use the default train/dev/test split. For each dataset, we fine-tune the pre-trained model separately, using only the corresponding singletask training data (i.e., without multi-task training). Our fine-tuning procedure follows the original RoBERTa paper. We consider a limited hyperparameter sweep for each task, with batch sizes ∈ {16, 32} and learning rates ∈ {1e − 5, 2e − 5, 3e − 5}, with a linear warm up for the first 6% of steps followed by a linear decay to 0. We fine-tune for 10 epochs. The rest of the hyperparameters remain the same as during pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>As shown in Table <ref type="table">3</ref>, our models outperform baseline models in most tasks, and achieve more significant gains in tasks with small training datasets, such as CoLA, RTE, MRPC.</p><p>The performance on CoLA is discussed Section 4.1. On RTE, compared with baseline models, SynCLM obtains significant gains of 1.4% and 1.1% in base size and large size, respectively. Similarly, it brings accuracy improvement of 1.3% for base model and 0.5% for large model on MRPC. Moreover, incorporating syntactic knowledge into base models brings greater improvements in some datasets. From the results of all downstream tasks, it can be seen that syntactic information is more useful when task's training data is small or the computation power is limited. We think that more training data in the fine-tuning stage will lead to greater loss of syntactic knowledge encoded in the last layer's hidden representations, as last two layers encode task-specific features and undergo the largest changes <ref type="bibr" target="#b14">(Kovaleva et al., 2019)</ref>.</p><p>Besides, SynCLM achieves larger improvements on single-sentence tasks, but does not always perform well on sentence-pair tasks. We think this is because the cross-sentence interactions are more important for sentence-pair tasks. How to use syntactic information effectively in the sentence-pair tasks is a problem we plan to explore in the future.</p><p>Finally, we can conclude that SynCLM is still effective in general tasks, especially in tasks with small training data.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis</head><p>Structural Probing Tasks To check whether the representation learned by SynCLM captures syntactic knowledge effectively, following <ref type="bibr" target="#b10">Hewitt and Manning (2019)</ref>, we construct a syntax tree of a sentence with linear transformation learned for the embedding space. If the syntax tree is better constructed, the model is considered having learned more syntactic information. We use the pre-trained LM based on phrase-guided method to capture the Stanford Dependencies formalism <ref type="bibr" target="#b4">(De Marneffe et al., 2006)</ref>. Similarly, we use the undirected attachment score (UUAS) denoting the percentage of correctly placed undirected edges as the main metric. We also report spearman correlation (Spr.) between predicted and the actual distance between each token pair in a sentence. The results are shown in Table <ref type="table" target="#tab_5">5</ref>. It can be seen that our base model obtains SOTA results on UUAS, indicating that our method can enhance the model capability of capturing syntactic structures.</p><p>Case Study Figure <ref type="figure" target="#fig_1">2</ref> gives three examples to illustrate the effectiveness of incorporating syntactic information. These examples show that SynCLM can capture syntactic information and make correct predictions based on the obtained information. To give further insight into how syntactic knowledge affects prediction, we highlight the main syntax structures that affect prediction. Here, we take the third case for detailed analysis. The core tokens in the syntax trees of the two sentences are the same, so the model predicts they have the same semantics.</p><p>Through more data analysis, we find that SynCLM enhances the attention weight between syntactically related words, thus increasing the importance of non-leaf tokens in model prediction. Please note that this feature also leads to wrong predictions. In the future we will attend to the problem of how to integrate syntactic and semantic information in model prediction.</p><p>Attention Visualization In order to verify the impact of syntactic information in the attention mechanisms of the pre-trained LM, we plot attention weights of baseline models and our models in Figure <ref type="figure" target="#fig_2">3</ref>. We mainly focus on the interactions of tokens, except for [CLS] and <ref type="bibr">[SEP]</ref>. Then the attention weights are averaged over all heads and layers. This visualization demonstrates the effectiveness of injecting syntactic information into self-attention.</p><p>From Figure <ref type="figure" target="#fig_2">3</ref>, we can see that higher attention weight between directly syntactically related tokens in our model. For example, our model assigns strong attentions from the token "John" to "go" and "abroad", while the baseline model assigns lower attentions for these correlated tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>To the best of our knowledge, this is the first work of leveraging contrasting learning to inject syntax knowledge into pre-trained LMs. Motivated by the properties of constituent and dependency structures of syntax, we design phrase-guided and tree-guided learning objectives to guide the learning of attention distributions and hidden representations in the transformer. Through extensive experiments, we show that SynCLM consistently improves a wide range of tasks, from GED, entity tasks to GLEU, which confirms the advantage of our syntax-guided contrastive learning. Detailed analysis also shows that SynCLM does incorporate rich syntax knowledge and learn explainable attention weights.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Overview of our pre-training framework. P and N i represent the positive sample and the i-th negative sample, respectively. The phrase-guided contrastive objective is based on the constituent structure of inputs, focusing on using local syntactic information to guide the learning of attention distributions. The tree-guided objective is based on the dependency structure, using global syntactic information to enhance the hidden representations.</figDesc><graphic url="image-1.png" coords="2,82.20,70.84,430.88,202.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Case study. The third column shows the simplified syntax tree of each example. P base represents the label predicted by RoBERTa-base model, and P syntax represents the label predicted by SynCLM-base.</figDesc><graphic url="image-2.png" coords="7,109.41,248.30,376.46,89.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visualization of attention weight scores. This case is from CoLA and has grammatical error. The red rectangle indicates higher scores in our model but lower scores in the baseline model.</figDesc><graphic url="image-3.png" coords="8,306.43,173.56,217.70,99.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Besides, for fair comparison, we report continue training results Results on GED datasets. Results with "⋆" are taken from</figDesc><table><row><cell>Models</cell><cell>BLiMP 1P/2P Acc</cell><cell>CoLA MCC</cell><cell>FCE Acc/F0.5</cell></row><row><cell>BERT-large (Devlin)</cell><cell>-/-</cell><cell>63.9 ⋆</cell><cell>-/57.3 ⋆</cell></row><row><cell>BiLSTM-Joint (Rei)</cell><cell>-/-</cell><cell>-</cell><cell>80.1/52.1</cell></row><row><cell>SLA-large (Li20)</cell><cell>-/-</cell><cell>64.5</cell><cell>-/58.0</cell></row><row><cell>GPT-2 large (Rad19)</cell><cell>78.0/81.6</cell><cell>-</cell><cell>-/-</cell></row><row><cell>RoBERTa-base (Liu)</cell><cell>74.9/78.5</cell><cell>63.6</cell><cell>83.3/68.6</cell></row><row><cell>+ continuous</cell><cell>75.0/79.6</cell><cell>63.8</cell><cell>83.5/68.6</cell></row><row><cell>+ PHRASE</cell><cell>75.5/81.2</cell><cell>64.5</cell><cell>83.9/69.0</cell></row><row><cell>+ TREE</cell><cell>76.4/80.6</cell><cell>64.9</cell><cell>84.2/68.9</cell></row><row><cell>SynCLM-base</cell><cell>77.3 ↑ /81.0 ↑</cell><cell>65.3 ↑</cell><cell>84.3 ↑ 69.2 ↑</cell></row><row><cell>RoBERTa-base + SLA</cell><cell>-/-</cell><cell>64.2</cell><cell>83.2/68.3</cell></row><row><cell>+ PHRASE</cell><cell>-/-</cell><cell>65.1</cell><cell>83.7/67.3</cell></row><row><cell>+ TREE</cell><cell>-/-</cell><cell>65.8</cell><cell>84.3/68.4</cell></row><row><cell>SynCLM-base + SLA</cell><cell>-/-</cell><cell>66.3 ↑</cell><cell>83.6/68.7</cell></row><row><cell>RoBERTa-large (Liu)</cell><cell>77.3/79.4</cell><cell>68.0</cell><cell>85.3/72.2</cell></row><row><cell>SynCLM g</cell><cell>79.5 ↑ /81.1 ↑</cell><cell>69.3 ↑</cell><cell>86.1 ↑ /72.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>91.6/88.9 ⋆ 90.1/92.7 ⋆ 90.9/90.7 ⋆ 94.8 63.6 78.7 ↑ /93.7 ↑ 90.8/90.6 95.1 ↑ 65.3 ↑ 80.1 ↑ 87.2 93.0 RoBERTa-large (Liu et al., 2019) 92.1/89.5 ⋆ 90.7/93.2 ⋆ 92.2/92.1 ⋆ 96.4 68.0 86.3 ⋆ ↑ /93.6 ↑ 92.0/91.8 96.7 ↑ 69.3 ↑ 87.4 ↑</figDesc><table><row><cell></cell><cell></cell><cell>87.6</cell><cell>92.8</cell></row><row><cell>+ continuous</cell><cell>91.6/88.8 90.2/92.8 90.2/90.1 94.9 63.8 79.1</cell><cell>87.2</cell><cell>92.8</cell></row><row><cell>+ PHRASE</cell><cell>91.7/88.9 90.5/93.0 90.3/90.2 95.2 64.5 79.8</cell><cell>87.0</cell><cell>92.9</cell></row><row><cell>+ TREE</cell><cell>91.7/88.9 91.2/93.6 90.6/90.4 95.1 64.9 80.1</cell><cell>87.4</cell><cell>93.0</cell></row><row><cell>SynCLM-base</cell><cell cols="2">91.7/88.9 91.4 90.2</cell><cell>94.7</cell></row><row><cell>SynCLM-large</cell><cell cols="2">92.3/89.7 91.2 90.5</cell><cell>94.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>↑ / 93.0 ↑ / 93.0 ↑ 76.6 / 74.6 / 75.6 ↑</figDesc><table><row><cell></cell><cell></cell><cell>1 / 79.1</cell></row><row><cell>LUKE (Yamada)</cell><cell>-/ -/ 94.3</cell><cell>79.9 / 76.6 / 78.2</cell></row><row><cell>SEPREM-base</cell><cell>84.0 / 92.9 /88.2</cell><cell>76.7 / 73.5 / 75.1</cell></row><row><cell>RoBERTa-base</cell><cell>92.4 / 92.9 / 92.6</cell><cell>75.7 / 74.6 / 75.1</cell></row><row><cell>+ PHRASE</cell><cell>92.9 / 93.0 / 93.0</cell><cell>75.9 / 74.9 / 75.4</cell></row><row><cell>+ TREE</cell><cell>92.7 / 92.9 / 92.8</cell><cell>75.6 / 75.2 / 75.4</cell></row><row><cell cols="2">SynCLM-base 93.0 + SLA 92.1 / 94.1 / 93.1 ↑</cell><cell>76.7 / 74.6 / 75.7 ↑</cell></row><row><cell>RoBERTa-large</cell><cell>93.0 / 93.5 / 93.2</cell><cell>76.3 / 76.1 / 76.2</cell></row><row><cell>SynCLM-large</cell><cell>93.4 ↑ / 93.8 ↑ / 93.6 ↑</cell><cell>76.8 / 76.1 / 76.4</cell></row><row><cell cols="3">Table 4: Results (average of 5 runs) on entity tasks. We</cell></row><row><cell cols="3">report continue training results for RoBERTa-base.</cell></row><row><cell cols="3">attention mechanism to learn pre-trained contextu-</cell></row><row><cell cols="3">alized representations for words and entities, and</cell></row><row><cell cols="3">obtains SOTA results on five entity-related datasets,</cell></row><row><cell cols="3">including CoNLL-2003 and OpenEntity.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>The results of structural probing task.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/PaddlePaddle/ Research/tree/master/NLP/ACL2022-SynCLM</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">In our experiments, the sampled phrase has no more than two hierarchical layers, that is to say, the height of its corresponding subtree is no more than 2.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://github.com/stanfordnlp/stanza</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://github.com/pytorch/fairseq/ tree/master/examples/roberta</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">Due to the limitation of space and computing resources, we only give continue training results of base models.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We are very grateful to our anonymous reviewers for their helpful feedback on this work. We would like to thank Ying Chen for examination and revision in paper writing; Can Gao for the help on model training. This work was supported in part by the National Key R&amp;D Program of China under Grant 2020YFB1406701.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Syntaxbert: Improving pre-trained transformers with syntax trees</title>
		<author>
			<persName><forename type="first">Jiangang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiren</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhai</forename><surname>Tong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.04350</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Minimum hellinger distance estimates for parametric models. The annals of Statistics</title>
		<author>
			<persName><forename type="first">Rudolf</forename><surname>Beran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977">1977</date>
			<biblScope unit="page" from="445" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enhancing machine translation with dependency-aware self-attention</title>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Bugliarello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1618" to="1627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ultra-fine entity typing</title>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lrec</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="449" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A new metric for probability distributions</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Dominik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><forename type="middle">E</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName><surname>Schindelin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information theory</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1858" to="1860" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Constituency and the grammar. The language of turn and sequence</title>
		<author>
			<persName><forename type="first">Cecilia</forename><forename type="middle">E</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbara</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Noisecontrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
				<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9726" to="9735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A structural probe for finding syntax in word representations</title>
		<author>
			<persName><forename type="first">John</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4129" to="4138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">What does bert learn about the structure of language</title>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoît</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Djamé</forename><surname>Seddah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2019-57th Annual Meeting of the Association for Computational Linguistics</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning</title>
		<author>
			<persName><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Are pre-trained language models aware of phrases? simple but strong baselines for grammar induction</title>
		<author>
			<persName><forename type="first">Taeuk</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jihun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Edmiston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang-Goo</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.00737</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Revealing the dark secrets of bert</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Romanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4365" to="4374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">On information and sufficiency. The annals of mathematical statistics</title>
		<author>
			<persName><forename type="first">Solomon</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Leibler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1951">1951</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A unified mrc framework for named entity recognition</title>
		<author>
			<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020a</date>
			<biblScope unit="page" from="5849" to="5859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Improving bert with syntax-aware local attention</title>
		<author>
			<persName><forename type="first">Zhongli</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunbo</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15150</idno>
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Assessing the ability of lstms to learn syntax-sensitive dependencies</title>
		<author>
			<persName><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="521" to="535" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">From balustrades to pierre vinken: Looking for syntax in transformer self-attentions</title>
		<author>
			<persName><forename type="first">David</forename><surname>Mareček</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rudolf</forename><surname>Rosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
				<meeting>the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="263" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Comparison of the predicted and observed secondary structure of t4 phage lysozyme</title>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">W</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biochimica et Biophysica Acta (BBA)-Protein Structure</title>
		<imprint>
			<biblScope unit="volume">405</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="442" to="451" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Syntactic data augmentation increases robustness to inference heuristics</title>
		<author>
			<persName><forename type="first">Junghyun</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2339" to="2352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tree-structured attention with hierarchical accumulation</title>
		<author>
			<persName><forename type="first">Xuan-Phi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Jointly learning to label sentences and tokens</title>
		<author>
			<persName><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6916" to="6923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Do syntax trees help pretrained transformers extract information?</title>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Singh Sachan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Hamilton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.09084</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fien</forename><surname>De Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</title>
				<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Linguisticallyinformed self-attention for semantic role labeling</title>
		<author>
			<persName><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5027" to="5038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bert rediscovers the classical nlp pipeline</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4593" to="4601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
				<meeting>the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">K-adapter: Infusing knowledge into pre-trained models with adapters</title>
		<author>
			<persName><forename type="first">Ruize</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cuihong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.01808</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Kepler: A unified model for knowledge embedding and pre-trained language representation</title>
		<author>
			<persName><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="176" to="194" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tree transformer: Integrating tree structures into self-attention</title>
		<author>
			<persName><forename type="first">Yaushian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1060" to="1070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Blimp: The benchmark of linguistic minimal pairs for english</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alicia</forename><surname>Parrish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haokun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anhad</forename><surname>Mohananey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng-Fu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Transactions of the Association for Computational Linguistics</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="377" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Neural network acceptability judgments</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="625" to="641" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Structural supervision improves learning of non-local grammatical dependencies</title>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Futrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3302" to="3312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Zenan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinliang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanjun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.14116</idno>
		<title level="m">Syntax-enhanced pre-trained model</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Luke: Deep contextualized entity representations with entityaware self-attention</title>
		<author>
			<persName><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6442" to="6454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A new dataset and method for automatically grading esol texts</title>
		<author>
			<persName><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Medlock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies</title>
				<meeting>the 49th annual meeting of the association for computational linguistics: human language technologies</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fine-tuning pre-trained language model with weak supervision: A contrastive-regularized self-training approach</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simiao</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter</title>
				<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1063" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sg-net: Syntax-guided machine reading comprehension</title>
		<author>
			<persName><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junru</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sufeng</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9636" to="9643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
