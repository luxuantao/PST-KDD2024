<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab of CAD&amp;CG</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310058</postCode>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4D0783DCB74E687B7F7E8DE4F583DF43</idno>
					<idno type="DOI">10.1109/TIP.2009.2032342</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Laplacian Regularized D-Optimal Design for Active</head><p>Learning and Its Application to Image Retrieval Xiaofei He, Member, IEEE Abstract-In increasingly many cases of interest in computer vision and pattern recognition, one is often confronted with the situation where data size is very large. Usually, the labels are expensive and the challenge is, thus, to determine which unlabeled samples would be the most informative (i.e., improve the classifier the most) if they were labeled and used as training samples. Particularly, we consider the problem of active learning of a regression model in the context of experimental design. Classical optimal experimental design approaches are based on least square errors over the measured samples only. They fail to take into account the unmeasured samples. In this paper, we propose a novel active learning algorithm which operates over graphs. Our algorithm is based on a graph Laplacian regularized regression model which simultaneously minimizes the least square error on the measured samples and preserves the local geometrical structure of the data space. By constructing a nearest neighbor graph, the geometrical structure of the data space can be described by the graph Laplacian. We discuss how results from the field of optimal experimental design may be used to guide our selection of a subset of data points, which gives us the most amount of information. Experiments demonstrate its superior performance in comparison with conventional algorithms. Index Terms-Active learning, experimental design, image retrieval, regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>I N many computer vision and pattern recognition tasks, there is no shortage of unlabeled data but labels are expensive. The challenge is, thus, to determine which unlabeled samples would be the most informative (i.e., improve the classifier the most) if they were labeled and used as training samples. This problem is typically called active learning <ref type="bibr" target="#b0">[1]</ref>.</p><p>Here, the task is to minimize an overall cost, which depends both on the classifier accuracy and the cost of data collection. Particularly, we consider the problem of active learning of a regression model in the context of experimental design <ref type="bibr" target="#b1">[2]</ref>.</p><p>In statistics, the problem of selecting samples to label is typically referred to as experimental design. The sample is referred to as experiment, and its label is referred to as measurement. The study of optimal experimental design (OED) <ref type="bibr" target="#b1">[2]</ref> is concerned with the design of experiments that are expected to minimize variances of a parameterized model. The intent of optimal experimental design is usually to maximize confidence in a given model, minimize parameter variances for system identification, or minimize the model's output variance. Classical experimental design approaches include A-Optimal Design, D-Optimal Design, and E-Optimal Design <ref type="bibr" target="#b1">[2]</ref>. All of these approaches are based on a least squares regression model. They evaluate the model over the measured (labeled) points, whereas the unmeasured (unlabeled) points are ignored.</p><p>Besides regression based active learning approaches, many other active learning techniques have been proposed, mostly based on support vector machines. Based on the observation that the closer to the SVM boundary a data point is, the less reliable its classification is, Tong and Chung proposed a method which selects those unlabeled data points closest to the boundary to solicit user's labeling so as to achieve maximal refinement on the hyperplane between the two classes <ref type="bibr" target="#b2">[3]</ref>. The major disadvantage of SVM active learning is that the estimated boundary may not be accurate enough. Moreover, because it needs an initial classification boundary, it cannot be applied when there is no labeled data points. It would be important to note that the experimental design methods described above <ref type="bibr" target="#b1">[2]</ref> do not suffer from these limitations. Some other SVM based active learning algorithms can be found in <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b4">[5]</ref>.</p><p>In the last decade, there has been considerable interest in the problem of learning from both labeled (measured) and unlabeled (unmeasured) points <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b8">[9]</ref>. Two well known algorithms are transductive SVM <ref type="bibr" target="#b9">[10]</ref> and co-training <ref type="bibr" target="#b10">[11]</ref>. Transductive SVM aims at finding a separating hyperplane for the training set that is also maximally distant from the (unlabeled) working set. This hyperplane is used to predict the labels for the working set points. In this form, the problem has exponential computational complexity, and several approaches have been attempted to solve it. Generally, they involve some form of local search <ref type="bibr" target="#b11">[12]</ref>, integer programming methods <ref type="bibr" target="#b12">[13]</ref>, or semi-definite programming <ref type="bibr" target="#b13">[14]</ref>. The co-training setting applies when a dataset has a natural division of its features. An algorithm that uses the co-training setting may learn separate classifier over each of the feature sets, and combine their predictions to decrease classification error. Co-training algorithms using label and unlabeled data explicitly leverage this split during learning. Recently, geometry-based semi-supervised learning methods have attracted an increasing amount of attention <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. These algo-rithms explicitly take into account the intrinsic manifold structure by making use of both labeled and unlabeled data points. For a detailed treatment, please see <ref type="bibr" target="#b5">[6]</ref>.</p><p>Active learning and semi-supervised learning can be thought of as two sides of the same coin. Both of them aim at reducing the labeling task. In this paper, we propose a novel active learning algorithm over graphs, called Laplacian Regularized D-optimal Design (LapRDD), which is motivated by recent progress on manifold based semi-supervised learning <ref type="bibr" target="#b14">[15]</ref>. Our algorithm is fundamentally based on experimental design framework. However, unlike traditional experimental design methods whose loss functions are only defined on the measured points, the loss function of our proposed algorithm is defined on both measured and unmeasured points. Specifically, we use a loss function which imposes a locality preserving regularizer into the standard least-square-error based loss function. The new loss function aims at finding a classifier which is locally as smooth as possible. Using techniques from experimental design <ref type="bibr" target="#b1">[2]</ref>, we can select the most informative data points which are presented to the user for labeling.</p><p>One of the most natural application of active learning is relevance feedback driven image retrieval <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b23">[24]</ref>. The user is required to provide his/her relevance judgements on the top images returned by the system. The labeled images are then used to train a classifier to separate images that match the query concept from those that do not. However, in general, the top returned images may not be the most informative ones. In the worst case, all the top images labeled by the user may be positive, and, thus, the standard classification techniques cannot be applied due to the lack of negative examples. Unlike the standard classification problems where the labeled samples are pregiven, in relevance feedback image retrieval the system can actively select the images to label. Thus, active learning can be naturally introduced into image retrieval. Our previous work has demonstrated that image retrieval performance can be improved by actively selecting images for labeling <ref type="bibr" target="#b24">[25]</ref>. The major issue is that the computational complexity is relatively high, which restricts its real application.</p><p>The rest of the paper is organized as follows. In Section II, we provide a brief description of the related work. Our proposed Laplacian Regularized D-optimal Design algorithm is introduced in Section III. We describe the nonlinear generalization of our algorithm in Section IV. The extensive experimental results on image retrieval are presented in Section V. Finally, we provide some concluding remarks and suggestions for future work in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Our algorithm is fundamentally based on Laplacian regularized least squares (LapRLS, <ref type="bibr" target="#b14">[15]</ref>). Also, for regression based active learning, the most related work is optimal experimental design <ref type="bibr" target="#b1">[2]</ref>, including A-Optimal Design, D-Optimal Design, and E-Optimal Design. In this section, we give a brief description of these approaches. Throughout this paper, we use to denote the labeled point, and to denote any point (either labeled or unlabeled).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Active Learning Problem</head><p>The generic problem of active learning is the following. Given a set of points in , find a subset which contains the most informative points. In other words, the points can improve the classifier the most if they are labeled and used as training points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Optimal Experimental Design</head><p>We consider a linear regression model <ref type="bibr" target="#b0">(1)</ref> where is the observation, is the independent variable, is the weight vector and is an unknown error with zero mean. Different observations have errors that are independent, but with equal variances . We define to be the learner's output given input and the weight vector . Suppose we have a set of labeled sample points , where is the label of . Thus, the maximum likelihood estimate for the weight vector, , is that which minimizes the sum squared error <ref type="bibr" target="#b1">(2)</ref> With some simple algebraic steps, we have <ref type="bibr" target="#b2">(3)</ref> where and . The estimate gives us an estimate of the output at a novel input:</p><p>. By Gauss-Markov theorem, we know that has a zero mean and a covariance matrix given by , where is the Hessian of where . The three most common scalar measures of the size of the parameter covariance matrix in optimal experimental design are as follows.</p><p>• D-optimal design: determinant of . • A-optimal design: trace of . • E-optimal design: maximum eigenvalue of . Some recent work on experimental design can be found in <ref type="bibr" target="#b25">[26]</ref> and <ref type="bibr" target="#b26">[27]</ref>.</p><p>In this work, we adopt the similar optimality criterion to D-optimal design for selecting the most informative data points. However, the major drawback of the above methods is that they only take into account the measured (labeled) data points whereas the unmeasured (unlabeled) data points are ignored. There is no theoretical guarantee how the trained classifier performs on the entire data set. To overcome this limitation, we use regularized least squares as the objective function which takes into account the local manifold structure of both measured and unmeasured data points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Laplacian Regularized Least Squares</head><p>Different from the standard regression framework which makes use of only labeled points, the LapRLS algorithm makes use of both labeled and unlabeled points to discover the intrinsic geometrical structure in the data. It is assumed that if two points , are sufficiently close to each other, then their measurements ( , ) are close as well. Suppose there are totally points out of which points are labeled. Let be a similarity matrix. Thus, the LapRLS algorithm solves the following optimization problem: <ref type="bibr" target="#b3">(4)</ref> where is the measurement (or, label) of . The loss function with our choice of symmetric weights incurs a heavy penalty if neighboring points and are mapped far apart. Therefore, minimizing is an attempt to ensure that if and are close then and are close as well. There are many choices of the similarity matrix . A simple definition is as follows:</p><p>if is among the nearest neighbors of or is among the nearest neighbors of otherwise.</p><p>(5) Let be a diagonal matrix, , and . The matrix is called graph Laplacian in spectral graph theory <ref type="bibr" target="#b27">[28]</ref>. <ref type="bibr">Let</ref> . The solution to the minimization problem (4) is given as follows: <ref type="bibr" target="#b5">(6)</ref> where is an identity matrix. For more details about the algorithm, please see <ref type="bibr" target="#b14">[15]</ref>.</p><p>LapRLS is a passive learning algorithm in which the training data points are pregiven. Our algorithm shares the same objective function as LapRLS but actively selects the data points for labeling. There are several contributions of the proposed framework of analysis and the algorithm. First, this is the first work which takes into account the intrinsic manifold structure by using both measured and unmeasured data points. In some applications like image retrieval, the labeling resource is usually limited. Thus, the selected data points are very crucial for training a good classifier. Second, we introduce a simple yet effective scheme to solve the optimization problem. Comparing to previous work <ref type="bibr" target="#b24">[25]</ref>, the computational complexity of the proposed approach is much lower and, thus, more suitable for real applications. Third, a unified active and semi-supervised learning framework is proposed in a principled manner. During the retrieval process, the system has opportunity to present the most informative images to the user for feedbacks, which is essentially an active learning problem. Once the feedbacks are obtained, both the labeled and unlabeled images are used to learn a classifier, which is essentially a semi-supervised problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. LAPLACIAN REGULARIZED D-OPTIMAL DESIGN</head><p>In this section, we introduce our active learning algorithm which is based on Laplacian Regularized Least Squares. Often estimates of the parameters are of interest together with predictions of the responses from the fitted model. The variances of the parameter estimates and predictions depend on the particular experimental designs used and should be as small as possible. We begin with the analysis of bias and variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Analysis of Bias and Variance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Since</head><p>, we have , where . Clearly</p><p>, where . We define <ref type="bibr" target="#b6">(7)</ref> and ( <ref type="formula">8</ref>)</p><p>It can be shown that is the hessian of . The bias of the estimator for the coefficient vector can be computed as follows: <ref type="bibr" target="#b8">(9)</ref> By noticing that and is symmetric, the covariance matrix of has the expression <ref type="bibr" target="#b9">(10)</ref> In order to make the estimator as stable as possible, the size of covariance matrix has to be as small as possible. Different measures of the size of the covariance matrix lead to different optimality criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Algorithm</head><p>The most important design criterion in application is that of D-optimality, in which the determinant of the covariance matrix is minimized. Two other popular criteria that have a statistical interpretation in terms of the covariance matrix are A-and E-optimality <ref type="bibr" target="#b1">[2]</ref>. In A-optimality , the total variance of the parameter estimates, is minimized, equivalent to minimizing the average variance. In E-optimality the variance of the least well estimated linear combination is minimized subject to the constraint that . Thus, the in the name stands for extreme. For a detailed description about these optimality criteria, please see <ref type="bibr" target="#b1">[2]</ref>. Let denote the set of all the data points and denote the set of the first selected points. Clearly, and . In this paper, we apply D-optimality to select the most informative samples due to its connection to the confidence region for the parameters <ref type="bibr" target="#b1">[2]</ref>. Since the regularization parameters ( and ) are usually set to be very small, we have where denote the determinant. So the smaller is, the smaller is the size of the covariance matrix. Also, since , by minimizing the size of , the bias is also minimized. Noticing that , the problem is, thus, formally defined below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Laplacian Regularized D-Optimal Design</head><p>where are selected from</p><p>In the following, we describe a sequential construction of Laplacian regularized D-optimal designs. Let be the hessian of after points are selected <ref type="bibr" target="#b10">(11)</ref> where</p><p>. It is easy to show that the graph Laplacian is positive semi-definite. Therefore, is positive definite and invertible. The first data point is selected such that is maximized. Suppose points have been selected. In other words, is given. Thus, the -th point is selected such that is maximized <ref type="bibr" target="#b11">(12)</ref> By using matrix determinant lemma <ref type="bibr" target="#b28">[29]</ref>, the determinant of can be written as a multiplicative updated of the determinant of <ref type="bibr" target="#b12">(13)</ref> Since is a constant while selecting the -th point, (12) can be rewritten as follows: <ref type="bibr" target="#b13">(14)</ref> Once the -th point is selected, the inverse of can be updated based on the inverse of , by using the Sherman-Morrison formula <ref type="bibr" target="#b29">[30]</ref> (15)</p><p>Our analysis shows that we do not need to compute the matrix determinant and inverse. Instead, at each iteration we select a new point such that is maximized and the inverse of can be efficiently updated in terms of <ref type="bibr" target="#b14">(15)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. NONLINEAR GENERALIZATION</head><p>In this section, we describe how to generalize our algorithm to nonlinear problem by using kernel tricks <ref type="bibr" target="#b30">[31]</ref>.</p><p>We consider the problem in a feature space induced by some nonlinear mapping:</p><p>. For a properly chosen , an inner product can be defined on which makes for a so-called reproducing kernel Hilbert space (RKHS). More specifically, holds where is a positive semi-definite kernel function. Several popular kernel functions are: Gaussian kernel ; polynomial kernel ; Sigmoid kernel . Let and denote the corresponding data matrices in the Hilbert space Thus, the hessian in the Hilbert space can be written as follows: <ref type="bibr" target="#b15">(16)</ref> Since is unknown, there is no way to compute the determinant of . Instead, we consider the regression model in RKHS <ref type="bibr" target="#b16">(17)</ref> Thus, the objective function (4) in RKHS can be written as follows:</p><p>(18)</p><p>We have the following proposition. Proposition IV.1: Let be a subspace of , the minimum solution to the problem ( <ref type="formula">18</ref>) is in . Proof: Let be the orthogonal complement of , i.e., . Thus, for any point</p><p>, it has orthogonal decomposition as follows: Since , , and , it is easy to see that This completes the proof.</p><p>From Proposition IV.1, we see that can be represented as a linear combination of ,</p><p>where . Since is a constant matrix in RKHS, <ref type="bibr" target="#b18">(19)</ref> tells us that the only parameter we need to estimate is . Similar to the linear algorithm described in Section III-B, here we select the points such that the size of is minimized.</p><p>From ( <ref type="formula" target="#formula_0">19</ref>), we have <ref type="bibr" target="#b19">(20)</ref> Let be the Moore-Penrose inverse of . By noticing that , we have Let be a matrix , be a matrix , and be a matrix . Thus, we have (21) The nonlinear problem is defined as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kernel Laplacian Regularized D-Optimal Design</head><p>where are selected from Let be the th column vector of and be the set of . corresponds to , and . Thus, the first data point is selected such that is maximized. Suppose points have been selected, and we define <ref type="bibr" target="#b21">(22)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Let</head><p>. The -th point is selected by solving the following optimization problem: <ref type="bibr" target="#b22">(23)</ref> which is equivalent to the following problem by matrix determinant lemma <ref type="bibr" target="#b28">[29]</ref>: <ref type="bibr" target="#b23">(24)</ref> Similar to the linear algorithm described in Section III-B, the inverse of can be updated as follows:</p><p>(25)</p><p>As can be seen, the computation of the nonlinear algorithm is essentially the same as that of the linear algorithm, except that the data vectors are replaced by .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONTENT-BASED IMAGE RETRIEVAL USING LAPRDD</head><p>In this section, we describe how to apply LapRDD to contentbased image retrieval (CBIR). We begin with a brief description of image representation using low level visual features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Low-Level Image Representation</head><p>Low-level image representation is a crucial problem in CBIR. General visual features includes color <ref type="bibr" target="#b31">[32]</ref>, texture <ref type="bibr" target="#b32">[33]</ref>, shape <ref type="bibr" target="#b33">[34]</ref>, etc. Color and texture features are the most extensively used visual features in CBIR. Compared with color and texture features, shape features are usually described after images have been segmented into regions or objects. Since robust and accurate image segmentation is difficult to achieve, the use of shape features for image retrieval has been limited to special applications where objects or regions are readily available.</p><p>In this work, We combine 64-dimensional color histogram and 64-dimensional color texture moment (CTM, <ref type="bibr" target="#b34">[35]</ref>) to represent the images. The color histogram is calculated using bins in HSV space. The color texture moment is proposed by Yu et al. <ref type="bibr" target="#b34">[35]</ref>, which integrates the color and texture characteristics of the image in a compact form. CTM adopts local Fourier transform as a texture representation scheme and derives eight characteristic maps to describe different aspects of co-occurrence relations of image pixels in each channel of the (SVcosH, SVsinH, V) color space. Then CTM calculates the first and second moment of these maps as a representation of the natural color image pixel distribution. Please see <ref type="bibr" target="#b34">[35]</ref> for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Relevance Feedback Image Retrieval</head><p>Relevance feedback is one of the most important techniques to narrow down the gap between low level visual features and high level semantic concepts <ref type="bibr" target="#b16">[17]</ref>. Traditionally, the user's relevance feedbacks are used to update the query vector or adjust the weighting of different dimensions. This process can be viewed as an on-line learning process in which the image retrieval system acts as a learner and the user acts as a teacher. The typical retrieval process is outlined as follows.</p><p>1) The user submits a query image example to the system. The system ranks the images in database according to some predefined distance metric and presents to the user the top ranked images. 2) The system selects some images from the database and request the user to label them as "relevant" or "irrelevant." 3) The system uses the user's provided information to re-rank the images in database and returns to the user the top images. Go to step 2 until the user is satisfied. Our LapRDD algorithm is applied in the second step for selecting the most informative images. Once we get the labels for the images selected by LapRDD, we apply Laplacian regularized least squares to solve the optimization problem ( <ref type="formula">4</ref>) and build the classifier. The classifier is then used to re-rank the images in database. Note that, in order to reduce the computational  complexity, we do not use all the unlabeled images in the database but only those within top 500 returns of previous iteration. We have also tried using different unlabeled images rather than those within the top 500 returns. We found that there is almost no difference by using different unlabeled images for training the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTAL EVALUATION</head><p>In this section, we apply our proposed algorithm to relevance feedback driven image retrieval <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b35">[36]</ref> and evaluate its performance on a large image database. The image database we used consists of 7,900 images of 79 semantic categories, from COREL data set. Fig. <ref type="figure" target="#fig_1">2</ref> shows some sample images. We begin with a simple synthetic example to give some intuition about how LapRDD works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Simple Synthetic Example</head><p>A simple synthetic example is given in Fig. <ref type="figure" target="#fig_0">1</ref>. The data set is generated from mixture of three Gaussians. Three points are selected by AOD and LapRDD. As can be seen, the points selected by AOD are far from the cluster centers, whereas LapRDD selects three points right at the center of the clusters. Clearly, the points selected by our LapRDD algorithm can better represent the original data set. We did not compare our algorithm with because cannot be applied in this case due to the lack of the labeled points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Settings</head><p>In this subsection, we describe the experimental settings. We start with the evaluation metric. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Evaluation Metric:</head><p>We use precision-scope curve and precision rate <ref type="bibr" target="#b36">[37]</ref> to evaluate the effectiveness of the image retrieval algorithms. The scope is specified by the number of top-ranked images presented to the user. The precision is the ratio of the number of relevant images presented to the user to the scope . The precision-scope curve describes the precision with various scopes and, thus, gives an overall performance evaluation of the algorithms. On the other hand, the precision rate emphasizes the precision at a particular value of scope. In general, it is appropriate to present 20 images on a screen. Putting more images on a screen might affect the quality of the presented images. Therefore, the precision at top 20 is especially important.</p><p>In a real image retrieval system, a query image is usually not in the image database. To simulate such environment, we use five-fold cross validation to evaluate the algorithms. More precisely, we divide the whole image database into five subsets with equal size. Thus, there are 20 images per category in each subset. At each run of cross validation, one subset is selected as the query set, and the other four subsets are used as the database for retrieval. The precision-scope curve and precision rate are computed by averaging the results from the five-fold cross validation.</p><p>2) Automatic Relevance Feedback Scheme: We designed an automatic feedback scheme to model the retrieval process. For each submitted query, our system retrieves and ranks the im-ages in the database. At each iteration, ten images were selected from the database for user labeling and the label information is used by the system for re-ranking. Note that, the images which have been selected at previous iterations are excluded from later selections. For each query, the automatic relevance feedback mechanism is performed for four iterations.</p><p>3) Compared Algorithms: To demonstrate how our proposed algorithm improves the performance of image retrieval, we compared the following five algorithms.</p><p>• Our LapRDD algorithm.</p><p>• Ridge regression (RidgeReg).</p><p>• Support Vector Machines (SVM).</p><p>• Support Vector Machine Active Learning (SVM-AL, <ref type="bibr" target="#b2">[3]</ref>).</p><p>• Laplacian Regularized Least Squares (LapRLS, <ref type="bibr" target="#b14">[15]</ref>). Out of these five algorithm, LapRDD and SVM-AL are active learning algorithms, whereas RidgeReg, SVM, and LapRLS are standard classification algorithms. Both RidgeReg and SVM only make use of the labeled samples, whereas LapRLS is a semi-supervised learning algorithm which makes use of both labeled and unlabeled samples. For RidgeReg, SVM, and LapRLS, the user is required to label the top 10 images at each iteration, whereas for LapRDD and SVM-AL, the ten 0 training images are selected by the algorithms themselves. It would be important to note that SVM-AL is based on the standard SVM, and our LapRDD algorithm is based on LapRLS. The regularization parameters and are both set to be 0.01. For both LapRDD and LapRLS algorithms, we use the same graph structure [see <ref type="bibr" target="#b4">(5)</ref>] and set the value of (number of nearest neighbors) to be 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance Evaluation</head><p>In the real world, it is not practical to require the user to provide many rounds of feedbacks. The retrieval performance after the first two rounds of feedbacks (especially the first round) is the most important. Table <ref type="table" target="#tab_0">I</ref> shows the precision at the top 20 after the first round of feedbacks for all the 79 categories. The baseline method describes the initial retrieval result without feedback information. Specifically, at the beginning of retrieval, the Euclidean distances are used to rank the images in the database. The retrieval performance of all the algorithms varies with the different categories. As can be seen, among all the 79 categories, our LapRDD algorithm performs the best on 54 categories. For the rest 25 categories, LapRLS performs the best on 16 of them, SVM performs the best on 4 of them, Ridge Regression performs the best on two of them, and the baseline performs the best on three of them. Note that, SVM-AL can only be applied when there is a initial classifier available. Therefore, it cannot be applied at the first round and we use the standard SVM to build the initial classifier.</p><p>We noticed that for categories containing images with similar colors and textures, the baseline method which simply uses Euclidean distance to sort the images works the best. For example, for category 44 as shown in Fig. <ref type="figure" target="#fig_1">2(c</ref>), the baseline method achieved 95.2% retrieval accuracy, while Ridge Regression, SVM, LapRLS, and LapRDD achieved 89.6%, 87.6%, 89.2%, 74.2% retrieval accuracies, respectively. In this case, our algorithm performs the worst. One reason might be that for these categories, the images are well clustered. Therefore, the Euclidean distance is the best choice for distinguishing one category from the others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Discussions</head><p>In general, active learning algorithms perform better than passive learning algorithms. This is because active learning algorithms select the data points for labeling in a principled manner to minimize the expected prediction error. We believe the main reason that our algorithm works better than other active learning algorithms (e.g., A-optimal design and SVM active learning) is that our algorithm explicitly considers the local geometrical structure and tries to find data points on which the trained classifier is as smooth as possible on the whole data set. In other words, our algorithm tries to minimize the expected prediction error on the whole data set rather than just the selected data points. For A-optimal design, it essentially tries to find data points such that the variance is maximized. So, the selected data points can well represent the whole data set. It is, therefore, optimal in the sense of representation. However, it is not optimal in the sense of classification because the unmeasured (unlabeled) points are not considered while estimating the coefficients. Regarding SVM active learning, it relies heavily on the initial classifier since it selects data points closest to the decision boundary. If the initial classifier is not that good, the selected data points may not be good for classification. Fig. <ref type="figure" target="#fig_2">3</ref> shows the precision-scope curves of the five algorithms for the first two feedback iterations. By iteratively adding the user's feedbacks, the corresponding precisions at top 10 (20, 30) of the five algorithms are shown in Fig. <ref type="figure" target="#fig_3">4</ref>. These results reveal a number of interesting points.</p><p>1) In all the results, our LapRDD algorithm consistently outperforms the other four algorithms. It especially performs well at the first round of feedback. 2) As more relevance feedbacks are provided by the user, the precisions obtained by different algorithms get close. As can be seen from Fig. <ref type="figure" target="#fig_2">3</ref>(b), at the second round of feedback there is almost no performance difference between SVM, SVM-AL, and LapRLS. On the other hand, the user is usually not willing to provide too many feedbacks. Therefore, the retrieval performance at the first round of feedback is crucial. 3) SVM-AL gains little improvement over the standard SVM, whereas our LapRDD algorithm significantly outperforms LapRLS. This indicates that our algorithm is more effective than SVM-AL as to selecting the most informative samples. 4) Out of the three passive learning algorithms (RidgeReg, SVM, and LapRLS), LapRLS significantly outperforms the other two. This shows that the unlabeled samples are very useful to discover the geometrical structure in the data and, thus, improve the learning performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this paper, we introduce a novel active learning algorithm called Laplacian Regularized D-optimal Design. The proposed algorithm is motivated from recent progress on graph based semi-supervised learning. Using techniques from optimal experimental design, we select those points such that the size of the covariance matrix of the estimated coefficients is minimized. Central to the algorithm is a graph structure that is inferred on the data points and captures the intrinsic geometry of the data space. We have demonstrated the effectiveness of our proposed method through extensive experiments on image retrieval. Our proposed algorithm outperforms ridge regression, and two state-of-the-art algorithms, that is, Laplacian regularized least squares and support vector machines.</p><p>There are several interesting directions for extending the present work. One of the most important direction would be to automate model selection issues (such as the regularization parameters). This is especially difficult for active learning scenario, since there is no label information available at the beginning. Thus, standard model selection methods such as cross validation cannot be applied. In this work, we use -optimality criterion to measure the size of the covariance matrix of the coefficients. It remains unclear how other criteria work when graph Laplacian regularization is introduced. Moreover, our previous work has shown that active learning also can be effectively applied to image compression <ref type="bibr" target="#b37">[38]</ref>. It is interesting to explore application domains where our proposed algorithm can yield good results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Data selection by active learning algorithms. The "stars" denote the points selected by A-optimal design and LapRDD. Clearly, the points selected by LapRDD can better represent the original data set. Note that, the SVM algorithm cannot be applied in this case due to the lack of labeled points. (a) Data set; (b) AOD; (c) LapRDD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Sample images from categories (a) eagle, (b) waterfall, and (c) kungfu.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Average precision-scope curves of different algorithms for the first two feedback iterations. The LapRDD algorithm performs the best on the entire scope. Note that, at the first round of feedback, the SVM Active Laerning algorithm cannot be applied. It uses the ordinary SVM to build the initial classifier. (a) Feedback iteration 1; (b) feedback iteration 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Performance evaluation of the five learning algorithms for relevance feedback image retrieval. (a) Precision at top 10. (b) Precision at top 20. (c) Precision at top 30. As can be seen, our LapRDD algorithm consistently outperforms the other four algorithms. (a) Precision at top 10; (b) precision at top 20; (c) precision at top 30.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I PRECISION</head><label>I</label><figDesc>AT TOP 20 RETURNS OF THE FOUR ALGORITHMS AFTER THE FIRST FEEDBACK ITERATION</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science Foundation of China under Grant 60875044, in part by the National Key Basic Research Foundation of China under Grant 2009CB320801, and in part by the Program for Changjiang Scholars and Innovative Research Team in University (IRT0652, PCSIRT). The associate editor coordinating the review of this manuscript and approving it for publication was Prof. Sabine Susstrunk.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Active learning with statistical models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="129" to="145" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Donev</surname></persName>
		</author>
		<title level="m">Optimum Experimental Designs</title>
		<meeting><address><addrLine>Oxford, U.K.</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford Univ. Press</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Support vector machine active learning for image retrieval</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th ACM Int. Conf. Multimedia</title>
		<meeting>9th ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="107" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multimodal concept-dependent active learning for image retrieval</title>
		<author>
			<persName><forename type="first">K.-S</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conf. Multimedia</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-10">Oct. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Less is more: Active learning with support vector machines</title>
		<author>
			<persName><forename type="first">G</forename><surname>Schohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th Int. Conf. Machine Learning</title>
		<meeting><address><addrLine>Stanford, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Semi-Supervised Learning</title>
		<editor>O. Chapelle, B. Schölkopf, and A. Zien</editor>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Beyond the point cloud: From transductive to semi-supervised learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Machine Learning</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-supervised discriminant analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Computer Vision</title>
		<meeting><address><addrLine>Rio de Janeriro, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning a maximum margin subspace for image retrieval</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008-02">Feb. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">Statistical Learning Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th Annu. Conf. Computational Learning Theory</title>
		<meeting><address><addrLine>Madison, WI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Transductive inference for text classification using support vector machines</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th Int. Conf. Machine Learning</title>
		<meeting><address><addrLine>Bled, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised support vector machines</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Demiriz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convex methods for transduction</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">presented at the Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from examples</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Relevance feedback: A power tool for interative content-based image retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mehrotra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A unified log-based relevance feedback scheme for image retrieval</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="509" to="524" />
			<date type="published" when="2006-04">Apr. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semantic manifold learning for image retrieval</title>
		<author>
			<persName><forename type="first">Y.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conf. Multimedia</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-11">Nov. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Asymmetric bagging and random subspace for support vector machines-based relevance feedback in image retrieval</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1088" to="1099" />
			<date type="published" when="2006-07">Jul. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kernel direct biased discriminant analysis: A new content-based image retrieval relevance feedback algorithm</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="716" to="727" />
			<date type="published" when="2006-04">Apr. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Negative samples analysis in relevance feedback</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="568" to="580" />
			<date type="published" when="2007-04">Apr. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Regularized regression on image manifold for retrieval</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMM Int. Workshop Multimedia Information Retrieval</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<pubPlace>Augsburg, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spectral regression: A unified subspace learning framework for content-based image retrieval</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th ACM Int. Conf. Multimedia</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Laplacian optimal design for image retrieval</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int Conf. Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-07">Jul. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robust design of biological experiments</title>
		<author>
			<persName><forename type="first">P</forename><surname>Flaherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Arkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Active learning via transductive experimental design</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd Int. Conf. Machine Learning</title>
		<meeting><address><addrLine>Pittsburgh, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R K</forename><surname>Chung</surname></persName>
		</author>
		<title level="m">Spectral Graph Theory, ser. Regional Conference Series in Mathematics</title>
		<meeting><address><addrLine>Providence, RI</addrLine></address></meeting>
		<imprint>
			<publisher>AMS</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">92</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Matrix Algebra From a Statistician&apos;s Perspective</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Harville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adjustment of an inverse matrix corresponding to a change in one element of a given matrix</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sherman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Morrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Statist</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="124" to="127" />
			<date type="published" when="1950">1950</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning With Kernels</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Color indexing</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="32" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Texture features and learning similarity</title>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="425" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Modified fourier descriptors for shape representation-A practical approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st Int. Workshop Image Databases and Multi Media Search</title>
		<meeting>1st Int. Workshop Image Databases and Multi Media Search</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Color texture moments for content-based image retrieval</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Processing</title>
		<meeting>Int. Conf. Image essing</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="24" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Relevance feedback: A power tool in interactive content-based image retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mehrotra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="644" to="655" />
			<date type="published" when="1998-05">May 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">How to complete performance graphs in content-based image retrieval: Add generality and normalize scope</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Huijsmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="245" to="251" />
			<date type="published" when="2005-02">Feb. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A unified active and semi-supervised learning framework for image compression</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2000, and the Ph.D. degree in computer science from the University of Chicago</title>
		<meeting><address><addrLine>Miami, FL; China; Chicago, Illinois; Burbank, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2009. 2005</date>
		</imprint>
		<respStmt>
			<orgName>from Zhejiang University ; Zhejiang University. Prior to joining Zhejiang University</orgName>
		</respStmt>
	</monogr>
	<note>Yahoo! Research Labs. information retrieval, and computer vision</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
