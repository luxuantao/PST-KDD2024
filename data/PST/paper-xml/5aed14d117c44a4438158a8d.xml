<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Deep Learning Approach for Multimodal Deception Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Gangeshwar</forename><surname>Krishnamurthy</surname></persName>
							<email>gangeshwark@ihpc.a-star.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">A*STAR Artificial Intelligence Initiative (A*AI)</orgName>
								<orgName type="institution">Institute of High Performance Computing</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
							<email>navonil@sentic.net</email>
							<affiliation key="aff1">
								<orgName type="department">Centro de Investigacin en Computacin</orgName>
								<orgName type="institution">IPN</orgName>
								<address>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
							<email>sporia@ntu.edu.sg</email>
							<affiliation key="aff2">
								<orgName type="department">Temasek Laboratories</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
							<email>cambria@ntu.edu.sg</email>
							<affiliation key="aff3">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Deep Learning Approach for Multimodal Deception Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic deception detection is an important task that has gained momentum in computational linguistics due to its potential applications. In this paper, we propose a simple yet tough to beat multi-modal neural model for deception detection. By combining features from different modalities such as video, audio, and text along with Micro-Expression features, we show that detecting deception in real life videos can be more accurate. Experimental results on a dataset of real-life deception videos show that our model outperforms existing techniques for deception detection with an accuracy of 96.14% and ROC-AUC of 0.9799.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We face deceptive behavior in our day-to-day life. People lie to escape from a situation that seems unfavorable to them. As a consequence, some lies are innocuous but others may have severe ramifications in the society. Reports suggest that the ability of humans to detect deception without special aids is only 54% <ref type="bibr" target="#b0">[1]</ref>. A study by DePaulo et al. <ref type="bibr" target="#b1">[2]</ref> found that deception without any particular motivation or intention exhibited almost no detectable cues of deception. However, cues were significantly more when lies were about transgressions.</p><p>With the rise in the number of criminal cases filed every year in the US <ref type="foot" target="#foot_0">5</ref> , it is ethically and morally important to accuse only the guilty defendant and free the innocent. Since the judgment for any case is mostly based on the hearings and evidence from the stakeholders (accused, witnesses, etc.), the judgment is most likely to go wrong if the stakeholders do not speak the truth. It is, hence, important to detect deceptive behavior accurately in order to upkeep the law and order.</p><p>Social media can be characterized as a virtual world where people interact with each other without the human feel and touch. It is easy to not reveal one's identity and/or pretend to be someone else on the social media. Cyberbullying is increasingly becoming a common problem amongst the teenagers nowadays <ref type="bibr" target="#b2">[3]</ref>. These include spreading rumors about a person, threats, and sexual harassment. Cyberbullying adversely affects the victim and leads to a variety of emotional responses such as lowered self-esteem, increased suicidal thoughts, anger, and depression <ref type="bibr" target="#b3">[4]</ref>. Teenagers fall prey to these attacks due to their inability to comprehend the chicanery and pretentious behavior of the attacker.</p><p>Another area where deception detection is of paramount importance is with the increased number of false stories, a.k.a Fake News, on the Internet. Recent reports suggest that the outcome of the U.S. Presidential Elections is due to the rise of online fake news. Propagandists use arguments that, while sometimes convincing, are not necessarily valid. Social media, such as Facebook and Twitter, have become the propellers for this political propaganda. Countries around the world, such as France <ref type="bibr" target="#b4">[5]</ref>, are employing methods that would prevent the spread of fake news during their elections. Though these measures might help, there is a pressing need for the computational linguistics community to devise efficient methods to fight Fake News given that humans are poor at detecting deception.</p><p>This paper is organized as follows. In section 2, we will talk about the past work in deception detection; section 3 describes our approach to solving deception detection. Section 4 explains our experimental setup. In section 5 and 6, we discuss our results and drawbacks respectively. And finally, conclude with future work in section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Past research in the detection of deception can be broadly classified as Verbal and Non-verbal. In verbal deception detection, the features are based on the linguistic characteristics, such as n-grams and sentence count statistics <ref type="bibr" target="#b5">[6]</ref>, of the statement by the subject under consideration. Use of more complex features such as psycholinguistic features <ref type="bibr" target="#b6">[7]</ref> based on the Linguistic Inquiry and Word Count (LIWC) lexicon, have also been explored by <ref type="bibr" target="#b5">[6]</ref> and shown that they are helpful in detecting deceptive behavior. Yancheva and Rudzicz studied the relation between the syntactic complexity of text and deceptive behavior <ref type="bibr" target="#b7">[8]</ref>. 1 In non-verbal deception detection, physiological measures were the main source of signals for detecting deceptive behavior. Polygraph tests measure physiological features such as heart rate, respiration rate, skin temperature of the subject under investigation. But these tests are not reliable and often misleading as indicated by <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> since judgment made by humans are often biased. Facial expressions and hand gestures were found to be very helpful in detecting deceptive nature. Ekman <ref type="bibr" target="#b10">[11]</ref> defined micro-expressions as short involuntary expressions, which could potentially indicate deceptive behavior. Caso et al. <ref type="bibr" target="#b11">[12]</ref> identified particular hand gesture to be important to identify the act of deception. Cohen et al. <ref type="bibr" target="#b12">[13]</ref> found that fewer iconic hand gestures were a sign of a deceptive narration.</p><p>Previous research was focused on detecting deceit behavior under constrained environments which may not be applicable in real life surroundings. Recently, the focus has been towards experiments in real life scenarios. Towards this, Pérez-Rosas et al. <ref type="bibr" target="#b13">[14]</ref> introduced a new multi-modal deception dataset having real-life videos of courtroom trials. They demonstrated the use of features from different modalities and the importance of each modality in detecting deception. They also evaluated the performance of humans in deception detection and compared it with their machine learning models. Wu et al. <ref type="bibr" target="#b14">[15]</ref> have developed methods that leverage multi-modal features for detecting detection. Their method heavily emphasizes on feature engineering along with manual cropping and annotating videos for feature extraction.</p><p>In this paper, we describe our attempt to use neural models that uses features from multiple modalities for detecting deception. We believe our work is the first attempt at using neural networks for deceit classification. We show that with the right features and simple models, we can detect deceptive nature in real life trial videos more accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-Modal Feature Extraction</head><p>The first stage is to extract unimodal features from each video. We extract textual, audio and visual features as described below.</p><p>Visual Feature Extraction For extracting visual features from the videos, we use 3D-CNN <ref type="bibr" target="#b15">[16]</ref>. 3D-CNN has achieved state-of-the-art results in object classification on tridimensional data <ref type="bibr" target="#b15">[16]</ref>. 3D-CNN not only extracts features from each image frame, but also extracts spatiotemporal features <ref type="bibr" target="#b16">[17]</ref> from the whole video which helps in identifying the facial expressions such as smile, fear, or stress.</p><p>The input to 3D-CNN is a video v of dimension (c, f, h, w), where c represents the number of channels and f, h, w are the number of frames, height, and width of each frames respectively. A 3D convolutional filter, f l of dimension (f m , c, f d , f h , f w ) is applied, where f m = number of feature maps, c = number of channels, f d = number of frames (also called depth of the filter), f h = height of the filter, and f w = width of the filter. This filter, f l , produces an output,</p><formula xml:id="formula_0">convout of dimension (f m , c, f − f d + 1, h − f h + 1, w − f w + 1)</formula><p>after sliding across the input video, v. Max pooling is applied to convout with window size being (m p , m p , m p ). Subsequently, this output is fed to a dense layer of size d f and softmax. The activations of this dense layer is used as the visual feature representation of the input video, v.</p><p>In our experiments, we consider only RBG channel images, hence c = 3. We use 32 feature maps and 3D filters of size, f d = f h = f w = 5. Hence, the dimension of the filter, f l , is 32 × 3 × 5 × 5 × 5 . The window size, m p , of the max pooling layer is 3. Finally, we obtain a feature vector, v f , of dimension 300 for an input video, v.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Textual Features Extraction</head><p>We use Convolutional Neural Networks (CNN) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> to extract features from the transcript of a video, v. First, we use pretrained Word2Vec <ref type="bibr" target="#b19">[20]</ref> model to extract the vector representations for every word in the transcript. These vectors are concatenated and fed as input vector to the CNN. We use a simple CNN with one convolutional layer and a maxpooling layer, to get our sentence representation. In our experiments, filters of size 3, 5 and 8 with 20 feature maps each is used. Window-size of 2 is employed for max-pooling over these feature maps. Subsequently, a full-connected layer with 300 neurons is used with rectified linear unit (ReLU) <ref type="bibr" target="#b20">[21]</ref> as the activation function. The activations of this full-connected layer is used as the textual feature representation of the input video, v. Finally, we obtain a feature vector, t f , of dimension 300 for an input text (transcript), t.</p><p>Audio Feature Extraction openSMILE <ref type="bibr" target="#b21">[22]</ref> is an open-source toolkit used to extract high dimensional features from an audio file. In this work, we use openS-MILE to extract features from the input audio. Before extracting the features, we make sure that there are no unnecessary signals in the audio that affects the quality of the extracted features. Hence, the background noise is removed from the audio and Z-standardization is used to perform voice normalization. To remove the background noise, we use SoX (Sound eXchange) <ref type="bibr" target="#b22">[23]</ref> audio processing tool. The noiseless input audio is then fed to the openSMILE tool to extract high-dimensional features. These features are functions of low-level descriptor (LLD) contours. Specifically, we use the IS13-ComParE openSMILE configuration to extract features which are of dimension 6373 for every input audio, a.</p><p>After these features are extracted, a simple fully-connected neural network is trained to reduce the dimension to 300. Finally, we obtain a feature vector, a f , of dimension 300 for an input audio, a.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Micro-Expression Features Veronica et al. manually annotated facial ex-</head><p>pressions and use binary features derived from the ground truth annotations to predict deceptive behavior. Facial micro-expressions are also considered to play an important role in detecting deceptive behavior. The data provided by <ref type="bibr" target="#b13">[14]</ref> contains 39 facial micro-expressions such as frowning, smiling, eyebrows raising, etc. These are binary features and taken as a feature vector, m p of dimension 39. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Description</head><p>Multi-modal Model We use a simple Multi-Layer perceptron (MLP) (cite) with hidden layer of size 1024 followed by a linear output layer. We use the rectified linear unit (ReLU) activation function <ref type="bibr" target="#b20">[21]</ref> for non-linearity at the hidden layer. A dropout <ref type="bibr" target="#b23">[24]</ref> of keep probability, p = 0.5, is applied to the hidden layer for regularization.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> and 2 shows the architecture of our models, M LP C and M LP H+C .</p><p>Unimodal Models We perform the evaluation on individual modalities and use the same architecture as multi-modal model. The only difference is that the input is either v f , or a f , or t f , or m f . Hence no data fusion is performed. We name the model as M LP U .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Data Fusion</head><p>We fuse the features from individual modalities to map them into a joint space.</p><p>To achieve this, we try different kinds of data fusion techniques:</p><p>Concatenation In this method, the features from all the modalities are simply concatenated into a single feature vector. Thus, the extracted features, t f , a f , v f </p><formula xml:id="formula_1">z f = [t f ; a f ; v f ; m f ] of dimension d in = 939.</formula><p>We call this model configuration as M LP C and is shown in the figure <ref type="figure" target="#fig_0">1</ref>.</p><p>Hadamard + Concatenation In this method, the audio features, visual features and, textual features are fusion by using hadamard product. Then the Micro-Expression features are concatenated with the product. Thus, we have</p><formula xml:id="formula_2">z f = [t f a f v f ; m f ] of dimension d in = 339</formula><p>, where (A B) is element-wise multiplication between matrices A and B (also known as Hadamard product).</p><p>We call this model configurtion as M LP H+C and is shown in the figure <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Loss</head><p>The training objective is to minimise the cross-entropy between the model's output and the true labels. We trained the models with back-propagation using Stochastic Gradient Descent optimizer. The loss function is:</p><formula xml:id="formula_3">J = −1 N N i=1 C j=1 y i,j log 2 (ŷ i,j )<label>(1)</label></formula><p>Here, N is the number of samples, C is the number of categories (in our case, C = 2). y i is the one-hot vector ground truth of i th sample and ŷi,j is its predicted probability of belonging to class j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>For evaluating our deception detection model, we use a real-life deception detection dataset by <ref type="bibr" target="#b13">[14]</ref>. This dataset contains 121 video clips of courtroom trials.</p><p>Out of these 121 videos, 61 of them are of deceptive nature while the remaining 60 are of truthful nature. The dataset contains multiple videos from one subject. In order to avoid bleeding of personalities between train and test set, we perform a 10-fold cross validation with subjects instead of videos as suggested by Wu et al. <ref type="bibr" target="#b14">[15]</ref>. This ensures that videos of the same subjects are not in both training and test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>Wu et al. <ref type="bibr" target="#b14">[15]</ref> have made use of various classifiers such as Logistic Regression (LR), Linear SVM (L-SVM), Kernel SVM (K-SVM). They report the AUC-ROC values obtained by the classifiers for different combination of modalities. We compare the AUC obtained by our models against only Linear SVM (L-SVM) and Logistic Regression (LR).</p><p>Pérez-Rosas et al. <ref type="bibr" target="#b13">[14]</ref> use Decision Trees (DT) and Random Forest (RF) as their classifiers. We compare the accuracies of our models against DT and RF. Tables <ref type="table" target="#tab_1">1 &amp; 2</ref> presents the performances of M LP and its variants along with the state-of-the-art models. During feature extraction from text, we train the TextCNN model with two different settings: one, by keeping the word vector representation static; two, by optimizing the vector along with the training (nonstatic). In our results, we also show the performance of the model with these two textual features separately. Additionally, we also mention the results we got from our models for feature vectors initialized with random numbers.</p><p>Table <ref type="table" target="#tab_0">1</ref> shows that our model, M LP H+C , obtains an AUC of 0.9799 while outperforming all other competitive baselines with a huge margin. Table <ref type="table" target="#tab_1">2</ref> compares the performance our models with Decision Tree and Linear Regression models <ref type="bibr" target="#b13">[14]</ref>. Our model, M LP H+C , again outperforms other baselines by achieving an accuracy of 96.14%. We can also infer that visual and textual features play a major role in the performance of our models; followed by Micro-Expressions and audio. This conforms with the findings by <ref type="bibr" target="#b13">[14]</ref> that facial display features and unigrams contribute the most to detecting deception.</p><p>As we can see that, our approach outperforms the baselines by a huge margin. Our neural models simple and straightforward, hence the results show that right feature extraction methods can help in unveiling significant signals that are useful for detecting deceptive nature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Drawbacks</head><p>Though our models outperform the previous state-of-the-art models, we still acknowledge the drawbacks of our approach as follows:</p><p>-Our models still rely on a small dataset with only 121 videos. Due to this, our models are prone to over-fitting if not carefully trained with proper regularization.</p><p>-Also, due to the limited scenarios in the dataset, the model may not show the same performance for out-of-domain scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future Work</head><p>In this paper, we presented a system to detect deceptive nature from videos. Surprisingly, our model performed well even with only 121 videos provided in the dataset, which is generally not a feasible number of data points for neural models. As a result, we conclude that there exists a certain pattern in the videos that provide highly important signals for such precise classification. We performed various other evaluations not presented in this paper, to confirm the performance of our model. From these experiments, we observed that visual and textual features predominantly contributed to accurate predictions followed by Micro-Expression features. Empirically, we observed that our model M LP H+C converged faster in comparison with M LP C .</p><p>While our system performs well on the dataset by <ref type="bibr" target="#b13">[14]</ref>, we can not claim the same performance of our model for larger datasets covering a larger number of environments into consideration. Hence, creating a large multi-modal dataset with a large number of subjects under various environmental condition is part of our future work. This would pave a way to build more robust and efficient learning systems for deception detection. Another interesting path to explore is detecting deception under social dyadic conversational setting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Architecture of model M LPC</figDesc><graphic url="image-1.png" coords="5,134.77,115.83,345.83,241.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Architecture of model M LPH+C</figDesc><graphic url="image-2.png" coords="6,134.77,115.83,345.83,247.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparision of AUC</figDesc><table><row><cell>Features</cell><cell cols="5">L-SVM [15] LR [15] M LPU M LPC M LPH+C</cell></row><row><cell>Random</cell><cell>-</cell><cell>-</cell><cell cols="3">0.4577 0.4788 0.4989</cell></row><row><cell>Audio</cell><cell>0.7694</cell><cell cols="2">0.6683 0.5231</cell><cell>-</cell><cell>-</cell></row><row><cell>Visual</cell><cell>0.7731</cell><cell cols="2">0.6425 0.9596</cell><cell>-</cell><cell>-</cell></row><row><cell>Textual (Static)</cell><cell>0.6457</cell><cell cols="2">0.5643 0.8231</cell><cell>-</cell><cell>-</cell></row><row><cell>Textual (Non-static)</cell><cell>-</cell><cell>-</cell><cell>0.9455</cell><cell>-</cell><cell>-</cell></row><row><cell>Micro-Expression</cell><cell>0.7964</cell><cell cols="2">0.8275 0.7512</cell><cell>-</cell><cell>-</cell></row><row><cell>All Features (Static)</cell><cell>0.9065</cell><cell>0.9221</cell><cell>-</cell><cell cols="2">0.9033 0.9348</cell></row><row><cell>All Features (Non-static)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">0.9538 0.9799</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparing accuracies of our model with baselines</figDesc><table><row><cell>Features</cell><cell cols="5">DT [14] RF [14] M LPU M LPC M LPH+C</cell></row><row><cell>Random</cell><cell>-</cell><cell>-</cell><cell cols="3">43.90% 45.32% 48.51%</cell></row><row><cell>Audio</cell><cell>-</cell><cell>-</cell><cell>52.38%</cell><cell>-</cell><cell>-</cell></row><row><cell>Visual</cell><cell>-</cell><cell>-</cell><cell>93.08%</cell><cell>-</cell><cell>-</cell></row><row><cell>Textual (Static)</cell><cell cols="3">60.33% 50.41% 80.16%</cell><cell>-</cell><cell>-</cell></row><row><cell>Textual (Non-static)</cell><cell>-</cell><cell>-</cell><cell>90.24%</cell><cell>-</cell><cell>-</cell></row><row><cell>Micro-Expression</cell><cell cols="3">68.59% 73.55% 76.19%</cell><cell>-</cell><cell>-</cell></row><row><cell>All Features (Static)</cell><cell cols="2">75.20% 50.41%</cell><cell>-</cell><cell cols="2">90.49% 90.99%</cell></row><row><cell>All Features (Non-static)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">95.24% 96.14%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_0">http://www.uscourts.gov</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Accuracy of deception judgments</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Bond</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Depaulo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personality and Social Psychology Review</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="214" to="234" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cues to deception</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Depaulo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Lindsay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Malone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Muhlenbruck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Charlton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page">74</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cyberbullying: its nature and impact in secondary school pupils</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tippett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Child Psychology and Psychiatry</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="376" to="385" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Bullying beyond the schoolyard: Preventing and responding to cyberbullying</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hinduja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Patchin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Corwin Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">France to impose restrictions on facebook, twitter in fight against fake news during elections</title>
		<author>
			<persName><forename type="first">J</forename><surname>Baptiste Su</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-01">2018. January-2018</date>
		</imprint>
	</monogr>
	<note>Online; posted 09</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Linguistic ethnography: Identifying dominant word classes in text</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Pulman</surname></persName>
		</author>
		<editor>CICLing, Springer</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="594" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Linguistic inquiry and word count: Liwc</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Pennebaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Booth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mahway: Lawrence Erlbaum Associates</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page">2001</biblScope>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic detection of deception in child-produced speech using syntactic complexity features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yancheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rudzicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="944" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Detecting lies and deceit: The psychology of lying and implications for professional practice</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vrij</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Risk assessment and the polygraph. The use of the polygraph in assessing, treating and supervising sex offenders: A practitioner&apos;s guide</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Gannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Beech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="129" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<title level="m">Telling lies: Clues to deceit in the marketplace, politics, and marriage</title>
				<imprint>
			<publisher>WW Norton &amp; Company</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>revised edition</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The impact of deception and suspicion on different hand movements</title>
		<author>
			<persName><forename type="first">L</forename><surname>Caso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Maricchiolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bonaiuto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vrij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Nonverbal behavior</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Nonverbal indicators of deception: How iconic gestures reveal thoughts that cannot be suppressed</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shovelton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Semiotica</title>
		<imprint>
			<biblScope unit="page" from="133" to="174" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deception detection using real-life trial data</title>
		<author>
			<persName><forename type="first">V</forename><surname>Pérez-Rosas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abouelenien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM on International Conference on Multimodal Interaction</title>
				<meeting>the 2015 ACM on International Conference on Multimodal Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Subrahmanian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04415</idno>
		<title level="m">Deception detection in videos</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<title level="m">Convolutional neural networks for sentence classification</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.2188</idno>
		<title level="m">A convolutional neural network for modelling sentences</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning</title>
				<meeting>the 27th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recent developments in opensmile, the munich open-source multimedia feature extractor</title>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM International Conference on Multimedia. MM &apos;13</title>
				<meeting>the 21st ACM International Conference on Multimedia. MM &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="835" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Sound exchange</title>
		<author>
			<persName><forename type="first">L</forename><surname>Norskog</surname></persName>
		</author>
		<ptr target="http://sox.sourceforge.net/" />
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
