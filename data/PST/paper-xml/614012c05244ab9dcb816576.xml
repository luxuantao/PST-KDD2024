<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Extracting Event Temporal Relations via Hyperbolic Geometry</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xingwei</forename><surname>Tan</surname></persName>
							<email>xingwei.tan@warwick.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gabriele</forename><surname>Pergola</surname></persName>
							<email>gabriele.pergola@warwick.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yulan</forename><surname>He</surname></persName>
							<email>yulan.he@warwick.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Alan Turing Institute</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Extracting Event Temporal Relations via Hyperbolic Geometry</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Detecting events and their evolution through time is a crucial task in natural language understanding. Recent neural approaches to event temporal relation extraction typically map events to embeddings in the Euclidean space and train a classifier to detect temporal relations between event pairs. However, embeddings in the Euclidean space cannot capture richer asymmetric relations such as event temporal relations. We thus propose to embed events into hyperbolic spaces, which are intrinsically oriented at modeling hierarchical structures. We introduce two approaches to encode events and their temporal relations in hyperbolic spaces. One approach leverages hyperbolic embeddings to directly infer event relations through simple geometrical operations. In the second one, we devise an end-to-end architecture composed of hyperbolic neural units tailored for the temporal relation extraction task. Thorough experimental assessments on widely used datasets have shown the benefits of revisiting the tasks on a different geometrical space, resulting in state-of-the-art performance on several standard metrics. Finally, the ablation study and several qualitative analyses highlighted the rich event semantics implicitly encoded into hyperbolic spaces. 1  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Successful understanding of natural language depends, among other factors, on the capability to accurately detect events and their evolution through time. This has recently led to increasing interest in research for temporal relation extraction <ref type="bibr" target="#b3">(Chambers et al., 2014;</ref><ref type="bibr" target="#b37">Wang et al., 2020)</ref> with the aim of understanding events and their temporal orders. Temporal reasoning has been proven beneficial, for example, in understanding narratives <ref type="bibr" target="#b5">(Cheng et al., 2013)</ref>, answering questions <ref type="bibr" target="#b22">(Ning et al., 2020)</ref>, or summarizing events <ref type="bibr" target="#b36">(Wang et al., 2018)</ref>. West German and French authorities have (e3: cleared) Dresdner Bank AG's takeover of a majority stake in Banque Internationale de Placement. The approval, which had been (e2: expected), (e4:permits) West Germany's second-largest bank to acquire shares of the French investment bank. Dresdner Bank (e0:said) it will (e5: buy) all shares (e1: tendered) by shareholders on the Paris Stock Exchange at the same price from today through Nov. 17. However, events that occurred in text are not just simple and standalone predicates, they rather form complex and hierarchical structures with different granularity levels (Fig. <ref type="figure" target="#fig_1">1</ref>), a characteristic that still challenges existing models and restricts their performance on real-world datasets for temporal relation extraction <ref type="bibr">(Ning et al., 2018a,c)</ref>. Addressing such challenges requires models to not only recognize accurately the events and their hierarchical and chronological properties but also encode them in appropriate representations enabling effective temporal reasoning. Although this has prompted the recent development of neural architectures for automatic feature extraction <ref type="bibr" target="#b21">(Ning et al., 2019;</ref><ref type="bibr" target="#b37">Wang et al., 2020;</ref><ref type="bibr" target="#b10">Han et al., 2019b)</ref>, which achieves better generalization and avoids costly design of statistical methods leveraging hand-crafted features <ref type="bibr" target="#b17">(Mani et al., 2006;</ref><ref type="bibr" target="#b4">Chambers et al., 2007;</ref><ref type="bibr" target="#b35">Verhagen and Pustejovsky, 2008)</ref>, the inherent complexity of temporal relations still hinders approaches that just rely on the scarce availability of annotated data. Some of the intrinsic limitations of the mentioned approaches are due to the adopted embedding space. Existing approaches to temporal relation extraction typically operate in the Euclidean arXiv:2109.05527v1 [cs.CL] 12 Sep 2021 space, in which an event embedding is represented as a point. Although such embeddings exhibit a linear algebraic structure which captures cooccurrence patterns among events, they are not able to reveal richer asymmetric relations, such as event temporal order (e.g., 'event A happens before event B' but not vice versa). Inspired by recent works on learning non-Euclidean embeddings, such as Poincarè embeddings <ref type="bibr" target="#b19">(Nickel and Kiela, 2017;</ref><ref type="bibr" target="#b32">Tifrea et al., 2019)</ref> showing superior performance in capturing asymmetrical relations of objects, we propose to learn event embeddings in hyperbolic spaces <ref type="bibr" target="#b7">(Ganea et al., 2018b)</ref>.</p><p>Hyperbolic spaces can be viewed as continuous versions of trees, thus naturally oriented to encode hierarchical and asymmetrical structures. For instance, <ref type="bibr" target="#b28">Sala et al. (2018)</ref> showed that hyperbolic spaces with just two dimensions (i.e., Poincaré disk) could easily embed tree structures with arbitrarily low distortion <ref type="bibr" target="#b29">(Sarkar, 2011)</ref>, while Euclidean spaces cannot achieve any comparable distortion even with an unbounded number of dimensions <ref type="bibr" target="#b14">(Linial et al., 1994)</ref>. Despite the hierarchical properties arising in modeling event relations, there are still very few studies on how to leverage those models for temporal event extraction. We propose a framework for temporal relation (TempRel) extraction based on the Poincaré ball model, a hyperbolic space well-suited for the efficient computation of embeddings based on the Riemannian optimization.</p><p>Our contributions can be summarized as follows:</p><p>• We propose an embedding learning approach with a novel angular loss to encode events onto hyperbolic spaces, which pairs with a simple rule-based classifier to detect event temporal relations. With only 1.5k parameters and trained in about 4 minutes, it achieves results on-par with far more complex models in the recent literature. • Apart from directly learning hyperbolic event embeddings for TempRel extraction, we propose an alternative end-to-end hyperbolic neural architecture to model events and their temporal relations in hyperbolic spaces. • We conduct a thorough experimental assessment on widely used datasets, with ablation studies and qualitative analyses demonstrating the benefits of tackling the TempRel extraction task in hyperbolic spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our work is related to at least two lines of research: one about event temporal relation extraction and another on hyperbolic neural models.</p><p>Event TempRel Extraction Approaches to Tem-pRel extraction are largely built on neural models in recent years. These models have been proven capable of extracting automatically reliable event features for TempRel extraction when provided with high-quality data <ref type="bibr" target="#b21">(Ning et al., 2019)</ref>, alleviating significantly the required human-engineer effort and yielding results outperforming the above mentioned methodologies. In particular, <ref type="bibr" target="#b21">Ning et al. (2019)</ref> employed an LSTM network <ref type="bibr" target="#b11">(Hochreiter and Schmidhuber, 1997)</ref> to encode the textual events, taking into account their global context and feeding their representations into a multi-layer perceptron for TempRel classification. In addition, to enhance the generalization to unseen event-tuples, they simultaneously trained a Siamese network bridging common-sense knowledge across event relations. Similarly, <ref type="bibr" target="#b9">Han et al. (2019a)</ref> combined a bidirectional LSTM (BiLSTM) with a structured support vector machine (SSVM), with the BiLSTM extracting the pair of events and the SSVM incorporating structural linguistic constraints across them<ref type="foot" target="#foot_0">2</ref> . <ref type="bibr" target="#b37">Wang et al. (2020)</ref> proposed a constrained learning framework, where event pairs are encoded via a BiLSTM, enhanced with common-sense knowledge from ConceptNet <ref type="bibr" target="#b30">(Speer et al., 2017)</ref> and TEMPROB <ref type="bibr" target="#b23">(Ning et al., 2018b)</ref>, while enforcing a set of logical constraints at training time. The aim is to train the model to detect and extract the event relations while regularizing towards consistency on logic converted into differentiable objective functions, similarly to what was proposed in <ref type="bibr" target="#b13">Li et al. (2019)</ref>.</p><p>Hyperbolic Neural Models The aforementioned models are all designed to process data representations in the Euclidean space. However, several studies <ref type="bibr" target="#b18">(Nickel et al., 2014;</ref><ref type="bibr" target="#b1">Bouchard et al., 2015)</ref> have shown the inherent limitations of the Euclidean space in terms of representing asymmetric relations and tree-like graphs <ref type="bibr" target="#b18">(Nickel et al., 2014;</ref><ref type="bibr" target="#b1">Bouchard et al., 2015)</ref>. Hyperbolic spaces, instead, are promising alternatives that have a natural hierarchical structure and can be thought of as continuous versions of trees. This makes them highly suitable and efficient to encode tree-like networks <ref type="bibr" target="#b19">(Nickel and Kiela, 2017;</ref><ref type="bibr" target="#b33">Tran et al., 2020)</ref>.</p><p>Previous works have explored their use in embedding taxonomy for network link prediction or modeling lexical entailment. In particular, <ref type="bibr" target="#b19">Nickel and Kiela (2017)</ref> proposed to learn word hierarchies through a negative-sampling training, based on the distance metric on the Poincaré ball. <ref type="bibr" target="#b6">Ganea et al. (2018a)</ref> generalized the idea of order embeddings <ref type="bibr" target="#b34">(Vendrov et al., 2016)</ref> to the Poincaré ball, levering the projected areas to infer data relations. <ref type="bibr" target="#b7">Ganea et al. (2018b)</ref> introduced a framework of hyperbolic neural networks composed of neural units learning and optimizing parameters in hyperbolic spaces. Their experiments show that, without increasing the number of parameters of the models, hyperbolic neural networks outperform their Euclidean counterparts on natural language inference and detection of noisy prefixes tasks. There have been a few attempts in revisiting NLP tasks in the hyperbolic space framework by generalizing hyperbolic neural activation functions for machine translation <ref type="bibr" target="#b8">(Gulcehre et al., 2018)</ref>, to detect hierarchical entity types <ref type="bibr" target="#b16">(López and Strube, 2020)</ref>, and for document classification <ref type="bibr" target="#b39">(Zhang and Gao, 2020)</ref>. Compared to the above works, our model is the first attempt in devising an end-to-end hyperbolic architecture showing the benefit of addressing event TempRel extraction in hyperbolic spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>In this section, we give a brief introduction of hyperbolic geometry and hyperbolic neural networks.</p><p>Hyperbolic geometry. A hyperbolic space is a non-Euclidean space that has the same negative sectional curvature at every point (i.e., a constant negative curvature). Intuitively, that a space has constant curvature implies that it keeps the same "curveness" at every point. An example of constant positive curvature space is a perfect globe. On the other hand, an example of an n-D hyperbolic space is a hyperboloid in a R n+1 space.</p><p>One of the widely-used models of hyperbolic space is the Poincaré model, which is an open n-</p><formula xml:id="formula_0">dimensional unit ball D n = {x ∈ R n | x &lt; 1}</formula><p>equipped with the Riemannian metric tensor:</p><formula xml:id="formula_1">g D x = (λ 2 x ) 2 g E , where λ x := 2 1 − x 2 ,<label>(1)</label></formula><p>x ∈ D n , • denotes the Euclidean norm, and g E denotes the Euclidean metric tensor. The geodesics (i.e., the shortest path between two points) on the Poincaré ball are all intersections of circles with the unit ball D n perpendicular to the boundary sphere.</p><p>Based on the metric tensor g D x , the distance of two points x, y ∈ D n is defined as <ref type="bibr" target="#b19">(Nickel and Kiela, 2017)</ref>:</p><formula xml:id="formula_2">d D (x, y) = arcosh(1 + 2 x − y 2 (1 − x 2 )(1 − y 2 )</formula><p>).</p><p>(2)</p><p>The Poincaré norm is the distance between the origin and the given point:</p><formula xml:id="formula_3">x D = d D (0, x) = 2 arctan( x ).<label>(3)</label></formula><p>Also, an angle ∠ABC in the Poincaré model can be derived as <ref type="bibr" target="#b6">(Ganea et al., 2018a)</ref>:</p><formula xml:id="formula_4">cos (∠(z 1 , z 2 )) = g D x (z 1 , z 2 ) g D x (z 1 , z 2 ) g D x (z 1 , z 2 ) ,<label>(4)</label></formula><p>where z 1 , z 2 ∈ T x D n \ {0} are the initial tangent vectors of the geodesics connecting B with A, and B with C. An important property of Poincaré model is its conformality with a Euclidean space, which means their metrics define the same angles (Eq. 4= z 1 ,z 2 z 1 z 2 , where •, • denotes Euclidean inner product).</p><p>It is known <ref type="bibr" target="#b6">(Ganea et al., 2018a</ref>) that an exponential map can be defined for each point</p><formula xml:id="formula_5">x ∈ D n to map any point z ∈ R n (= T x D n ) onto D n : exp x (z) = 1 z sinh(λx z ) 1 + (λx − 1) cosh(λx z ) + λx x, z z sinh(λx z ) z+ λx cosh(λx z ) + x, z z sinh(λx z ) 1 + (λx − 1) cosh(λx z ) + λx x, z z sinh(λx z ) x.</formula><p>(5)</p><p>Hyperbolic neural networks. In order to provide an algebraic setting for a hyperbolic space, which is not a vector space, </p><formula xml:id="formula_6">z t =σ log 0 (((W z ⊗ h t−1 ) ⊕ (U z ⊗ x t )) ⊕ b z ), r t =σ log 0 (((W r ⊗ h t−1 ) ⊕ (U r ⊗ x t )) ⊕ b r ), ht =ϕ ⊗ ((([W h diag(r t )] ⊗ h t−1 ) ⊕ (U h ⊗ x t )) ⊕ b h ), h t =h t−1 ⊕ (diag(z t ) ⊗ ((−h t−1 ) ⊕ ht )),</formula><p>where ⊗ is the Möbius product, ⊕ is the Möbius addition, ϕ ⊗ is a hyperbolic non-linearity, diag(x) is the square diagonal matrix of x, and</p><formula xml:id="formula_7">{W z , U z , b z , W r , U r , b r , W h , U h , b h } is the pa- rameter set.</formula><p>In Hyperbolic MLR, the prediction probability of a given class k ∈ 1, ..., K is computed as:</p><formula xml:id="formula_8">p(y = k|x) ∝ exp sign( −p k ⊕ x, a k ) g D p k (a k , a k )d D (x, Ha k ,p k ) ,<label>(6)</label></formula><p>where x ∈ D n is the output vector of the previous layer, p k ∈ D n and a k ∈ T p k D n \ {0} are parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Event Temporal Relation Extraction in the Hyperbolic Space</head><p>In this section, we propose two approaches to leverage a hyperbolic space for TempRel extraction. The first approach learns event embeddings that encode temporal order via a hyperbolic space; while the second one is an end-to-end hyperbolic neural network tailored for the TempRel extraction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Hyperbolic Event Embedding Learning</head><p>We first explore how to learn embeddings of events in a hyperbolic space while preserving their temporal orders. Temporal relations are asymmetric and transitive, exhibiting similar properties to hierarchical relations. Inspired by previous successes of hyperbolic embeddings for word hierarchies <ref type="bibr" target="#b19">(Nickel and Kiela, 2017;</ref><ref type="bibr" target="#b6">Ganea et al., 2018a)</ref>, we propose to learn event embeddings based on the Poincaré model to capture their temporal relations. For a given text sequence containing an event pair (u, v), we first extract the contextualized embeddings of the event tokens<ref type="foot" target="#foot_1">3</ref> , e u and e v , from their, for example, ELMo <ref type="bibr" target="#b27">(Peters et al., 2018)</ref> or RoBERTa <ref type="bibr" target="#b15">(Liu et al., 2019)</ref> sequence encodings, and use the exponential mapping function to map them onto a Poincaré ball. The embeddings are then further projected to a lower-dimensional space through a hyperbolic feed-forward layer:</p><formula xml:id="formula_9">s u = HFFL exp 0 (e u ) s v = HFFL exp 0 (e v ) (7)</formula><p>where exp 0 (•) is the exponential map at the origin of a Poincaré ball as defined by Eq. ( <ref type="formula">5</ref>), HFFL is a hyperbolic feed-forward layer, s u and s v are the final Poincaré embeddings of event u and v.</p><p>To encode temporal connections in the embeddings, we want to pull events that have temporal connections close to each other, while pushing events that have no temporal relations far apart. Thus, inspired by Poincaré embeddings <ref type="bibr" target="#b19">(Nickel and Kiela, 2017)</ref>, we define the first loss term:</p><formula xml:id="formula_10">L 1 = (u,v)∈D log e −dD(su,sv) v ∈N (u) e −dD(su,s v ) , (<label>8</label></formula><formula xml:id="formula_11">)</formula><p>where D is the set of event pairs that have temporal connections, N (u) is the set of events that have no temporal relations with the event u; s u and s v denote the Poincaré embedding of event u and v, respectively. For example, in the MATRES dataset, event pairs are annotated with one of the following four relations: BEFORE, AFTER, EQUAL and VAGUE. We can consider the first three relations as temporal connections, and regard VAGUE as no relation <ref type="bibr" target="#b21">(Ning et al., 2019)</ref>. Therefore, the set D contains event pairs in the training set that have BEFORE, AFTER, or EQUAL labels. The set N (u) contains the events that are in the same documents with u, but cannot reach u using only BEFORE, AFTER, or EQUAL edge. It is worth noting that because we do not encode the relation type explicitly, the order of input event pair (u, v) matters. We explicitly model the BEFORE relation in the event pair (u, v) (i.e., u happens earlier than v). For event pairs with the AFTER relation, we simply swap the events since AFTER and BEFORE are reciprocal.</p><p>In addition to the first loss term, we introduce a second novel loss term to enforce an angular property. As the example shown in Figure <ref type="figure" target="#fig_2">2</ref>, we want to make ∠θ 1 of a positive event pair (u, v) smaller. Based on preliminary tests, the angular loss can further enforce the first loss term which is driving the norm of u to be larger than the norm of v and thus increases the performance. Moreover, this angular property can help to distinguish VAGUE pairs by using a threshold on the ∠θ 2 to determine whether to assign the event pair to the VAGUE label. We define the second loss term as the degree of ∠θ 1 , which is known to be equal to <ref type="bibr" target="#b6">(Ganea et al., 2018a)</ref>:</p><formula xml:id="formula_12">L2 = (u,v)∈D arccos su, sv (1 + sv 2 ) − sv 2 (1 + su 2 ) sv • su − sv 1 + su 2 sv 2 − 2 su, sv .<label>(9)</label></formula><p>Then, we train an HFFL based on the following objective function:</p><formula xml:id="formula_13">L = αL 1 + (1 − α)L 2 , (<label>10</label></formula><formula xml:id="formula_14">)</formula><p>where α is a hyperparameter to balance the importance of the two loss terms.</p><p>The training objective will push s u towards the boundary, while pulling s v close to the origin. Thus, the norm of s u and s v will provide key information to determine the temporal order of an event pair, while the angle ∠θ 1 can help to distinguish VAGUE event pairs. We propose the following score function:</p><formula xml:id="formula_15">score(u, v) = s u D − s v D d D (s u , s v ) + . (<label>11</label></formula><formula xml:id="formula_16">)</formula><p>Based on the score function, different types of relations can be predicted using the following rules:</p><formula xml:id="formula_17">TempRel(u, v) =          BEFORE, if score ∈ (t, 1) AFTER, if score ∈ (−1, −t) EQUAL, if score ∈ [− , ] VAGUE, if score ∈ [−t, − ) ∪ ( , t]<label>(12)</label></formula><p>where the value of threshold t ∈ ( , 1) is adjusted on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hyperbolic Neural Network for Temporal Relation Detection</head><p>Apart from the aforementioned approach, an alterative method is to train an end-to-end hyperbolic neural network using classification objective directly. We propose a hyperbolic neural network model for TempRel detection based on the operations defined on the usual Poincaré ball</p><formula xml:id="formula_18">D n = {x ∈ R n | x &lt; 1}.</formula><p>Given an input sequence containing an event pair (u, v), we first obtain its contextualized sentence representations from a pre-trained language model. The representations are denoted as a matrix B ∈ R l×d , where l represents the sentence length and d is the dimension of word embeddings. The sentence representations are then projected onto a Poincaré ball and fed into a hyperbolic feed-forward layer, C = exp 0 (B). The outputs are passed to a Hyperbolic Gate Recurrent Unit (HGRU) to derive the hidden state of each word. A position masking vector m u or m v , which has a value '1' in the position of an event and '0' otherwise, is applied to retrieve the hidden state representation of the corresponding event:</p><formula xml:id="formula_19">h u = HGRU(C) • m u , h v = HGRU(C) • m v .</formula><p>The HGRU on top of the RoBERTa output can further compose the information from the event triggers and corresponding subjects and objects.</p><p>Afterward, the two event hidden states are combined by performing weighted Möbius aggregation:</p><formula xml:id="formula_20">s uv = W u ⊗ h u ⊕ W v ⊗ h v ⊕ b k . (<label>13</label></formula><formula xml:id="formula_21">)</formula><p>The s uv is further combined with the distance between the two event hidden states, d D (h u , h v ), before applying a hyperbolic non-linear function:</p><formula xml:id="formula_22">o = ϕ ⊗ s uv ⊕ d D (h u , h v ) ⊗ w o . (<label>14</label></formula><formula xml:id="formula_23">)</formula><p>The output o is then passed to a Hyperbolic Multinomial Logistic Regression (HMLR) layer (Eq. 6) to generate the event temporal relation classification result, ŷ = HMLR(o). Figure <ref type="figure" target="#fig_4">3</ref> shows a schematic depiction of the network architecture. Additionally, commonsense knowledge can be incorporated within the HGRU. We follow <ref type="bibr" target="#b21">Ning et al. (2019)</ref> and use a Siamese network trained on TEMPROB<ref type="foot" target="#foot_2">4</ref> , discretize its output, and turn the output into categorical embeddings. Then, we project the categorical embeddings onto a hyperbolic space  and use a Riemannian optimizer to update them. The commonsense features can be directly combined with the components of Eq. 14.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The Use of Pre-trained Language Model</head><p>Both of our proposed methods incorporate pretrained language models. We investigated several ways to utilize them, and include two of them in this paper. The first one follows <ref type="bibr" target="#b21">Ning et al. (2019)</ref>, which only uses the static output of the pre-trained models. This approach is fast and allows a fair comparison with the models in <ref type="bibr" target="#b21">Ning et al. (2019)</ref>.</p><p>The second approach fine-tunes the pre-trained language models during training on the TempRel Extraction objective. This approach can achieve better performance, but takes a longer time to train.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>We describe the datasets, methodologies used in the recent literature for the TempRel extraction tasks. We also briefly present the parameter setup of our experiments. A description of the evaluation metrics can be found in the Appendix.</p><p>Dataset MATRES <ref type="bibr" target="#b24">(Ning et al., 2018c)</ref>  With the novel multi-axis annotation scheme, MA-TRES achieves much higher inter-annotator agreements (IAA) than previous temporal datasets, such as TB-Dense <ref type="bibr" target="#b2">(Cassidy et al., 2014)</ref>, RED <ref type="bibr" target="#b26">(O'Gorman et al., 2016)</ref> and <ref type="bibr">THYME-TimeML (Styler IV et al., 2014)</ref>. MATRES consists of documents from three sources: TimeBank (183 documents), AQUAINT (72 documents), and Platinum (20 documents). We follow the official split in which TimeBank and AQUAINT are used for training, while Platinum is used for testing. We further split 20% of the training data as the validation set.</p><p>Temporal and Causal Reasoning (TCR) <ref type="bibr" target="#b20">(Ning et al., 2018a)</ref>  Parameter Setup Based on the results of our preliminary experiments, the contextualized embeddings are produced from RoBERTa in both of our proposed approaches. We also conducted preliminary experiments to determine the best dimension for the Poincaré embeddings, and found variation in performance having no statistical significance.</p><p>We then adopted the 2D embeddings for the sake of simplicity and ease of visualization. More details about the model architecture and hyperparameter setting can be found in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Results</head><p>Overall Comparison Existing methodologies adopted for TempRel extraction commonly leverage several auxiliary components, such as external commonsense knowledge and multi-task objectives. Therefore, to better understand the impact made by the adoption of the hyperbolic geometry, we conduct additional experiments over ablated versions of the baseline models. In particular, in Table <ref type="table" target="#tab_2">2</ref>, results for the LSTM and LSTM+knowledge are produced by employing the original code provided by the authors<ref type="foot" target="#foot_3">5</ref> . Other results of compared methods are directly taken from the cited papers.</p><p>For consistency and fair comparison with <ref type="bibr" target="#b21">Ning et al. (2019)</ref>, we test our method with inputs from the ELMo <ref type="bibr" target="#b27">(Peters et al., 2018)</ref>, which has been reported achieving the best overall results for the models proposed in <ref type="bibr" target="#b21">(Ning et al., 2019)</ref>. We observe that the proposed Poincaré event embedding learning method presented in Section 4.1 outperforms LSTM and its variants which rely on fairly complex auxiliary features and constraints on both the MATRES and the TCR datasets, and produced more accurate event TempRel detection results even when compared to the JCL-base model. It is worth noticing that the Poincaré event embeddings (static RoBERTa) are trained with a shallow network with just 1.5k parameters and only takes about 4 minutes to train on a single RTX 2080 Ti. If further fine-tuning RoBERTa on MATRES, we observe a further improvement on F 1 by 1.8%.</p><p>Using our proposed alternative end-to-end HGRU model, HGRU (static ELMo) outperforms both the standard LSTM model and the variant incorporating commonsense knowledge (LSTM+knowledge) on MATRES. This verifies that the hyperbolic-based method is more efficient than its Euclidean counterparts. In order to fairly compare with the state-of-the-art model <ref type="bibr" target="#b37">(Wang et al., 2020)</ref> on this task, we utilize RoBERTa and the auxiliary temporal commonsense knowledge since <ref type="bibr" target="#b37">Wang et al. (2020)</ref> also fine-tunes RoBERTa on MATRES and uses external commonsense knowledge. The results show that HGRU (RoBERTa) + knowledge outperforms JCL-base <ref type="bibr" target="#b37">(Wang et al., 2020)</ref> significantly by 7% in F 1 . It even outperforms JCL-all, which further incorporates logic constraints and multi-task learning.</p><p>On the TCR dataset, the proposed hyperbolicbased methods also see similar improvement over existing methods. In terms of the difference between the two proposed methods, Poincaré Events Embeddings achieve a higher F 1 score on TCR. The reason is that HGRU tends to predict more VAGUE labels, but there is no VAGUE in TCR. Interestingly, both proposed methods predict less VAGUE labels when using static RoBERTa. A detailed breakdown of results for each temporal relation and training cost is presented in the Appendix.</p><p>Ablation Study In order to study the impact of different components of HGRU on event TempRel extraction, we conduct the ablation study.</p><p>First, the HGRU layer is removed and the contextual embeddings of events are directly fed into hyperbolic FFNN (HFFNN) while all the other hyperparameters are frozen. shows the impact of the hyperbolic distance feature d D (h u , h v ) and its parameter w o in Eq. 14. The results show that the hyperbolic distance between the hidden states of two events encodes relevant information to predict the event temporal relations. Although, <ref type="bibr" target="#b7">Ganea et al. (2018b)</ref> reported that mixing hyperbolic neural networks with Euclidean Multinomial Logistic Regression (EMLR) can at times achieve better performance than pure hyperbolic networks, yet we observe no significant difference on the MATRES dataset. Finally, as discussed earlier, fine-tunning RoBERTa gives better performance compared to static RoBERTa and ELMo. Additionally, our ablation study on the Poincaré Event Embedding shows that without the angular loss (α = 1, Eq. 10) it can only achieve 62.4 in F 1 , compared to 77.1 while using it (static RoBERTa).</p><p>Case Study Figure <ref type="figure" target="#fig_6">4</ref> shows a set of Poincaré event embeddings resulting from the method proposed in section 4.1<ref type="foot" target="#foot_4">6</ref> . The events are numbered according to the temporal relations detected by the model (a smaller number denotes an earlier event). Among them, it is worth noting the two temporal paths between the expected approval from the bank (e2) and the final acquisition of shares (e5), i.e., e2 → e5 and e2 → e3 → e4 → e5. The first direct path is accompanied by a more fine-grained path, specifying the clearance granted from the authorities (e3) to permit (e4) the bank acquisition and the consequential buying of tendered shares (e5). The model has encoded more recent events closer to the origin, and events in the past closer to the border, while simultaneously shaping a hi- West German and French authorities have (e3: cleared) Dresdner Bank AG's takeover of a majority stake in Banque Internationale de Placement. The approval, which had been (e2: expected), (e4:permits) West Germany's second-largest bank to acquire shares of the French investment bank. Dresdner Bank (e0:said) it will (e5: buy) all shares (e1: tendered) by shareholders on the Paris Stock Exchange at the same price from today through Nov. 17. erarchical structure to link their information with different granularity, for a resulting hierarchical structure with asymmetric connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we proposed to model event temporal relations overcoming the limitations of Euclidean spaces, and designing two TempRel extraction methods using hyperbolic geometry. The first approach highlighted the convenience of learning event embedding in the Poincaré ball, achieving performance on-par with recent methodologies by using just a simple rule-based classifier. Then, we designed a hyperbolic neural network, incorporating temporal commonsense, outperforming stateof-the-art models on the standard datasets. Finally, a qualitative analysis pointed out the inherent advantage of employing hyperbolic spaces to encode asymmetric relations. In the future, we plan to extend our frameworks to a wider spectrum of event relations, including causal and sub-event relations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Events annotated with temporal relations from a document excerpt. Arrow lines represent the Before relations, while red dashed lines the Vague ones.</figDesc><graphic url="image-1.png" coords="1,306.45,288.62,217.58,78.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: An illustration of the Poincaré embedding used to encode two events u and v with known temporal relation. θ 1 is the angle between the event pair (u, v), while θ 2 is the angle of an event pair (u, v ) resulting from the negative sampling process.</figDesc><graphic url="image-3.png" coords="5,121.91,80.81,116.18,116.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>is (e1:leaving) for the US to start a new career, he (e2:said).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: In the hyperbolic neural architecture for temporal relation extraction, sentence tokens are first associated to standard RoBERTa vectors (within the Euclidean space).They are subsequently mapped into a Poincaré ball and processed using Hyperbolic Feed-Forward Layers (H-FFL) and Hyperbolic-GRUs (H-GRU). Then, a masking process ensures that only the event-related vectors are aggregated via Möbius along with their d D distance and the relevant temporal common sense, extracted by a Siamese network pre-trained on TEMPROB knowledge base. Finally, the distribution over event temporal relations is derived using a Hyperbolic Multinomial Logistic Regression (H-MLR), analogous to a traditional Softmax layer in the Euclidean space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: A document excerpt from the MATRES dataset and the related temporal event embedding generated by the Poincaré embedding method.</figDesc><graphic url="image-6.png" coords="9,79.02,70.87,196.47,193.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>is a Tem-pRel dataset that is composed of news documents. The number of event pairs under each of the four relation classes in the MATRES and TCR datasets.</figDesc><table><row><cell>Class</cell><cell cols="2">MATRES Train MATRES Test</cell><cell>TCR</cell></row><row><cell>BEFORE</cell><cell>6, 425</cell><cell cols="2">427 1, 780</cell></row><row><cell>AFTER</cell><cell>4, 481</cell><cell>271</cell><cell>862</cell></row><row><cell>EQUAL</cell><cell>418</cell><cell>30</cell><cell>4</cell></row><row><cell>VAGUE</cell><cell>1, 416</cell><cell>109</cell><cell>0</cell></row><row><cell>Total</cell><cell>12, 740</cell><cell cols="2">837 2, 646</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>is another dataset that adopts the annotation scheme defined in MATRES. It is a much Experimental results on MATRES and TCR. Results presented in the top half are either directly taken from the cited papers or produced by employing the original source code supplied by the authors (models denoted by '*'). Results presented in the lower half are generated from our proposed models and their variants. smaller dataset, with just 25 documents and 2.6K TempRels. Due to the TCR limited size, we follow<ref type="bibr" target="#b21">Ning et al. (2019)</ref> by using the temporal relations in TCR to test the model trained on MATRES. The statistics of the data used in our experiments is shown in Table1.</figDesc><table><row><cell>MATRES</cell><cell>TCR</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The resulting performance of HFFNN is significantly lower than HGRU, which indicates that the temporal information is spread across different time steps of the pre-trained language model output, and it is better encoded by a recurrent architecture. HGRU w/o d D Ablation experiments on the MATRES dataset.</figDesc><table><row><cell>Model</cell><cell></cell><cell>Acc</cell><cell>F 1</cell></row><row><cell>HFFNN (static ELMo)</cell><cell></cell><cell cols="2">67.1 72.7</cell></row><row><cell cols="2">HFFNN (static RoBERTa)</cell><cell cols="2">67.9 73.6</cell></row><row><cell>HGRU (static ELMo)</cell><cell>w/o d D</cell><cell cols="2">67.7 74.3</cell></row><row><cell>HGRU (static ELMo)</cell><cell></cell><cell cols="2">69.2 75.8</cell></row><row><cell cols="2">HGRU (static RoBERTa) w/o d D</cell><cell cols="2">71.0 76.1</cell></row><row><cell cols="2">HGRU (static RoBERTa) with EMLR</cell><cell cols="2">71.8 76.8</cell></row><row><cell>HGRU (static RoBERTa)</cell><cell></cell><cell cols="2">72.2 77.6</cell></row><row><cell cols="4">HGRU (static RoBERTa) + knowledge 73.4 78.5</cell></row><row><cell cols="2">HGRU (RoBERTa) + knowledge</cell><cell cols="2">74.2 80.5</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">They compute the metrics differently and use an old version of MATRES. Thus, their results are not directly comparable</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">The contextualized embeddings capture the context information around the event triggers through pre-trained language models. Thus, the resulting event embedding essentially also encodes the information about its subject and object.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">https://github.com/qiangning/TemProb-NAACL18</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3">https://github.com/qiangning/ NeuralTemporalRelation-EMNLP19</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4">Each point corresponds to the contextualized embedding of an event represented by its predicate, e.g., e1: said, e2: expected, etc. We denote each point in Figure4by a tuple (subject, predicate, object) for easy inspection.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was funded in part by the UK Engineering and Physical Sciences Research Council (grant no. EP/V048597/1, EP/T017112/1). YH is supported by a Turing AI Fellowship funded by the UK Research and Innovation (grant no. EP/V020579/1).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Model Architecture and Hyperparameter Setting</head><p>The implementation of the proposed models is based on the geoopt package <ref type="bibr" target="#b12">(Kochurov et al., 2020)</ref>. RoBERTa used in the experiments is downloaded from Huggingface 1 <ref type="bibr" target="#b38">(Wolf et al., 2020)</ref>.</p><p>ELMo is downloaded from AllenNLP 2 . The implementation architecture of the TempRel HGRU model is shown in Table <ref type="table">A2</ref>. We use the RoBERTa base model, whose output dimension d 1 is 768. The dimension of the hidden states d 2 is set to 128. The commonsense embedding dimension d 3 is set to 32. The FFNN output dimension d 4 is 64. The activation function before the HMLR layer is ReLU. We use Riemannian Adam <ref type="bibr" target="#b0">(Becigneul and Ganea, 2019)</ref> to optimize the hyperbolic parameters, and the standard Adam optimizer for parameters in the Euclidean space. The learning rate for RoBERTa fine-tuning is 1 × 10 −5 , and for other parameters in our proposed models is set to 1 × 10 −3 .</p><p>For the Poincaré event embeddings ( §4.1), the contextualized embeddings are produced from RoBERTa-base. We conducted preliminary experiments to determine the best dimension for the Poincaré embeddings, and found variation in performance having no statistical significance. We then adopted the 2D embeddings for the sake of simplicity and ease of visualization. The weight α for balancing two loss terms is set to 0.5. The number of negative samples is set to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B A Breakdown of Evaluation Results by Temporal Relation Types</head><p>Table <ref type="table">A1</ref> shows the model performance with respect to each relation type. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Evaluation Metrics</head><p>For the evaluation scores used in Table <ref type="table">2</ref>, we follow the widely adopted evaluation metrics proposed in <ref type="bibr" target="#b21">Ning et al. (2019)</ref>, which is also adopted by <ref type="bibr" target="#b37">Wang et al. (2020)</ref>. In particular, given the four tempo-Input Sentences {s i } N i=1 each of which containing one of more event pairs (u, v) with a token sequence s i = {x ij } L j=1 , the masks (m u , m v ) indicate the position of the events in a sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contextual embedding</head><p>ELMo or RoBERTa Encoded by a pre-trained language model {x ij } L j=1 − {ELMo/RoBERTa} →:</p><p>Hyperbolic encoding</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H-distance</head><p>Compute the distance between two points on the Poincaré model </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Riemannian adaptive optimization methods</title>
		<author>
			<persName><forename type="first">Gary</forename><surname>Becigneul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Octavian-Eugen</forename><surname>Ganea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference Track Proceedings of the 6th International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2019. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On approximate reasoning capabilities of low-rank vector spaces</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Representation and Reasoning: Integrating Symbolic and Neural Approaches</title>
		<title level="s">AAAI Spring Symposium Series</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An annotation framework for dense event ordering</title>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Cassidy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Mcdowell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="501" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dense event ordering with a multi-pass architecture</title>
		<author>
			<persName><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Cassidy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Mcdowell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="273" to="284" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Classifying temporal relations between events</title>
		<author>
			<persName><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions</title>
				<meeting>the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume the Demo and Poster Sessions<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Temporal relation discovery between events and temporal expressions identified in clinical narrative</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Anick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengyu</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="page">46</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hyperbolic entailment cones for learning hierarchical embeddings</title>
		<author>
			<persName><forename type="first">Octavian</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>Becigneul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018a</date>
			<biblScope unit="page" from="1646" to="1655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hyperbolic neural networks</title>
		<author>
			<persName><forename type="first">Octavian</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>Bécigneul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2018">2018b</date>
			<biblScope unit="page" from="5345" to="5355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hyperbolic attention networks</title>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep structured neural network for event temporal relation extraction</title>
		<author>
			<persName><forename type="first">Rujun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I-Hung</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</title>
				<meeting>the 23rd Conference on Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="page" from="666" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint event and temporal relation extraction with shared representations and structured prediction</title>
		<author>
			<persName><forename type="first">Rujun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019b</date>
			<biblScope unit="page" from="434" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Geoopt: Riemannian Optimization in Py-Torch</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Kochurov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasul</forename><surname>Karimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergei</forename><surname>Kozlukov</surname></persName>
		</author>
		<idno>ArXiv, abs/2005.02819</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A logic-driven framework for consistency of neural models</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maitrey</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><surname>Vivek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019-03">Srikumar. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The geometry of graphs and some of its algorithmic applications</title>
		<author>
			<persName><forename type="first">N</forename><surname>Linial</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th Annual Symposium on Foundations of Computer Science, SFCS &apos;94</title>
				<meeting>the 35th Annual Symposium on Foundations of Computer Science, SFCS &apos;94</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="577" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A fully hyperbolic neural model for hierarchical multi-class classification</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
				<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="460" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Machine learning of temporal relations</title>
		<author>
			<persName><forename type="first">Inderjeet</forename><surname>Mani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Verhagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Wellner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chungmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Pustejovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="753" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reducing the rank in relational factorization models by including observable patterns</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueyan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><surname>Volker Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1179" to="1187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Poincaré embeddings for learning hierarchical representations</title>
		<author>
			<persName><forename type="first">Maximillian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6338" to="6347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Joint reasoning for temporal and causal relations</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhili</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018a</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2278" to="2288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An improved neural baseline for temporal relation extraction</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6204" to="6210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">TORQUE: A reading comprehension dataset of temporal ordering questions</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rujun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improving temporal relation extraction with a globally acquired statistical resource</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoruo</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018">2018b</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A multiaxis annotation scheme for event temporal relations</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018c</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1318" to="1328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">CogCompTime: A tool for understanding time in natural language</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhili</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoruo</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018d</date>
			<biblScope unit="page" from="72" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Richer event description: Integrating event coreference with temporal, causal and bridging annotation</title>
		<author>
			<persName><forename type="first">Kristin</forename><surname>Tim O'gorman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Wright-Bettner</surname></persName>
		</author>
		<author>
			<persName><surname>Palmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">47</biblScope>
		</imprint>
	</monogr>
	<note>Computing News Storylines</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Representation tradeoffs for hyperbolic embeddings</title>
		<author>
			<persName><forename type="first">Frederic</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">De</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Re</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning (ICML)</title>
				<meeting>the 35th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4460" to="4469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Low distortion delaunay embedding of trees in hyperbolic plane</title>
		<author>
			<persName><forename type="first">Rik</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Graph Drawing, GD&apos;11</title>
				<meeting>the 19th International Conference on Graph Drawing, GD&apos;11</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="355" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Conceptnet 5.5: An open multilingual graph of general knowledge</title>
		<author>
			<persName><forename type="first">Robyn</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI&apos;17</title>
				<meeting>the Thirty-First AAAI Conference on Artificial Intelligence, AAAI&apos;17</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4444" to="4451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Temporal annotation in the clinical domain</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>William F Styler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Finan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piet</forename><forename type="middle">C</forename><surname>De Groen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brad</forename><surname>Erickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guergana</forename><surname>Savova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Transactions of the Association for Computational Linguistics</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="143" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Poincaré GloVe: Hyperbolic word embeddings</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tifrea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Becigneul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O.-E</forename><surname>Ganea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hyperml: A boosting metric learning approach in hyperbolic space for recommender systems</title>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Vinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tran</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoli</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="609" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Order-embeddings of images and language</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vendrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations, ICLR 2016</title>
		<title level="s">Conference Track Proceedings</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-02">2016. May 2-4, 2016</date>
		</imprint>
	</monogr>
	<note>Sanja Fidler, and Raquel Urtasun</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Temporal processing with the tarsqi toolkit</title>
		<author>
			<persName><forename type="first">Marc</forename><surname>Verhagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Pustejovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2008: Companion Volume: Demonstrations</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="189" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Event phase oriented news summarization</title>
		<author>
			<persName><forename type="first">Chengyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aoying</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Wide Web</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1069" to="1092" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Joint constrained learning for event-event relation extraction</title>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="696" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hype-han: Hyperbolic hierarchical attention network for semantic embedding</title>
		<author>
			<persName><forename type="first">Chengkun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbin</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20</title>
				<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3990" to="3996" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
