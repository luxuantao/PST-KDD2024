<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Very Deep Self-Attention Networks for End-to-End Speech Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ngoc-Quan</forename><surname>Pham</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Interactive Systems Lab</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
								<address>
									<settlement>Karlsruhe</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Thai-Son</forename><surname>Nguyen</surname></persName>
							<email>thai.nguyen@kit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Interactive Systems Lab</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
								<address>
									<settlement>Karlsruhe</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jan</forename><surname>Niehues</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Interactive Systems Lab</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
								<address>
									<settlement>Karlsruhe</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Markus</forename><surname>Müller</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Interactive Systems Lab</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
								<address>
									<settlement>Karlsruhe</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sebastian</forename><surname>Stüker</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Interactive Systems Lab</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
								<address>
									<settlement>Karlsruhe</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alex</forename><surname>Waibel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Interactive Systems Lab</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
								<address>
									<settlement>Karlsruhe</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Very Deep Self-Attention Networks for End-to-End Speech Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>speech recognition</term>
					<term>sequence-to-sequence models</term>
					<term>stochastic transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, end-to-end sequence-to-sequence models for speech recognition have gained significant interest in the research community. While previous architecture choices revolve around time-delay neural networks (TDNN) and long shortterm memory (LSTM) recurrent neural networks, we propose to use self-attention via the Transformer architecture as an alternative. Our analysis shows that deep Transformer networks with high learning capacity are able to exceed performance from previous end-to-end approaches and even match the conventional hybrid systems. Moreover, we trained very deep models with up to 48 Transformer layers for both encoder and decoders combined with stochastic residual connections, which greatly improve generalizability and training efficiency. The resulting models outperform all previous end-to-end ASR approaches on the Switchboard benchmark. An ensemble of these models achieve 9.9% and 17.7% WER on Switchboard and CallHome test sets respectively. This finding brings our end-to-end models to competitive levels with previous hybrid systems. Further, with model ensembling the Transformers can outperform certain hybrid systems, which are more complicated in terms of both structure and training procedure.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, the sequence-to-sequence (S2S) approach in automatic speech recognition (ASR) has received a considerable amount of interest, due to the ability to jointly train all components towards a common goal which reduces complexity and error propagation compared to traditional hybrid systems. Traditional systems divide representation into different levels in the acoustic model, in particular separating global features (such as channel and speaker characteristics) and local features (on the phoneme level). The language model and acoustic model are trained with different loss functions and then combined during decoding. In contrast, neural S2S models perform a direct mapping from audio signals to text sequences based on dynamic interactions between two main model components, an encoder and a decoder, which are jointly trained towards maximizing the likelihood of the generated output sequence. The neural encoder reads the audio features into high-level representations, which are then fed into an auto-regressive decoder which attentively generates the output sequences <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>In this context, we aim at reconsidering acoustic modeling within end-to-end models. Previous approaches in general had long short-term memory neural networks (LSTM) <ref type="bibr" target="#b2">[3]</ref> or time-delay neural networks <ref type="bibr" target="#b3">[4]</ref> operating on top of frame-level features to learn sequence-level representation. These neural networks are able to capture long range and local dependencies between different timesteps.</p><p>Recently, self-attention has been shown to efficiently represent different structures including text <ref type="bibr" target="#b0">[1]</ref>, images <ref type="bibr" target="#b4">[5]</ref>, and even acoustic signals <ref type="bibr" target="#b5">[6]</ref> with impressive results. The Transformer model using self-attention achieved the state-of-the-art in mainstream NLP tasks <ref type="bibr" target="#b6">[7]</ref>. The attractiveness of self-attention networks originates from the ability to establish a direct connection between any element in the sequence. Self-attention is able to scale with the length of the input sequence without any limiting factor such as, e.g., the kernel size of CNNs, or the vanishing gradient problem of LSTMs. Moreover, the selfattention network is also computationally advantageous compared to recurrent structures because intermediate states are no longer connected recurrently, leading to more efficient batched operations. As a result, self-attention networks can be reasonably trained with many layers leading to state-of-the-art performance in various tasks <ref type="bibr" target="#b7">[8]</ref>. Self-attention and the Transformer have been exploratorily applied to ASR, but so far with unsatisfactory results. <ref type="bibr" target="#b5">[6]</ref> found that self-attention in the encoder (acoustic model) was not effective, but combined with an LSTM brought marginal improvement and greater interpretability, while <ref type="bibr" target="#b8">[9]</ref> did not find any notable improvement using the Transformer in which the encoder combines self-attention with convolution/LSTM compared to other model architectures. In this work, we show that the Transformer requires little modification to adapt on the speech recognition task. Specifically, we exploit the advantages of self-attention networks for ASR such that both our acoustic encoder and character-generating decoder are constructed without any recurrence or convolution. This is the first attempt to propose this system architecture to the best of our knowledge, and we show that a competitive end-to-end ASR model can be achieved solely using standard training techniques from general S2S systems.</p><p>Our contributions are as follows. First, we show that depth is an important factor to acquire competitive end-to-end ASR models with the Transformer. Second, in order to facilitate training of very deep configurations, we propose a variation of stochastic depth for the Transformer inspired by the Stochastic Residual Network for image classification <ref type="bibr" target="#b9">[10]</ref>.</p><p>We discovered that its ability to regularize is the key contribution to obtain the state-of-the-art result among end-to-end ASR models for the standard 300h Switchboard (SWB) benchmark. This result is achieved using a total of 48 Transformer layers across the encoder and decoder.<ref type="foot" target="#foot_0">1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Model Description 2.1. Encoder-Decoder with Attention</head><p>The main components of the model include an encoder, which consumes the source sequence and then generates a high-level representation, and a decoder generating the target sequence. The decoder models the data as a conditional language modelthe probability of the sequence of discrete tokens is decomposed into an ordered product of distributions conditioned on both the previously generated tokens and the encoder representation.</p><p>Both encoder and decoder are neural networks and require neural components that are able to learn the relationship between the time steps in the input and output sequence. The decoder also requires a mechanism to condition on specific components of the encoder representation. For the Transformer, attention or its common variation multi-head attention, is the core of the model in place of recurrence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multi-Head Attention</head><p>Fundamentally, attention refers to the method of using a content-based information extractor from a set of queries Q, keys K and values K. The retrieval function is based on similarities <ref type="bibr" target="#b10">[11]</ref> between the queries and the keys, and in turn returns the the weighted sum of the values as following:</p><formula xml:id="formula_0">Attention(Q, K, V ) = softmax(QK T )V<label>(1)</label></formula><p>Recently, <ref type="bibr" target="#b6">[7]</ref> improves dot-product attention by scaling the queries before hand and introducing sub-space projection for keys, queries and values into n parallel heads, in which n attention operations are performed with corresponding heads. The result is the concatenation of attention outputs from each head. Notably, unlike recurrent connections which use single states with gating mechanism to transfer data or convolution connections linearly combining local states limited in a kernel size, self-attention aggregates the information in all time-steps without any intermediate transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Layer Architecture</head><p>The overall architecture is demonstrated in Figure <ref type="figure" target="#fig_0">1</ref>. The encoder and decoder of the Transformers are constructed by layers, each of which contains self-attentional sub-layers coupled with feed-forward neural networks.</p><p>To adapt the encoder to long speech utterances, we follow the reshaping practice from <ref type="bibr" target="#b5">[6]</ref> by grouping consecutive frames into one step. Subsequently we combine the input features with sinusoidal positional encoding <ref type="bibr" target="#b6">[7]</ref>. While directly adding acoustic features to the positional encoding is harmful, potentially leading to divergence during training <ref type="bibr" target="#b5">[6]</ref>, we resolved that problem by simply projecting the concatenated features to a higher dimension before adding (512, as other hidden layers in the model). In the case of speech recognition specifically, the positional encoding offers a clear advantage compared to learnable positional embeddings <ref type="bibr" target="#b11">[12]</ref>, because the speech signals can be arbitrarily long with a higher variance compared to text sequences.</p><p>The Transformer encoder passes the input features to a selfattention layer followed by a feed-forward neural network with 1 hidden layer with the ReLU activation function. Before these sub-modules, we follow the original work to include residual connections which establishes short-cuts between the lowerlevel representation and the higher layers. The presence of the residual layer massively increases the magnitude of the neuron values which is then alleviated by the layer-normalization <ref type="bibr" target="#b12">[13]</ref> layers placed after each residual connection.</p><p>The decoder is the standard Transformer decoder in the recent translation systems <ref type="bibr" target="#b6">[7]</ref>. The notable difference between the decoder and the encoder is that to maintain the auto-regressive nature of the model, the self-attention layer of the decoder must be masked so that each state has only access to the past states. Moreover, an additional attention layer using the target hidden layer layers as queries and the encoder outputs as keys and values is placed between the self-attention and the feed-forward layers. Residual and layer-normalization are setup identically to the encoder.</p><p>This particular design of the Transformer has various advantages compared to previously proposed RNNs and CNNs networks. First, computation of each layer and sub-module can be efficiently parallelized over both mini-batch and time dimensions of the input. Second, the combination of residual and layer normalization is the key to enable greater depth configurations to be trainable, which is the main reason for the performance breakthrough in recent works in both MT and natural language processing <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Stochastic Layers</head><p>The high density of residual connections is the reason why Transformer is favourably trained with many layers. However, deep models in general suffer from either overfitting due to more complex architectures and optimization difficulty <ref type="bibr" target="#b14">[15]</ref>. Studies about residual networks have shown that during training the network consists of multiple sub-networks taking different paths through shortcut connections <ref type="bibr" target="#b15">[16]</ref>, and thus there are redundant layers. Motivated by the previous work of <ref type="bibr" target="#b9">[10]</ref>, we propose to apply stochastic residual layers into our Transformers. The method resembles Dropout <ref type="bibr" target="#b16">[17]</ref>, in which the key idea is the layers are randomly dropped during training. The original residual connection of an input x and its corresponding neural layer F has the following form:</p><formula xml:id="formula_1">R(x) = LayerNorm(F (x) + x)<label>(2)</label></formula><p>In equation 3, the inner function F is either self-attention, feedforward layers or even decoder-encoder attention. The layer normalization as in <ref type="bibr" target="#b12">[13]</ref> keeps the magnitude of the hidden layers from growing large. Stochastic residual connections fundamentally apply a mask M on the function F , as follows:</p><formula xml:id="formula_2">R(x) = LayerNorm(M * F (x) + x)<label>(3)</label></formula><p>Mask M only takes 0 or 1 as values, generated from a Bernoulli distribution similar to dropout <ref type="bibr" target="#b16">[17]</ref>. When M = 1, the inner function F is activated, while it is skipped when M = 0. These stochastic connections enables more sub-network configurations to be created during training, while during inference the full network is presented, causing the effect of ensembling different sub-networks, as analyzed in <ref type="bibr" target="#b15">[16]</ref>. It is non-trivial regarding how to the parameter p for dropping layers, since the amount of residual connections for the Transformer is considerable. In other words, the lower the layer is, the lower the probability p is required to be set. As a result, p values are set with the following policy:</p><p>• Sub-layers inside each encoder or decoder layer share the same mask, so each mask decides to drop or to keep the whole layer (including the sub-layers inside). This way we have one hyper-parameter p for each layer.</p><p>• As suggested by <ref type="bibr" target="#b9">[10]</ref>, the lower layers of the networks handle raw-level acoustic features on the encoder side, and the character embeddings on the decoder side. Therefore, lower layers l have lower probability linearly scaled by their depth according to equation 4 with p is the global-level parameter and L is the total number of layers. <ref type="foot" target="#foot_1">2</ref>Lastly, since the layers are selected with probability 1 − p l during training and are always presented during inference, we scale the layers' output by 1 1−p l whenever they are not skipped. Therefore, each stochastic residual connection has the following form during training (the scalar is removed during testing):</p><formula xml:id="formula_3">p l = l L (1 − p)<label>(4)</label></formula><formula xml:id="formula_4">R(x) = LayerNorm(M * F (x) * 1 1 − p l + x)<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data</head><p>Our experiments were conducted on the Switchboard-1 Release 2 (LDC97S62) corpus which contains over 300 hours of speech. The Hub5'00 evaluation data (LDC2002S09) was used as our test set. All the models were trained on 40 log mel filter-bank features which are extracted and normalized per conversation. We also adopted a simple down-sampling method in which we stacked 4 consecutive features vectors to reduce the length of input sequences by a factor of 4. Beside the filter-bank features, we did not employ any auxiliary features. We followed the approach from <ref type="bibr" target="#b17">[18]</ref> to generate a speech perturbation training set.</p><p>Extra experiments are also conducted on the TED-LIUM 3 <ref type="bibr" target="#b18">[19]</ref> dataset which is more challenging due to longer sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Implementation Details</head><p>Our hyperparameter search revolves around the Base configuration of the machine translation model in the original Transformer paper <ref type="bibr" target="#b6">[7]</ref>. For all of our experiments in this work, the embedding dimension d is set to 512 and the size of the hidden state in the feed-forward sub-layer is 1024. The minibatch size is set so that we can fit our model in the GPU, and we accumulate the gradients and update every 25000 characters. Adam <ref type="bibr" target="#b19">[20]</ref> with adaptive learning rate over the training progress: lr = init lr * d −0.5 * min(step −0.5 , step * warmup −1.5 ) <ref type="bibr" target="#b5">(6)</ref> in which the init lr is set to 2, and we warm up the learning rate for 8000 steps. Dropout <ref type="bibr" target="#b16">[17]</ref> (applied before residual connection and on the attention weights) is set at 0.2. We also apply character dropout <ref type="bibr" target="#b20">[21]</ref> with p = 0.1 and label smoothing <ref type="bibr" target="#b21">[22]</ref> with = 0.1. The experiment results on SWB testsets are shown in Table 1. A shallow configuration (i.e 4 layers) is not sufficient for the task, and the WER reduces from 20.8% to 12.1% on the SWB test as we increase the depth from 4 to 24. The improvement is less significant between 12 and 24 (only 5% relative WER), which seems to be a symptom of overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>Our suspicion of overfitting is confirmed by the addition of stochastic networks. At 12 layers, the stochastic connections only improve the CH performance by a small margin, while the improvement was substantially greater on the 24 layer setting. Following this trend, the stochastic 48-layer model keeps improving on the CH test set, showing the model's ability to generalize better. Arguably, the advantage of deeper models is to offer more parameters, as shown in the second column. We performed a contrastive experiment using a shallow model of 8 layers, but doubling the model size so that its parameter count is larger than the deep 24-layer model. The performance of this model is considerable worse than the 24 layer model, demonstrating deeper networks with smaller size are more beneficial than a wider yet shallower configuration. Reversely, we found that the 48-layer model with half size is equivalent with the 12-layer variant, possibly due to over-regularization 3 .</p><p>Our second discovery that the encoder requires deeper networks than the decoder for the Transformer. This is inline with the previous work from <ref type="bibr" target="#b28">[29]</ref> who increases depth for the CNN encoder. As shown above, the encoder has learn representations starting from audio features, while the decoder handles the generation of character sequences, conditionally based on the encoder representation. The difference in modalities suggest different configurations. Holding the total number of layers as 48, we shift depth to the encoder. Our result with a much shallower decoder, only 8 layers, but with 40 encoder layers is as good as the 24-layer configuration. More stunningly, we were able to obtain our best result with the 36 − 12 configuration with 20.6% WER, which is competitive with the best previous end-to-end work using data augmentation.</p><p>Third, it was revealed that the combination of our regularization techniques (dropout, label-smoothing and stochastic networks) are additive with data augmentation, which further improved our result to 18.1% with the 36 − 12 setup. This model, as far as we know, establishes the state-of-the-art result for the SWB benchmark among end-to-end ASR models, as shown in table <ref type="table" target="#tab_1">2</ref>. Comparing to the best hybrid models with similar data constraints, our models outperformed on the CH test set while remaining competitive on the SWB test set without any additional language model training data. This result suggests the strong generalizability of the Stochastic Transformer.</p><p>Finally, the experiments with similar depth suggest that self-attention performs competitively compared to LSTMs <ref type="bibr" target="#b2">[3]</ref> or TDNNs <ref type="bibr" target="#b3">[4]</ref>. The former benefits strongly from building deep residual networks, in which our main finding shows that depth is crucial for using self-attention in the regimen of ASR. 3 We did not change dropout values for this model, so each layer's hidden layers are dropped more severely Table <ref type="table" target="#tab_2">3</ref> shows our result on TED-LIUM (version 3) dataset. With a similar configuration to the SWB models, we outperformed a strong baseline which uses both an external language model trained on larger data than the available transcription and speed perturbation, using our model with 36 encoder layers and 12 decoder layers. This result continues the trend that these models benefit from a deeper encoder, and together with the stochastic residual connections we further improved WER by 21.8% relatively, from 14.2 to 11.1%. Given the potential of the models <ref type="foot" target="#foot_2">4</ref> , it is strongly suggested that better results can be obtained by further hyper-parameter optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">On TED-LIUM dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>The idea of using self-attention as the main component of ASR models has been investigated in various forms. <ref type="bibr" target="#b5">[6]</ref> combines self-attention with LSTMs, while <ref type="bibr" target="#b29">[30]</ref> uses self-attention as an alternative in CTC models. A variation of the Transformer has been applied to ASR with additional TDNN layers to downsample the acoustic signal <ref type="bibr" target="#b8">[9]</ref>. Though self-attention has provided various benefits such as training speed or model interpretability, previous works have not been able to point out any enhancement in terms of performance. Our work provides a self-attentiononly model and showed that with high capacity and regularization, such a network can exceed previous end-to-end models and approach the performance of hybrid systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Directly mapping from acoustics to text transcriptions is a challenging task for the S2S model. Theoretically, self-attention can be used alternatively to TDNNs or LSTMs for acoustic modeling, and here we are the first demonstrate that the Transformer can be effective for ASR with the key is to setup very deep stochastic models. State-of-the-art results among end-to-end models on 2 standard benchmarks are achieved and our networks are among the deepest configurations for ASR. Future works will involve developing the framework under more realistic and challenging conditions such as real-time recognition, in which latency and streaming are crucial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head><p>The work leading to these results has received funding from the European Union under grant agreement N o 825460 and the Federal Ministry of Education and Research (Germany) / DLR Projektträger Bereich Gesundheit under grant agreement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A diagram of transformation from acoustic features to character-level transcriptions. The red connections represent the residual connections, which are rescaled according to Equation 5 for stochastic Transformers.</figDesc><graphic url="image-1.png" coords="2,62.56,65.02,216.85,193.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The performance of deep self-attention networks with and without stochastic layers on Hub5'00 test set with 300h SWB training set.</figDesc><table><row><cell>Layers</cell><cell cols="3">#Param SWB CH</cell></row><row><cell>04Enc-04Dec</cell><cell>21M</cell><cell>20.8</cell><cell>33.2</cell></row><row><cell>08Enc-08Dec</cell><cell>42M</cell><cell>14.8</cell><cell>25.5</cell></row><row><cell>12Enc-12Dec</cell><cell>63M</cell><cell>13.0</cell><cell>23.9</cell></row><row><cell>+Stochastic Layers</cell><cell></cell><cell>13.1</cell><cell>23.6</cell></row><row><cell>24Enc-24Dec</cell><cell>126M</cell><cell>12.1</cell><cell>23.0</cell></row><row><cell>+Stochastic Layers</cell><cell></cell><cell>11.7</cell><cell>21.5</cell></row><row><cell>+Speed Perturbation</cell><cell></cell><cell>10.6</cell><cell>20.4</cell></row><row><cell>48Enc-48Dec</cell><cell>252M</cell><cell>-</cell><cell>-</cell></row><row><cell>+Stochastic Layers</cell><cell></cell><cell>11.6</cell><cell>20.9</cell></row><row><cell>48Enc-48Dec (half-size)</cell><cell>63M</cell><cell>-</cell><cell>-</cell></row><row><cell>+Stochastic Layers</cell><cell></cell><cell>12.5</cell><cell>22.9</cell></row><row><cell>08Enc-08Dec (big)</cell><cell>168M</cell><cell>13.8</cell><cell>25.1</cell></row><row><cell>24Enc-12Dec</cell><cell>113M</cell><cell>13.3</cell><cell>23.7</cell></row><row><cell>+Stochastic Layers</cell><cell></cell><cell>11.9</cell><cell>21.6</cell></row><row><cell>36Enc-8Dec</cell><cell>113M</cell><cell>12.4</cell><cell>22.6</cell></row><row><cell>+Stochastic Layers</cell><cell></cell><cell>11.5</cell><cell>20.6</cell></row><row><cell>36Enc-12Dec</cell><cell>113M</cell><cell>12.4</cell><cell>22.6</cell></row><row><cell>+Speed Perturbation</cell><cell></cell><cell>11.2</cell><cell>20.6</cell></row><row><cell>+Stochastic Layers</cell><cell></cell><cell>11.3</cell><cell>20.7</cell></row><row><cell>+Both</cell><cell></cell><cell>10.4</cell><cell>18.6</cell></row><row><cell>40Enc-8Dec</cell><cell>109M</cell><cell>-</cell><cell>-</cell></row><row><cell>+Stochastic Layers</cell><cell></cell><cell>11.9</cell><cell>21.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparing our best model to other hybrid and endto-end systems reporting on Hub5'00 test set with 300h SWB training set.</figDesc><table><row><cell cols="2">Hybrid/End-to-End Models</cell><cell cols="2">Tgt Unit SWB CH</cell></row><row><cell cols="2">TDNN +LFMMI [23]</cell><cell>Phone</cell><cell>10.0 20.1</cell></row><row><cell cols="2">BLSTM +LFMMI [23]</cell><cell>Phone</cell><cell>9.6 19.3</cell></row><row><cell cols="2">CTC+CharLM [24]</cell><cell>Char</cell><cell>21.4 40.2</cell></row><row><cell cols="2">LSTM w/attention [1]</cell><cell>Char</cell><cell>15.8 36.0</cell></row><row><cell cols="2">Iterated-CTC +LSTM-LM [25]</cell><cell>Char</cell><cell>14.0 25.3</cell></row><row><cell>Seq2Seq</cell><cell>+LSTM-LM [26]</cell><cell>BPE</cell><cell>11.8 25.7</cell></row><row><cell cols="2">Seq2Seq +Speed Perturbation [27]</cell><cell>Char</cell><cell>12.2 23.3</cell></row><row><cell cols="2">CTC-A2W +Speed Perturbation [28]</cell><cell>Word</cell><cell>11.4 20.8</cell></row><row><cell cols="2">36Enc-12Dec (Ours)</cell><cell>Char</cell><cell>10.4 18.6</cell></row><row><cell cols="2">48Enc-12Dec (Ours)</cell><cell>Char</cell><cell>10.7 19.4</cell></row><row><cell cols="2">60Enc-12Dec (Ours)</cell><cell>Char</cell><cell>10.6 19.0</cell></row><row><cell>Ensemble</cell><cell></cell><cell></cell><cell>9.9 17.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The Transformer results on the TED-LIUM test set using TED-LIUM 3 training set.</figDesc><table><row><cell>Models</cell><cell>Test WER</cell></row><row><cell>CTC [19]</cell><cell>17.4</cell></row><row><cell>CTC/LM + speed perturbation [19]</cell><cell>13.7</cell></row><row><cell>12Enc-12Dec (Ours)</cell><cell>14.2</cell></row><row><cell>Stc. 12Enc-12Dec (Ours)</cell><cell>12.4</cell></row><row><cell>Stc. 24Enc-24Dec (Ours)</cell><cell>11.3</cell></row><row><cell>Stc. 36Enc-12Dec (Ours)</cell><cell>10.6</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Our source code and final model are available at https://github.com/quanpn90/NMTGMinor/tree/audio-encoder/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Our early experiments with a constant p for all connections provide evidence that dropping lower-level representations is less tolerable than dropping higher-level representations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">We did not have enough time for a thorough hyper-parameter search by the time of submission</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-to-end attention-based large vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4945" to="4949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Phoneme recognition using time-delay neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hanazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shikano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="328" to="339" />
			<date type="published" when="1989-03">March 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Self-attentional acoustic models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sperber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stüker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09519</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Bert: Pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5884" to="5888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The karlsruhe institute of technology systems for the news translation task in wmt 2018</title>
		<author>
			<persName><forename type="first">N.-Q</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W18-6422" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Shared Task Papers</title>
				<meeting>the Third Conference on Machine Translation: Shared Task Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="467" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Training deeper neural machine translation models with transparent attention</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D18-1338" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11">Oct.-Nov. 2018</date>
			<biblScope unit="page" from="3028" to="3033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Residual networks behave like ensembles of relatively shallow networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="550" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Audio augmentation for speech recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ted-lium 3: twice as much data and corpus repartition for experiments on speaker adaptation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghannay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Estève</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Speech and Computer</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="198" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
				<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1019" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Purely sequence-trained neural networks for asr based on lattice-free mmi</title>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Galvez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Interspeech</publisher>
			<biblScope unit="page" from="2751" to="2755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Lexicon-free conversational speech recognition with neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="345" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Advances in allneural speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint/>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improved training of end-to-end attention models for speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="7" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improving attention based sequence-to-sequence models for end-toend english conversational speech recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="761" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A multistage training framework for acoustic-to-word model</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4845" to="4849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Self-attention networks for connectionist temporal classification in speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Salazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kirchhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10055</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
