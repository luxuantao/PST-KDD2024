<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic 3D Facial Expression Analysis in Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ya</forename><surname>Chang</surname></persName>
							<email>yachang@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>93106</postCode>
									<settlement>Santa Barbara</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marcelo</forename><surname>Vieira</surname></persName>
							<email>mbvieira@impa.br</email>
							<affiliation key="aff1">
								<orgName type="department">Instituto de Matemática Pura e Aplicada</orgName>
								<address>
									<addrLine>110 Jardim Botânico</addrLine>
									<postCode>22460-320</postCode>
									<settlement>Est. Dona Castorina, Rio de Janeiro</settlement>
									<region>RJ</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthew</forename><surname>Turk</surname></persName>
							<email>mturk@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>93106</postCode>
									<settlement>Santa Barbara</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luiz</forename><surname>Velho</surname></persName>
							<email>lvelho@impa.br</email>
							<affiliation key="aff1">
								<orgName type="department">Instituto de Matemática Pura e Aplicada</orgName>
								<address>
									<addrLine>110 Jardim Botânico</addrLine>
									<postCode>22460-320</postCode>
									<settlement>Est. Dona Castorina, Rio de Janeiro</settlement>
									<region>RJ</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automatic 3D Facial Expression Analysis in Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">367318B71213A37FAEA36F9D800F86A0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a novel framework for automatic 3D facial expression analysis in videos. Preliminary results demonstrate editing facial expression with facial expression recognition. We first build a 3D expression database to learn the expression space of a human face. The real-time 3D video data were captured by a camera/projector scanning system. From this database, we extract the geometry deformation independent of pose and illumination changes. All possible facial deformations of an individual make a nonlinear manifold embedded in a high dimensional space. To combine the manifolds of different subjects that vary significantly and are usually hard to align, we transfer the facial deformations in all training videos to one standard model. Lipschitz embedding embeds the normalized deformation of the standard model in a low dimensional generalized manifold. We learn a probabilistic expression model on the generalized manifold. To edit a facial expression of a new subject in 3D videos, the system searches over this generalized manifold for optimal replacement with the 'target' expression, which will be blended with the deformation in the previous frames to synthesize images of the new expression with the current head pose. Experimental results show that our method works effectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Facial expression analysis and synthesis is an active and challenging research topic in computer vision, impacting important applications in areas such as human-computer interaction and data-driven animation. We introduce a novel framework for automatic facial expression editing in 3D videos. The system recognizes the expressions and replaces them by expression mapping functions smoothly. We expect to use this 3D system in the future as the core element of a facial expression analysis that takes 2D video input.</p><p>3D information is becoming widely used in this field <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. A combination of image texture and 3D geometry can be used to considerably reduce the variation due to pose and illumination changes. Recent technical progress allows the capture of accurate dense 3D data in real time, which enables us to build a 3D expression database for learning the deformation space of human faces. The data capture system was de-veloped by <ref type="bibr" target="#b3">[4]</ref>. A coarse mesh model is fitted to track the inter-frame point motion and a dense mesh is used for synthesis of new expressions.</p><p>The nonlinear expression manifolds of different subjects share a similar structure but vary significantly in the high dimensional space. Researchers have proposed many approaches, such as locally linear embedding (LLE) <ref type="bibr" target="#b4">[5]</ref> and Isomap <ref type="bibr" target="#b5">[6]</ref> to embed the nonlinear manifolds in a low dimensional space. Expression manifolds from different subjects remain difficult to align in the embedded space due to various causes: <ref type="bibr" target="#b0">(1)</ref> subjects have different face geometries; (2) facial expression styles vary by subject;</p><p>(3) some persons cannot perform certain expressions; and (4) the whole expression space is large including blended expressions, so only a small portion of it can be sampled. Considering these factors, bilinear <ref type="bibr" target="#b6">[7]</ref> and multi-linear <ref type="bibr" target="#b7">[8]</ref> models have been successful in decomposing the static image ensembles into different sources of variation, such as identity and content. Elgammal and Lee <ref type="bibr" target="#b8">[9]</ref> applied a decomposable generative model to separate the content and style on the manifold representing dynamic objects. It learned a unified manifold by transforming the embedded manifolds of different subjects into one. This approach assumes that the same kind of expression performed by different subjects match each other strictly. However, one kind of expression can be performed in multiple styles, such as laughter with closed mouth or with open mouth. The matching between these styles is very subjective.</p><p>To solve this problem, we built a generalized manifold that is capable of handling multiple kinds of expressions with multiple styles. We transferred the 3D deformation from the models in the training videos to a standard model. Sumner and Popovic <ref type="bibr" target="#b9">[10]</ref> designed a special scheme for triangle meshes where the deformed target mesh is found by minimizing the transformation between the matching triangles while enforcing the connectivity. We added a temporal constraint to ensure the smooth transfer of the facial deformations in the training videos to the standard model. This model is scalable and extensible. New subjects with new expressions can be easily added in. The performance of the system will improve continuously with new data.</p><p>We built a generalized manifold from normalized motion of the standard model. Lipschitz embedding was developed to embed the manifold to a low dimensional space. A probabilistic model was learned on the generalized manifold in the embedded space as in <ref type="bibr" target="#b10">[11]</ref>.</p><p>In this framework, a complete expression sequence becomes a path on the expression manifold, emanating from a center that corresponds to the neutral expression. Each path consists of several clusters. A probabilistic model of transition between the clusters and paths is learned through training videos in the embedded space. The likelihood of one kind of facial expression is modeled as a mixture density with the clusters as mixture centers. The transition between different expressions is represented as the evolution of the posterior probability of six basic expression paths. In a video with a new subject, the deformation can be transferred to the standard model and recognized correctly.</p><p>For expression editing, the user can define any expression mapping function F:</p><formula xml:id="formula_0">6 6 R R →</formula><p>, where the domain and range are the likelihood of one kind of facial expression. We currently use 3D videos as input data. Many algorithms <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> have been proposed to fit 3D deformable models on 2D image sequences. So the next step will be to take 2D videos as input with a system (such as <ref type="bibr" target="#b12">[13]</ref>) used as a preprocessing module.</p><p>When the expression in the domain of F is detected, the system will search over the generalized manifold for an optimal replacement in the 'range' expression. The deformation of the standard model is transferred back to the subject, and blended with the facial deformation in the previous frame to ensure smooth editing. Fig. <ref type="figure" target="#fig_0">1</ref> illustrates the overall system structure.</p><p>The main contributions of this paper are the following: (1) We constructed a 3D expression database with good scalability. (2) We proposed and defined a generalized manifold of facial expression. Deformation data from different subjects complement each other for a better description of the true manifold. <ref type="bibr" target="#b2">(3)</ref> We learned a probabilistic model to automatically implement the expression mapping function.</p><p>The remainder of the paper is organized as follows. We present the related work in Section 2. We then describe how to construct the 3D expression database in Section 3. Section 4 presents how to build generalized expression manifold. Section 5 discusses the probabilistic model. Section 6 presents the experimental results. Section 7 concludes the paper with discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Many researchers have explored the nature of the space of facial expressions. Zhang et al. <ref type="bibr" target="#b13">[14]</ref> used a two-layer perceptron to classify facial expressions. They found that five to seven hidden perceptrons are probably enough to represent the space of facial expressions. Chuang et al. <ref type="bibr" target="#b14">[15]</ref> showed that the space of facial expression could be modeled with a bilinear model. Two formulations of bilinear models, asymmetric and symmetric, were fit to facial expression data.</p><p>There are several publicly available facial expression databases: Cohen-Kanade facial expression database <ref type="bibr" target="#b15">[16]</ref> provided by CMU has 97 subjects, 481 video sequences with six kinds of basic expressions. Subjects in every video began from a neutral expression, and ended at the expression apex. FACS coding of every video is also provided. The CMU PIE database <ref type="bibr" target="#b16">[17]</ref> includes 41,368 face images of 68 people captured under 13 poses, 43 illuminations conditions, and with 3 different expressions: neutral, smile, and blinking. The Human ID database provided by USF has 100 exemplar 3D faces. The exemplar 3D faces were put in full correspondence as explained by Blanz and Vetter <ref type="bibr" target="#b0">[1]</ref>.</p><p>Facial animation can be generated from scratch, or by reusing existing data. Noh and Neumann <ref type="bibr" target="#b17">[18]</ref> proposed a heuristic method to transfer the facial expression from one mesh to another based on 3D geometry morphing. Lee and Shin <ref type="bibr" target="#b18">[19]</ref> retargeted motions by using a hierarchical displacement mapping based on multilevel B-spline approximation. Zhang <ref type="bibr" target="#b19">[20]</ref> proposed a geometry-driven photorealistic facial expression synthesis method. Example-based motion synthesis is another stream of research. Ryun et al. <ref type="bibr" target="#b20">[21]</ref> proposed an example-based approach for expression retargeting. We improve the deformation transfer scheme in <ref type="bibr" target="#b9">[10]</ref> by adding temporal constraints to ensure smooth transfer of source dynamics.</p><p>We were inspired by the work of Wang et al. <ref type="bibr" target="#b2">[3]</ref>. The main difference is that we build a generalized expression manifold by deformation transfer, which is capable of handling multiple expressions with multiple styles. The probabilistic model also takes the blended expression into consideration and enables automatic expression editing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">3D Expression Database</head><p>To our knowledge, there is no 3D expression database publicly available, so we built our own 3D database by capturing real-time range data of people making different facial expressions. The database includes 6 subjects and 36 videos, with a total of 2581 frames. Each subject performed all six basic expressions from neutral to apex and back to neutral. The range data were registered by robust feature tracking and 3D mesh model fitting. We intend to make the database publicly available with more subjects in the near future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Real-Time 3D Scanner</head><p>To construct a high quality 3D expression database, the capture system should provide high quality texture and geometry data in real-time. Quality is crucial for accurate analysis and realistic synthesis. Real-time is important for subtle facial motion capture and temporal study of facial expression.</p><p>The system used for obtaining 3D data <ref type="bibr" target="#b3">[4]</ref> is based on a camera/projector pair and active stereo. It was built with off-the-shelf NTSC video equipment. The key of this system is the combination of the color code (b,s)-BCSL <ref type="bibr" target="#b21">[22]</ref> with a synchronized video stream.</p><p>The (b,s)-BCSL code provides an efficient camera/projector correspondence scheme. Parameter b is the number of colors and s is the number of patterns to be projected. Two patterns is the minimum, giving the best time coherence compromise. The complementary patterns are used to detect stripe transitions and colors robustly.</p><p>Our system applies six colors that can be unambiguously detected through zerocrossings: RGBCMY. In our experiments, we use a (6,2)-BCSL code that features two patterns of 900 stripes.</p><p>To build camera/projector correspondence, we project a subsequence of these two patterns onto the scene and detect the projected stripe colors and boundaries from the image obtained by a high-speed camera. The four projected colors, two for each pattern, detected close to any boundary are uniquely decoded to the projected stripe index p (Fig. <ref type="figure" target="#fig_1">2</ref>). The correspondent column in the projector space is detected in O(1) by using (6,2)-BCSL decoding process. The depth is then computed by the camera/projector intrinsic parameters and the rigid transformation between their reference systems.</p><p>We project every color stripe followed by its complementary color to facilitate the robust detection of stripe boundaries from the difference of the two resulting images. The stripe boundaries become zero-crossings in the consecutive images and can be detected with sub-pixel precision. One complete geometry reconstruction is obtained after the projection of the pattern 1 and its complement followed by pattern 2 and its complement.</p><p>The (6,2)-BCSL can be easily combined with video streams. Each 640x480 video frame in NTSC standard is composed of two interlaced 640x240 fields. Each field is exposed/captured in 1/60 sec. The camera and projector are synchronized using genlock. For projection, we generate a frame stream interleaving the two patterns that is coded with its corresponding complement as fields in a single frame. This video signal is sent to the projector and connected to the camera's genlock pin. The sum of its fields gives a texture image and the difference provides projected stripe colors and boundaries. The complete geometry and texture acquisition is illustrated in Fig. <ref type="figure">3</ref>.</p><p>This system is suitable for facial expression capture because it maintains a good balance between texture, geometry and motion detection. Our videos were obtained by projecting 25-35 stripes over the face and the average resolutions are: vertical = 12 points/cm and horizontal = 1.25 points/cm (right bottom window of Fig. <ref type="figure">4</ref>). We used a Sony HyperHAD camera and an Infocus LP-70 projector. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">3D Data Registration</head><p>The acquired range data need to be registered for the following analysis. The range points are first smoothed by radial basis functions (RBF). We build a coarse mesh model with 268 vertices and 244 quadrangles for face tracking. A coarse generic model is fitted manually at the first frame. A robust feature tracker from Nevengineering [23] provides the 2D positions of 22 prominent feature points. The mesh's projection was warped by the 22 feature points. The depth of the vertex was recovered by minimizing the distance between the mesh and the range data <ref type="bibr" target="#b22">[24]</ref>. An example of the 3D viewer is shown in Fig. <ref type="figure">4</ref>. The left bottom window shows the range data with the fitted mesh. The right bottom window is the texture image with the projected 3D points. Fig. <ref type="figure">5 (a)</ref> shows the texture image with the 22 tracked feature points. Fig. <ref type="figure">5 (b)</ref> shows the dense mesh with 4856 vertices and 4756 quadrangles. The dense model is used for the synthesis of new expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Generalized Expression Manifold</head><p>We built the generalized expression manifold by transferring the facial deformations in the training videos to a standard model. The standard model serves as the interface between the models in the training videos and models in the testing videos. The generalized manifold, that is the expression manifold of the standard model, includes all information in the training videos. The more training data we have, the better it approximates the true manifold. We can define expression similarity on this manifold and use it to search the optimal approximation for any kind of expression. The expression synthesis will involve only the standard model and target model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Deformation Transfer with Temporal Constraints</head><p>Sumner <ref type="bibr" target="#b9">[10]</ref> proposed a novel method to transfer the deformation of the source triangle mesh to the target one by minimizing the transformation between the matching triangles while enforcing the connectivity. This optimization problem can be rewritten in linear equations: </p><formula xml:id="formula_1">c A b A A S ' , ' = = . (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>The result is unique up to a global translation. We fix the rigid vertex, such as inner eyes corners to resolve the global position. </p><formula xml:id="formula_3">t i = =</formula><p>k is the length of the video. We add a constraint for temporal coherence and the optimization problem becomes</p><formula xml:id="formula_4">∑ = ∂ ∂ + - k t t t t v v v v t xm xm Am d k n k n ,..., 1 2 2 2 2 ... ,..., ... || || || * || min 1 1 1 1 σ (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>where σ is the weight for temporal smoothing. This problem can be solved in a progressive way by approximating xm is the vertex locations of the undeformed target mesh. Eq. ( <ref type="formula" target="#formula_4">3</ref>) can be rewritten as </p><formula xml:id="formula_6">∑ = - k t t t v v v v p xm Q k n k n ,...,</formula><formula xml:id="formula_7">+ = + = t t t xm d Am p Q I Am Am Q Q σ σ σ is chosen to guarantee I Am Am σ + '*</formula><p>is symmetric positive matrix. Q always exists, while it is not needed to solve Q explicitly. Eq. ( <ref type="formula">4</ref>) has a closed solution:</p><formula xml:id="formula_8">t t p Q xm Q Q ' * ' = .</formula><p>For efficiency, we compute and store the LU factorization of Q'Q only once.</p><p>We separate the motion of the tracked source mesh into a global transformation due to head movement and a local deformation due to facial expression. The local deformation is used for facial expression (deformation) transfer.</p><p>Fig. <ref type="figure" target="#fig_4">6</ref> shows an example of transferring the source mesh to the target mesh with synthesized texture data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Lipschitz Embedding</head><p>We get the deformation vectors of the standard model as</p><formula xml:id="formula_9">k t R x n t s ,... 1 , 3 * , = ∈</formula><p>, where n is the number of vertices; s is the number of videos and k is the length of the video. We normalize the duration of every video by re-sampling the deformation vectors at equal intervals. The interpolation is implemented by a cubic spline. We build the manifold by using the coarse mesh such that expression can be recognized quickly. The dense mesh of the standard model is saved for synthesis of the new expression.</p><p>Lipschitz embedding <ref type="bibr" target="#b23">[25]</ref> is a powerful embedding method used widely in image clustering and image search. For a finite set of input data S , Lipschitz embedding is defined in terms of a set R of subsets of S , } ,..., , { </p><formula xml:id="formula_10">F such that )) ; ( );..., ; ( ); ; ( ( ) ( 2 1 k A o d A o d A o d o F = .</formula><p>For our experiments, we used six reference sets, each of which contains only the deformation vectors of one kind of basic facial expression at its apex. The embedded space is six dimensional. The distance function in the Lipschitz embedding should reflect the distance between points on the manifold. We use the geodesic manifold distance <ref type="bibr" target="#b4">[5]</ref> to preserve the intrinsic geometry of the data. After we apply the Lipschitz embedding with geodesic distance to the training set, there are six basic paths in the embedded space, emanating from the center that corresponds to the neutral image. The images with blended expression lie between the basic paths.</p><p>An example of the generalized expression manifold projected on its first three dimensions can be found in the middle of the second row of Fig. <ref type="figure" target="#fig_0">1</ref>. Points with different colors represent embedded deformation vectors of different expressions. Anger: red; Disgust: green; Fear: blue; Sad: cyan; Smile: pink; Surprise: yellow. In the embedded space, expressions can be recognized by using the probabilistic model described in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Probabilistic Model on the Generalized Manifold</head><p>The goal of the probabilistic model is to exploit the temporal information in video sequences in order to recognize expression correctly and find the optimal replacement for expression editing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Model Learning</head><p>On the standard model, assume there are K videos sequences for each kind of basic ex- pression } 6 ,..., 1 { , = S S</p><p>. The embedded vector for the ith frame in the jth video for expression S is . We compute a cluster frequency measure ) 6 ..</p><formula xml:id="formula_11">= ∈ ∈ = + S K j c I c I T n i j s n i j s n n 2 , 1 n n T<label>1 , .. 1 , &amp; ( # 2 1 , , 1 , , 2 , 1 =</label></formula><p>represents how many time the situation occurs in all videos that one frame belongs to cluster </p><formula xml:id="formula_12">⎩ ⎨ ⎧ = = otherwise scale T T c c p n n n n n n , * 0 , ) | ( 2 , 1 2 , 1 1 2 δ</formula><p>where δ is a small empirical number. Scale and δ are selected such that  is also a PMF for the same reason. Using Eq. ( <ref type="formula">5</ref>), the Markov property, statistical independence, and time recursion in the model, we can derive:  </p><formula xml:id="formula_13">∏ = - - - = = t i i i i i i i i i i t t</formula><formula xml:id="formula_14">t i t t i i i i i i i i i c S c S c t t t t t - = - - - - ∏ ∫ ∫ ∫ ∫ ∫ - - =<label>(6)</label></formula><p>which can be computed by the priors and the likelihood</p><formula xml:id="formula_15">t i c S I p i i i ,..., 1 ), , | ( =</formula><p>. This provides us the probability distribution of the expression categories, given the sequence of embedded deformation vectors of the standard model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Expression Editing</head><p>The user can define the any expression editing function F as needed. F:</p><formula xml:id="formula_16">6 6 R R → . ] ,..., , [ )) 6 ( ),..., 1 ( ( 6 2 1 q q q S p S p F = = = where ∑ = = 6 1 1 i i q</formula><p>, q is the new likelihood of one kind of facial expression. For example, if we want to edit all sadness (S=1) videos to anger (S=2), the mapping function can be defined as F (p (S=1), p (S=2), …, p (S=6))=</p><p>[p (S=2), p (S=1), …, p (S=6)], when p (S=1)&gt; γ .</p><p>This function will increase the likelihood of anger when the sadness is detected, that is, its likelihood is above a threshold γ .</p><p>The system automatically searches for the embedded vector with likelihood that is closest to the "range" expression. It first looks for the cluster whose center has the closest likelihood. In that cluster, the point closest to the embedded vector of the input frame is selected. We transfer the corresponding deformation vector back to the model in the new video. The deformation vector is blended with the deformation at the previous frame to ensure smooth editing. The synthesized 2D image uses the head pose in the real input frame and the texture information of the dense model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Results</head><p>We collected 3D training videos from 6 subjects (3 males, 3 females). Every subject performed six kinds of basic expressions. The total number of frames in the training videos is 2581. We use Magic Morph morphing software to estimate the average of the training faces, and we use that average as the standard model. The standard model only contains geometrical data, no texture data. It will approach the "average" shape of human faces when the number of training subjects increases. Fig. <ref type="figure" target="#fig_10">7</ref> includes some examples of the mesh fitting results. We change the viewpoints of 3D data to show that the fitting is very robust. A supplementary video is available at http://ilab.cs.ucsb.edu/demos/AMFG05.mpg. This video gave a snapshot of our database by displaying the texture sequences and 3D view of the range data with the fitted mesh at the same time.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We introduced a novel framework for automatic facial expression analysis in 3D videos. A generalized manifold of facial expression is learned through a 3D expression database. This database provides a potential to learn the complete deformation space of human faces when more and more subjects are added in. Expression recognition and editing is accomplished automatically by using the probabilistic model on the generalized expression manifold of the standard model.</p><p>The current input is 3D videos. We plan to take 2D video input by using a system like <ref type="bibr" target="#b12">[13]</ref>. The output video is a synthesized face with a new expression. How to separate and keep the deformation due to speech and merge the synthesized face smoothly with the background in videos <ref type="bibr" target="#b24">[26]</ref> are important topics for the future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. System diagram</figDesc><graphic coords="3,104.25,56.33,222.22,153.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Decoding stripe transitions</figDesc><graphic coords="5,145.89,124.97,62.40,53.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>I 1 I 1 'FrameFig. 3 .Fig. 4 .Fig. 5 .</head><label>11345</label><figDesc>Fig. 3. Input video frames, and the texture and geometry output streams at 30 fps</figDesc><graphic coords="6,109.89,211.85,222.58,152.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 .</head><label>1</label><figDesc>norm, or the square root of the sum of the square matrix elements. of the unknown deformed target mesh. x is a vector of the locations of n v v ,..., c is a vector containing entries from the source transformations, and A is a large sparse matrix that relates x to c, which is determined by the undeformed target mesh. This classic least-square optimization problem has closed form solution as b Sx = , where</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Example of deformation transfer with texture synthesis. The first row is the texture image of the source video at frames 1, 12, and 24. The second row is the dense mesh of the target face with transferred deformation. The first image of the third row is the texture image of the undeformed target model. The second and the third images are the corresponding synthesized faces by the deformed dense mesh.</figDesc><graphic coords="8,104.13,256.36,222.24,280.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>to the expression intensity of the cluster center, varying from 0 to 1. By Bayes' rule, between different expressions can be computed as the transition between the clusters:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Mesh fitting for training videos. Images in each row are from the same subject. The first column is the neutral expression. The second and third columns represent large deformation during the apex of expressions.</figDesc><graphic coords="12,109.89,343.73,222.12,194.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Two different styles of the anger in training videos transferred to the standard mesh model. The first row and second row is images of anger and the corresponding deformed standard mesh model. The first to the third column is one style of anger at frame 1, 6, and 29. The fourth to sixth column is another style of anger at frames 1, 20, and 48.</figDesc><graphic coords="13,42.57,56.33,345.44,145.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9.Expression editing examples. First row is from the input video of sadness. We define the expression mapping function as Eq. 7. The second row is the deformed dense mesh by our algorithm. The third row is the output: the first image is unchanged, the following images are synthesized anger faces by the expression mapping function.</figDesc><graphic coords="13,109.89,264.89,222.42,149.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 8</head><label>8</label><figDesc>Fig. 8 shows examples of deformation transfer. The motions of the training videos are well retargeted on the standard model.Fig.9is an example of expression editing. The system recognized the sadness correctly and synthesized new faces with anger expression correspondingly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 9</head><label>9</label><figDesc>Fig. 8 shows examples of deformation transfer. The motions of the training videos are well retargeted on the standard model.Fig.9is an example of expression editing. The system recognized the sadness correctly and synthesized new faces with anger expression correspondingly.</figDesc></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Expression Recognition</head><p>Given a probe video, the facial deformation is first transferred to the standard model, and the deformation vector is embedded as ,... , the transition probability can be computed as:</p><p>We define the likelihood computation as follows</p><p>where c u is the center of cluster c , c σ is the variation of cluster c .</p><p>Given this model, our goal is to compute the posterior </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Morphable Model for the Synthesis of 3D Face</title>
		<author>
			<persName><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH</title>
		<imprint>
			<biblScope unit="page" from="187" to="194" />
			<date type="published" when="1999">1999</date>
			<pubPlace>Los Angeles, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A New Physical Model with Multi-layer Architecture for Facial Expression Animation Using Dynamic Adaptive Mesh</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="339" to="352" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">High Resolution Acquisition, Learning and Transfer of Dynamic 3-D Facial Expressions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eurographics</title>
		<meeting>Eurographics<address><addrLine>Grenoble, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Camera-Projector System for Real-Time 3D Video</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Vieira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Velho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Carvalho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Workshop on Projector-Camera Systems</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Nonlinear Dimensionality Reduction by Locally Linear Embedding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Global Geometric Framework For Nonlinear Dimensionality Reduction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Separating Style and Content with Bilinear Models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenebaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation J</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1247" to="1283" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multilinear Subspace Analysis for Image Ensembles</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O</forename><surname>Vasilescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition</title>
		<meeting>Computer Vision and Pattern Recognition<address><addrLine>Madison, WI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Separating Style and Content on a Nonlinear Manifold</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition</title>
		<meeting>Computer Vision and Pattern Recognition<address><addrLine>Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deformation Transfer for Triangle Meshes</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sumner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Popovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<pubPlace>Los Angeles, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Probabilistic Expression Analysis on Manifolds</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition</title>
		<meeting>Computer Vision and Pattern Recognition<address><addrLine>Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Stable Real-time 3D Tracking Using Online and Offline Information</title>
		<author>
			<persName><forename type="first">L</forename><surname>Vacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1385" to="1391" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Statistical Cue Integration in DAG Deformable Models</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Goldenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vogler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="801" to="813" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Comparison Between Geometry-based and Gabor-wavelets-based Facial Expression Recognition Using Multi-layer Perceptron</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Akamatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Automatic Face and Gesture Recognition</title>
		<meeting><address><addrLine>Nara, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Facial Expression Space Learning. Pacific Graphics</title>
		<author>
			<persName><forename type="first">E</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Comprehensive Database for Facial Expression Analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="46" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The CMU Pose, Illumination, and Expression Database</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bsat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1615" to="1618" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Expression Cloning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<pubPlace>Los Angeles, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Hierarchical Approach to Interactive Motion Editing for Human-like Figures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH</title>
		<imprint>
			<biblScope unit="page" from="39" to="48" />
			<date type="published" when="1999">1999</date>
			<pubPlace>Los Angeles, CA, Los Angeles, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Geometry-Driven Photorealistic Facial Expression Synthesis</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Symposium on Computer Animation</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An Example-Based Approach for Facial Expression Cloning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pyun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Siggraph Symposium on Computer Animation</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Sá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Velho</surname></persName>
		</author>
		<title level="m">(b,s)-BCSL: Structured Light Color Boundary Coding for 3D photography. Int. Fall Workshop on Vision, Modeling, and Visualization</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Free-Form Deformation of Solid Geometric Models</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Sederberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Parry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>ACM SIGGRAPH</publisher>
			<biblScope unit="page" from="151" to="159" />
			<pubPlace>Dallas, TX</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On Lipschitz Embedding of Finite Metric Spaces in Hilbert Space</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bourgain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Israel J. Math</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="46" to="52" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exchanging Faces in Images</title>
		<author>
			<persName><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scherbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eurographics</title>
		<meeting>Eurographics<address><addrLine>Grenoble, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
