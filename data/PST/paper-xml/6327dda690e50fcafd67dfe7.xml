<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GRAPH CONTRASTIVE LEARNING WITH CROSS-VIEW RECONSTRUCTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qianlong</forename><surname>Wen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Notre Dame</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhongyu</forename><surname>Ouyang</surname></persName>
							<email>zouyang2@nd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Notre Dame</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chunhui</forename><surname>Zhang</surname></persName>
							<email>2tchunhuizhang@brandeis.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Brandeis University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yiyue</forename><surname>Qian</surname></persName>
							<email>yqian5@nd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Notre Dame</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Notre Dame</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
							<email>chuxuzhangu@brandeis.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Brandeis University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GRAPH CONTRASTIVE LEARNING WITH CROSS-VIEW RECONSTRUCTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph self-supervised learning is commonly taken as an effective framework to tackle the supervision shortage issue in the graph learning task. Among different existing graph self-supervised learning strategies, graph contrastive learning (GCL) has been one of the most prevalent approaches to this problem. Despite the remarkable performance those GCL methods have achieved, existing GCL methods that heavily depend on various manually designed augmentation techniques still struggle to alleviate the feature suppression issue without risking losing task-relevant information. Consequently, the learned representation is either brittle or unilluminating. In light of this, we introduce the Graph Contrastive Learning with Cross-View Reconstruction (GraphCV), which follows the information bottleneck principle to learn minimal yet sufficient representation from graph data. Specifically, GraphCV aims to elicit the predictive (useful for downstream instance discrimination) and other non-predictive features separately. Except for the conventional contrastive loss which guarantees the consistency and sufficiency of the representation across different augmentation views, we introduce a cross-view reconstruction mechanism to pursue the disentanglement of the two learned representations. Besides, an adversarial view perturbed from the original view is added as the third view for the contrastive loss to guarantee the intactness of the global semantics and improve the representation robustness. We empirically demonstrate that our proposed model outperforms the state-of-the-art on graph classification task over multiple benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph representation learning (GRL) has attracted significant attention due to its widespread applications in the real-world interaction systems, such as social, molecules, biological and citation networks <ref type="bibr" target="#b12">(Hamilton et al., 2017b)</ref>. The current state-of-the-art supervised GRL methods are mostly based on Graph Neural Networks (GNNs) <ref type="bibr" target="#b19">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b35">Veličković et al., 2018;</ref><ref type="bibr" target="#b11">Hamilton et al., 2017a;</ref><ref type="bibr" target="#b43">Xu et al., 2019)</ref>, which require a large amount of task-specific supervised information. Despite the remarkable performance, they are usually limited by the deficiency of label supervision in real-world graph data due to the fact that it is usually easy to collect unlabeled graph but very costly to obtain enough annotated labels, especially in certain fields like biochemistry. Therefore, many recent works <ref type="bibr" target="#b28">(Qiu et al., 2020;</ref><ref type="bibr" target="#b13">Hassani &amp; Khasahmadi, 2020a;</ref><ref type="bibr" target="#b32">Sun et al., 2019)</ref> study how to fully utilize the unlabeled information on graph and further stimulate the application of self-supervised learning (SSL) for GRL where only limited or even no label is needed.</p><p>As a prevalent and effective strategy of SSL, contrastive learning follows the mutual information maximization principle (InfoMax) <ref type="bibr" target="#b36">(Veličković et al., 2019)</ref> to maximize the agreements of the positive pairs while minimizing that of the negative pairs in the embedding space. However, the graph contrastive learning paradigm guided by the InfoMax principle is insufficient to learn robust and transferable representation. State-of-the-art GCL methods <ref type="bibr" target="#b28">(Qiu et al., 2020;</ref><ref type="bibr" target="#b13">Hassani &amp; Khasahmadi, 2020a;</ref><ref type="bibr" target="#b47">You et al., 2020)</ref> usually rely on augmentor(s) tp¨q (e.g., Identity, Subgraph Sampling, Node Dropping, Edge Removing and Attributes Masking.) applied to the anchor graph G to generate a positive pair of graphs t 1 pGq and t 2 pGq. Then, the graph feature encoder f will be trained to ensure The usual optimization result of graph contrastive learning, where the shared features of two augmentation view is extracted for the learned representation z. Owing to the lack of supervision or domain knowledge, redundant and biased information (shadow area) is usually included in z; (b) G p cover the feature subset which is sufficient to make correct graph label identification (Ipy; G | G p q " 0), other features (G c ) is either useless or misguiding; (c) G p and G c are supposed to be mutually disentangled with each other (IpG p ; G c q " 0). The union of them cover all the features of original data. the representation consistency within the positive pair, i.e., z " f pt 1 pGqq " f pt 2 pGqq. Consequently, such training strategy is heavily dependent on the choice and strength of graph augmentation techniques. To be more specific, moderate graph augmentation will push encoders to capture redundant and biased information <ref type="bibr" target="#b33">(Tschannen et al., 2019)</ref>, which could inadvertently suppress the space of important predictive features and negatively affect the representation transferability via the so-called "shortcut" solution <ref type="bibr" target="#b9">(Geirhos et al., 2020;</ref><ref type="bibr" target="#b25">Minderer et al., 2020)</ref>. A more intuitively illustration is provided in the Figure <ref type="figure" target="#fig_0">1</ref> (a), where the shared part of the two augmentation view include both predictive information (the overlapping area with y) and non-predictive information (shadow area). Such optimization result usually yield lower contrastive loss, however, it has been empirically proved that the redundant information could lead to poor robustness <ref type="bibr" target="#b29">(Robinson et al., 2021)</ref>, especially under the out-of-distribution (OOD) setting <ref type="bibr" target="#b46">(Ye et al., 2021)</ref>. We provide a showcase example in Appendix A to illustrate the OOD scenario on graph learning task. On the other hand, overly aggressive augmentation may easily lead to another extreme where many predictive features are randomly dropped and the learned representation does not contain sufficient predictive information for downstream instance discrimination. Recent works <ref type="bibr">(Suresh et al., 2021;</ref><ref type="bibr" target="#b21">Li et al., 2022;</ref><ref type="bibr" target="#b47">You et al., 2021)</ref> propose to use automated augmentations to extract the invariant rationale features <ref type="bibr" target="#b39">(Wu et al., 2022b;</ref><ref type="bibr">a)</ref>. These methods assume the most salient sub-structure (those are resistant to graph augmentation) is sufficient to make rational and correct label identification, and thereby implement trainable augmentation operations (e.g., edge deleting, node dropping) to strictly regularize the graph topological structure. Despite that these methods can alleviate the aforementioned feature suppression issue to some extent, they still suffer from inherent limitation. The harsh regularization may force the encoders focusing on the easy-learned "shallower" features (e.g. graph size and node degree), which might be helpful under certain domains but not necessarily for others <ref type="bibr" target="#b3">(Bevilacqua et al., 2021)</ref>, thus fail to guarantee stronger robustness. Therefore, the GCL methods guided with the saliency philosophy is not flexible enough to balance the representation sufficiency and robustness without the guidance of explicit domain knowledge. To reconcile the robustness and sufficiency of the learned representation, a method which can reduce redundant and biased information without sacrificing the sufficiency of the predictive graph features is in urgent need.</p><p>Recently, the information bottleneck (IB) principle <ref type="bibr">(Tishby et al., 2000)</ref> has been introduced to the graph learning, which encourages extracting minimal yet sufficient information for representation learning. The core idea of IB principle is in accordance with the ultimate optimization objective to solve the feature suppression issue <ref type="bibr" target="#b29">(Robinson et al., 2021)</ref>, thus shed more light on this problem. Moreover, the representation learning guided by the IB principle has been empirically proved to generate more robust and transferable representations at different domains <ref type="bibr" target="#b38">(Wu et al., 2020)</ref>. Therefore, a graph contrastive learning framework in accordance with the IB principle is promising in balancing the representation robustness and sufficiency. Given an input graph G, we denote G p and G c as its predictive feature subset and the complementary non-predictive feature subset, respectively. According to the assumption of recent studies about rationale invariance discover <ref type="bibr" target="#b39">(Wu et al., 2022b;</ref><ref type="bibr">a)</ref>, the two features subsets would the satisfy Ipy; G | G p q " 0 (sufficiency condition) and disentanglement condition (i.e., IpG p ; G c q " 0). We illustrate the relations among the two Pre-print feature subsets and G in Figure <ref type="figure" target="#fig_0">1</ref> (b) and (c). It is inevitable that the learned representation maintains some redundant information for a specific downstream task. However, a GCL framework under the guidance of the IB principle is expected to suppress the feature space of G c as much as possible while keeping the predictive feature G p intact simultaneously in the learned representation.</p><p>In this paper, we propose the novel Graph Contrastive Learning with Cross-View Reconstruction, named GraphCV, to pursue the optimization objective of the IB principle. Specifically, GraphCV consists of a graph encoder followed with two decoders that are trained to extract information specific to the predictive and non-predictive features, respectively. To approximate the disentanglement objective, we propose the reconstruction-based representation learning scheme, including the intraview and inter-view reconstructions, to reconstruct the original learned representation with the two separated feature subsets. Furthermore, the encoded representation from the original view perturbed in the adversarial fashion serves as the third view when computing the contrastive loss, apart from the predictive relevant representations of the two augmentation views, to further improve the representations' robustness and prevent them from collapsing into partial or even trivial ones. We provide theoretical analysis to show that GraphCV is capable to learn minimal sufficient representations. Finally, we conduct experiments to validate the effectiveness of GraphCV over the commonly-used graph benchmark datasets. The experimental results demonstrate that GraphCV achieves significant performance gains over different datasets and settings compared with state-of-the-art baselines.</p><p>The main contributions of this work are summarized from three aspects: (i) We propose the GraphCV to alleviate the feature suppression issue with the cross-view reconstruction mechanism; (ii) We provide solid theoretical analysis on our model designs; (iii) Thorough experiments are conducted to demonstrate the robustness and transferability of the learned representations via GraphCV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>2.1 GRAPH REPRESENTATION LEARNING In this work, we focus on the graph-level task, let G " tG i " pV i , E i qu N i"1 denote a graph dataset with N graphs, where V i and E i are the node set and edge set of graph G i , respectively. We use x v P R d and x e P R d to denote the attribute vector of each node v P V i and edge e P E i . Each graph is associated with a label, denoted as y i , the goal the graph representation learning is to learn an encoder f : G i Ñ R d so that the learned representation z i " f pG i q is sufficient to predict y i related to the downstream task. We clarify the sufficiency of z i as containing no less information of the label of G i <ref type="bibr" target="#b0">(Achille &amp; Soatto, 2018)</ref>, and it is formulated as:</p><formula xml:id="formula_0">IpG i ; y i | z i q " 0,<label>(1)</label></formula><p>where I p; q denotes the mutual information between two variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">CONTRASTIVE LEARNING</head><p>Contrastive Learning (CL) is a self-supervised representation learning method which leverages instance-level identity for supervision. During the training phase, each graph G firstly goes through proper data augmentation to generate two data augmentation views t 1 pGq and t 2 pGq, where t 1 p¨q and t 2 p¨q are two augmentation operators. Then, the CL method encourages the encoder f (a backbone network plus a projection layer) to map t 1 pxq and t 2 pxq closer in the hidden space so that the learned representations z 1 and z 2 maintain all the information shared by t 1 pGq and t 2 pGq. The learning of the encoder is usually directed by a contrastive loss, such as NCE loss <ref type="bibr" target="#b41">(Wu et al., 2018b)</ref>, InfoNCE loss (van den <ref type="bibr" target="#b34">Oord et al., 2018)</ref> and NT-Xent loss <ref type="bibr" target="#b5">(Chen et al., 2020)</ref>. In Graph Contrastive Learning (GCL), we usually adopt a GNN, such as GCN <ref type="bibr" target="#b19">(Kipf &amp; Welling, 2017)</ref> or GIN <ref type="bibr" target="#b43">(Xu et al., 2019)</ref>, as the backbone network, and the commonly-used graph data augmentation operators <ref type="bibr" target="#b47">(You et al., 2020)</ref>, such as node dropping, edge perturbation, subgraph sampling, and attribute masking.</p><p>All the GCL-based methods are built on the assumption that augmentations do not break the sufficiency requirement to make correct prediction. Here, we follow <ref type="bibr" target="#b8">(Federici et al., 2020)</ref>   representation is dominated by non-predictive features in SSL <ref type="bibr" target="#b6">(Chen et al., 2021)</ref> and it is no more informative enough to make correct prediction. Therefore, feature suppression is not just a prevalent issue in supervised learning, but also in SSL. Due to the page limitation, we provide more discussion about the relation between feature suppression and GCL in Appendix B</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED MODEL</head><p>In this section, we introduce the details of our proposed GraphCV whose framework is shown in Figure <ref type="figure" target="#fig_1">2</ref>. Corresponding theoretical analysis are provided to justify the rationality of our designs. Before diving into the details of GraphCV, we briefly introduce the overall framework of our model.</p><p>The proposed GraphCV model is designed in accordance with the IB principle to extract minimal yet sufficient representation through the designed cross-view reconstruction mechanism. Given f p¨q as the graph encoder, we aim to map the graph representation z " f pGq P R d into two different feature spaces pz p , z c q, where z p P R d is expected to be specific to the predictive information G p , and z c P R d is optimized to elicit the complementary non-predictive factors G c . Later, we reconstruct the representation z with the feature subsets mapped from same and different augmentation views to approximate the disentanglement objective demonstrated in Figure <ref type="figure" target="#fig_0">1</ref>. By separating the learned representation into two sets of disentangled features and later utilizing them to reconstruct the two, we alleviate the feature suppression issue <ref type="bibr" target="#b29">(Robinson et al., 2021)</ref> at no cost of information sufficiency. We further add extra regularization to guarantee z p does not collapse into shallow or partial features during the reconstruction process. We will introduce more details of GraphCV in the later contents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">DISENTANGLEMENT BY CROSS-VIEW RECONSTRUCTION</head><p>In GCL, we usually leverage a graph encoder, such as a GCN <ref type="bibr" target="#b19">(Kipf &amp; Welling, 2017)</ref> or a GIN <ref type="bibr" target="#b43">(Xu et al., 2019)</ref>, to encode the graph data into its representation. There are multiple choices of graph encoders in GCL, including GCN <ref type="bibr" target="#b19">(Kipf &amp; Welling, 2017)</ref> and GIN <ref type="bibr" target="#b43">(Xu et al., 2019)</ref>, etc. In this work, we adopt GIN as the backbone network f for simplicity. Note that any other commonly-used graph encoders can also be adapted to our model. Given two augmentation views t 1 pGq and t 1 pGq (where t 1 p¨q and t 2 p¨q are IID sampled from the same family of augmentation T ), we firstly use the encoder f p¨q to map them into a lower dimension hidden space for the two embeddings, z 1 and z 2 . Instead of directly maximizing the agreement between the two representations z 1 and z 2 , we further feed each of them into a pair decoders pg p , g c q (both of them are MLP-based networks or GNN) and optimize the two decoders to map each of the presentation into the two disentangled feature sub-spaces:</p><formula xml:id="formula_1">rz p " g p pf ptpGqqq, z c " g c pf ptpGqqqs ,<label>(3)</label></formula><p>where a pair of embeddings for both t 1 pGq and t 2 pGq are generated. Ideally, z p 1 and z p 2 suffice the mutual redundancy assumption stated in 2.2 because t 1 pGq and t 1 pGq are augmented from the same original graph, and thus naturally share the same predictive factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-print</head><p>Here, we clarify the lower bound of the mutual information between one augmented view and the two mapped representations learned from the other augmented view in Theorem 1.</p><p>Theorem 1 Suppose f p¨q is a GNN encoder as powerful as 1-WL test. Let z p 1 and z p 2 be specific to the predictive information of G, meanwhile z c 1 and z c 2 account for the non-predictive factors of t 1 pGq and t 2 pGq. Then we have:</p><formula xml:id="formula_2">I pt 1 pGq; z p 2 , z c 2 q ě I pz p 1 ; z p 2 q</formula><p>where G P G and t 1 p¨q, t 2 p¨q P T .</p><p>The detailed proof is provided in Appendix E. Given the lower bound, we substitute the objective by the mutual information between the two representations in the predictive view (z p 1 and z p 2 ) to maximize the consistency between the information of the two views. Therefore, we derive the objective function ensuring view invariance as follows:</p><formula xml:id="formula_3">L pre " 1 N N ÿ i"1 L CL pz p 1,i , z p 2,i q, (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where L CL p¨q is the adopted InfoNCE loss (van den <ref type="bibr" target="#b34">Oord et al., 2018)</ref>. To further pursue the feature disentanglement as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>(c), we thus propose the cross-view reconstruction mechanism. To be specific, we would like the representation pair pz p , z c q within and cross the augmentation views be able to recover the raw data so that the two objectives can be approached simultaneously. Due to the fact that graphs are non-Euclidean structured data, we instead try to recover z " f ptpGqq given pz c and z p q.</p><p>More specifically, we first perform the reconstruction within the augmentation view, namely mapping pz p w , z c w q to z w , where w P t1, 2u representing the augmentation view. Then, we define the pz p w 1 , z c w q as a cross-view representation pair and the reconstruction procedure is repeated on it to predict z w , aiming to ensure z p and z c is optimized to approximate mutual disentanglement, where w " 1, w 1 " 2 or w " 2, w 1 " 1. Intuitively, the reconstruction process is capable of separating the information of the shared features sets from the one resided in the unique feature sets between the two augmentation views. Since the two IID sampled augmentation operators (t 1 p¨q and t 2 p¨q) are expected to preserve the predictive/rational features while varying the augmentation-related ones, we disentangle the rational features from G according to the rationale discover studies <ref type="bibr" target="#b4">(Chang et al., 2020)</ref> to ensure the features' robustness for downstream tasks. Here, we formulate the reconstruction procedures as:</p><formula xml:id="formula_5">z r w " g r pz p w d z c w q , z cr w " g r pz p w 1 d z c w q ,<label>(5)</label></formula><p>where g r is the parameterized reconstruction model and d is the free-to-choose fusion operator, such as element-wise product or concatenation. The reconstruction procedures are optimized by minimizing the entropy H pz w | z p w 1 , z c w q, where w " w 1 or w ‰ w 1 . Ideally, we reach the optimal sufficiency and disentanglement conditions illustrated in Figure <ref type="figure" target="#fig_0">1</ref> </p><formula xml:id="formula_6">(b) and (c) iff H pz w | z p w 1 , z c w q " ´Eppzw,z p w 1 ,z c w q rlog p pz w | z p w 1 , z c</formula><p>w qs " 0, where z w is exactly recovered given its complementary representation and the predictive representation of any view. Nevertheless, the condition probability p pz w | z p w 1 , z c w q is intractable, we hence use the variational distribution approximated by g r instead, denoted as q pz w | z p w 1 , z c w q. We provide the upper bound of H pz w | z p w 1 , z c w q in Theorem 2.</p><p>Theorem 2 Assume q is a Gaussian distribution, g r is the parameterized reconstruction model which infers z w from pz p w 1 , z c w q. Then we have:</p><formula xml:id="formula_7">H pz w | z p w 1 , z c w q ď }z w ´gr pz p w 1 d z c w q} 2 2 where w " w 1 or w ‰ w 1 .</formula><p>The detailed proof is demonstrated in Appendix E. Since we adopt two augmentation views, the objective function constraining representation disentanglement can be formulated as:</p><formula xml:id="formula_8">L recon " 1 2N N ÿ i"1 2 ÿ w"1 " › › z w,i ´zr w,i › › 2 2 `› › z w,i ´zcr w,i › › 2 2 ı .<label>(6)</label></formula><p>Pre-print</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ADVERSARIAL CONTRASTIVE VIEW</head><p>With the cross-view reconstruction mechanism above, the two learned representations stated above are optimized towards the disentangled manner. However, it is still necessary to further prevent the learned predictive representation from focusing on the partial features, because we do not have access to the explicit domain knowledge and such small scope will increase the risk of shortcut solution. Therefore, we extend the Equation 4 to three contrastive views and add an extra global view without topological perturbation as the third views to guarantee the learned z p maintain the global semantics instead of partial or even trivial features, i.e., z p 1 " G and z p 2 " G. During the experiments, we find an adversarial graph sample perturbed from original graph view can help the model achieve stronger robustness. A possible explanation is that there is still redundant information that is not predictive left in the shared information of the two z p 's in the two augmentation views, especially when the implemented augmentations are moderate. An adversarial view may further alleviate redundancy. We define the adversarial objective as follows:</p><formula xml:id="formula_9">δ ˚" argmax }δ} 8 ď L adv pt 1 pGq, t 2 pGq, G `δq ,<label>(7)</label></formula><p>where the adversarial sample G `δ together with the two augmentation views, i.e., t 1 pGq and t 2 pGq are employed as the positive pair. Our crafted perturbation is spurred by recent work <ref type="bibr" target="#b45">(Yang et al., 2021)</ref> that add perturbation δ on the output of first hidden layer h p1q , since it is empirically proved to generate more challenging views than adding perturbation on the initial node feature. Therefore, the adversarial contrastive objective is defined as:</p><formula xml:id="formula_10">L adv " 1 N N ÿ i"1 max δ ˚"L CL `zp 1,i , G `δ˚˘`L CL `zp 2,i , G `δ˚˘‰ .<label>(8)</label></formula><p>where the optimized perturbation δ 1 is solved by projected gradient descent (PGD) <ref type="bibr" target="#b24">(Madry et al., 2018)</ref>. Finally, we derive the joint objective of GraphCV by combining all of objectives above together. The joint objective is as follow:</p><formula xml:id="formula_11">min f,g E GPG " L pre `λr L recon `λa max }δ} 8 ď L adv  ,<label>(9)</label></formula><p>where λ r and λ a are the coefficients to balance the magnitude of each loss term. Our proposed model is able to learn optimal representation illustrated in Figure <ref type="figure" target="#fig_0">1</ref>(c) with the joint objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we demonstrate the empirical evaluation results of GraphCV on public graph benchmark datasets under different settings. Ablation study and hyper-parameter analysis are also conducted to evaluate the effectiveness of the designs in GraphCV. We further compare the robustness of GraphCV with the adversarial training-based GCL method. More content about dataset statistics, training details and other empirical analysis are provided in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EXPERIMENTAL SETUPS</head><p>Datasets. For unsupervised learning setting, we evaluate our model on five graph benchmark datasets from the field of bioinformatics, including MUTAG, PTC-MR, NCI1, DD, and PROTEINS, and other four from the field of social network, which are COLLAB, IMDB-B, RDT-B, and IMDB-M, for the task of graph-level property classification. For the transfer learning setting, we follow previous work <ref type="bibr" target="#b47">(You et al., 2020;</ref><ref type="bibr" target="#b44">Xu et al., 2021b</ref>) to pretrain our model on the ZINC-2M dataset, which cotains 2 million unlabeled molecule graphs sampled from MoleculeNet <ref type="bibr" target="#b40">(Wu et al., 2018a)</ref>, then evaluate its performance on eight binary classification datasets from chemistry domain, where the eight datasets are splitted according to the scaffold to simulate the out-of-distribution scenario in real-world. Additionally, We use ogbg-molhiv from Open Graph Benchmark Dataset <ref type="bibr" target="#b16">(Hu et al., 2020a)</ref> to evaluate our model over large-scale dataset under semi-supervised setting. More details about dataset statistics are included in Appendix C.</p><p>Baselines. Under the unsupervised representation learning setting, we compare GraphCV with the eight SOTA self-supervised learning methods GraphCL <ref type="bibr" target="#b47">(You et al., 2020)</ref>, InfoGraph <ref type="bibr" target="#b32">(Sun et al., 2019)</ref>, MVGRL <ref type="bibr" target="#b13">(Hassani &amp; Khasahmadi, 2020a)</ref>, AD-GCL(Suresh et al., 2021), GASSL <ref type="bibr" target="#b45">(Yang et al., 2021)</ref>, InfoGCL <ref type="bibr" target="#b42">(Xu et al., 2021a)</ref>, RGCL <ref type="bibr" target="#b21">(Li et al., 2022)</ref> and DGCL <ref type="bibr" target="#b20">(Li et al., 2021)</ref>, as well as three classical unsupervised representation learning methods, including node2vec ( Leskovec, 2016), graph2vec <ref type="bibr" target="#b27">(Narayanan et al., 2017)</ref>, and GVAE <ref type="bibr" target="#b18">(Kipf &amp; Welling, 2016)</ref>. Besides, we employ AttrMasking <ref type="bibr" target="#b17">(Hu et al., 2020b)</ref>, ContextPred <ref type="bibr" target="#b17">(Hu et al., 2020b)</ref>, GraphCL <ref type="bibr" target="#b47">(You et al., 2020)</ref>, GraphLoG <ref type="bibr" target="#b44">(Xu et al., 2021b)</ref>, AD-GCL (Suresh et al., 2021) and RGCL <ref type="bibr" target="#b21">(Li et al., 2022)</ref> as baselines to evaluate the effectiveness of our proposed GraphCV under transfer learning setting.</p><p>Evaluation Protocol. For unsupervised setting, we follow the evaluation protocols in the previous works <ref type="bibr" target="#b32">(Sun et al., 2019;</ref><ref type="bibr" target="#b47">You et al., 2020;</ref><ref type="bibr" target="#b20">Li et al., 2021)</ref> to verify the effectiveness of our model.</p><p>The mean test accuracy score evaluated by a 10-fold cross validation with standard deviation of five random seeds as the final performance. For transfer learning setting, we follow the finetuning procedures of previous work <ref type="bibr" target="#b47">(You et al., 2020;</ref><ref type="bibr" target="#b44">Xu et al., 2021b)</ref> and report the mean ROC-AUC scores with standard deviation of 10 repeated runs on each downstream datasets. In addition, we follow the setting of semi-supervised representation learning from GraphCL on the ogbg-molhiv dataset, with the finetune label rates as 1%, 10%, and 20%. The final performance is reported as the mean ROC-AUC of five initialization random seeds 4.2 OVERALL PERFORMANCE COMPARISON Unsupervised learning. The overall performance comparison is shown in Table <ref type="table" target="#tab_1">1</ref> and we can have three observations: (1) The GCL-based methods generally yield higher performances than classical unsupervised learning methods, indicating the effectiveness of utilizing instance-level supervision;</p><p>(2) RGCL, AD-GCL, and GASSL achieve better performances than GraphCL, which empirically proves the conclusion that InfoMax object could bring overwhelmed redundant information and thus suffer from feature suppression issue; (3) Our proposed GraphCV and DGCL consistently outperform other baselines, proving the advantage of disentangled representation. More importantly, GraphCV achieves state-of-the-art results on most of the datasets, demonstrating the model effectiveness.</p><p>Transfer learning. Table <ref type="table" target="#tab_2">2</ref> demonstrates the experimental results under transfer learning setting, where No Pre-Train skips self-supervised pre-training process on the ZINC-2M dataset for model initialization before finetune. It is noteworthy that some strong baselines (AttrMasking and Con-textPred) are trained under the guidance of domain knowledge. Despite in lacking of such domain knowledge, our model still outperforms all the other baselines on 3 out 8 datasets and achieve highest average performance. More importantly, JOAO, RGCL and our proposed GraphCV are all developed from GraphCL, but achieve higher average performance than GraphCL. This observation further empirically prove the poisoning effect of biased information and the necessity to to suppress them.</p><p>Figure <ref type="figure">3</ref>: Performance comparison of semisupervised learning on ogbg-molhiv.</p><p>Semi-supervised learning. The experimental results are shown in Figure <ref type="figure">3</ref>. It is obvious that our model gains significant improvements under the three label-rate fine-tuning settings. We also notice that as the label rate increases, the amount of improvement increases as well (1%, 1.8%, and 4.4% for label rate 1%, 10%, and 20%, respectively). A possible explanation could be that more trainable data could bring more redundant information, thereby further deteriorate the feature suppression issue. Therefore, removing redundant information causes a higher performance boost.</p><p>Pre-print   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Graph contrastive learning. Contrastive learning is firstly proposed in the compute vision field <ref type="bibr" target="#b5">(Chen et al., 2020)</ref> and raises a surge of interests in the area of self-supervised graph representation learning for the past few years. The principle behind contrastive learning is to utilize the instancelevel identity as supervision and maximize the consistency between positive pairs in hidden space through designed contrast mode. Previous graph contrastive learning works generally rely on various graph augmentation (transformation) techniques <ref type="bibr" target="#b36">(Veličković et al., 2019;</ref><ref type="bibr" target="#b28">Qiu et al., 2020;</ref><ref type="bibr" target="#b14">Hassani &amp; Khasahmadi, 2020b;</ref><ref type="bibr" target="#b47">You et al., 2020;</ref><ref type="bibr" target="#b32">Sun et al., 2019)</ref> to generate positive pair from original data as similar samples. Recent works in this field try to improve the effectiveness of graph contrastive learning by finding more challenge view <ref type="bibr">(Suresh et al., 2021;</ref><ref type="bibr" target="#b42">Xu et al., 2021a;</ref><ref type="bibr" target="#b47">You et al., 2021)</ref> or adding adversarial perturbation <ref type="bibr" target="#b45">(Yang et al., 2021)</ref>. However, most of the existing methods contrast over entangled embeddings, where the complex intertwined information may pose obstacles to extracting useful information for downstream tasks. Our model is spared from the issue by contrasting over disentangled representations.</p><p>Disentangled representation learning on graphs. Disentangled representation learning arises from the computer vision field <ref type="bibr" target="#b15">(Hsieh et al., 2018;</ref><ref type="bibr" target="#b48">Zhao et al., 2021)</ref> to disentangle the heterogeneous latent factors of the representations, and therefore making the representations more robust and interpretable <ref type="bibr" target="#b2">(Bengio et al., 2013)</ref>. This idea has now been widely adopted in graph representation learning. <ref type="bibr" target="#b22">(Liu et al., 2020;</ref><ref type="bibr" target="#b23">Ma et al., 2019)</ref> utilizes neighborhood routing mechanism to identify the latent factors in the node representations. Some other generative models <ref type="bibr" target="#b18">(Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b31">Simonovsky &amp; Komodakis, 2018)</ref> utilize Variational Autoencoders to balance reconstruction and disentanglement.</p><p>Recent work <ref type="bibr" target="#b20">(Li et al., 2021)</ref> outspreads the application of disentangled representations learning in self-supervised graph learning by contrasting the factorized representations. Although these methods gain significant benefit from the representation disentanglement, the underlined excessive information could still overload the model, thus resulting in limited capacities. Our model targets the issue by removing the redundant information that is considered irrelevant to the graph property.</p><p>Graph information bottleneck. The Information bottleneck (IB) <ref type="bibr">(Tishby et al., 2000)</ref> has been widely adopted as a critical principle of representation learning. A representation contains minimal yet sufficient information is considered to be in compliance with the IB priciple and many works <ref type="bibr" target="#b1">(Alemi et al., 2017;</ref><ref type="bibr" target="#b30">Shwartz-Ziv &amp; Tishby, 2017;</ref><ref type="bibr" target="#b8">Federici et al., 2020)</ref> have empirically and theoretically proved that representation agree with IB principle is both informative and robust. Recently, IB principle is also borrowed to guide the representation learning of graph structure data. Current methods <ref type="bibr" target="#b38">(Wu et al., 2020;</ref><ref type="bibr" target="#b42">Xu et al., 2021a;</ref><ref type="bibr">Suresh et al., 2021;</ref><ref type="bibr" target="#b21">Li et al., 2022)</ref> usually propose different regularization designs to learn compressed yet informative representations in accordance with IB principle. We follow the information bottleneck to learn the expressive and robust representation from disentangled information in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we study the feature suppression problem in representation learning. To avoid the predictive features being suppressed in learned representation, we propose a novel model, namely GraphCV, which is designed in accordance with the information bottleneck principle. The cross-view reconstruction in GraphCV can disentangle those more robust and transferable features from those easily-disturbed ones. Meanwhile, we also add an adversarial view as the third view of the contrastive learning to to guarantee the global semantics and further enhance representation robustness. In addition, we theoretically analyze the effectiveness of each component in our model and derive the objective based on the analysis. Extensive experiments on multiple graph benchmark datasets and different settings prove the ability of GraphCV to learn robust and transferable graph representation.</p><p>In the future, we can explore how to come up with a practical objective to further decrease the upper bound of the mutual information between the disentangled representations and try to utilize more efficient training strategy to make the proposed model more time-saving on large-scale graphs.</p><p>Pre-print</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ETHICS STATEMENT</head><p>This idea is proposed to solve the general graph learning problem, so we believe there should exist no ethical concern applicable to our work. Any unethical application that benefits from our work is against our initial intent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REPRODUCIBILITY STATEMENT</head><p>We provide the source code along with the submission in the supplementary materials for reproducibility. The source code and all the implementation details will be open to public once upon the acceptance of this paper.</p><p>Pre-print observation G " gpG i q. Due to the reason that G i is not explicit, so we aim to train a encoder f : G i Ñ R d to map input graph data G into a latent space to extract useful high-level information z i corresponding to each feature sub-space G i of input data G during cotrastive learning. Therefore, we use ppG | z i q as the approximated value of the measurement ppG | G i q. Then we have,</p><p>• For any feature sub-space G i and its complementary feature sub-subspace G ī, f suppress feature i Ď rns if we have ppG | z i q " ppG | z īq</p><p>• For any feature sub-space G i and its complementary feature sub-subspace G ī, f distinguish feature i Ď rns if ppG | z i q and ppG | z īq have disjoint support.</p><p>To sum up, a feature is suppressed if it does not make any difference to the instance discrimination.</p><p>One of the common acknowledgements for unsuprevised learning strategy is that it can usually produce representation with uniform feature space distribution due to the lack of supervision, i.e., every feature sub-space is equally treated without feature suppression. However, it could not be the situation in contrastive learning. Taking the commonly used InfoNCE ](van den <ref type="bibr" target="#b34">Oord et al., 2018)</ref> as an example, it can be divided into two parts, i.e. align term and uniform term <ref type="bibr" target="#b5">(Chen et al., 2020)</ref>, as follow:</p><formula xml:id="formula_12">τ L InfoNCE " ´1 m ÿ i,j sim pz i , z j q looooooooooomooooooooooon Lalignment `τ m ÿ i log 2m ÿ k"1 1 rk‰is exp psim pz i , z k q {τ q looooooooooooooooooooooooomooooooooooooooooooooooooon Luniform<label>(10)</label></formula><p>Aligning the positive pair will distinguish the shared feature subspace G i . Meanwhile, there also exits random negative samples that might own same factors in G i , so the uniform term might suppress the feature sub-space G i . Therefore, for any feature i Ď rns, the optimization process can either suppress or distinguish it, but both of them can reach to lower contrastive loss. From the analysis we can derive the conclusion mentioned in Section 1 that lower contrastive loss might not yield better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C SUMMARY OF DATASETS</head><p>In this work, we use nine datasets from TU Benchmark Datasets <ref type="bibr" target="#b26">(Morris et al., 2020)</ref> to evaluate our proposed GraphCV under unsupervised setting, where five of them are biochemical datasets and the other four belong to social network datasets. We also utilize the ogng-molhiv dataset from Open Graph Benchmark (OGB) <ref type="bibr" target="#b16">(Hu et al., 2020a)</ref> to further evaluate GraphCV under semi-supervised setting. Besides, the datasets sampled from MoleculeNet <ref type="bibr" target="#b40">(Wu et al., 2018a)</ref> are employed to evaluate our model under transfer learning setting. The statistics of these datasets are shown in Table <ref type="table" target="#tab_4">4 and 5</ref> </p><p>After that, by applying the chain rule to I pt 1 pGq; z p 2 , z c 2 q, we have,</p><formula xml:id="formula_14">I pt 1 pGq; z p 2 , z c 2 q " I pt 1 pGq; z p 2 | z c 2 q `I pt 1 pGq; z c 2 q p2q " I pt 1 pGq; z p 2 q `I pt 1 pGq; z c 2 q paq ě I pt 1 pGq; z p 2 q pbq ě I pz c 1 , z p 1 ; z p 2 q p2q " I pz c 1 ; z p 2 q `I pz p 1 ; z p 2 q paq ě I pz p 1 ; z p 2 q ,<label>(13) where p2q</label></formula><p>" is derived from the conclusion we get in Equation <ref type="formula" target="#formula_13">12</ref>, paq ě is based on the non-negativity of mutual information, i.e., Ip; q ě 0, and pbq ě is because data processing inequality <ref type="bibr" target="#b7">(Cover, 1999)</ref>. Finally, we reach to the lower bound of I pt 1 pGq; z p 2 , z c 2 q in Equation 12, thus we can maximize the consistency between the information we capture from the two augmentation graph views by minimizing L pre .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 PROOF OF THEOREM 2</head><p>We repeat Theorem 2 as follows.</p><p>Theorem 2. Assume q is a Gaussian distribution, g r is the parameterized reconstruction model which infer z w from pz p w 1 , z c w q. Then we have: H pz w | z p w 1 , z c w q ď }z w ´gr pz p w 1 d z c w q} 2 2 where w " w 1 or w ‰ w 1 .</p><p>Proof. To reconstruct the entangled representation z w from its corresponding non-predictive representation z c w and the predictive representation of any augmentation view z p w 1 (w and w 1 are not necessarily equal), we need to minimize the conditional entropy:</p><formula xml:id="formula_15">H pz w | z p w 1 , z c w q " ´Eppzw,z p w 1 ,z c w q rlog p pz w | z p w 1 , z c w qs .<label>(14)</label></formula><p>Since the real distribution of p pz w | z p w 1 , z c w 1 q is unknown and intractable, we hereby introduce a variational distribution q pz w | z p w 1 , z c w q to approximate it. Therefore, we have,</p><formula xml:id="formula_16">E ppzw,z p w 1 ,z c w q rlog p pz w | z p w 1 , z c w qs " E ppzw,z p w 1 ,z c w q rlog q pz w | z p w 1 , z c w qs `DKL pp pz w | z p w 1 , z c w q }q pz w | z p w 1 , z c w qq .<label>(15)</label></formula><p>Due to the non-negativity of KL-divergence between any two distributions, it is safe to say ´Eppzw,z p w 1 ,z c w q rlog q pz w | z p w 1 , z c w qs is the upper bound of H pz w | z p w 1 , z c w q. Based on the assumption of Theorem 2, let q pz w | z p w 1 , z c w q being a Gaussian distribution N `zw | g r pz p w 1 d z c w q , σ 2 I ˘, where g r p¨q is the reconstruct network that predict z w from pz p w 1 , z c w q and σ is the variance. Thus we have,</p><formula xml:id="formula_17">H pz w | z p w 1 , z c w q ď ´Eppzw,z p w 1 ,z c w q rlog q pz w | z p w 1 , z c w qs " ´Eppzw,z p w 1 ,z c w q « log ˜1 ? 2πIσ e ´1 2 p zw ´gr p z p w 1 dz c w qq 2 pσ 2 Iq ¸ff " ´Eppzw,z p w 1 ,z c w q « log ˆ1 ? 2πIσ ˙´pz w ´gr pz p w 1 d z c w qq 2 2σ 2 I ff .<label>(16)</label></formula><p>Hence, we get the upper bound of H pz w | z p w 1 , z c w q as Equation 16. To minimize the value of the unsolvable entropy, we can instead minimize the value of its upper bound and thereby derive the objective function as follow by neglecting the constant terms,</p><formula xml:id="formula_18">min E ppzw,z p w 1 ,z c w q }z w ´gr pz p w 1 d z c w q} 2 2 . (<label>17</label></formula><formula xml:id="formula_19">)</formula><p>Since we adopt two augmentation views and propose the cross-view reconstruction mechanism in our method, we can minimize the entropy by minimizing L recon and thus guarantee the disentanglement of z p and z c .</p><p>F EFFECTS OF REPRESENTATION DISENTANGLEMENT Pre-print other datasets. From the loss curves in Figure <ref type="figure" target="#fig_3">6</ref> we can find that contrastive loss between predictive representations gradually decreases, indicating the predictive representation is optimized to capture all the shared information between the two augmentation view. Meanwhile, we can see contrastive loss between the non-predictive representations achieve a noticeable increases, which is consistent with our expectation that the two independent sampled augmentation operators cause a distribution shift between the two augmentation views. To further investigate whether the feature suppression problem is equally serious in z p and z c , we conduct experiments to compare the performance of the two representation on downstream tasks. The comparison results are as follow: It is easily to observe that there is a obvious performance gap between the two learned representation, indicating the different feature suppression issue between them and the features subset that are more robust to augmentation is more informative and transferable that those sensitive to augmentations. Therefore, we believe our proposed GraphCV can further alleviate the feature suppression issue with the disentanglement design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G TRAINING ALGORITHM</head><p>In this sectionm we summarized the details of our proposed method in the following Algorithm. z 1,i " f pt 1 pG i qq, z 2,i " f pt 2 pG i qq ; Ź t 1 p¨q, t 2 p¨q P T 5 z p 1,i " g p pz 1,i q, z p 2,i " g p pz 1,i q ; 6 z c 1,i " g c pz 1,i q, z c 2,i " g c pz 1,i q ;</p><p>7 Calculate L pre according to Equation 6; </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H HYPER-PARAMETER SENSITIVITY</head><p>In this section, we study the impacts of some important hyper-parameters in our method, including reconstruction loss coefficient λ r , adversarial loss coefficient λ a , embedding dimension d, batch size</p><p>Pre-print values of the two hyper-parameters generally increase as the dataset scale increases. The reason behind this phenomena could be large datasets usually contain more latent factors than the small datasets, therefore a model with larger capacity is needed to fit the large datasets. However, such high-capacity message-passing model will deteriorate the performance of small dataset because it cause the learned representation over-smoothing and hence less informative.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Illustration of the relation between graph G, label y, predictive feature subsets G p and non-predictive feature subset G c in terms of information entropy. Ideally, the green areas in the three figures is null. (a) The usual optimization result of graph contrastive learning, where the shared features of two augmentation view is extracted for the learned representation z. Owing to the lack of supervision or domain knowledge, redundant and biased information (shadow area) is usually included in z; (b) G p cover the feature subset which is sufficient to make correct graph label identification (Ipy; G | G p q " 0), other features (G c ) is either useless or misguiding; (c) G p and G c are supposed to be mutually disentangled with each other (IpG p ; G c q " 0). The union of them cover all the features of original data. the representation consistency within the positive pair, i.e., z " f pt 1 pGqq " f pt 2 pGqq. Consequently, such training strategy is heavily dependent on the choice and strength of graph augmentation techniques. To be more specific, moderate graph augmentation will push encoders to capture redundant and biased information<ref type="bibr" target="#b33">(Tschannen et al., 2019)</ref>, which could inadvertently suppress the space of important predictive features and negatively affect the representation transferability via the so-called "shortcut" solution<ref type="bibr" target="#b9">(Geirhos et al., 2020;</ref><ref type="bibr" target="#b25">Minderer et al., 2020)</ref>. A more intuitively illustration is provided in the Figure1 (a), where the shared part of the two augmentation view include both predictive information (the overlapping area with y) and non-predictive information (shadow area). Such optimization result usually yield lower contrastive loss, however, it has been empirically proved that the redundant information could lead to poor robustness<ref type="bibr" target="#b29">(Robinson et al., 2021)</ref>, especially under the out-of-distribution (OOD) setting<ref type="bibr" target="#b46">(Ye et al., 2021)</ref>. We provide a showcase example in Appendix A to illustrate the OOD scenario on graph learning task. On the other hand, overly aggressive augmentation may easily lead to another extreme where many predictive features are randomly dropped and the learned representation does not contain sufficient predictive information for downstream instance discrimination. Recent works(Suresh et al., 2021;<ref type="bibr" target="#b21">Li et al., 2022;</ref><ref type="bibr" target="#b47">You et al., 2021)</ref> propose to use automated augmentations to extract the invariant rationale features<ref type="bibr" target="#b39">(Wu et al., 2022b;</ref>a). These methods assume the most salient sub-structure (those are resistant to graph augmentation) is sufficient to make rational and correct label identification, and thereby implement trainable augmentation operations (e.g., edge deleting, node dropping) to strictly regularize the graph topological structure. Despite that these methods can alleviate the aforementioned feature suppression issue to some extent, they still suffer from inherent limitation. The harsh regularization may force the encoders focusing on the easy-learned "shallower" features (e.g. graph size and node degree), which might be helpful under certain domains but not necessarily for others<ref type="bibr" target="#b3">(Bevilacqua et al., 2021)</ref>, thus fail to guarantee stronger robustness. Therefore, the GCL methods guided with the saliency philosophy is not flexible enough to balance the representation sufficiency and robustness without the guidance of explicit domain knowledge. To reconcile the robustness and sufficiency of the learned representation, a method which can reduce redundant and biased information without sacrificing the sufficiency of the predictive graph features is in urgent need.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The illustration of the proposed GraphCV. (1) Graph augmentations are applied to the input graph G to produce two augmented graphs, which are then fed into the shared graph encoder f p¨q to generate two graph representations z 1 and z 2 . (2) z 1 and z 2 are used as the inputs of the two decoder to generate two pairs of graph representations, z p captures the predictive factors and z c keep other complementary non-predictive features. Then we use the two pairs of representations to reconstruct z 1 and z 2 in both of the intra-view and inter-view. (3) An adversarial sample generated by G will go through the same procedure to generate z p adv . We take it as the third view besides z p 1 and z p 2 in CL guarantee the z p can keep the global semantics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 4: The model performance under different perturbation bound, attack step and analysis the sensitivity of the two important hyper-parameters (i.e., λ r and λ a ).In this section, we firstly conduct extra experiments on ogbg-molhiv dataset to evaluate the representation robustness under aggressive augmentation and perturbation. The results are shown in left two subplots of Figure4, we compare our method with GASSL under different perturbation bounds and attack steps to demonstrate its robustness against adversarial attacks. Since both our model and GASSL use GIN as the backbone network, we hereby add the performance of GIN as the compared baseline. Although aggressive adversarial attacks can largely deteriorate the performance, our proposed GraphCV still achieves more robust performance than GASSL. In the right two subplots, we analysis the model sensitivity of the two important hyper-parameters in our model, λ r and λ a . The consistent superiority of different values over the initial point (i.e., λ r , λ a " 0) prove the effectiveness of our design once again. We can also observe that the appropriate range of the two hyper-parameters are 5.0 to 10.0 and 0.0 to 0.5, respectively. Depend on the datasets size and attributes, the range can</figDesc><graphic url="image-44.png" coords="8,108.00,482.05,395.99,99.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: InfoNCE loss of the two disentangled representations between the two augmentation graph views, where orange lines are the InfoNCE loss between the two non-predictive representations and blue lines are the InfoNCE loss between the two predictive representaitons In this section, we set experiments to investigate the representation disentanglement of our proposed GraphCV. Specifically, we use the InfoNCE loss (van den Oord et al., 2018) to dynamically measure the representation difference between the two augmentation graph views based on the two disentangled representations, where blue lines indicate the InfoNCE loss between z p 1 and z p 2 and orange lines represent the InfoNCE loss between z c 1 and z c 2 . For simplicity, we only demonstrate the first 100 pre-training epochs of PROTEINS and COLLAB in Figure 6, we can observe similar phenomena on</figDesc><graphic url="image-45.png" coords="17,108.00,468.96,396.00,152.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 1 :</head><label>1</label><figDesc>The training algorithm of GraphCV Input: Graph dataset G " tG i " pV i , E i qu N i"1 ; augmentation family T ; loss coefficient λ r , λ a ; ascernt step T ; ascent step size α; perturbation bound . Output: The disentangled predictive representations Z p " tz p</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-48.png" coords="19,108.00,504.63,396.01,172.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>to clear up the definition of mutual redundancy. t 1 pGq is redundant to t 2 pGq with respect of y iff t 1 pGq and t 2 pGq share the same predictive information. Mathematically, the mutual redundancy in CL exists when:</figDesc><table><row><cell cols="2">Pre-print</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>𝐺</cell><cell>𝑡 ! (𝐺) Encoder Forward path 𝑡 $ (𝐺)</cell><cell>Encoder 𝒛 %&amp;' Encoder Use perturb to perform attack Perturb 𝛿 Adversarial attack</cell><cell>𝒛 ! Decoder Pre. 𝒛 $</cell><cell>Com. Decoder Pre. Decoder " 𝒛 %&amp;' Pre. Decoder Com. Decoder</cell><cell>𝒛 ! 𝒛 ! # " CL Loss 𝒛 $ " 𝒛 $ #</cell><cell>( 𝒛 ! #( 𝒛 ! #( 𝒛 $ 𝒛 $ (</cell><cell>Rec. Loss Rec. Loss Rec. Loss Rec. Loss</cell><cell>𝒛 ! 𝒛 $</cell></row><row><cell></cell><cell></cell><cell cols="5">Ipt 1 pGq; y | t 2 pGqq " Ipt 2 pGq; y | t 1 pGqq " 0.</cell><cell></cell><cell>(2)</cell></row><row><cell cols="9">Although GCL-based methods are usaully capable to extract useful information for label identification,</cell></row><row><cell cols="9">it is unavoidable to include non-predictive features under the SSL setting owing lack of explicit</cell></row><row><cell cols="9">domain knowledge. There exist the situation (e.g., OOD setting) that the latent space of learned</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Grover &amp;   Overall comparison on multiple graph classification benchmarks under unsupervised learning setting. Results are reported as mean±std%, the best performance is bolded and runner-ups are underlined. "-" indicates the result is not reported in original papers.</figDesc><table><row><cell></cell><cell cols="7">MUTAG PTC-MR COLLAB NCI1 PROTEINS IMDB-B RDT-B IMDB-M</cell><cell>DD</cell></row><row><cell cols="3">node2vec 72.6±10.2 58.6±8.0</cell><cell>-</cell><cell cols="2">54.9±1.6 57.5±3.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">graph2vec 83.2±9.3 60.2±6.9</cell><cell>-</cell><cell cols="4">73.2±1.8 73.3±2.1 71.1±0.5 75.8±1.0 50.4±0.9</cell><cell>-</cell></row><row><cell cols="8">InfoGraph 89.0±1.1 61.7±1.4 70.7±1.1 76.2±1.1 74.4±0.3 73.0±0.9 82.5±1.4 49.7±0.5 72.9±1.8</cell></row><row><cell>VGAE</cell><cell cols="2">87.7±0.7 61.2±1.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">70.7±0.7 87.1±0.1 49.3±0.4</cell><cell>-</cell></row><row><cell cols="3">MVGRL 89.7±1.1 62.5±1.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">74.2±0.7 84.5±0.6 51.2±0.5</cell><cell>-</cell></row><row><cell cols="8">GraphCL 86.8±1.3 63.6±1.8 71.4±1.2 77.9±0.4 74.4±0.5 71.1±0.4 89.5±0.8</cell><cell>-</cell><cell>78.6±0.4</cell></row><row><cell cols="5">InfoGCL 91.2±1.3 63.5±1.5 80.0±1.3 80.2±0.6</cell><cell>-</cell><cell>75.1±0.9</cell><cell>-</cell><cell>51.4±0.8</cell><cell>-</cell></row><row><cell>DGCL</cell><cell cols="7">92.1±0.8 65.8±1.5 81.2±0.3 81.9±0.2 76.4±0.5 75.9±0.7 91.8±0.2 51.9±0.4</cell><cell>-</cell></row><row><cell cols="2">AD-GCL 89.7±1.0</cell><cell>-</cell><cell cols="5">73.3±0.6 69.7±0.5 73.8±0.5 72.3±0.6 85.5±0.8 49.9±0.7 75.1±0.4</cell></row><row><cell>RGCL</cell><cell>87.7±1.0</cell><cell>-</cell><cell cols="5">70.9±0.7 78.1±1.1 75.0±0.4 71.9±0.8 90.3±0.6</cell><cell>-</cell><cell>78.9±0.5</cell></row><row><cell cols="5">GASSL 90.9±7.9 64.6±6.1 78.0±2.0 80.2±1.9</cell><cell>-</cell><cell>74.2±0.5</cell><cell>-</cell><cell>51.7±2.5</cell><cell>-</cell></row><row><cell cols="8">GraphCV 92.3±0.7 67.4±1.3 80.5±0.5 82.0±1.0 76.8±0.4 75.6±0.4 92.4±0.9 52.2±0.5 80.5±0.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Overall comparison on multiple graph classification benchmarks under transfer learning setting. Results are reported as mean±std%, the best performance is bolded and runner-ups are underlined. the effectiveness of different modules in GraphCV, we perform ablation studies on each one of the module by creating two model variants: (1) w/o CV Recon, the cross-view reconstruction process is discarded; (2) w/o Adv. Training, the third adversarial view is discarded. The comparison results are shown in Table3. We can observe from Table3that our model with the combination of cross-view reconstruction and adversarial training module outperforms all of the variants. Omitting the reconstruction process could cause the failure to optimize the representation in a disentangled manner illustrated in Figure1(c), thereby the learned representation still suffer from features suppression issue. Compared with our model, the variant w/o Adv. Training may lead to representation collapse and bring extra redundant information, therefore resulting in sub-optimal performances in downstream tasks.</figDesc><table><row><cell></cell><cell>BBBP</cell><cell>Tox21 ToxCast SIDER ClinTox</cell><cell>MUV</cell><cell>HIV</cell><cell>BACE Avg</cell></row><row><cell cols="6">No Pre-Train 65.8±4.5 74.0±0.8 63.4 ±0.6 57.3±1.6 58.0±4.4 71.8±2.5 75.3±1.9 70.1±5.4 67.0</cell></row><row><cell cols="6">AttrMasking 64.3±2.8 76.7±0.4 64.2±0.5 61.0±0.7 71.8±4.1 74.7±1.4 77.2±1.1 79.3±1.6 71.1</cell></row><row><cell cols="6">ContextPred 68.0±2.0 75.7±0.7 63.9±0.6 60.9±0.6 65.9±3.8 75.8±1.7 77.3±1.0 79.6±1.2 70.9</cell></row><row><cell cols="6">GraphCL 69.5±0.5 75.4±0.9 63.8±0.4 60.8±0.7 70.1±1.9 74.5±1.3 77.6±0.9 78.2±1.2 70.8</cell></row><row><cell cols="6">GraphLoG 72.5±0.8 75.7±0.5 63.5±0.7 61.2±1.1 76.7±3.3 76.0±1.1 77.8±0.8 83.5±1.2 73.4</cell></row><row><cell>JOAO</cell><cell cols="5">70.2±1.0 75.0±0.3 62.9±0.5 60.0±0.8 81.3±2.5 71.7±1.4 76.7±1.2 51.5±0.4 71.9</cell></row><row><cell>RGCL</cell><cell cols="5">71.4±0.7 75.2±0.3 63.3±0.2 61.4±0.6 83.4 ±0.9 76.7 ±1.0 77.9±0.8 76.03±0.8 73.2</cell></row><row><cell cols="6">GraphCV 71.6±0.6 75.7±0.6 63.2±0.5 62.2±0.7 83.6±1.5 76.4±0.8 77.9±1.0 80.8±1.8 73.9</cell></row><row><cell cols="3">4.3 ABLATION STUDY</cell><cell></cell><cell></cell></row><row><cell cols="2">To further verify</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Overall comparison of the model variants' performance. Results are reported as mean±std%, the best performance is bolded.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>. Statistics of TU-datasets and OGB dataset.</figDesc><table><row><cell>Dataset</cell><cell cols="4">#Graphs Avg #Nodes Avg #Edges #Class</cell><cell>Metric</cell><cell>Category</cell></row><row><cell>MUTAG</cell><cell>188</cell><cell>17.93</cell><cell>19.79</cell><cell>2</cell><cell>Accuracy</cell><cell>biochemical</cell></row><row><cell>PTC-MR</cell><cell>344</cell><cell>14.29</cell><cell>14.69</cell><cell>2</cell><cell>Accuracy</cell><cell>biochemical</cell></row><row><cell>PROTEINS</cell><cell>1,113</cell><cell>39.06</cell><cell>72.82</cell><cell>2</cell><cell>Accuracy</cell><cell>biochemical</cell></row><row><cell>NCI1</cell><cell>4,110</cell><cell>29.87</cell><cell>32.30</cell><cell>2</cell><cell>Accuracy</cell><cell>biochemical</cell></row><row><cell>DD</cell><cell>1,178</cell><cell>284.32</cell><cell>715.66</cell><cell>2</cell><cell>Accuracy</cell><cell>biochemical</cell></row><row><cell>COLLAB</cell><cell>5,000</cell><cell>74.49</cell><cell>2457.78</cell><cell>3</cell><cell cols="2">Accuracy social network</cell></row><row><cell>IMDB-B</cell><cell>1,000</cell><cell>19.77</cell><cell>96.53</cell><cell>2</cell><cell cols="2">Accuracy social network</cell></row><row><cell>RDT-B</cell><cell>2,000</cell><cell>429.63</cell><cell>497.75</cell><cell>2</cell><cell cols="2">Accuracy social network</cell></row><row><cell>IMDB-M</cell><cell>1,500</cell><cell>13.00</cell><cell>65.94</cell><cell>3</cell><cell cols="2">Accuracy social network</cell></row><row><cell>ogbg-molhiv</cell><cell>41,127</cell><cell>25.50</cell><cell>27.50</cell><cell>2</cell><cell>ROC-AUC</cell><cell>MoleculeNet</cell></row></table><note>All of the eleven datasets are public available, we attach attach their links as follow:• TU datasets: https://chrsmrrs.github.io/datasets/docs/datasets/ • MoleculeNet datasets: http://snap.stanford.edu/gnn-pretrain/ p pa, bq log p pb | aq p pbq " I pa; bq .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Performance comparison of the two learned representations. Results are reported as mean±std%, the best performance is bolded. MUTAG COLLAB NCI1 PROTEINS IMDB-B RDT-B DD ogbg-molhiv z c 88.1±1.2 75.1±0.7 72.2±2.0 73.5±0.8 71.8±0.9 89.4±1.0 75.8±0.6 69.7.0±2.8 z p 92.3±0.7 80.5±0.5 82.0±1.0 76.8±0.4 75.6±0.4 92.5±0.9 80.5±0.5 75.36±1.4</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Ð δ t´1 `α∇ δ L adv ; Ź Update perturbation to maximize L adv Update the parameter θ of f and g with the gradient ∇ θ Lpθ, Bq over a minibatch;</figDesc><table><row><cell>14</cell><cell cols="2">L Ð L `λa T L adv</cell></row><row><cell>15</cell><cell></cell></row><row><cell cols="2">16 return Z p " tz p i u</cell><cell>N i"1 , where z p i " g p pf pG i qq</cell></row></table><note>8Calculate L recon according to Equation8;9 L Ð L pre `λr L recon ; 10 δ 0 Ð U p´ , q; 11 for each t " 1 to T do 12Calculate the L adv according to Equation10;13 δ t</note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this section, we will illustrate the out-of-distribution scenario in graph learning task. During molecule property study, A specific kind of property (e.g., toxicity and lipophilicity) of a molecule is usually dependent on if it has corresponding sub-structures (termed as functional group). For example, hydrophilic molecules usually have oxhydryl group (´OH) Therefore, a well-trained GNN model on molecule graph prediction task is capable to reflect the sub-structure information in the graph representation. However, it is usually the case in real-world scenario that the predictive functional group is usually accompanied by some irrelevant groups in some environments, thus causing spurious correlations. This correlation usually lead to poor generalization performance when the model is evaluated on another environment with different spurious correlation. Figure <ref type="figure">5</ref> intuitively demonstrates this kind of scenario, where the red subgraph is the feature we can rely on to make casual prediction. But it usually show up with green subgraph that do not serve as the functional graph of the property in training set. Consequently, the model are easily misguided that the green subgraph is an important indicator of the property. When we evaluate the model on the testing set where the casual graph is correlated with another kind of group (yellow subgraph), there usually exists a huge gap between its performances on the two sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B DISCUSSION ON FEATURE SUPPRESSION</head><p>In this section, we will follow the previous works <ref type="bibr" target="#b6">(Chen et al., 2021;</ref><ref type="bibr" target="#b29">Robinson et al., 2021)</ref> to present a more formal definition of the feature suppression and clarify its relation with contrastive learning.</p><p>First of all, we assume graph data G has n feature sub-spaces, G 1 , . . . , G n , where each G i P G corresponding to a distinct feature of G. To quantify the relation between G and its feature sub-spaces, we need to measure the conditional probability of G given a specific kind of feature sub-space G i (i Ď rns), denoted as ppG | G i q. Finally, we define a an injective map g : </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D IMPLEMENTATION DETAILS</head><p>All experiments are conducted with the following settings:</p><p>• Operating System: Ubuntu 18.04.5 LTS • CPU: AMD(R) Ryzen 9 3900x • GPU: NVIDIA GeForce RTX 2080ti • Software: Python 3.8.5; Pytorch 1.10.1; PyTorch Geometric 2.0.4; PyGCL 0.1.2; Numpy 1.20.1; scikit-learn 0.24.1.</p><p>We implement our framework with PyTorch and PyGCL library <ref type="bibr" target="#b49">(Zhu et al., 2021)</ref>. We choose GIN <ref type="bibr" target="#b43">(Xu et al., 2019)</ref> as the backbone graph encoder and the model is optimized through Adam optimizer.</p><p>We choose GIN <ref type="bibr" target="#b43">(Xu et al., 2019)</ref> as the backbone graph encoder and the model is optimized through Adam optimizer. We follow <ref type="bibr" target="#b47">(You et al., 2020;</ref><ref type="bibr" target="#b45">Yang et al., 2021;</ref><ref type="bibr" target="#b20">Li et al., 2021)</ref> to employ a linear SVM classifier for downstream task-specific classification. The graph augmentation operations used in our work are same as <ref type="bibr" target="#b47">(You et al., 2020)</ref>, including node dropping, edge perturbation, attribute masking and subgraph sampling, all of them are borrowed from the implementation of <ref type="bibr" target="#b49">(Zhu et al., 2021)</ref>. There are two specific hyper-parameters in our model, namely λ r and λ a , the search space of them are t0.0, 1.0, 3.0, 5.0, 10.0u and t0.0, 0.25, 0.5, 0.75, 1.0u, respectively. For other important hyper-parameters, we find the best value of learning rate from t0.01, 0.005, 0.001, 0.0005, 0.0001u, embedding dimension from t32, 64, 128, 256, 512u, number of GNN layers from t2, 3, 4, 5u, batch size from t32, 64, 128, 256, 512u (except for ogbg-molhiv t64, 128, 256, 512, 1024u). Besides, we fix the perturbation bound , ascent step size α and ascent step T as 0.008, 0.008 and 5 during hyper-parameter fine-tuning. As for the implementation details of transfer learning, we basically follow the pre-training setting of previous works <ref type="bibr" target="#b47">(You et al., 2020;</ref><ref type="bibr" target="#b44">Xu et al., 2021b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E PROOF E.1 PROOF OF THEOREM 1</head><p>We repeat Theorem 1 as follows.</p><p>Theorem 1. Suppose f p¨q is a GNN encoder as powerful as 1-WL test. Let g p p¨q elicits only the augmentation information from z meanwhile g c p¨q extracts the essential factors of G from z 1 and z 2 . Then we have:</p><p>where G P G and t 1 p¨q, t 2 p¨q P T .</p><p>Proof. According to the assumption in Theorem 1, for any two graphs G, G 1 P G, if G -G 1 then we have z " z 1 , where z " f pGq and z 1 " f pG 1 q.</p><p>Pre-print Besides, z p " g p pzq is specific to the predictive factors and z c " g c pzq is particular to the nonpredictive factors, which means z p and z c are mutually excluded and z p " G. So we have, p pz p , z c q " p pz p q p pz c q p pz p , z c | tpGqq " p pz p | tpGqq p pz c | tpGqq .  From the result demonstrated in Figure <ref type="figure">7</ref>, we can see the optimal reconstruction loss coefficient λ r is different dependent on the specific dataset, but all the values in our experiment can enhance the performance compared with non-reconstruction variant, i.e., λ r " 0, indicating the effectiveness of our proposed cross-view reconstruction mechanism.</p><p>Figure <ref type="figure">8</ref>: Impact of adversarial loss coefficient λ a on different datasets, we specify the non-adversarial situation (λ a " 0) with the dashed line for comparison.</p><p>The Figure <ref type="figure">8</ref> shows that we could further raise the model performance through the adversarial training, which proves a robust representation with less redundant information usually achieve more performance gain compared with the brittle one. During this process, we need to choose a appropriate adversarial loss coefficient λ a , otherwise a too large λ a may hurt the information sufficiency of the learned representation. We put the impacts embedding dimension d and GNN layer number L together because we can find a similar observation form their experimental results. From Figure <ref type="figure">9</ref>, we observe that the optimal</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Emergence of invariance and disentanglement in deep representations</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep Variational Information Bottleneck</title>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Representation Learning: A Review and New Perspectives</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2013.50</idno>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Size-invariant graph representations for graph classification extrapolations</title>
		<author>
			<persName><forename type="first">Beatrice</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangze</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Invariant rationalization</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Simple Framework for Contrastive Learning of Visual Representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v119/chen20j.html" />
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Intriguing properties of contrastive losses</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Calvin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lala</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Elements of information theory</title>
		<author>
			<persName><forename type="first">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><surname>Cover</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning robust representations via multi-view information bottleneck</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Federici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjan</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Forré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Shortcut learning in deep neural networks</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jörn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">node2vec: Scalable Feature Learning for Networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939754</idno>
		<ptr target="https://doi.org/10.1145/2939672.2939754" />
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05584</idno>
		<title level="m">Representation learning on graphs: Methods and applications</title>
				<imprint>
			<date type="published" when="2017">2017b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Contrastive Multi-View Representation Learning on Graphs</title>
		<author>
			<persName><forename type="first">Kaveh</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hosein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khasahmadi</forename></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v119/hassani20a.html" />
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Contrastive multi-view representation learning on graphs</title>
		<author>
			<persName><forename type="first">Kaveh</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hosein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khasahmadi</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to Decompose and Disentangle Representations for Video Prediction</title>
		<author>
			<persName><forename type="first">Pre-Print Jun-Ting</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">De-An</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><forename type="middle">F</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2018/hash/496e05e1aea0a9c4655800e8a7b9ea28-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Open Graph Benchmark: Datasets for Machine Learning on Graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/fb60d411a5c5b72b2e7d3527cfc84fd0-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Variational Graph Auto-Encoders</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Disentangled Contrastive Learning on Graphs</title>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=C_L0Xw_Qf8M" />
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Let invariant rationale discovery inspire graph contrastive learning</title>
		<author>
			<persName><forename type="first">Sihang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Independence promoted graph disentangled networks</title>
		<author>
			<persName><forename type="first">Yanbei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Disentangled Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v97/ma19a.html" />
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automatic shortcut removal for self-supervised representation learning</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">TUDataset: A collection of benchmark datasets for learning with graphs</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franka</forename><surname>Bause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petra</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2007.08663" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning Distributed Representations of Graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mahinthan Chandramohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihui</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Jaiswal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394486.3403168</idno>
		<ptr target="https://doi.org/10.1145/3394486.3403168" />
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Can contrastive learning avoid shortcut solutions? In NeurIPS</title>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kayhan</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suvrit</forename><surname>Sra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Opening the Black Box of Deep Neural Networks via Information</title>
		<author>
			<persName><forename type="first">Ravid</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Ziv</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00810</idno>
		<ptr target="http://arxiv.org/abs/1703" />
		<imprint>
			<date type="published" when="2017-04">April 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">GraphVAE: Towards generation of small graphs using variational autoencoders</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">InfoGraph: Unsupervised and Semisupervised Graph-Level Representation Learning via Mutual Information Maximization</title>
		<author>
			<persName><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno>arXiv preprint physics/0004057</idno>
		<ptr target="https://proceedings.neurips.cc/paper/2021/hash/854f1fb6f65734d9e49f708d6cd84ad6-Abstract.html.NaftaliTishby" />
	</analytic>
	<monogr>
		<title level="m">The information bottleneck method</title>
				<meeting><address><addrLine>Pan Li, Cong Hao, and Jennifer Neville; Fernando C Pereira</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2019. 2021. 2000</date>
		</imprint>
	</monogr>
	<note type="report_type">Pre-print Susheel Suresh</note>
	<note>NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">On mutual information maximization for representation learning</title>
		<author>
			<persName><forename type="first">Josip</forename><surname>Michael Tschannen</surname></persName>
		</author>
		<author>
			<persName><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Paul K Rubenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><surname>Lucic</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<ptr target="https://ui.adsabs.harvard.edu/abs/2018arXiv180703748V" />
		<title level="m">Representation Learning with Contrastive Predictive Coding. arXiv e-prints</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJXMpikCZ" />
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep Graph Infomax</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rklz9iAcKQ" />
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Handling distribution shifts on graphs: An invariance perspective</title>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2022">2022a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graph Information Bottleneck</title>
		<author>
			<persName><forename type="first">Tailin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/ebc2aa04e75e3caabda543a1317160c0-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">NeurIPS. Curran Associates, Inc</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Xiangnan He, and Tat seng Chua. Discovering invariant rationales for graph neural networks</title>
		<author>
			<persName><forename type="first">Ying-Xin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022b</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Moleculenet: a benchmark for molecular machine learning</title>
		<author>
			<persName><forename type="first">Zhenqin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caleb</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aneesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised Feature Learning via Non-Parametric Instance Discrimination</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="https://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Unsupervised_Feature_Learning_CVPR_2018_paper.html" />
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">InfoGCL: Information-Aware Graph Contrastive Learning</title>
		<author>
			<persName><forename type="first">Dongkuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsheng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2021/hash/ff1e68e74c6b16a1a7b5d958b95e120c-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">How Powerful are Graph Neural Networks? In ICLR</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ryGs6iA5Km" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Self-supervised graph-level representation learning with local and global structure</title>
		<author>
			<persName><forename type="first">Minghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Graph Adversarial Self-Supervised Learning</title>
		<author>
			<persName><forename type="first">Longqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjing</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2021/hash/7d3010c11d08cf990b7614d2c2ca9098-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Towards a theoretical framework of out-of-distribution generalization</title>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanlong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruichen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Graph Contrastive Learning with Augmentations</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen ; Tianlong Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/3fe230348e9a12c13120749e3f9fa4cd-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<editor>
			<persName><surname>Neurips</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">Pre-print Yuning You</note>
	<note>Graph contrastive learning automated</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning View-Disentangled Human Pose Representation by Contrastive Cross-View Mutual Information Maximization</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaping</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangzhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://openaccess.thecvf.com/content/CVPR2021/html/Zhao_Learning_View-Disentangled_Human_Pose_Representation_by_Contrastive_Cross-View_Mutual_Information_CVPR_2021_paper.html" />
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">An Empirical Study of Graph Contrastive Learning</title>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<idno>arXiv.org</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
