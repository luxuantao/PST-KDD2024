<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Meta Learning via Local Subgraphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-06-19">19 Jun 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kexin</forename><surname>Huang</surname></persName>
							<email>kexinhuang@hsph.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
							<email>marinka@hms.harvard.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Meta Learning via Local Subgraphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-06-19">19 Jun 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2006.07889v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Prevailing methods for graphs require abundant label and edge information for learning. When data for a new task are scarce, meta learning allows us to learn from prior experiences and form much-needed inductive biases for fast adaption to the new task. Here, we introduce G-META, a novel meta-learning approach for graphs. G-META uses local subgraphs to transfer subgraph-specific information and make the model learn the essential knowledge faster via meta gradients. G-META learns how to quickly adapt to a new task using only a handful of nodes or edges in the new task and does so by learning from data points in other graphs or related, albeit disjoint label sets. G-META is theoretically justified as we show that the evidence for a particular prediction can be found in the local subgraph surrounding the target node or edge. Experiments on seven datasets and nine baseline methods show that G-META can considerably outperform existing methods by up to 16.3%. Unlike previous methods, G-META can successfully learn in challenging, few-shot learning settings that require generalization to completely new graphs and neverbefore-seen labels. Finally, G-META scales to large graphs, which we demonstrate on our new Tree-of-Life dataset comprising of 1,840 graphs, a two-orders of magnitude increase in the number of graphs used in prior work.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph Neural Networks (GNNs) have achieved remarkable results on many tasks such as recommender systems <ref type="bibr" target="#b46">[45]</ref>, molecular predictions <ref type="bibr" target="#b15">[15]</ref>, and knowledge graphs <ref type="bibr" target="#b53">[52]</ref>. Performance in such tasks is typically evaluated after extensive training on datasets where majority of labels are available <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b43">42,</ref><ref type="bibr" target="#b45">44]</ref>. In contrast, many problems require rapid learning from only a few labeled nodes or edges in the graph. Such flexible adaptation, known as meta-learning, has been extensively studied for images and language, e.g., <ref type="bibr" target="#b32">[31,</ref><ref type="bibr" target="#b42">41,</ref><ref type="bibr" target="#b20">20]</ref>. However, meta-learning on graphs has received considerably less research attention and has remained a problem beyond the reach of prevailing GNN models.</p><p>Meta-learning on graphs generally refers to a scenario in which a model learns at two levels. In the first level, rapid learning occurs within a task. For example, when a GNN learns to classify nodes in a particular graph accurately. In the second level, this learning is guided by knowledge accumulated gradually across tasks that captures how task structure changes across target domains <ref type="bibr" target="#b28">[27,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b27">26]</ref>. A powerful GNN trained to meta-learn can quickly learn never-before-seen labels and relations using a low number of data points. As an example, a key problem in biology is to translate insights from non-human organisms (such as yeast, zebrafish, and mouse) to humans <ref type="bibr" target="#b54">[53]</ref>. How to train a GNN to effectively meta-learn on a large number of incomplete and scarcely labeled protein-protein interaction (PPI) graphs from various organisms, transfer the accrued knowledge to humans, and use it to predict the roles of protein nodes in the human PPI graph? While this is a hard unsolved task, it is only an instance of a particular graph meta-learning problem (Figure <ref type="figure" target="#fig_5">1C</ref>). In this problem, the GNN needs to learn on a large number of graphs, each scarcely labeled with a unique label set. Then, it needs to quickly adapt to a never-before-seen graph (e.g., a PPI graph from a new species) and never-before-seen labels (e.g., newly discovered roles of proteins). Current methods V h S L g N N W M L q a + q 0 n q j S L 5 J 0 Z x 9 Q X e C B Z y A g 2 V n o s d z U T 6 L p 3 V u 4 V S 2 7 F n Q E t E y 8 j J c j Q 6 B W / u v 2 I J I J K Q z j W u u O 5 s f F T r A w j n E 4 K 3 U T T G J M R H t C O p R I L q v 1 0 t v E E n V i l j 8 J I 2 S c N m q m / O 1 I s t B 6 L w F Y K b I Z 6 0 Z u K / 3 m d x I Q X f s p k n B g q y X x Q m H B k I j Q 9 H / W Z o s T w s S W Y K G Z 3 R W S I F S b G h l S w I X i L J y + T h 2 r F q 1 X s V 7 2 t l u q X W S J 5 O I J j O A U P z q E O N 9 C A J h C Q 8 A y v 8 O Z o 5 8 V 5 d z 7 m p T k n 6 z m E P 3 A + f w C o S Z B a &lt; / l a t e x i t &gt; ⇠ Gi &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W D J S a g m D k 0 A N 3 s 0 d n b n E r g Z B c B w = "    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single graph &amp; disjoint labels</head><formula xml:id="formula_0">&gt; A A A B 9 H i c b V B N T w I x E J 3 F L 8 Q v 1 K O X R j D x R H b x o E e i B z 1 i I o L C h n R L F x r a 7 q b t m p A N / 8 K L B 7 0 Y r / 4 Y b / 4 b y 7 I H B V / S 5 u W 9 m c z M C 2 L O t H H d b 6 e w s r q 2 v l H c L G 1 t 7 + z u l f c P 7 n W U K E J b J O K R 6 g R Y U 8 4 k b R l m O O 3 E i m I R c N o O x l c z v / 1 E l W a R v D O T m P o C D y U L G c H G S o / V n m Y C X f d Z t V + u u D U 3 A 1 o m X k 4 q k K P Z L 3 / 1 B h F J B J W G c K x 1 1 3 N j 4 6 d Y G U Y 4 n Z Z 6 i a Y x J m M 8 p F 1 L J R Z U + 2 m 2 8 R S d W G W A w k j Z J w 3 K 1 N 8 d K R Z a T 0 R g K w U 2 I 7 3 o z c T / v G 5 i w g s / Z T J O D J V k P i h M O D I R m p 2 P B k x R Y v j E E k w U s 7 s i M s I K E 2 N D K t k Q v M W T l 8 l D v e a d 1 e x X v 6 1 X G p d 5 I k U 4 g m M 4 B Q / O o Q E 3 0 I Q W E J D w D K / w 5 m j n x X l 3 P u a l B S f v O Y Q / c D 5 / A A j Q k J k = &lt; /</formula><formula xml:id="formula_1">n E r g Z B c B w = " &gt; A A A B 9 H i c b V B N T w I x E J 3 F L 8 Q v 1 K O X R j D x R H b x o E e i B z 1 i I o L C h n R L F x r a 7 q b t m p A N / 8 K L B 7 0 Y r / 4 Y b / 4 b y 7 I H B V / S 5 u W 9 m c z M C 2 L O t H H d b 6 e w s r q 2 v l H c L G 1 t 7 + z u l f c P 7 n W U K E J b J O K R 6 g R Y U 8 4 k b R l m O O 3 E i m I R c N o O x l c z v / 1 E l W a R v D O T m P o C D y U L G c H G S o / V n m Y C X f d Z t V + u u D U 3 A 1 o m X k 4 q k K P Z L 3 / 1 B h F J B J W G c K x 1 1 3 N j 4 6 d Y G U Y 4 n Z Z 6 i a Y x J m M 8 p F 1 L J R Z U + 2 m 2 8 R S d W G W A w k j Z J w 3 K 1 N 8 d K R Z a T 0 R g K w U 2 I 7 3 o z c T / v G 5 i w g s / Z T J O D J V k P i h M O D I R m p 2 P B k x R Y v j E E k w U s 7 s i M s I K E 2 N D K t k Q v M W T l 8 l D v e a d 1 e x X v 6 1 X G p d 5 I k U 4 g m M 4 B Q / O o Q E 3 0 I Q W E J D w D K / w 5 m j n x X l 3 P u a l B S f v O Y Q / c D 5 / A A j Q k J k = &lt; /</formula><formula xml:id="formula_2">h E T E Q h t y H a Z w o b t t t m d G k n D 3 / D i Q S / G q 7 / F m / / G B X p Q 8 C U z e X l v J j v 7 g k R w j Y 7 z b R X W 1 j c 2 t 4 r b p Z 3 d v f 2 D 8 u H R g 4 5 T x a D F Y h G r T k A 1 C C 6 h h R w F d B I F N A o E t I P x z c x v P 4 L S P J b 3 O E n A j + h Q 8 p A z i k b y q h 7 C E 2 q W d a f V f r n i 1 J w 5 7 F X i 5 q R C c j T 7 5 S 9 v E L M 0 A o l M U K 1 7 r p O g n 1 G F n A m Y l r x U Q 0 L Z m A 6 h Z 6 i k E W g / m 9 8 8 t c + M M r D D W J m S a M / V 3 x s Z j b S e R I G Z j C i O 9 L I 3 E / / z e i m G V 3 7 G Z Z I i S L Z 4 K E y F j b E 9 C 8 A e c A U M x c Q Q y h Q 3 t 9 p s R B V l a G I q m R D c 5 S + v k m 6 9 5 l 7 U T K v f 1 S u N 6 z y R I j k h p + S c u O S S N M g t a Z I W Y S Q h z + S V v F m p 9 W K 9 W x + L 0 Y K V 7 x y T P 7 A + f w C V f Z I h &lt; / l a t e x i t &gt; Y &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m O I Z Z x f z X W p 2 7 x O X p X s F + N T P x z 0 = " &gt; A A A B 9 n i c b V B N T 8 J A E N 3 i F + I X 6 t F L I 5 h 4 I i 0 e 9 E j 0 4 h E T E Q h t y H a Z w o b t t t m d G k n D 3 / D i Q S / G q 7 / F m / / G B X p Q 8 C U z e X l v J j v 7 g k R w j Y 7 z b R X W 1 j c 2 t 4 r b p Z 3 d v f 2 D 8 u H R g 4 5 T x a D F Y h G r T k A 1 C C 6 h h R w F d B I F N A o E t I P x z c x v P 4 L S P J b 3 O E n A j + h Q 8 p A z i k b y q h 7 C E 2 q W d a f V f r n i 1 J w 5 7 F X i 5 q R C c j T 7 5 S 9 v E L M 0 A o l M U K 1 7 r p O g n 1 G F n A m Y l r x U Q 0 L Z m A 6 h Z 6 i k E W g / m 9 8 8 t c + M M r D D W J m S a M / V 3 x s Z j b S e R I G Z j C i O 9 L I 3 E / / z e i m G V 3 7 G Z Z I i S L Z 4 K E y F j b E 9 C 8 A e c A U M x c Q Q y h Q 3 t 9 p s R B V l a G I q m R D c 5 S + v k m 6 9 5 l 7 U T K v f 1 S u N 6 z y R I j k h p + S c u O S S N M g t a Z I W Y S Q h z + S V v F m p 9 W K 9 W x + L 0 Y K V 7 x y T P 7 A + f w C V f Z I h &lt; / l a t e x i t &gt; Y &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m O I Z Z x f z X W p 2 7 x O X p X s F + N T P x z 0 = " &gt; A A A B 9 n i c b V B N T 8 J A E N 3 i F + I X 6 t F L I 5 h 4 I i 0 e 9 E j 0 4 h E T E Q h t y H a Z w o b t t t m d G k n D 3 / D i Q S / G q 7 / F m / / G B X p Q 8 C U z e X l v J j v 7 g k R w j Y 7 z b R X W 1 j c 2 t 4 r b p Z 3 d v f 2 D 8 u H R g 4 5 T x a D F Y h G r T k A 1 C C 6 h h R w F d B I F N A o E t I P x z c x v P 4 L S P J b 3 O E n A j + h Q 8 p A z i k b y q h 7 C E 2 q W d a f V f r n i 1 J w 5 7 F X i 5 q R C c j T 7 5 S 9 v E L M 0 A o l M U K 1 7 r p O g n 1 G F n A m Y l r x U Q 0 L Z m A 6 h Z 6 i k E W g / m 9 8 8 t c + M M r D D W J m S a M / V 3 x s Z j b S e R I G Z j C i O 9 L I 3 E / / z e i m G V 3 7 G Z Z I i S L Z 4 K E y F j b E 9 C 8 A e c A U M x c Q Q y h Q 3 t 9 p s R B V l a G I q m R D c 5 S + v k m 6 9 5 l 7 U T K v f 1 S u N 6 z y R I j k h p + S c u O S S N M g t a Z I W Y S Q h z + S V v F m p 9 W K 9 W x + L 0 Y K V 7 x y T P 7 A + f w C V f Z I h &lt; / l a t e x i t &gt; Y ⇤ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I d L a B Q D F T R C 0 q f 7 t a 1 T x a 1 L N W p U = " &gt; A A A B + H i c b V A 9 T w J B E J 3 z E / E L t b S 5 C C b G g t x h o S X R x h I T E Q i c Z G / Z g w 1 7 e 5 f d O Z V c + B 8 2 F t o Y W 3 + K n f / G B a 5 Q 8 C U z e X l v J j v 7 / F h w j Y</formula><formula xml:id="formula_3">K k K H W L X x 1 e h F N Q i a R C q J 1 2 3 V i 9 F K i k F P B x v l O o l l M 6 J D 0 W d t Q S U K m v X R 6 9 d g + N k r P D i J l S q I 9 V X 9 v p C T U e h T 6 Z j I k O N D z 3 k T 8 z 2 s n G F x 4 K Z d x g k z S 2 U N B I m y M 7 E k E d o 8 r R l G M D C F U c X O r T Q d E E Y o m q L w J w Z 3 / 8 i J p V c r u W d m 0 y k 2 l W L 3 M E s n B I R z B C b h w D l W 4 h h r U g Y K C Z 3 i F N + v R e</formula><formula xml:id="formula_4">K k K H W L X x 1 e h F N Q i a R C q J 1 2 3 V i 9 F K i k F P B x v l O o l l M 6 J D 0 W d t Q S U K m v X R 6 9 d g + N k r P D i J l S q I 9 V X 9 v p C T U e h T 6 Z j I k O N D z 3 k T 8 z 2 s n G F x 4 K Z d x g k z S 2 U N B I m y M 7 E k E d o 8 r R l G M D C F U c X O r T Q d E E Y o m q L w J w Z 3 / 8 i J p V c r u W d m 0 y k 2 l W L 3 M E s n B I R z B C b h w D l W 4 h h r U g Y K C Z 3 i F N + v R e r</formula><p>H e r Y / Z 6 J K V 7 R z A H 1 i f P 7 q H k r 4 = &lt; / l a t e x i t &gt; are specialized techniques specifically designed for a particular graph meta-learning problem and a particular task <ref type="bibr" target="#b51">[50,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b44">43]</ref>. While these methods provide a promising approach to meta-learning in GNNs, their specific strategy does not scale well and does not extend to other graph meta-learning problems (Figure <ref type="figure" target="#fig_5">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Present work.</head><p>Here, we introduce G-META,<ref type="foot" target="#foot_1">1</ref> a general approach for a variety of graph meta-learning problems (Figure <ref type="figure" target="#fig_5">1</ref>). The core principle of G-META is to represent every node with a local subgraph and use subgraphs to train GNNs to meta-learn. In contrast to G-META, current methods are trained to meta-learn on entire graphs. As we show theoretically and empirically, these methods are unlikely to succeed in few-shot learning settings when the labels are scarce and scattered around multiple graphs. Furthermore, these methods capture the overall graph structure, but at the loss of finer local structures. Our theoretical analysis (Section 4) suggests that the evidence for a particular prediction can be found in the subgraph surrounding the target node or edge when using GNNs. Besides, G-META's construction of local subgraphs gives local structural representations that enable direct structural similarity comparison using GNNs, based on its connection to Weisfeiler-Lehman test <ref type="bibr" target="#b45">[44,</ref><ref type="bibr" target="#b49">48]</ref>. Further, structural similarity enables G-META to form the much-needed inductive bias by using a metric-learning algorithm <ref type="bibr" target="#b30">[29]</ref>. Moreover, local subgraphs also allow for effective feature propagation and label smoothing within a GNN.</p><p>(1) G-META is general. While previous meta graph learning methods <ref type="bibr" target="#b51">[50,</ref><ref type="bibr" target="#b2">2]</ref> apply only to one graph meta-learning problem (Figure <ref type="figure" target="#fig_5">1</ref>), G-META works for all of them (Appendix B). (2) G-META yields powerful predictors. We demonstrate G-META's performance on seven datasets and nine baselines, where G-META considerably outperform baselines by up to 16.3%. (3) G-META is scalable. By operating on subgraphs, only small neighborhoods need to be examined by the GNN. We show how G-META scales to large graphs by applying it to our new Tree-of-Life dataset comprising of 1,840 graphs, a two-orders of magnitude increase in the number of graphs used in prior work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background and Problem Formulation</head><p>Let G = {G 1 , . . . , G N } denotes N graphs. For each graph G = (V, E, X), V is the set of nodes, E is the set of edges and X = {x 1 , . . . , x n }, x u ∈ R d is the d-dimensional node feature for each node in V. We denote Y = {Y 1 , . . . , Y M } as M labels and a label set Y is a set of labels selected from Y. The G-META's core principle is to represent nodes with local subgraphs and then use subgraphs to easily transfer knowledge across tasks, graphs, and label sets. We use S to denote local subgraphs for nodes, S = {S 1 , . . . , S n }. Hence, the classification task is to learn a GNN f θ : S → {1, . . . , |Y|}, i.e., to correctly map node u's local subgraph S u to labels in Y given only a handful of labeled nodes.</p><p>Background on Graph Neural Networks. GNNs learn compact representations, i.e., embeddings, that capture network structure and node features. The input to GNN is a graph G. The output is generated via a series of propagation layers <ref type="bibr" target="#b10">[10]</ref> </p><formula xml:id="formula_5">uv = MSG(h (l−1) u , h (l−1) v</formula><p>) for every linked nodes u, v based on their embeddings from the previous layer h </p><formula xml:id="formula_6">(l−1) u and h (l−1) v . (<label>2</label></formula><formula xml:id="formula_7">u = UPD( m(l) u , h (l−1) u</formula><p>) using the aggregated message and the embedding from the previous layer, for each node u.</p><p>Background on Meta-learning. In meta-learning, we have a meta-set D that consists of D train , D val , D test . This meta-set consists of many tasks. Each task T i ∈ D can be divided into T support i and T query i . T support i has K support labeled data points in each label for learning and T query i has K query data points in each label for evaluation. The size of the label set for the meta-set |Y| is N. It is also called N-ways K support -shots learning problem. During meta-training, for T i from D train , the model first learns from T support i and then evaluates on T query i to see how well the model performs on that task. The goal of Model-Agnostic Meta-Learning (MAML) <ref type="bibr" target="#b9">[9]</ref> is to obtain a parameter initialization θ * that can adapt to unseen tasks quickly, such as D test , using gradients information learnt during meta-training. Hyperparameters are tuned via D val .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">G-META: Problem Formulation</head><p>G-META is an approach that can tackle three fundamentally different meta-learning problems (Figure <ref type="figure" target="#fig_5">1</ref>). Shared label refers that every task shares the same label set Y. Disjoint label refers to the situation where the label sets for tasks i and j are disjoint, i.e., Y i ∩ Y j = ∅. Each data point in a task T i is a local subgraph S u , along with its associated label Y u . The overall goal is to adapt to an unseen task T * ∼ p(T ) with a few examples after observing related tasks T i ∼ p(T ).</p><p>Graph meta-learning problem 1: Single Graph and Disjoint Labels. We have a graph G with a distribution of label set p(Y|G). The goal is to adapt to an unseen label set Y * ∼ p(Y|G) by learning from tasks with other label sets Y i ∼ p(Y|G), where Y i ∩ Y * = ∅ for every label set Y i .</p><p>Graph meta-learning problem 2: Multiple Graphs and Shared Labels. We have a distribution of graphs p(G) but one label set Y. The goal is to learn from another graph G j ∼ p(G) and quickly adapt to an unseen graph G * ∼ p(G), where G j and G * are disjoint. All tasks share the same labels.</p><p>Graph meta-learning problem 3: Multiple Graphs and Disjoint Labels. We have a distribution of label sets p(Y|G) conditioned on multiple graphs G. In contrast to Problem 1, each task has its own label set Y i but those label sets can appear in one or more graphs. The goal is to adapt to an unseen label set Y * ∼ p(Y|G) after learning from some other label set Y i ∼ p(Y|G), where Y i ∩ Y * = ∅.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Local Subgraphs and Theoretical Motivation for G-META</head><p>We start by describing how to construct local subgraphs in G-META. We then provide a theoretical justification showing that local subgraphs preserve useful information from the entire graph. We then argue how subgraphs enable G-META to capture sufficient information on the graph structure, node features, and labels, and use that information for graph meta-learning.</p><p>For node u, a local subgraph is defined as a subgraph S u = (V u , E u , X u ) induced from a set of nodes {v|d(u, v) ≤ h}, where d(u, v) is the shortest path distance between node u and v, and h defines the neighborhood size. In a meta-task, subgraphs are sampled from graphs or label sets, depending on the graph meta-learning problems defined in Section 3. We then use GNNs to encode the local subgraphs. However, one straightforward question raised is if this subgraph loses information by excluding nodes outside of it. Here, we show in theory that applying GNN on the local subgraph preserve useful information compared to using GNN on the entire graph.</p><p>Preliminaries and definitions. Next, we use GCN <ref type="bibr" target="#b17">[17]</ref> as an exemplar GNN to understand how nodes influence each other during neural message passing. The assumptions are based on <ref type="bibr" target="#b40">[39]</ref>  </p><formula xml:id="formula_8">I u,v = ∂x (∞) u /∂x (∞) v</formula><p>, where the norm is any subordinate norm and the Jacobian measures how a change in v translates to a change in u <ref type="bibr" target="#b40">[39]</ref>. ( <ref type="formula" target="#formula_6">2</ref> </p><formula xml:id="formula_9">(u) = I G (u) − I Su (u)</formula><p>, where I G (u) is the influence of entire graph G, and I Su (u) is the influence of local subgraph S u . Next, we show how influence spreads between nodes depending on how far the nodes are from each other in a graph. Theorem 1 (Decaying Property of Node Influence). Let t be a path between node u and node v and let D t GM be a geometric mean of node degrees occurring on path t.</p><formula xml:id="formula_10">Let D t * GM = min t {D t GM } and h * = d(u, v). Consider the node influence I u,v from v to u. Then, I u,v ≤ C/(D t * GM ) h * .</formula><p>The proof is deferred to Appendix C. Theorem 1 states that the influence of a node v on node u decays exponentially as their distance h * increases. We then show that for a node u, operating GNN on a local subgraph does not lose useful information than operating GNN on the entire graph. Theorem 2 (Local Subgraph Preservation Property). Let S u be a local subgraph for node u with neighborhood size h. Let node v be defined as: v = argmax w ({I u,w |w ∈ V \ V u }). Let t be a path between u and v and let D t GM be a geometric mean of node degrees occurring on path t. Let D t * GM = mint{D t GM }. The following holds: R h (u) ≤ C/(D t * GM ) h+1 . The proof is deferred to Appendix D. Theorem 2 states that the graph influence loss is bounded by an exponentially decaying term as h increases. In other words, our local subgraph formulation is an h-th order approximation of applying GNN for the entire graph.</p><p>Local subgraphs enable few-shot meta-learning. Equipped with the theoretical justification, we now describe how the local subgraph allows G-META to do few-shot meta-learning.  <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b37">36]</ref>, especially when node labels are scarce. The GNN representations cannot fully capture large graphs structure because they are too complicated <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b45">44]</ref>. However, they can learn representations that capture the structure of small graphs, such as our local subgraphs, as is evidenced by the connection to the Weisfeiler-Lehman test <ref type="bibr" target="#b45">[44,</ref><ref type="bibr" target="#b49">48]</ref>  <ref type="formula">3</ref>) Labels Labels Labels Labels Labels Labels Labels Labels Labels Labels Labels Labels Labels Labels Labels Labels Labels. When only a handful of nodes are labeled, it is challenging to efficiently propagate the labels through the entire graph <ref type="bibr" target="#b52">[51,</ref><ref type="bibr" target="#b18">18]</ref>. Metric-learning methods <ref type="bibr" target="#b30">[29]</ref> learn a task-specific metric to classify query set data using the closest point from the support set. It has been proved as an effective inductive bias <ref type="bibr" target="#b30">[29,</ref><ref type="bibr" target="#b35">34]</ref>. Equipped with subgraph representations that capture both structure and feature information, G-META uses metric-learning by comparing the query subgraph embedding to the support subgraph embedding. As such, it circumvents the problem of having too little label information for effective propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">G-META: Meta Learning via Local Subgraphs</head><p>G-META (Figure <ref type="figure" target="#fig_20">2</ref>) is an approach for meta-learning on graphs. Building on theoretical motivation from Section 4, G-META first constructs local subgraphs. It then uses a GNN encoder to generate </p><formula xml:id="formula_11">M t V + u u D V 3 D r x K v J x U U I 5 m v / z V C x V N Y y a B C m J M 1 3 M T 8 D O i g V P B p q V e a l h C 6 I g M W N d S S W J m / G y + 7 x S f W S X E k d L 2 S c B z 9 X d H R m J j J n F g K 2 M C Q 7 P s z c T / v G 4 K 0 Z W f c Z m k w C R d D I p S g U H h 2 f E 4 5 J p R E B N L C N X c 7 o r p k G h C w U Z U s i F 4 y y e v k s d 6 z b u o 2 a 9 + V 6 8 0 r v N E i u g E n a J z 5 K F L 1 E C 3 q I l a i C K B n t E r e n O e n B f n 3 f l Y l B a c v O c Y / Y H z + Q N M K 5 A 9 &lt; / l a t e x i t &gt; G 2</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 f q H r 3 C Q / C P P H B l m 9 g S T 0 m 0 Z 5               <ref type="formula">9</ref>) Using the support prototypes and the query embeddings, the query loss L i query for task Ti is computed. Steps (2-9) are repeated for η update steps. The same process repeats for the other m sampled tasks, starting from the same GNN f θ . <ref type="bibr" target="#b10">(10)</ref> The last update step's query loss from all the tasks are summed up and used to update θ. Then, another batch of tasks are sampled, and step (1-10) are repeated. Then, for meta-testing tasks, steps (1-9) are repeated with the GNN using the meta-learned parameter θ * , which enables generalization over unseen tasks. See Algorithm 1 (Appendix E). centroid embeddings for subgraphs. Finally, it uses prototypical loss for inductive bias and MAML for knowledge transfer across graphs and labels. The overview is in Algorithm 1 (Appendix E).</p><formula xml:id="formula_12">0 k = " &gt; A A A B 7 3 i c b V A 9 T 8 M w E L 2 U r 1 K + C o w s F i 0 S U 5 W E A c Y K B h i L R K C o j S r H d V q r j h 3 Z D l I V 9 T e w M M C C W P k 7 b P w b 3 D Y D t D z J 1 t N 7 d 7 q 7 F 6 W c a e O 6 3 0 5 p Z X V t f a O 8 W d n a 3 t n d q + 4 f 3 G u Z K U I D I r l U 7 Q h r y p m g g W G G 0 3 a q K E 4 i T h + i 0 d X U f 3 i i S j M p 7 s w 4 p W G C B 4 L F j G B j p a B + 3 f P r v W r N b b g z o G X i F a Q G B V q 9 6 l e 3 L 0 m W U G E I x 1 p 3 P D c 1 Y Y 6 V Y Y T T S a W b a Z p i M s I D 2 r F U 4 I T q M J 8 t O 0 E n V u m j W C r 7 h E E z 9 X d H j h O t x 0 l k K x N s h n r R m 4 r / e Z 3 M x B d h z k S a G S r I f F C c c W Q k m l 6 O + k x R Y v j Y E k w U s 7 s i M s Q K E 2 P z q d g Q v M W T l</formula><formula xml:id="formula_13">P O + n Z X V t f W N z c J W c X t n d 2 + / d H B 4 b 1 S q K W t S J Z R u h 8 Q w w S V r I k f B 2 o l m J A 4 F a 4 W j 6 6 n f e m L a c C X v c J y w I C Y D y S N O C V q p X e n S v k J T 6 Z X K X t W b w V 0 m f k 7 K k K P R K 3 1 1 + 4 q m M Z N I B T G m 4 3 s J B h n R y K l g k 2 I 3 N S w h d E Q G r G O p J D E z Q T b b d + K e W q X v R k r b J 9 G d q b 8 7 M h I b M 4 5 D W x k T H J p F b y r + 5 3 V S j C 6 D j M s k R S b p f F C U C h e V O z 3 e 7 X P N K I q x J Y R q b n d 1 6 Z B o Q t F G V L Q h + I s n L 5 O H W t U /</formula><formula xml:id="formula_14">P O + n Z X V t f W N z c J W c X t n d 2 + / d H B 4 b 1 S q K W t S J Z R u h 8 Q w w S V r I k f B 2 o l m J A 4 F a 4 W j 6 6 n f e m L a c C X v c J y w I C Y D y S N O C V q p X e n S v k J T 6 Z X K X t W b w V 0 m f k 7 K k K P R K 3 1 1 + 4 q m M Z N I B T G m 4 3 s J B h n R y K l g k 2 I 3 N S w h d E Q G r G O p J D E z Q T b b d + K e W q X v R k</formula><formula xml:id="formula_15">U K v m Q 3 L + / N Z G Z e m A h u 0 P O + n Z X V t f W N z c J W c X t n d 2 + / d H B 4 b 1 S q K W t S J Z R u h 8 Q w w S V r I k f B 2 o l m J A 4 F a 4 W j 6 6 n f e m L a c C X v c J y w I C Y D y S N O C V q p X e n S v k J T 6 Z X K X t W b w V 0 m f k 7 K k K P R K 3 1 1 + 4 q m M Z N I B T G m 4 3 s J B h n R y K l g k 2 I 3 N S w h d E Q G r G O p J D E z Q T b b d + K e W q X v R k r b J 9 G d q b 8 7 M h I b M 4 5 D W x k T H J p F b y r + 5 3 V S j C 6 D j M s k R S b p f F C U C h e V O z 3 e 7 X P N K I q x J Y R q b n d 1 6 Z B o Q t F G V L Q h + I s n L 5 O H W t U / r 9 q v d l s r 1 6 / y R A p w D C d w B j 5 c Q B 1 u o A F N o C D g G V 7 h z X l 0 X p x 3 5 2 N e u u L k P U f w B 8 7 n D y 7 H k C o = &lt; / l a t e x i t &gt; • • • &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a 0 m 1 w 3 r G 2 B Q W F o Q a N X e l H o b Y l 7 4 = " &gt; A A A B 8 n i c b V A 9 T w J B E J 3 z E / E L t b S 5 C C Z W 5 A 4 L L Y k 2 l p i I Y O B C 9 p Y 9 2 L C 3 e + 7 O m Z A L f 8 L G Q h t j 6 6 + x 8 9 + 4 w B U K v m Q 3 L + / N Z G Z e m A h u 0 P O + n Z X V t f W N z c J W c X t n d 2 + / d H B 4 b 1 S q K W t S J Z R u h 8 Q w w S V r I k f B 2 o l m J A 4 F a 4 W j 6 6 n f e m L a c C X v c J y w I C Y D y S N O C V q p X e n S v k J T 6 Z X K X t W b w V 0 m f k 7 K k K P R K 3 1 1 + 4 q m M Z N I B T G m 4 3 s J B h n R y K l g k 2 I 3 N S w h d E Q G r G O p J D E z Q T b b d + K e W q X v R k r b J 9 G d q b 8 7 M h I b M 4 5 D W x k T H J p F b y r + 5 3 V S j C 6 D j M s k R S b p f F C U C h e V O z 3 e 7 X P N K I q x J Y R q b n d 1 6 Z B o Q t F G V L Q h + I s n L 5 O H W t U / r 9 q v d l s r 1 6 / y R A p w D C d w B j 5 c Q B 1 u o A F N o C D g G V 7 h z X l 0 X p</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " V W H H j q y 1 8 M S Y + 8 D q b 2 u J Q P G h e u A = " &gt;</head><formula xml:id="formula_16">A A A B / X i c b V C 9 T s M w G P x S / k r 5 S 2 F k i W i R m K q k D D B W s D A W q a V F b R Q 5 r t N a d Z z I d k B V y K O w M M C C W H k P N t 4 G p 8 0 A L S f Z O t 1 9 n 3 w + P 2 Z U K t v + N k p r 6 x u b W + X t y s 7 u 3 v 6 B W T 2 8 k 1 E i M O n i i E W i 7 y N J G O W k q 6 h i p B 8 L g k K f k Z 4 / v c 7 9 3 g M R k k a 8 o 2 Y x c U M 0 5 j S g G C k t e W a 1 P g y R m m D E 0 k 7 m p T S r e 2 b N b t h z W K v E K U g N C r Q 9 8 2 s 4 i n A S E q 4 w Q 1 I O H D t W b o q E o p i R r D J M J I k R n q I x G W j K U U i k m 8 6 j Z 9 a p V k Z W E A l 9 u L L m 6 u + N F I V S z k J f T + Y 5 5 b K X i / 9 5 g 0 Q F l 2 5 K e Z w o w v H i o S B h l o q s v A d r R A X B i s 0 0 Q V h Q n d X C E y Q Q V r q t i i 7 B W f 7 y K r l v N p z z h</formula><formula xml:id="formula_17">P O + n Z X V t f W N z c J W c X t n d 2 + / d H B 4 b 1 S q K W t S J Z R u h 8 Q w w S V r I k f B 2 o l m J A 4 F a 4 W j 6 6 n f e m L a c C X v c J y w I C Y D y S N O C V q p X e n S v k J T 6 Z X K X t W b w V 0 m f k 7 K k K P R K 3 1 1 + 4 q m M Z N I B T G m 4 3 s J B h n R y K l g k 2 I 3 N S w h d E Q G r G O p J D E z Q T b b d + K e W q X v R k</formula><formula xml:id="formula_18">P O + n Z X V t f W N z c J W c X t n d 2 + / d H B 4 b 1 S q K W t S J Z R u h 8 Q w w S V r I k f B 2 o l m J A 4 F a 4 W j 6 6 n f e m L a c C X v c J y w I C Y D y S N O C V q p X e n S v k J T 6 Z X K X t W b w V 0 m f k 7 K k K P R K 3 1 1 + 4 q m M Z N I B T G m 4 3 s J B h n R y K l g k 2 I 3 N S w h d E Q G r G O p J D E z Q T b b d + K e W q X v R k</formula><formula xml:id="formula_19">P O + n Z X V t f W N z c J W c X t n d 2 + / d H B 4 b 1 S q K W t S J Z R u h 8 Q w w S V r I k f B 2 o l m J A 4 F a 4 W j 6 6 n f e m L a c C X v c J y w I C Y D y S N O C V q p X e n S v k J T 6 Z X K X t W b w V 0 m f k 7 K k K P R K 3 1 1 + 4 q m M Z N I B T G m 4 3 s J B h n R y K l g k 2 I 3 N S w h d E Q G r G O p J D E z Q T b b d + K e W q X v R k</formula><formula xml:id="formula_20">c + N l k 4 x E 5 t k q X h L G 2 T y G Z q L 8 7 M h o Z M 4 w C W x l R 7 J t 5 b y z + 5 7 V S D C / 8 T K g k R a 7 Y d F C Y S o I x G Z 9 P u k J z h n J o C W V a 2 F 0 J 6 1 N N G d q Q 8 j Y E b / 7 k R X J f K X t n Z f t V b i r F 6 u U s k R w c w h G c g A f n U I V r q E E d G C h</formula><formula xml:id="formula_21">I k Q m q V N O x Y 3 R T K p E z A V m + l S i I K e v T L j Q 1 D W k A y k 1 H V 2 T W g V b a V i e S + o V o j d T f H S k N l B o E v q 4 c</formula><formula xml:id="formula_22">S i Q n X V W N Q a 4 1 8 E z 2 T I Q 4 = " &gt; A A A C C H i c b V A 9 T 8 M w E H X 4 L O U r w A h D R I v E V C V l g L G C h Y G h S J Q W N V H k u E 5 r 1 X E i + 4 K o o i 4 s / B U W B l g Q K z + B j X + D 0 2 a A l i f Z e n r v T n f 3 g o Q z B</formula><formula xml:id="formula_23">S i Q n X V W N Q a 4 1 8 E z 2 T I Q 4 = " &gt; A A A C C H i c b V A 9 T 8 M w E H X 4 L O U r w A h D R I v E V C V l g L G C h Y G h S J Q W N V H k u E 5 r 1 X E i + 4 K o o i 4 s / B U W B l g Q K z + B j X + D 0 2 a A l i f Z e n r v T n f 3 g o Q z B</formula><formula xml:id="formula_24">F o = " &gt; A A A C C n i c b V C 7 T s N A E D y H V w g v A y U S s k i Q q C I 7 F F B G 0 F A G K S F B s b H O l 3 N y y v m h u z U i s t z R 8 C s 0 F N A g W r 6 A j r / h n L i A h J H u N J</formula><formula xml:id="formula_25">Z G C d A Q z I b 5 C f c g M j I Q z E G T F A C f K I I J o K p X Q 0 y w g I T U N F V V A j W / M m L 5 L Z R t 0 7 r 6 m t c N 6 r N i y K R M j p A R + g E W e g M N d E V a q E O I u g R P a N X 9 K Y 9 a S / a u / Y x K y 1 p R c 8 + + g P t 8 w f V Z J q c &lt; / l a t e x i t &gt; T query i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x 6 6 R q E O / 2 U 6 U r J c d k P 6 A U 6 h X i E g = " &gt; A A A C C H i c b V C 7 T s N A E D z z D O F l o I T C I k G i i u x Q Q B l B Q x m k h A Q l J j p f 1 s k p 5 w d 3 a 0 R k u a H h V 2 g o o E G 0 f A I d f 8 P l U U D C S H c a z e x q d 8 e L B V d o 2 9 / G w u L S 8 s p q b i 2 / v r G 5 t W 3 u 7 F 6 r K J E M 6 i w S k W x 6 V I H g I d S R o 4 B m L I E G n o C G N 7 g Y + Y 1 7 k I p H Y Q 2 H M b g B 7 Y X c 5 4 y i l j r m Q b E d U O w z K t J a d t t G e M D 0 L g E 5 z D q 8 2 D E L d s k e w 5 o n z p Q U y B T V j v n V 7 k Y s C S B E J q h S L c e O 0 U 2 p R M 4 E Z P l 2 o i C m b E B 7 0 N I 0 p A E o N x 1 f k V l H W u l a f i T 1 C 9 E a q 7 8 7 U h o o N Q w 8 X T l a W c 1 6 I / E / r 5 W g f + a m P I w T h J B N B v m J s D C y R p F Y X S 6 B o R h q Q p n</formula><p>Neural encoding of subgraph centroids. In each meta-task, we first construct subgraphs for each node. Then, each subgraph S u is fed into a h-layer GNN to obtain an embedding for every node in the subgraph. h is set to be the subgraph neighborhood size. The centroid node u's embedding is used to represent the subgraph h u = GNN(S u ) centroid . We use centroid node embedding instead of a readout function such as sum/mean over every node in the subgraph, as in the typical graph classification settings <ref type="bibr" target="#b45">[44,</ref><ref type="bibr" target="#b4">4]</ref>. The reason is that as we relax h to be a large number, the subgraph could potentially be large. A standard readout function over every node in the subgraph may add noise. When h is set to be the graph's diameter, the centroid embedding is the same as the node embedding generated by applying GNN on the entire graph. Note that for faster computation, we mini-batch the subgraphs for each meta-task's support and query set.</p><p>Prototypical loss. After we obtain a sufficient subgraph representation, we further leverage the inductive bias between representation and labels to circumvent the problem of scarce label information for effective propagation in the few-shot settings. For each label k, we take the mean of the support set subgraph embeddings to obtain prototype c k : c k = 1/N k yj =k h j . The prototype c k serves as a landmark for the label k. Then, for each local subgraph S u in both support and query set, a class distribution vector p is computed via the euclidean distance between the support prototypes for each class and the subgraph centroid embeddings:</p><formula xml:id="formula_26">p k = exp(− hu−c k ) k exp(− hu−c k )</formula><p>. After obtaining a prediction probability distribution over label classes p for each local subgraph S u , a cross-entropy loss is used to backpropagate L(p, y) = j y j log p j , where y is the true label's one hot encoding.</p><p>Optimization-based meta-learning. To transfer the structural knowledge across graphs and labels, we use MAML, an optimization-based meta-learning approach. We break the node's dependency </p><formula xml:id="formula_27">i : θ j = θ j−1 − α∇L support .</formula><p>The updated parameter is then evaluated using the query set, and the query loss for task i is recorded as L i query . The above steps are repeated for η update steps. Then, the L i query from the last update step is summed up across the batch of tasks, and then we perform a meta-update step: θ = θ − β∇ i L i query . Next, new tasks batch are sampled, and the same iteration is applied on the meta-updated θ. During meta-testing, the same procedure above is applied using the final meta-updated parameter θ * . θ * is learned from knowledge across meta-training tasks and is the optimal parameter to adapt to unseen tasks quickly.  <ref type="bibr" target="#b9">[9]</ref> switches ProtoNet to MAML as the meta-learner. All experiments use the same number of query points per label. We use multi-class accuracy metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Results</head><p>Overview of results. Synthetic dataset results are reported in Table <ref type="table" target="#tab_7">2</ref> and real-world datasets in Table <ref type="table" target="#tab_8">3</ref>. We see that G-META can consistently achieve the best accuracy in almost all tested graph meta-learning problems and on both node classification and link prediction tasks. Notably, G-META achieves a 15.1% relative increase in the single graph disjoint setting, a 16.3% relative increase in G-META is highly predictive, general graph meta-learning method. Across meta-learning models, we observe G-META is consistently better than others such as MAML and ProtoNet. MAML and ProtoNet have volatile results across different problems and tasks, whereas G-META is stable. This result confirms our analysis of using the prototypical loss to leverage the label inductive bias and MAML to transfer knowledge across graphs and labels. By comparing G-META to two relevant existing works Meta-GNN and Meta-Graph, we see G-META can work across different problems. In contrast, the previous two methods are restricted by the meta graph learning problems. In real-world datasets, we observe No-Finetune is better than Finetune in many problems (ogbn-arxiv &amp; Fold-PPI). This observation shows that the meta-training datasets bias the meta-testing result, suggesting the importance of meta-learning algorithm to achieve positive transfer in meta graph learning.</p><p>Local subgraphs are vital for successful graph few-shot learning. We observe that Meta-GNN, FS-GIN, and FS-SGC, which operate on the entire graph, perform poorly. In contrast, G-META can learn transferable signal, even when label sets are disjoint. Since the problem requires learning using only 3-shots in a large graph of around 160 thousands nodes and 1 million edges (ogbn-arxiv), we posit that operating GNN on the entire graph would not propagate useful information. Hence, the performance increase suggests that local subgraph construction is useful, especially in large graphs. Finally, in the Tree-of-Life link prediction problem, we studied the largest-ever graph meta-learning dataset, spanning 1,840 PPI graphs. This experiment also supports the scalability of G-META, which is enabled by the local subgraphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We introduce G-META, a scalable and inductive meta-learning method for graphs. Unlike earlier methods, G-META excels at difficult few-shot graph learning tasks and can tackle a variety of graph meta-learning problems. The core principle in G-META is to use local subgraphs to identify and transfer useful information across tasks. The local subgraph approach is fundamentally different from prior work using entire graphs, which only captures broad structure at the loss of finer topological structure. Local subgraphs are theoretically justified and stem from our observation that evidence for a particular label/link can be found in the local subgraph surrounding the target nodes. G-META outperforms nine baselines across seven datasets, including a novel dataset of 1,840 graphs.</p><p>Definition (Graph Influence): Graph influence I G on u is: I G (u) = [I u,v1 , . . . , I u,vn ] 1 , where [I u,v1 , . . . , I u,vn ] is a vector representing the influence of other nodes on u.</p><p>The graph influence of graph G on node u is: I G (u) = i∈V I u,vi . Similarly, the influence of a h-hop neighborhood subgraph S u on node u is: I Su (u) = i∈V u I u,vi where V u contains nodes that are at most h hops away from node u, i.e., {i x |d(i x , u) ≤ h}.</p><p>Definition (Graph Influence Loss): Graph influence loss R h is defined as: R h (u) = I G (u) − I Su (u), where I G (u) is the influence of entire graph G and I Su (u) is the influence of subgraph S u .</p><p>Notationally, we denote m paths between node u and v as: p 1 , ..., p m . We use p x i , p x j , ..., p x nv to represent nodes occurring on path p x and use n v to denote the length of the path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Theorem and Proof</head><p>Theorem 1 (Decaying Property of Node Influence). Let t be a path between node u and node v and let D t GM be a geometric mean of node degrees occurring on path t. Let D t * GM = min t {D t GM } and h * = d(u, v). Consider the node influence I u,v from v to u. Then, I u,v ≤ C/(D t * GM ) h * .</p><p>Proof. Using the GCN propagation rule, we see that node u's output is defined as:</p><formula xml:id="formula_28">x (∞) u = 1 D uu i∈N (u) a ui x (∞) i</formula><p>, where a ui is the edge weight between node u and node i. In our datasets and many other real-world graphs, all edges have the same weight 1.</p><p>By an expansion of nodes in the neighbor N (u), we have:</p><formula xml:id="formula_29">x (∞) u = 1 D uu i∈N (u) a ui 1 D ii j∈N (i) a ij x (∞) j .</formula><p>In the same logic, it can be further expanded as:</p><formula xml:id="formula_30">x (∞) u = 1 D uu i∈N (u) a ui 1 D ii j∈N (i) a ij • • • 1 D mm o∈N (m) a mo x (∞) o .<label>(1)</label></formula><p>The node influence</p><formula xml:id="formula_31">I u,v = ∂x (∞) u ∂x (∞) v</formula><p>can then be computed via: u , the partial derivative on nodes that are not in the paths p 1 , ..., p m between node u and v becomes 0 and these nodes are thus removed. Note that each term in line 2 is for one path between node u and v. This also means that the feature influence can be decomposed into the sum of all paths feature influence. (Line 3) We separate the scalar terms and the derivative term within the matrix norm and then uses the absolute homogeneous property (i.e. αA = |α| A ) of the matrix norm to convert them into the matrix norm of Jacobian matrix times the absolute value of the scalar. (Line 4) We know the Jacobian of the same vectors = 1. We also move the degree and edge weight terms around to group each together for each path. (Line 5) Notice now the equation becomes the sum of terms around paths between node u and v. We can identify the maximum of these terms and it is smaller than the m times the maximum term. We denote the term with the maximum value as the path p t * . (Line 6) Clean the notations. (Line 7) We move all the constant as C in the last line. Note that in all of our datasets and many real-world datasets, the network is usually binary and the edge weight a are non-negative (in many cases a = 1), thus we remove the absolute values. (Line 8) We rephrase the degree term into geometric mean format 1/D GM . (Line 9) h * is the shortest path and shorter than n * , thus the inequality. We then obtain the bound of the node influence.</p><formula xml:id="formula_32">∂x (∞) u ∂x (∞) v = ∂ ∂x (∞) v   1 D uu i∈N (u) a ui 1 D ii j∈N (i) a ij • • • 1 D mm o∈N (m) a mo x (∞) o   [1] = ∂ ∂x (∞) v 1 D uu a (up 1 i ) 1 D p 1 i p 1 i a (p 1 i p 1 j ) • • • 1 D p 1 n 1 p 1 n 1 a (p 1 n 1 v) x (∞) v + • • • + 1 D uu a (up m i ) 1 D p m i p m i a (p m i p m j ) • • • 1 D p m nm p m nm a (p m nm v) x (∞) v [2] = ∂x (∞) v ∂x (∞) v • | 1 D uu a (up 1 i ) 1 D p 1 i p 1 j a (p 1 i p 1 j ) • • • 1 D p 1 n 1 p 1 n 1 a (p 1 n 1 v) + • • • + 1 D uu a (up m i ) 1 D p m i p m i a (p m i p m j ) • • • 1 D p m nm p m nm a (p m nm v) | [3] = | 1 D uu D p 1 i p 1 i • • • D p 1 n 1 p 1 n 1 a (up 1 i ) a (p 1 i p 1 j ) • • • a (p 1 n 1 v) + • • • + 1 D uu D p 1 i p 1 j • • • D p m nm p m nm a (up m i ) a (p m i p m j ) • • • a (p m nm v) | [4] ≤ |m * max 1 D uu D p 1 i p 1 i • • • D p 1 n 1 p 1 n 1 a (up 1 i ) a (p 1 i p 1 j ) • • • a (p 1 n 1 v) , • • •, 1 D uu D p 1 i p 1 i • • • D p m nm p m</formula><p>Note that if we assume degree of nodes along paths are random, then the p t * , which is the path that has the smallest geometric mean of node degrees, is the shortest path from node u to v.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " 7 M k p R O u O 7 c l F A b 9 g x o 7 F Q w Z 8 p 0 g = " &gt; A A A B 8 n i c b V B N T w I x E J 3 F L 8 Q v 1 K O X R j D x R H b x o E e i B z 1 i I o K B D e m W L j S 0 3 b X t m p A N f 8 K L B 7 0 Y r / 4 a b / 4 b y 7 I H B V / S 5 u W 9 m c z M C 2 L O t H H d b 6 e w s r q 2v l H c L G 1 t 7 + z u l f c P 7 n W U K E J b J O K R 6 g R Y U 8 4 k b R l m O O 3 E i m I R c N o O x l c z v / 1 E l W a R v D O T m P o C D y U L G c H G S p 1 q T z O B r q v 9 c s W t u R n Q M v F y U o E c z X 7 5 q z e I S C K o N I R j r b u e G x s / x c o w w u m 0 1 E s 0 j T E Z 4 y H t W i q x o N p P s 3 2 n 6 M Q q A x R G y j 5 p U K b + 7 k i x 0 H o i A l s p s B n p R W 8 m / u d 1 E x N e + C m T c W K o J P N B Y c K R i d D s e D R g i h L D J 5 Z g o p j d F Z E R V p g Y G 1 H J h u A t nr x M H u o 1 7 6 x m v / p t v d K 4 z B M p w h E c w y l 4 c A 4 N u I E m t I A A h 2 d 4 h T f n 0 X l x 3 p 2 P e W n B y X s O 4 Q + c z x + H x 4 + 9 &lt; / l a t e x i t &gt; ⇠ G &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 M k p R O u O 7 c l F A b 9 g x o 7 F Q w Z 8 p 0 g = " &gt; A A A B 8 n i c b V B N T w I x E J 3 F L 8 Q v 1 K O X R j D x R H b x o E e i B z 1 i I o K B D e m W L j S 0 3 b X t m p A N f 8 K L B 7 0 Y r / 4 a b / 4 b y 7 I H B V / S 5 u W 9 m c z M C 2 L O t H H d b 6 e w s r q 2 v l H c L G 1 t 7 + z u l f c P 7 n W U K E J b J O K R 6 g R Y U 8 4 k b R l m O O 3 E i m I R c N o O x l c z v / 1 E l W a R v D O T m P o C D y U L G c H G S p 1 q T z O B r q v 9 c s W t u R n Q M v F y U o E c z X 7 5 q z e I S C K o N I R j r b u e G x s / x c o w w u m 0 1 E s 0 j T E Z 4 y H t W i q x o N p P s 3 2 n 6 M Q q A x R G y j 5 p U K b + 7 k i x 0 H o i A l s p s B n p R W 8 m / u d 1 E x N e + C m T c W K o J P N B Y c K R i d D s e D R g i h L D J 5 Z g o p j d F Z E R V p g Y G 1 H J h u A t n r x M H u o 1 7 6 x m v / p t v d K 4 z B M p w h E c w y l 4 c A 4 N u I E m t I A A h 2 d 4 h T f n 0 X l x 3 p 2 P e W n B y X s O 4 Q + c z x + H x 4 + 9 &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " j i d H U 9 L w m s h b l P N 5 + L N r r P b 6 B d U = " &gt; A A A B 9 H i c b V B N T w I x E J 3 F L 8 Q v 1 K O X R j A x H s g u H P R I 9 K B H T E R Q 2 J B u 6 U J D 2 9 2 0 X R O y 4 V 9 4 8 a A X 4 9 U f 4 8 1 / Y 4 E 9 K P i S N i / v z W R m X h B z p o 3 r f j u 5 l d W 1 9 Y 3 8 Z m F r e 2 d 3 r 7 h / c K + j R B H a J B G P V D v A m n I m a d M w w 2 k 7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " W D J S a g m D k 0 A N 3 s 0 d n b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " m O I Z Z x f z X W p 2 7 x O X p X s F + N T P x z 0 = " &gt; A A A B 9 n i c b V B N T 8 J A E N 3 i F + I X 6 t F L I 5 h 4 I i 0 e 9 E j 0 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>7 z b S 0 t r 6 y u r e c 2 8 p t b 2 z u 7 h b 3 9 O x 0 l i r I 6 j U S k m j 7 R T H D J 6 s h R s G a s G A l 9 w R r + 8 G r i N x 6 Y 0 j y S t z i K m R e S v u Q B p w S N d F / q I H t C T d P W u H t a 6 h a K T t m Z w l 4 k b k a</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>r H e r Y / Z 6 J K V 7 R z A H 1 i f P 7 q H k r 4 = &lt; / l a t e x i t &gt; Y ⇤ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "I d L a B Q D F T R C 0 q f 7 t a 1 T x a 1 L N W p U = " &gt; A A A B + H i c b V A 9 T w J B E J 3 z E / E L t b S 5 C C b G g t x h o S X R x h I T E Q i c Z G / Zg w 1 7 e 5 f d O Z V c + B 8 2 F t o Y W 3 + K n f / G B a 5 Q 8 C U z e X l v J j v 7 / F h w j Y 7 z b S 0 t r 6 y u r e c 2 8 p t b 2 z u 7 h b 3 9 O x 0 l i r I 6 j U S k m j 7 R T H D J 6 s h R s G a s G A l 9 w R r + 8 G r i N x 6 Y 0 j y S t z i K m R e S v u Q B p w S N d F / q I H t C T d P W u H t a 6 h a K T t m Z w l 4 k b k a</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Graph meta-learning problems. A. Meta-learner classifies unseen label set by observing other label sets in the same graph. B. Meta-learner learns unseen graph by learning from other graphs with the same label set. C. Meta-learner classifies unseen label set by learning from other label sets across multiple graphs. Unlike existing methods, G-META solves all three problems and also works for link prediction (see Section 6.1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " 2 o Y n + c 4 W Q O 5 p 0 g i I c B u l E 7 T I M 3 w = " &gt; A A A B 8 n i c b V B N T w I x E O 3 i F + I X 6 t F L I 5 h 4 I r t 4 0 C P R i 0 d M R D C w I d 1 u F x q 6 7 d r O k p A N f 8 K L B 7 0 Y r / 4 a b / 4 b C + x B w Z e 0 e X l v J j P z g k R w A 6 7 7 7 R T W 1 j c 2 t 4 r b p Z 3 d v f 2 D 8 u H R g 1 G p p q x F l V C 6 E x D D B J e s B R w E 6 y S a k T g Q r B 2 M b m Z + e 8 y 0 4 U r e w y R h f k w G k k e c E r B S p 9 o b h w p</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>8 m j 3 / D O G v b z b / 1 a 8 7 J I p A x H c A y n 4 M E 5 N O E G W h A A A Q b P 8 A p v j n B e n H f n Y 1 5 a c o q e Q / g D 5 / M H O + q O a w = = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " a 0 m 1 w 3 r G 2 B Q W F o Q a N X e l H o b Y l 7 4 = " &gt; A A A B 8 n i c b V A 9 T w J B E J 3 z E / E L t b S 5 C C Z W 5 A 4 L L Y k 2 l p i I Y O B C 9 p Y 9 2 L C 3 e + 7 O m Z A L f 8 L G Q h t j 6 6 + x 8 9 + 4 w B U K v m Q 3 L + / N Z G Z e m A h u 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>r 9 q v d l s r 1 6 / y R A p w D C d w B j 5 c Q B 1 u o A F N o C D g G V 7 h z X l 0 X p x 3 5 2 N e u u L k P U f w B 8 7 n D y 7 H k C o = &lt; / l a t e x i t &gt; • • • &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a 0 m 1 w 3 r G 2 B Q W F o Q a N X e l H o b Y l 7 4 = " &gt; A A A B 8 n i c b V A 9 T w J B E J 3 z E / E L t b S 5 C C Z W 5 A 4 L L Y k 2 l p i I Y O B C 9 p Y 9 2 L C 3 e + 7 O m Z A L f 8 L G Q h t j 6 6 + x 8 9 + 4 w B U K v m Q 3 L + / N Z G Z e m A h u 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>r b J 9 G d q b 8 7 M h I b M 4 5 D W x k T H J p F b y r + 5 3 V S j C 6 D j M s k R S b p f F C U C h e V O z 3 e 7 X P N K I q x J Y R q b n d 1 6 Z B o Q t F G V L Q h + I s n L 5 O H W t U / r 9 q v d l s r 1 6 / y R A p w D C d w B j 5 c Q B 1 u o A F N o C D g G V 7 h z X l 0 X p x 3 5 2 N e u u L k P U f w B 8 7 n D y 7 H k C o = &lt; / l a t e x i t &gt; Meta-Testing Subgraphs • • • &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a 0 m 1 w 3 r G 2 B Q W F o Q a N X e l H o b Y l 7 4 = " &gt; A A A B 8 n i c b V A 9 T w J B E J 3 z E / E L t b S 5 C C Z W 5 A 4 L L Y k 2 l p i I Y O B C 9 p Y 9 2 L C 3 e + 7 O m Z A L f 8 L G Q h t j 6 6 + x 8 9 + 4 w B</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>x 3 5 2 N e u u L k P U f w B 8 7 n D y 7 H k C o = &lt; / l a t e x i t &gt; GNN Support Prototypes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>r 6 a t 8 1 a 6 6 p o p A z H c A J n 4 M A F t O A G 2 t A F D I / w D K / w Z j w Z L 8 a 7 8 b E Y L R n F z h H 8 g f H 5 A + Y x l H 4 = &lt; / l a t e x i t &gt; Support Centroids • • • &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a 0 m 1 w 3 r G 2 B Q W F o Q a N X e l H o b Y l 7 4 = " &gt; A A A B 8 n i c b V A 9 T w J B E J 3 z E / E L t b S 5 C C Z W 5 A 4 L L Y k 2 l p i I Y O B C 9 p Y 9 2 L C 3 e + 7 O m Z A L f 8 L G Q h t j 6 6 + x 8 9 + 4 w B U K v m Q 3 L + / N Z G Z e m A h u 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>r b J 9 G d q b 8 7 M h I b M 4 5 D W x k T H J p F b y r + 5 3 V S j C 6 D j M s k R S b p f F C U C h e V O z 3 e 7 X P N K I q x J Y R q b n d 1 6 Z B o Q t F G V L Q h + I s n L 5 O H W t U / r 9 q v d l s r 1 6 / y R A p w D C d w B j 5 c Q B 1 u o A F N o C D g G V 7 h z X l 0 X p x 3 5 2 N e u u L k P U f w B 8 7 n D y 7 H k C o = &lt; / l a t e x i t &gt; Query Centroids • • • &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a 0 m 1 w3 r G 2 B Q W F o Q a N X e l H o b Y l 7 4 = " &gt; A A A B 8 n i c b V A 9 T w J B E J 3 z E / E L t b S 5 C C Z W 5 A 4 L L Y k 2 l p i I Y O B C 9 p Y 9 2 L C 3 e + 7 O m Z A L f 8 L G Q h t j 6 6 + x 8 9 + 4 w B U K v m Q 3 L + / N Z G Z e m A h u 0 P O + n Z X V t f W N z c J W c X t n d 2 + / d H B 4 b 1 S q K W t S J Z R u h 8 Q w w S V r I k f B 2 o l m J A 4 F a 4 W j 6 6 n f e m L a c C X v c J y w I C Y D y S N O C V q p X e n S v k J T 6 Z X K X t W b w V 0 m f k 7 K k K P R K 3 1 1 + 4 q m M Z N I B T G m 4 3 s J B h n R y K l g k 2 I 3 N S w h d E Q G r G O p J D E z Q T b b d + K e W q X v R k r b J 9 G d q b 8 7 M h I b M 4 5 D W x k T H J p F b y r + 5 3 V S j C 6 D j M s k R S b p f F C U C h e V O z 3 e 7 X P N K I q x J Y R q b n d 1 6 Z B o Q t F G V L Q h + I s n L 5 O H W t U /r 9 q v d l s r 1 6 / y R A p w D C d w B j 5 c Q B 1 u o A F N o C D g G V 7 h z X l 0 X p x 3 5 2 N e u u L k P U f w B 8 7 n D y 7 H k C o = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " a 0 m 1 w 3 r G 2 B Q W F o Q a N X e l H o b Y l 7 4 = " &gt; A A A B 8 n i c b V A 9 T w J B E J 3 z E / E L t b S 5 C C Z W 5 A 4 L L Y k 2 l p i I Y O B C 9 p Y 9 2 L C 3 e + 7 O m Z A L f 8 L G Q h t j 6 6 + x 8 9 + 4 w B U K v m Q 3 L + / N Z G Z e m A h u 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>r b J 9 G d q b 8 7 M h I b M 4 5 D W x k T H J p F b y r + 5 3 V S j C 6 D j M s k R S b p f F C U C h e V O z 3 e 7 X P NK I q x J Y R q b n d 1 6 Z B o Q t F G V L Q h + I s n L 5 O H W t U / r 9 q v d l s r 1 6 / y R A p w D C d w B j 5 c Q B 1 u o A F N o C D g G V 7 h z X l 0 X px 3 5 2 N e u u L k P U f w B 8 7 n D y 7 H k C o = &lt; / l a t e x i t &gt; Query Centroids • • • &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a 0 m 1 w 3 r G 2 B Q W F o Q a N X e l H o b Y l 7 4 = " &gt; A A A B 8 n i c b V A 9 T w J B E J 3 z E / E L t b S 5 C C Z W 5 A 4 L L Y k 2 l p i I Y O B C 9 p Y 9 2 L C 3 e + 7 O m Z A L f 8 L G Q h t j 6 6 + x 8 9 + 4 w B U K v m Q 3 L + / N Z G Z e m A h u 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>r b J 9 G d q b 8 7 M h I b M 4 5 D W x k T H J p F b y r + 5 3 V S j C 6 D j M s k R S b p f F C U C h e V O z 3 e 7 X P NK I q x J Y R q b n d 1 6 Z B o Q t F G V L Q h + I s n L 5 O H W t U / r 9 q v d l s r 1 6 / y R A p w D C d w B j 5 c Q B 1 u o A F N o C D g G V 7 h z X l 0 X p x 3 5 2 N e u u L k P U f w B 8 7 n D y 7 H k C o = &lt; / l a t e x i t &gt; Inner Loop Update ✓ ⇤ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o M j W B 6 s 0 u F X J z P r m k E 1 N T G O W p x A = " &gt; A A A B 9 H i c b V A 9 T w J B E J 3 z E / E L t b T Z C C b G g t x h o S X R x h I T E R R O sr f s w Y a 9 v c v u n A m 5 8 C 9 s L L Q x t v 4 Y O / + N y 0 e h 4 E t 2 8 / L e T G b m B Y k U B l 3 3 2 1 l a X l l d W 8 9 t 5 D e 3 t n d 2 C 3 v 7 d y Z O N e N 1 F s t Y N w N q u B S K 1 1 G g 5 M 1 E c x o F k j e C w d X Y b z x x b U S s b n G Y c D + i P S V C w S h a 6 a H U x j 5 H + n h a 6 h S K b t m d g C w S b 0 a K M E O t U / h q d 2 O W R l w h k 9 S Y l u c m 6 G d U o 2 C S j / L t 1 P C E s g H t 8 Z a l i k b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>4 h l d 4 c 4 z z 4 r w 7 H 9 P S J W f W c w B / 4 H z + A E Q Q k L 8 = &lt; / l a t e x i t &gt; 11 Lquery &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P / 6 N t J g b r 7v 3 k O 9 g 7 W w R 0 A q W f H s = " &gt; A A A C B n i c b V A 9 T w J B E N 3 D L 8 Q v 1 E 6 b i 2 B i R e 6 w 0 J J o Y 2 G B i Q i G u 5 C 9 Z Y A N e x / u z h n J 5 R I b / 4 q N h T b G 1 t 9 g 5 7 9 x D y g U f M l u X t 6 b y c w 8 L x J c o W V 9 G 7 m F x a X l l f x q Y W 1 9 Y 3 O r u L 1 z o 8 J Y M m i w U I S y 5 V E F g g f Q Q I 4 C W p E E 6 n s C m t 7 w P P O b 9 y A V D 4 N r H E X g + r Q f 8 B 5 n F L X U K e 6 V H Z / i g F G R X K Y d B + E B k 7 s Y 5 C g t d 4 o l q 2 K N Y c 4 T e 0 p K Z I p 6 p / j l d E M W + x A g E 1 S p t m 1 F 6 C Z U I m c C 0 o I T K 4 g o G 9 I + t D U N q A / K T c Y 3 p O a h V r p m L 5 T 6 B W i O 1 d 8 d C f W V G v m er s w W V r N e J v 7 n t W P s n b o J D 6 I Y I W C T Q b 1 Y m B i a W S B m l 0 t g K E a a U C a 5 3 t V k A y o p Q x 1 b Q Y d g z 5 4 8 T 2 6 r F f u 4 o r / q V b V U O 5 s m k i f 7 5 I A c E Z u c k B q 5 I H X S I I w 8 k m f y S t 6 M J + P F e D c + J q U 5 Y 9 q z S / 7 A + P w B d U G Y v g = = &lt; / l a t e x i t &gt; L i query &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " s V e S p a g V P 4 T 1 Q/ i Q P k P 2 F E D L W G g = " &gt; A A A C C H i c b V C 7 T s N A E D z z D O F l o I T C I k G i i u x Q Q B l B Q 0 E R J E K C E m O d L5 v k l P O D u z U i s t z Q 8 C s 0 F N A g W j 6 B j r / h 8 i g g Y a Q 7 j W Z 2 t b v j x 4 I r t O 1 v Y 2 5 + Y X F p O b e S X 1 1 b 3 9 g 0 t 7 a v V Z R I B j U W i U g 2 f K p A 8 B B q y F F A I 5 Z A A 1 9 A 3 e + f D f 3 6 P U j F o / A K B z G 4 A e 2 G v M M Z R S 1 5 5 l 6 x F V D s M S r S i 8 x r I T x g e p e A H G S 3 v O i Z B b t k j 2 D N E m d C C m S C q m d + t d o R S w I</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>r q y m v a H 4 n 9 d M s H P i p j y M E 4 S Q j Q d 1 E m F h Z A 0 j s d p c A k M x 0 I Q y y f W u F u t R S R n q 4 P I 6 B G f 6 5 F l y U y 4 5 R y X 9 l S / L h c r p J J E c 2 S X 7 5 J A 4 5 J h U y D m p k h p h 5 J E 8 k 1 f y Z j w Z L 8 a 7 8 T E u n T M m P T v k D 4 z P H w V / m Z k = &lt; / l a t e x i t &gt; Lsupport &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a m G z o B W</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>b b 9 b S w s L i 2 v r J b W y u s b m 1 v b 5 s 7 u r Y p T S W i L x D y W n Q A r y p m g L W D A a S e R F E c B p + 1 g e J H 7 7 X s qF Y v F D Y w S 6 k W 4 L 1 j I C A Y t + e Z B 1 Y 0 w D A j m 2 d X Y d 4 E + Q K b S J I k l j K u + W b F r 9 g T W P H E K U k E F m r 7 5 5 f Z i k k Z U A O F Y q a 5 j J + B l W A I j n I 7 L b q p o g s k Q 9 2 l X U 4 E j q r x s c s X Y O t J K z w p j q Z 8 A a 6 L + 7 s h w p N Q o C n R l v r K a 9 X L x P 6 + b Q n j m Z U w k K V B B p o P C l F s Q W 3 k k V o 9 J S o C P N M F E M r 2 r R Q Z Y Y g I 6 u L I O w Z k 9 e Z 7 c 1 W v O S U1 / 9 e t 6 p X F e J F J C + + g Q H S M H n a I G u k R N 1 E I E P a J n 9 I r e j C f j x X g 3 P q a l C 0 b R s 4 f + w P j 8 A T a e m b k = &lt; / l a t e x i t &gt; Lsupport &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a m G z o B W</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>b b 9 b S w s L i 2 v r J b W y u s b m 1 v b 5 s 7 u r Y p T S W i L x D y W n Q A r y p m g L W D A a S e R F E c B p + 1 g e J H 7 7 X s q F Y v F D Y w S 6 k W 4 L 1 j I C A Y t + e Z B 1 Y 0 w D A j m 2 d X Y d 4 E + Q K b S J I k l j K u + W b F r 9 g TW P H E K U k E F m r 7 5 5 f Z i k k Z U A O F Y q a 5 j J + B l W A I j n I 7 L b q p o g s k Q 9 2 l X U 4 E j q r x s c s X Y O t J K z w p j q Z 8 A a 6 L + 7 s h w p N Q o C n R l v r K a 9 X L x P 6 + b Q n j m Z U w k K V B B p o P C l F s Q W 3 k k V o 9 J S o C P N M F E M r 2 r R Q Z Y Y g I 6 u L I O w Z k 9 e Z 7 c 1 W v O S U1 / 9 e t 6 p X F e J F J C + + g Q H S M H n a I G u k R N 1 E I E P a J n 9 I r e j C f j x X g 3 P q a l C 0 b R s 4 f + w P j 8 A T a e m b k = &lt; / l a t e x i t &gt;T query i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x 6 6 R q E O / 2 U 6 U r J c d k P 6 A U 6 h X i E g = " &gt; A A A C C H i c b V C 7 T s N A E D z z D O F l o I T C I k G i i u x Q Q B l B Q x m k h A Q l J j p f 1 s k p 5 w d 3 a 0 R k u a H h V 2 g o o E G 0 f A I d f 8 P l U U D C SH c a z e x q d 8 e L B V d o 2 9 / G w u L S 8 s p q b i 2 / v r G 5 t W 3 u 7 F 6 r K J E M 6 i w S k W x 6 V I H g I d S R o 4 B m L I E G n o C G N 7 g Y + Y 1 7 k I p H Y Q 2 H M b g B 7 Y X c 5 4 y i l j r m Q b E d U O w z K t J a d t t G e M D 0 L g E 5 z D q 8 2 D E L d s k e w 5 o n z p Q U y B T V j v n V 7 k Y s C S B E J q h S L c e O 0 U 2 p R M 4 E Z P l 2 o i C m b E B 7 0 N I 0 p A E o N x 1 f k V l H W u l a f i T 1 C 9 E a q 7 8 7 U h o o N Q w 8 X T l a W c 1 6 I / E / r 5 W g f + a m P I w T h J B N B v m J s D C y R p F Y X S 6 B o R h q Q p n k e l e L 9 a m k D H V w e R 2 C M 3 v y P L k p l 5 y T k v 7 K V + V C 5 X y a S I 7 s k 0 N y T B x y S i r k k l R J n T D y S J 7 J K 3 k z n o w X 4 9 3 4 m J Q u G N O e P f I H x u c P E j K Z o Q = = &lt; / l a t e x i t &gt; T support i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " j J + p u y F 7 Z 5 X 4 e k 2 L v N k B 5 A d y u F o = " &gt; A A A C C n i c b V C 7 T s N A E D y H V w g v A y U S s k i Q q C I 7 F F B G 0 F A G K S F B s b H O l 3 N y y v m h u z U i s t z R 8 C s 0 F N A g W r 6 A j r / h n L i A h J H u N J r Z 1 e 6 O F 3 M m w T S / t d L S 8 s r q W n m 9 s r G 5 t b 2 j 7 + 7 d y C g R h H Z I x C P R 8 7 C k n I W 0 A w w 4 7 c W C 4 s D j t O u N L 3 O / e 0 + F Z F H Y h k l M n Q A P Q + Y z g k F J r n 5 Y s w M M I 4 J 5 2 s 7 u b K A P k M o k j i M B m c t q r l 4 1 6 + Y U x i K x C l J F B V q u / m U P I p I E N A T C s Z R 9 y 4 z B S b E A R j j N K n Y i a Y z J G A 9 p X 9 E Q B 1 Q 6 6 f S O z D h W y s D w I 6 F e C M Z U / d 2 R 4 k D K S e C p y n x p O e / l 4 n 9 e P w H / 3 E l Z G C d A Q z I b 5 C f c g M j I Q z E G T F A C f K I I J o K p X Q 0 y w g I T U N F V V A j W / M m L 5 L Z R t 0 7 r 6 m t c N 6 r N i y K R M j p A R + g E W e g M N d E V a q E O I u g R P a N X 9 K Y 9 a S / a u / Y x K y 1 p R c 8 + + g P t 8 w f V Z J q c &lt; / l a t e x i t &gt; T support i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " j J + p u y F 7 Z 5 X 4 e k 2 L v N k B 5 A d y u</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>r Z 1 e 6 O F 3 M m w T S / t d L S 8 s r q W n m 9 s r G 5 t b 2 j 7 + 7 d y C g R h H Z I x C P R 8 7 C k n I W 0 A w w 4 7 c W C 4 s D j t O u N L 3 O / e 0 + F Z F H Y h k l M n Q A P Q + Y z g k F J r n 5 Y s w M M I 4 J 5 2 s 7 u b K A P k M o k j i M B m c t q r l 4 1 6 + Y U x i K x C l J F B V q u / m U P I p I E N A T C s Z R 9 y 4 z B S b E A R j j N K n Y i a Y z J G A 9 p X 9 E Q B 1 Q 6 6 f S O z D h W y s D w I 6 F e C M Z U / d 2 R 4 k D K S e C p y n x p O e / l 4 n 9 e P w H / 3 E l</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (1) We first construct a batch of m meta-training tasks and extract local subgraphs on the fly for nodes in the meta-tasks. For each task Ti, (2) subgraphs from the support set are mini-batched and are fed into a GNN parameterized by θ. (3) The support set embeddings using the centroid nodes are generated, and (4) the prototypes are computed from the support centroid embeddings. Then, (5) the support set loss Lsupport is computed, and (6) back-propagates to update the GNN parameter. (7) T query i subgraphs then feed into the updated GNN to (8) generate query centroid embeddings. (9) Using the support prototypes and the query embeddings, the query loss L i query for task Ti is computed. Steps (2-9) are repeated for η update steps. The same process repeats for the other m sampled tasks, starting from the same GNN f θ .<ref type="bibr" target="#b10">(10)</ref> The last update step's query loss from all the tasks are summed up and used to update θ. Then, another batch of tasks are sampled, and step (1-10) are repeated. Then, for meta-testing tasks, steps (1-9) are repeated with the GNN using the meta-learned parameter θ * , which enables generalization over unseen tasks. See Algorithm 1 (Appendix E).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>≤ 9 ]</head><label>9</label><figDesc>C/(D t * GM ) h * . [Explanations of the proof. (Line 1) Substitution of the term x (∞) u via Equation 1. (Line 2) Since we are calculating the gradient between two feature vectors x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head></head><label></label><figDesc>matrix I and we know for any subordinate norm ( A = sup x =1 { Ax }), we have I = 1. Therefore,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>and are detailed in Appendix C. We need the following definitions. (1) Node influence</figDesc><table><row><cell>Node influence Node influence Node influence Node influence Node influence Node influence Node influence Node influence Node influence Node influence Node influence Node influence Node influence Node influence Node influence Node influence I u,v of v on u in</cell></row><row><cell>the final GNN output is:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>G on u is: I G (u) = [I u,v1 , . . . , I u,vn ] 1 , where [I u,v1 , . . . , I u,vn ] is a vector representing the influence of other nodes on u. (3) Graph influence loss</figDesc><table><row><cell>) Graph influence Graph influence Graph influence Graph influence Graph influence Graph influence Graph influence Graph influence Graph influence Graph influence Graph influence Graph influence Graph influence Graph influence Graph influence Graph influence Graph influence I Graph influence loss Graph influence loss Graph influence loss Graph influence loss Graph influence loss Graph influence loss Graph influence loss Graph influence loss Graph influence loss Graph influence loss Graph influence loss Graph influence loss Graph influence loss Graph influence loss Graph influence loss Graph influence loss R</cell></row></table><note>h is defined as: R h</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 1 :</head><label>1</label><figDesc>Dataset statistics. Fold-PPI and Tree-of-Life are new datasets introduced in this work. by framing nodes into independent local subgraph classification tasks. It allows direct adaptation to MAML since individual subgraphs can be considered as an individual image in the classic few-shot meta-learning setups. More specifically, we first sample a batch of tasks, where each task consists of a set of subgraphs. During meta-training inner loop, we perform the regular stochastic gradient descent on the support loss for each task T</figDesc><table><row><cell>Dataset</cell><cell>Task</cell><cell># Graphs</cell><cell># Nodes</cell><cell># Edges</cell><cell cols="2"># Features # Labels</cell></row><row><cell>Synthetic Cycle</cell><cell>Node</cell><cell>10</cell><cell>11,476</cell><cell>19,687</cell><cell>N/A</cell><cell>17</cell></row><row><cell>Synthetic BA</cell><cell>Node</cell><cell>10</cell><cell>2,000</cell><cell>7,647</cell><cell>N/A</cell><cell>10</cell></row><row><cell>ogbn-arxiv</cell><cell>Node</cell><cell>1</cell><cell>169,343</cell><cell>1,166,243</cell><cell>128</cell><cell>40</cell></row><row><cell>Tissue-PPI</cell><cell>Node</cell><cell>24</cell><cell>51,194</cell><cell>1,350,412</cell><cell>50</cell><cell>10</cell></row><row><cell>FirstMM-DB</cell><cell>Link</cell><cell>41</cell><cell>56,468</cell><cell>126,024</cell><cell>5</cell><cell>2</cell></row><row><cell>Fold-PPI</cell><cell>Node</cell><cell>144</cell><cell>274,606</cell><cell>3,666,563</cell><cell>512</cell><cell>29</cell></row><row><cell>Tree-of-Life</cell><cell>Link</cell><cell>1,840</cell><cell cols="2">1,450,633 8,762,166</cell><cell>N/A</cell><cell>2</cell></row><row><cell>on a graph</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 :</head><label>2</label><figDesc>Graph meta-learning performance on synthetic datasets. Reported is multi-class classification accuracy (five-fold average) for 1-shot node classification in various graph meta-learning problems. N/A means the method does not apply for the problem. Disjoint label problems use a 2-way setup. In the shared label problem, the cycle graph has 17 labels and the BA graph has 10 labels. The results use 5 gradient update steps in meta-training and 10 gradient update steps in meta-testing. Full table with standard deviations is in Appendix J.</figDesc><table><row><cell>Graph Meta-</cell><cell cols="2">Single graph</cell><cell cols="4">Multiple graphs Multiple graphs</cell></row><row><cell cols="3">Learning Problem Disjoint labels</cell><cell cols="2">Shared labels</cell><cell cols="2">Disjoint labels</cell></row><row><cell>Prediction Task</cell><cell cols="2">Node</cell><cell cols="2">Node</cell><cell cols="2">Node</cell></row><row><cell>Base graph</cell><cell>Cycle</cell><cell>BA</cell><cell>Cycle</cell><cell>BA</cell><cell>Cycle</cell><cell>BA</cell></row><row><cell>G-META (Ours)</cell><cell cols="3">0.872 0.867 0.542</cell><cell>0.734</cell><cell>0.767</cell><cell>0.867</cell></row><row><cell>Meta-Graph</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>Meta-GNN</cell><cell cols="2">0.720 0.694</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>FS-GIN</cell><cell cols="2">0.684 0.749</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>FS-SGC</cell><cell cols="2">0.574 0.715</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>KNN</cell><cell cols="3">0.918 0.804 0.343</cell><cell>0.710</cell><cell>0.753</cell><cell>0.769</cell></row><row><cell>No-Finetune</cell><cell cols="3">0.509 0.567 0.059</cell><cell>0.265</cell><cell>0.592</cell><cell>0.577</cell></row><row><cell>Finetune</cell><cell cols="3">0.679 0.671 0.385</cell><cell>0.517</cell><cell>0.599</cell><cell>0.629</cell></row><row><cell>ProtoNet</cell><cell cols="3">0.821 0.858 0.282</cell><cell>0.657</cell><cell>0.749</cell><cell>0.866</cell></row><row><cell>MAML</cell><cell cols="3">0.842 0.848 0.511</cell><cell>0.726</cell><cell>0.653</cell><cell>0.844</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>Graph meta-learning performance on real-world datasets. Reported is multi-class classification accuracy (five-fold average) and standard deviation. N/A means the method does not work in the graph meta-learning problem. Further details on performance are in Appendix I. disjoint label setting, over the best performing baseline. This performance boost suggests that G-META can learn across graphs and disjoint labels. Besides, this increase also shows G-META obtains good predictive performance, using only a few gradient update steps given a few examples on the target tasks.G-META captures local structural information. In the synthetic datasets, we see that Meta-GNN, FS-GIN, and FS-SGC, which base on the entire graph, are inferior to subgraph-based methods, such as G-META. This finding demonstrates that subgraph embedding can capture the local structural roles, whereas using the entire graph cannot. It further supports our theoretical motivation. In the single graph disjoint label setting, KNN achieves the best result. This result suggests that the subgraph representation learned from the trained embedding function is sufficient to capture the structural roles, further confirming the usefulness of local subgraph construction.</figDesc><table><row><cell>Graph Meta-</cell><cell>Single graph</cell><cell cols="4">Multiple graphs Multiple graphs Multiple graphs Multiple graphs</cell></row><row><cell cols="2">Learning Problem Disjoint labels</cell><cell>Shared labels</cell><cell>Disjoint labels</cell><cell>Shared labels</cell><cell>Shared labels</cell></row><row><cell>Prediction Task</cell><cell>Node</cell><cell>Node</cell><cell>Node</cell><cell>Link</cell><cell>Link</cell></row><row><cell>Dataset</cell><cell>ogbn-arxiv</cell><cell>Tissue-PPI</cell><cell>Fold-PPI</cell><cell>FirstMM-DB</cell><cell>Tree-of-Life</cell></row><row><cell>G-META (Ours)</cell><cell>0.451±0.032</cell><cell>0.768±0.029</cell><cell>0.561±0.059</cell><cell>0.784±0.028</cell><cell>0.722±0.032</cell></row><row><cell>Meta-Graph</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>0.719±0.020</cell><cell>0.705±0.004</cell></row><row><cell>Meta-GNN</cell><cell>0.273±0.122</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>FS-GIN</cell><cell>0.336±0.042</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>FS-SGC</cell><cell>0.347±0.005</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>KNN</cell><cell>0.392±0.015</cell><cell>0.619±0.025</cell><cell>0.433±0.034</cell><cell>0.603±0.072</cell><cell>0.649±0.012</cell></row><row><cell>No-Finetune</cell><cell>0.364±0.014</cell><cell>0.516±0.006</cell><cell>0.376±0.017</cell><cell>0.509±0.006</cell><cell>0.505±0.001</cell></row><row><cell>Finetune</cell><cell>0.359±0.010</cell><cell>0.521±0.013</cell><cell>0.370±0.022</cell><cell>0.511±0.007</cell><cell>0.504±0.003</cell></row><row><cell>ProtoNet</cell><cell>0.372±0.017</cell><cell>0.546±0.025</cell><cell>0.382±0.031</cell><cell>0.779±0.020</cell><cell>0.697±0.010</cell></row><row><cell>MAML</cell><cell>0.389±0.021</cell><cell>0.745±0.051</cell><cell>0.482±0.062</cell><cell>0.758±0.025</cell><cell>0.719±0.012</cell></row><row><cell>the multiple graph</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">Related Work(1) Few-shot meta-learning Few-shot meta-learning Few-shot meta-learning Few-shot meta-learning Few-shot meta-learning Few-shot meta-learning Few-shot meta-learning Few-shot meta-learning Few-shot meta-learning Few-shot meta-learning Few-shot meta-learning Few-shot meta-learning Few-shot meta-learning Few-shot meta-learning Few-shot meta-learning Few-shot meta-learning Few-shot meta-learning. Few-shot meta-learning learns from prior experiences and transfers</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">We will share the G-META implementation as well as all datasets and baseline algorithms upon acceptance.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>more problems. (3) Subgraphs and GNNs</head><p>Subgraphs and GNNs Subgraphs and GNNs Subgraphs and GNNs Subgraphs and GNNs Subgraphs and GNNs Subgraphs and GNNs Subgraphs and GNNs Subgraphs and GNNs Subgraphs and GNNs Subgraphs and GNNs Subgraphs and GNNs Subgraphs and GNNs Subgraphs and GNNs Subgraphs and GNNs Subgraphs and GNNs Subgraphs and GNNs. Subgraph structure is key information for graph-related tasks <ref type="bibr" target="#b37">[36,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b47">46,</ref><ref type="bibr" target="#b33">32]</ref>. For example, Patchy-San <ref type="bibr" target="#b23">[23]</ref> uses local receptive fields to extract useful features from graphs. Ego-CNN uses ego graphs to find critical structures <ref type="bibr" target="#b36">[35]</ref>. SEAL <ref type="bibr" target="#b50">[49]</ref> develops a theory showing that local enclosing subgraphs capture several graph heuristics, which we extend to GNNs here. Cluster-GCN <ref type="bibr" target="#b6">[6]</ref> and GraphSAINT <ref type="bibr" target="#b48">[47]</ref> show that subgraphs improve GNN scalability. In contrast, G-META is the first to use subgraphs for meta-learning.</p><p>Attractive properties of G-META. (1) Scalability: Scalability: Scalability: Scalability: Scalability: Scalability: Scalability: Scalability: Scalability: Scalability: Scalability: Scalability: Scalability: Scalability: Scalability: Scalability: Scalability: G-META operates on a mini-batch of local subgraphs where the subgraph size and the batch size (few-shots) are both small. This allows fast computation and low memory requirement because G-META's aggregation field is smaller than that of previous methods that operate on entire graphs <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b51">50]</ref>. We further increase scalability by sub-sampling subgraphs with more than 1,000 nodes. ( <ref type="formula">2</ref>) Inductive learning: Inductive learning: Inductive learning: Inductive learning: Inductive learning: Inductive learning: Inductive learning: Inductive learning: Inductive learning: Inductive learning: Inductive learning: Inductive learning: Inductive learning: Inductive learning: Inductive learning: Inductive learning:</p><p>Inductive learning: Since G-META operates on different subgraphs in each GNN encoding, it forces inductiveness over unseen subgraphs. This inductiveness is crucial for few-shot learning, where the trained model needs to adapt to unseen nodes. Inductive learning also allows for knowledge transfer from meta-training subgraphs to meta-testing</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b2">(2)</ref> <p>Meta-learning for graphs Meta-learning for graphs Meta-learning for graphs Meta-learning for graphs Meta-learning for graphs Meta-learning for graphs Meta-learning for graphs Meta-learning for graphs Meta-learning for graphs Meta-learning for graphs Meta-learning for graphs Meta-learning for graphs Meta-learning for graphs Meta-learning for graphs Meta-learning for graphs Meta-learning for graphs Meta-learning for graphs. Recent studies integrate graphs into meta-learning.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>subgraphs. (3) Few-shot learning: Few-shot learning: Few-shot learning: Few-shot learning: Few-shot learning: Few-shot learning: Few-shot learning: Few-shot learning: Few-shot learning: Few-shot learning: Few-shot learning: Few-shot learning: Few-shot learning: Few-shot learning: Few-shot learning: Few-shot learning: Few-shot learning: G-META needs only a tiny number of labeled nodes for successful learning, as demonstrated in experiments. This property is in contrast with prevailing GNNs, which require a large fraction of labeled nodes to propagate neural messages in the graph successfully. <ref type="bibr" target="#b4">(4)</ref>  Broad applicability: G-META applies to many graph meta-learning problems (Figure <ref type="figure">1</ref>) whereas previous methods apply to at most one <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b51">50]</ref>. Unlike earlier methods, G-META works for node classification and also few-shot link prediction (i.e., via local subgraphs for a pair of nodes).</p><p>6 Experiments Synthetic datasets. We have two synthetic datasets whose labels depend on nodes' structural roles <ref type="bibr" target="#b13">[13]</ref>, which we use to confirm that G-META captures local graph structure (Table <ref type="table">1</ref> we use a cycle basis graph and attach a distribution of shapes: House, Star, Diamond, Fan <ref type="bibr" target="#b7">[7]</ref>. The label of each node is the structural role in different shapes. We also add x random edges as noise. In the multiple graphs problem, each graph is with varying distribution of number of shapes. ( <ref type="formula">2</ref>  that we constructed based on 1,840 protein interaction networks, each originating from a different species <ref type="bibr" target="#b54">[53]</ref>. Node features are not provided, we use node degrees instead. Details are in Appendix F. Node classification. For disjoint label setups, we sample 5 labels for metatesting, 5 for meta-validation, and use the rest for meta-training. For multiple graph setups, 10% (10%) of all graphs are held out for testing (validation). The remaining graphs are used for training. In each task, 2-ways 1-shot learning is used for synthetic datasets and 3-ways 3-shots learning for real-world datasets. (2) Link prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random Edges</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>House</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Star</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Planted Edges</head><p>Link prediction. Link prediction. Link prediction. Link prediction. Link prediction. Link prediction. Link prediction. Link prediction. Link prediction. Link prediction. Link prediction. Link prediction. Link prediction. Link prediction. Link prediction. Link prediction. 10% of graphs are held out for testing and another 10% for validation. For each graph, the support set consists of 30% edges and the query set 70%. Negative edges are sampled randomly to match the same number of positive edges, and we follow the experiment setting from <ref type="bibr" target="#b50">[49]</ref> to construct the training graph. We use 16-shots for each task, i.e., using only 32 node pairs to predict links for an unseen graph. Hyperparameters are in Appendix G.  " Meta-GNN <ref type="bibr" target="#b51">[50]</ref> " Meta-Graph <ref type="bibr" target="#b2">[2]</ref> " G-META (Ours) " " " "</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Further Details on Notation</head><p>Appendix C Assumptions and Proof of Theorem 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Assumptions</head><p>We use a popular GCN model <ref type="bibr" target="#b17">[17]</ref> as the exemplar GNN model. The l-th layer GCN propagation rule is defined as:</p><p>, where H (l) , W (l) are node embedding and parameter weight matrices at layer l, respectively, and Â = D −1 A is the normalized adjacency matrix. Following <ref type="bibr" target="#b40">[39]</ref>, throughout these derivations, we assume that σ is an identity function and that W an identity matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Definitions and Notation</head><p>Definition (Node Influence): Node influence I u,v of v on u in the final GNN output is:</p><p>, where the norm is any subordinate norm and the Jacobian measures how a change in v translates to a change in u <ref type="bibr" target="#b40">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D Theorem 2 and its Proof</head><p>Theorem 2 (Local Subgraph Preservation Property). Let S u be a local subgraph for node u with neighborhood size h. Let node v be defined as: v = argmax w ({I u,w |w ∈ V \ V u }). Let t be a path between u and v and let D t GM be a geometric mean of node degrees occurring on path t. Let D t * GM = mint{D t GM }. The following holds: R h (u) ≤ C/(D t * GM ) h+1 .</p><p>Proof.</p><p>Explanation of the proof. (Line 1) Definition of graph influence loss. (Line 2) Substitution of definition. (Line 3) Subtract same node influence term from the subgraph and the entire network. (Line 4) Use Theorem 1. (Line 5) They are smaller than (n − m) times the maximum term, which is the node that has the highest node influence in the node set that is outside of the immediate neighborhood of u. (Line 6) We know nodes in the outside of the immediate neighborhood of u is more than h hops away from the node u, hence the path length h t * between the maximum influence node to the node u is larger than h. (Line 7) Move the constant term and we have the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix E Algorithm Overview</head><p>We provide the pseudo-code for G-META in Algorithm 1.</p><p>Algorithm 1: G-META Algorithm. Steps in the algorithm correspond to steps in Figure <ref type="figure">2</ref>.  We proceed by describing the construction and processing of synthetic as well as real-world datasets.</p><p>Upon publication, we will share G-META implementation as well as all datasets and baseline algorithms with the community through a public project website, which will be accompanied by a Github repository with relevant data loaders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Synthetic Datasets</head><p>We have two synthetic datasets where labels are depended on local structural roles. They are to show G-META's ability to capture local network structures. For the first synthetic dataset, we use the graphs with planted structural equivalence from GraphWave <ref type="bibr" target="#b7">[7]</ref>. We use a cycle basis network and attach a distribution of shapes: House, Star, Diamond, Fan on the cycle basis. The label of each node is the structural role in different shapes. Hence, the label reflects the local structural information. We also add n random edges to add noise. For the single graph and disjoint label setting, we use 500 nodes for the cycle basis, and add 100 shapes for each type with 1,000 random edges. In the multiple graph setting, we sample 10 graphs with varying distribution of number of shapes for each graph. Each graph uses 50 nodes for the cycle basis, and add randomly generated [1-15] shapes for each type with 100 random edges. There are 17 labels. To model local structural information under a more realistic homophily network, we first construct a Barabási-Albert (BA) network with 200 nodes and 3 nodes are preferential attached based on the degrees. Then, we plant shapes to the BA network by first sampling nodes and adding edges corresponding to the shapes. Then, to generate label, we compute the Graphlet Distribution Vector <ref type="bibr" target="#b24">[24]</ref> for each node, which characterizes the local network structures and then we apply spectral clustering on this vector to generate the labels. There are 10 labels in total. For multiple graph setting, the same varying distribution of numbers of shapes as in the cycle dataset are used to plant each BA network. See Figure <ref type="figure">3</ref> for a visual illustration of synthetic datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Real-world Datasets and Novel Graph Meta-Learning Datasets</head><p>We use three real world datasets for node classification and two real world datasets for link prediction to evaluate G-META. (1) arXiv is a citation network from the entire Computer Science arXiv papers, where features are title and abstract word embeddings, and labels are the subject areas <ref type="bibr" target="#b14">[14]</ref>. (2) Tissue-PPI is 24 protein-protein interaction networks from different tissues, where features are gene signatures and labels are gene ontology functions <ref type="bibr" target="#b55">[54,</ref><ref type="bibr" target="#b12">12]</ref>. Each label is a binary protein function classification task. We select the top 10 balanced tasks. (3) Fold-PPI is a novel dataset, which we constructed for the multiple graph and disjoint label setting. It has 144 tissue networks <ref type="bibr" target="#b55">[54]</ref>, and the labels are classified using protein structures defined in SCOP database <ref type="bibr">[1]</ref>. We screen fold groups that have more than 9 unique proteins across the networks. It results in 29 unique labels. The features are conjoint triad protein descriptor <ref type="bibr" target="#b29">[28]</ref>. In Fold-PPI, the majority of the nodes do not have associated labels. Note that G-META operates on label scarce settings. (4) For link prediction, the first dataset FirstMM-DB <ref type="bibr" target="#b21">[21]</ref> is the standard 3D point cloud data, which consists of 41 graphs. <ref type="bibr" target="#b5">(5)</ref> The second link prediction dataset is the Tree-of-Life dataset. This is a new dataset, which we constructed based on 1,840 protein interaction networks (PPIs), each originating from a different species <ref type="bibr" target="#b54">[53]</ref>. Node features are not provided, we use node degrees instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix G Further Details on Hyperparameter Selection</head><p>We use random hyperparameter search over the following set of hyperparameters. For task numbers in each batch, we use 4, 8, 16, 32, 64; for inner update learning rate, 1×10 −2 , 5×10 −3 , 1×10 −3 , 5×10 −4 ; for outer update learning rate, 1×10 −2 , 5×10 −3 , 1×10 −3 , 5×10 −4 ; for hidden dimension, we select from 64, 128, and 256.</p><p>For arxiv-ogbn dataset, we set task numbers to 32, inner update learning rate to 1×10 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix I Further Details on Performance Evaluation</head><p>All of our experiments are done on an Intel Xeon CPU 2.50GHz and using an NVIDIA K80 GPU.</p><p>For synthetic datasets disjoint label problems, we use a 2-way setup. In the shared label problem, the cycle graph has 17 labels and the BA graph has 10 labels. The results use 5 gradient update steps in meta-training and 10 gradient update steps in meta-testing. For real-world datasets node classification uses 3-shots and link prediction uses 16-shots. For disjoint labels problems, we set it to 3-way classification task. The results use 10 gradient update steps in meta-training and 10 gradient update steps in meta-testing. For Tissue-PPI, we have ten 2-way protein function tasks average where each task is performed three times.</p><p>For link prediction problem, to ensure the support and query set are distinct in all meta-training tasks, we separate a fixed 30% of edges for support set and 70% of edges for query set as a preprocessing step for every graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix J Further Results on Synthetic Datasets</head><p>Full version is reported in Table <ref type="table">6</ref>. The large standard deviation is because we sample only two-labels for meta-testing in one data fold, due to the limit number of labels in synthetic datasets. If the shape structural roles corresponding to the label set in the meta-testing set is structurally distinct from all the meta-training label sets, the performance would be bad since there is no transferability for meta-learner to learn. In our data splits, we find there is one fold that performs bad across all methods, thus the reason for the large standard deviation. For real-world datasets, as the label set is large, we sample more labels for meta-testing (e.g., five labels for 3-way classification where each meta-testing task is of 5 3 label sets) such that the result is averaged across different label sets. This makes the standard deviation smaller. In both settings, the mean accuracy reflects the predictive performance.</p><p>Table <ref type="table">6</ref>: Further results on graph meta-learning performance for synthetic datasets. Five-fold average multi-class classification accuracy on synthetic datasets for 1-shot node classification in various graph metalearning problem. N/A means the method does not work in this task setting. The disjoint label setting uses 2-way setup and in the shared labels settings, cycle basis has 17 labels and BA has 10 labels. The results use 5-gradient update steps in meta-training and 10-gradient update steps in meta-testing. See Figure <ref type="figure">3</ref>  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">For multiple graphs problem, a varying distribution of numbers of shapes are used to plant each BA graph. Details are in Appendix F. Real-world datasets and new meta-learning datasets. We use three real world datasets for node classification and two for link prediction to evaluate G-META (Table 1). (1) ogbn-arxiv ogbn-arxiv ogbn-arxiv ogbn-arxiv ogbn-arxiv ogbn-arxiv ogbn-arxiv ogbn-arxiv ogbn-arxiv ogbn-arxiv ogbn-arxiv ogbn-arxiv ogbn-arxiv ogbn-arxiv ogbn-arxiv ogbn-arxiv ogbn-arxiv is a CS citation network, where features are titles, and labels are the subject areas</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Ba ; Meta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Gnn</forename><surname>Meta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Gnn</forename><surname>Meta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Gnn</forename><surname>Meta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Gnn</forename><surname>Meta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Gnn</forename><surname>Meta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Gnn</forename><surname>Meta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Gnn</forename><surname>Meta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Gnn</forename><surname>Meta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Gnn</forename><surname>Meta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Gnn</forename><surname>Meta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Gnn</forename><surname>Meta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Gnn</forename><surname>Meta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Gnn</forename><surname>Meta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Gnn Meta-Gnn</forename><surname>Meta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">) Fold-PPI Fold-PPI Fold-PPI Fold-PPI Fold-PPI Fold-PPI Fold-PPI Fold-PPI Fold-PPI Fold-PPI Fold-PPI Fold-PPI Fold-PPI Fold-PPI Fold-PPI Fold-PPI Fold-PPI is a novel dataset, which we constructed for the multiple graph and disjoint label problem</title>
		<title level="s">Tree-of-Life Tree-of-Life Tree-of-Life Tree-of-Life Tree-of-Life Tree-of-Life Tree-of-Life Tree-of-Life Tree-of-Life Tree-of-Life Tree-of-Life Tree-of-Life Tree-of-Life Tree-of-Life Tree-of-Life Tree-of-Life Tree-of-Life is a new dataset Nine baseline methods. Meta</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">FirstMM-DB FirstMM-DB FirstMM-DB FirstMM-DB FirstMM-DB FirstMM-DB FirstMM-DB FirstMM-DB FirstMM-DB FirstMM-DB FirstMM-DB FirstMM-DB FirstMM-DB FirstMM-DB FirstMM-DB FirstMM-DB FirstMM-DB [</note>
	<note>-Graph Meta-Graph Meta-Graph Meta-Graph Meta-Graph Meta-Graph Meta-Graph Meta-Graph Meta-Graph Meta-Graph Meta-Graph Meta-Graph Meta-Graph Meta-Graph Meta-Graph Meta-Graph Meta-Graph [2] injects graph signature in VGAE [16] to do few-shot multigraph link prediction. GNN Meta-GNN [50] applies MAML [9] to Simple Graph Convolution (SGC) [42</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Few-shot Graph Isomorphism Network (FS-GIN) Few-shot Graph Isomorphism Network (FS-GIN) Few-shot Graph Isomorphism Network (FS-GIN) Few-shot Graph Isomorphism Network (FS-GIN) Few-shot Graph Isomorphism Network (FS-GIN) Few-shot Graph Isomorphism Network (FS-GIN) Few-shot Graph Isomorphism Network (FS-GIN) [44] applies GIN on the entire graph and retrieve the few-shot nodes to propagate loss and enable learning. Similarly, Few-shot SGC (FS-SGC) Few-shot SGC (FS-SGC) Few-shot SGC (FS-SGC) Few-shot SGC (FS-SGC) Few-shot SGC (FS-SGC) Few-shot SGC (FS-SGC) Few-shot SGC (FS-SGC) Few-shot SGC (FS-SGC) Few-shot SGC (FS-SGC) Few-shot SGC (FS-SGC) Few-shot SGC (FS-SGC) Few-shot SGC (FS-SGC) Few-shot SGC (FS-SGC) Few-shot SGC (FS-SGC) Few-shot SGC (FS-SGC) Few-shot SGC (FS-SGC) Few-shot SGC (FS-SGC) [42] switches GIN to SGC for GNN encoder. Note that the previous four baselines only work in a few graph meta-learning problems. We also test on different meta-learning models, using the top performing ones in [34]. We operate on subgraph level for them since it allows comparison in all problems. No-Finetune No-Finetune No-Finetune No-Finetune No-Finetune No-Finetune No-Finetune No-Finetune No-Finetune No-Finetune No-Finetune No-Finetune No-Finetune No-Finetune No-Finetune No-Finetune No-Finetune performs training on the support set and use the trained model to classify each query example, using only meta-testing set</title>
		<author>
			<persName><forename type="first">Knn Knn Knn Knn Knn Knn Knn Knn Knn Knn Knn Knn Knn Knn Knn Knn</forename><surname>Knn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonina</forename><surname>Andreeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Kulesha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Gough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><forename type="middle">G</forename><surname>Murzin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Few-shot Graph Isomorphism Network (FS-GIN) Few-shot Graph Isomorphism Network (FS-GIN) Few-shot Graph Isomorphism Network (FS-GIN) Few-shot Graph Isomorphism Network (FS-GIN) Few-shot Graph Isomorphism Network (FS-GIN) Few-shot Graph Isomorphism Network (FS-GIN) Few-shot Graph Isomorphism Network (FS-GIN) Few-shot Graph Isomorphism Network (FS-GIN) Few-shot Graph Isomorphism Network (FS-GIN) Few-shot Graph Isomorphism Network (FS-GIN)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="D376" to="D382" />
		</imprint>
	</monogr>
	<note>first trains a GNN using all data in the meta-training set and it is used as an embedding function. Then, it uses the label of the voted K-closest example in the support set for each query example. Finetune Finetune Finetune Finetune Finetune. The scop database in 2020: expanded classification of representative family and superfamily domains of known protein structures</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Meta-Graph: few shot link prediction via meta learning</title>
		<author>
			<persName><forename type="first">Joey</forename><surname>Avishek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piero</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName><surname>Hamilton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.09867</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Few-shot learning on graphs via superclasses based on graph spectral measures</title>
		<author>
			<persName><forename type="first">Jatin</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Nathani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manohar</forename><surname>Kaul</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Meta relational learning for few-shot link prediction in knowledge graphs</title>
		<author>
			<persName><forename type="first">Mingyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
				<meeting><address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11">November 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning structural node embeddings via diffusion wavelets</title>
		<author>
			<persName><forename type="first">Claire</forename><surname>Donnat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Hallac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1320" to="1329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The distance-weighted k-nearest-neighbor rule</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sahibsingh</surname></persName>
		</author>
		<author>
			<persName><surname>Dudani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="325" to="327" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rolx: structural role extraction &amp; mining in large graphs</title>
		<author>
			<persName><forename type="first">Keith</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sugato</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1231" to="1239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bayesian Deep Learning Workshop at NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning to propagate for graph meta-learning</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">U</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alché-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1039" to="1050" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The natural language decathlon: Multitask learning as question answering</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08730</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Graph kernels for object category prediction in task-dependent robot grasping</title>
		<author>
			<persName><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Plinio</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Antanas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Online Proceedings of the Eleventh Workshop on Mining and Learning with Graphs</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="0" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Biological network comparison using graphlet degree distribution</title>
		<author>
			<persName><forename type="first">Nataša</forename><surname>Pržulj</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="e177" to="e183" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Shifting inductive bias with successstory algorithm, adaptive Levin search, and incremental self-improvement</title>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Wiering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="105" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Predicting protein-protein interactions based only on sequences information</title>
		<author>
			<persName><forename type="first">Juwen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaomin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiliang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hualiang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PNAS</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4337" to="4341" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Philip Hs Torr</surname></persName>
		</author>
		<author>
			<persName><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Inductive relation prediction by subgraph reasoning. ICML</title>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Komal K Teru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName><surname>Hamilton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning to learn</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorien</forename><surname>Pratt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Meta-dataset: A dataset of datasets for learning to learn from few examples</title>
		<author>
			<persName><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Utku</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carles</forename><surname>Gelada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Distributed, egocentric representations of graphs for detecting critical structures</title>
		<author>
			<persName><forename type="first">Ruo-</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Chun</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shan-Hung</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<editor>
			<persName><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6354" to="6362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Deep graph infomax</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A perspective view and survey of meta-learning</title>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Vilalta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youssef</forename><surname>Drissi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="77" to="95" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Matching networks for one shot learning</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3630" to="3638" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Unifying graph convolutional neural networks and label propagation</title>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06755</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Memory networks</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.3916</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning to learn how to learn: Self-adaptive visual navigation using meta-learning</title>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiana</forename><surname>Ehsani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6750" to="6759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">One-shot relational learning for knowledge graphs</title>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<meeting><address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks? In ICLR</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Gnnexplainer: Generating explanations for graph neural networks</title>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dylan</forename><surname>Bourgeois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9240" to="9251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Graphsaint: Graph sampling based inductive learning method</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman neural machine for link prediction</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="575" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="5165" to="5175" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Meta-GNN: On few-shot node classification in graph meta-learning</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtai</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goce</forename><surname>Trajcevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2357" to="2360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Modeling polypharmacy side effects with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="457" to="466" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Evolution of resilience in protein interactomes across the tree of life</title>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><forename type="middle">W</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PNAS</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4426" to="4433" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Predicting multicellular function through multi-layer tissue networks</title>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="190" to="198" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Details on Baselines We use nine baselines. (1) Meta-Graph Meta-Graph Meta-Graph Meta-Graph Meta-Graph Meta-Graph Meta-Graph Meta-Graph Meta-Graph Meta-Graph Meta-Graph Meta-Graph Meta-Graph Meta-Graph Meta-Graph Meta-Graph Meta-Graph [2] uses VGAE to do few-shot multi-graph link prediction. It uses a graph signature function to capture the characteristics of a graph, which enables knowledge transfer. Then, it applies MAML to learn across graphs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Appendix</surname></persName>
		</author>
		<author>
			<persName><surname>Further</surname></persName>
		</author>
		<editor>Meta-GNN Meta-GNN Meta-GNN Meta-GNN Meta-GNN Meta-GNN Meta-GNN Meta-GNN Meta-GNN Meta-GNN Meta-GNN Meta-GNN Meta-GNN Meta-GNN Meta-GNN Meta</editor>
		<imprint/>
	</monogr>
	<note>GNN Meta-GNN [50] applies MAML [9</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">first trains a GNN using all data in the meta-training set and it is used as an embedding function. Then, during meta-testing, it uses the label of the voted K-closest example in the support set for each query example. Finetune Finetune Finetune Finetune Finetune Finetune Finetune Finetune Finetune Finetune Finetune Finetune Finetune Finetune Finetune Finetune Finetune [34] uses the embedding function generated from meta-training set and the models are then finetuned on the meta-testing set</title>
		<author>
			<persName><forename type="first">Fs-Gin Fs-Gin Fs-Gin Fs-Gin Fs-Gin Fs-Gin Fs-Gin Fs-Gin Fs-Gin Fs-Gin Fs-Gin Fs-Gin Fs-Gin Fs-Gin Fs-Gin Fs-Gin Fs-Gin ; Fs-Sgc Fs-Sgc Fs-Sgc Fs-Sgc Fs-Sgc Fs-Sgc Fs-Sgc Fs-Sgc Fs-Sgc Fs-Sgc Fs-Sgc Fs-Sgc Fs-Sgc Fs-Sgc Fs-Sgc Fs-Sgc Fs-Sgc ; Knn Knn Knn Knn Knn Knn Knn Knn Knn Knn Knn Knn Knn Knn Knn Knn</forename><surname>Knn</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="report_type">ProtoNet ProtoNet ProtoNet ProtoNet ProtoNet ProtoNet ProtoNet ProtoNet ProtoNet ProtoNet ProtoNet ProtoNet ProtoNet ProtoNet ProtoNet ProtoNet</note>
	<note>42] switches GIN to SGC for GNN encoder. meta-learning problems. No-Finetune No-Finetune No-Finetune No-Finetune No-Finetune No-Finetune No-Finetune No-Finetune No-Finetune No-Finetune No-Finetune No-Finetune No-Finetune No-Finetune No-Finetune No-Finetune No-Finetune performs training on the support set and use the trained model to classify each query example, using only meta-testing set, i.e.without access to the external graphs or labels to transfer. ProtoNet [29] applies prototypical learning on each subgraph embeddings, following the standard few-shot learning setups. MAML MAML MAML MAML MAML MAML MAML MAML MAML MAML MAML MAML MAML MAML MAML MAML MAML [9] switches ProtoNet to MAML model as the meta-learner</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
