<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Empirical Study of Zero-Shot NER with ChatGPT</title>
				<funder ref="#_Ypk2eud">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
				<funder ref="#_spqMgtu">
					<orgName type="full">Zhejiang Provincial Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tingyu</forename><surname>Xie</surname></persName>
							<email>tingyuxie@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ZJU-UIUC Institute</orgName>
								<orgName type="institution" key="instit2">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ZJU-UIUC Institute</orgName>
								<orgName type="institution" key="instit2">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ZJU-UIUC Institute</orgName>
								<orgName type="institution" key="instit2">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zuozhu</forename><surname>Liu</surname></persName>
							<email>zuozhuliu@intl.zju.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ZJU-UIUC Institute</orgName>
								<orgName type="institution" key="instit2">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
							<email>hongweiwang@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ZJU-UIUC Institute</orgName>
								<orgName type="institution" key="instit2">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Empirical Study of Zero-Shot NER with ChatGPT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>The player who temporarily ranks second is German athlete Bao Lizzo</term>
					<term>with a total score of 355.02 points</term>
					<term>slightly lower than Lanwei. Gold Label: {&quot;German&quot;:&quot;Geo-Political Entity&quot;</term>
					<term>&quot;Lanwei&quot;: &quot;Person&quot;</term>
					<term>&quot;Bao?Lizzo&quot;: &quot;Person&quot;} Vanilla Ans: {&quot;German athlete Bao Lizzo&quot;: &quot;Person&quot;</term>
					<term>&quot;Lanwei&quot;: &quot;Person&quot;} TS-SC Ans: {&quot;Bao?Lizzo&quot;: &quot;Person&quot;: &quot;Person&quot;</term>
					<term>&quot;Lanwei&quot;: &quot;Person&quot;</term>
					<term>&quot;German&quot;: &quot;Geo-Political Entity&quot;} Contain gold mention Possible Reason: Benefits from syntactic information</term>
					<term>i.e.</term>
					<term>word segmentation here. Involved word segmentation: [&quot;German&quot;</term>
					<term>&quot;athlete&quot;</term>
					<term>&quot;Bao?Lizzo&quot;] Gold Label: {&quot;Luo Li&quot;: &quot;Person&quot;} Vanilla Ans: {&quot;Luo Li&quot;: &quot;Person&quot;</term>
					<term>&quot;Silver medal&quot;: &quot;Location&quot;} TS-SC Ans: {&quot;Luo Li&quot;: &quot;Person&quot;} Corrected Error Types: Completely-O Possible Reason: Better understanding of the input text due to syntactic information Gold Label: {&quot;research collection team&quot;: &quot;Organization&quot;</term>
					<term>&quot;Lu Xiuqin&quot;: &quot;Person&quot;} Vanilla Ans: {&quot;Lu Xiuqin&quot;: &quot;Person&quot;} (Omitted {&quot;research collection team&quot;: &quot;Organization&quot;}) TS-SC Ans: {&quot;Lu Xiuqin&quot;: &quot;Person&quot;</term>
					<term>&quot;collection team&quot;: &quot;Organization&quot;} Contained by gold mention Possible Reason: Misguided by syntactic information. Involved word segmentation: [&quot;research&quot;</term>
					<term>&quot;collection team&quot;]. However</term>
					<term>without the word segmentation</term>
					<term>the model could completely overlook this entity. Optimization Direction: Provide more accurate and comprehensive syntactic information. E.g.</term>
					<term>Using more powerful parsing tool and combinations of different syntactic information Xinhua News Agency</term>
					<term>Tianjin</term>
					<term>December 15th (Reporter Li Jianchang</term>
					<term>intern Ge Suhong). Gold Label: {&quot;Li Jianchang&quot;: &quot;Person&quot;</term>
					<term>&quot;Tianjin&quot;: &quot;Geo-Political Entity&quot;</term>
					<term>&quot;Xinhua News Agency&quot;: &quot;Organization&quot;</term>
					<term>&quot;Ge Suhong&quot;: &quot;Person&quot;} Vanilla Ans: {&quot;Xinhua News Agency&quot;: &quot;Organization&quot;</term>
					<term>&quot;Tianjin&quot;: &quot;Location&quot;</term>
					<term>&quot;Li Jianchang&quot;: &quot;Person&quot;</term>
					<term>&quot;Ge Suhong&quot;: &quot;Person&quot;} TS-SC Ans: {&quot;Li Jianchang&quot;: &quot;Person&quot;</term>
					<term>&quot;Ge Suhong&quot;: &quot;Person&quot;</term>
					<term>&quot;Xinhua News Agency&quot;: &quot;Organization&quot;} (Omitted {&quot;Tianjin&quot;: &quot;Geo-Political Entity&quot;}) Increased Error Types: Omitted mention Possible Reason: Inadequate understanding of entity types. Optimization Direction: Provide entity type information</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large language models (LLMs) exhibited powerful capability in various natural language processing tasks. This work focuses on exploring LLM performance on zero-shot information extraction, with a focus on the ChatGPT and named entity recognition (NER) task. Inspired by the remarkable reasoning capability of LLM on symbolic and arithmetic reasoning, we adapt the prevalent reasoning methods to NER and propose reasoning strategies tailored for NER. First, we explore a decomposed question-answering paradigm by breaking down the NER task into simpler subproblems by labels. Second, we propose syntactic augmentation to stimulate the model's intermediate thinking in two ways: syntactic prompting, which encourages the model to analyze the syntactic structure itself, and tool augmentation, which provides the model with the syntactic information generated by a parsing tool. Besides, we adapt self-consistency to NER by proposing a two-stage majority voting strategy, which first votes for the most consistent mentions, then the most consistent types. The proposed methods achieve remarkable improvements for zero-shot NER across seven benchmarks, including Chinese and English datasets, and on both domainspecific and general-domain scenarios. In addition, we present a comprehensive analysis of the error types with suggestions for optimization directions. We also verify the effectiveness of the proposed methods on the few-shot setting and other LLMs. 1  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large language models (LLMs) <ref type="bibr" target="#b15">(OpenAI, 2022;</ref><ref type="bibr">Thoppilan et al., 2022;</ref><ref type="bibr" target="#b29">Chowdhery et al., 2022)</ref> have brought revolutions in natural language processing (NLP) due to the remarkable zero-shot and few-shot generalization. One of the most wellknown LLMs, ChatGPT (OpenAI, 2022) powered by <ref type="bibr">GPT3.5 and GPT4 (OpenAI, 2023)</ref>, has exhibited strong dialogue capabilities. As a closed model, ChatGPT sparked a lot of work for its evaluation and application on diverse tasks and aspects <ref type="bibr" target="#b20">(Qin et al., 2023;</ref><ref type="bibr" target="#b31">Wei et al., 2023;</ref><ref type="bibr" target="#b13">Liang et al., 2023)</ref>.</p><p>Information extraction (IE) is a fundamental topic in NLP, which aims to extract structured information from unstructured text, including tasks such as named entity recognition (NER) <ref type="bibr" target="#b32">(Yu et al., 2020)</ref>, relation extraction (RE) <ref type="bibr" target="#b1">(Baldini Soares et al., 2019)</ref>, event extraction (EE) <ref type="bibr" target="#b2">(Chen et al., 2015)</ref>, etc. Evaluating ChatGPT's performance on IE is important for understanding its capabilities in structured prediction and language understanding <ref type="bibr" target="#b10">(Li et al., 2023;</ref><ref type="bibr" target="#b31">Wei et al., 2023)</ref>.</p><p>With recent techniques for eliciting complex multi-step reasoning <ref type="bibr" target="#b30">(Wei et al., 2022;</ref><ref type="bibr">Wang et al., 2022b)</ref>, LLMs have shown remarkable zero-shot reasoning ability in arithmetic and symbolic reasoning <ref type="bibr" target="#b8">(Kojima et al., 2022)</ref>. However, the reasoning ability of LLM on IE remained unexplored. To mitigate this gap, we present a systematic empirical study exploring the reasoning capability of LLM on IE, with a focus on the ChatGPT and zero-shot NER task. By adapting the prevalent reasoning techniques <ref type="bibr" target="#b36">(Zhou et al., 2022;</ref><ref type="bibr" target="#b30">Wei et al., 2022;</ref><ref type="bibr">Wang et al., 2022b)</ref> to NER, we propose the following strategies to stimulate the reasoning potential of LLM on NER:</p><p>? We break down the NER task into a series of simpler subproblems by labels and perform a decomposed-question-answering (Decomposed-QA) paradigm, where the model extracts entities of only one label at a time.</p><p>? We propose syntactic augmentation of two ways: syntactic prompting, which encourages the model to first analyze the syntactic structure of the input text itself, then recog-nize the named entities based on the syntactic structure; tool augmentation, which provides the syntactic information generated by a parsing tool to the model.</p><p>? We tailor the self-consistency (SC) <ref type="bibr">(Wang et al., 2022b)</ref> for NER and propose a twostage majority voting strategy: after sampling multiple responses of the model, we first vote for the most consistent mentions, then the most consistent types.</p><p>The main contributions of this paper include:</p><p>? We present a systematic empirical investigation of zero-shot NER with LLM, with a specific emphasis on ChatGPT as one of the most robust LLMs available.</p><p>? We adapt prevalent reasoning methods to NER and propose four strategies tailored for NER: decomposed-QA, syntactic prompting, tool augmentation, and two-stage majority voting.</p><p>? We evaluate our strategies across seven benchmarks. Experiment results reveal that the proposed strategies significantly facilitate zero-shot NER across domain-specific outof-distribution and general-domain datasets, including Chinese and English scenarios.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Reasoning with LLM</head><p>LLM has shown remarkable zero-shot reasoning ability, in the way of explicitly encouraging the LLM to generate intermediate rational for solving a problem. On the one hand, recent works, in both the few-shot <ref type="bibr" target="#b30">(Wei et al., 2022;</ref><ref type="bibr" target="#b34">Zhang et al., 2022;</ref><ref type="bibr">Wang et al., 2022a)</ref> and zero-shot <ref type="bibr" target="#b8">(Kojima et al., 2022)</ref> setting, elicit chain-of-thought (CoT) from LLM and modify the answer by step-by-step. On the other hand, the problem decomposition, like least-to-most prompting <ref type="bibr" target="#b36">(Zhou et al., 2022)</ref>, reduces complex problems to the sub-problems, and then solves these sub-problems sequentially; the SC strategy <ref type="bibr">(Wang et al., 2022b)</ref> generates a diverse set of answers by sampling from LLM, and then marginalizes out the sampled answers to determine the optimal answer. In this work, we focus on investigating the zero-shot reasoning ability of LLM on the NER task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">LLM on IE</head><p>A few works study the performance of the powerful LLM ChatGPT <ref type="bibr" target="#b10">(Li et al., 2023;</ref><ref type="bibr" target="#b14">Ma et al., 2023;</ref><ref type="bibr" target="#b9">Laskar et al., 2023)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Adapting the prevalent reasoning techniques <ref type="bibr" target="#b36">(Zhou et al., 2022;</ref><ref type="bibr" target="#b30">Wei et al., 2022;</ref><ref type="bibr">Wang et al., 2022b)</ref> to NER, we propose four strategies to stimulate the reasoning capabilities of LLM on NER. Examples of the proposed methods are shown in Fig. <ref type="figure" target="#fig_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Decomposed-QA</head><p>Inspired by least-to-most prompting <ref type="bibr" target="#b36">(Zhou et al., 2022)</ref>, we improve zero-shot NER by decomposing the task into a set of simpler questions. Recognizing entities of all labels at one time may be too challenging for ChatGPT (as the vanilla zero-shot method shown in (a) of Fig. <ref type="figure" target="#fig_1">1</ref>), especially when the label size is large, or the data is from a specific out-of-distribution domain. This motivates us to break down the NER task by labels. Given an input sentence, the whole process of recognizing entities is a multi-turn dialogue paradigm. Each time, Chat-GPT is asked to recognize entities of a single label.</p><p>After ChatGPT provides its response to the current question, we proceed to ask questions related to the next label, incorporating all the previous questions and answers as part of the dialogue context. Once all questions pertaining to each label have been addressed, we conclude the entire conversation.</p><p>We name this paradigm Decomposed-QA. The {ChatGPT response} (questions of each label) ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(c) Syntactic Prompting</head><p>Given entity label set: {label set}  We obtain the label order used in the multi-turn dialogue by asking ChatGPT. For each dataset, we provide the task requirement and the label set to ChatGPT, then ask it to give a reasonable label order based on its understanding of the labels. For domain-specific datasets, PowerPlantFlat and Pow-erPlantNested, which will be introduced in Section 4.1, we also use a manual label order provided by the domain experts. The label orders are shown in Appendix G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Syntactic Augmentation</head><p>Aiming to guide the model to think step by step while extracting information, we encourage Chat-GPT to first grasp the syntactic structure of the input text and then leverage this syntactic structure to extract relevant information. Among them,  five kinds of syntactic information are utilized: word segmentation, noun phrases, Part-of-Speech (POS) tags, constituency trees, and dependency trees. Word segmentation is only for Chinese. We propose the following two ways of syntactic augmentation.</p><p>Syntactic Prompting. We encourage the model to analyze the syntactic structure itself by inserting the syntactic reasoning hint in the input instruction, as shown in (c) of Fig. <ref type="figure" target="#fig_1">1</ref>. We explore two positions of syntactic reasoning hint, i.e., in the back or front of the instruction, as shown in Fig. <ref type="figure" target="#fig_3">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tool Augmentation.</head><p>We first obtain the syntactic information of the input text via a parsing tool;<ref type="foot" target="#foot_0">2</ref> Then, we feed the input text together with the syntactic information to ChatGPT, as shown in (d) of Fig. <ref type="figure" target="#fig_1">1</ref>. We do not apply noun phrases in tool augmentation since we do not obtain a parsing tool with a reliable ability to extract noun phrases.</p><p>We further explore the combination of tool augmentation and syntactic prompting. To enhance the utilization of syntactic information from the parsing tool, we insert a syntactic reasoning hint. The example is shown in (e) of Fig. <ref type="figure" target="#fig_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Self-Consistency with Two-Stage Majority Voting</head><p>Harnessing the power of SC <ref type="bibr">(Wang et al., 2022b)</ref>, we sample multiple responses from the model and select the most acknowledged answers as the final prediction. We design a two-stage majority voting for NER, as shown in (f) of Fig. <ref type="figure" target="#fig_1">1</ref>. At stage one, for each candidate mention appeared in all responses, we consider it as an entity if it appeared in more than half of the responses; otherwise, we discard this mention. At stage two, for each mention kept in stage one, we choose the entity label predicted by the majority of responses as the final predicted label.</p><p>We explore two levels of SC for decomposed-QA: question-level and sample-level. For questionlevel, we sample multiple responses for the current question and conduct majority voting; then, we fill the voted answer into the dialogue context for all subsequent questions. For sample-level, we run the whole dialogue multiple times independently and obtain the answer of each run, then conduct majority voting on these answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>Datasets. We evaluate ChatGPT performance on both domain-specific and general-domain datasets. For domain-specific datasets, we present two Chinese NER datasets of the electric power domain, PowerPlantFlat (PPF) and PowerPlantNested (PPN). The two datasets are collected from the technical reports, which are formed during nuclear power plant operation and maintenance. Pow-erPlantFlat only contains flat cases, while Pow-erPlantNested contains nested entities. The two datasets are formed in the vertical industrial domain, and thus serve as out-of-distribution data for ChatGPT. The statistics of the two datasets are shown in Appendix A. For general-domain datasets, we evaluate on commonly used benchmarks, including two English datasets, ACE05,<ref type="foot" target="#foot_1">3</ref> and ACE04,<ref type="foot" target="#foot_2">4</ref> and three Chinese datasets, OntoNotes 4 (Onto. 4),<ref type="foot" target="#foot_3">5</ref> MSRA <ref type="bibr" target="#b33">(Zhang et al., 2006)</ref> and Weibo NER <ref type="bibr" target="#b18">(Peng and Dredze, 2015)</ref>. For evaluation on more datasets, please refer to Appendix E.</p><p>Model. We mainly evaluate on GPT-3.5 (gpt-3.5turbo) with official API.<ref type="foot" target="#foot_4">6</ref> For Decomposed-QA, we maintain a dialogue paradigm for each test sample. For vanilla setting, we generate the response separately for each test sample.</p><p>We also evaluate on GPT-3 (text-davinci-003) <ref type="bibr" target="#b17">(Ouyang et al., 2022)</ref> and Llama2 <ref type="bibr" target="#b24">(Touvron et al., 2023)</ref> to verify the effectiveness of the proposed methods on other LLMs. We use the 13B chat model of Llama2. <ref type="foot" target="#foot_5">7</ref> The results of these two LLMs are in Section 4.5.</p><p>Self-consistency. We set the temperature to 0.7 and 0 for settings with and without SC, respectively. For cost saving, we conduct majority voting of 5 responses in our main experiments. We first conduct both question-level and sample-level consistency on each dataset; then, we choose the way of higher performance for the rest of the experiments on the corresponding dataset.</p><p>Data sampling. For syntactic augmentation, we evaluate on the entire test sets of seven datasets. For SC and combinations of techniques, for cost saving, we evaluate on partial datasets and randomly sampled subsets of test sets: We evaluate on the two domain-specific datasets, PowerPlantFlat and PowerPlantNested, with entire test sets, and two general-domain datasets, Ontonotes 4 and ACE05, by randomly sampling 300 samples from the test set three times and reporting the average results. SOTA of fully-supervised methods. For Pow-erPlantFlat and PowerPlantNested, we use Global-Pointer <ref type="bibr" target="#b22">(Su et al., 2022)</ref> since it performs well on both flat and nested cases. For other benchmarks, we refer to corresponding papers: Weibo <ref type="bibr" target="#b28">(Wang et al., 2021)</ref>, MSRA <ref type="bibr" target="#b12">(Li et al., 2020)</ref>, Ontonotes 4 <ref type="bibr" target="#b12">(Li et al., 2020)</ref>, ACE05 (Zhong and Chen, 2021), ACE04 <ref type="bibr" target="#b35">(Zhong and Chen, 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Overall Performance</head><p>Table <ref type="table" target="#tab_2">1</ref> summarizes the performances of decomposed-QA and syntactic augmentation.</p><p>For the two domain-specific datasets, we use the manual label orders as they show better performance in preliminary experiments. For cost saving, we explore SC and combinations of reasoning techniques on selected datasets and sampled test sets, which are detailed in Section 4.3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Effect of Decomposed-QA</head><p>From Table <ref type="table" target="#tab_2">1</ref>, we have the following observations: (1) Compared with the vanilla method, decomposed-QA achieves significant improvements across all benchmarks, including both Chinese and English scenarios, and both domainspecific and general-domain scenarios. This demonstrates that decomposing by labels makes the NER task much more manageable for Chat-GPT.</p><p>(2) Decomposed-QA exhibits more significant improvements on domain-specific datasets (with an average 9.22% F1 gain) than on generaldomain datasets (with an average 3.82% F1 gain). This is presumably because out-of-distribution data are more challenging for ChatGPT. Decomposing makes ChatGPT acquire a better understanding of the out-of-distribution data.</p><p>(3) We also explore the effect of reasoning techniques under the vanilla setting, and the results are in Table <ref type="table" target="#tab_16">9</ref> of Appendix C. We found that the vanilla setting fails to stimulate the potential of reasoning techniques. Contrarily, decomposed-QA stimulates the potential of syntactic augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Effect of Syntactic Augmentation</head><p>As shown in Table <ref type="table" target="#tab_2">1</ref>, we draw the following conclusions: (1) Syntactic prompting alone brings limited benefits. This is presumably because conducting syntactic analysis without any other augmentation is challenging for ChatGPT.</p><p>(2) Tool augmentation exhibits consistent improvements across six datasets, showing that syntactic information helps ChatGPT better understand the input text.</p><p>(3) Tool augmentation achieves more improvements on Chinese than English datasets. This may be due to the fact that Chinese is harder than English for Chat-GPT to handle, and syntactic information provides a clue on how to understand the Chinese input better. ( <ref type="formula">4</ref>) Different kinds of syntactic information exhibit various performances. On Chinese datasets, word segmentation shows the best performance. On English datasets, POS tags boost the most. This is presumably because simpler syntactic information is easier for ChatGPT to understand. Complex syntactic information, such as dependency tree, though informative, can be hard to understand, thereby, exhibiting unstable performance.    in Fig. <ref type="figure" target="#fig_4">3</ref> for better analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effect of Self-Consistency and Combinations of Reasoning Techniques</head><p>From the table and the figure, we have the following observations and conclusions: (1) SC shows consistent improvements on almost all methods. As long as the syntactic information is involved, SC can always boost performance. This may be due to the fact that syntactic information is helpful but hard to understand or analyze. Thus, syntactic information gives ChatGPT the potential to perform better but also a higher possibility of making mistakes. SC can filter out errors, thereby, leveraging the advantages, and eliminating the disadvantages of syntactic information. (2) Syntactic prompting fails to boost tool augmentation and even hurts the performance. However, when equipped with SC, syntactic prompting improves tool augmentation. This may be due to the complexity of information provided by the combination of tool augmentation and syntactic prompting. The complex information leads the model to think and explore more, and of course, it is also accompanied by more possibilities for errors. This makes SC an effective means of filtering out errors here. (3) SC improves more when syntactic reasoning hints are put on the back than on the front. This is presumably because the closer the reasoning hint is to the answer, the more it can stimulate the model's thinking. Hence, putting the reasoning hints on the back encourages the model to generate more diverse answers, which provides better search spaces for majority voting. We explore the effect of increasing sampled responses in SC, which are shown in Fig. <ref type="figure" target="#fig_5">4</ref>. We sample up to 30 responses for cost saving. As seen in the figure, sampling a higher number of responses improves the performance. We conjecture that combining diverse syntactic information may further benefit SC on NER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Error Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Error Types</head><p>We take Ontonotes 4 for error analysis. Table <ref type="table" target="#tab_6">3</ref> summarizes the statistics of error types. Fig. <ref type="figure">6</ref> visualize the percentages of error types. Below is the introduction of error types:</p><p>Type. OOD types: predicted entity types not in the given label set; Wrong types: predicted types Figure <ref type="figure">5</ref>: Case study of error correction and error increase with the proposed methods. We translate the original Chinese text into English in the demonstrations for readability. The upper two cases are errors corrected, and the lower two are errors increased. Texts in blue are involved entities in the error cases. Our method shows effectiveness on error corrections. With the suggested optimization strategies, the error increased might be eliminated. incorrect but in the given label set.</p><p>Boundary. Contain gold.: predicted mentions containing gold mentions; Contained by gold.: predicted mentions contained by gold mentions; Overlap with gold.: predicted mentions not in the two above situations but overlap with gold mentions.</p><p>Completely-O: predicted mentions that do not have any of the three above boundary situations with any gold mentions.</p><p>OOD mentions: predicted mentions that do not appear in the input text.</p><p>As shown in Fig. <ref type="figure">6</ref>, the majority error types are complete-O and wrong types, which account for over 80% of all errors. The former may be due to the incomplete annotation or that ChatGPT would guess entities based on its prior common knowledge. The latter may be due to the inadequate understanding of entity types. As seen in Table <ref type="table" target="#tab_6">3</ref>, decomposed-QA reduces the total error numbers by 9.4%; The combination of Tool augmentation, Syntactic prompting and SC (TS-SC) reduces the error numbers by 24.1%, showing remarkable capability in error corrections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Case Study of Error Correction and Error Increase</head><p>As seen in Table <ref type="table" target="#tab_6">3</ref>, TS-SC reduces errors mainly in types of contain gold. and completely-O, and increases errors mainly in types of contained by gold. and omitted mentions. Thus, we conduct case study on these four types, which are shown in Fig. <ref type="figure">5</ref>. TS-SC corrects errors of contain gold. and completely-O presumably by providing syntactic information and making the model better understand the input text. Meanwhile, TS-SC increases errors of contain gold. and omitted mentions presumably because of the misguiding of syntactic information and inadequate understanding of entity types, respectively. For the former, providing more accurate and comprehensive syntactic information might be a solution; for the latter, providing type information might be a direction of optimization.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">More analysis</head><p>Few-shot setting. We evaluate the proposed syntactic augmentation under few-shot setting. We take a different perspective: we encourage the model to explore syntactic information as their intermediate thinking steps. Detailed adaptations of our methods to the few-shot setting are explained in Appendix D. For the decomposed-QA and SC, we leave them to future work due to the cost budget. We compared our methods to the vanilla method and standard CoT. We use ChatGPT to generate rationales in standard CoT, following <ref type="bibr" target="#b6">(Han et al., 2023)</ref>. Here, we use one general domain dataset, Ontonotes 4, and one domain-specific dataset, Pow-erPlantFlat, for demonstrations. The results are shown in Table <ref type="table" target="#tab_9">4</ref>, in which the word segmentation is used for demonstration. The results of various syntactic information are in Appendix D.</p><p>As observed in Table <ref type="table" target="#tab_9">4</ref>, the standard CoT does not bring improvements, even hurt the performance. This is presumably because standard CoT is very sensitive to the rationales constructed, which is also mentioned in <ref type="bibr" target="#b6">(Han et al., 2023)</ref>. However, our strategies have achieved significant improvements. This shows that the proposed methods are effective not only in the zero-shot scenario but also in the few-shot setting.</p><p>Other LLMs. We also evaluate our methods on GPT-3 (text-davinci-003) <ref type="bibr" target="#b17">(Ouyang et al., 2022)</ref> and Llama2 <ref type="bibr" target="#b24">(Touvron et al., 2023)</ref>. Since Llama2 still has poor support for Chinese yet, we evaluate on two English datasets, one general-domain dataset, ACE05, and one biomedical dataset, BC5CDR <ref type="bibr" target="#b11">(Li et al., 2016)</ref>. The results are shown in Table <ref type="table" target="#tab_10">5</ref>, in which the dependency tree is used for demonstration. The complete results are in Appendix F. The main results of BC5CDR are in Appendix E. Table <ref type="table" target="#tab_10">5</ref> shows that our methods exhibit consistent improvements across different LLMs, including the close-sourced ChatGPT model series and typical open-sourced model Llama.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present an empirical study of zero-shot NER with ChatGPT, with four proposed strategies to simulate the reasoning potential of ChatGPT on NER. Inspired by the powerful reasoning capabilities of LLM on logical and arithmetic reasoning tasks, the proposed strategies involve task decomposition, syntactic augmentation, and tailored SC. We verify the effectiveness of our methods on Chinese and English scenarios, and on both domain-specific and general-domain datasets. We provide an analysis of the error types with suggested solutions. Besides, we verify the effectiveness of the proposed methods on the few-shot setting and other LLMs.</p><p>For cost saving, we focus on the investigation of each individual syntactic information and have not explored the combinations of different kinds of syntactic information. Also, we have not investigated manual label orders on general-domain datasets for the same reason. We leave them to future work.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Statistics of PowerPlant Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Statistics of Errors on PowerPlantFlat</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Performance Under Vanilla Setting</head><p>We also investigate the effect of our proposed reasoning techniques on the standard setting. The results are shown in Table <ref type="table" target="#tab_16">9</ref>. From the table, we can conclude that the potential of the syntactic information cannot be fully exploited under the standard setting. On the contrary, the proposed decomposed-QA paradigm effectively utilizes the syntactic information, as shown in Tabel 2 and Figure <ref type="figure" target="#fig_4">3</ref>. Under standard setting, the reasoning techniques bring limited benefits for general-domain datasets, sometimes even hurting the performance. However, these techniques exhibit improvements on domainspecific datasets, i.e., out-of-distribution datasets. This is presumably because out-of-distribution data is much more challenging than general-domain data for ChatGPT. The reasoning techniques lead the model to have a better understanding of the out-of-distribution data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Syntactic Augmentation Under</head><p>Few-shot Setting</p><p>The following are the adaptations of our proposed syntactic augmentation strategies to the few-shot setting.</p><p>(1) Syntactic prompting: For the test sample, we ask the model to first perform syntactic analysis and then recognize entities. For demonstrations, we use parsing tools to generate intermediate syntactic parsing results. (2) Tool augmentation:</p><p>We provide both the text and syntactic information for the demonstrations and the test sample.</p><p>Table <ref type="table" target="#tab_17">10</ref> shows the experiment results under the 3-shot setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Evaluation on More Datasets</head><p>We additionally evaluate the proposed methods on more datasets, including general-domain datasets, <ref type="bibr">CoNLL-2003 (Sang and</ref><ref type="bibr" target="#b21">De Meulder, 2003)</ref>, <ref type="bibr">WNUT-17(Derczynski et al., 2017)</ref>, and domain-specific datasets (i.e., biomedical domain), BC5CDR <ref type="bibr" target="#b11">(Li et al., 2016)</ref>, BioNLP11 <ref type="bibr" target="#b19">(Pyysalo et al., 2012)</ref>, and CRAFT <ref type="bibr" target="#b0">(Bada et al., 2012;</ref><ref type="bibr" target="#b4">Crichton et al., 2017)</ref>.</p><p>The results are shown in Table <ref type="table" target="#tab_18">11</ref>. We found that the proposed reasoning techniques cannot guarantee performance improvements on CoNLL-2003 and WNUT-17 and even hurt the performance. This is presumably because the label logic of these two datasets is not suitable for decomposition, and the syntactic information generated for them is noisy.</p><p>Meanwhile, we conjecture that this is also due to the fact that CoNLL-2003 and WNUT-17 contain more numbers of shorter texts, on which the reasoning techniques are difficult to leverage their advantages. However, the proposed methods achieve significant improvements in biomedical domain datasets BC5CDR, BioNLP11, and CRAFT. This demonstrates that the proposed methods can also improve zero-shot NER of other challenging domains besides the electric power domain. Plus the five datasets evaluated in Table <ref type="table" target="#tab_18">11</ref>, we evaluate on twelve benchmarks in total and achieve remarkable improvements on ten datasets among them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Evaluation on Other LLMs</head><p>The complete results on GPT-3 and Llama2 are shown in Table <ref type="table" target="#tab_19">12</ref>. These results show that our methods exhibit consistent improvements across different LLMs. On the smallest LLM evaluated, Llama2 13B, our proposed strategies still achieve remarkable performance improvements, with 19.72% and 17.51% F1 improvements on ACE05 and BC5CDR, respectively. This reveals that our methods have wide applicability to various sizes of LLMs, which is beneficial for low-resource scenarios such as when only smaller LLMs are affordable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Label Order</head><p>Table <ref type="table" target="#tab_20">13</ref> displays label orders used in our main experiments and the corresponding results under basic decomposed-QA. On the power plant datasets, manual label orders provided by domain experts achieve significantly better results. This demonstrates that when dealing with domain-specific datasets with ChatGPT, one may turn to domain knowledge to boost performance.</p><p>Table <ref type="table" target="#tab_9">14</ref> displays the label orders of additional datasets.</p><p>Table <ref type="table" target="#tab_10">15</ref> shows the instructions for asking Chat-GPT to provide label orders of PowerPlantFlat and ACE05 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Prompts</head><p>We show all of our prompts with Ontonotes 4 and ACE05 as examples. The prompts are in    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Input text: Could Tony Blair be in line for a gold medal? Gold label: {'Tony Blair': 'Person'} Label set: ['Person', 'Organization', 'Location', 'Facility', 'Weapon', 'Vehicle', 'Geo-Political Entity'] Given entity label set: {label set} Based on the given entity label set, please recognize the named entities in the set: {label set} Based on the given entity label set, please recognize the named entities in the given text. Text: {input text} Question: What are the named entities labeled as 'Person' in the text? Answer: First, let's perform Parf-of-Speech tagging. Then, we recognize named entities based on the Part-of-Speech tags. {ChatGPT response} (questions of each label) ...(c) Syntactic Prompting Given entity label set: {label set} Based on the given entity label set, please recognize the named entities in the given text. Text: {input text} Question: What are the named entities labeled as 'Person' in the text? Answer: First, let's perform Parf-of-Speech tagging. Then, we recognize named entities based on the Part-of-Speech tags.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of proposed methods for zero-shot NER with ChatGPT. (a) Vanilla zero-shot method. (b) Basic decomposed-QA, where the NER task is broken down into simpler subproblems. (c) Decomposed-QA with syntactic prompting. Texts in green are the proposed syntactic reasoning hint . (d) Decomposed-QA with tool augmentation. Texts in orange are the content of syntactic information. (e) Decomposed-QA with tool augmentation and syntactic prompting. (f) SC with two-stage majority voting, where stage one votes for the mentions and stage two votes for types. We use part-of-speech tags as an example syntactic information in this figure. The detailed prompts are shown in Appendix H. example is shown in (b) of Fig. 1.We obtain the label order used in the multi-turn dialogue by asking ChatGPT. For each dataset, we provide the task requirement and the label set to ChatGPT, then ask it to give a reasonable label order based on its understanding of the labels. For domain-specific datasets, PowerPlantFlat and Pow-erPlantNested, which will be introduced in Section 4.1, we also use a manual label order provided by the domain experts. The label orders are shown in Appendix G.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Two positions of syntactic reasoning hint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance of combinations of reasoning techniques. For methods involving syntactic augmentation, we plot the average results over all kinds of syntactic information. The vertical lines on the top part of some bars represent the performances range over all kinds of syntactic information. With SC of two-stage majority voting, the combinations of reasoning techniques further improve the performances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Increasing sampled responses generally improves performance under SC with two-stage majority voting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Percentage of different error types on Power-PlantFlat under the vanilla setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>on IE tasks.<ref type="bibr" target="#b31">Wei et al. (2023)</ref> propose a two-stage chatting paradigm for IE. At stage one, it asks ChatGPT to recognize the types of elements; at stage two, it asks ChatGPT to extract the mentions corresponding to each type recognized at stage one.</figDesc><table><row><cell>Han et al. (2023) presents</cell></row><row><cell>an analysis of ChatGPT's performance on IE tasks</cell></row><row><cell>from four aspects: performance, evaluation criteria,</cell></row><row><cell>robustness, and errors. Wang et al. (2023) apply</cell></row><row><cell>in-context learning (ICL) to NER by inserting spe-</cell></row><row><cell>cial tokens into the demonstrations retrieved from</cell></row><row><cell>the training set. Wan et al. (2023) apply CoT to</cell></row><row><cell>relation extraction (RE) and use ChatGPT to gen-</cell></row><row><cell>erate intermediate rationales for demonstrations</cell></row><row><cell>retrieved from the training set. Different from pre-</cell></row><row><cell>vious works, we focus on exploring the ChatGPT</cell></row><row><cell>abilities for zero-shot reasoning on IE, with a fo-</cell></row><row><cell>cus on the NER task. We explore the prevalent</cell></row><row><cell>reasoning methods with LLM, which exhibited re-</cell></row><row><cell>markable performance on arithmetic and logical</cell></row><row><cell>reasoning tasks. Most importantly, these methods</cell></row><row><cell>are first adapted to the NER task based on the task</cell></row><row><cell>characteristics.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Overall performance. We report the F1 values. Vanilla for vanilla zero-shot method without any techniques; Syn. for syntactic prompting; Tool. for tool augmentation. We use the same abbreviations in the rest of this paper when necessary. Syntactic augmentation is all conducted under the decomposed-QA setting. Numbers in bold are the best results in the corresponding categories; Numbers underlined are the best results among all methods in the zero-shot scenario. The proposed decomposed-QA and syntactic augmentation achieve significant improvements for zero-shot NER on both Chinese and English datasets and on both domain-specific and general-domain scenarios.</figDesc><table><row><cell></cell><cell>Method</cell><cell>PPF</cell><cell cols="6">PPN Weibo MSRA Onto. 4 ACE05 ACE04</cell></row><row><cell></cell><cell>Vanilla</cell><cell cols="2">27.85 20.43</cell><cell>30.09</cell><cell>45.51</cell><cell>33.74</cell><cell>28.12</cell><cell>20.09</cell></row><row><cell cols="2">Decomposed-QA</cell><cell cols="2">36.57 30.14</cell><cell>34.04</cell><cell>48.60</cell><cell>37.45</cell><cell>34.37</cell><cell>22.19</cell></row><row><cell></cell><cell cols="3">Word segmentation 38.16 30.38</cell><cell>32.72</cell><cell>47.52</cell><cell>37.47</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Noun phrases</cell><cell cols="2">37.46 30.02</cell><cell>33.93</cell><cell>46.05</cell><cell>38.31</cell><cell>33.22</cell><cell>20.99</cell></row><row><cell>Front</cell><cell>POS tag</cell><cell cols="2">36.89 30.60</cell><cell>32.68</cell><cell>46.87</cell><cell>36.82</cell><cell>34.31</cell><cell>21.74</cell></row><row><cell></cell><cell>Constituency tree</cell><cell cols="2">36.21 29.88</cell><cell>31.85</cell><cell>46.02</cell><cell>36.52</cell><cell>33.22</cell><cell>20.86</cell></row><row><cell>Syn.</cell><cell>Dependency tree</cell><cell cols="2">36.33 29.82</cell><cell>33.49</cell><cell>45.61</cell><cell>35.90</cell><cell>34.21</cell><cell>21.04</cell></row><row><cell></cell><cell cols="3">Word segmentation 34.89 25.87</cell><cell>32.43</cell><cell>48.74</cell><cell>37.48</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Noun phrases</cell><cell cols="2">32.59 24.32</cell><cell>28.71</cell><cell>46.84</cell><cell>38.27</cell><cell>29.36</cell><cell>21.74</cell></row><row><cell>Back</cell><cell>POS tag</cell><cell cols="2">36.18 26.11</cell><cell>33.51</cell><cell>44.40</cell><cell>36.82</cell><cell>28.84</cell><cell>23.88</cell></row><row><cell></cell><cell>Constituency tree</cell><cell cols="2">35.71 23.93</cell><cell>30.46</cell><cell>45.84</cell><cell>39.00</cell><cell>21.37</cell><cell>18.81</cell></row><row><cell></cell><cell>Dependency tree</cell><cell cols="2">31.05 21.02</cell><cell>27.61</cell><cell>44.87</cell><cell>38.52</cell><cell>25.57</cell><cell>21.04</cell></row><row><cell></cell><cell cols="3">Word segmentation 39.77 33.81</cell><cell>36.30</cell><cell>53.67</cell><cell>39.20</cell><cell>-</cell><cell>-</cell></row><row><cell>Tool.</cell><cell>POS tag Constituency tree</cell><cell cols="2">38.11 30.97 36.51 30.25</cell><cell>35.14 32.00</cell><cell>51.99 48.32</cell><cell>37.61 38.40</cell><cell>34.33 32.96</cell><cell>22.41 22.15</cell></row><row><cell></cell><cell>Dependency tree</cell><cell cols="2">39.50 32.12</cell><cell>36.16</cell><cell>48.82</cell><cell>38.05</cell><cell>33.38</cell><cell>22.37</cell></row><row><cell cols="2">SOTA (fully-supervised)</cell><cell cols="2">68.54 70.41</cell><cell>72.77</cell><cell>96.72</cell><cell>84.47</cell><cell>90.90</cell><cell>90.30</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance of SC and combinations of reasoning techniques. We report the F1 values. Numbers in parentheses are the standard deviations. Numbers in bold are the best results in the corresponding categories; Numbers underlined are the best results among all methods in the zero-shot scenario. SC with two-stage majority voting and combinations of reasoning techniques brings further improvements.</figDesc><table><row><cell>Method</cell><cell></cell><cell>PPF</cell><cell>PPN</cell><cell>Onto. 4</cell><cell>ACE05</cell></row><row><cell>Vanilla</cell><cell></cell><cell cols="3">27.85 20.43 35.16 (1.57)</cell><cell>29.45 (0.69)</cell></row><row><cell cols="2">+ SC</cell><cell cols="4">28.85 20.72 35.79 (1.36 ) 29.37 (1.35 )</cell></row><row><cell>Decomposed-QA</cell><cell>-</cell><cell cols="3">36.57 30.14 38.79 (1.66)</cell><cell>35.57 (0.83)</cell></row><row><cell>+ SC</cell><cell>question-level sample-level</cell><cell cols="3">33.46 32.15 39.57 (1.50) 26.98 31.92 39.15 (0.76)</cell><cell>31.98 (0.31) 34.38 (0.85)</cell></row><row><cell></cell><cell cols="4">Word segmentation 38.16 30.38 37.67 (1.22)</cell><cell>-</cell></row><row><cell></cell><cell>Noun phrases</cell><cell cols="3">37.46 30.02 38.83 (1.24)</cell><cell>34.63 (0.78)</cell></row><row><cell>Front</cell><cell>POS tag</cell><cell cols="3">36.89 30.60 37.94 (1.49)</cell><cell>34.28 (0.45)</cell></row><row><cell></cell><cell>Constituency tree</cell><cell cols="3">36.21 29.88 38.43 (0.84)</cell><cell>34.47 (0.77)</cell></row><row><cell>Syn.</cell><cell>Dependency tree</cell><cell cols="3">36.33 29.82 36.85 (1.16)</cell><cell>35.77 (0.45)</cell></row><row><cell></cell><cell cols="4">Word segmentation 34.89 25.87 39.16 (1.52)</cell><cell>-</cell></row><row><cell></cell><cell>Noun phrases</cell><cell cols="3">32.59 24.32 39.52 (0.82)</cell><cell>29.78 (0.64)</cell></row><row><cell>Back</cell><cell>POS tag</cell><cell cols="3">36.18 26.11 37.00 (2.41)</cell><cell>29.72 (2.06)</cell></row><row><cell></cell><cell>on_conj</cell><cell cols="3">35.71 23.93 40.53 (2.54)</cell><cell>22.23 (0.40)</cell></row><row><cell></cell><cell>Dependency tree</cell><cell cols="3">31.05 21.02 39.06 (2.88)</cell><cell>26.65 (0.78)</cell></row><row><cell></cell><cell cols="4">Word segmentation 38.64 32.32 39.23 (1.13)</cell><cell>-</cell></row><row><cell></cell><cell>Noun phrases</cell><cell cols="3">38.16 32.11 40.34 (1.30)</cell><cell>32.35 (1.18)</cell></row><row><cell>Front</cell><cell>POS tag</cell><cell cols="3">38.06 31.75 38.71 (1.91)</cell><cell>33.02 (1.11)</cell></row><row><cell></cell><cell>Constituency tree</cell><cell cols="3">37.24 31.60 38.99 (1.52)</cell><cell>32.00 (0.42)</cell></row><row><cell>Syn. + SC</cell><cell>Dependency tree</cell><cell cols="3">37.65 31.30 37.17 (2.21)</cell><cell>34.59 (0.14)</cell></row><row><cell></cell><cell cols="4">Word segmentation 38.43 30.81 40.23 (2.59)</cell><cell>-</cell></row><row><cell></cell><cell>Noun phrases</cell><cell cols="3">38.73 29.19 39.79 (2.24)</cell><cell>34.92 (0.72)</cell></row><row><cell>Back</cell><cell>POS tag</cell><cell cols="3">38.48 30.77 40.27 (1.37)</cell><cell>34.40 (1.93)</cell></row><row><cell></cell><cell>Constituency tree</cell><cell cols="3">38.02 31.31 39.84 (1.90)</cell><cell>33.95 (0.90)</cell></row><row><cell></cell><cell>Dependency tree</cell><cell cols="3">37.24 31.20 40.15 (1.94)</cell><cell>34.42 (0.37)</cell></row><row><cell></cell><cell cols="4">Word segmentation 39.77 33.81 40.78 (2.58)</cell><cell>-</cell></row><row><cell>Tool.</cell><cell>POS tag Constituency tree</cell><cell cols="3">38.11 30.97 38.15 (2.82) 36.51 30.25 38.54 (3.19)</cell><cell>35.35 (0.34) 34.54 (2.26)</cell></row><row><cell></cell><cell>Dependency tree</cell><cell cols="3">39.50 32.12 38.13 (3.04)</cell><cell>34.34 (0.52)</cell></row><row><cell></cell><cell cols="4">Word segmentation 39.63 33.97 41.84 (2.63)</cell><cell>-</cell></row><row><cell>Tool. + SC</cell><cell>POS tag Constituency tree</cell><cell cols="3">37.92 31.72 38.96 (4.21) 36.59 28.35 40.40 (3.98)</cell><cell>33.42 (0.64) 34.60 (0.21)</cell></row><row><cell></cell><cell>Dependency tree</cell><cell cols="3">40.86 33.59 38.82 (2.61)</cell><cell>30.69 (0.97)</cell></row><row><cell></cell><cell cols="4">Word segmentation 39.67 32.97 41.09 (3.19)</cell><cell>-</cell></row><row><cell></cell><cell>POS tag</cell><cell cols="3">38.85 31.82 39.69 (3.98)</cell><cell>36.78 (1.36)</cell></row><row><cell>Front</cell><cell>Constituency tree</cell><cell cols="3">36.02 30.65 39.44 (2.92)</cell><cell>33.51 (3.04)</cell></row><row><cell>Tool. + Syn.</cell><cell>Dependency tree</cell><cell cols="3">37.16 32.06 38.83 (3.29)</cell><cell>34.09 (0.78)</cell></row><row><cell></cell><cell cols="4">Word segmentation 36.24 31.46 39.68 (1.15)</cell><cell>-</cell></row><row><cell></cell><cell>POS tag</cell><cell cols="3">34.71 26.51 36.62 (1.05)</cell><cell>35.70 (1.17)</cell></row><row><cell>Back</cell><cell>Constituency tree</cell><cell cols="3">33.76 29.53 39.67 (1.55)</cell><cell>29.64 (2.95)</cell></row><row><cell></cell><cell>Dependency tree</cell><cell cols="3">33.18 27.73 36.85 (0.43)</cell><cell>29.19 (2.17)</cell></row><row><cell></cell><cell cols="4">Word segmentation 40.31 34.85 42.46 (2.20)</cell><cell>-</cell></row><row><cell></cell><cell>POS tag</cell><cell cols="3">38.21 30.89 40.86 (2.48)</cell><cell>33.19 (1.39)</cell></row><row><cell>Front</cell><cell>Constituency tree</cell><cell cols="3">35.76 29.00 41.36 (3.58)</cell><cell>33.42 (2.35)</cell></row><row><cell>Tool. + Syn. + SC</cell><cell>Dependency tree</cell><cell cols="3">39.97 33.23 40.49 (3.49)</cell><cell>30.29 (0.71)</cell></row><row><cell></cell><cell cols="4">Word segmentation 40.83 30.78 41.40 (2.81)</cell><cell>-</cell></row><row><cell></cell><cell>POS tag</cell><cell cols="3">38.00 30.64 38.58 (2.77)</cell><cell>30.28 (2.21)</cell></row><row><cell>Back</cell><cell>Constituency tree</cell><cell cols="3">36.26 26.36 40.53 (3.38)</cell><cell>29.78 (1.64)</cell></row><row><cell></cell><cell>Dependency tree</cell><cell cols="3">41.97 32.73 40.19 (2.13)</cell><cell>29.87 (0.17)</cell></row><row><cell cols="2">SOTA (fully-supervised)</cell><cell cols="2">68.54 70.41</cell><cell>84.47</cell><cell>90.90</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Table 2 summarizes the performance of SC and the combinations of reasoning techniques. We visualize the results on PowerPlantFlat and Ontonotes 4</figDesc><table><row><cell>Vanilla</cell><cell>QA</cell><cell>Syn. Method</cell><cell>Tool. Tool. + Syn.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell cols="2">Error Types</cell><cell cols="3">Vanilla QA TS-SC</cell></row><row><cell>Type</cell><cell>OOD types</cell><cell>4</cell><cell>1</cell><cell>1</cell></row><row><cell></cell><cell>Wrong types</cell><cell>141</cell><cell>150</cell><cell>156</cell></row><row><cell cols="2">Boundary Cotain gold.</cell><cell>70</cell><cell>54</cell><cell>35</cell></row><row><cell></cell><cell>Cotained by gold.</cell><cell>9</cell><cell>27</cell><cell>24</cell></row><row><cell></cell><cell>Overlap with gold.</cell><cell>0</cell><cell>1</cell><cell>0</cell></row><row><cell cols="2">Completely-O</cell><cell>334</cell><cell>220</cell><cell>176</cell></row><row><cell cols="2">Omitted mentions</cell><cell>23</cell><cell>41</cell><cell>43</cell></row><row><cell cols="2">OOD mentions</cell><cell>3</cell><cell>36</cell><cell>10</cell></row><row><cell>Total</cell><cell></cell><cell>585</cell><cell>530</cell><cell>444</cell></row></table><note><p>Numbers of error types on Ontonotes 4. "QA" for decomposed-QA, "TS-SC" for combinations of tool augmentation, syntactic prompting, and SC. Numbers in bold denote the best results, i.e., the least errors. The proposed methods significantly reduce the total amount of error.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Tool. w. word segmentation (Ours) 40.78 (2.58) 42.48 (3.34) 47.16 (5.42) 54.40 (2.68) Syn. w. word segmentation (Ours) 37.94 (1.49) 43.89 (3.67) 50.70 (7.26) 56.71 (3.70)</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>0-shot</cell><cell>3-shot</cell><cell>5-shot</cell><cell>10-shot</cell></row><row><cell></cell><cell>Vanilla</cell><cell cols="4">35.16 (1.57) 38.67 (3.57) 44.51 (5.78) 52.45 (4.13)</cell></row><row><cell>Ontonotes 4</cell><cell>Standard CoT</cell><cell>-</cell><cell cols="3">34.34 (6.61) 41.13 (6.31) 41.90 (2.43)</cell></row><row><cell></cell><cell>Vanilla</cell><cell>27.85</cell><cell cols="3">35.81 (2.94) 37.44 (3.88) 41.13 (4.89)</cell></row><row><cell>PowerPlantFlat</cell><cell>Standard CoT Tool. w. word segmentation (Ours)</cell><cell>-32.41</cell><cell cols="3">30.63 (6.45) 33.95 (3.59) 38.02 (1.03) 39.43 (1.91) 41.12 (4.35) 42.05 (4.74)</cell></row><row><cell></cell><cell>Syn. w. word segmentation (Ours)</cell><cell>28.09</cell><cell cols="3">37.84 (2.59) 39.72 (2.79) 42.52 (3.71)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Results under few-shot setting, where the number of shots is the number of texts. We randomly sample three sets of demonstrations and take the averages. Results for Ontonotes 4 are averaged over three sets of randomly sampled 300 samples from the test set. We report F1 values. Numbers in parentheses are the standard deviations. Numbers in bold are the best results. Our methods also achieve significant improvements in few-shot scenarios.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>ACE05</cell><cell></cell><cell></cell><cell>BC5CDR</cell><cell></cell></row><row><cell>Model</cell><cell cols="6">GPT-3.5 GPT-3 Llama2 GPT-3.5 GPT-3 Llama2</cell></row><row><cell>Vanilla</cell><cell>29.45</cell><cell>14.03</cell><cell>9.07</cell><cell>61.28</cell><cell>29.49</cell><cell>26.12</cell></row><row><cell>Decomposed-QA</cell><cell>35.57</cell><cell>23.88</cell><cell>15.53</cell><cell>65.45</cell><cell>38.73</cell><cell>28.30</cell></row><row><cell>Syn. w. dependency tree</cell><cell>26.65</cell><cell>27.93</cell><cell>16.98</cell><cell>59.69</cell><cell>41.62</cell><cell>34.46</cell></row><row><cell>Tool. w. dependency tree</cell><cell>34.34</cell><cell>27.59</cell><cell>17.31</cell><cell>62.79</cell><cell>43.69</cell><cell>39.94</cell></row><row><cell>Tool. + Syn. w. dependency tree</cell><cell>29.19</cell><cell>18.38</cell><cell>26.99</cell><cell>57.28</cell><cell>16.38</cell><cell>39.57</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Performance on and Llama2 13B chat model. Results are averaged over three sets of randomly sampled 300 samples from the test set. We report the F1 values. Our proposed strategies show consistent improvements on various LLMs.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Table 6 shows the overall statistics of the Power-Plant datasets, and Table 8 displays the classwise statistics. Statistics of PowerPlant datasets.</figDesc><table><row><cell cols="4">Dataset Split #Sentences #Entities</cell></row><row><cell>Flat</cell><cell>Train Test</cell><cell>3087 401</cell><cell>4379 540</cell></row><row><cell>Nested</cell><cell>Train Test</cell><cell>3047 492</cell><cell>6924 1109</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Table7summarizes the statistics of error types on PowerPlantFlat under vanilla, decomposed-QA, and TS-SC methods. Fig.7shows the percentages of error types on PowerPlantFlat under vanilla method. Numbers of different error types on Power-PlantFlat. "QA" refers to decomposed-QA, "TS-SC" refers to the combination of tool augmentation, syntactic prompting, and SC. Numbers in bold denote the best results on PowerPlantFlat, i.e., the least errors.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Omitted mention</cell></row><row><cell>Wrong types</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Error Types</cell><cell></cell><cell cols="2">Vanilla QA TS-SC</cell><cell>OOD mentions</cell></row><row><cell></cell><cell>Type</cell><cell cols="2">OOD types</cell><cell></cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>OOD types</cell><cell></cell><cell cols="2">Wrong types</cell><cell></cell><cell>90</cell><cell>66</cell><cell>65</cell></row><row><cell></cell><cell cols="3">Boundary Cotain gold.</cell><cell></cell><cell>153</cell><cell>167</cell><cell>110</cell></row><row><cell>Cotained by gold.</cell><cell></cell><cell cols="3">Cotained by gold. Overlap with gold.</cell><cell>31 8</cell><cell>51 8</cell><cell>76 3</cell></row><row><cell></cell><cell cols="2">Completely-O</cell><cell></cell><cell></cell><cell>439</cell><cell>406</cell><cell>279</cell></row><row><cell></cell><cell cols="2">Omitted mentions</cell><cell></cell><cell></cell><cell>65</cell><cell>37</cell><cell>56</cell></row><row><cell></cell><cell cols="2">OOD mentions</cell><cell></cell><cell></cell><cell>14</cell><cell>32</cell><cell>14</cell></row><row><cell></cell><cell>Total</cell><cell></cell><cell></cell><cell></cell><cell>800</cell><cell>767</cell><cell>603</cell></row><row><cell></cell><cell>3.90% 0.00%</cell><cell>11.29%</cell><cell>8.09%</cell><cell>1.80%</cell></row><row><cell>Cotain gold.</cell><cell>19.08% 1.00%</cell><cell></cell><cell></cell><cell>54.85%</cell><cell>Error Types Wrong types OOD types Cotained by gold. Cotain gold. Overlap with gold. Completely -O OOD mentions Omitted mention</cell></row><row><cell>Overlap with gold.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 16 ,</head><label>16</label><figDesc>17, 18 and 19.    </figDesc><table><row><cell>Label</cell><cell>Chinese Label</cell><cell>Flat</cell><cell></cell><cell cols="2">Nested</cell></row><row><cell></cell><cell></cell><cell cols="4">Train Test Train Test</cell></row><row><cell>System name</cell><cell>????</cell><cell>132</cell><cell>10</cell><cell>143</cell><cell>21</cell></row><row><cell>System identity</cell><cell>????</cell><cell>357</cell><cell>49</cell><cell cols="2">1654 270</cell></row><row><cell>Device name</cell><cell>????</cell><cell cols="4">1239 159 1191 199</cell></row><row><cell>Device identity</cell><cell>????</cell><cell cols="4">1517 185 1462 243</cell></row><row><cell cols="2">Component name ????</cell><cell>763</cell><cell>97</cell><cell>771</cell><cell>108</cell></row><row><cell>Location name</cell><cell>??</cell><cell>200</cell><cell>24</cell><cell>197</cell><cell>30</cell></row><row><cell>Person</cell><cell>??</cell><cell>171</cell><cell>16</cell><cell>184</cell><cell>24</cell></row><row><cell>Reactor Status</cell><cell>?????</cell><cell>-</cell><cell>-</cell><cell>88</cell><cell>13</cell></row><row><cell cols="2">Power Plant Event ????</cell><cell>-</cell><cell>-</cell><cell cols="2">1234 201</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8 :</head><label>8</label><figDesc>Classwise statistics of PowerPlant datasets.</figDesc><table><row><cell>Method</cell><cell></cell><cell>PPF</cell><cell cols="6">PPN Weibo MSRA Ontonotes 4 ACE04 ACE05</cell></row><row><cell>Vanilla</cell><cell></cell><cell cols="2">27.85 20.43</cell><cell>30.09</cell><cell>45.51</cell><cell>33.74</cell><cell>20.09</cell><cell>28.12</cell></row><row><cell cols="2">Self-Consistency</cell><cell cols="2">28.85 20.72</cell><cell>31.02</cell><cell>-</cell><cell>-</cell><cell>19.97</cell><cell>28.21</cell></row><row><cell cols="2">Syntactic Prompting</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Word segmentation 28.09 20.37</cell><cell>28.48</cell><cell>41.72</cell><cell>30.82</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Noun phrases</cell><cell cols="2">28.94 21.81</cell><cell>28.89</cell><cell>41.5</cell><cell>30.89</cell><cell>18.77</cell><cell>26.21</cell></row><row><cell>Front</cell><cell>POS tags</cell><cell cols="2">30.12 22.47</cell><cell>27.23</cell><cell>41.20</cell><cell>30.59</cell><cell>19.54</cell><cell>28.27</cell></row><row><cell></cell><cell>Constituency Tree</cell><cell cols="2">26.38 20.47</cell><cell>28.23</cell><cell>40.62</cell><cell>30.61</cell><cell>19.75</cell><cell>28.49</cell></row><row><cell></cell><cell>Dependency Tree</cell><cell>27.21</cell><cell>20.7</cell><cell>28.51</cell><cell>40.44</cell><cell>30.77</cell><cell>19.68</cell><cell>28.46</cell></row><row><cell></cell><cell cols="3">Word segmentation 27.37 20.58</cell><cell>20.10</cell><cell>42.92</cell><cell>32.03</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Noun phrases</cell><cell cols="2">31.65 21.36</cell><cell>17.09</cell><cell>42.60</cell><cell>31.62</cell><cell>19.69</cell><cell>26.05</cell></row><row><cell>Back</cell><cell>POS tags</cell><cell cols="2">28.24 17.70</cell><cell>19.38</cell><cell>42.34</cell><cell>31.86</cell><cell>20.60</cell><cell>25.65</cell></row><row><cell></cell><cell>Constituency Tree</cell><cell cols="2">30.31 20.53</cell><cell>17.74</cell><cell>42.52</cell><cell>31.88</cell><cell>20.42</cell><cell>23.98</cell></row><row><cell></cell><cell>Dependency Tree</cell><cell cols="2">26.08 17.47</cell><cell>14.09</cell><cell>42.68</cell><cell>31.64</cell><cell>20.33</cell><cell>26.31</cell></row><row><cell cols="2">Tool augmentation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Word segmentation 32.28 26.57</cell><cell>25.37</cell><cell>38.84</cell><cell>29.75</cell><cell>-</cell><cell>-</cell></row><row><cell>Front</cell><cell>POS tags Constituency Tree</cell><cell cols="2">28.13 24.17 23.62 20.97</cell><cell>24.86 22.98</cell><cell>37.29 30.45</cell><cell>29.95 26.1</cell><cell>19.14 16.97</cell><cell>28.14 27.65</cell></row><row><cell></cell><cell>Dependency Tree</cell><cell cols="2">26.08 17.47</cell><cell>14.09</cell><cell>42.68</cell><cell>31.64</cell><cell>20.33</cell><cell>26.31</cell></row><row><cell></cell><cell cols="3">Word segmentation 28.57 26.81</cell><cell>21.88</cell><cell>34.19</cell><cell>26.77</cell><cell>-</cell><cell>-</cell></row><row><cell>Back</cell><cell>POS tags Constituency Tree</cell><cell cols="2">22.04 202.5 22.46 21.82</cell><cell>24.69 20.79</cell><cell>34.77 30.81</cell><cell>27.84 24.62</cell><cell>18.16 16.09</cell><cell>25.17 23.60</cell></row><row><cell></cell><cell>Dependency Tree</cell><cell cols="2">21.36 20.25</cell><cell>25.18</cell><cell>32.73</cell><cell>26.88</cell><cell>16.13</cell><cell>21.67</cell></row><row><cell cols="2">SOTA (fully-supervised)</cell><cell cols="2">68.54 70.41</cell><cell>72.77</cell><cell>96.72</cell><cell>84.47</cell><cell>90.3</cell><cell>90.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 9 :</head><label>9</label><figDesc>Performance of reasoning techniques under the vanilla setting (without decomposition). In this table, "vanilla" specifically refers to the zero-shot method without any techniques. We report the F1 values on entire test sets. We spare the SC on MSRA and Ontonotes 4 for cost saving.</figDesc><table><row><cell cols="2">Method</cell><cell cols="2">Ontonotes 4 PowerPlantFlat</cell></row><row><cell cols="2">Vanilla</cell><cell>38.71 (3.34)</cell><cell>35.81 (2.94)</cell></row><row><cell cols="2">Decomposed-QA</cell><cell>43.30 (1.84)</cell><cell>43.75 (3.06)</cell></row><row><cell></cell><cell>Word segmentation</cell><cell>40.12 (3.22)</cell><cell>37.33 (2.70)</cell></row><row><cell>Syn.</cell><cell>POS tag Constituency tree</cell><cell>44.11 (3.52) 38.81 (2.38)</cell><cell>36.42 (0.26) 33.66 (2.50)</cell></row><row><cell></cell><cell>Dependency tree</cell><cell>35.05 (1.41)</cell><cell>36.16 (1.43)</cell></row><row><cell></cell><cell>Word segmentation</cell><cell>41.28 (3.65)</cell><cell>38.7 (2.27)</cell></row><row><cell>Syn. + SC</cell><cell>POS tag Constituency tree</cell><cell>44.93 (4.02) 41.89 (3.50)</cell><cell>36.19 (0.65) 36.05 (2.65)</cell></row><row><cell></cell><cell>Dependency tree</cell><cell>37.98 (2.56)</cell><cell>39.06 (0.78)</cell></row><row><cell></cell><cell>Word segmentation</cell><cell>42.33 (3.14)</cell><cell>39.43 (1.91)</cell></row><row><cell>Tool.</cell><cell>POS tag Constituency tree</cell><cell>42.51 (2.44) 38.51 (4.17)</cell><cell>37.04 (0.72) 35.14 (2.84)</cell></row><row><cell></cell><cell>Dependency tree</cell><cell>36.12 (1.93)</cell><cell>33.54 (1.44)</cell></row><row><cell></cell><cell cols="2">Word segmentation 40.49 (12.05)</cell><cell>41.09 (2.71)</cell></row><row><cell>Tool. + SC</cell><cell>POS tag Constituency tree</cell><cell>43.28 (2.69) 40.12 (4.31)</cell><cell>38.66 (2.05) 35.54 (2.66)</cell></row><row><cell></cell><cell>Dependency tree</cell><cell>38.39 (2.50)</cell><cell>35.79 (1.87)</cell></row><row><cell></cell><cell>Word segmentation</cell><cell>43.11 (2.52)</cell><cell>39.69 (2.61)</cell></row><row><cell>Tool. + Syn.</cell><cell>POS tag Constituency tree</cell><cell>42.44 (2.67) 38.31 (3.30)</cell><cell>36.35 (1.81) 34.45 (2.53)</cell></row><row><cell></cell><cell>Dependency tree</cell><cell>35.21 (2.26)</cell><cell>34.1 (1.06)</cell></row><row><cell></cell><cell>Word segmentation</cell><cell>42.18 (2.29)</cell><cell>41.43 (1.78)</cell></row><row><cell>Tool. + Syn. + SC</cell><cell>POS tag Constituency tree</cell><cell>41.53 (3.25) 41.57 (4.54)</cell><cell>37.95 (2.03) 35.32 (3.82)</cell></row><row><cell></cell><cell>Dependency tree</cell><cell>40.33 (4.87)</cell><cell>35.85 (2.14)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 10 :</head><label>10</label><figDesc>Performance of syntactic augmentation under 3-shot setting. We randomly sample three sets of demonstrations and report the means and standard deviations of F1 values. Numbers in parentheses are standard deviations. Numbers in bold are the best results in each category. The proposed syntactic augmentation exhibits significant improvements in the few-setting.</figDesc><table><row><cell>Dataset</cell><cell>CoNLL-2003</cell><cell>WNUT-17</cell><cell>BC5CDR</cell><cell>BioNLP11</cell><cell>CRAFT</cell></row><row><cell>Vanilla</cell><cell>69.42 (0.91)</cell><cell cols="4">46.61 (2.97) 61.28 (3.11) 51.29 (2.48) 21.66 (1.41)</cell></row><row><cell>Decomposed-QA</cell><cell>59.67 (0.36)</cell><cell cols="2">42.39 (1.99) 65.45 (0.89)</cell><cell>55.3 (0.54)</cell><cell>23.99 (2.65)</cell></row><row><cell>Syn. (Front)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Noun phrases</cell><cell>57.13 (0.41)</cell><cell cols="4">39.75 (2.42) 64.41 (1.74) 52.73 (1.89) 22.92 (3.04)</cell></row><row><cell>POS tag</cell><cell>55.14 (0.98)</cell><cell cols="4">39.74 (1.98) 66.24 (2.40) 53.97 (0.99) 23.59 (2.01)</cell></row><row><cell>Constituency tree</cell><cell>56.36 (1.07)</cell><cell>39.66 (1.51)</cell><cell>64.7 (0.80)</cell><cell cols="2">53.98 (1.24) 23.84 (1.94)</cell></row><row><cell>Dependency tree</cell><cell>54.27 (1.54)</cell><cell cols="4">38.36 (0.37) 65.56 (1.20) 54.37 (0.83) 23.51 (2.06)</cell></row><row><cell>Syn. (Back)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Noun phrases</cell><cell>58.89 (1.39)</cell><cell cols="4">36.47 (1.10) 60.44 (0.57) 51.99 (1.27) 21.71 (1.89)</cell></row><row><cell>POS tag</cell><cell>56.12 (1.54)</cell><cell cols="4">38.12 (0.55) 58.19 (1.55) 54.41 (2.62) 22.69 (3.76)</cell></row><row><cell>Constituency tree</cell><cell>55.66 (2.17)</cell><cell cols="4">37.75 (2.04) 47.81 (1.99) 43.58 (1.44) 23.86 (1.30)</cell></row><row><cell>Dependency tree</cell><cell>58.36 (1.11)</cell><cell cols="4">37.49 (1.56) 59.69 (0.76) 55.23 (0.77) 22.84 (1.59)</cell></row><row><cell>Tool.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>POS tag</cell><cell>62.79 (2.54)</cell><cell>43.81 (2.15)</cell><cell>66.4 (1.44)</cell><cell cols="2">52.38 (0.35) 24.54 (3.87)</cell></row><row><cell>Constituency tree</cell><cell>60.96 (0.34)</cell><cell>44.3 (1.02)</cell><cell cols="3">65.02 (3.00) 51.44 (0.81) 23.88 (2.24)</cell></row><row><cell>Dependency tree</cell><cell>59.23 (3.13)</cell><cell>41.6 (1.30)</cell><cell cols="3">62.79 (2.62) 42.71 (0.73) 24.86 (2.45)</cell></row><row><cell>Tool. + Syn. (Front)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>POS tag</cell><cell>63.46 (1.02)</cell><cell>43.9 (2.21)</cell><cell cols="3">64.24 (1.83) 49.87 (0.85) 25.05 (2.12)</cell></row><row><cell>Constituency tree</cell><cell>59.59 (0.99)</cell><cell cols="2">44.68 (2.59) 65.43 (2.51)</cell><cell>50.6 (1.39)</cell><cell>24.5 (2.76)</cell></row><row><cell>Dependency tree</cell><cell>57.93 (2.05)</cell><cell cols="4">40.96 (3.57) 59.93 (1.88) 40.18 (1.63) 24.46 (2.50)</cell></row><row><cell>Tool. + Syn. (Back)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>POS tag</cell><cell>58.08 (0.27)</cell><cell cols="2">39.32 (4.42) 60.84 (3.47)</cell><cell>47.9 (1.88)</cell><cell>13.71 (1.90)</cell></row><row><cell>Constituency tree</cell><cell>58.95 (2.19)</cell><cell cols="4">37.91 (5.46) 54.57 (1.86) 42.27 (2.78) 20.64 (1.85)</cell></row><row><cell>Dependency tree</cell><cell>54.68 (1.27)</cell><cell>36.7 (3.07)</cell><cell cols="3">57.28 (1.37) 45.62 (3.46) 24.05 (4.05)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 11 :</head><label>11</label><figDesc>Performance on additional datasets. Results are averaged over three sets of randomly sampled 300 samples from the test set. We report the means and standard deviations of F1 values. Numbers in parentheses are standard deviations. Numbers in bold are best results in each category. .69) 14.03 (0.94) 9.07 (1.33) 61.28 (3.11) 29.49 (3.12) 26.12 (2.94) Decomposed-QA 35.57 (0.83) 23.88 (2.21) 15.53 (1.53) 65.45(0.89) 38.73 (2.58) 28.30 (0.73) Syn. (Front) Noun phrases 34.63 (0.78) 21.74 (2.16) 17.30 (1.08) 64.41 (1.74) 39.38 (3.01) 36.20 (1.24) POS tag 34.28 (0.45) 21.98 (2.90) 23.86 (6.79) 66.24 (2.40) 45.31 (1.34) 34.65 (2.35) Constituency tree 34.47 (0.77) 22.38 (2.09) 21.76 (0.31) 64.70 (0.80) 42.09 (1.51) 36.92 (0.37) Dependency tree 35.77 (0.45) 22.84 (2.81) 25.91 (1.20) 65.56 (1.20) 43.19 (0.93) 33.11 (0.31) Syn. (Back) Noun phrases 29.78 (0.64) 24.45 (2.29) 15.73 (1.93) 60.44 (0.57) 35.17 (1.88) 33.75 (1.26) POS tag 29.72 (2.06) 30.73 (2.90) 16.51 (1.82) 58.19 (1.55) 45.17 (3.00) 34.62 (3.02) Constituency tree 22.23 (0.40) 27.08 (2.82) 16.45 (1.74) 47.81 (1.99) 39.72 (1.96) 35.17 (0.88) Dependency tree 26.65 (0.78) 27.93 (2.73) 16.98 (1.23) 59.69 (0.76) 41.62 (2.30) 34.46 (1.81) Tool. POS tag 35.35 (0.34) 24.74 (0.88) 18.00 (1.02) 66.40 (1.44) 47.04 (2.39) 40.45 (1.11)</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>ACE05</cell><cell></cell><cell></cell><cell>BC5CDR</cell><cell></cell></row><row><cell>Model</cell><cell>GPT-3.5</cell><cell>GPT-3</cell><cell>Llama2</cell><cell>GPT-3.5</cell><cell>GPT-3</cell><cell>Llama2</cell></row><row><cell cols="7">Vanilla 29.45 (0Constituency tree 34.54 (2.26) 26.84 (2.46) 17.36 (0.65) 65.02 (3.00) 52.77 (2.73) 38.95 (0.85)</cell></row><row><cell>Dependency tree</cell><cell cols="6">34.34 (0.52) 27.59 (2.05) 17.31 (2.14) 62.79 (2.62) 43.69 (2.33) 39.94 (1.05)</cell></row><row><cell>Tool. + Syn. (Front)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>POS tag</cell><cell cols="6">36.78 (1.36) 26.21 (1.88) 17.94 (1.28) 64.24 (1.83) 47.70 (2.46) 33.84 (1.63)</cell></row><row><cell>Constituency tree</cell><cell cols="6">33.51 (3.04) 29.93 (2.03) 18.11 (1.42) 65.43 (2.51) 54.12 (3.00) 30.23 (1.65)</cell></row><row><cell>Dependency tree</cell><cell cols="6">34.09 (0.78) 30.62 (1.79) 15.75 (2.38) 59.93 (1.88) 43.26 (2.11) 38.16 (4.50)</cell></row><row><cell>Tool. + Syn. (Back)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>POS tag</cell><cell cols="6">35.70 (1.17) 22.08 (1.32) 24.50 (2.09) 60.84 (3.47) 17.72 (1.87) 43.63 (2.61)</cell></row><row><cell>Constituency tree</cell><cell cols="6">29.64 (2.95) 15.06 (0.23) 23.87 (1.18) 54.57 (1.86) 11.44 (1.72) 36.48 (1.91)</cell></row><row><cell>Dependency tree</cell><cell cols="6">29.19 (2.17) 18.38 (1.10) 26.99 (0.49) 57.28 (1.37) 16.38 (1.27) 39.57 (2.04)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 12 :</head><label>12</label><figDesc>Complete results on various LLMs. We use gpt-3.5-turbo for GPT-3.5, text-davinci-003 for GPT-3, and 13B chat model for Llama2. For cost saving, we sample 300 samples from the test set three times, and report the average results of F1 values. Numbers in parentheses are the standard deviations. Numbers in bold are the best results in the corresponding categories.</figDesc><table><row><cell>Dataset</cell><cell>Label order</cell><cell>Order generation</cell><cell>F1</cell></row><row><cell></cell><cell>vanilla</cell><cell>-</cell><cell>27.85</cell></row><row><cell>PowerPlantFlat</cell><cell>[["????"],["????"], ["????"], ["????"],["????"], ["??"],["??"]]</cell><cell>manual</cell><cell>36.57</cell></row><row><cell></cell><cell>[["??"],["????"], "????"], ["?? ??"], ["????"], ["????"], ["??"]]</cell><cell>ChatGPT</cell><cell>30.52</cell></row><row><cell></cell><cell>vanilla</cell><cell>-</cell><cell>20.43</cell></row><row><cell></cell><cell>[["????"],["????"], ["????"],</cell><cell></cell><cell></cell></row><row><cell>PowerPlantNested</cell><cell>["??"], ["?????"],["????"]] ["????"],["????"], ["??"],</cell><cell>manual</cell><cell>30.14</cell></row><row><cell></cell><cell></cell><cell>ChatGPT</cell><cell>20.16</cell></row><row><cell></cell><cell>vanilla</cell><cell>-</cell><cell>30.09</cell></row><row><cell>Weibo</cell><cell>[['??'], ['??'], ['????'], ['?? ????']]</cell><cell>ChatGPT</cell><cell>34.04</cell></row><row><cell>MSRA</cell><cell>vanilla</cell><cell>-</cell><cell>45.51</cell></row><row><cell></cell><cell>[['??'], ['??'], ['??']]</cell><cell>ChatGPT</cell><cell>48.60</cell></row><row><cell></cell><cell>vanilla</cell><cell>-</cell><cell>33.74</cell></row><row><cell>Ontonotes 4</cell><cell>[['??'], ['??'], ['????'], ['?? ????']]</cell><cell>ChatGPT</cell><cell>37.45</cell></row><row><cell></cell><cell>vanilla</cell><cell>-</cell><cell>20.09</cell></row><row><cell>ACE04</cell><cell>[["Person"],["Organization"],["Location"],</cell><cell></cell><cell></cell></row><row><cell></cell><cell>["Facility"], ["Weapon"],["Vehicle"], ["Geo</cell><cell>ChatGPT</cell><cell>22.19</cell></row><row><cell></cell><cell>-Political Entity"]]</cell><cell></cell><cell></cell></row><row><cell></cell><cell>vanilla</cell><cell>-</cell><cell>28.12</cell></row><row><cell>ACE05</cell><cell>[["Person"],["Organization"],["Location"],</cell><cell></cell><cell></cell></row><row><cell></cell><cell>["Facility"], ["Weapon"],["Vehicle"], ["Geo</cell><cell>ChatGPT</cell><cell>34.37</cell></row><row><cell></cell><cell>-Political Entity"]]</cell><cell></cell><cell></cell></row></table><note><p>[["??"], ["??"],["?????"], ["?? ??"], ["????"], ["????"], ["?? ??"], ["????"], ["????"]]</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 13 :</head><label>13</label><figDesc>Label orders with corresponding performances. The results are from the entire test set. "vanilla" refers to the standard setting without any techniques.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>We use Hanlp<ref type="bibr" target="#b7">(He and Choi, 2021)</ref> to generate syntactic information, since we found it performs well on both Chinese and English in our preliminary experiments.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>catalog.ldc.upenn.edu/LDC2006T06</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>catalog.ldc.upenn.edu/LDC2005T09</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>catalog.ldc.upenn.edu/LDC2011T03</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>The results of ChatGPT are obtained during May and June 2023 with official API.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>https://huggingface.co/meta-llama/ Llama-2-13b-chat-hf</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank the anonymous reviewers for their insightful comments and constructive suggestions. This research is supported by the <rs type="funder">National Key Research and Development Program of China</rs> (Grant No. <rs type="grantNumber">2020YFB1707803</rs>) and <rs type="funder">Zhejiang Provincial Natural Science Foundation of China</rs> (<rs type="grantNumber">LDT23F02023F02</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Ypk2eud">
					<idno type="grant-number">2020YFB1707803</idno>
				</org>
				<org type="funding" xml:id="_spqMgtu">
					<idno type="grant-number">LDT23F02023F02</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>https://github.com/Emma1066/ Zero-Shot-NER-with-ChatGPT</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Constituency parsing</head><p>First, you should perform constituency parsing. Then, you should recognize named entities based on the constituency tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dependency parsing</head><p>First, you should perform dependency parsing. Then, you should recognize named entities based on the dependency tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Syntactic reasoning hint (back)</head><p>Noun phrases First, let's recognize the noun phrases. Then, we recognize named entities based on the noun phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>POS tagging</head><p>First, let's perform Part-of-Speech tagging. Then, we recognize named entities based on the Part-of-Speech tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Constituency parsing</head><p>First, let's perform constituency parsing. Then, we recognize named entities based on the constituency tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dependency parsing</head><p>First, let's perform dependency parsing. Then, we recognize named entities based on the dependency tree. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>POS tagging</head><p>Given the text and the corresponding Part-of-Speech tags, please recognize the named entities in the given text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Constituency parsing</head><p>Given the text and the corresponding constituency tree, please recognize the named entities in the given text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dependency parsing</head><p>Given the text and the corresponding dependency tree, please recognize the named entities in the given text. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Constituency parsing</head><p>Please infer named entities step by step from the text based on the given constituency tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dependency parsing</head><p>Please infer named entities step by step from the text based on the given dependency tree.</p><p>{syntactic reasoning hint (back)}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>POS tagging</head><p>Let's infer named entities step by step from the text based on the given Part-of-Speech tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Constituency parsing</head><p>Let's infer named entities step by step from the text based on the given constituency tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dependency parsing</head><p>Let's infer named entities step by step from the text based on the given dependency tree. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Concept annotation in the craft corpus</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miriam</forename><surname>Eckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristin</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krista</forename><surname>Shipley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Sitnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">A</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bretonnel Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karin</forename><surname>Verspoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judith</forename><forename type="middle">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Matching the blanks: Distributional similarity for relation learning</title>
		<author>
			<persName><forename type="first">Baldini</forename><surname>Livio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><surname>Kwiatkowski</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1279</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2895" to="2905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Event extraction via dynamic multipooling convolutional neural networks</title>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1017</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Schuh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kensen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasha</forename><surname>Tsvyashchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Vinodkumar Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiner</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toju</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunipa</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeontaek</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Spiridonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivani</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><surname>Omernick ; Oleksandr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongwei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brennan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Saeta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName><surname>Eck</surname></persName>
		</author>
		<editor>M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child,</editor>
		<imprint>
			<pubPlace>Jeff Dean, Slav Petrov</pubPlace>
		</imprint>
	</monogr>
	<note>and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A neural network multi-task learning approach to biomedical named entity recognition</title>
		<author>
			<persName><forename type="first">Sampo</forename><surname>Gamal Crichton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Billy</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Results of the wnut2017 shared task on novel and emerging entity recognition</title>
		<author>
			<persName><forename type="first">Leon</forename><surname>Derczynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marieke</forename><surname>Van Erp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nut</forename><surname>Limsopatham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Noisy Usergenerated Text</title>
		<meeting>the 3rd Workshop on Noisy Usergenerated Text</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="140" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Is information extraction solved by chatgpt? an analysis of performance, evaluation criteria, robustness and errors</title>
		<author>
			<persName><forename type="first">Ridong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaohao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benyou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.14450</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The stem cell hypothesis: Dilemma behind multi-task learning with transformer encoders</title>
		<author>
			<persName><forename type="first">Han</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinho</forename><forename type="middle">D</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5555" to="5577" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Takeshi</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Shixiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Machel</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName><surname>Iwasawa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.11916</idno>
		<title level="m">Large language models are zero-shot reasoners</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Md</forename><surname>Tahmid Rahman Laskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saiful Bari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mizanur</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Amran Hossen Bhuiyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><forename type="middle">Xiangji</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.18486</idno>
		<title level="m">A systematic study and comprehensive evaluation of chatgpt on benchmark datasets</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Evaluating chatgpt&apos;s information extraction capabilities: An assessment of performance, explainability, calibration, and faithfulness</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gexiang</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quansen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.11633</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Biocreative v cdr task corpus: a resource for chemical disease relation extraction</title>
		<author>
			<persName><forename type="first">Jiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueping</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Sciaky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Hsuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><forename type="middle">Peter</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carolyn</forename><forename type="middle">J</forename><surname>Mattingly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">C</forename><surname>Wiegers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dice loss for dataimbalanced nlp tasks</title>
		<author>
			<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.45</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="465" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Prompting large language models with chain-of-thought for fewshot knowledge base question generation</title>
		<author>
			<persName><forename type="first">Yuanyuan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanlun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weining</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunshi</forename><surname>Lan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Star: Boosting low-resource event extraction by structure-to-text data generation with large language models</title>
		<author>
			<persName><forename type="first">Derek</forename><surname>Mingyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Nien</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Jeffrey Brantingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.15090</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Introducing chatgpt</title>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<title level="m">Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fraser</forename><surname>Kelton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maddie</forename><surname>Simens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Paul F Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27730" to="27744" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Named entity recognition for Chinese social media with jointly trained embeddings</title>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1064</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="548" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Overview of the id, epi and rel tasks of bionlp shared task 2011</title>
		<author>
			<persName><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoko</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Rak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunxia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Sobral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sophia</forename><surname>Jun'ichi Tsujii</surname></persName>
		</author>
		<author>
			<persName><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMC bioinformatics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Is chatgpt a general-purpose natural language processing task solver?</title>
		<author>
			<persName><forename type="first">Chengwei</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Erik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fien</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meulder</forename><surname>De</surname></persName>
		</author>
		<idno>arXiv preprint cs/0306050</idno>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Global pointer: Novel efficient span-based approach for named entity recognition</title>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Murtadha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfeng</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Romal</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alicia</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaguang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongrae</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaixiu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Steven Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcelo</forename><surname>Ghafouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Menegali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Ching</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Krivokon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Rusch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranesh</forename><surname>Pickett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laichee</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Man</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meredith</forename><forename type="middle">Ringel</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tulsee</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renelito Delos</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toju</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johnny</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Soraker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinodkumar</forename><surname>Zevenbergen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><surname>Olson</surname></persName>
		</author>
		<editor>Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak</editor>
		<imprint/>
	</monogr>
	<note>and Quoc Le. 2022. Lamda: Language models for dialog applications</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
	</analytic>
	<monogr>
		<title level="m">Open foundation and fine-tuned chat models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Gpt-re: In-context learning for relation extraction using large language models</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoyuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyue</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.02105</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">2022a. Towards understanding chain-of-thought prompting: An empirical study of what matters</title>
		<author>
			<persName><forename type="first">Boshi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">You</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.10001</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Shuhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongbin</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.10428</idno>
		<title level="m">Gpt-ner: Named entity recognition via large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving named entity recognition by external context retrieving and cooperative learning</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nguyen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.142</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1800" to="1812" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Self-consistency improves chain of thought reasoning in language models</title>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.11171</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11903</idno>
		<title level="m">Chain of thought prompting elicits reasoning in large language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.10205</idno>
		<title level="m">Zeroshot information extraction via chatting with chatgpt</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Named entity recognition as dependency parsing</title>
		<author>
			<persName><forename type="first">Juntao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.577</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6470" to="6476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Word segmentation and named entity recognition for SIGHAN bakeoff3</title>
		<author>
			<persName><forename type="first">Suxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Fifth SIGHAN Workshop on Chinese Language Processing<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="158" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Automatic chain of thought prompting in large language models</title>
		<author>
			<persName><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.03493</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A frustratingly easy approach for entity and relation extraction</title>
		<author>
			<persName><forename type="first">Zexuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.5</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="50" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Least-to-most prompting enables complex reasoning in large language models</title>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathanael</forename><surname>Sch?rli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Scales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2205.10625</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
