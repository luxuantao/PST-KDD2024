<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Functionality Matters in Netlist Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ziyi</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Cuhk</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Ho</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Cuhk</forename><surname>Bei</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Cuhk</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Huang</surname></persName>
						</author>
						<title level="a" type="main">Functionality Matters in Netlist Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3489517.3530602</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning feasible representation from raw gate-level netlists is essential for incorporating machine learning techniques in logic synthesis, physical design, or verification. Existing message-passingbased graph learning methodologies focus merely on graph topology while overlooking gate functionality, which often fails to capture underlying semantic, thus limiting their generalizability. To address the concern, we propose a novel netlist representation learning framework that utilizes a contrastive scheme to acquire generic functional knowledge from netlists effectively. We also propose a customized graph neural network (GNN) architecture that learns a set of independent aggregators to better cooperate with the above framework. Comprehensive experiments on multiple complex realworld designs demonstrate that our proposed solution significantly outperforms state-of-the-art netlist feature learning flows.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As machine learning (ML) techniques develop rapidly, there is a surge in incorporating ML in electronic design automation (EDA) <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. Most existing works follow a representation learning paradigm consisting of two steps: first, learn low-dimensional representations from the high-dimensional raw data and then conduct classification or regression based on the learned representations. The learned representations play a dominant role in improving model performance. In this work, we focus on representation learning for electronic circuit netlists. This is non-trivial as a netlist contains circuit components, which might vary largely in structures and connectivity. Therefore, it is worth designing subtle representation learning methodologies dedicated to netlists.</p><p>Early netlist representation construction methods focus on the structural information of netlists. Structural information mainly includes the topology of circuit components. A representative art <ref type="bibr" target="#b3">[4]</ref> developed a shape hashing technique to group wires with similar local topology into words, where a sequence representation named shape is proposed. Specifically, a target wire's shape is produced by serializing gate and wire types along depth-first-search (DFS) traversal of its fan-in cone. However, the generated representation is highly related to the traversing order, which is stochastic. As a consequence, wires with isomorphic fan-in structures may be pushed apart in the representation space. Besides, the information contained in the traversal sequence is relatively shallow, leading to insufficient understanding of the netlists.</p><p>In recent years, the fast-growing deep neural network techniques have shown great power in netlist representation learning. A compact representation termed level-dependent decaying sum (LDDS) existence vector (EV) is introduced in <ref type="bibr" target="#b4">[5]</ref> to embed a circuit node with its neighbors. A fixed number of EVs are selected to satisfy the fixed-input-size requirement of convolutional neural networks. Ma et al. <ref type="bibr" target="#b0">[1]</ref> propose an iterative process to insert observation points into gate-level netlists based on the node representations learned by a graph neural network. <ref type="bibr" target="#b5">[6]</ref> develop a graph learning-based solution to extract desired logic components from a flattened netlist, where a novel graph neural network customized for directed acyclic graph (DAG) is proposed to generate gate representations. The embeddings are then fed into a classifier to predict the boundaries of desired components. While generating more powerful representations, the above machine learning-driven methods can likewise be categorized as structural ones since they take merely topological information into consideration.</p><p>Though existing structure-based netlist representation learning methods have achieved state-of-the-art performance in many netlistlevel tasks, we argue that they are far from good enough. The main point is that these methods ignore the boolean functionality, which plays a dominant role in understanding the semantics of netlists. Figure <ref type="figure" target="#fig_0">1</ref> gives two examples to illustrate the drawbacks of existing structure-based netlist representation learning methods. Netlists A, B, and C are three different netlists. A and B implement the same function, sharing similar semantics, which means they should be close in the representation space. However, existing methods would push their representations apart since they are topologically disparate. On the other hand, A and C implement different functions, thus having different semantics, and are expected to be distant in the representation space. Nevertheless, their representations would be pulled together by existing methods based on their similar structures.</p><p>To address the above concerns, we develop a novel contrastive learning (CL)-based netlist representation learning framework to extract the basic logic functionality of netlists, which is universal and transferable across different designs. We further propose a customized graph neural network architecture to better cooperate with the above framework. The proposed framework aims at encoding functional information of netlists, independently of specific structural patterns, thus improving the capability of generalizing to unseen designs.</p><p>Our major contributions are summarized as follows:</p><p>• For the first time, to the best of our knowledge, we present a contrastive learning-based pre-training framework customized for gate-level circuits, extracting universal knowledge of logic functionality. • We design a novel GNN architecture for circuit representation learning that encodes the basic functional information of gatelevel netlists. • We conduct comprehensive experiments on several complex real-world designs, which confirms the effectiveness of our proposed framework compared with state-of-the-art netlist representation learning arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries 2.1 Graph Neural Network</head><p>Graph neural networks (GNNs) <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref> have emerged as a promising approach for analyzing graph-structured data in recent years. They follow an iterative neighborhood aggregation scheme to capture the structural information within nodes' neighborhoods. Let 𝐺 = ⟨V, E⟩ denotes a graph, where V = {𝑣 1 , 𝑣 2 , • • • , 𝑣 𝑛 } is the vertex set, and E ⊆ V × V is the edge set. Considering a K-layer GNN, the propagation of the 𝑘-th layer is represented as</p><formula xml:id="formula_0">𝒂 (𝑘) 𝑣 = AGGREGATE({𝒉 (𝑘−1) 𝑢 : 𝑢 ∈ N(𝑣)}), 𝒉 (𝑘) 𝑣 = COMBINE(𝒂 (𝑘) 𝑣 , 𝒉 (𝑘−1) 𝑣 ),<label>(1)</label></formula><p>where 𝒉 (𝑘) 𝑣 is the representation vector of vertex 𝑣 at the 𝑘-th layer. N(𝑣) denotes the neighboring nodes of 𝑣, and AGGREGATE is a function used to collect messages from a node's neighborhood. COMBINE is leveraged to combine the node's previous representation with its neighborhood message.</p><p>Various GNNs <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref> have been proposed, achieving state-ofthe-art performance in related graph learning tasks. Notably, there emerges growing interest in a unique graph type, directed acyclic graph (DAG) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. DAGs are widely applied to model many realworld data, including gate-level netlists. To generate better globallevel embeddings for DAGs, <ref type="bibr" target="#b12">[13]</ref> construct an impressive GNN architecture driven by the partial order induced by DAG. Besides, <ref type="bibr" target="#b13">[14]</ref> propose an asynchronous message passing scheme to encode computation graphs and further develop a variational autoencoder for DAGs, D-VAE. However, these methods can only be applied to handle structural information of DAGs while omitting the underlying semantics of target graphs, which is vital for generating better representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Contrastive Learning</head><p>Contrastive learning (CL) is employed as a new paradigm to pretrain the model to improve the modeling performance on downstream tasks. The main idea of contrastive learning is to capture statistical dependencies of interest and those that do not by separating positive samples from negative samples in the n-dimensional embedding space 𝑅 𝑛 as much as possible. The goal of CL is to learn an encoder 𝑓 : 𝑥 → 𝒆, 𝒆 ∈ R 𝑛 that for any sample 𝑥: 𝑠𝑐𝑜𝑟𝑒 (𝑓 (𝑥), 𝑓 (𝑥 + )) &gt;&gt; 𝑠𝑐𝑜𝑟𝑒 (𝑓 (𝑥), 𝑓 (𝑥 − )).</p><p>(</p><p>Here 𝑥 + refers to positive samples that are similar or equal to 𝑥, 𝑥 − refers to negative samples that are different from 𝑥, and 𝑠𝑐𝑜𝑟𝑒 (𝒆 1 , 𝒆 2 ) measures the similarity (distance) between embedding 𝒆 1 and 𝒆 2 .</p><p>Due to its outstanding performance, contrastive learning achieves great success in the computer vision domain <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. Theoretical analyses shed light on the reasons behind their success <ref type="bibr" target="#b16">[17]</ref>: objectives used in contrastive methods can be seen as maximizing a lower bound of mutual information between the input data and the output representations.</p><p>In recent years, many researchers have been focusing on extending the contrastive methods to handle graph data. DGI <ref type="bibr" target="#b17">[18]</ref> embeds high-order global contextual features into node representations by maximizing mutual information between global and local embeddings. You et al. <ref type="bibr" target="#b18">[19]</ref> build multiple views of a graph by incorporating several perturbations, e.g., edge dropping, node dropping, feature masking, etc. GCA <ref type="bibr" target="#b19">[20]</ref> is proposed to explore graph data augmentation.</p><p>However, most existing methods only take the structural information into account during the data augmentation procedures, leading to inadequate or even wrong understanding of the target graphs. We argue that structural information is not consistent with the semantics in many situations, e.g., two structurally different netlists may share the same function (e.g., ripple-carry adder and carry lookahead adder). Besides, even a small perturbation may totally change the semantics of a graph (e.g., replacing an XOR gate with an OR gate may change a circuit's function). Therefore, existing methods fail to guarantee the consistency between the augmented view and the original input since they randomly introduce structural perturbations (e.g., random edge/node dropping), which may change the underlying semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Formulation</head><p>We first introduce the gate-level netlist and then give the problem formulation. The gate-level netlist of an electric circuit consists of a list of gate-level circuit components, e.g., AND gates and interconnects (wires) between them. Gate-level netlists are generated by converting a description of circuit behavior at register transfer level (RTL) into design implementation in logic gates.</p><p>A gate-level netlist can be formulated as a DAG with vertices denoting circuit components and edges representing wires. Based on the gate-level netlist, our problem can be formulated.</p><p>Problem 1 (Netlist Representation Learning ). Design a novel learning methodology that automatically discovers gate/netlist representations capturing their boolean functional semantics. We hope the representation facilitates multiple downstream netlist tasks, covering: (1) local scenario, e.g., identifying desired components (viz., subnetlists) located in the netlist and (2) global scenario, e.g., classifying the netlist into one of the categories according to its functionality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Netlist Representation Learning Framework</head><p>Before introducing algorithmic details, we briefly overview our proposed netlist representation learning flow, as shown in Figure <ref type="figure" target="#fig_1">2</ref>. The whole flow can be summarized as three steps: Preprocessing: Firstly, the input gate-level netlist is transformed to a DAG. Each vertex corresponds to a gate in the netlist, and directed edges represent the interconnection between gates, where the source vertices denote the driven gates.</p><p>2. Pre-training: Secondly, functionality graph neural network (FGNN) (Section 4.2) is dedicated designed for learning functionality of gate-level netlists. The pre-training of FGNN is guided by the proposed gate-level netlist contrastive scheme (Section 4.1).</p><p>3. Fine-tuning: Finally, pre-trained FGNN is equipped with a classifier and fine-tuned to adapt downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Netlist Contrastive Scheme</head><p>To extract the prior knowledge of netlists' basic logic functionality, we develop a novel netlist contrastive learning (NCL) scheme, as shown in Figure <ref type="figure" target="#fig_3">4</ref>. Our proposed scheme follows a standard contrastive learning paradigm, where the model seeks to maximize the agreement of different views (constructed through data augmentation) of the same item. Researches have shown that the success of contrastive learning lies in the assumption that important information is shared between different views <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. It indicates that the semantics of augmented views should be maintained following the original input.</p><p>While generating augmented views without causing semantic changes is relatively natural for images (e.g., translation, scale) <ref type="bibr" target="#b15">[16]</ref>, it is not explicit for graph data. The primary difficulty here is that the semantics of graphs are not apparent under many situations, and sometimes even a tiny perturbation may completely alter its meaning. To resolve the above difficulty and extend the contrastive learning scheme to handle graphs, it is critical to explore a customized data augmentation scheme that introduces perturbations without changing the original semantics.</p><p>When it comes to a gate-level netlist, its semantics can be denoted by its function. Given a gate-level netlist, we can treat it as a computation graph and describe its function through a boolean expression, whose basic operators are the logic gates, as demonstrated in Figure <ref type="figure" target="#fig_2">3</ref>. Therefore, based on the observation of equivalent boolean transformation, we carefully design a data augmentation scheme to maximize the mutual information between functionally similar netlists. Specifically, for a given netlist 𝑐, we conduct the augmentation by replacing a randomly picked sub-netlist in 𝑐 with equivalent Boolean transformations (described in Figure <ref type="figure" target="#fig_3">4</ref>), resulting in a functionally constant but topologically different netlist 𝑐 ′ . This procedure guarantees the semantic (functionality) consistency between the augmented netlist 𝑐 ′ and the original netlist 𝑐, making the contrastive objective clear and explicit. By applying the above augmentation We apply mini-batch gradient descent to train our netlist contrastive learning scheme. Each time a mini-batch of 𝑁 netlists are randomly sampled and processed through the above functionalityconstant augmentation, resulting in 2𝑁 augmented netlists. Following the sampling strategy in <ref type="bibr" target="#b22">[23]</ref>, we define the negative samples for any positive pair (𝑐 ′ 1 , 𝑐 ′ 2 ) as the other (2𝑁 − 2) netlists within the mini-batch. A normalized temperature-scaled cross-entropy loss (NT-Xent) <ref type="bibr" target="#b23">[24]</ref> is then applied to maximize the consistency between positive pairs compared with negative samples, which is formulated as follows:</p><formula xml:id="formula_2">L = 1 𝑁 𝑁 𝑛=1 𝑙 1,2 (𝑛) + 𝑙 2,1 (𝑛),<label>(3)</label></formula><p>where 𝑙 1,2 (𝑛) + 𝑙 2,1 (𝑛) gives the loss for the positive pair of the 𝑛-th netlist, and 𝑙 𝑖,𝑗 is defined as:</p><formula xml:id="formula_3">𝑙 𝑖,𝑗 (𝑛) = − log exp(∼ (𝒉 𝑛,𝑖 , 𝒉 𝑛,𝑗 )/𝜏) 𝑁 𝑢=1,𝑢≠𝑛 exp(∼ (𝒉 𝑛,𝑖 , 𝒉 𝑢,𝑖 )/𝜏) + 𝑁 𝑢=1 exp(∼ (𝒉 𝑛,𝑖 , 𝒉 𝑢,𝑗 )/𝜏) ,<label>(4)</label></formula><p>where 𝒉 𝑛,𝑖 denotes the embedding of the 𝑖-th augmentation of 𝑛-th netlist, ∼ (𝒉 𝑛,𝑖 , 𝒉 𝑛,𝑗 ) = 𝒉 ⊤ 𝑛,𝑖 𝒉 𝑛,𝑗 /∥𝒉 𝑛,𝑖 ∥∥𝒉 𝑛,𝑗 ∥ is the cosine similarity between the two embeddings 𝒉 𝑛,𝑖 and 𝒉 𝑛,𝑗 and 𝜏 is a temperature parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Customized Graph Neural Network: FGNN</head><p>As mentioned in Section 2.1, though enabling a powerful representation learning paradigm for graphs, existing GNNs fail to capture the underlying semantic of netlists. Hence, designing customized architecture that adapts to netlist is still crucial to achieving better performance. This part discusses how to design a novel graph neural network architecture that complements our netlist contrastive learning scheme.</p><p>Regarding the unique properties of gate-level netlists, GNN customization should take two aspects into account: (1) how to learn logic functionality and (2) how to guarantee knowledge transferability between different netlists. We find that most existing GNNs do not fit well since they learn to encode topological information (e.g., node connectivity) instead of logic functionality. Consequently, the learned knowledge is highly related to the training graphs' topology, leading to poor generalization ability.</p><p>To overcome the above drawbacks, we propose a novel GNN architecture that targets learning the basic logical functionality of netlists, namely functional graph neural network (FGNN). Specifically, instead of learning a shared aggregator for all the nodes, FGNN learns a set of independent aggregators (functions), one for each gate type (e.g., AND, XOR. . . ). The insight behind this is that, as the most fundamental building component of netlists, the logic gates' functionalities naturally reflect the underlying semantics of netlists and keep constant across different designs. By learning the essential gate functions, our proposed model manages to capture the generic knowledge shared between different netlists. In practice, we learn 8 basic gate (cell) functions including AND, OR, INV, MAJ, MUX, NAND, NOR and XOR.</p><p>Motivated by ABGNN <ref type="bibr" target="#b5">[6]</ref>, our proposed FGNN follows an asynchronous message passing scheme. For any target vertex 𝑣 𝑖 , the message passing scheme starts from the Primary Inputs (PIs) of 𝑣 𝑖 's fanin-cone and all the way to 𝑣 𝑖 . During the procedure, a vertex stays inactive until all its predecessors' representations have been computed. When a vertex is activated, FGNN aggregates the messages (representations) received from its predecessors to construct its own representation and then sends the newly-built representation to all its successors. Our work differs from ABGNN <ref type="bibr" target="#b5">[6]</ref> since we use independent aggregators for vertices of different types. The above message passing scheme is illustrated in Figure <ref type="figure" target="#fig_4">5</ref>.</p><p>Formally, given a target vertex 𝑣, the aggregation scheme of the k-th iteration of a depth-𝛿 FGNN can be described as follows:</p><formula xml:id="formula_4">𝒉 (𝑘) {i:D(i,v)=𝛿−k} = A {g:T (i)=g} ({𝒉 (𝑘−1) 𝑢 : 𝑢 ∈ N(𝑖)}) = 𝜎 (𝑊 𝑔 • 𝑓 𝑔 ({𝒉 (𝑘−1) 𝑢 : 𝑢 ∈ N(𝑖)})),<label>(5)</label></formula><p>where D(𝑖, 𝑣) is the distance between vertices 𝑖 and 𝑣 in the graph, N(𝑖) is the set of direct neighbors of vertex 𝑖, T(𝑖) gives the type of vertex 𝑖, A 𝑔 is the aggregator for vertex type 𝑔, 𝜎 is an activation function, 𝑊 𝑔 is the weight matrix for A 𝑔 and 𝑓 𝑔 is the reduce function for A 𝑔 . The initial message for each vertex 𝑖 is an all-one vector. The red part in the formulation emphasizes the difference between our work and ABGNN <ref type="bibr" target="#b5">[6]</ref>.</p><p>The vertex representations learned by FGNN can be directly fed into a classifier/regression model to handle local-level tasks like link prediction or node classification. For global scenarios, e.g., graph classification, we first select a set of representative vertices 𝑉 from the target graph and then use a readout function READOUT (e.g., mean, sum, etc.) to combine the selected vertices' representations into a single global-level representation 𝒉 𝑔𝑙𝑜𝑏𝑎𝑙 , as described in Equation ( <ref type="formula" target="#formula_5">6</ref>),</p><formula xml:id="formula_5">𝒉 𝑔𝑙𝑜𝑏𝑎𝑙 = READOUT({𝒉 𝑢 : 𝑢 ∈ 𝑉 })<label>(6)</label></formula><p>In practice, we select the Primary Outputs (POs) of a target netlist as the representative vertices and use a mean function to readout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Curriculum Learning</head><p>We further introduce a curriculum learning scheme to guide the pre-training procedure. We first train the model on a small number of easy cases and then train on successively more complex cases with increased batch size. Two aspects determine the perplexity of a positive pair: (1) netlist complexity (depth/number of PIs) and ( <ref type="formula" target="#formula_1">2</ref>) topological similarity between the two composed netlists (measured by the times of functionalityconstant replacements). Simpler cases and cases with similar positive pairs are regarded as easier ones.</p><p>The curriculum is initialized with netlists with 4 PIs. Each time the training performance plateaus below a threshold loss, the perplexity of the cases is increased by either adding number of PIs or introducing more variance (applying more cell replacements) during data augmentation, up to a maximum of K PIs and M times replacements.</p><p>In our experiments, we set 𝐾 = 8 and 𝑀 = 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We implemented the netlist representation learning framework with DGL <ref type="bibr" target="#b4">[5]</ref>, a graph learning library based on PyTorch. FGNN is pre-trained/fine-tuned on a Linux machine with 48 Intel Xeon Silver 4212 cores (2.20GHz), 1 GeForce RTX 2080 Ti GPU, and a 32 GB main memory.</p><p>As mentioned in Section 4.3, we utilize the curriculum learning technique to guide the pre-training procedure. Specifically, we denote a dataset composed of netlists with m Primary Inputs (PIs) and up to n times cell replacements during augmentation as (m,n). The pre-training procedure starts from learning on easy cases denoted as (4,1). Each time the loss goes continuously lower than a threshold 𝑡, we change to more challenging cases with mores PIs or increased augmenting variance (more replacements). The training dataset sequence is set as (</p><p>. During training, we set the loss threshold 𝑡 = 0.1 and temperature parameter 𝜏 = 0.065. The datasets we use are composed of 50,000 5-PI netlists, 75,000 6-PI netlists, 100,000 7-PI netlists, and 100,000 8-PI netlists.</p><p>The performance of our proposed framework is evaluated on two different downstream netlist tasks covering both local and global scenarios. We feed the downstream target netlists into our pre-trained FGNN to generate node representations and further construct a global-level representation for each netlist as described in Equation <ref type="bibr" target="#b5">(6)</ref>. The representations are then fed into a classifier (Multilayer Perceptron, MLP) to fine-tune the model and make predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation on Sub-netlist Identification</head><p>We first assess our proposed framework on a local scenario, arithmetic block identification. Arithmetic blocks are the building blocks within a netlist that perform certain arithmetic operations (e.g., integer addition), whose boundaries are defined as the input/output wires interacting with external circuits <ref type="bibr" target="#b5">[6]</ref>. In general, our task is to recognize the boundaries of target arithmetic blocks from a large netlist design. Here we focus on identifying the output boundaries of adders, following the same experimental setting as <ref type="bibr" target="#b5">[6]</ref>, where the performance is measured in terms of recall and F1-score. The detail of the dataset we use is shown in Table <ref type="table" target="#tab_1">2</ref>.</p><p>We consider representative baselines that are dedicated to generating node-level representations, covering the following three categories: (1) CNN-based method <ref type="bibr" target="#b4">[5]</ref>, (2) general GNN-based method <ref type="bibr" target="#b7">[8]</ref>, and (3) customized GNN-based method that adapts to DAGs <ref type="bibr" target="#b5">[6]</ref>. All these baselines are trained end-to-end, and we report their performance based on their official implementations.</p><p>To evaluate the models' generalization ability, we fix the testing/validation dataset containing all the six different adder architectures (shown in table <ref type="table" target="#tab_1">2</ref>) and train/fine-tune the models with data that involves only part of the adder architectures. (e.g., 𝑟𝑎𝑡𝑖𝑜 = 1/6 means using one out of six different types). The result in Table <ref type="table" target="#tab_0">1</ref> demonstrates the superiority of our methods compared with several State-of-the-Art netlist representation learning methods. Our method stands out in all the cases and achieves 2.4% ∼ 12.3% recall gain and 1.2% ∼ 10.5% F1-Score improvement compared with the second-best solution <ref type="bibr" target="#b5">[6]</ref>. Additionally, we can see that GNN-based methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8]</ref> significantly outperform previous CNN-based work on all the cases and achieve relatively good performance when testing on data similar to the training dataset, confirming the power of GNN in netlist representation learning. However, they are subjected to sharp performance degradation when generalizing to unseen data. For instance, their performance drops by around 8% when half of the test adder structures are not involved in the training dataset (case 3). In contrast, the performance of our FGNN is much more stable, suffering from pretty minor degradation on all the cases (e.g., decreased by only 4% on case 3). Moreover, when combined with our novel netlist contrastive learning flow, our model's generalization ability is further enhanced, achieving comparable results on case 3 with other methods' best performance. This result shows the effectiveness of our proposed contrastive framework in extracting high-level prior knowledge of netlists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation on Nelist Classification</head><p>For the global scenario, we choose the netlist classification task, which targets distinguishing between netlists of different functions.</p><p>Here we focus on classifying arithmetic netlists, including adder, subtractor, multiplier, and divider. The training netlists are randomly generated in word-level Verilog and synthesized into gate-level circuits by Synopsys Design Compiler. Different constraints are used during synthesis to involve diverse architectures for each module. The validate/test dataset is composed of netlists with unseen architectures to evaluate the generalization ability. Specifically, we use adder/multiplier designs from <ref type="bibr" target="#b24">[25]</ref>, whose architectures are distinguished from training ones. The test subtractor designs are generated similarly as training data but with different constraints and thus disparate architectures. The netlists operate on word lengths ranging from 8 to 32 bits, with the number of gates ranging from hundreds to thousands. The statistic of the datasets is shown in Table <ref type="table" target="#tab_2">3</ref>. For evaluation, we use accuracy as the performance metric.</p><p>We reimplemented several representative prior works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14]</ref> as the baseline methods for comparison. These works have covered CNN-based method <ref type="bibr" target="#b4">[5]</ref>, general GNN-based method <ref type="bibr" target="#b6">[7]</ref>, as well as customized GNN-based method that adapts to DAGs <ref type="bibr" target="#b13">[14]</ref>. All these baselines are trained end-to-end, and we report their performance based on their official implementations.</p><p>To thoroughly test the models' generalization ability and robustness against the scarcity of training data, we fix the validation/testing data and train/fine-tune the models with datasets of different scales. The results are summarized in Table <ref type="table" target="#tab_3">4</ref>. From the table, we can see that our proposed framework shows substantial performance superiority over the baseline methods across all the cases. Remarkably, when trained with an unreduced dataset (first row), our framework correctly classifies 97.5% target netlists, achieving a performance gain of 6.2% accuracy over the second-best method <ref type="bibr" target="#b13">[14]</ref>. Moreover, our proposed framework suffers from slighter performance degradation when trained with decreasing data size. From the last row of the table, we can see that our method manages to achieve relatively high accuracy (94.5%) even with a training data size that is only 30% of the testing data size, dropped by only 3.1% compared with the best performance. In contrast, the performance of the baseline methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14]</ref> on the same case is decreased by 5.0%, 4.6% and 4.9% respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Learning feasible representations from raw gate-level netlists is critical for applying machine learning techniques to EDA. In this work, a novel netlist representation learning framework based on a graph contrastive scheme that extracts basic boolean functionality of netlists is proposed. FGNN, a specialized graph neural network, is further introduced to improve the performance of the framework. Experimental results on in-order and out-of-order RISC-V designs and two distinct downstream tasks verified the framework's effectiveness. The proposed framework can be applied to more downstream netlist tasks, which will be left for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the drawback of existing structurebased netlist representation learning methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Our proposed netlist representation learning flow 1.Preprocessing: Firstly, the input gate-level netlist is transformed to a DAG. Each vertex corresponds to a gate in the netlist, and directed edges represent the interconnection between gates, where the source vertices denote the driven gates.2. Pre-training: Secondly, functionality graph neural network (FGNN) (Section 4.2) is dedicated designed for learning functionality of gate-level netlists. The pre-training of FGNN is guided by the proposed gate-level netlist contrastive scheme (Section 4.1).3. Fine-tuning: Finally, pre-trained FGNN is equipped with a classifier and fine-tuned to adapt downstream tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of Boolean Equivalence scheme to netlist 𝑐 twice, we obtain two augmented netlists (𝑐 ′ 1 , 𝑐 ′ 2 ) as a positive pair .We apply mini-batch gradient descent to train our netlist contrastive learning scheme. Each time a mini-batch of 𝑁 netlists are randomly sampled and processed through the above functionalityconstant augmentation, resulting in 2𝑁 augmented netlists. Following the sampling strategy in<ref type="bibr" target="#b22">[23]</ref>, we define the negative samples for any positive pair (𝑐 ′ 1 , 𝑐 ′ 2 ) as the other (2𝑁 − 2) netlists within the mini-batch. A normalized temperature-scaled cross-entropy loss (NT-Xent)<ref type="bibr" target="#b23">[24]</ref> is then applied to maximize the consistency between positive pairs compared with negative samples, which is formulated as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Overview of our Netlist Contrastive representation learning framework. Refer to Section 4.1 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: An illustration of the representation generating procedure. The target vertex 𝑣 is emphasized with bold outlines. Square vertices are primary inputs initializing with all-one representations. Different states of vertices are indicated by fill colors: blank means inactive, red means being computed at current iteration and blue means having been computed in previous iterations. INV, AND, OR are different aggregators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance of different models on adder output boundary prediction in terms of recall and F1-score. Best results are emphasized with boldface. Our proposed FGNN + NCL framework outperforms other models in all the test cases.</figDesc><table><row><cell cols="2">Case Ratio</cell><cell cols="10">EV-CNN [5] Recall F1-Score Recall F1-Score Recall F1-Score Recall F1-Score Recall F1-Score GraphSage [8] ABGNN [6] FGNN FGNN + NCL</cell></row><row><cell>1</cell><cell>1/6</cell><cell>0.602</cell><cell>0.575</cell><cell>0.643</cell><cell>0.656</cell><cell>0.657</cell><cell>0.682</cell><cell>0.684</cell><cell>0.715</cell><cell>0.734</cell><cell>0.753</cell></row><row><cell>2</cell><cell>2/6</cell><cell>0.612</cell><cell>0.605</cell><cell>0.758</cell><cell>0.757</cell><cell>0.734</cell><cell>0.74</cell><cell>0.784</cell><cell>0.788</cell><cell>0.857</cell><cell>0.839</cell></row><row><cell>3</cell><cell>3/6</cell><cell>0.633</cell><cell>0.615</cell><cell>0.854</cell><cell>0.865</cell><cell>0.877</cell><cell>0.881</cell><cell>0.916</cell><cell>0.914</cell><cell>0.940</cell><cell>0.937</cell></row><row><cell>4</cell><cell>4/6</cell><cell>0.662</cell><cell>0.637</cell><cell>0.883</cell><cell>0.889</cell><cell>0.921</cell><cell>0.917</cell><cell>0.931</cell><cell>0.933</cell><cell>0.954</cell><cell>0.947</cell></row><row><cell>5</cell><cell>5/6</cell><cell>0.738</cell><cell>0.648</cell><cell>0.905</cell><cell>0.898</cell><cell>0.927</cell><cell>0.922</cell><cell>0.952</cell><cell>0.944</cell><cell>0.966</cell><cell>0.951</cell></row><row><cell>6</cell><cell>6/6</cell><cell>0.768</cell><cell>0.655</cell><cell>0.919</cell><cell>0.917</cell><cell>0.945</cell><cell>0.941</cell><cell>0.963</cell><cell>0.952</cell><cell>0.969</cell><cell>0.957</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the dataset for sub-netlist identification with 6 different types of adders.</figDesc><table><row><cell>Architecture</cell><cell>Rocket (test)</cell><cell>BOOM (train)</cell></row><row><cell></cell><cell cols="2">#gates #wires #gates #wires</cell></row><row><cell cols="3">Brent-Kung 24340 58124 139526 366280</cell></row><row><cell>Cond-sum</cell><cell cols="2">24737 57708 138358 360455</cell></row><row><cell>Hybrid</cell><cell cols="2">25491 60287 141319 369622</cell></row><row><cell cols="3">Kogge-Stone 24540 57726 139005 361962</cell></row><row><cell>Ling</cell><cell cols="2">26179 62864 143903 378354</cell></row><row><cell>Sklansky</cell><cell cols="2">25208 59567 141093 369774</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Statistics of the dataset for netlist classification, including adder, subtractor, multiplier, and divider. We try to avoid involving similar designs used for training in the test dataset.</figDesc><table><row><cell>Module</cell><cell>Train architectures</cell><cell>#</cell><cell>Validate / Test architectures</cell><cell>#</cell></row><row><cell></cell><cell>Brent-Kung,</cell><cell></cell><cell>Block Carry Look-head,</cell><cell></cell></row><row><cell></cell><cell>Cond-Sum,</cell><cell></cell><cell>Carry Look-head,</cell><cell></cell></row><row><cell>Adder</cell><cell>Hybrid, Koggle-Stone,</cell><cell>450</cell><cell>Carry Select, Carry-skip,</cell><cell>100 + 300</cell></row><row><cell></cell><cell>Ling,</cell><cell></cell><cell>Ripple-Carry</cell><cell></cell></row><row><cell></cell><cell>Sklansky</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Hybrid,</cell><cell></cell><cell>Brent-Kung,</cell><cell></cell></row><row><cell>Subtractor</cell><cell>Koggle-Stone,</cell><cell>250</cell><cell>Cond-Sum,</cell><cell>50 + 150</cell></row><row><cell></cell><cell>Ling</cell><cell></cell><cell>Sklansky</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Wallace,</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Dadda,</cell><cell></cell></row><row><cell>Multiplier</cell><cell>Array, Booth-Encoding</cell><cell>550</cell><cell>Overturned-stairs, (4,2) compressor,</cell><cell>150 + 500</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(7,3) counter,</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Redundant binary addition</cell><cell></cell></row><row><cell>Divider</cell><cell>Array</cell><cell>250</cell><cell>Array</cell><cell>50 + 200</cell></row><row><cell>Total</cell><cell>/</cell><cell>1500</cell><cell>/</cell><cell>350 + 1150</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Summary of performance on netlist classification in terms of accuracy. The second column gives the ratio of the training data size to the testing data size. Our proposed FGNN + NCL framework achieves the best performance on all the cases and suffers from slighter degradation when the training data scale is reduced.</figDesc><table><row><cell cols="2">Case Ratio</cell><cell>GIN [7]</cell><cell>EV-CNN [5] DVAE [14]</cell><cell>Ours</cell></row><row><cell>1</cell><cell>1.3</cell><cell cols="3">0.762±0.020 0.904±0.011 0.913±0.005 0.975±0.008</cell></row><row><cell>2</cell><cell>1</cell><cell cols="3">0.745±0.026 0.896±0.009 0.902±0.007 0.962±0.007</cell></row><row><cell>3</cell><cell>0.7</cell><cell cols="3">0.737±0.022 0.884±0.003 0.895±0.009 0.960±0.009</cell></row><row><cell>4</cell><cell>0.5</cell><cell cols="3">0.730±0.015 0.877±0.006 0.885±0.010 0.951±0.005</cell></row><row><cell>5</cell><cell>0.3</cell><cell cols="3">0.725±0.028 0.859±0.015 0.871±0.003 0.945±0.007</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work is partially supported by HiSilicon and The Research Grants Council of Hong Kong SAR CUHK14209420, CUHK14208021.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">High performance graph convolutional networks with applications in testability analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Khailany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. DAC</title>
				<meeting>DAC</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">High-speed adder design space exploration via graph neural processes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCAD</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Machine learning for electronic design automation: A survey</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TODAES</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="46" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Wordrev: Finding word-level structures in a sea of bit-level gates</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gascon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Subramanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Seshia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HOST. IEEE</title>
				<meeting>HOST. IEEE</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="67" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep learning-based circuit recognition using sparse mapping and level-dependent decaying sum circuit representations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fayyazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shababi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nuzzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nazarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pedram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. DATE</title>
				<meeting>DATE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="638" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Graph learning-based arithmetic block identification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCAD</title>
				<meeting>ICCAD</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
				<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
				<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="5165" to="5175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exact-K Recommendation via Maximal Clique Optimization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. KDD</title>
				<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="617" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep h-gcn: Fast analog ic aging-induced degradation estimation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCAD</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Directed acyclic graph neural networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Thost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.07965</idno>
		<imprint>
			<biblScope unit="volume">01</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">D-vae: A variational autoencoder for directed acyclic graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
				<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning Representations by Maximizing Mutual Information Across Views</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
				<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="15" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
				<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On variational bounds of mutual information</title>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
				<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5171" to="5180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep graph infomax</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
				<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5812" to="5823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with adaptive augmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2069" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">What makes for good views for contrastive learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10243</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">What should not be contrastive in contrastive learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.05659</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On sampling strategies for neural networkbased collaborative filtering</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. KDD</title>
				<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="767" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
				<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Formal design of arithmetic circuits based on arithmetic description language</title>
		<author>
			<persName><forename type="first">N</forename><surname>Homma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Higuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Trans. Fundamentals</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3500" to="3509" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
