<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-04-28">28 Apr 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University ‡ BAAI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University ‡ BAAI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University ‡ BAAI</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
							<email>jietang@mail.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University ‡ BAAI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-04-28">28 Apr 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2204.14217v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Preprint. Under review</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The development of the transformer-based text-to-image models are impeded by its slow generation and complexity for high-resolution images. In this work, we put forward a solution based on hierarchical transformers and local parallel autoregressive generation. We pretrain a 6B-parameter transformer with a simple and flexible self-supervised task, Cross-modal general language model (CogLM), and finetune it for fast super-resolution. The new text-to-image system, CogView2, shows very competitive generation compared to concurrent state-of-the-art DALL-E-2, and naturally supports interactive text-guided editing on images.</p><p>A lion man is typing in the office.</p><p>A beautiful girl is hugging a husky. A lion teacher wearing a suit is in front of a blackboard.</p><p>A robot is riding under the blue and cloudy sky.</p><p>Several youths are talking in a bar.</p><p>A young woman is taking photos.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, Text-to-image generation has been greatly advanced by large-scale pretrained transformers, e.g. DALL-E <ref type="bibr" target="#b23">[24]</ref> and CogView <ref type="bibr" target="#b2">[3]</ref>. These models generally learn to generate image tokens in an auto-regressive way, thus suffer from the following defects: Slow generation. The generation of auto-regressive models is usually much slower than nonautoregressive models, e.g. GANs <ref type="bibr" target="#b9">[10]</ref>, with the same FLOPs. Instead of the large number of parameters, this shortcoming is mainly attributed to the nature of token-by-token generation of auto-regressive models, which cannot fully utilize the parallel computing ability of GPUs even after caching hidden states <ref type="bibr" target="#b22">[23]</ref>.</p><p>Expensive high-resolution training. The current large-scale pretrained models are usually based on Transformers <ref type="bibr" target="#b26">[27]</ref>, where the attention operation has both time and space complexity of O(n 2 ) for training sequences of length n. Within a limited budget, we face a trade-off between the number of parameters, which represents the modeling power, and the resolution of the generated images. Due to this reason, the majority of current text-to-image models chose a resolution of 32 × 32 tokens (usually 256 × 256 pixels) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b10">11]</ref>, which is far lower than the resolution of the real photos.</p><p>Uni-direction. Auto-regressive models, e.g. GPTs, for images usually generate tokens following raster-scan order. This order shows the best perplexity during the evaluation <ref type="bibr" target="#b6">[7]</ref>. However, this order makes the models unaware of the tokens below or on the right side during generation, so that text-guided infilling is not supported. Moreover, the uni-direction creates a gap between the pretrained text-to-image models and vision transformers (ViTs) <ref type="bibr" target="#b4">[5]</ref> based on bidirectional masked prediction, e.g. MAE <ref type="bibr" target="#b11">[12]</ref> and SimMIM <ref type="bibr" target="#b29">[30]</ref>, limiting their application on traditional visual tasks, e.g. image classification and object detection.</p><p>Present Work. To overcome the defects above, we first propose a simple and versatile pretraining method, Cross-Modal general Language Model (CogLM). Our CogLM masks various types of tokens in the sequence of text and image tokens, and learns to predict them in an auto-regressive way. Specifically, <ref type="bibr" target="#b0">(1)</ref> if we mask all the image tokens, it is equivalent to the original CogView to perform a text-to-image generation task. (2) If we mask random patches of image tokens, it works similar to MAE as an infilling task. (3) If we mask text tokens, the task becomes image captioning.</p><p>The versatility of CogLM enables us to finetune a pretrained CogLM for different downstream tasks, and constructs a hierarchical model, CogView2.There are three steps in the hierarchical generation process as follows:</p><p>1. First, we generate a batch of low-resolution images (20 × 20 tokens in CogView2) using the pretrained CogLM, and then (optionally) filter out the bad samples based on the perplexity of CogLM image captioning, which is the post-selection method introduced in CogView <ref type="bibr" target="#b2">[3]</ref>.</p><p>2. The generated images are directly mapped into 60 × 60-token images by a direct superresolution module finetuned from the pretrained CogLM. We use local attention implemented by our customized CUDA kernel to reduce the training expense. The high-resolution images from this step usually have inconsistent textures and lack of details.</p><p>3. These high-resolution images are refined via another iterative super-resolution module finetuned from the pretrained CogLM. Most tokens are re-masked and re-generated in a local parallel auto-regressive (LoPAR) way, which is much faster than usual auto-regressive generation.</p><p>How does CogView2 conquer the three defects? Firstly, during pretraining the masked patch prediction task trains CogLM to handle bidirectional context, making it easy to be adapted to bidirectional tasks, e.g. the direct and iterative super-resolution. Secondly, the hierarchical design allows us to only care about local coherence at high-resolution level, so that local attention is leveraged to reduce the training expense. Thirdly, the local parallel auto-regressive generation can reduce the times of model forward from 3,600 to 6, greatly accelerating the generation of high-resolution images. CogView2 is about 10× faster than the CogView (with sliding-window super-resolution) to generate images of similar resolution and better quality.      x 4</p><formula xml:id="formula_0">Z i l E V f I J D W m 4 7 k J + h n V K J j k k 1 I 3 N T y h b E Q H v G O p o h E 3 f j Y 7 d U J O r N I n Y a x t K S Q z 9 f d E R i N j x l F g O y O K Q 7 P o T c X / v E 6 K 4 Z W f C Z W k</formula><formula xml:id="formula_1">Z i l E V f I J D W m 4 7 k J + h n V K J j k k 2 I 3 N T y h b E Q H v G O p o h E 3 f j Y 7 d U J O r d I n Y a x t K S Q z 9 f d E R i N j x l F g O y O K Q 7 P o T c X / v E 6 K 4 Z W f C Z W k</formula><formula xml:id="formula_2">W i 1 i 1 A 6 p R c I k N w 4 3 A d q K Q R o H A V j C 6 n v q t B 1 S a x / L e j B P 0 I z q Q P O S M G i v d P f Z O e + W K W 3 V n I H + J l 5 M K 5 K j 3 y p / d f s z S C K V h g m r d 8 d z E + B l V h j O B k 1 I 3 1 Z h Q N q I D 7 F g q a Y T a z 2 a n T s i R V f o k j J U t a c h M / T m R 0 U j r c R T Y z o i a o V 7 0 p u J / X i c 1 4 a W f c Z m k B i W b L w p T Q U x M p n + T P l f I j B h b Q p n i 9 l b C h l R R Z m w 6 J R u C t / j y X 9 I 8 q X r n V e / 2</formula><formula xml:id="formula_3">W i 1 i 1 A 6 p R c I k N w 4 3 A d q K Q R o H A V j C 6 n v q t B 1 S a x / L e j B P 0 I z q Q P O S M G i v d P f Z O e + W K W 3 V n I H + J l 5 M K 5 K j 3 y p / d f s z S C K V h g m r d 8 d z E + B l V h j O B k 1 I 3 1 Z h Q N q I D 7 F g q a Y T a z 2 a n T s i R V f o k j J U t a c h M / T m R 0 U j r c R T Y z o i a o V 7 0 p u J / X i c 1 4 a W f c Z m k B i W b L w p T Q U x M p n + T P l f I j B h b Q p n i 9 l b C h l R R Z m w 6 J R u C t / j y X 9 I 8 q X r n V e / 2</formula><formula xml:id="formula_4">U I = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G P R i 8 e K 9 g P a U D b b T b t 0 s w m 7 E 7 G E / g Q v H h T x 6 i / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S K Q w 6 L r f T m F l d W 1 9 o 7 h Z 2 t r e 2 d 0 r 7 x 8 0 T Z x q x h s s l r F u B 9 R w K R R v o E D J 2 4 n m N A o k b w W j m 6 n f e u T a i F g 9 4 D j h f k Q H S o S C U b T S / V P v v F e u u F V 3 B r J M v J x U I E e 9 V / 7 q 9 m O W R l w h k 9 S Y j u c m 6 G d U o 2 C S T 0 r d 1 P C E s h E d 8 I 6 l i k b c + N n s 1 A k 5 s U q f h L G 2 p Z D M 1 N 8 T G Y 2 M G U e B 7 Y w o D s 2 i N x X / 8 z o p h l d + J l S S I l d s v i h M J c G Y T P 8 m f a E 5 Q z m 2 h D I t 7 K 2 E D a m m D G 0 6 J R u C t / j y M m m e V b 2 L q n d 3 X q l d 5 3 E U 4 Q i O 4 R Q 8 u I Q a 3 E I d G</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = "</p><formula xml:id="formula_5">I i 4 v K Y p P s B Q p Y W W F k R 6 w U 7 t K V m s = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o m I 9 V j 0 4 r G i / Y A 2 l M 1 2 0 y 7 d b M L u R C y h P 8 G L B 0 W 8 + o u 8 + W / c t j l o 6 4 O B x 3 s z z M w L E i k M u u 6 3 s 7 K 6 t r 6 x W d g q b u / s 7 u 2 X D g 6 b J k 4 1 4 w 0 W y 1 i 3 A 2 q 4 F I o 3 U K D k 7 U R z G g W S t 4 L R z d R v P X J t R K w e c J x w P 6 I D J U L B K F r p / q l X 7 Z X K b s W d g S w T L y d l y F H v l b 6 6 / Z i l E V f I J D W m 4 7 k J + h n V K J j k k 2 I 3 N T y h b E Q H v G O p o h E 3 f j Y 7 d U J O r d I n Y a x t K S Q z 9 f d E R i N j x l F g O y O K Q 7 P o T c X / v E 6 K 4 Z W f C Z W k y B W b L w p T S T A m 0 7 9 J X 2 j O U I 4 t o U w L e y t h Q 6 o p Q 5 t O 0 Y b g L b 6 8 T J r n F e + y 4 t 1 d l G v X e R w F O I Y T O A M P q l C D W 6 h D A x g M 4 B l e 4 c 2 R z o v z 7 n z M W 1 e c f O Y I / s D 5 / A E W w I 2 s &lt; / l a t e x i t &gt; x 7 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R 9 s X I a f g y D f V + T U X 6 o G I m e 1 v d C E = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o m I 9 l j 0 4 r G i / Y A 2 l M 1 2 0 y 7 d b M L u R C y h P 8 G L B 0 W 8 + o u 8 + W / c t j l o 6 4 O B x 3 s z z M w L E i k M u u 6 3 s 7 K 6 t r 6 x W d g q b u / s 7 u 2 X D g 6 b J k 4 1 4 w 0 W y 1 i 3 A 2 q 4 F I o 3 U K D k 7 U R z G g W S t 4 L R z d R v P X J t R K w e c J x w P 6 I D J U L B K F r p / q l X 7 Z X K b s W d g S w T L y d l y F H v l b 6 6 / Z i l E V f I J D W m 4 7 k J + h n V K J j k k 2 I 3 N T y h b E Q H v G O p o h E 3 f j Y 7 d U J O r d I n Y a x t K S Q z 9 f d E R i N j x l F g O y O K Q 7 P o T c X / v E 6 K Y d X P h E p S 5 I r N F 4 W p J B i T 6 d + k L z R n K M e W U K a F v Z W w I d W U o U 2 n a E P w F l 9 e J s 3 z i n d Z 8 e 4 u y r X r P I 4 C H M M J n I E H V 1 C D W 6 h D A x g M 4 B l e 4 c 2 R z o v z 7 n z M W 1 e c f O Y I / s D 5 / A E Y R I 2 t &lt; / l a t e x i t &gt; x 8</formula><p>Target tokens for prediction Attention Mask</p><formula xml:id="formula_6">&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W x m w / E M J s Z Z s s M A 9 Y 5 C v R F 7 2 / d I = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G P R i 8 e K 9 g P a U D b b S b t 0 s w m 7 G 7 G E / g Q v H h T x 6 i / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S A T X x n W / n c L K 6 t r 6 R n G z t L W 9 s 7 t X 3 j 9 o 6 j h V D B s s F r F q B 1 S j 4 B I b h h u B 7 U Q h j Q K B r W B 0 M / V b j 6 g 0 j + W D G S f o R 3 Q g e c g Z N V a 6 f + q 5 v X L F r b o z k G X i 5 a Q C O e q 9 8 l e 3 H 7 M 0 Q m m Y o F p 3 P D c x f k a V 4 U z g p N R N N S a U j e g A O 5 Z K G q H 2 s 9 m p E 3 J i l T 4 J Y 2 V L G j J T f 0 9 k N N J 6 H A W 2 M 6 J m q B e 9 q f i f 1 0 l N e O V n X C a p Q c n m i 8 J U E B O T 6 d + k z x U y I 8 a W U K a 4 v Z W w I V W U G Z t O y Y b g L b 6 8 T J p n V e + i 6 t 2 d V 2 r X e R x F O I J j O A U P L q E G t 1 C H B j A Y w D O 8 w p s j n B f n 3 f m Y t x a c f O Y Q / s D 5 / A E M J I 2 l &lt; / l a t e x i t &gt; x 0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r J O A y / 5 9 q N k G P b V t P K V u T / I n E O k = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G P R i 8 e K 9 g P a U D b b T b t 0 s w m 7 E 7 G E / g Q v H h T x 6 i / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S K Q w 6 L r f T m F l d W 1 9 o 7 h Z 2 t r e 2 d 0 r 7 x 8 0 T Z x q x h s s l r F u B 9 R w K R R v o E D J 2 4 n m N A o k b w W j m 6 n f e u T a i F g 9 4 D j h f k Q H S o S C U b T S / V P P 6 5 U r b t W d g S w T L y c V y F H v l b + 6 / Z i l E V f I J D W m 4 7 k J + h n V K J j k k 1 I 3 N T y h b E Q H v G O p o h E 3 f j Y 7 d U J O r N I n Y a x t K S Q z 9 f d E R i N j x l F g O y O K Q 7 P o T c X / v E 6 K 4 Z W f C Z W k y B W b L w p T S T A m 0 7 9 J X 2 j O U I 4 t o U w L e y t h Q 6 o p Q 5 t O y Y b g L b 6 8 T J p n V e + i 6 t 2 d V 2 r X e R x F O I J j O A U P L q E G t 1 C H B j A Y w D O 8 w p s j n R f n 3 f m Y t x a c f O Y Q / s D 5 / A E N q I 2 m &lt; / l a t e x i t &gt; x 1</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " y J q Z U l w 9       </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>d z F l w 0 h G A O H e W j A 4 i c w = " &gt;</head><formula xml:id="formula_7">A A A B 6 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k k R 9 V j 0 4 r G i / Y A 2 l M 1 2 0 y 7 d b M L u R C y h P 8 G L B 0 W 8 + o u 8 + W / c t j l o 6 4 O B x 3 s z z M w L E i k M u u 6 3 s 7 K 6 t r 6 x W d g q b u / s 7 u 2 X D g 6 b J k 4 1 4 w 0 W y 1 i 3 A 2 q 4 F I o 3 U K D k 7 U R z G g W S t 4 L R z d R v P X J t R K w e c J x w P 6 I D J U L B K F r p / q l X 7 Z X K b s W d g S w T L y d l y F H v l b 6 6 / Z i l E V f I J D W m 4 7 k J + h n V K J j k k 2 I 3 N T y h b E Q H v G O p o h E 3 f j Y 7 d U J O r d I n Y a x t K S Q z 9 f d E R i N j x l F g O y O K Q 7 P o T c X / v E 6 K 4 Z W f C Z W k</formula><formula xml:id="formula_8">g 0 W i 1 i 1 A 6 p R c I k N w 4 3 A d q K Q R o H A V j C 6 n v q t B 1 S a x / L e j B P 0 I z q Q P O S M G i v d P f Z O e + W K W 3 V n I H + J l 5 M K 5 K j 3 y p / d f s z S C K V h g m r d 8 d z E + B l V h j O B k 1 I 3 1 Z h Q N q I D 7 F g q a Y T a z 2 a n T s i R V f o k j J U t a c h M / T m R 0 U j r c R T Y z o i a o V 7 0 p u J / X i c 1 4 a W f c Z m k B i W b L w p T Q U x M p n + T P l f I j B h b Q p n i 9 l b C h l R R Z m w 6 J R u C t / j y X 9 I 8 q X r n V e / 2</formula><formula xml:id="formula_9">g 0 W i 1 i 1 A 6 p R c I k N w 4 3 A d q K Q R o H A V j C 6 n v q t B 1 S a x / L e j B P 0 I z q Q P O S M G i v d P f b O e u W K W 3 V n I H + J l 5 M K 5 K j 3 y p / d f s z S C K V h g m r d 8 d z E + B l V h j O B k 1 I 3 1 Z h Q N q I D 7 F g q a Y T a z 2 a n T s i R V f o k j J U t a c h M / T m R 0 U j r c R T Y z o i a o V 7 0 p u J / X i c 1 4 a W f c Z m k B i W b L w p T Q U x M p n + T P l f I j B h b Q p n i 9 l b C h l R R Z m w 6 J R u C t / j y X 9 I 8 q X r n V e /</formula><formula xml:id="formula_10">F I o 3 U K D k 7 U R z G g W S t 4 L R z d R v P X J t R K w e c J x w P 6 I D J U L B K F r p / q l X 7 Z X K b s W d g S w T L y d l y F H v l b 6 6 / Z i l E V f I J D W m 4 7 k J + h n V K J j k k 2 I 3 N T y h b E Q H v G O p o h E 3 f j Y 7 d U J O r d I n Y a x t K S Q z 9 f d E R i N j x l F g O y O K Q 7 P o T c X / v E 6 K 4 Z W f C Z W k</formula><formula xml:id="formula_11">Z i l E V f I J D W m 4 7 k J + h n V K J j k k 1 I 3 N T y h b E Q H v G O p o h E 3 f j Y 7 d U J O r N I n Y a x t K S Q z 9 f d E R i N j x l F g O y O K Q 7 P o T c X / v E 6 K 4 Z W f C Z W k</formula><formula xml:id="formula_12">Z i l E V f I J D W m 4 7 k J + h n V K J j k k 2 I 3 N T y h b E Q H v G O p o h E 3 f j Y 7 d U J O r d I n Y a x t K S Q z 9 f d E R i N j x l F g O y O K Q 7 P o T c X / v E 6 K 4 Z W f C Z W k</formula><formula xml:id="formula_13">Z i l E V f I J D W m 4 7 k J + h n V K J j k k 2 I 3 N T y h b E Q H v G O p o h E 3 f j Y 7 d U J O r d I n Y a x t K S Q z 9 f d E R i N j x l F g O y O K Q 7 P o T c X / v E 6 K Y d X P h E p S 5 I r N F 4 W p J B i T 6 d + k L z R n K M e W U K a F v Z W w I d W U o U</formula><formula xml:id="formula_14">k O Q q V 4 y / G 2 C w E G j e y + G F e k F c = " &gt; A A A C A n i c b V D L S g M x F M 3 U V 6 2 v U V f i J l g E V 2 V G i 7 o s u H F Z w T 6 g M 5 R M e q c N z W S G J C O U o b j x V 9 y 4 U M S t X + H O v z H T z k J b D 4 Q c z r n 3 J v c E C W d K O 8 6 3 V V p Z X V v f K G 9 W t r Z 3 d v f s / Y O 2 i l N J o U V j H s t u Q B R w J q C l m e b Q T S S Q K O D Q C c Y 3 u d 9 5 A K l Y L O 7 1 J A E / I k P B Q k a J N l L f P v J i Y w e S U M i 8 k U r y + 9 J J 9 H T a t 6 t O z Z k B L x O 3 I F V U o N m 3 v 7 x B T N M I h K a c K N V z z R w / I 1 I z y m F a 8 V I F Z v 6 Y D K F n q C A R K D + b r T D F p 0 Y Z 4 D C W 5 g i N Z + r v j o x E S k 2 i w F R G R I / U o p e L / 3 m 9 V I f X f s Z E k m o Q d P 5 Q m H K s Y 5 z n g Q d M A t V 8 Y g i h k p m / Y j o i J g 9 t U q u Y E N z F l Z d J + 7 z m X t S c u 3 q 1 U S / i K K N j d I L O k I u u U A P d o i Z q I Y o e 0 T N 6 R W / W k / V i v V s f 8 9 K S V f Q c o j + w P n 8 A 7 1 2 X u A = = &lt; / l a t e x i t &gt;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Icetk package</head><p>Supports tokenization of both Image Chinese and English (150,000 tokens). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>20*20=400 image tokens Text tokenization Sample Mask Regions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Text-to-image generation for arbitrary inputs is a long-held dream for many cross-modal machine learning researchers. Early attempts for this task are usually based on Generative Adversarial Nets <ref type="bibr" target="#b9">[10]</ref>, including AttnGAN <ref type="bibr" target="#b30">[31]</ref>, DM-GAN <ref type="bibr" target="#b34">[35]</ref>, DF-GAN <ref type="bibr" target="#b24">[25]</ref> et al. Although they can perform vivid synthesis on domain-specific datasets, e.g. Caltech-UCSD Birds 200, general-domain datasets, e.g. MS COCO <ref type="bibr" target="#b14">[15]</ref>, are great challenges for these methods. DALL-E <ref type="bibr" target="#b23">[24]</ref>, CogView <ref type="bibr" target="#b2">[3]</ref> and similar works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b7">8]</ref> leverage VQ-VAE <ref type="bibr" target="#b25">[26]</ref> to compress an image to a sequence of discrete tokens and pretrain large transformers for auto-regressive generation, greatly advancing the task in general domain. LAFITE <ref type="bibr" target="#b33">[34]</ref> learns to invert the pretrained CLIP <ref type="bibr" target="#b20">[21]</ref> embeddings in the shared space of text and image for text-free training. Recently, many researchers turn to diffusion models, e.g. Glide <ref type="bibr" target="#b16">[17]</ref>, largely due to the slow generation defect of the auto-regressive models.</p><p>Non-autoregressive generation (NAR) is a popular topic recently in natural language generation, e.g. Mask-Predict <ref type="bibr" target="#b8">[9]</ref> and GLAT <ref type="bibr" target="#b18">[19]</ref>, exploring parallel decoding methods for auto-regressive-like models. The speed of generation was not an issue at the era when GANs dominated the image generation, while constitutes a considerate challenge for current auto-regressive text-to-image models. M6-UFC <ref type="bibr" target="#b32">[33]</ref> first introduces NAR methods into the VQ-VAE framework, and similar ideas are adopted by VQ-diffusion <ref type="bibr" target="#b10">[11]</ref> and MaskGIT <ref type="bibr" target="#b0">[1]</ref>. A possible drawback of pure NAR methods is that tokens sampled at the meantime might lead to global inconsistency in later steps during the generation of complex scenes. Our method introduces a hierarchical design to combine the consistency merit of auto-regressive models and the speed advantage of NAR methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Cross-Modal General Language Model</head><p>As the previous self-supervised pretext tasks often target at mask prediction in the computer vision <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b11">12]</ref>, our approach pursues a unification of auto-regressive generation and bidirectional context-aware mask prediction.</p><p>In NLP, General Language Model (GLM) <ref type="bibr" target="#b5">[6]</ref> first proposes to change the direct mask prediction into blockwise auto-regressive generation. However, a part of its design is redundant for images. For instance, the sizes of the masked image patches are fixed, so that we do not require the capacity of filling blocks of indefinite length as in NLP. Moreover, GLM inserts a sentinel token for each mask region to predict its first token, which will greatly increase the sequence length and restrict the usage of 2D local attention.</p><p>Based on the analysis above, we present a more simple and general language model for both text and image data, Cross-modal general Language Model (CogLM). As shown in Figure <ref type="figure" target="#fig_3">2</ref>, CogLM takes as input a concatenation of text and images tokenized by icetk<ref type="foot" target="#foot_0">1</ref> , whose dictionary contains 20,000 image tokens and 130,000 text (both Chinese and English) tokens. Formally, let t = [t 1 , ..., t M ] be the text tokens and im = [im 1 , ..., im N 2 ] be the image tokens, where M and N 2 are the lengths of text and image tokens respectively. The crucial step in CogLM is to sample k mask regions R = {[l 0 , r 0 ], ..., [l k , r k ]} according to various strategies. In practice, the following two strategies are used:</p><formula xml:id="formula_15">• (Text-to-Image GPT.) The input sequence is x = [t [BOI] im ].</formula><p>We mask all the image tokens, which is similar to the pretraining task of CogView <ref type="bibr" target="#b2">[3]</ref>.</p><p>• (A Combination of Mask Prediction and Image Captioning.) The input sequence is</p><formula xml:id="formula_16">x = [im 0 ... im i ... im j ... im N 2 [BOE/C] t ],</formula><p>where [BOE],[BOC] are seperators meaning begin-of-English and begin-of-Chinese used for the corresponding language. we mask random patches and the text tokens. In the ideal strategy, the two tasks should be separated but we combine them together for training efficiency.</p><p>Instead of replacing the tokens in the mask regions as [MASK], we make no change in the input but build an attention mask A based on the mask regions. All the tokens outside mask regions are seen as context and can be attended to by all the other tokens. A token in mask regions can only be attended to by the tokens in mask regions and behind it. Specifically,</p><formula xml:id="formula_17">A[i, j] =    1, if ∀ [l u , r u ] ∈ R, j / ∈ [l u , r u ]. 1, if j ≤ i and ∃ u, v (indices), i ∈ [l u , r u ] ∈ R, j ∈ [l v , r v ] ∈ R. 0, else.</formula><p>(1) Figure <ref type="figure" target="#fig_3">2</ref> shows an example of the attention mask matrix of two mask regions.</p><p>In the mask regions, the model learns to predict the next token. The loss function can be written as follows,</p><formula xml:id="formula_18">L = −1 u r u − l u v rv−1 i=lv log p(x i+1 |x ≤i , x context ),<label>(2)</label></formula><p>The order of Generation A fox is siting on the books. where the x context denotes the tokens outside the mask regions.</p><p>Infilling. Note that the first token in each mask region is not predicted during training. This feature seems to disable CogLM from image infilling or cloze filling in natural language, but this problem actually has a simple solution. During inference, we can move the last context token before each mask region into it, which is illustrated in Figure <ref type="figure" target="#fig_5">3</ref>. Although these moved tokens becomes blind spots for mask regions before them, it causes minor influence in practice. To further avoid this minor influence and fully maintain the context information, we can deal with each mask region one by one. For each region, we only move the last context token before this region, and keep all the known token outside the mask regions. In this way, we cannot use the cached hidden states from the last region, slightly slowing down the multi-region infilling.</p><p>Advantages over GPT <ref type="bibr" target="#b19">[20]</ref>, GLM <ref type="bibr" target="#b5">[6]</ref> and MAE <ref type="bibr" target="#b11">[12]</ref>. (GPT)</p><p>The main advantage over GPT is that the modeling of bidirectional context are considered in CogLM, which will benefit many tasks relying on global information, e.g. super-resolution in the next section and image classification. The importance of bidirectional context has been verified in the comparison of BERT <ref type="bibr" target="#b1">[2]</ref> and GPT on GLUE <ref type="bibr" target="#b27">[28]</ref>. (GLM) The main advantage over GLM is simplicity. To unify the generation and bidirectional understanding, GLM needs to define many new special tokens and a new type of position embedding, insert a sentinel for each mask region and change the order of input tokens. It destroys the spatial relevance in the image data and excludes the possibility of the usage of 2D local attention or convolution. (MAE) MAE is designed for self-supervised learning on pure image data and not ready for generation. Even without text, CogLM is more parameter-efficient because MAE is an encoder-decoder structure.</p><p>A considerable part of parameters in encoder and decoder are learned for the same function, e.g. extracting the basic feature from inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pretraining</head><p>As we have introduced CogLM as a general pretraining framework, in this section, we will describe the details and hyperparameters of our pretrained CogLM.</p><p>Tokenization. We develop a unified tokenizer icetk of Image, Chinese and English. As shown in DebertaV2 <ref type="bibr" target="#b12">[13]</ref>, a large vocabulary (128,000 tokens) benefits. For text, we extract a bilingual vocabulary of 130,000 tokens in icetk and explicitly classify them as Chinese, English, Common or Rare Symbols, so that we can specify the generated language via a sampling mask. The image tokenizer is a 20,000-token first-stage VQ-VAE <ref type="bibr" target="#b25">[26]</ref>, largely following the tokenizer in CogView <ref type="bibr" target="#b2">[3]</ref>.</p><p>Inspired by Esser et al. <ref type="bibr" target="#b6">[7]</ref>, a term of perceptual loss <ref type="bibr" target="#b31">[32]</ref> is added to the reconstruction loss, significantly improving the reconstruction performance.</p><p>Transformer. The backbone of our pretrained CogLM is a Transformer with Sandwich Layer-Norm <ref type="bibr" target="#b2">[3]</ref>. The model has 6 billion parameters (48 layers, hidden size 3072, 48 attention heads) and been trained for 300,000 iterations in FP16 with batch size 4,096. The sequence length is 512, consisting of 400 image tokens, 1 separator and up to 111 text tokens.</p><p>Masking Strategy. We assign 50% percentage for each sampling strategy of mask regions. The analysis from SimMIM <ref type="bibr" target="#b29">[30]</ref> exhibits the great importance of the mask percentage and patch distribution. We follow their results to sample 4 × 4 token patches at random until 75% of the tokens are in the mask regions. For bilingual samples, we randomly choose one of the languages during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hierarchical Generation</head><p>Although the pretrained CogLM can generate images from text, the resolution is only 20 × 20 tokens (160 × 160 pixels). Actually the short sequence is an intentional design for fast generation. The versatility of CogLM allows us to finetune it into super-resolution models. The whole hierarchical pipeline makes up our CogView2 system.</p><p>Direct super-resolution. In this step, we want a model to map a generated low-resolution image token sequence im 0 ∈ [0, 20000) 20×20 to a higher-resolution sequence im 1 ∈ [0, 20000) 60×60 . We finetune the pretrained CogLM into an encoder-decoder architecture. The input of the encoder is the 20 × 20 sequence of generated image tokens, and the input of the decoder is just a 60 × 60 sequence of [MASK]. We do not follow the original transformer <ref type="bibr" target="#b26">[27]</ref> to add a cross-attention layer, instead we make the tokens in the decoder attend both local tokens in decoder and encoder. This cross-resolution local attention is implemented via a customized CUDA kernel introduced in section 4.3. Both encoder and decoder are initialized using the pretrained CogLM. In practice, we find it enough to only finetune the weights of the attention layers in the decoder, so that we can fix and share the other parameters between the encoder and decoder to reduce the memory consumption.</p><p>Although the direct-mapping is a traditional practice for super-resolution, e.g. SRCNN <ref type="bibr" target="#b3">[4]</ref>, it is hardly qualified as generation; it focuses more on texture transformation. The loss function of directmapping is token-based or pixel-based (MAE), meaning that it predicts or maximizes the marginal distribution p(im 1 i |im 0 ) for each token i instead of p(im 1 |im 0 ). As we use the cross-entropy loss and a multinomial sampling during generation, we get</p><formula xml:id="formula_19">im 1 = [im 1 1 , ..., im 1 60×60 ], im 1 i ∼ p θ (im 1 i |im 0 ), im 1 i and im 1 j are independent if i = j.<label>(3)</label></formula><p>Therefore, we need to refine the im 1 using another module.</p><p>Iterative super-resolution. In this step, we aim to refine the initial high-resolution sequence im 1 into a better one im 2 . The working principle of the refinement is to break the independence of the generated tokens, while keep the parallelism. Thus, we propose a local parallel auto-regressive (LoPAR) way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input text:</head><p>A great church.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mask 75%</head><p>Local window details.  The motivation of LoPAR is that the hierarchical process frees us from the global dependence. As long as we maintain 25% -a ratio from MAE <ref type="bibr" target="#b11">[12]</ref> -random tokens as context, it is enough to recover the scene of the image. If the re-generated tokens are coherent locally with the 25% kept tokens, the global coherence is also guaranteed. We mask 75% tokens of the im 1 and assume that there is a local window size σ, p(im<ref type="foot" target="#foot_1">2</ref> i |im 1 ) = p(im 2 i |{im 1 j | dist(i, j) &lt; σ and j is not masked.}), (4)</p><formula xml:id="formula_20">p(im 2 i |im 1 , im 2 j ) = p(im 2 i |im 1 ) if dist(i, j) &gt; σ,<label>(5)</label></formula><p>so that the local attention is sufficient and tokens from different local windows can be generated in parallel. To further increase the parallelism, we find the local inconsistency usually occurs when directly adjacent (vertically or horizontally) tokens are generated at the same time. We factorize the generation process into different iterations diagonally as in Figure <ref type="figure" target="#fig_17">4</ref> and follows,</p><formula xml:id="formula_21">p(im 2 |im 1 ) = 2σ−1 k=0 row(i)+col(i)=k i p(im 2 i |im 1 , {im 2 j | row(j) + col(j) &lt; k}) ,<label>(6)</label></formula><p>where row(i) = i−1 60 mod σ and col(i) = (i − 1) mod σ are the indices of row and column in the local window.</p><p>To implement the iterative super-resolution module, we finetune the pretrained CogLM for 20,000 iterations into a BERT-style masked prediction model on 60 × 60-token sequences with local attention. The mask ratio is sampled from {0.2, 0.4, 0.6, 0.8, 0.9} for each sample. During inference, we set the local window size σ = 6 and compress the iterative process from 2σ − 1 to 6 iterations by arranging the unmasked tokens and merging the first and final iterations 2 .</p><p>4 Plug-in Improved Techniques for Transformers</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Cluster Sampling</head><p>In auto-regressive generation, the sampling strategy over the predicted distribution of the tokens are crucial. Top-k or top-p (nucleus) sampling <ref type="bibr" target="#b13">[14]</ref> are the most common strategies but suffer from an incomplete truncation problem.  The vocabulary of the image tokens are learned by VQVAE <ref type="bibr" target="#b25">[26]</ref>, where the embeddings of some tokens are very similar. To represent the frequent patterns at a finer granularity, we use a large vocabulary of 20,000 tokens, three times larger than that of the previous works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b2">3]</ref>, further exacerbating the situation. For instance, there are about 42 tokens basically "white" in icetk, which show subtle differences only when connected to some other tokens. Although the sum of the probabilities of these "white" token might be large enough, most of them could be filtered by top-k sampling. Figure <ref type="figure" target="#fig_9">5</ref> illustrates the problem.</p><p>To solve the incomplete sampling problem, we propose cluster sampling. We group the 20,000 tokens into 500 clusters via Kmeans <ref type="bibr" target="#b15">[16]</ref> based on their vectors in VQVAE. During sampling, we first sample a cluster using top-k sampling based on the sum of probabilities of tokens in the clusters, and then sample in the cluster. All the tokens within a cluster are treated as a whole and will be filtered or kept together, alleviating the incomplete truncation problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Upweighting Textual Attention</head><p>Most text-image pairs are weakly relevant in the large training data of CogLM. Even the model perfectly fits the data, it should have a considerate probability to generate irrelevant images. To strengthen the relevance, we leverage the explainability of the attention operation. We directly add a constant c to all the attention scores from image tokens to text tokens. This technique costs ignorable time consumption but largely improves the textual relevance of the generated images. In practice, c &lt; 3 will not influence the quality of the images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Local Attention</head><p>Locality is one of the most important properties of image data. Local operations, e.g. convolution, dominated the visual computing before ViTs <ref type="bibr" target="#b4">[5]</ref>. Even attention in the ViTs mainly deals with the interactions between local tokens <ref type="bibr" target="#b21">[22]</ref>. We find it possible to finetune the pretrained CogLM using local attention and textual attention, which is generally compatible with the global attention weights from pretraining. However, 2D local attention cannot be implemented efficiently using high-level framework, e.g. Pytorch <ref type="bibr" target="#b17">[18]</ref>. We develop a customized CUDA kernel to support both 2D local attention, 2D auto-regressive local attention and cross-resolution local attention. In the super-resolution modules, we use local attention with the kernel size of 9 × 9, which is 40× faster and consumes 1% memory than global attention on 4,096 sequence with hidden size 64 per head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset</head><p>Our dataset for pretraining contains about 30 million text-image pairs, mostly overlapped with that of CogView <ref type="bibr" target="#b2">[3]</ref>. We filter about 5 million text-image pairs from the CogView dataset with some keywords, e.g. "abstract" and "texture", because they are mostly background images used for design. These images consist of repeated patterns and contribute little to text-to-image generation. We then replenish the dataset with 5 million tag-image pairs. About half of the text are translated from English, and both Chinese and English text are kept to train our bilingual CogLM. Only the images whose resolution is at least 480 × 480 are used to train the super-resolution modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Machine Evaluation</head><p>To compare with the previous and concurrent works, we follow the most popular benchmark originated from DALL-E <ref type="bibr" target="#b23">[24]</ref>, Fréchet Inception Distances and Inception Scores evaluated on MS-COCO <ref type="bibr" target="#b14">[15]</ref>. 30,000 captions from validation set are sampled to evaluate the FID. Since each image in COCO has up to 5 different captions, we carefully select the sampled captions to describe different images. We generate 16 samples for each caption (translated into Chinese), and select the best one with the lowest caption perplexity (the Caption Score in <ref type="bibr" target="#b2">[3]</ref>). Note that FID is not the perfect metric to evaluate CogView2 because (1) the advantage of CogView2 is to generate high-resolution images, but we need to resize the images back to 256 × 256 for meaningful comparison. (2) There are mistakes when translating English captions into Chinese. <ref type="bibr" target="#b2">(3)</ref> Our training data contain many single-object images, which are quite different with the distribution of COCO (common objects in context).</p><p>The results of machine evaluation are demonstrated in Table <ref type="table" target="#tab_0">1</ref>. We find that finetuning CogLM on MS-COCO dataset will largely improve the FID. During our finetuning, we witness the FID reducing from 24.0 (0 iteration)→ 19.2 (2,500 iterations) → 17.7 (5,000 iterations). However, we find that the quality (human evaluation) of generation deteriorates. Though the style is more similar to COCO, the generation is not as accurate as the non-finetuned version. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Auto-regressive or Diffusion? Although GPTs achieved great success in text generation, diffusion models become increasingly popular in image generation. Here we compare diffusion models with auto-regressive models from the aspect of speed, the largest disadvantage of the auto-regressive models discussed in the section 1. With the same architecture, diffusion models require more FLOPs but have a high degree of parallelism. They can also make a trade-off between the quality and time consumption by manually scheduling the stride of sampling. For example, Glide <ref type="bibr" target="#b16">[17]</ref> samples 250 diffusion steps for evaluation, and 27 steps for interactive sampling to reduce the latency to 15s. Auto-regressive models must generate the image token-by-token, but our LoPAR can upsample the image with a high parallelism degree, so that (potentially) we can reduce the time cost by introducing more hierarchies to design models much faster than diffusion models.</p><p>Comparison between DALL-E-2 and CogView2. DALL-E-2 is a recently released work for textto-image generation on 1024 × 1024 resolution. Although its probabilistic model and architecture are quite different from CogView2, they share the same spirit -hierarchical generation. Its quality gain over CogView2 is mainly originated from a third-level super-resolution and a "zeroth"-level image prior generation. Moreover, DALL-E-2 is trained on 650M text-image pairs, about 20× the size of CogView2, which might also influence the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>The breakthrough in the text-to-image domain are made by auto-regressive models. However, the slow generation and high complexity hinder researchers from improving the quality in this direction.</p><p>In this paper, we put forward the way of hierarchical transformers to help auto-regressive models conquer these disadvantages, and bridge the gap between text-to-image pretraining and recent visual representation learning methods, e.g. MAE <ref type="bibr" target="#b11">[12]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Text-to-Image samples from CogView2, which supports both Chinese and English. The actual input text is in Chinese, translated into English here for better understanding. Codes and demo website will be updated at https://github.com/THUDM/CogView2.</figDesc><graphic url="image-8.png" coords="1,207.31,593.06,94.99,94.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>&lt; l a t e x i t s h a 1 _</head><label>1</label><figDesc>b a s e 6 4 = " W x m w / E M J s Z Z s s M A 9 Y 5 C v R F 7 2 / d I = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G P R i 8 e K 9 g P a U D b b S b t 0 s w m 7G 7 G E / g Q v H h T x 6 i / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S A T X x n W / n c L K 6 t r 6 R n G z t L W 9 s 7 t X 3 j 9 o 6 j h V D B s s F r F q B 1 S j 4 B I b h h u B 7 U Q h j Q K B r W B 0 M / V b j 6 g 0 j + W D G S f o R 3 Q g e c g Z N V a 6 f + q 5 v X L F r b o z k G X i 5 a Q C O e q 9 8 l e 3 H 7 M 0 Q m m Y o F p 3 P D c x f k a V 4 U z g p N R N N S a U j e g A O 5 Z K G q H 2 s 9 m p E 3 J i l T 4 J Y 2 V L G j J T f 0 9 k N N J 6 H A W 2 M 6 J m q B e 9 q f i f 1 0 l N e O V nX C a p Q c n m i 8 J U E B O T 6 d + k z x U y I 8 a W U K a 4 v Z W w I V W U G Z t O y Y b g L b 6 8 T J p n V e + i 6 t 2 d V 2 r X e R x F O I J j O A U P L q E G t 1 C H B j A Y w D O 8 w p s j n B f n 3 f m Y t x a c f O Y Q / s D 5 / A E M J I 2 l &lt; / l a t e x i t &gt; x 0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r J O A y / 5 9 q N k G P b V t P K V u T / I n E O k = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G P R i 8 e K 9 g P a U D b b T b t 0 s w m 7 E 7 G E / g Q v H h T x 6 i / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S K Q w 6 L r f T m F l d W 1 9 o 7 h Z 2 t r e 2 d 0 r 7 x 8 0 T Z x q x h s s l r F u B 9 R w K R R v o E D J 2 4 n m N A o k b w W j m 6 n f e u T a i F g 9 4 D j h f k Q H S o S C U b T S / V P P 6 5 U r b t W d g S w T L y c V y F H v l b + 6 /</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 &lt;</head><label>1</label><figDesc>y B W b L w p T S T A m 0 7 9 J X 2 j O U I 4 t o U w L e y t h Q 6 o p Q 5 t O y Y b g L b 6 8 T J p n V e + i 6 t 2 d V 2 r X e R x F O I J j O A U P L q E G t 1 C H B j A Y w D O 8 w p s j n R f n 3 f m Y t x a c f O Y Q / s D 5 / A E N q I 2 m &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " y J q Z U l w 9 d z F l w 0 h G A O H e W j A 4 i c w = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k k R 9 V j 0 4 r G i / Y A 2 l M 1 2 0 y 7 d b M L u R C y h P 8 G L B 0 W 8 + o u 8 + W / c t j l o 6 4 O B x 3 s z z M w L E i k M u u 6 3 s 7 K 6 t r 6 x W d g q b u / s 7 u 2 X D g 6 b J k 4 1 4 w 0 W y 1 i 3 A 2 q 4 F I o 3 U K D k 7 U R z G g W S t 4 L R z d R v P X J t R K w e c J x w P 6 I D J U L B K F r p / q l X 7 Z X K b s W d g S w T L y d l y F H v l b 6 6 /</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 &lt;</head><label>2</label><figDesc>y B W b L w p T S T A m 0 7 9 J X 2 j O U I 4 t o U w L e y t h Q 6 o p Q 5 t O 0 Y b g L b 6 8 T J r V i n d R 8 e 7 O y 7 X r P I 4 C H M M J n I E H l 1 C D W 6 h D A x g M 4 B l e 4 c 2 R z o v z 7 n z M W 1 e c f O Y I / s D 5 / A E P L I 2 n &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " o 9 F h R 2 D A Z S o X b C z u 0 m 5 s S F L h y E s = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l U 1 G P R i 8 e K 9 g P a U D b b S b t 0 s w m 7 G 7 G E / g Q v H h T x 6 i / y 5 r 9 x 2 + a g 1 Q c D j / d m m J k X J I J r 4 7 p f T m F p e W V 1 r b h e 2 t j c 2 t 4 p 7 + 4 1 d Z w q h g 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3 &lt; 4 &lt; 5 &lt; 6 &lt; 7 &lt; 8</head><label>345678</label><figDesc>r F K 7 y u M o w g E c w j F 4 c A E 1 u I E 6 N I D B A J 7 g B V 4 d 4 T w 7 b 8 7 7 v L X g 5 D P 7 8 A v O x z c Q s I 2 o &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " O p A X x 6 Y k p G G / 9 c 9 2 C w u C Z L P c L U I = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G P R i 8 e K 9 g P a U D b b T b t 0 s w m 7 E 7 G E / g Q v H h T x 6 i / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S K Q w 6 L r f T m F l d W 1 9 o 7 h Z 2 t r e 2 d 0 r 7 x 8 0 T Z x q x h s s l r F u B 9 R w K R R v o E D J 2 4 n m N A o k b w W j m 6 n f e u T a i F g 9 4 D j h f k Q H S o S C U b T S / V P v v F e u u F V 3 B r J M v J x U I E e 9 V / 7 q 9 m O W R l w h k 9 S Y j u c m 6 G d U o 2 C S T 0 r d 1 P C E s h E d 8 I 6 l i k b c + N n s 1 A k 5 s U q f h L G 2 p Z D M 1 N 8 T G Y 2 M G U e B 7 Y w o D s 2 i N x X / 8 z o p h l d + J l S S I l d s v i h M J c G Y T P 8 m f a E 5 Q z m 2 h D I t 7 K 2 E D a m m D G 0 6 J R u C t / j y M m m e V b 2 L q n d 3 X q l d 5 3 E U 4 Q i O 4 R Q 8 u I Q a 3 E I d G s B g A M / w C m + O d F 6 c d + d j 3 l p w 8 p l D + A P n 8 w c S N I 2 p &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " N o i t s K 8 o V 4 r Z t R D S p p g u G u D y F n g = " &gt; A A A B 6 n i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K r 2 P Q i 8 e I 5 g HJ E m Y n v c m Q 2 d l l Z l Y M S z 7 B i w d F v P p F 3 v w b J 8 k e N F r Q U F R 1 0 9 0 V J I J r 4 7 p f T m F p e W V 1 r b h e 2 t j c 2 t 4 p 7 + 4 1 d Z w q h g 0 W i 1 i 1 A 6 p R c I k N w 4 3 A d q K Q R o H A V j C 6 n v q t B 1 S a x / L e j B P 0 I z q Q P O S M G i v d P f b O e u W K W 3 V n I H + J l 5 M K 5 K j 3 y p / d f s z S C K V h g m r d 8 d z E + B l V h j O B k 1 I 3 1 Z h Q N q I D 7 F g q a Y T a z 2 a n T s i R V f o k j J U t a c h M / T m R 0 U j r c R T Y z o i a o V 7 0 p u J / X i c 1 4 a W f c Z m k B i W b L w p T Q U x M p n + T P l f I j B h b Q p n i 9 l b C h l R R Z m w 6 J R u C t / j y X 9 I 8 q X r n V e /2 t F K 7 y u M o w g E c w j F 4 c A E 1 u I E 6 N I D B A J 7 g B V 4 d 4 T w 7 b 8 7 7 v L X g 5 D P 7 8 A v O x z c T u I 2 q &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " X C + 4 / N K m Q q I H k Z U 3 X z l C 1 n K K w / M = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o l I 9 V j 0 4 r G i / Y A 2 l M 1 2 0 y 7 d b M L u R C y h P 8 G L B 0 W 8 + o u 8 + W / c t j l o 6 4 O B x 3 s z z M w L E i k M u u 6 3 s 7 K 6 t r 6 x W d g q b u / s 7 u 2 X D g 6 b J k 4 1 4 w 0 W y 1 i 3 A 2 q 4 F I o 3 U K D k 7 U R z G g W S t 4 L R z d R v P X J t R K w e c J x w P 6 I D J U L B K F r p / q l X 7 Z X K b s W d g S w T L y d l y F H v l b 6 6 / Z i l E V f I J D W m 4 7 k J + h n V K J j k k 2 I 3 N T y h b E Q H v G O p o h E 3 f j Y 7 d U J O r d I n Y a x t K S Q z 9 f d E R i N j x l F g O y O K Q 7 P o T c X / v E 6 K 4 Z W f C Z W k y B W b L w p T S T A m 0 7 9 J X 2 j O U I 4 t o U w L e y t h Q 6 o p Q 5 t O 0 Y b g L b 6 8 T J r n F a 9 a 8 e 4 u y r X r P I 4 C H M M J n I E H l 1 C D W 6 h D A x g M 4 B l e 4 c 2 R z o v z 7 n z M W 1 e c f O Y I / s D 5 / A E V P I 2 r &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " I i 4 v K Y p P s B Q p Y W W F k R 6 w U 7 t K V m s = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o m I 9 V j 0 4 r G i / Y A 2 l M 1 2 0 y 7 d b M L u R C y h P 8 G L B 0 W 8 + o u 8 + W / c t j l o 6 4 O B x 3 s z z M w L E i k M u u 6 3 s 7 K 6 t r 6 x W d g q b u / s 7 u 2 X D g 6 b J k 4 1 4 w 0 W y 1 i 3 A 2 q 4 F I o 3 U K D k 7 U R z G g W S t 4 L R z d R v P X J t R K w e c J x w P 6 I D J U L B K F r p / q l X 7 Z X K b s W d g S w T L y d l y F H v l b 6 6 /Z i l E V f I J D W m 4 7 k J + h n V K J j k k 2 I 3 N T y h b E Q H v G O p o h E 3 f j Y 7 d U J O r d I n Y a x t K S Q z 9 f d E R i N j x l F g O y O K Q 7 P o T c X / v E 6 K 4 Z W f C Z W ky B W b L w p T S T A m 0 7 9 J X 2 j O U I 4 t o U w L e y t h Q 6 o p Q 5 t O 0 Y b g L b 6 8 T J r n F e + y 4 t 1 d l G v X e R w F O I Y T O A M P q l C D W 6 h D A x g M 4 B l e 4 c 2 R z o v z 7 n z M W 1 e c f O Y I / s D 5 / A E W w I 2 s &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " R 9 s X I a f g y D f V + T U X 6 o G I m e 1 v d C E = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o m I 9 l j 0 4 r G i / Y A 2 l M 1 2 0 y 7 d b M L u R C y h P 8 G L B 0 W 8 + o u 8 + W / c t j l o 6 4 O B x 3 s z z M w L E i k M u u 6 3 s 7 K 6 t r 6 x W d g q b u / s 7 u 2 X D g 6 b J k 4 1 4 w 0 W y 1 i 3 A 2 q 4 F I o 3 U K D k 7 U R z G g W S t 4 L R z d R v P X J t R K w e c J x w P 6 I D J U L B K F r p / q l X 7 Z X K b s W d g S w T L y d l y F H v l b 6 6 / Z i l E V f I J D W m 4 7 k J + h n V K J j k k 2 I 3 N T y h b E Q H v G O p o h E 3 f j Y 7 d U J O r d I n Y a x t K S Q z 9 f d E R i N j x l F g O y O K Q 7 P o T c X / v E 6 K Y d X P h E p S 5 I r N F 4 W p J B i T 6 d + k L z R n K M e W U K a F v Z W w I d W U o U 2 n a E P w F l 9 e J s 3 z i n d Z 8 e 4 u y r X r P I 4 C H M M J n I E H V 1 C D W 6 h D A x g M 4 B l e 4 c 2 R z o v z 7 n z M W 1 e c f O Y I / s D 5 / A E Y R I 2 t &lt; / l a t e x i t &gt; x Transformer … &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o 9 F h R 2 D A Z S o X b C z u 0 m 5 s S F L h y E s = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l U 1 G P R i 8 e K 9 g P a U D b b S b t 0 s w m 7 G 7 G E / g Q v H h T x 6 i / y 5 r 9 x 2 + a g 1 Q c D j / d m m J k X J I J r 4 7 p f T m F p e W V 1 r b h e 2 t j c 2 t 4 p 7 + 4 1 d Z w q h g 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>3 &lt;</head><label>3</label><figDesc>r F K 7 y u M o w g E c w j F 4 c A E 1 u I E 6 N I D B A J 7 g B V 4 d 4 T w 7 b 8 7 7 v L X g 5 D P 7 8 A v O x z c Q s I 2 o &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " O p A X x 6 Y k p G G / 9 c 9 2 C w u C Z L P c L</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>s B g A M / w C m + O d F 6 c d + d j 3 l p w 8 p l D + A P n 8 w c S N I 2 p &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>2 &lt;</head><label>2</label><figDesc>y B W b L w p T S T A m 0 7 9 J X 2 j O U I 4 t o U w L e y t h Q 6 o p Q 5 t O 0 Y b g L b 6 8 T J r V i n d R 8 e 7 O y 7 X r P I 4 C H M M J n I E H l 1 C D W 6 h D A x g M 4 B l e 4 c 2 R z o v z 7 n z M W 1 e c f O Y I / s D 5 / A E P L I 2 n &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " o 9 F h R 2 D A Z S o X b C z u 0 m 5 s S F L h y E s = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l U 1 G P R i 8 e K 9 g P a U D b b S b t 0 s w m 7 G 7 G E / g Q v H h T x 6 i / y 5 r 9 x 2 + a g 1 Q c D j / d m m J k X J I J r 4 7 p f T m F p e W V 1 r b h e 2 t j c 2 t 4 p 7 + 4 1 d Z w q h</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>3 &lt; 4 &lt;</head><label>34</label><figDesc>r F K 7 y u M o w g E c w j F 4 c A E 1 u I E 6 N I D B A J 7 g B V 4 d 4 T w 7 b 8 7 7 v L X g 5 D P 7 8 A v O x z c Q s I 2 o &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " O p A X x 6 Y k p G G / 9 c 9 2 C w u C Z L P c L U I = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G P R i 8 e K 9 g P a U D b b T b t 0 s w m 7 E 7 G E / g Q v H h T x 6 i / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S K Q w 6 L r f T m F l d W 1 9 o 7 h Z 2 t r e 2 d 0 r 7 x 8 0 T Z x q x h s s l r F u B 9 R w K R R v o E D J 2 4 n m N A o k b w W j m 6 n f e u T a i F g 9 4 D j h f k Q H S o S C U b T S / V P v v F e u u F V 3 B r J M v J x U I E e 9 V / 7 q 9 m O W R l w h k 9 S Y j u c m 6 G d U o 2 C S T 0 r d 1 P C E s h E d 8 I 6 l i k b c + N n s 1 A k 5 s U q f h L G 2 p Z D M 1 N 8 T G Y 2 M G U e B 7 Y w o D s 2 i N x X / 8 z o p h l d + J l S S I l d s v i h M J c G Y T P 8 m f a E 5 Q z m 2 h D I t 7 K 2 E D a m m D G 0 6 J R u C t / j y M m m e V b 2 L q n d 3 X q l d 5 3 E U 4 Q i O 4 R Q 8 u I Q a 3 E I d G s B g A M / w C m + O d F 6 c d + d j 3 l p w 8 p l D + A P n 8 w c S N I 2 p &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " N o i t s K 8 o V 4 r Z t R D S p p g u G u D y F n g = " &gt; A A A B 6 n i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K r 2 P Q i 8 e I 5 g H J E m Y n v c m Q 2 d l l Z l Y M S z 7 B i w d F v P p F 3 v w b J 8 k e N F r Q U F R 1 0 9 0 V J I J r 4 7 p f T m F p e W V 1 r b h e 2 t j c 2 t 4 p 7 + 4 1 d Z w q h</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>5 &lt;</head><label>5</label><figDesc>2 t F K 7 y u M o w g E c w j F 4 c A E 1 u I E 6 N I D B A J 7 g B V 4 d 4 T w 7 b 8 7 7 v L X g 5 D P 7 8 A v O x z c T u I 2 q &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " X C + 4 / N K m Q q I H k Z U 3 X z l C 1 n K K w / M = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o l I 9 V j 0 4 r G i / Y A 2 l M 1 2 0 y 7 d b M L u R C y h P 8 G L B 0 W 8 + o u 8 + W / c t j l o 6 4 O B x 3 s z z M w L E i k M u u 6 3 s 7 K 6 t r 6 x W d g q b u / s 7 u 2 X D g 6 b J k 4 1 4 w 0 W y 1 i 3 A 2 q 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>6 &lt; 7 &lt; 8 &lt;</head><label>678</label><figDesc>y B W b L w p T S T A m 0 7 9 J X 2 j O U I 4 t o U w L e y t h Q 6 o p Q 5 t O 0 Y b g L b 6 8 T J r n F a 9 a 8 e 4 u y r X r P I 4 C H M M J n I E H l 1 C D W 6 h D A x g M 4 B l e 4 c 2 R z o v z 7 n z M W 1 e c f O Y I / s D 5 / A E V P I 2 r &lt; / l a t e x i t &gt;x l a t e x i t s h a 1 _ b a s e 6 4 = " I i 4v K Y p P s B Q p Y W W F k R 6 w U 7 t K V m s = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o m I 9 V j 0 4 r G i / Y A 2 l M 1 2 0 y 7 d b M L u R C y h P 8 G L B 0 W 8 + o u 8 + W / c t j l o 6 4 O B x 3 s z z M w L E i k M u u 6 3 s 7 K 6 t r 6 x W d g q b u / s 7 u 2 X D g 6 b J k 4 1 4 w 0 W y 1 i 3 A 2 q 4 F I o 3 U K D k 7 U R z G g W S t 4 L R z d R v P X J t R K w e c J x w P 6 I D J U L B K F r p / q l X 7 Z X K b s W d g S w T L y d l y F H v l b 6 6 / Z i l E V f I J D W m 4 7 k J + h n V K J j k k 2 I 3 N T y h b E Q H v G O p o h E 3 f j Y 7 d U J O r d I n Y a x t K S Q z 9 f d E R i N j x l F g O y O K Q 7 P o T c X / v E 6 K 4 Z W f C Z W k y B W b L w p T S T A m 0 7 9 J X 2 j O U I 4 t o U w L e y t h Q 6 o p Q 5 t O 0 Y b g L b 6 8 T J r n F e + y 4 t 1 d l G v X e R w F O I Y T O A M P q l C D W 6 h D A x g M 4 Bl e 4 c 2 R z o v z 7 n z M W 1 e c f O Y I / s D 5 / A E W w I 2 s &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " R 9 s X I a f g y D f V + T U X 6 o G I m e 1 v d C E = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o m I 9 l j 0 4 r G i / Y A 2 l M 1 2 0 y 7 d b M L u R C y h P 8 G L B 0 W 8 + o u 8 + W / c t j l o 6 4 O B x 3 s z z M w L E i k M u u 6 3 s 7 K 6 t r 6 x W d g q b u / s 7 u 2 X D g 6 b J k 4 1 4 w 0 W y 1 i 3 A 2 q 4 F I o 3 U K D k 7 U R z G g W S t 4 L R z d R v P X J t R K w e c J x w P 6 I D J U L B K F r p / q l X 7 Z X K b s W d g S w T L y d l y F H v l b 6 6 /Z i l E V f I J D W m 4 7 k J + h n V K J j k k 2 I 3 N T y h b E Q H v G O p o h E 3 f j Y 7 d U J O r d I n Y a x t K S Q z 9 f d E R i N j x l F g O y O K Q 7 P o T c X / v E 6 K Y d X P h E p S 5 I r N F 4 W p J B i T 6 d + k L z R n K M e W U K a F v Z W w I d W U o U2 n a E P w F l 9 e J s 3 z i n d Z 8 e 4 u y r X r P I 4 C H M M J n I E H V 1 C D W 6 h D A x g M 4 B l e 4 c 2 R z o v z 7 n z M W 1 e c f O Y I / s D 5 / A E Y R I 2 t &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " W x m w / E M J s Z Z s s M A 9 Y 5 C v R F 7 2 / d I = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G P R i 8 e K 9 g P a U D b b S b t 0 s w m 7 G 7 G E / g Q v H h T x 6 i / y 5 r 9 x 2 + a g r Q8 G H u / N M D M v S A T X x n W / n c L K 6 t r 6 R n G z t L W 9 s 7 t X 3 j 9 o 6 j h V D B s s F r F q B 1 S j 4 B I b h h u B 7 U Q h j Q K B r W B 0 M / V b j 6 g 0 j + W D G S f o R 3 Q g e c g Z N V a 6 f + q 5 v X L F r b o z k G X i 5 a Q C O e q 9 8 l e 3 H 7 M 0 Q m m Y o F p 3 P D c x f k a V 4 U z g p N R N N S a U j e g A O 5 Z K G q H 2 s 9 m p E 3 J i l T 4 J Y 2 V L G j J T f 0 9 k N N J 6 H A W 2 M 6 J m q B e 9 q f i f 1 0 l N e O V nX C a p Q c n m i 8 J U E B O T 6 d + k z x U y I 8 a W U K a 4 v Z W w I V W U G Z t O y Y b g L b 6 8 T J p n V e + i 6 t 2 d V 2 r X e R x F O I J j O A U P L q E G t 1 C H B j A Y w D O 8 w p s j n B f n 3 f m Y t x a c f O Y Q / s D 5 / A E M J I 2 l &lt; / l a t e x i t &gt; x 0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r J O A y / 5 9 q N k G P b V t P K V u T / I n E O k = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G P R i 8 e K 9 g P a U D b b T b t 0 s w m 7 E 7 G E / g Q v H h T x 6 i / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S K Q w 6 L r f T m F l d W 1 9 o 7 h Z 2 t r e 2 d 0 r 7 x 8 0 T Z x q x h s s l r F u B 9 R w K R R v o E D J 2 4 n m N A o k b w W j m 6 n f e u T a i F g 9 4 D j h f k Q H S o S C U b T S / V P P 6 5 U r b t W d g S w T L y c V y F H v l b + 6 /</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>1 &lt; 2 &lt; 3 &lt; 4 &lt; 5 &lt; 6 &lt;</head><label>123456</label><figDesc>y B W b L w p T S T A m 0 7 9 J X 2 j O U I 4 t o U w L e y t h Q 6 o p Q 5 t O y Y b g L b 6 8 T J p n V e + i 6 t 2 d V 2 r X e R x F O I J j O A U P L q E G t 1 C H B j A Y w D O 8 w p s j n R f n 3 f m Y t x a c f O Y Q / s D 5 / A E N q I 2 m &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " y J q Z U l w 9 d z F l w 0 h G A O H e W j A 4 i c w = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k k R 9 V j 0 4 r G i / Y A 2 l M 1 2 0 y 7 d b M L u R C y h P 8 G L B 0 W 8 + o u 8 + W / c t j l o 6 4 O B x 3 s z z M w L E i k M u u 6 3 s 7 K 6 t r 6 x W d g q b u / s 7 u 2 X D g 6 b J k 4 1 4 w 0 W y 1 i 3 A 2 q 4F I o 3 U K D k 7 U R z G g W S t 4 L R z d R v P X J t R K w e c J x w P 6 I D J U L B K F r p / q l X 7 Z X K b s W d g S w T L y d l y F H v l b 6 6 / Z i l E V f I J D W m 4 7 k J + h n V K J j k k 2 I 3 N T y h b E Q H v G O p o h E 3 f j Y 7 d U J O r d I n Y a x t K S Q z 9 f d E R i N j x l F g O y O K Q 7 P o T c X / v E 6 K 4 Z W f C Z W ky B W b L w p T S T A m 0 7 9 J X 2 j O U I 4 t o U w L e y t h Q 6 o p Q 5 t O 0 Y b g L b 6 8 T J r V i n d R 8 e 7 O y 7 X r P I 4 C H M M J n I E H l 1 C D W 6 h D A x g M 4 B l e 4 c 2 R z o v z 7 n z M W 1 e c f O Y I / s D 5 / A E P L I 2 n &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " o 9 F h R 2 D A Z S o X b C z u 0 m 5 s S F L h y E s = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l U 1 G P R i 8 e K 9 g P a U D b b S b t 0 s w m 7G 7 G E / g Q v H h T x 6 i / y 5 r 9 x 2 + a g 1 Q c D j / d m m J k X J I J r 4 7 p f T m F p e W V 1 r b h e 2 t j c 2 t 4 p 7 + 4 1 d Z w q h g 0 W i 1 i 1 A 6 p R c I k N w 4 3 A d q K Q R o H A V j C 6 n v q t B 1 S a x / L e j B P 0 I z q Q P O S M G i v d P f Z O e + W K W 3 V n I H + J l 5 M K 5 K j 3 y p / d f s z S C K V h g m r d 8 d z E + B l V h j O B k 1 I 3 1 Z h Q N q I D 7 F g q a Y T a z 2 a n T s i R V f o k j J U t a c h M / T m R 0 U j r c R T Y z o i a o V 7 0 p u J / X i c 1 4 a W f c Z m k B i W b L w p T Q U x M p n + T P l f I j B h b Q p n i 9 l b C h l R R Z m w 6 J R u C t / j y X 9 I 8 q X r n V e / 2r F K 7 y u M o w g E c w j F 4 c A E 1 u I E 6 N I D B A J 7 g B V 4 d 4 T w 7 b 8 7 7 v L X g 5 D P 7 8 A v O x z c Q s I 2 o &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " O p A X x 6 Y k p G G / 9 c 9 2 C w u C Z L P c L U I = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G P R i 8 e K 9 g P a U D b b T b t 0 s w m 7 E 7 G E / g Q v H h T x 6 i / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S K Q w 6 L r f T m F l d W 1 9 o 7 h Z 2 t r e 2 d 0 r 7 x 8 0 T Z x q x h s s l r F u B 9 R w K R R v o E D J 2 4 n m N A o k b w W j m 6 n f e u T a i F g 9 4 D j h f k Q H S o S C U b T S / V P v v F e u u F V 3 B r J M v J x U I E e 9 V / 7 q 9 m O W R l w h k 9 S Y j u c m 6 G d U o 2 C S T 0 r d 1 P C E s h E d 8 I 6 l i k b c + N n s 1 A k 5 s U q f h L G 2 p Z D M 1 N 8 T G Y 2 M G U e B 7 Y w o D s 2 i N x X / 8 z o p h l d + J l S S I l d s v i h M J c G Y T P 8 m f a E 5 Q z m 2 h D I t 7 K 2 E D a m m D G 0 6 J R u C t / j y M m m e V b 2 L q n d 3 X q l d 5 3 E U 4 Q i O 4 R Q 8 u I Q a 3 E I d G s B g A M / w C m + O d F 6 c d + d j 3 l p w 8 p l D + A P n 8 w c S N I 2 p &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " N o i t s K 8 o V 4 r Z t R D S p p g u G u D y F n g = " &gt; A A A B 6 n i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K r 2 P Q i 8 e I 5 g H J E m Y n v c m Q 2 d l l Z l Y M S z 7 B i w d F v P p F 3 v w b J 8 k e N F r Q U F R 1 0 9 0 V J I J r 4 7 p f T m F p e W V 1 r b h e 2 t j c 2 t 4 p 7 + 4 1 d Z w q h g 0W i 1 i 1 A 6 p R c I k N w 4 3 A d q K Q R o H A V j C 6 n v q t B 1 S a x / L e j B P 0 I z q Q P O S M G i v d P f b O e u W K W 3 V n I H + J l 5 M K 5 K j 3 y p / d f s z S C K V h g m r d 8 d z E + B l V h j O B k 1 I 3 1 Z h Q N q I D 7 F g q a Y T a z 2 a n T s i R V f o k j J U t a c h M / T m R 0 U j r c R T Y z o i a o V 7 0 p u J / X i c 1 4 a W f c Z m k B i W b L w p T Q U x M p n + T P l f I j B h b Q p n i 9 l b C h l R R Z m w 6 J R u C t / j y X 9 I 8 q X r n V e /2 t F K 7 y u M o w g E c w j F 4 c A E 1 u I E 6 N I D B A J 7 g B V 4 d 4 T w 7 b 8 7 7 v L X g 5 D P 7 8 A v O x z c T u I 2 q &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " X C + 4 / N K m Q q I H k Z U 3 X z l C 1 n K K w / M = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o l I 9 V j 0 4 r G i / Y A 2 l M 1 2 0 y 7 d b M L u R C y h P 8 G L B 0 W 8 + o u 8 + W / c t j l o 6 4 O B x 3 s z z M w L E i k M u u 6 3 s 7 K 6 t r 6 x W d g q b u / s 7 u 2 X D g 6 b J k 4 1 4 w 0 W y 1 i 3 A 2 q 4 F I o 3 U K D k 7 U R z G g W S t 4 L R z d R v P X J t R K w e c J x w P 6 I D J U L B K F r p / q l X 7 Z X K b s W d g S w T L y d l y F H v l b 6 6 / Z i l E V f I J D W m 4 7 k J + h n V K J j k k 2 I 3 N T y h b E Q H v G O p o h E 3 f j Y 7 d U J O r d I n Y a x t K S Q z 9 f d E R i N j x l F g O y O K Q 7 P o T c X / v E 6 K 4 Z W f C Z W k y B W b L w p T S T A m 0 7 9 J X 2 j O U I 4 t o U w L e y t h Q 6 o p Q 5 t O 0 Y b g L b 6 8 T J r n F a 9 a 8 e 4 u y r X r P I 4 C H M M J n I E H l 1 C D W 6 h D A x g M 4 B l e 4 c 2 R z o v z 7 n z M W 1 e c f O Y I / s D 5 / A E V P I 2 r &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " I i 4 v K Y p P s B Q p Y W W F k R 6 w U 7 t K V m s = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o m I 9 V j 0 4 r G i / Y A 2 l M 1 2 0 y 7 d b M L u R C y h P 8 G L B 0 W 8 + o u 8 + W / c t j l o 6 4 O B x 3 s z z M w L E i k M u u 6 3 s 7 K 6 t r 6 x W d g q b u / s 7 u 2 X D g 6 b J k 4 1 4 w 0 W y 1 i 3 A 2 q 4 F I o 3 U K D k 7 U R z G g W S t 4 L R z d R v P X J t R K w e c J x w P 6 I D J U L B K F r p / q l X 7 Z X K b s W d g S w T L y d l y F H v l b 6 6 /</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>7 &lt;</head><label>7</label><figDesc>y B W b L w p T S T A m 0 7 9 J X 2 j O U I 4 t o U w L e y t h Q 6 o p Q 5 t O 0 Y b g L b 6 8 T J r n F e + y 4 t 1 d l G v X e R w F O I Y T O A M P q l C D W 6 h D A x g M 4 B l e 4 c 2 R z o v z 7 n z M W 1 e c f O Y I / s D 5 / A E W w I 2 s &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " R 9 s X I a f g y D f V + T U X 6 o G I m e 1 v d C E = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o m I 9 l j 0 4 r G i / Y A 2 l M 1 2 0 y 7 d b M L u R C y h P 8 G L B 0 W 8 + o u 8 + W / c t j l o 6 4 O B x 3 s z z M w L E i k M u u 6 3 s 7 K 6 t r 6 x W d g q b u / s 7 u 2 X D g 6 b J k 4 1 4 w 0 W y 1 i 3 A 2 q 4 F I o 3 U K D k 7 U R z G g W S t 4 L R z d R v P X J t R K w e c J x w P 6 I D J U L B K F r p / q l X 7 Z X K b s W d g S w T L y d l y F H v l b 6 6 /</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>8 Cat</head><label>8</label><figDesc>2 n a E P w F l 9 e J s 3 z i n d Z 8 e 4 u y r X r P I 4 C H M M J n I E H V 1 C D W 6 h D A x g M 4 B l e 4 c 2 R z o v z 7 n z M W 1 e c f O Y I / s D 5 / A E Y R I 2 t &lt; / l a t e x i t &gt; x t e x i t s h a 1 _ b a s e 6 4 = " W k m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: CogLM. (Left) The sequence consists of both text and image tokens. [BOI] (Begin-Of-Image) is the separator token. The mask regions are sampled according to different strategies. Only the second to last tokens in the mask regions are predicted to compute the loss. (Right) The mask will not change the input sequence, but change the attention map, where rows and columns of all the masked tokens together form a low-triangle attention mask matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Image Infilling of CogLM. Tokens (viewed as patches) in light green mean mask regions.</figDesc><graphic url="image-42.png" coords="4,389.00,487.77,110.49,110.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>tokens) Iterative super-resolution. All the local windows generate simultaneously.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Super-resolution modules. The low-resolution images are mapped into high-resolution images via the direct super-resolution module. In each snapshot during the iterative super-resolution, the tokens in the same color are generated at the same time. All the local windows work in parallel.</figDesc><graphic url="image-43.png" coords="6,347.60,118.19,233.00,234.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>Truncation with top-k sampling pTokens sorted by probabilities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: (Best viewed in color.) Incomplete truncation. The same color indicates very similar embeddings of the tokens. The hard truncation of top-k sampling twists the proportion between blue, green and red tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Machine Evaluation Results on MS-COCO. (Downsampling CogView2 images to 256×256.) * means finetuning on MS-COCO.</figDesc><table><row><cell>Model</cell><cell cols="5">FID-0 FID-1 FID-2 FID-4 FID-8</cell><cell>IS</cell></row><row><cell>AttnGAN*</cell><cell>35.2</cell><cell>44.0</cell><cell>72.0</cell><cell cols="3">108.0 100.0 23.3</cell></row><row><cell>DM-GAN*</cell><cell>26.0</cell><cell>39.0</cell><cell>73.0</cell><cell cols="3">119.0 112.3 32.2</cell></row><row><cell>DF-GAN*</cell><cell>26.0</cell><cell>33.8</cell><cell>55.9</cell><cell>91.0</cell><cell>97.0</cell><cell>18.7</cell></row><row><cell>DALL-E</cell><cell>27.5</cell><cell>28.0</cell><cell>45.5</cell><cell>83.5</cell><cell>85.0</cell><cell>17.9</cell></row><row><cell>CogView</cell><cell>27.1</cell><cell>19.4</cell><cell>13.9</cell><cell>19.4</cell><cell>23.6</cell><cell>18.2</cell></row><row><cell>XMC-GAN*</cell><cell>9.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>30.5</cell></row><row><cell>NVWA*</cell><cell>12.9</cell><cell>13.8</cell><cell>15.7</cell><cell>19.3</cell><cell>24</cell><cell>27.2</cell></row><row><cell>LAFITE</cell><cell>26.9</cell><cell>23.0</cell><cell>18.7</cell><cell>15.7</cell><cell>14.8</cell><cell>26.0</cell></row><row><cell>Make-A-Scene*</cell><cell>7.55</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DALL-E-2</cell><cell>10.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CogView2</cell><cell>24.0</cell><cell>19.7</cell><cell>16.8</cell><cell>17.2</cell><cell>17.2</cell><cell>22.4</cell></row><row><cell>CogView2*</cell><cell>17.7</cell><cell>13.8</cell><cell>11.7</cell><cell>12.2</cell><cell>12.3</cell><cell>26.4</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">http://github.com/THUDM/icetk</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Implemented by a manually designed 6 × 6 matrix. Details are included in our released codes.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The advancement of text-to-image generation, especially text-guided image editing, will benefit the creation of artists and designers, and will also cause the risk of misinformation, leading to permanent damage to the reliability of web photos.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.04200</idno>
		<title level="m">Maskgit: Masked generative image transformer</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cogview: Mastering text-to-image generation via transformers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">All nlp tasks are generation tasks: A general pretraining framework</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10360</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Taming transformers for high-resolution image synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09841</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Make-a-scene: Scene-based text-to-image generation with human priors</title>
		<author>
			<persName><forename type="first">O</forename><surname>Gafni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ashual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sheynin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.13131</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mask-predict: Parallel decoding of conditional masked language models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6112" to="6121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2661</idno>
		<title level="m">Generative adversarial networks</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Vector quantized diffusion model for text-to-image synthesis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno>CoRR, abs/2111.14822</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno>CoRR, abs/2111.06377</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deberta: Decoding-enhanced bert with disentangled attention</title>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03654</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09751</idno>
		<title level="m">The curious case of neural text degeneration</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth Berkeley symposium on mathematical statistics and probability</title>
				<meeting>the fifth Berkeley symposium on mathematical statistics and probability<address><addrLine>Oakland, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1967">1967</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Glide: Towards photorealistic image generation and editing with text-guided diffusion models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10741</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Glancing transformer for non-autoregressive neural machine translation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<title level="s">Long Papers</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1993" to="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Do vision transformers see like convolutional neural networks?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Fast generation for convolutional autoregressive models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Khorrami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06001</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12092</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-Y</forename><surname>Jing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.05865</idno>
		<title level="m">Df-gan: Deep fusion generative adversarial networks for text-to-image synthesis</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
				<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6309" to="6318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
				<meeting>the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">N\&quot; uwa: Visual synthesis pre-training for neural visual world creation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12417</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Simmim: A simple framework for masked image modeling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<idno>CoRR, abs/2111.09886</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attngan: Fine-grained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1316" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.14211</idno>
		<title level="m">M6-ufc: Unifying multi-modal controls for conditional image synthesis</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tensmeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Lafite</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.13792</idno>
		<title level="m">Towards language-free training for text-to-image generation</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dm-gan: Dynamic memory generative adversarial networks for text-to-image synthesis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5802" to="5810" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
