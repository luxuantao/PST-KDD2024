<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Human action recognition using genetic algorithms and convolutional neural networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-01-20">January 20, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Earnest</forename><forename type="middle">Paul</forename><surname>Ijjina</surname></persName>
						</author>
						<author>
							<persName><forename type="first">C</forename><forename type="middle">Krishna</forename><surname>Mohan</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">PII</orgName>
								<address>
									<postCode>S0031-3203(16)00016-9</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science &amp; Engineering</orgName>
								<orgName type="laboratory">Visual Learning and Intelligence Group</orgName>
								<orgName type="institution">Indian Institute of Technology Hyderabad Telangana</orgName>
								<address>
									<postCode>502285</postCode>
									<country key="IN">INDIA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Human action recognition using genetic algorithms and convolutional neural networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-01-20">January 20, 2016</date>
						</imprint>
					</monogr>
					<idno type="MD5">C276F607DD655EEC6C010ADE751B2668</idno>
					<idno type="DOI">10.1016/j.patcog.2016.01.012</idno>
					<note type="submission">Received date: 30 August 2015 Revised date: 13 January 2016 Accepted date: 13 January 2016 Preprint submitted to Journal of Pattern Recognition</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Pattern Recognition Convolutional Neural Network (CNN)</term>
					<term>Genetic algorithms (GA)</term>
					<term>human action recognition</term>
					<term>action bank features</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, an approach for human action recognition using genetic algorithms (GA) and deep convolutional neural networks (CNN) is proposed. We demonstrate that initializing the weights of a convolutional neural network (CNN) classifier based on solutions generated by genetic algorithms (GA) minimizes the classification error. A gradient descent algorithm is used to train the CNN classifiers (to find a local minimum) during fitness evaluations of GA chromosomes.</p><p>The global search capabilities of genetic algorithms and the local search ability of gradient descent algorithm are exploited to find a solution that is closer to global-optimum. We show that combining the evidences of classifiers generated using genetic algorithms helps to improve the performance. We demonstrate the efficacy of the proposed classification system for human action recognition on UCF50 dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Inspired by biological neural networks, artificial neural networks were proposed for function approximation. Shortly after their introduction, the failure of shallow neural network models to classify non-linearly separable data resulted in the emergence of deep neural networks that contain more than two hidden layers but lacked an effective training algorithm due to vanishing gradient problem <ref type="bibr" target="#b1">[1]</ref>.</p><p>In the last decade, the advancements in computational capabilities and the introduction of effective approaches to train deep neural network architectures has lead to their wide usage to address various computer vision challenges. Some of the well known machine learning tasks addressed by deep neural network models include MNIST handwritten digit recognition <ref type="bibr" target="#b2">[2]</ref>, ILSVRC object recognition <ref type="bibr" target="#b3">[3]</ref> and facial expression recognition in the wild <ref type="bibr" target="#b4">[4]</ref>. A convolutional neural network is the most popular approach among deep neural network model that generally consists of an alternating sequence of convolution and sub-sampling layers.</p><p>In the recent years, human action recognition in videos has become a major domain of research due to its applications in video retrieval, sports analysis, health monitoring, human computer interaction and video surveillance. Several surveys papers were published in the literature, each one emphasizing a particular characteristic of recognition. The various methodologies for recognizing actions performed by a single person are covered in <ref type="bibr" target="#b5">[5]</ref> and <ref type="bibr" target="#b6">[6]</ref> focuses on the approaches to classify full body motions by categorizing them into spatial and temporal structures. Approaches for multi-view 2D and 3D human action recognition are discussed in <ref type="bibr" target="#b7">[7]</ref>. Several human action recognition datasets were proposed in the literature <ref type="bibr" target="#b8">[8]</ref> to address different types of problems like recognition of realistic activities, interaction and multi-view analysis from varying sources. Most of the action recognition techniques rely on some extracted features or descriptors for discriminative information for classification. Some of the most commonly used features/descriptors for human action recognition are bag-of-visual-words (BoVW) <ref type="bibr" target="#b9">[9]</ref>, histograms oriented gradient (HOG) <ref type="bibr" target="#b10">[10]</ref>, histograms of optical flow (HOF) <ref type="bibr" target="#b10">[10]</ref>, motion boundary histograms (MBH) <ref type="bibr" target="#b11">[11]</ref>, action bank features <ref type="bibr" target="#b12">[12]</ref> and dense trajectories <ref type="bibr" target="#b13">[13]</ref>. Xiaodan Liang et al. <ref type="bibr" target="#b14">[14]</ref> proposed a hierarchical human action recognition system by modeling each observation as an ensemble of spatio-temporal compositions. The latent structure of actions is represented by spatio-temporal and-or graphs with the leaf-nodes containing the spatial and temporal contextual interactions. The inability of these approaches to scale across multiple datasets has lead to the research on learning from data. In the recent years, deep learning gained a lot of focus due to its ability to learn features from data <ref type="bibr" target="#b15">[15]</ref>. The effectiveness of convolutional neural networks for object recognition was demonstrated in ILSVRC <ref type="bibr" target="#b3">[3]</ref> (IMAGENET large scale visual recognition challenge) <ref type="bibr" target="#b16">[16]</ref> <ref type="bibr" target="#b17">[17]</ref> after which it was used to address various other visual recognition tasks like face recognition <ref type="bibr" target="#b18">[18]</ref>, facial expression recognition <ref type="bibr" target="#b19">[19]</ref> <ref type="bibr" target="#b4">[4]</ref>, video quality assessment <ref type="bibr" target="#b20">[20]</ref> and action recognition <ref type="bibr" target="#b21">[21]</ref>[22] <ref type="bibr" target="#b23">[23]</ref>.</p><p>The convolutional neural network (CNN) introduced by LeCun et al. in <ref type="bibr" target="#b24">[24]</ref> [25], is the most popular deep neural network model in use for computer vision problems. One of the major initial attempts to use CNN for action recognition was by Baccouche et al. in <ref type="bibr" target="#b26">[26]</ref>. In this work, a 3D convolutional neural network is trained to assign a vector of features to a small number of consecutive frames. The spatio-temporal evolution of these features is used by a recurrent neural network for classification. In <ref type="bibr" target="#b21">[21]</ref> descriptor for human action recognition in <ref type="bibr" target="#b29">[29]</ref>, that extracts and aggregates appearance and flow information at characteristic positions obtained from human pose. A differential recurrent neural network to model the temporal evolution of state dynamics is proposed by Vivek Veeriah et al. in <ref type="bibr" target="#b30">[30]</ref> for action recognition.</p><p>A differential gating scheme emphasizing the information gain caused by salient motions between successive frames is used to learn spatio-temporal dynamics associated with salient motion patters. Simonyan et al. proposed a two-stream convolutional network for action recognition <ref type="bibr" target="#b31">[31]</ref> that uses appearance from still frames (spatial information) and motion between frames (temporal information) as separate recognition streams. The softmax scores of the two streams are combined using late fusion for classification.</p><p>Deep learning aims to learn multiple levels of representation with an intent to discover high-level abstractions for discrimination. In spite of the expressive power of deep architectures <ref type="bibr" target="#b32">[32]</ref>, learning in deep architectures <ref type="bibr" target="#b33">[33]</ref> is still a challenge. Since 2006, several deep learning algorithms like greedy layerwise training of deep networks <ref type="bibr" target="#b34">[34]</ref> that initializes weights by greedy layer-wise unsupervised training, a fast learning algorithm for deep belief nets <ref type="bibr" target="#b35">[35]</ref> and strategies for training deep neural networks <ref type="bibr" target="#b36">[36]</ref> were proposed. There has been studies on the difficulty of training a deep feed-forward neural network <ref type="bibr" target="#b37">[37]</ref> and techniques to improve generalization like: 1) early stopping <ref type="bibr" target="#b38">[38]</ref> to avoid overfitting, 2) dropout <ref type="bibr" target="#b39">[39]</ref> to avoid co-adaptation by randomly dropping neural units during training, 3) use of rectified linear units <ref type="bibr" target="#b40">[40]</ref> whose activation function has linear response in a short range, 4) unsupervised pre-training for effective initialization of weights in deep neural networks <ref type="bibr" target="#b41">[41]</ref>, and 5) the importance of a well-designed initialization of network in deep learning <ref type="bibr" target="#b42">[42]</ref>. There are even studies confirming that randomly chosen trails may be more effective than grid search and manual search as they effectively search a larger and less promising configuration space for hyper-parameter optimization <ref type="bibr" target="#b43">[43]</ref>.</p><p>To address these challenges in training deep neural networks, we explore the use of evolutionary algorithms (genetic algorithms in particular) for optimization of weights of neural network. In literature, genetic algorithms (GA) were used to optimize neural network systems by feature selection <ref type="bibr" target="#b44">[44]</ref> [45], topology selection <ref type="bibr" target="#b46">[46]</ref> [47], weight selection <ref type="bibr">[48][49]</ref>. GA is also used to optimize both weights and topology simultaneously <ref type="bibr" target="#b50">[50]</ref> [51] [52] <ref type="bibr" target="#b53">[53]</ref>. Most of the existing evolutionary neural networks <ref type="bibr" target="#b54">[54]</ref> [55] <ref type="bibr">[56] [49]</ref> are shallow and a straightfor-ward optimization of a deep neural network weights could be computationally expensive. Some of the approaches using GA for training deep neural networks includes the one proposed by David et al. <ref type="bibr" target="#b57">[57]</ref> to optimize a sparse autoencoder by learning the weights using GA assisted back-propagation. In <ref type="bibr" target="#b58">[58]</ref>, Oullette et al. used genetic algorithm to train the weights of a CNN without getting trapped in a local minimum. The trained classifier is used for crack detection and was evaluated on a dataset of 100 images. In <ref type="bibr" target="#b59">[59]</ref>, Fedorovici et al. proposed the use of evolutionary optimization techniques like gravitational search algorithm <ref type="bibr" target="#b60">[60]</ref> and particle swarm optimization <ref type="bibr" target="#b61">[61]</ref> to find the optimum weights of a convolutional neural network. The weights of CNN are further optimized using back-propagation algorithm for optical character recognition. <ref type="bibr">Koutnk et al.</ref> proposed an online evolutionary training algorithm <ref type="bibr" target="#b62">[62]</ref> for driving a race car in TORCS racing simulator using recurrent neural network controller and maxpooling convolutional neural network for feature extraction. The controller and CNN are simultaneously optimized using CoSyNE <ref type="bibr" target="#b63">[63]</ref> using the images generated due to the turn and speed predictions of the controller.</p><p>In this work, we propose a hybrid search approach for training the weights of a convolutional neural network classifier exploiting the efficient global and local search abilities of evolutionary and classical optimization algorithms for the prediction of human actions in unconstrained videos. The novelty of the proposed approach lies in: 1) modeling a convolutional neural network classifier as a GA-chromosome and its use in improving classification performance, 2) the use of genetic algorithms to explore different basins (weight initializations) in the parameter space and steepest-descent algorithm to expedite the search for finding the local optimum in a given basin, and 3) combining evidences from classifiers (that are generated by GA-framework) to overcome the limitation of individual classifiers. The reminder of the paper is organized as follows:</p><p>Section 2 describes the proposed approach and its rationale. The details of the experimental set up and performance analysis are discussed in section 3. Finally, the conclusions and future work are presented in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Proposed approach</head><p>In this work, we present a hybrid approach to train a CNN classifier by effective utilization of global and local search capabilities of genetic and steepestdescent algorithms, respectively. Training a neural network using gradientdescent algorithm may result in finding a solution that is stuck in a local minimum. As the performance of a trained neural network classifier depends on its initial weights, we explore different sets of initial weights to find the optimum weight initialization using genetic algorithms. The weights of masks in convolution layers (that act as feature detectors) and the seed value used by the random number generator to initialize the fully-connected neural network are considered as the GA chromosome, as shown in Fig. <ref type="figure" target="#fig_1">1</ref>. The proposed approach begins with the initialization of GA population, followed by the fitness evaluation step in GA framework. During fitness evaluation, the fitness score of each chromosome in the GA population is computed by decoding the chromosome to initialize the weights of a CNN classifier, as illustrated in step 2 of Fig. <ref type="figure" target="#fig_1">1</ref>. The classification accuracy of the CNN classifier, after being trained for p 1 epochs using steepest descent algorithm, is considered as the fitness value of the corresponding GA chromosome. Using GA, several local basins were identified and the steepest-descent algorithm is used to expedite the search to find the local optimum in a given basin. After executing the GA framework for several cycles with a population size of n, the final GA-population is harvested to obtain n sets of initial weights. These n sets of initial weights are used to initialize the convolutional neural network (CNN) classifiers as shown in step 5 of Fig. <ref type="figure" target="#fig_1">1</ref>.</p><p>The classification evidences of these n convolutional neural network classifiers is combined to improve the performance. The next subsection introduces genetic algorithms and explains how the fitness of a chromosome (quality of solution) improves over GA cycles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Genetic algorithms</head><p>Genetic algorithm is an adaptive heuristic search method based on the evolutionary ideas of natural selection and genetics proposed by Holland <ref type="bibr" target="#b64">[64]</ref>. In-spired by the Darwin's Theory of evolution (survival of the fittest) <ref type="bibr" target="#b65">[65]</ref>, this approach considers a population of GA chromosomes (candidate solutions) that go through a series of changes due to selection, crossover and mutation (operations) resulting in a modified set of chromosomes at the end of each GA cycle.</p><p>Assuming that the GA-chromosome captures the key characteristics of the system being modeled, the average fitness of the population is expected to improve over generations due to the use of fitness measure (quality of the solution) of GA chromosome in GA-operations. Refer <ref type="bibr" target="#b66">[66]</ref> for a comprehensive overview of genetic algorithms. The next section describes the fusion of evidences from 165 multiple classifiers for performance evaluation.  Combining evidences across classifiers would generally result in a classifier that correctly labels the observations which are misclassified by some classifiers (the limitation of a single classifier). An overview of ensemble methods is given in <ref type="bibr" target="#b67">[67]</ref>. The next subsection introduces the representation of videos as action bank features and describes the architecture of CNN classifier used for human action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Combining evidences from multiple classifiers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">CNN classifier for human action recognition</head><p>In this section, we describe the underlying principles in the computation of action bank features for a video. We will later explain some of the characteristics and advantages of action bank features that motivated us in their use as input features. Finally, the design of the convolutional neural network classifier for human action recognition from action bank features is explained in detail.    The next section discusses the experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">Input features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental results</head><p>The proposed CNN classifier approach is implemented by customizing the deep learning toolbox <ref type="bibr" target="#b68">[68]</ref> to use linear masks and using the native GA functionality available in Matlab. The range of weights in convolution masks is in between -100 and 100. The range of seed value is 0 to 5000. The GA with a population size of 20 (n in Fig. <ref type="figure" target="#fig_1">1</ref>) is run for 5 generations considering a cross-over probability of 0.8 and mutation probability of 0.01. Low mutation probability is used in the GA-framework as GA relies on the construction capability of crossover operator rather than on the disruptive power of mutation operator.</p><p>The optimum range of these parameters and GA configuration is determined empirically. By expediting local-search using steepest-descent algorithm, we aim to find an optimal solution even with a small number of GA-generations and population size. The experimental results on UCF50 dataset are discussed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">UCF50 dataset</head><p>The proposed approach is evaluated on UCF50 dataset <ref type="bibr" target="#b69">[69]</ref>, that consists of unconstrained realistic videos for 50 action categories taken from Youtube.</p><p>UCF50 dataset is selected due to its high number of action categories and the availability of pre-computed action bank features <ref type="bibr" target="#b70">[70]</ref>. The use of pre-computed action bank features in this work, facilitates the comparative study with existing approaches. The evaluation is done using 5-fold cross validation. Here, k-fold cross validation refers to splitting the dataset into k splits say S 1 ,S 2 ,. . . ,S k followed by using split S i for testing and the remaining (k-1) splits for training in Fold-i. This process is repeated k times as i is varied from 1 to k. During fitness computation of GA-chromosomes, the initialized CNN classifier is trained using back-propagation algorithm in batch mode for 50 (p 1 ) epochs. A batchsize of 10 is used for the first four folds and 8 for the fifth fold. The best and mean fitness value (indicating the classification error in %) of population in each GA generation, for the 5-folds of UCF50 dataset (on training data) is given in Table <ref type="table" target="#tab_0">1</ref>. The consistent decrease in mean and best fitness value of the population over generations for the 5-folds of UCF50 dataset indicates the proper selection of GA parameters. This also confirms the proper balance between exploration (due to mutation) and exploitation (due to crossover). This completes step 3 of Fig. <ref type="figure" target="#fig_1">1</ref> and produces 20 (n) candidate classifier initializations for each fold.</p><p>As mentioned in steps 5 and 6 of Fig. <ref type="figure" target="#fig_1">1</ref>, the candidate solutions are used to initialize the CNN classifiers and their classification evidences are combined to assign the class labels. The performance of the n CNN classifiers using neural network and extreme learning machine (ELM) <ref type="bibr" target="#b71">[71]</ref> classifiers is given in Table <ref type="table" target="#tab_1">2</ref>. From the average accuracy given in the last row of this table, it can be observed that extreme learning machine (ELM) classifier gives better performance than neural network classifier. This could be due to the better generalization capability of ELM over gradient-based training algorithms.</p><p>As discussed in step 6 of Fig. <ref type="figure" target="#fig_1">1</ref>, the n classifiers generated at the end of  classifier is given in Table <ref type="table" target="#tab_2">3</ref>. It can be observed that the performance remains the same irrespective of the fusion-rule. This may be due to the small deviation in performance of the classifiers used in the ensemble.</p><p>From Table <ref type="table" target="#tab_2">3</ref>, it can be observed that one observation gets misclassified irrespective of the fusion rule. Thus, a classification accuracy of 99.98% is achieved by the proposed approach for 5-fold cross-validation of UCF50 dataset. The confusion matrix of the proposed approach for UCF50 dataset is shown in Ta- The performance of the CNN classifier when trained using back-propagation algorithm (BPA), genetic algorithms (GA) and both is given in Table <ref type="table" target="#tab_4">5</ref>. It can be observed that CNN classifiers whose weights are initialized using GA and trained using back-propagation algorithm gives better performance than the rest of the approaches. As a set of solutions gets generated when GA is   with population a size of 200 for 5 generations. Thus, training the CNN classifier initialized by GA with BPA finds an optimal solution in less number of generations even with a small population size. The performance of the proposed classification framework using neural network (NN) and extreme learning machine (ELM) classifiers is shown in Table <ref type="table" target="#tab_5">6</ref>. From the table, it can be concluded that better performance can be achieved using ELM classifier compared to NN classifier. From Table <ref type="table" target="#tab_2">3</ref>, it can be concluded that the performance of the proposed approach using an ensemble of CNN classifiers employing ELM classification is 99.98% as one observation among 6617 test cases got misclassified.</p><p>The performance of the proposed approach against exiting techniques for 5-fold cross-validation on UCF50 dataset is given in Table <ref type="table" target="#tab_6">7</ref>.</p><p>It can be observed from Table <ref type="table" target="#tab_6">7</ref> that an accuracy of 94.1% is achieved by Nicolas Ballas et al. in <ref type="bibr" target="#b77">[77]</ref> by building an action model from salient regions using spatio-temporal context and weighted SVM. In the proposed approach, a classification accuracy of 99.98% is achieved using action bank features. The improvement in classification performance using the proposed classification system is indicative of the effectiveness of the proposed hybrid search approach to  the proposed approach are the large variation is illumination conditions, change in scale and the existence of camera shake. The predicted top-5 class labels for this observation using the proposed approach with various fusion rules is given in Table <ref type="table" target="#tab_7">8</ref>. It can be observed that 100% prediction accuracy is achieved by the proposed approach if top-2 predictions are used for performance evaluation.</p><p>The feasibility to extend this approach to solve problems in other domains, is demonstrated by evaluating this approach for handwritten character recognition on MNIST dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">MNIST dataset</head><p>The recognition of hand-written characters using computer vision algorithms is a challenging task with practical applications. The MNIST dataset <ref type="bibr" target="#b25">[25]</ref> is     The performance of the CNN classifier trained using back propagation algorithm (BPA), genetic algorithms (GA) and both is given in Table <ref type="table" target="#tab_8">9</ref>. The table</p><p>shows the performance of CNN classifier without GA (i.e., trained using BPA)</p><p>against the average performance of solutions generated with GA (i.e., using GA and BPA). As the best performance and standard deviation of solutions generated by with GA training approach are 96.92% and 22.91, respectively, it can be concluded that CNN classifiers initialized by genetic algorithms and trained with back propagation algorithm gives better performance than the rest of the approaches. The performance of the proposed classification framework using neural network (NN) and extreme learning machine (ELM) classifiers is given in Table <ref type="table" target="#tab_9">10</ref>. The performance of ensemble of CNN classifiers using ELM classification is also shown in the last row of this table. The performance of the proposed and existing approaches for character recognition on MNIST dataset is given in Table <ref type="table" target="#tab_10">11</ref>. The table also shows the number of layers with trainable weights, the size and count of masks in the CNN architecture. From the table, it can be observed that the proposed approach uses less number of layers, masks and training epochs to achieve comparable performance with the existing approach. The performance can be further improved by considering deeper architectures with more number of masks. The next section analyzes the solutions explored by the proposed approach for MNIST dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Analysis</head><p>We visualize the performance of solutions explored by the proposed approach during the GA cycles, to validate the improvement of candidate solutions (GA population) over generations. The solutions explored by the proposed approach by initializing the weights using GA and training the generated classifiers using   Proposed approach 3, 5×5 3 97.9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Computational complexity</head><p>The existing approach uses genetic algorithms and training of convolutional </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion and future work</head><p>In this paper, we proposed a deep learning algorithm inspired by hybrid search approach of evolutionary and classical algorithms. As the performance of a neural network classifier (after training) depends on its weight initialization, we aim to optimize the initial weights using a GA framework. The proposed approach finds the weights of a convolutional neural network classifier that is neither overfit for training data nor stuck in a local minimum. The fusion across models identified using GA framework aims to overcome the limitations of individual models, by combining evidences across classifiers. Experimental</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>, Shuiwang Ji et al. extracted gray, gradient and optical-flow information along x and y directions from video frames and used them as input to a 3D CNN model for human action recognition in surveillance videos. Keze Wang et al. proposed a deep learning model for human activity recognition in [27] by extending a CNN to incorporate structure alternatives by using latent variables in convolutional layers to manipulate the activation of neurons. The variation in temporal composition of activities during recognition in handled through partial activation of network configuration. A spatio-temporal CNN is used by Liang Lin et al. in [28] to decompose videos into temporal segments of sub-activities. The model is iteratively optimized by a learning algorithm with radius-margin regularization for human action recognition in RGBD videos. Guilhem Chéron et al. proposed a pose-based CNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of the proposed classification system. The various steps involved in the proposed approach are numbered and a short description of each step is given on the top right corner. Best viewed in color.</figDesc><graphic coords="9,95.00,3.34,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>If o 1 ,</head><label>1</label><figDesc>o 2 , . . . , o c are the binary decoded outputs of a classifier, then the classifier is trained to output o p = 1 and o j = 0, for all j = p and 1 ≤ j ≤ c for an observation of class p, where c represents the number of classes. During testing, an observation will be labeled as class p if o p &gt; o j , for all j = p and 1 ≤ j ≤ c. The fusion (combination) of evidences across n classifiers involves the use of a fusion function like Max -rule, across the same index of classifier outputs to find the binary decoded output of the combined model. If o 1i , o 2i , . . . , o ci are the outputs of the i th classifier in the combined model, the j th output of the combined model is defined as f j =max{o j1 , o j2 , . . . , o jn }. An observation will be labeled as class p by the combined model if f p &gt; f j , for all j = p and 1 ≤ j ≤ c.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Introduced by Sadanand et al. in<ref type="bibr" target="#b12">[12]</ref>, the action bank representation of videos is a high level representation used for activity recognition. An action bank is a collection of multiple action detectors covering a broad semantic and viewpoint space. An action detector is a template video of an action. Some of the action detectors in the action bank are shown in Fig.2with columns depicting different types of actions and rows indicating different examples for the corresponding action.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A screen-shot of 36 videos in the standard action bank with 205 elements. Best viewed in color. (Fig. 2 in [12])</figDesc><graphic coords="11,135.48,166.57,340.30,120.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Action bank representation of boxing and running videos in KTH dataset: (a)(b)(c) are for boxing and (d)(e)(f) are for running action</figDesc><graphic coords="12,133.52,125.55,57.20,154.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 : 2 . 4 .</head><label>424</label><figDesc>Figure 4: Architecture CNN classifier for human action recognition</figDesc><graphic coords="13,133.58,201.29,206.66,145.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>step 5</head><label>5</label><figDesc>are used as base classifiers in an ensemble model and various fusion functions are considered to combine their classification evidences. The performance</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>ble 4 .</head><label>4</label><figDesc>The labels on the vertical axis indicate the true class labels and the labels on the horizontal axis indicate the predicted class labels. The diagonal elements represent the correctly predicted test cases and the non-diagonal elements represent the misclassified test cases. It can be observed that one test instance of W alkingW ithDog is misclassified as RockClimbingIndoor by the proposed approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>corner in graphs depicting the solutions initialized using GA indicates the use of GA to identify optimum initial weights of the classifier rather than the final weights used for fitness computation, and 4) the high concentration of yellow circles closer in the bottom left corner (area with low classification error) in graphs with solutions trained using back-propagation algorithm demonstrates the improvement of solutions generated by GA over generations. The most likely reasons for misclassification of WalkingWithDog observation in Fig 7 by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>(a) Solutions corresponding to weight initialization using GA chromosome (b) Solutions in (a) after training with back-propagation algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Solutions explored by the proposed approach for Fold-1 of UCF50 dataset: a) after initialization using GA chromosomes and b) after training the classifier using back-propagation algorithm for p 1 epochs. Best viewed in color.</figDesc><graphic coords="23,143.38,370.11,259.70,218.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>(a) Sols. with weight initialization using GA (b) Sols. in (a) after training with BP algorithm (c) Sols. with weight initialization using GA (d) Sols. in (c) after training with BP algorithm (e) Sols. with weight initialization using GA (f) Sols. in (e) after training with BP algorithm (g) Sols. with weight initialization using GA (h) Sols. in (g) after training with BP algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Solutions explored by the proposed approach for Fold-2, Fold-3, Fold-4 and Fold-5 of UCF50 dataset. The sub figures (a), (b) correspond to Fold-2 ; (c), (d) are for Fold-3 ; (e), (f) correspond to Fold-4 and (g), (h) are for Fold-5. (Here, BP represents back-propagation algorithm and Sols represent solutions). Best viewed in color.</figDesc><graphic coords="24,139.62,487.05,129.86,109.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7:</figDesc><graphic coords="25,135.54,124.79,340.16,55.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>back-propagation algorithm for p 1</head><label>1</label><figDesc>epochs for MNIST dataset are shown in Fig 8 (a) and (b), respectively. Each circle in these graphs correspond to a CNN classifier, with the error for training and testing data used as x and y coordinates of the circle and the time at which the solution is generated during the GA cycles determines the color of the circle. From Fig 8 (b), it can be observed that the classification error of the solutions initialized using GA and trained using back propagation algorithms decreases significantly with generations. The next section discusses the time complexity of this approach and its suitability for use in real time applications.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>neural network (CNN) classifier using back propagation algorithm, which can be parallelized. By parallel evaluation of candidate solutions (population) in genetic algorithms and use of efficient GPU based CNN implementation (like cuDNN<ref type="bibr" target="#b78">[78]</ref>) to train CNN classifiers for p 1 epochs results in a significant reduction in computation time. In this work, the CNN classifiers are trained for a small number of epochs (p 1 ) i.e., 50 epochs for UCF50 and 10 epochs for MNIST dataset. Several efficient multi-GPU implementations of CNN were proposed in the last few years like Berkeley's Caffe, Torch and Theano. Several browser-based user-friendly platforms like NVIDIA's DIGITS, Google's Tensor and Microsoft's Azure are proposed to aid the design and deployment of CNN classifiers for real-time applications. As inferencing is less expensive than training a deep neural network, trained CNN classifiers are used in many online systems like mobile applications for speech processing, image recognition etc., Thus, the proposed approach generates a set of optimized CNN classifiers, which could then be deployed for real time online application. The computational complexity of action back features restrict the feasibility to use this approach for real-time human action recognition. The next section gives the conclusions of this work.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Best and mean fitness (classification error in %) of GA population across generations for the 5 folds in UCF50 dataset.</figDesc><table><row><cell></cell><cell>Fold-1</cell><cell>Fold-2</cell><cell>Fold-3</cell><cell>Fold-4</cell><cell>Fold-5</cell></row><row><cell>Generation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">Best Mean Best Mean Best Mean Best Mean Best Mean</cell></row><row><cell>1</cell><cell cols="5">9.21 59.57 3.18 45.7 8.67 64.9 6.15 44.5 5.79 41.6</cell></row><row><cell>2</cell><cell cols="5">5.57 38.78 3.10 23.8 3.47 51.2 3.04 11.6 5.79 32.5</cell></row><row><cell>3</cell><cell cols="2">3.49 33.19 2.95 5.8</cell><cell cols="2">3.47 35.2 1.82 4.2</cell><cell>3.73 8.0</cell></row><row><cell>4</cell><cell cols="2">2.67 5.66 2.87 3.6</cell><cell cols="2">2.18 18.4 1.52 3.1</cell><cell>3.04 3.8</cell></row><row><cell>5</cell><cell cols="2">2.45 3.45 2.80 3.4</cell><cell>1.81 2.4</cell><cell>1.44 2.8</cell><cell>3.04 3.5</cell></row><row><cell cols="6">on UCF50 dataset for various folds with different fusion functions using ELM</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance of candidate solutions (in %) generated from final GA population on</figDesc><table><row><cell cols="3">test data using neural network classifier and extreme learning machine (ELM) classifier for the</cell></row><row><cell cols="3">5-folds of UCF50 dataset. (Here Avg represents the average performance across all candidate</cell></row><row><cell cols="2">solutions)</cell><cell></cell></row><row><cell>Sol.</cell><cell>Neural Network (NN) classifier</cell><cell>Extreme Learning Machine (ELM) classifier</cell></row><row><cell cols="3"># Fold-1 Fold-2 Fold-3 Fold-4 Fold-5 Fold-1 Fold-2 Fold-3 Fold-4 Fold-5</cell></row><row><cell>1</cell><cell cols="2">97.40 97.20 98.19 98.48 96.95 100.00 99.70 100.00 100.00 100.00</cell></row><row><cell>2</cell><cell cols="2">96.36 95.98 98.19 84.18 96.42 100.00 99.77 100.00 97.87 100.00</cell></row><row><cell>3</cell><cell cols="2">96.88 96.97 96.60 98.02 96.65 100.00 99.85 100.00 100.00 100.00</cell></row><row><cell>4</cell><cell cols="2">97.03 96.82 97.81 98.17 96.11 100.00 99.85 100.00 100.00 100.00</cell></row><row><cell>5</cell><cell cols="2">97.55 96.59 96.91 98.33 96.65 100.00 99.85 100.00 100.00 100.00</cell></row><row><cell>6</cell><cell cols="2">97.40 96.97 98.19 98.33 96.49 100.00 99.77 100.00 100.00 100.00</cell></row><row><cell>7</cell><cell cols="2">97.17 96.52 97.74 96.88 96.65 100.00 99.92 100.00 99.77 100.00</cell></row><row><cell>8</cell><cell cols="2">96.88 96.89 97.74 98.17 96.34 100.00 99.77 100.00 100.00 100.00</cell></row><row><cell>9</cell><cell cols="2">97.40 96.52 98.04 98.02 96.95 100.00 99.85 100.00 100.00 100.00</cell></row><row><cell>10</cell><cell cols="2">90.93 97.05 98.04 97.72 96.80 100.00 99.77 100.00 99.85 100.00</cell></row><row><cell>11</cell><cell cols="2">97.10 96.89 97.89 97.57 96.80 100.00 100.00 100.00 100.00 100.00</cell></row><row><cell>12</cell><cell cols="2">96.95 96.74 97.06 97.03 96.49 100.00 99.77 100.00 99.92 100.00</cell></row><row><cell>13</cell><cell cols="2">92.86 96.14 98.11 98.25 96.49 100.00 99.70 100.00 100.00 100.00</cell></row><row><cell>14</cell><cell cols="2">96.58 96.74 97.96 98.40 96.57 100.00 99.92 100.00 100.00 100.00</cell></row><row><cell>15</cell><cell cols="2">96.80 95.91 98.11 98.48 96.65 100.00 99.77 100.00 100.00 100.00</cell></row><row><cell>16</cell><cell cols="2">97.17 97.12 97.43 98.56 96.42 100.00 99.70 100.00 100.00 100.00</cell></row><row><cell>17</cell><cell cols="2">96.51 96.06 96.60 96.05 96.27 100.00 99.77 100.00 100.00 100.00</cell></row><row><cell>18</cell><cell cols="2">97.40 95.91 97.81 98.33 96.57 100.00 99.17 100.00 100.00 100.00</cell></row><row><cell>19</cell><cell cols="2">97.10 96.67 95.09 98.25 95.96 100.00 99.85 98.94 100.00 100.00</cell></row><row><cell>20</cell><cell cols="2">97.40 96.52 97.89 96.43 96.49 100.00 99.85 100.00 99.77 100.00</cell></row><row><cell cols="3">Avg 96.50 96.60 97.56 97.18 96.53 100.00 99.78 99.94 99.85 100.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance of the proposed classification system (in terms of # of misclassified</figDesc><table><row><cell cols="8">observations) using ELM classifier with various fusion functions for 5-fold cross-validation of</cell></row><row><cell cols="2">UCF50 dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>data</cell><cell>number of</cell><cell></cell><cell cols="3">Fusion function</cell><cell></cell><cell>Majority</cell></row><row><cell>fold</cell><cell cols="2">observations Min</cell><cell>Max</cell><cell>Avg</cell><cell cols="2">Prod Median</cell><cell>voting</cell></row><row><cell>Fold-1</cell><cell>1345</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Fold-2</cell><cell>1320</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>Fold-3</cell><cell>1325</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Fold-4</cell><cell>1315</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Fold-5</cell><cell>1312</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Total</cell><cell>6617</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell cols="2">Accuracy (in %) =</cell><cell cols="4">99.98 99.98 99.98 99.98</cell><cell>99.98</cell><cell>99.98</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Confusion matrix of the proposed approach for 5-fold cross validation of UCF50 dataset</figDesc><table><row><cell>BaseballPitch Basketball</cell><cell>BenchPress Biking</cell><cell>Billiards BreastStroke CleanAndJerk Diving Drumming Fencing</cell><cell>GolfSwing</cell><cell>HighJump HorseRace HorseRiding</cell><cell>HulaHoop</cell><cell>JavelinThrow JugglingBalls JumpRope</cell><cell>JumpingJack</cell><cell>Kayaking Lunges</cell><cell>MilitaryParade Mixing</cell><cell>Nunchucks PizzaTossing</cell><cell>PlayingGuitar</cell><cell>PlayingPiano</cell><cell>PlayingTabla</cell><cell>PlayingViolin</cell><cell>PoleVault PommelHorse PullUps</cell><cell>Punch PushUps</cell><cell>RockClimbingIndoor</cell><cell>RopeClimbing Rowing</cell><cell>SalsaSpin</cell><cell>SkateBoarding</cell><cell>Skiing</cell><cell>Skijet</cell><cell>SoccerJuggling Swing</cell><cell>TaiChi TennisSwing</cell><cell>ThrowDiscus TrampolineJumping</cell><cell>VolleyballSpiking</cell><cell>WalkingWithDog YoYo</cell></row><row><cell cols="3">BaseballPitch150 Basketball 137 BenchPress 160 Biking 145</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Billiards BreastStroke CleanAndJerk Diving Drumming Fencing</cell><cell cols="3">150 101 112 153 161 111</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GolfSwing</cell><cell></cell><cell cols="3">142</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HighJump HorseRace HorseRiding</cell><cell></cell><cell></cell><cell cols="3">123 127 196</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HulaHoop</cell><cell></cell><cell></cell><cell></cell><cell cols="3">125</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>JavelinThrow JugglingBalls JumpRope</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">117 122 144</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>JumpingJack</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">123</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Kayaking Lunges</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">157 141</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MilitaryParade Mixing</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">127 141</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Nunchucks PizzaTossing</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">132 114</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PlayingGuitar</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">160</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PlayingPiano</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">105</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PlayingTabla</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">111</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PlayingViolin</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PoleVault PommelHorse PullUps</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">160 123 100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Punch PushUps</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">160 102</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RockClimbingIndoor</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">144</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RopeClimbing Rowing</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">130 140</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SalsaSpin</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">133</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SkateBoarding</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">120</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Skiing</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">144</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Skijet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">100</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SoccerJuggling Swing</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">156 137</cell><cell></cell><cell></cell><cell></cell></row><row><cell>TaiChi TennisSwing</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">100 167</cell><cell></cell><cell></cell></row><row><cell>ThrowDiscus TrampolineJumping</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">131 119</cell><cell></cell></row><row><cell>VolleyballSpiking</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">116</cell></row><row><cell>WalkingWithDog YoYo</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">122 128</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Performance of CNN classifier (in %) using back propagation algorithm (BPA), genetic algorithms (GA) and both for 5-fold cross-validation of UCF50 dataset.</figDesc><table><row><cell>Training approach</cell><cell cols="6">Fold-1 Fold-2 Fold-3 Fold-4 Fold-5 Average</cell></row><row><cell>CNN classifier with only GA</cell><cell>2.22</cell><cell>1.87</cell><cell>2.18</cell><cell>1.98</cell><cell>1.87</cell><cell>2.02</cell></row><row><cell>(i.e., initialized using GA)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CNN classifier without GA</cell><cell>86.02</cell><cell>87.50</cell><cell>18.26</cell><cell>80.30</cell><cell>23.39</cell><cell>59.19</cell></row><row><cell>(i.e., trained using BPA)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CNN classifier with GA</cell><cell>96.54</cell><cell>96.60</cell><cell>97.56</cell><cell>97.18</cell><cell>96.53</cell><cell>96.88</cell></row><row><cell>(using GA and BPA)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>used for training, the average performance of the resulting classifiers is reported in the table. Here, experiments for training using only GA were conducted</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Performance of the proposed classification framework (in %) using neural network</figDesc><table><row><cell cols="7">(NN) and extreme learning machine (ELM) classifiers for 5-fold cross-validation of UCF50</cell></row><row><cell>dataset.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Classification methodology Fold-1 Fold-2 Fold-3 Fold-4 Fold-5 Average</cell></row><row><cell>Proposed framework</cell><cell>96.54</cell><cell>96.60</cell><cell>97.56</cell><cell>97.18</cell><cell>96.53</cell><cell>96.88</cell></row><row><cell>using NN classifier</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Proposed framework</cell><cell>100</cell><cell>99.78</cell><cell>99.94</cell><cell>99.85</cell><cell>100</cell><cell>99.91</cell></row><row><cell>using ELM classifier</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Performance comparison of the proposed approach with existing techniques for 5-fold</figDesc><table><row><cell>cross-validation on UCF50 dataset</cell><cell></cell></row><row><cell>Approach</cell><cell>Accuracy (in %)</cell></row><row><cell>Sadanand and J. Corso [12]</cell><cell>57.9</cell></row><row><cell>Kliper-Gross et al. [72]</cell><cell>68.51</cell></row><row><cell>Shi Feng et al. [73]</cell><cell>71.7</cell></row><row><cell>LiMin Wang et al. [74]</cell><cell>71.7</cell></row><row><cell>H. Wang et al. [13]</cell><cell>75.7</cell></row><row><cell>Qiang Zhou et al. [75]</cell><cell>80.2</cell></row><row><cell>Ijjina Earnest et al. [76]</cell><cell>94.02</cell></row><row><cell>Nicolas Ballas et al. [77]</cell><cell>94.1</cell></row><row><cell>Proposed approach</cell><cell>99.98</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Top 5 class labels predicted by the proposed approach using various fusion rules, for the misclassified W alkingW ithDog observation. (Here, RCI denotes RockClimbingIndoor, WWD represents W alkingW ithDog, P denotes P unch, L denotes Lunges, D represents Diving, K denotes Kayaking and HR denotes HorseRiding action)</figDesc><table><row><cell>Top</cell><cell></cell><cell></cell><cell>Fusion-rule</cell><cell></cell><cell></cell></row><row><cell>#</cell><cell>Min</cell><cell>Max</cell><cell>Avg</cell><cell>Prod</cell><cell>Median</cell></row><row><cell>1</cell><cell>RCI</cell><cell>RCI</cell><cell>RCI</cell><cell>RCI</cell><cell>RCI</cell></row><row><cell>2</cell><cell>WWD</cell><cell>WWD</cell><cell>WWD</cell><cell>WWD</cell><cell>WWD</cell></row><row><cell>3</cell><cell>P</cell><cell>L</cell><cell>P</cell><cell>P</cell><cell>P</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Performance of CNN classifiers using back propagation algorithm (BPA), genetic algorithms (GA) and both for MNIST dataset.</figDesc><table><row><cell>Training approach</cell><cell>Performance (in %)</cell></row><row><cell>CNN classifier with only GA</cell><cell>12.58</cell></row><row><cell>(i.e., initialized using GA)</cell><cell></cell></row><row><cell>CNN classifier without GA</cell><cell>91.0</cell></row><row><cell>(i.e., trained using BPA)</cell><cell></cell></row><row><cell>CNN classifier with GA</cell><cell>87.85</cell></row><row><cell>(using GA and BPA)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Performance of the proposed classification framework using neural network (NN) and extreme learning machine (ELM) classifiers for MNIST dataset.</figDesc><table><row><cell cols="2">Classification methodology Performance (in %)</cell></row><row><cell>Proposed framework</cell><cell>87.85</cell></row><row><cell>using NN classifier</cell><cell></cell></row><row><cell>Proposed framework</cell><cell>96.74</cell></row><row><cell>using ELM classifier</cell><cell></cell></row><row><cell>Proposed approach</cell><cell>97.9</cell></row><row><cell>with ensemble of classifiers</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Performance comparison of the proposed approach with existing techniques on</figDesc><table><row><cell>MNIST dataset</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Approach</cell><cell>Masks</cell><cell cols="2">Layers Accuracy (in %)</cell></row><row><cell></cell><cell>count, size</cell><cell></cell><cell></cell></row><row><cell>Convolutional net LeNet-5 [25]</cell><cell>22, 5×5</cell><cell>7</cell><cell>99.0</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>find optimum initial weights of the CNN classifier. The next section analyzes the results presented in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Analysis</head><p>The two important issues in training a neural network using back-propagation are: 1) over-fitting of training data, and 2) the possibility of solution getting stuck in a local minimum. When the neural network is over-fit due to excessive training, the error on training set will be very low but on testing set will be high.</p><p>In this section, we analyze the classification error of CNN classifiers immediately after weight initialization using GA, and also after training the CNN classifier  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">on UCF50 dataset to recognize human actions from action bank features suggests that the proposed approach achieves a recognition accuracy of 99.98%. The future work will consider other spatio-temporal features like exmoves</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<title level="m">The MNIST database of handwritten digits</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno>CoRR abs/1409.0575</idno>
		<imprint/>
	</monogr>
	<note>Imagenet large scale visual recognition challenge</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Combining modality specific deep neural networks for emotion recognition in video</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bouthillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Froumenty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-L</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Carrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boulanger-Lewandowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Côté</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1145/2522848.2531745</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM International Conference on Multimodal Interaction, ICMI &apos;13</title>
		<meeting>the 15th ACM International Conference on Multimodal Interaction, ICMI &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="543" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Human activity analysis: A review</title>
		<author>
			<persName><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ryoo</surname></persName>
		</author>
		<idno type="DOI">10.1145/1922649.1922653</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="43" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A survey of vision-based methods for action representation, segmentation and recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ronfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cviu.2010.10.002</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="224" to="241" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Human action recognition using multiple views: A comparative perspective on recent developments</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Holte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<idno type="DOI">10.1145/2072572.2072588</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Joint ACM Workshop on Human Gesture and Behavior Understanding</title>
		<meeting>the 2011 Joint ACM Workshop on Human Gesture and Behavior Understanding<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="47" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A survey of video datasets for human action and activity recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Chaquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Carmona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fernndez-Caballero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="633" to="659" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recognizing human actions by a bag of visual words</title>
		<author>
			<persName><forename type="first">P</forename><surname>Foggia</surname></persName>
			<affiliation>
				<orgName type="collaboration">SMC</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Percannella</surname></persName>
			<affiliation>
				<orgName type="collaboration">SMC</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saggese</surname></persName>
			<affiliation>
				<orgName type="collaboration">SMC</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vento</surname></persName>
			<affiliation>
				<orgName type="collaboration">SMC</orgName>
			</affiliation>
		</author>
		<idno type="DOI">10.1109/SMC.2013.496</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 IEEE International Conference on Systems, Man, and Cybernetics</title>
		<meeting>the 2013 IEEE International Conference on Systems, Man, and Cybernetics</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2910" to="2915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the 2008 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Human detection using oriented histograms of flow and appearance</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th European Conference on Computer Vision -Volume Part II, ECCV 06</title>
		<meeting>the 9th European Conference on Computer Vision -Volume Part II, ECCV 06<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="428" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Action bank: A high-level representation of activity in video</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sadanand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2012 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1234" to="1241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning latent spatio-temporal compositional model for human action recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<publisher>ACM MM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2013.50</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS 2012</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno>abs/1311.2524</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Face recognition: a convolutional neural-network approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Back</surname></persName>
		</author>
		<idno type="DOI">10.1109/72.554195</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="113" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Subject independent facial expression recognition with robust face detection using a convolutional neural network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Matsugu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mitari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kaneda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="555" to="559" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A convolutional neural network approach for objective video quality assessment, Neural Networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Le</forename><surname>Callet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Viard-Gaudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barba</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNN.2006.879766</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1316" to="1327" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>PAMI)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2014.223</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the 2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">D: generic features for video analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno>abs/1412.0767</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Convolutional networks and applications in vision</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCAS.2010.5537907</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2010 IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<meeting>2010 IEEE International Symposium on Circuits and Systems (ISCAS)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="253" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sequential deep learning for human action recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mamalet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baskurt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Human Behavior Unterstanding, HBU&apos;11</title>
		<meeting>the Second International Conference on Human Behavior Unterstanding, HBU&apos;11<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="29" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3d human activity recognition with reconfigurable convolutional neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<idno type="DOI">10.1145/2647868.2654912</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia, MM &apos;14</title>
		<meeting>the ACM International Conference on Multimedia, MM &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="97" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A deep structured model with radius-margin bound for 3d human activity recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0876-z</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">P-CNN: pose-based CNN features for action recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chéron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno>CoRR abs/1506.03607</idno>
		<ptr target="http://arxiv.org/abs/1506.03607" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Differential recurrent neural networks for action recognition</title>
		<author>
			<persName><forename type="first">V</forename><surname>Veeriah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<idno>CoRR abs/1504.06678</idno>
		<ptr target="http://arxiv.org/abs/1504.06678" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>CoRR abs/1406.2199</idno>
		<ptr target="http://arxiv.org/abs/1406.2199" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On the expressive power of deep architectures</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Delalleau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Algorithmic Learning Theory</title>
		<meeting>the 22nd International Conference on Algorithmic Learning Theory<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="18" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning deep architectures for ai</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.1561/2200000006</idno>
	</analytic>
	<monogr>
		<title level="j">Foundation and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="127" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">D</forename><surname>Montral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qubec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Exploring strategies for training deep neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Early stopping -but when?</title>
		<author>
			<persName><forename type="first">L</forename><surname>Prechelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">1524</biblScope>
			<biblScope unit="page" from="55" to="69" />
		</imprint>
	</monogr>
	<note>chapter 2</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improving deep neural networks for lvcsr using rectified linear units and dropout</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the 2013 International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="8609" to="8613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Why does unsupervised pre-training help deep learning?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="625" to="660" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML-13)</title>
		<title level="s">JMLR Proceedings</title>
		<meeting>the 30th International Conference on Machine Learning (ICML-13)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Random search for hyper-parameter optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Using genetic algorithms to improve pattern classification performance</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">I</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lippmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NIPS)</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Lippmann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Moody</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Touretzky</surname></persName>
		</editor>
		<meeting>Advances in Neural Information Processing Systems (NIPS)<address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1990">November 26-29. 1990</date>
			<biblScope unit="page" from="797" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A genetic algorithm and neural network hybrid classification scheme</title>
		<author>
			<persName><forename type="first">D</forename><surname>Decker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hintz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 9th AIAA Computers in Aerospace Conference</title>
		<meeting>9th AIAA Computers in Aerospace Conference</meeting>
		<imprint>
			<publisher>AIAA</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="473" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Towards the genetic synthesis of neural network</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Samad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Conference on Genetic Algorithms</title>
		<meeting>the Third International Conference on Genetic Algorithms<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="360" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Using genetic search to exploit the emergent behavior of neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schaffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Eshelman</surname></persName>
		</author>
		<idno type="DOI">10.1016/0167-2789</idno>
		<ptr target="http://dx.doi.org/10.1016/0167-2789" />
	</analytic>
	<monogr>
		<title level="j">Physica D: Nonlinear Phenomena</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="90078" to="90082" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Training feedforward neural networks using genetic algorithms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Montana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Joint Conference on Artificial Intelligence (IJCAI&apos;89</title>
		<meeting>the 11th International Joint Conference on Artificial Intelligence (IJCAI&apos;89<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="762" to="767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Evolutionary artificial neural networks: A review</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jin</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10462-011-9270-6</idno>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="251" to="260" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Genetic generation of both the weights and architecture for a neural network</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Koza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Rice</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.1991.155366</idno>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (IJCNN-91)</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">ii</biblScope>
			<biblScope unit="page" from="397" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Genetic synthesis of boolean neural networks with a cell rewriting developmental process</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gruau</surname></persName>
		</author>
		<idno type="DOI">10.1109/COGANN.1992.273948</idno>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Combinations of Genetic Algorithms and Neural Networks (COGANN-92)</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="55" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">An artificial neural network representation for artificial organisms, in: Parallel Problem Solving from Nature</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Jefferson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Springer-Verlag</publisher>
			<biblScope unit="page" from="259" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">General asymmetric neural networks and structure design by genetic algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bornholdt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Graudenz</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0893-6080(05)80030-9</idno>
		<ptr target="http://dx.doi.org/10.1016/S0893-6080(05)80030-9" />
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="327" to="334" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Combinations of genetic algorithms and neural networks: a survey of the state of the art</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schaffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Whitley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eshelman</surname></persName>
		</author>
		<idno type="DOI">10.1109/COGANN.1992.273950</idno>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Combinations of Genetic Algorithms and Neural Networks (COGANN-92)</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="1" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Evolving artificial neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<idno type="DOI">10.1109/5.784219</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1423" to="1447" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A review of evolutionary artificial neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="539" to="567" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Genetic algorithms for evolving deep neural networks</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">E</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename></persName>
		</author>
		<idno type="DOI">10.1145/2598394.2602287</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference Companion on Genetic and Evolutionary Computation Companion, GECCO Comp &apos;14, ACM</title>
		<meeting>the 2014 Conference Companion on Genetic and Evolutionary Computation Companion, GECCO Comp &apos;14, ACM<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1451" to="1452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Genetic algorithm optimization of a convolutional neural network for autonomous crack detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Oullette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hirasawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Congress on Evolutionary Computation (CEC2004)</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="516" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Evolutionary optimization-based training of convolutional neural networks for ocr applications</title>
		<author>
			<persName><forename type="first">L.-O</forename><surname>Fedorovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R.-E</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dragan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Purcaru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th International Conference on System Theory, Control and Computing (ICSTCC)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="207" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A gravitational search algorithm</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rashedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nezamabadi-Pour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saryazdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gsa</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="2232" to="2248" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Particle swarm optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Eberhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1942" to="1948" />
			<date type="published" when="1995">1995</date>
			<publisher>IEEE</publisher>
			<pubPlace>Perth, Australia, IEEE Service Center, Piscataway, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Online evolution of deep convolutional network for vision-based reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Koutnk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">From Animals to Animats 13</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Del Pobil</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Chinellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Martinez-Martin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hallam</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Cervera</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Morales</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8575</biblScope>
			<biblScope unit="page" from="260" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Accelerated neural evolution through cooperatively coevolved synapses</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="937" to="965" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Adaptation in Natural and Artificial Systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Holland</surname></persName>
		</author>
		<ptr target="http://books.google.com/books?id=YE5RAAAAMAAJ" />
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>University of Michigan Press</publisher>
			<pubPlace>Ann Arbor, MI, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Darwin&apos;s theory of the origin of species</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bascom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Theological Review</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="349" to="379" />
			<date type="published" when="1871">1871</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Genetic algorithms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Pearson Education India</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<title level="m">Ensemble Methods: Foundations and Algorithms, 1st Edition</title>
		<imprint>
			<publisher>Chapman &amp; Hall/CRC</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Prediction as a candidate for learning deep hierarchical models of data, Master&apos;s thesis</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Palm</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<pubPlace>Asmussens Alle, Denmark</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Technical University of Denmark</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Recognizing 50 human action categories of web videos</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00138-012-0450-4</idno>
	</analytic>
	<monogr>
		<title level="j">Machine Vision and Applications</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="971" to="981" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Action bank: A high-level representation of activity in video</title>
		<idno>2015-08-08</idno>
		<ptr target="http://www.cse.buffalo.edu/~jcorso/r/actionbank/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Extreme learning machine: Theory and applications</title>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Siew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="489" to="501" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Motion interchange patterns for action recognition in unconstrained videos</title>
		<author>
			<persName><forename type="first">O</forename><surname>Kliper-Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gurovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th European Conference on Computer Vision(ECCV) -Volume Part VI, ECCV&apos;12</title>
		<meeting>the 12th European Conference on Computer Vision(ECCV) -Volume Part VI, ECCV&apos;12<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="256" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Sampling strategies for real-time action recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Petriu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Laganiere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the 2013 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2595" to="2602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Motionlets: Mid-level 3d parts for human motion recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the 2013 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2674" to="2681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Learning to share latent tasks for action recognition</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the 2013 IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2264" to="2271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Human action recognition based on recognition of linear patterns in action bank features using convolutional neural networks</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Ijjina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mohan</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICMLA.2014</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Machine Learning and Applications (ICMLA)</title>
		<meeting>the 13th International Conference on Machine Learning and Applications (ICMLA)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="178" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Space-time robust representation for action recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Delezoide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Preteux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Shelhamer, cudnn: Efficient primitives for deep learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
		<idno>CoRR abs/1410.0759</idno>
		<ptr target="http://arxiv.org/abs/1410.0759" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">EXMOVES: classifier-based features for scalable action recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<idno>CoRR abs/1312.5785</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
