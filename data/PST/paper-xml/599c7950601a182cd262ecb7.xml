<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeepFM: A Factorization-Machine based Neural Network for CTR Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
							<email>huifengguo@yeah.net</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Graduate School</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Noah&apos;s Ark Research Lab</orgName>
								<address>
									<settlement>Huawei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yunming</forename><surname>Ye</surname></persName>
							<email>yeyunming@hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Graduate School</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Noah&apos;s Ark Research Lab</orgName>
								<address>
									<settlement>Huawei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
							<email>hexiuqiang@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Noah&apos;s Ark Research Lab</orgName>
								<address>
									<settlement>Huawei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Noah&apos;s Ark Research Lab</orgName>
								<address>
									<region>Huawei</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DeepFM: A Factorization-Machine based Neural Network for CTR Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low-or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low-and highorder feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide &amp; Deep model from Google, DeepFM has a shared input to its "wide" and "deep" parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The prediction of click-through rate (CTR) is critical in recommender system, where the task is to estimate the probability a user will click on a recommended item. In many recommender systems the goal is to maximize the number of clicks, so the items returned to a user should be ranked by estimated CTR; while in other application scenarios such as online advertising it is also important to improve revenue, so the ranking strategy can be adjusted as CTR×bid across all candidates, where "bid" is the benefit the system receives if the item is clicked by a user. In either case, it is clear that the key is in estimating CTR correctly.</p><p>It is important for CTR prediction to learn implicit feature interactions behind user click behaviors. By our study in a mainstream apps market, we found that people often download apps for food delivery at meal-time, suggesting that the (order-2) interaction between app category and time-stamp can be used as a signal for CTR. As a second observation, male teenagers like shooting games and RPG games, which means that the (order-3) interaction of app category, user gender and age is another signal for CTR. In general, such interactions of features behind user click behaviors can be highly sophisticated, where both low-and high-order feature interactions should play important roles. According to the insights of the Wide &amp; Deep model <ref type="bibr" target="#b1">[Cheng et al., 2016]</ref> from google, considering low-and high-order feature interactions simultaneously brings additional improvement over the cases of considering either alone.</p><p>The key challenge is in effectively modeling feature interactions. Some feature interactions can be easily understood, thus can be designed by experts (like the instances above). However, most other feature interactions are hidden in data and difficult to identify a priori (for instance, the classic association rule "diaper and beer" is mined from data, instead of discovering by experts), which can only be captured automatically by machine learning. Even for easy-to-understand interactions, it seems unlikely for experts to model them exhaustively, especially when the number of features is large.</p><p>Despite their simplicity, generalized linear models, such as FTRL <ref type="bibr" target="#b3">[McMahan et al., 2013]</ref>, have shown decent performance in practice. However, a linear model lacks the ability to learn feature interactions, and a common practice is to manually include pairwise feature interactions in its feature vector. Such a method is hard to generalize to model high-order feature interactions or those never or rarely appear in the training data <ref type="bibr">[Rendle, 2010]</ref>. Factorization Machines (FM) <ref type="bibr">[Rendle, 2010]</ref> model pairwise feature interactions as inner product of latent vectors between features and show very promising results. While in principle FM can model high-order feature interaction, in practice usually only order-2 feature interactions are considered due to high complexity.</p><p>As a powerful approach to learning feature representation, deep neural networks have the potential to learn sophisticated feature interactions. Some ideas extend <ref type="bibr">CNN and RNN for CTR predition [Liu et al., 2015;</ref><ref type="bibr" target="#b7">Zhang et al., 2014]</ref>, but CNN-based models are biased to the interactions between neighboring features while RNN-based models are more suitable for click data with sequential dependency. <ref type="bibr" target="#b7">[Zhang et al., 2016]</ref> studies feature representations and proposes Factorization-machine supported Neural Network (FNN). This model pre-trains FM before applying DNN, thus limited by the capability of FM. Feature interaction is studied in <ref type="bibr" target="#b3">[Qu et al., 2016]</ref>, by introducing a product layer between embedding layer and fully-connected layer, and proposing the Product-based Neural Network (PNN). As noted in <ref type="bibr" target="#b1">[Cheng et al., 2016]</ref>, PNN and FNN, like other deep models, capture little low-order feature interactions, which are also essential for CTR prediction. To model both lowand high-order feature interactions, <ref type="bibr" target="#b1">[Cheng et al., 2016]</ref> proposes an interesting hybrid network structure (Wide &amp; Deep) that combines a linear ("wide") model and a deep model. In this model, two different inputs are required for the "wide part" and "deep part", respectively, and the input of "wide part" still relies on expertise feature engineering.</p><p>One can see that existing models are biased to low-or highorder feature interaction, or rely on feature engineering. In this paper, we show it is possible to derive a learning model that is able to learn feature interactions of all orders in an endto-end manner, without any feature engineering besides raw features. Our main contributions are summarized as follows:</p><p>• We propose a new neural network model DeepFM (Figure <ref type="figure" target="#fig_0">1</ref>) that integrates the architectures of FM and deep neural networks (DNN). It models low-order feature interactions like FM and models high-order feature interactions like DNN. Unlike the wide &amp; deep model <ref type="bibr" target="#b1">[Cheng et al., 2016]</ref>, DeepFM can be trained endto-end without any feature engineering.</p><p>• DeepFM can be trained efficiently because its wide part and deep part, unlike <ref type="bibr" target="#b1">[Cheng et al., 2016]</ref>, share the same input and also the embedding vector. In <ref type="bibr" target="#b1">[Cheng et al., 2016]</ref>, the input vector can be of huge size as it includes manually designed pairwise feature interactions in the input vector of its wide part, which also greatly increases its complexity.</p><p>• We evaluate DeepFM on both benchmark data and commercial data, which shows consistent improvement over existing models for CTR prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Approach</head><p>Suppose the data set for training consists of n instances (χ, y), where χ is an m-fields data record usually involving a pair of user and item, and y ∈ {0, 1} is the associated label indicating user click behaviors (y = 1 means the user clicked the item, and y = 0 otherwise). χ may include categorical fields (e.g., gender, location) and continuous fields (e.g., age). Each categorical field is represented as a vector of one-hot encoding, and each continuous field is represented as the value itself, or a vector of one-hot encoding after discretization. Then, each instance is converted to (x, y) where x = [x f ield1 , x f ield2 , ..., x f iledj , ..., x f ieldm ] is a ddimensional vector, with x f ieldj being the vector representation of the j-th field of χ. Normally, x is high-dimensional and extremely sparse. The task of CTR prediction is to build a prediction model ŷ = CT R model(x) to estimate the probability of a user clicking a specific app in a given context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">DeepFM</head><p>We aim to learn both low-and high-order feature interactions.</p><p>To this end, we propose a Factorization-Machine based neural network (DeepFM). As depicted in Figure <ref type="figure" target="#fig_0">1</ref> <ref type="foot" target="#foot_1">1</ref> , DeepFM consists of two components, FM component and deep component, that share the same input. For feature i, a scalar w i is used to weigh its order-1 importance, a latent vector V i is used to measure its impact of interactions with other features.</p><p>V i is fed in FM component to model order-2 feature interactions, and fed in deep component to model high-order feature interactions. All parameters, including w i , V i , and the network parameters (W (l) , b (l) below) are trained jointly for the combined prediction model:</p><formula xml:id="formula_0">ŷ = sigmoid(y F M + y DN N ),<label>(1)</label></formula><p>where ŷ ∈ (0, 1) is the predicted CTR, y F M is the output of FM component, and y DN N is the output of deep component. The FM component is a factorization machine, which is proposed in <ref type="bibr">[Rendle, 2010]</ref> to learn feature interactions for recommendation. Besides a linear (order-1) interactions among features, FM models pairwise (order-2) feature interactions as inner product of respective feature latent vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FM Component</head><p>It can capture order-2 feature interactions much more effectively than previous approaches especially when the dataset is sparse. In previous approaches, the parameter of an interaction of features i and j can be trained only when feature i and feature j both appear in the same data record. While in FM, it is measured via the inner product of their latent vectors V i and V j . Thanks to this flexible design, FM can train latent vector V i (V j ) whenever i (or j) appears in a data record. Therefore, feature interactions, which are never or rarely appeared in the training data, are better learnt by FM.</p><p>As Figure <ref type="figure" target="#fig_1">2</ref> shows, the output of FM is the summation of an Addition unit and a number of Inner Product units:</p><formula xml:id="formula_1">y F M = w, x + d i=1 d j=i+1 V i , V j x i • x j ,<label>(2)</label></formula><p>where w ∈ R d and</p><formula xml:id="formula_2">V i ∈ R k (k is given) 2 .</formula><p>The Addition unit ( w, x ) reflects the importance of order-1 features, and the Inner Product units represent the impact of order-2 feature interactions. The deep component is a feed-forward neural network, which is used to learn high-order feature interactions. As shown in Figure <ref type="figure" target="#fig_2">3</ref>, a data record (a vector) is fed into the neural network. Compared to neural networks with image <ref type="bibr" target="#b2">[He et al., 2016]</ref> or audio <ref type="bibr" target="#b0">[Boulanger-Lewandowski et al., 2013]</ref> data as input, which is purely continuous and dense, the input of CTR prediction is quite different, which requires a new network architecture design. Specifically, the raw feature input vector for CTR prediction is usually highly sparse<ref type="foot" target="#foot_3">3</ref> , super high-dimensional<ref type="foot" target="#foot_4">4</ref> , categorical-continuous-mixed, and grouped in fields (e.g., gender, location, age). This suggests an embedding layer to compress the input vector to a lowdimensional, dense real-value vector before further feeding into the first hidden layer, otherwise the network can be overwhelming to train.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Component</head><p>Figure <ref type="figure">4</ref> highlights the sub-network structure from the input layer to the embedding layer. We would like to point out the two interesting features of this network structure: 1) while the lengths of different input field vectors can be different, Figure <ref type="figure">4</ref>: The structure of the embedding layer their embeddings are of the same size (k); 2) the latent feature vectors (V ) in FM now serve as network weights which are learned and used to compress the input field vectors to the embedding vectors. In <ref type="bibr" target="#b7">[Zhang et al., 2016]</ref>, V is pre-trained by FM and used as initialization. In this work, rather than using the latent feature vectors of FM to initialize the networks as in <ref type="bibr" target="#b7">[Zhang et al., 2016]</ref>, we include the FM model as part of our overall learning architecture, in addition to the other DNN model. As such, we eliminate the need of pre-training by FM and instead jointly train the overall network in an end-to-end manner. Denote the output of the embedding layer as:</p><formula xml:id="formula_3">a (0) = [e 1 , e 2 , ..., e m ],<label>(3)</label></formula><p>where e i is the embedding of i-th field and m is the number of fields. Then, a (0) is fed into the deep neural network, and the forward process is:</p><formula xml:id="formula_4">a (l+1) = σ(W (l) a (l) + b (l) ),<label>(4)</label></formula><p>where l is the layer depth and σ is an activation function. a (l) , W (l) , b (l) are the output, model weight, and bias of the l-th layer. After that, a dense real-value feature vector is generated, which is finally fed into the sigmoid function for CTR prediction:</p><formula xml:id="formula_5">y DN N = W |H|+1 • a |H| + b |H|+1</formula><p>, where |H| is the number of hidden layers.</p><p>It is worth pointing out that FM component and deep component share the same feature embedding, which brings two important benefits: 1) it learns both low-and high-order feature interactions from raw features; 2) there is no need for expertise feature engineering of the input, as required in Wide &amp; Deep <ref type="bibr" target="#b1">[Cheng et al., 2016]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Relationship with Other Neural Networks</head><p>Inspired by the enormous success of deep learning in various applications, several deep models for CTR prediction are developed recently. This section compares the proposed DeepFM with existing deep models for CTR prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FNN</head><p>As Figure <ref type="figure" target="#fig_3">5</ref> (left) shows, FNN is a FM-initialized feedforward neural network <ref type="bibr" target="#b7">[Zhang et al., 2016]</ref>. The FM pretraining strategy results in two limitations: 1) the embedding parameters might be over affected by FM; 2) the efficiency is reduced by the overhead introduced by the pre-training stage. In addition, FNN captures only high-order feature interactions. In contrast, DeepFM needs no pre-training and learns both high-and low-order feature interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PNN</head><p>For the purpose of capturing high-order feature interactions, PNN imposes a product layer between the embedding layer and the first hidden layer <ref type="bibr" target="#b3">[Qu et al., 2016]</ref>. According to   <ref type="bibr">et al., 2016]</ref>, there is a need for expertise feature engineering on the input to the "wide" part (for instance, cross-product of users' install apps and impression apps in app recommendation). In contrast, DeepFM needs no such expertise knowledge to handle the input by learning directly from the input raw features.</p><p>A straightforward extension to this model is replacing LR by FM (we also evaluate this extension in Section 3). This extension is similar to DeepFM, but DeepFM shares the feature embedding between the FM and deep component. The sharing strategy of feature embedding influences (in backpropagate manner) the feature representation by both lowand high-order feature interactions, which models the representation more precisely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summarizations</head><p>To summarize, the relationship between DeepFM and the other deep models in four aspects is presented in Table <ref type="table" target="#tab_0">1</ref>. As can be seen, DeepFM is the only model that requires no pretraining and no feature engineering, and captures both lowand high-order feature interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we compare our proposed DeepFM and the other state-of-the-art models empirically. The evaluation result indicates that our proposed DeepFM is more effective than any other state-of-the-art model and the efficiency of DeepFM is comparable to the best ones among all the deep models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experiment Setup Datasets</head><p>We evaluate the effectiveness and efficiency of our proposed DeepFM on the following two datasets. 1) Criteo Dataset: Criteo dataset<ref type="foot" target="#foot_5">5</ref> includes 45 million users' click records. There are 13 continuous features and 26 categorical ones. We split the dataset into two parts: 90% is for training, while the rest 10% is for testing.</p><p>2) Company * Dataset: In order to verify the performance of DeepFM in real industrial CTR prediction, we conduct experiment on Company * dataset. We collect 7 consecutive days of users' click records from the game center of the Company * App Store for training, and the next 1 day for testing. There are around 1 billion records in the whole collected dataset. In this dataset, there are app features (e.g., identification, category, and etc), user features (e.g., user's downloaded apps, and etc), and context features (e.g., operation time, and etc).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics</head><p>We use two evaluation metrics in our experiments: AUC (Area Under ROC) and Logloss (cross entropy). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter Settings</head><p>To evaluate the models on Criteo dataset, we follow the parameter settings in <ref type="bibr" target="#b3">[Qu et al., 2016]</ref> for FNN and PNN: (1) dropout: 0.5; (2) network structure: 400-400-400; (3) optimizer: Adam; (4) activation function: tanh for IPNN, relu for other deep models. To be fair, our proposed DeepFM uses the same setting. The optimizers of LR and FM are FTRL and Adam respectively, and the latent dimension of FM is 10.</p><p>To achieve the best performance for each individual model on Company * dataset, we conducted carefully parameter study, which is discussed in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance Evaluation</head><p>In this section, we evaluate the models listed in Section 3.1 on the two datasets to compare their effectiveness and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Efficiency Comparison</head><p>The efficiency of deep learning models is important to realworld applications. We compare the efficiency of different models on Criteo dataset by the following formula:</p><formula xml:id="formula_6">|training time of deep CT R model| |training time of LR|</formula><p>. The results are shown in Figure <ref type="figure" target="#fig_5">6</ref>, including the tests on CPU (left) and GPU (right), where we have the following observations: 1) pre-training of FNN makes it less efficient; 2) Although the speed up of IPNN and PNN * on GPU is higher than the other models, they are still computationally expensive because of the inefficient inner product operations; 3) The DeepFM achieves almost the most efficient in both tests. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness Comparison</head><p>The performance for CTR prediction of different models on Criteo dataset and Company * dataset is shown in Table <ref type="table" target="#tab_1">2</ref> (note that the numbers in the table are averaged by 5 runs of training-testing, and the variances of AUC and Logloss are in the order of 1E-5), where we have the following observations:</p><p>• Learning feature interactions improves the performance of CTR prediction model. This observation is from the fact that LR (which is the only model that does not consider feature interactions) performs worse than the other models. As the best model, DeepFM outperforms LR by 0.82% and 2.6% in terms of AUC (1.1% and 4.0% in terms of Logloss) on Company * and Criteo datasets. • Learning high-and low-order feature interactions simultaneously and properly improves the performance of CTR prediction model. DeepFM outperforms the models that learn only low-order feature interactions (namely, FM) or high-order feature interactions (namely, FNN, IPNN, OPNN, PNN * ). Compared to the second best model, DeepFM achieves more than 0.34% and 0.41% in terms of AUC (0.34% and 0.76% in terms of Logloss) on Company * and Criteo datasets. • Learning high-and low-order feature interactions simultaneously while sharing the same feature embedding for high-and low-order feature interactions learning improves the performance of CTR prediction model. DeepFM outperforms the models that learn high-and low-order feature interactions using separate feature embeddings (namely, LR &amp; DNN and FM &amp; DNN). Compared to these two models, DeepFM achieves more than 0.48% and 0.44% in terms of AUC (0.58% and 0.80% in terms of Logloss) on Company * and Criteo datasets. Overall, our proposed DeepFM model beats the competitors by more than 0.34% and 0.35% in terms of AUC and Logloss on Company * dataset, respectively. In fact, a small improvement in offline AUC evaluation is likely to lead to a significant increase in online CTR. As reported in <ref type="bibr" target="#b1">[Cheng et al., 2016]</ref>, compared with LR, Wide &amp; Deep improves AUC by 0.275% (offline) and the improvement of online CTR is 3.9%. The daily turnover of Company * 's App Store is millions of dollars, therefore even several percents lift in CTR brings extra millions of dollars each year. Moreover, we also conduct t-test between our proposed DeepFM and the other compared models. The p-value of DeepFM against FM &amp; DNN under Logloss metric on Company * is less than 1.5 × 10 −3 , and the others' p-values on both datasets are less than 10 −6 , which indicates that our improvement over existing models is significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hyper-Parameter Study</head><p>We study the impact of different hyper-parameters of different deep models, on Company * dataset. The order is: 1) activation functions; 2) dropout rate; 3) number of neurons per layer; 4) number of hidden layers; 5) network shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Activation Function</head><p>According to <ref type="bibr" target="#b3">[Qu et al., 2016]</ref>, relu and tanh are more suitable for deep models than sigmoid. In this paper, we compare the performance of deep models when applying relu and tanh. As shown in Figure <ref type="figure" target="#fig_6">7</ref>, relu is more appropriate than tanh for all the deep models, except for IPNN. Possible reason is that relu induces sparsity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dropout</head><p>Dropout <ref type="bibr" target="#b5">[Srivastava et al., 2014]</ref> refers to the probability that a neuron is kept in the network. Dropout is a regularization technique to compromise the precision and the complexity of the neural network. We set the dropout to be 1.0, 0.9, 0.8, 0.7, 0.6, 0.5. As shown in Figure <ref type="figure" target="#fig_7">8</ref>, all the models are able to reach their own best performance when the dropout is properly set (from 0.6 to 0.9). The result shows that adding reasonable randomness to model can strengthen model's robustness. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Neurons per Layer</head><p>When other factors remain the same, increasing the number of neurons per layer introduces complexity. As we can observe from Figure <ref type="figure" target="#fig_8">9</ref>, increasing the number of neurons does not always bring benefit. For instance, DeepFM performs stably when the number of neurons per layer is increased from 400 to 800; even worse, OPNN performs worse when we increase the number of neurons from 400 to 800. This is because an over-complicated model is easy to overfit. In our dataset, 200 or 400 neurons per layer is a good choice. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Hidden Layers</head><p>As presented in Figure <ref type="figure" target="#fig_9">10</ref>, increasing number of hidden layers improves the performance of the models at the beginning, however, their performance is degraded if the number of hidden layers keeps increasing, because of overfitting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Shape</head><p>We test four different network shapes: constant, increasing, decreasing, and diamond. When we change the network shape, we fix the number of hidden layers and the total number of neurons. For instance, when the number of hidden layers is 3 and the total number of neurons is 600, then four different shapes are: constant <ref type="bibr">(200-200-200)</ref>, increasing (100-200-300), decreasing (300-200-100), and diamond (150-300-150). As we can see from Figure <ref type="figure" target="#fig_10">11</ref>, the "constant" network shape is empirically better than the other three options, which is consistent with previous studies <ref type="bibr" target="#b3">[Larochelle et al., 2009]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>In this paper, a new deep neural network is proposed for CTR prediction. The most related domains are CTR prediction and deep learning in recommender system.</p><p>CTR prediction plays an important role in recommender system <ref type="bibr" target="#b4">[Richardson et al., 2007;</ref><ref type="bibr" target="#b3">Juan et al., 2016]</ref>. Besides generalized linear models and FM, a few other models are proposed for CTR prediction, such as tree-based model <ref type="bibr" target="#b2">[He et al., 2014]</ref>, tensor based model [Rendle and Schmidt-Thieme, 2010], support vector machine <ref type="bibr" target="#b1">[Chang et al., 2010]</ref>, and bayesian model <ref type="bibr" target="#b2">[Graepel et al., 2010]</ref>.</p><p>The other related domain is deep learning in recommender systems. In Section 1 and Section 2.2, several deep learning models for CTR prediction are already mentioned, thus we do not discuss about them here. Several deep learning models are proposed in recommendation tasks other than CTR prediction (e.g., <ref type="bibr" target="#b1">[Covington et al., 2016;</ref><ref type="bibr" target="#b5">Salakhutdinov et al., 2007;</ref><ref type="bibr" target="#b5">van den Oord et al., 2013;</ref><ref type="bibr" target="#b6">Wu et al., 2016;</ref><ref type="bibr">Zheng et al., 2016;</ref><ref type="bibr" target="#b6">Wu et al., 2017;</ref><ref type="bibr">Zheng et al., 2017]</ref>). <ref type="bibr" target="#b5">[Salakhutdinov et al., 2007;</ref><ref type="bibr" target="#b5">Sedhain et al., 2015;</ref><ref type="bibr">Wang et al., 2015]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we proposed DeepFM, a factorization-machine based neural network for CTR prediction, to overcome the shortcomings of the state-of-the-art models. DeepFM trains a deep component and an FM component jointly. It gains performance improvement from these advantages: 1) it does not need any pre-training; 2) it learns both high-and loworder feature interactions; 3) it introduces a sharing strategy of feature embedding to avoid feature engineering. The experiments on two real-world datasets demonstrate that 1) DeepFM outperforms the state-of-the-art models in terms of AUC and Logloss on both datasets; 2) The efficiency of DeepFM is comparable to the most efficient deep model in the state-of-the-art.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Wide &amp; deep architecture of DeepFM. The wide and deep component share the same input raw feature vector, which enables DeepFM to learn low-and high-order feature interactions simultaneously from the input raw features.</figDesc><graphic url="image-1.png" coords="1,323.10,216.00,226.80,112.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The architecture of FM.</figDesc><graphic url="image-2.png" coords="2,330.66,410.84,211.68,109.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The architecture of DNN.</figDesc><graphic url="image-3.png" coords="3,69.66,311.63,211.68,118.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The architectures of existing deep models for CTR prediction: FNN, PNN, Wide &amp; Deep Model</figDesc><graphic url="image-5.png" coords="4,74.16,54.00,463.68,106.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>We compare 9 models in our experiments: LR, FM, FNN, PNN (three variants), Wide &amp; Deep (two variants), and DeepFM. In the Wide &amp; Deep model, for the purpose of eliminating feature engineering effort, we also adapt the original Wide &amp; Deep model by replacing LR by FM as the wide part. In order to distinguish these two variants of Wide &amp; Deep, we name them LR &amp; DNN and FM &amp; DNN, respectively. 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Time comparison.</figDesc><graphic url="image-7.png" coords="5,169.89,241.98,113.39,61.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: AUC and Logloss comparison of activation functions.</figDesc><graphic url="image-8.png" coords="5,315.00,543.37,120.96,76.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: AUC and Logloss comparison of dropout.</figDesc><graphic url="image-10.png" coords="6,54.00,98.50,120.96,84.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: AUC and Logloss comparison of number of neurons.</figDesc><graphic url="image-12.png" coords="6,54.00,319.21,120.96,83.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: AUC and Logloss comparison of number of layers.</figDesc><graphic url="image-14.png" coords="6,54.00,483.89,120.96,83.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: AUC and Logloss comparison of network shape.</figDesc><graphic url="image-11.png" coords="6,177.45,99.75,120.96,83.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>propose to improve Collaborative Filtering via deep learning. The authors of [Wang and Wang, 2014; van den Oord et al., 2013] extract content feature by deep learning to improve the performance of music recommendation. [Chen et al., 2016] devises a deep learning network to consider both image feature and basic feature of display adverting. [Covington et al., 2016] develops a two-stage deep learning framework for YouTube video recommendation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of deep models for CTR prediction</figDesc><table><row><cell></cell><cell>No</cell><cell>High-order</cell><cell>Low-order</cell><cell>No Feature</cell></row><row><cell>FNN PNN Wide &amp; Deep DeepFM</cell><cell>Pre-training × √ √ √</cell><cell>Features √ √ √ √</cell><cell>Features × × √ √</cell><cell>Engineering √ √ × √</cell></row><row><cell cols="5">different types of product operation, there are three variants:</cell></row><row><cell cols="5">IPNN, OPNN, and PNN * , where IPNN is based on inner</cell></row><row><cell cols="5">product of vectors, OPNN is based on outer product, and</cell></row><row><cell cols="5">PNN *  is based on both inner and outer products. Like FNN,</cell></row><row><cell cols="4">all PNNs ignore low-order feature interactions.</cell><cell></cell></row></table><note>Wide &amp; Deep Wide &amp; Deep (Figure 5 (right)) is proposed by Google to model low-and high-order feature interactions simultaneously. As shown in [Cheng</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance on CTR prediction.</figDesc><table><row><cell></cell><cell cols="2">Company *</cell><cell></cell><cell>Criteo</cell></row><row><cell></cell><cell>AUC</cell><cell>LogLoss</cell><cell>AUC</cell><cell>LogLoss</cell></row><row><cell>LR</cell><cell cols="4">0.8641 0.02648 0.7804 0.46782</cell></row><row><cell>FM</cell><cell cols="4">0.8679 0.02632 0.7894 0.46059</cell></row><row><cell>FNN</cell><cell cols="4">0.8684 0.02628 0.7959 0.46350</cell></row><row><cell>IPNN</cell><cell cols="4">0.8662 0.02639 0.7971 0.45347</cell></row><row><cell>OPNN</cell><cell cols="4">0.8657 0.02640 0.7981 0.45293</cell></row><row><cell>PNN *</cell><cell cols="4">0.8663 0.02638 0.7983 0.45330</cell></row><row><cell cols="5">LR &amp; DNN 0.8671 0.02635 0.7858 0.46596</cell></row><row><cell cols="5">FM &amp; DNN 0.8658 0.02639 0.7980 0.45343</cell></row><row><cell>DeepFM</cell><cell cols="4">0.8715 0.02619 0.8016 0.44985</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">In all figures of this paper, a Normal Connection in black refers to a connection with weight to be learned; a Weight-1 Connection, red arrow, is a connection with weight 1 by default; Embedding, blue dashed arrow, means a latent vector to be learned; Addition means adding all input together; Product, including Innerand Outer-Product, means the output of this unit is the product of two input vector; Sigmoid Function is used as the output function in CTR prediction; Activation Functions, such as relu and tanh, are used for non-linearly transforming the signal;The yellow and blue circles in the sparse features layer represent one and zero in one-hot encoding of the input, respectively.Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">We omit a constant offset for simplicity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">Only one entry is non-zero for each field vector.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4">E.g., in an app store of billion users, the one field vector for user ID is already of billion dimensions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5">http://labs.criteo.com/downloads/2014-kaggle-displayadvertising-challenge-dataset/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6">  6  We do not use the Wide &amp; Deep API released by Google, as the efficiency of that implementation is very low. We implement Wide &amp; Deep by ourselves by simplifying it with shared optimizer for both deep and wide part.Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement This research was supported in part by NSFC under Grant No. 61572158, National Key Technology R&amp;D Program of MOST China under Grant No. 2014BAL05B06, Shenzhen Science and Technology Program under Grant No. JSGG20150512145714247 and JCYJ20160330163900579.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Audio chord recognition with recurrent neural networks</title>
		<author>
			<persName><surname>Boulanger-Lewandowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ISMIR</title>
		<imprint>
			<biblScope unit="page" from="335" to="340" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep neural networks for youtube recommendations</title>
		<author>
			<persName><forename type="first">Chang</forename></persName>
		</author>
		<idno>CoRR, abs/1606.07792</idno>
	</analytic>
	<monogr>
		<title level="m">Training and testing low-degree polynomial data mappings via linear SVM. JMLR</title>
				<imprint>
			<publisher>Paul Covington, Jay Adams, and Emre Sargin</publisher>
			<date type="published" when="2010">2010. 2010. 2016. 2016. 2016. 2016. 2016. 2016</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
	<note>RecSys</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Webscale bayesian click-through rate prediction for sponsored search advertising in microsoft&apos;s bing search engine</title>
		<author>
			<persName><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Xiangyu Zhang, Shaoqing Ren, and Jian Sun</title>
				<meeting><address><addrLine>Stuart Bowers, and Joaquin Quiñonero Candela</addrLine></address></meeting>
		<imprint>
			<publisher>Kaiming He</publisher>
			<date type="published" when="2010">2010. 2010. 2014. 2014. 2016. 2016</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Arnar Mar Hrafnkelsson, Tom Boulos, and Jeremy Kubica. Ad click prediction: a view from the trenches</title>
		<author>
			<persName><forename type="first">Juan</forename></persName>
		</author>
		<idno>CoRR, abs/1611.00144</idno>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
		<title level="s">Steffen Rendle and Lars Schmidt-Thieme</title>
		<editor>
			<persName><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kan</forename><surname>Ren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ying</forename><surname>Wen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</editor>
		<meeting><address><addrLine>Todd Phillips, Eugene Davydov, Daniel Golovin, Sharat Chikkerur, Dan Liu, Martin Wattenberg</addrLine></address></meeting>
		<imprint>
			<publisher>Rendle and Schmidt-Thieme</publisher>
			<date type="published" when="2009">2016. 2016. 2009. 2009. 2015. 2015. 2013. 2013. 2016. 2016. 2010. 2010. 2010</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="81" to="90" />
		</imprint>
	</monogr>
	<note>ICDM</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Predicting clicks: estimating the click-through rate for new ads</title>
		<author>
			<persName><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="521" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving content-based and hybrid music recommendation using deep learning</title>
		<author>
			<persName><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Aäron van den Oord, Sander Dieleman, and Benjamin Schrauwen</title>
				<editor>
			<persName><forename type="first">Suvash</forename><surname>Sedhain</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Scott</forename><surname>Sanner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lexing</forename><surname>Xie</surname></persName>
		</editor>
		<imprint>
			<publisher>Naiyan Wang, and Dit-Yan Yeung</publisher>
			<date type="published" when="2007">2007. 2007. 2015. 2015. 2014. 2014. 2013. 2013. 2014. 2014. 2015. 2015</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1235" to="1244" />
		</imprint>
	</monogr>
	<note>ACM SIGKDD</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Collaborative denoising autoencoders for top-n recommender systems</title>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM WSDM</title>
				<editor>
			<persName><forename type="first">Yuan</forename><surname>Wu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alex</forename><surname>Beutel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">How</forename><surname>Jing</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2016">2016. 2016. 2017. 2017</date>
			<biblScope unit="page" from="495" to="503" />
		</imprint>
	</monogr>
	<note>WSDM</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep learning over multi-field categorical data --A case study on user response prediction</title>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<publisher>Lei Zheng</publisher>
			<date type="published" when="2014">2014. 2014. 2016. 2016. 2016. 2016. 2017. 2017</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="425" to="434" />
		</imprint>
	</monogr>
	<note>WSDM</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
