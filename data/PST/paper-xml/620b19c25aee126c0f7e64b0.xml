<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modeling User Behavior with Graph Convolution for Personalized Product Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-02-12">12 Feb 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lu</forename><surname>Fan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiaotong</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fuyu</forename><surname>Lv</surname></persName>
							<email>lvfuyu91@sina.com</email>
						</author>
						<author>
							<persName><forename type="first">Guli</forename><surname>Lin</surname></persName>
							<email>linguli@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Sen</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Taiwei</forename><surname>Jin</surname></persName>
							<email>taiwei.jtw@alibaba-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Keping</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong Polytechnic University Hong Kong</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Dalian University of Technology Dalian</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Alibaba Group Hangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Modeling User Behavior with Graph Convolution for Personalized Product Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-02-12">12 Feb 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3485447.3511949</idno>
					<idno type="arXiv">arXiv:2202.06081v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Personalized Product Search</term>
					<term>User Preference Modeling</term>
					<term>Graph Convolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>User preference modeling is a vital yet challenging problem in personalized product search. In recent years, latent space based methods have achieved state-of-the-art performance by jointly learning semantic representations of products, users, and text tokens. However, existing methods are limited in their ability to model user preferences. They typically represent users by the products they visited in a short span of time using attentive models and lack the ability to exploit relational information such as user-product interactions or item co-occurrence relations. In this work, we propose to address the limitations of prior arts by exploring local and global user behavior patterns on a user successive behavior graph, which is constructed by utilizing short-term actions of all users. To capture implicit user preference signals and collaborative patterns, we use an efficient jumping graph convolution to explore high-order relations to enrich product representations for user preference modeling. Our approach can be seamlessly integrated with existing latent space based methods and be potentially applied in any product retrieval method that uses purchase history to model user preferences. Extensive experiments on eight Amazon benchmarks demonstrate the effectiveness and potential of our approach. The source code is available at https://github.com/floatSDSDS/SBG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Information systems → Novelty in information retrieval.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Convenience drives the growth of e-commerce platforms such as Taobao or Amazon. Product search is an essential module in online shopping platforms, which guides users to browse and purchase products from a huge collection of commodities. Product search has its unique characteristics, making it distinct from web search, where information retrieval has made considerable progress. First, in web search engines, web pages are usually represented by long descriptive texts, while in e-commerce platforms, products are mainly represented by short texts such as titles and reviews, which may not always be informative. Second, other than textual representations, products are also associated with diverse relational data including ontology, spec sheet, figures, etc. Third, there are various types of user-item interactions in e-commerce platforms. A user can browse, click, review, or purchase a product, or simply put it in his/her cart. Besides, there exist other structural information such as query-reformulation, shop browsing or category browsing, and shopping cart checkout.</p><p>It would be highly desirable yet challenging to utilize such rich information for personalized product search. Existing methods mainly exploit text data. Among them, a recent line of research <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20]</ref> proposes to projects queries, items, and users into the same latent space and learn the representations of all entities with language modeling and information retrieval tasks, which enables the model to learn domain-specific semantic representations. However, they are limited in their ability to model user preferences, which is the core problem in product search. A common way to represent users is by the products they've visited during a period of time, but longterm historical user behavior normally contains noisy preference signals. HEM <ref type="bibr" target="#b1">[2]</ref> suffers from this problem since it represents a user with all his/her reviews of purchased products. ZAM <ref type="bibr" target="#b0">[1]</ref>, TEM <ref type="bibr" target="#b5">[6]</ref>, and RTM <ref type="bibr" target="#b6">[7]</ref> employ attentive models such as Transformer-based encoder to model user preferences and take into account both user behavior and query. For computational efficiency, user behavior sequences are usually truncated, and only recent behaviors are considered. While this helps to eliminate noisy preference signals, short-term user behavior may not contain sufficient preference signals (see more discussion in Sec. 3).</p><p>To capture more useful user preference signals, it is a natural idea to explore various user-product interactions and product cooccurrence relationships, which are usually encoded in a graph. Some recent efforts <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27]</ref> have been devoted to exploiting structural graph information for personalized product search. Ai et al. <ref type="bibr" target="#b2">[3]</ref> proposed a dynamic relation embedding model (DREM). DREM constructs a unified knowledge graph to encode diverse relations and dynamic user-search/purchase behaviors and models the structural relationships via graph regularization. Liu et al. <ref type="bibr" target="#b26">[27]</ref> proposed graph embedding based structural relationship representation learning (GraphSRRL), which explicitly models the structural relationships, such as two users visiting the same product by a same query or a user visiting the same product by two different queries. While DREM and GraphSRRL can model complex relationships, they include all previous user behaviors for preference modeling and may suffer from the noise induced by overly diverse signals.</p><p>In this work, we propose to explore local and global user behavior patterns on a user successive behavior graph (SBG) for user preference modeling. The SBG is constructed by utilizing shortterm actions of all users, which collectively form a global behavior graph with rich relations among products. To capture implicit user preference signals and collaborative patterns, we employ graph convolution to learn enriched product representations, which can be subsequently used for user preference modeling. Since user purchase behaviors are often sparse, it is helpful to explore high-order information on the SBG to model potential user interest, which requires stacking many graph convolution layers and leads to the well-known over-smoothing problem. To address this issue, we adopt an efficient jumping graph convolution layer that can effectively alleviate the over-smoothing effect. To showcase the usefulness of our approach, we integrate it into a state-of-the-art latent space based model ZAM <ref type="bibr" target="#b0">[1]</ref> and evaluate its performance on eight Amazon public benchmarks. The results show that our approach can significantly improve upon the base model and achieve better performance than other graph-based methods including DREM <ref type="bibr" target="#b2">[3]</ref> and GraphSRRL <ref type="bibr" target="#b26">[27]</ref>. It is worth noting that our approach is generic and can be potentially applied in any product retrieval method that models users using their purchase history.</p><p>The contributions of this paper are summarized as follows.</p><p>• To our best knowledge, this is the first work to study how to improve product search with graph convolution, a technique that has recently been shown useful for many applications in various fields. • We propose to model successive user behavior and exploit local and global behavior patterns with graph convolution for user preference modeling. We also use an efficient graph convolution layer with jumping connections to alleviate the over-smoothing problem and theoretically analyze its effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Extensive comparative experiments and ablation studies on</head><p>eight Amazon benchmarks demonstrate the effectiveness of our proposed method, which can be potentially applied in any product retrieval method that models users with their purchase history for personalized product search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Product Search</head><p>Latent spaced based product search. In recent years, neural network based models have dominated the research of product search <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>. LSE is the first latent vector space based search framework proposed by Van Gysel et al. <ref type="bibr" target="#b19">[20]</ref>, which maps queries and products in the same space. Later on, Ai et al. <ref type="bibr" target="#b1">[2]</ref> proposed to consider user preferences and learn embeddings of users, products, and words jointly with two tasks: the language modeling task and the information retrieval task. Further, Ai et al. <ref type="bibr" target="#b0">[1]</ref> considered user history behaviors conditioned on the current query and proposed a zero attention vector to control the degree of personalization. In addition, in order to model long and shortterm user preferences simultaneously, Guo et al. <ref type="bibr" target="#b17">[18]</ref> designed a dual attention-based network to capture users' current search intentions and their long-term preferences. Recently, Transformerbased architecture have also been explored <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> for product search.</p><p>In this work, we employ the popular latent spaced based product search framework as in ZAM <ref type="bibr" target="#b0">[1]</ref> and propose to enrich product representations via graph convolution.</p><p>Graph-based product search. Early attempts employed relational information such as social signals or user behavior traces for search and ranking problems <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10]</ref>. Recently, some studies attempted to model such information with graphs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b38">39]</ref>. Zhang et al. <ref type="bibr" target="#b38">[39]</ref> proposed GEPS (Graph Embedding-based ranking model for Product Search) that employs pre-training techniques to learn product and query embeddings. As far as we know, it is the first attempt to use graphs for product search but it overlooks user preferences. Bu et al. <ref type="bibr" target="#b8">[9]</ref> proposed to model product textual semantic relationships with hypergraph to learn structural information. Ai et al. <ref type="bibr" target="#b2">[3]</ref> proposed DREM (Dynamic Relation Embedding Model) that constructs a directed unified knowledge graph and jointly learns all embeddings through graph regularization. However, DREM lacks the ability to select informative information and is easily susceptible to noise. In this work, we exploit information on a successive behavior graph to avoid this problem. Liu et al. <ref type="bibr" target="#b26">[27]</ref> proposed GraphSRRL (Graph embedding based Structural Relationship Representation Learning model) that explicitly utilizes specific user-query-product relationships. GraphSRRL pays attentions to local relations. In this work, we employ graph convolution with jump connections to aggregate high-order graph information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Information Retrieval with Graphs</head><p>Graph-based methods have been extensively explored in the literature of sequential recommendation. These methods can be broadly categorized into two groups: embedding based methods and graph neural networks (GNN) based methods. Embedding-based methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26]</ref> employ network embedding techniques such as DeepWalk <ref type="bibr" target="#b31">[32]</ref> or Node2Vec <ref type="bibr" target="#b16">[17]</ref>. They learn structural graph information by leveraging the skip-gram model <ref type="bibr" target="#b27">[28]</ref>. GNN based methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref> employ GNN <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22]</ref> to aggregate information over graphs. Our work is closer to GCE-GNN (Global Context Enhanced Graph Neural Networks) <ref type="bibr" target="#b35">[36]</ref> in that we both make use  of global information. The difference is that GCE-GNN focuses more on transitions between items and utilizes the order of items, whereas we do not consider the order of products and pay more attention to their co-occurrence relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MOTIVATION AND INSIGHT</head><p>In this section, we discuss the limitations of existing product search methods in user preference modeling and provide insight on our proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Limitations of Existing Methods</head><p>Existing product search methods commonly model user preferences by considering their long-term or short-term behavior, but both of them have limitations.</p><p>Long-term user behavior may contain noisy preference signals. Typically, the long-term preference of a user is represented by all his/her historical interactions during a long period of time, which may contain many items and be overly diverse. For example, as shown in Figure <ref type="figure" target="#fig_1">1</ref>, the current query of user 𝑢 2 is "bag", but she has visited a couple of electronic devices in her purchase history, which are not related to bags. Therefore, representing 𝑢 2 's preference with all his/her historical engagement may impose negative effect on the current search induced by the irrelevant products in the purchase history. This problem is even more severe in HEM <ref type="bibr" target="#b1">[2]</ref>, which models users independently with their previous purchase reviews that may contain a lot noisy textual information.</p><p>Short-term user behavior may not contain enough preference signals. To address the above-mentioned issue, recent works including ZAM <ref type="bibr" target="#b0">[1]</ref>, TEM <ref type="bibr" target="#b5">[6]</ref> and RTM <ref type="bibr" target="#b6">[7]</ref> only consider recent user actions in a short span of time and adopt attentive models such as Transformer-based encoder to put more emphasis on relevant products in the purchase record. However, a user's recent behavior may not contain enough preference signals for product retrieval. For example, when user 𝑢 2 searches for "bag", the remotely relevant item in his/her purchase history is "heels", which is not directly relevant to bags. Therefore, it is hard to tell whether the user is looking for handbags, backpacks, or other kinds of bags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Insight of Our Proposed Approach</head><p>In this work, we propose to overcome the limitations of prior works in user preference modeling by exploring local and global user behavior patterns on a user successive behavior graph (SBG), which is constructed by utilizing short-term actions of all users. We then exploit high-order relations in the SBG to capture implicit collaborative patterns and preference signals with an efficient jumping graph convolution and learn enriched product representations for user preference modeling. Our approach addresses the aforementioned problems in the following two aspects.</p><p>Expanding the set of potentially intended products. While short-term user behavior usually contains a limited number of products which may be inadequate to reflect user preferences, the global SBG connects the products recently purchased by a user to other relevant or similar ones on the graph. In effect, it expands the set of potentially intended products of the user. For example, in Figure <ref type="figure" target="#fig_1">1</ref>, the heels 𝑖 3 is connected to the handbag 𝑖 4 because of the co-occurrence of 𝑖 3 and 𝑖 4 in the behavior sequence Seq 4 of user 𝑢 3 . On the global SBG, the behavior sequences Seq 3 and Seq 4 are connected by 𝑖 3 , and hence 𝑖 4 could be a potentially intended product for user 𝑢 2 .</p><p>Making connected products more similar in the latent space.</p><p>Since a user is often represented as the ensemble of the products he/she bought, learning better product representations is crucial for user preference modeling. We leverage graph convolution to exploit the connectivity patterns in the SBG and make the embeddings of connected products more similar. The enriched product representations can better reflect user preference. For example, by graph convolution, the fashion items 𝑖 3 and 𝑖 4 will be more similar. Therefore, when user 𝑢 2 searches for "bag", the handbag 𝑖 4 would be ranked higher than the backpack 𝑖 5 , because 𝑖 4 is more similar to 𝑖 3 than 𝑖 5 , and 𝑖 3 partially represents user 𝑢 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PROPOSED APPROACH</head><p>In this section, we present our proposed approach in detail, which is built on the popular latent space based product search framework that learns semantic representations for products, users, and text tokens in the same embedding space. On top of it, we aim to learn enriched product representations to better represent users for search personalization. As shown in Figure <ref type="figure">2</ref>, we first construct a successive behavior graph from observed user behavior sequences. Then, we employ an efficient graph convolution with jumping connections to enrich product representations. The enriched product vectors can subsequently be used to represent users fro better preference modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Latent Space Based Product Search Framework</head><p>The goal of product search is to retrieve the most relevant products from a candidate pool 𝐶 for a user with his/her current query. In this More formally, after receiving a query 𝑞 from a user 𝑢, the system is expected to predict the probability 𝑃 (𝑖 |𝑢, 𝑞) of user 𝑢 purchasing product 𝑖, for each product in the candidate set 𝐶, and then rank all the products by the probability. The available information includes the purchasing history of all the users and the text associated with the products, such as product name, product description, and product review. The latent space based product search framework learns entity embeddings from two tasks: the product retrieval task and the language modeling task.</p><p>Product Retrieval Task. It aims to retrieve relevant products w.r.t. the current query. In <ref type="bibr" target="#b1">[2]</ref>, the user intent 𝑴 𝑢𝑞 is represented by a mix of the query 𝒒 and the user's preference vector 𝒖:</p><formula xml:id="formula_0">𝑴 𝑢𝑞 = 𝜆𝒒 + (1 − 𝜆)𝒖,<label>(1)</label></formula><p>where 𝜆 ∈ [0, 1] is a balancing parameter. As such, 𝑴 𝑢𝑞 encodes both the semantic meaning of the query and user preference. Then, the probability of purchasing product 𝑖 is computed as</p><formula xml:id="formula_1">𝑃 (𝑖 |𝑢, 𝑞) = exp(𝑓 (𝒊, 𝑴 𝑢𝑞 )) 𝑖 ′ ∈𝐶 exp(𝑓 (𝒊 ′ , 𝑴 𝑢𝑞 )) , (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where 𝐶 is the set of all possible candidate products and 𝑓 is a similarity measure such as cosine similarity. Language Modeling Task. It aims to learn the embeddings of queries and products by modeling the text information. <ref type="bibr" target="#b1">[2]</ref> proposes to jointly learn the word embedding 𝒘 and product embedding 𝒊 from the product's associated text by the paragraph vector (PV) model <ref type="bibr" target="#b22">[23]</ref>. The PV model assumes that words or tokens can be generated from the entity and maximizes the likelihood</p><formula xml:id="formula_3">𝑃 (𝑇 𝑖 |𝑖) = 𝑤 ∈𝑇 𝑖 exp(𝜏 (𝑤, 𝑖)) 𝑤 ′ ∈𝑉 exp(𝜏 (𝑤 ′ , 𝑖)) ,<label>(3)</label></formula><p>where 𝜏 denotes a scoring function of product 𝑖 and its associated word 𝑤, and 𝑇 𝑖 is the set of words associated with 𝑖.</p><p>With the learned word embeddings, a query is then represented by a function of the word embeddings:</p><formula xml:id="formula_4">𝒒 = 𝜙 ({𝒘 𝑞 |𝑤 𝑞 ∈ 𝑞}),<label>(4)</label></formula><p>where 𝑤 𝑞 is a word of query 𝑞, and 𝜙 can be a non-linear sequential encoder such as LSTM or Transformer. Since queries are usually short and the order of words often does not matter, we simply use an average function to obtain the query embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Efficient Graph Convolution with Jumping Connections</head><p>As mentioned in Sec. 3, to improve user preference modeling and learn better product representations, we propose to utilize a global successive behavior graph and perform graph convolution over the graph to capture implicit and complex collaborative signals.</p><p>Here, we adopt an efficient graph convolution layer with jumping connections and provide theoretical analysis to show its advantage. Efficient Graph Convolution. In the past few years, graph convolutional networks (GCN) and variants <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b34">35]</ref> have been successfully applied to learn useful graph node representations for various graph learning and mining tasks. In each layer of GCN, it performs feature propagation and transformation with connected nodes in the graph:</p><formula xml:id="formula_5">𝑯 (𝑙) = 𝜎 Â𝑯 (𝑙−1) 𝑾 (𝑙) ,<label>(5)</label></formula><p>where Â = 𝑰 + 𝑫 −1 𝑨 is the (normalized) adjacency matrix with self-loops, and 𝑫 is the degree matrix. 𝑯 (𝑙) is the node embeddings produced by layer 𝑙. 𝑾 (𝑙) denotes trainable parameters, and 𝜎 is a non-linear function such as ReLU(•). The projection layers (trainable parameters 𝑾 (𝑙) ) and activation layers (𝜎) as shown in Eq. ( <ref type="formula" target="#formula_5">5</ref>) are commonly included in many GCNbased methods. However, as observed from our empirical study, the projection layers may distort the semantic product representations learned by language modeling in methods such as ZAM or HEM. Hence, we propose to use graph convolution without the projection layers to enrich product representations. Following the efficient design in Li et al. <ref type="bibr" target="#b24">[25]</ref>, we remove the projection layers and activation layers. Further, we add a balancing parameter 𝜔 to control the strength of self-information:</p><formula xml:id="formula_6">𝑯 (𝑙) = 𝜔𝑰 + (1 − 𝜔)𝑫 −1 𝑨 𝑯 (𝑙−1) . (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>Jumping Graph Convolution Layer. Since user purchase behavior is often sparse, it is helpful to aggregate high-order information on the successive behavior graph to model potential user interest, as discussed in Sec. 3.2. However, the ordinary graph convolution suffers from the well-known over-smoothing problem <ref type="bibr" target="#b23">[24]</ref>, i.e., stacking too many convolution layers may make the node features (product representations) indistinguishable. To address this issue, Chen et al. <ref type="bibr" target="#b10">[11]</ref> proposed GCNII that adds initial residual connections to each GCN layer. We follow the same design to add jumping connections, i.e., feeding each convolution layer an additional input of the initial product representations 𝑯 (0) :</p><formula xml:id="formula_8">H (𝑙) = 𝜔𝑰 + (1 − 𝜔)𝑫 −1 𝑨 𝛽𝑯 (0) + (1 − 𝛽) H (𝑙−1) ,<label>(7)</label></formula><p>where 𝛽 is a weight parameter determining the portion of initial features. Our experiments in Sec. 5.3.2 verify the effectiveness of jumping connections, which can alleviate the over-smoothing effect and enable utilizing high-order relations on the graph. Further, we provide a theoretical analysis of jumping group convolution by measuring the diversity of product representations after graph convolution using Laplacian-Beltrami operator Ω(•) <ref type="bibr" target="#b11">[12]</ref>. We compare the diversity of product representations with jumping connections ( H (𝑙) in Eq. ( <ref type="formula" target="#formula_8">7</ref>)) and those without jumping connections (𝑯 (𝑙) in Eq. ( <ref type="formula" target="#formula_6">6</ref>)). We employ Laplacian-Beltrami operator, which measures the total variance of connected nodes:</p><formula xml:id="formula_9">Ω(𝑯 ) = ∑︁ 𝑘 ∑︁ 𝑖,𝑗 𝑎 𝑖 𝑗 (𝑯 𝑖,𝑘 − 𝑯 𝑗,𝑘 ) 2 . (<label>8</label></formula><formula xml:id="formula_10">)</formula><p>High Ω(𝑯 ) indicates high diversity, and low Ω(𝑯 ) indicates severe over-smoothing. The following theorem shows jumping connections can substantially alleviate the over-smoothing effect of graph convolution.</p><p>Theorem 1. If the initial diversity Ω(𝑯 (0) ) &gt; 0, then for any integer 𝑙 &gt; 0, 𝛽 ∈ (0, 1), and 𝜔 ∈ (0.5, 1), H (𝑙) is strictly more diverse than 𝑯 (𝑙) , i.e., Ω H (𝑙) &gt; Ω 𝑯 (𝑙) .</p><p>When 𝑙 approaches infinity, jumping connections can prevent the diversity of product representations from collapsing to 0 (over-smoothing), i.e.,</p><formula xml:id="formula_12">lim 𝑙→∞ Ω H (𝑙) &gt; lim 𝑙→∞ Ω 𝑯 (𝑙) = 0. (<label>10</label></formula><formula xml:id="formula_13">)</formula><p>Proof. The proof is provided in the Appendix. □</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Modeling User Behavior with Graph Convolution</head><p>Here, we show how to utilize local and global user behavior patterns to improve user representations in a zero attention model with the efficient jumping graph convolution. First, we construct a bipartite successive behavior graph. Then, we stack multiple jumping graph convolution layers to enrich product representations. Finally, we incorporate the graph-enriched product vectors into the zero attention model for user preference modeling. Graph Construction. To construct the successive behavior graph, we first define successive behavior sequences in the training set. First, we sort all the observed purchased records in a chronological order for each user. If the time interval between two consecutive actions is within a period 𝑅 (e.g., a day, a week, or a month), the two actions are considered as successive and will be placed in the same successive behavior sequence. Then, we construct a successive behavior graph 𝐺 𝑆𝐵 , which is a bipartite graph between sequences and products. If and only if a product 𝑖 is in a sequence 𝑆, we form an edge between 𝑖 and 𝑆, denoted as 𝐺 𝑆𝐵 (𝑖, 𝑆) = 1.</p><p>Enriching Product Representations with Graph Convolution. To enrich product representations, we apply jumping graph convolution as introduced in Sec. 4.2 on the successive behavior graph 𝐺 𝑆𝐵 . Let 𝒉 (0) 𝑗 denote the input of the first graph convolution layer for any entity 𝑗. We use the embeddings learned with PV as 𝒉 (0) 𝑖 for a product 𝑖. For a sequence 𝑠, 𝒉 (0) 𝑠 is randomly initialized. We then apply 𝐿 efficient jumping graph convolution layers as defined in Eq. ( <ref type="formula" target="#formula_8">7</ref>) and obtain the graph-enriched product embedding h(𝐿) 𝑖 for each product. Using Graph-enriched Product Representations for User Preference Modeling. Based on the observation that the effect of personalization varies significantly in respect of query characteristics. Ai et al. <ref type="bibr" target="#b0">[1]</ref> proposed ZAM that introduces a zero-vector to adaptively control the degree of personalization. The representation of a user 𝑢 is composed of his/her recently visited products, which is computed as</p><formula xml:id="formula_14">𝒖 = ∑︁ 𝑖 ∈𝐼 𝑢 ∪0 exp(𝑠 (𝑞, 𝑖)) exp(𝑠 (𝑞, 0)) + 𝑖 ′ ∈𝐼 𝑢 exp(𝑠 (𝑞, 𝑖 ′ )) 𝒊,<label>(11)</label></formula><p>where 𝐼 𝑢 is the product set in user 𝑢's history visit records, 0 is a zero vector. The attention score 𝑠 (𝑞, 𝑖) of a given product 𝑖 w.r.t. the current query 𝑞 is defined as</p><formula xml:id="formula_15">𝑠 (𝑞, 𝑖) = (𝒊 ⊤ tanh(𝑾 ⊤ 𝑓 𝒒 + 𝒃 𝑓 )) ⊤ 𝑾 ℎ ,<label>(12)</label></formula><p>where 𝑾 ℎ ∈ R 𝑑 𝑎 , 𝑾 𝑓 ∈ R 𝑑×𝑑 𝑎 ×𝑑 , 𝒃 𝑓 ∈ R 𝑑×𝑑 𝑎 are the trainable parameters, and 𝑑 𝑎 is the hidden dimension of the user-product attention network. In particular, 𝑒𝑥𝑝 (𝑠 (𝑞, 0)) is calculated by Eq. ( <ref type="formula" target="#formula_15">12</ref>) with 𝒊 as a learnable inquiry vector 0 ′ ∈ R 𝑑 .</p><p>To incorporate the graph-enriched product representations for user preference modeling, we use h(𝐿)</p><p>𝑖 to substitute 𝒊 in Eq. ( <ref type="formula" target="#formula_14">11</ref>) and Eq. ( <ref type="formula" target="#formula_15">12</ref>), i.e.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝒖 = ∑︁</head><p>𝑖 ∈𝐼 𝑢 ∪0 exp(𝑠 (𝑞, 𝑖)) exp(𝑠 (𝑞, 0))</p><formula xml:id="formula_16">+ 𝑖 ′ ∈𝐼 𝑢 exp(𝑠 (𝑞, 𝑖 ′ )) h(𝐿) 𝑖 , 𝑠 (𝑞, 𝑖) = ( h⊤ 𝑖 tanh(𝑾 ⊤ 𝑓 𝒒 + 𝒃 𝑓 )) ⊤ 𝑾 ℎ . (<label>13</label></formula><formula xml:id="formula_17">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Model Optimization</head><p>Following ZAM <ref type="bibr" target="#b0">[1]</ref>, we jointly optimize the product retrieval task and the language modeling task. The product retrieval loss is</p><formula xml:id="formula_18">𝐿 𝑃𝑅 = − ∑︁ (𝑢,𝑖,𝑞) log 𝑃 (𝑖 |𝑢, 𝑞) = − ∑︁ (𝑢,𝑖,𝑞) log exp(𝑓 (𝒊, 𝑴 𝑢𝑞 )) 𝑖 ′ ∈𝐶 exp(𝑓 (𝒊 ′ , 𝑴 𝑢𝑞 )) ,<label>(14)</label></formula><p>which is optimized over all triples (𝑢, 𝑖, 𝑞) in the training set, where a triple (𝑢, 𝑖, 𝑞) represents a product 𝑖 purchased by a user 𝑢 under the submitted query 𝑞. The language modeling loss is</p><formula xml:id="formula_19">𝐿 𝐿𝑀 = − ∑︁ 𝑖 log 𝑃 (𝑇 𝑖 |𝑖) = − ∑︁ 𝑖 ∑︁ 𝑤 ∈𝑇 𝑖 log exp(𝜏 (𝑤, 𝑖)) 𝑤 ′ ∈𝑉 exp(𝜏 (𝑤 ′ , 𝑖)) .<label>(15)</label></formula><p>Hence, the total loss is</p><formula xml:id="formula_20">𝐿 𝑡𝑜𝑡𝑎𝑙 = 𝐿 𝑃𝑅 + 𝐿 𝐿𝑀 = − ∑︁ (𝑢,𝑖,𝑞) log 𝑃 (𝑖 |𝑢, 𝑞) − ∑︁ 𝑖 log 𝑃 (𝑇 𝑖 |𝑖). (<label>16</label></formula><formula xml:id="formula_21">)</formula><p>Remark. It is worth noting that we only use the graph-enriched product embeddings to represent users but do not use them to represent products themselves in the product retrieval task (Eq. ( <ref type="formula" target="#formula_1">2</ref>)) or the language modeling task (Eq. ( <ref type="formula" target="#formula_3">3</ref>)), because the mixed representations may make products lose their uniqueness and hurt performance, which is verified by our empirical study. Since the candidate set 𝐶 and vocabulary 𝑉 are usually extremely large, it is impractical to compute the log likelihood in Eq. ( <ref type="formula" target="#formula_18">14</ref>) and Eq. <ref type="bibr" target="#b14">(15)</ref>. A common solution is to sample only a portion of negative products to approximate the denominator of Eq. ( <ref type="formula" target="#formula_18">14</ref>) and Eq. ( <ref type="formula" target="#formula_19">15</ref>):</p><formula xml:id="formula_22">𝐿 = − ∑︁ (𝑢,𝑖,𝑞) [log 𝜏 (𝑤, 𝑖) + 𝑘 𝑤 E 𝑤 ′ ∼𝑃 𝑤 log 𝜏 (−𝑤 ′ , 𝑖) + log 𝑓 (𝑖, 𝑀 𝑢𝑞 ) + 𝑘 𝑖 E 𝑖 ′ ∼𝑃 𝑖 (log 𝑓 (−𝑖, 𝑀 𝑢𝑞 ))],<label>(17)</label></formula><p>where 𝑘 𝑤 and 𝑘 𝑖 are the negative sampling rates for words and products, respectively. In this work, we follow HEM <ref type="bibr" target="#b1">[2]</ref> to randomly sample negative words from the vocabulary with 𝑃 𝑤 as the unigram distribution raised to the 3/4rd power, and randomly sample negative products from all products in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we introduce our experimental settings and present experimental results. Our experiments try to answer the following research questions:</p><p>• RQ1: How is the performances of SBG compared to the base model ZAM? • RQ2: How does SBG perform compared to state-of-the-art methods for personalized product search? • RQ3: How useful is graph convolution? Can the proposed jumping connection alleviate over-smoothing? • RQ4: What is the effect of time interval 𝑅 on the constructed successive behavior graph 𝐺 𝑆𝐵 ?</p><p>5.1 Experimental Setup 5.1.1 Datasets. Our experiments are conducted on the well-known Amazon review dataset<ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b29">[30]</ref>, which includes product reviews and metadata such as product titles and categories. It was first introduced for product search by Van Gysel et al. <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b33">34]</ref> and has become a benchmark dataset for evaluating product search methods as used in many recent studies <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b26">27]</ref>. Product reviews are generally used as text corpus for representing products or users, and product categories are used as queries to simulate a search scenario.</p><p>In our experiments, we use the 5-core data, and for each sub dataset, we filter out products with no positive records. The statistics of the filtered datasets are shown in Table <ref type="table" target="#tab_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Baselines.</head><p>We compare our proposed approach SBG with the following baselines.</p><p>• HEM <ref type="bibr" target="#b1">[2]</ref> assumes that users and products are independent. It employs PV to learn the representations of users, products, and words jointly. The user-query pair is projected to the same latent space with products. • ZAM <ref type="bibr" target="#b0">[1]</ref> also employs PV to learn semantic representations of products and words. Users are represented by the products they visited. In particular, ZAM proposes a zero attention vector to control the degree of personalization.</p><p>• DREM <ref type="bibr" target="#b2">[3]</ref> employs knowledge graph embedding techniques and constructs a unified knowledge graph that represents both static entity features and dynamic user searching behaviors. Embeddings of all entities are learned via a graph regularization loss.</p><p>• GraphSRRL <ref type="bibr" target="#b26">[27]</ref> explicitly utilizes structural patterns in a user-query-product graph. It defines three specific structural patterns that represent three frequent user-query-product interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Evaluation Protocol.</head><p>We partition each sub dataset into a training set, a validation set, and a test set. Similar to the process of constructing 𝐺 𝑆𝐵 , we sort the reviews in chronological order for each user and split the full record into successive behavior sequences. Then, the last sequence is used as the test set, and the second last sequence is used as the validation set.</p><p>For performance measurement, we adopt three metrics: hit rate (HR@K), normalized discounted cumulative gain (NDCG@K), and mean reciprocal rank (MRR). For each sequence, the target products are mixed up with candidates randomly sampled from the entire product set, forming a candidate set of size 1000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Implementation Details.</head><p>For a fair comparison, we re-implement HEM, ZAM, and DREM using the same encoder, evaluation setting, and negative sampling method. For DREM, we build a unified heterogeneous graph that contains the user-product review relation and the product-category belonging relation. In addition, we connect products and users with their associated words during training. For GraphSRRL, we use the official implementation<ref type="foot" target="#foot_1">2</ref> with our dataset splits and evaluation protocols. For all methods, the batch size is set to 1024, and the ADAM optimizer is used with an initial learning rate of 0.001. All the entity embeddings are initialized randomly with dimension 64. For our SBG, we set the attention dimension 𝑑 𝑎 to 8, and the user-query balancing parameter 𝜆 to 0.5. We employ 4 layers of jumping graph convolution, and the weight of self-loop is set to 0.1. The strength of jumping connection 𝛽 is also set to 0.1. The negative sampling rate for each word is set to 5, and that for each item is set to 2. We report the evaluation metrics on the converged model. The reported results are averaged over multiple runs, and the significant differences are computed based on the paired t-test with 𝑝 ≤ 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Main Results</head><p>Table <ref type="table">2</ref> summarizes the overall performance of our SBG and the baselines on eight Amazon review sub datasets. Besides, the improvement percentages of SBG and the best baseline DREM (also Table <ref type="table">2</ref>: Comparison of our proposed method with baselines on eight Amazon review sub datasets. * and † denote the significant differences to ZAM and the best baseline, respectively in paired t-test with 𝑝 ≤ 0.01. The best results are highlighted in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Software</head><p>Magazine HR@10 MRR@100 NDCG@10 NDCG@20 NDCG@100 HR@10 MRR@100 NDCG@10 NDCG@20 NDCG@100 HEM 0. Toys&amp;Games HR@10 MRR@100 NDCG@10 NDCG@20 NDCG@100 HR@10 MRR@100 NDCG@10 NDCG@20 NDCG@100 HEM Clothing HR@10 MRR@100 NDCG@10 NDCG@20 NDCG@100 HR@10 MRR@100 NDCG@10 NDCG@20 NDCG@100 HEM Home&amp;Kitchen HR@10 MRR@100 NDCG@10 NDCG@20 NDCG@100 HR@10 MRR@100 NDCG@10 NDCG@20 NDCG@100 HEM  graph-based) over ZAM are summarized in Table <ref type="table" target="#tab_6">3</ref>. We can make the following observations.</p><p>• First of all, SBG significantly improves over ZAM in every tested domain/dataset. Since NDCG reflects the quality of the entire ranking list, we calculate the performance gain of SBG compared to ZAM in NDCG@10 as shown in Table <ref type="table" target="#tab_6">3</ref> • GraphSRRL assumes that the pre-defined patterns are frequent, which may not hold in some domains, as evidenced by the experimental results. The results in HR@10 and NDCG@10 are reported. 𝑅 is varied from a day to a year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis and Discussion</head><p>layers 𝐿 and the jumping connections. We conduct experiments on Phones and Toys&amp;Games and vary 𝐿 from 0 to 64. We compare our SBG with jumping connections (denoted as SBG (wj)) with ZAM and a variant SBG (oj), which stands for SBG without jumping connections. The results are shown in Figure <ref type="figure">3</ref>.</p><p>It can be seen that a few graph convolution layers can bring substantial performance gains. In our experiments on Phones and Toys&amp;Games, we achieve the best performance with 𝐿 = 4. However, as 𝐿 increases, the performances of SBG (wj) and SBG (oj) drop due to the over-smoothing effect. Especially on Toys&amp;Games, we observe a significant performance drop when 𝐿 is larger than 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Effect of Jumping Connections.</head><p>It can be seen from Figure 3 that as 𝐿 increases, the performance of SBG (wj) drops much slower than that of SBG (oj), especially on Phones, demonstrating the effectiveness of jumping connections in alleviating the oversmoothing effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Effect of Time Interval</head><p>𝑅. To answer RQ4, we evaluate the performance of SBG with respect to different time scale 𝑅 ∈ {𝑎 𝑑𝑎𝑦, 𝑎 𝑤𝑒𝑒𝑘, 𝑎 𝑚𝑜𝑛𝑡ℎ, 𝑎 𝑞𝑢𝑎𝑡𝑒𝑟, 𝑎 𝑦𝑒𝑎𝑟 }. The results shown in Figure <ref type="figure">4</ref>. It can be observed that the best performance is usually achieved when 𝑅 is 𝑎 𝑑𝑎𝑦 or 𝑎 𝑤𝑒𝑒𝑘. When 𝑅 is 𝑎 𝑚𝑜𝑛𝑡ℎ or longer, the performances often drop. This is probably because the behavior sequences are overly diverse and contain noisy preference signals when the time scale is large. The only exception is on Magazine, where the performance of SBG slightly increases as 𝑅 becomes larger. However, Magazine is a small dataset that does not have much training data and many products have limited records in the test set. Hence, content-based product features may not be informative enough, and a larger 𝑅 helps a cold product reach to broader neighborhood and leads to better performance. in the experiments. As shown in Figure <ref type="figure">5</ref>, the running time of our method is comparable with that of ZAM and DREM. All the experiments are conducted on a platform with Intel(R) Xeon(R) Gold 6226R CPU @ 2.90GHz and GeForce RTX 3090.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this work, we have proposed a generic approach to model user preferences for personalized product search. To exploit local and global user behavior patterns for search personalization, we construct a successive behavior graph and capture implicit user preference signals with an efficient jumping graph convolution. Our approach can be used as a plug-and-play module in the popular latent space based product search framework and potentially in many other methods to improve their performance. Extensive experiments on public datasets demonstrate the effectiveness of our approach. In future work, we plan to investigate the possibility of applying our approach on dynamic behavior graphs for user preference modeling.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of exploiting global user successive behavior for personalized product search. Given the query bag issued by user 𝑢 2 , the system is expected to retrieve suitable bags satisfying the user intent. By modeling the global user successive behavior graph with graph convolution, our proposed approach can capture implicit preference signals and yield desirable results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>5. 3 . 1 Figure 4 :</head><label>314</label><figDesc>Figure4: Ablation study of the time interval 𝑅 for successive graph construction. The results in HR@10 and NDCG@10 are reported. 𝑅 is varied from a day to a year.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>5. 3 . 4 Figure 5 :</head><label>345</label><figDesc>Figure 5: Comparison of the average training time (in secon Home&amp;Kitchen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Dataset Statistics.</figDesc><table><row><cell></cell><cell>Magazine</cell><cell cols="3">Software Phones Toys&amp;Games</cell></row><row><cell cols="2"># reviews 4,583</cell><cell>25,086</cell><cell>133,792</cell><cell>148,756</cell></row><row><cell>#user</cell><cell>694</cell><cell>3,642</cell><cell>17,464</cell><cell>16,370</cell></row><row><cell>#query</cell><cell>170</cell><cell>999</cell><cell>163</cell><cell>399</cell></row><row><cell cols="2">#product 876</cell><cell>5,875</cell><cell>10,278</cell><cell>11,875</cell></row><row><cell>#seq</cell><cell>2,337</cell><cell>17,814</cell><cell>79,224</cell><cell>78,616</cell></row><row><cell>#edge</cell><cell>3,078</cell><cell>16,391</cell><cell>93,174</cell><cell>111,578</cell></row><row><cell></cell><cell cols="4">Instruments Clothing Health Home&amp;Kitchen</cell></row><row><cell cols="2"># reviews 209,229</cell><cell>270,854</cell><cell>334,025</cell><cell>545,083</cell></row><row><cell>#user</cell><cell>23,887</cell><cell>37,914</cell><cell>36,639</cell><cell>65,510</cell></row><row><cell>#query</cell><cell>492</cell><cell>2,000</cell><cell>793</cell><cell>900</cell></row><row><cell cols="2">#product 9,756</cell><cell>23,033</cell><cell>17,956</cell><cell>27,888</cell></row><row><cell>#seq</cell><cell>100,945</cell><cell>160,959</cell><cell>201,513</cell><cell>333,709</cell></row><row><cell>#edge</cell><cell>149,401</cell><cell>189,985</cell><cell>256,095</cell><cell>408,607</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>The improvement percentages of NDCG@10 over ZAM by DREM (the best baseline) and our SBG.</figDesc><table><row><cell></cell><cell>Magazine</cell><cell cols="2">Software Phones Toys&amp;Games</cell></row><row><cell cols="2">DREM +8.08%</cell><cell>-7.06%</cell><cell>+13.44% +33.96%</cell></row><row><cell>SBG</cell><cell>+25.99%</cell><cell>+11.10%</cell><cell>+15.30% +37.50%</cell></row><row><cell></cell><cell cols="3">Instruments Clothing Health Home&amp;Kitchen</cell></row><row><cell cols="2">DREM -10.83%</cell><cell>-25.34%</cell><cell>+35.00% +17.89%</cell></row><row><cell>SBG</cell><cell>+7.76%</cell><cell>+8.99%</cell><cell>+41.39% +26.53%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>. SBG improves ZAM by at least 7.76% on Instruments, and up to 41.39% on Health. RQ1 is answered.• SBG achieves significant improvements over other baselines in nearly all cases, which answers RQ2. • As shown in Table3, the performances of DREM and SBG are consistent on all domains. In the domains where DREM fails including Software, Instruments, and Clothing, the improvement percentages of SBG are also less significant. It indicates that the effectiveness of graph-based methods may be affected by the characteristics of different domains. Figure 3: Ablation studies of jumping connections and the number 𝐿 of the graph convolutional layers. The results in NDCG@10 with respect to 𝐿 on Toys&amp;Games and Phones are reported. SBG (wj) stands for our proposed SBG with jumping connections, and SBG (oj) stands for SBG without jumping connections.</figDesc><table><row><cell></cell><cell>0.39</cell><cell></cell><cell>Phones</cell><cell></cell></row><row><cell>NDCG@10</cell><cell>0.34 0.36 0.37 0.38 0.35</cell><cell>ZAM</cell><cell>SBG (oj)</cell><cell>SBG (wj)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Toys&amp;Games</cell><cell></cell></row><row><cell></cell><cell>0.36</cell><cell></cell><cell></cell><cell></cell></row><row><cell>NDCG@10</cell><cell>0.30 0.32 0.34</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.28</cell><cell>ZAM</cell><cell>SBG (oj)</cell><cell>SBG (wj)</cell></row><row><cell></cell><cell>1 2 3 4</cell><cell>8</cell><cell>16 L</cell><cell>32</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">http://jmcauley.ucsd.edu/data/amazon</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://github.com/Shawn-hub-hit/GraphSRRL-master</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>We would like to thank the anonymous reviewers for their helpful comments. This research was supported by the grants of P0034058 (ZGAL) and P0038850 (ZGD1) funded by Alibaba and the General Research Fund No.15222220 funded by the UGC of Hong Kong.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>In this section, we provide a proof for Theorem 1. For a better reading experience, we rewrite Eq. <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b6">7)</ref> here. The Laplacian-Beltrami operator, which measures the diversity of embeddings, is defined as:</p><p>The convolution without jumping connection is defined as:</p><p>The convolution with jumping connection is defined as:</p><p>Theorem 1. If the initial diversity Ω(𝑯 (0) ) &gt; 0, then for any integer 𝑙 &gt; 0, 𝛽 ∈ (0, 1), and 𝜔 ∈ (0.5, 1), H (𝑙) is strictly more diverse than 𝑯 (𝑙) :</p><p>While 𝑙 approaches infinity, jumping connections can prevent the diversity of product representations from collapsing to 0 (over-smoothing), i.e.,</p><p>Proof. Let 𝑳 = 𝑰 − 𝑫 −1 𝑨, then Ω(•) becomes</p><p>where 𝑯 :,𝑘 is the 𝑘-th column of 𝑯 . Denote arbitrary column of 𝑯 by ℎ, then we only need to prove <ref type="formula">19</ref>) and ( <ref type="formula">20</ref>), we could obtain general formula for ℎ (𝑙) and h(𝑙) :</p><p>Denote the eigen-decomposition of 𝑳 by 𝑼 𝚲𝑼 ⊤ , where 𝑼 is the eigenbasis and 𝚲 is a diagonal matrix with corresponding eigenvalues, then</p><p>where 𝑴 = 𝑰 − (1 − 𝜔)𝚲. Denote 𝑐 = 𝑼 ⊤ ℎ (0) and substitute 𝑭 in Eq. <ref type="bibr" target="#b23">(24,</ref><ref type="bibr" target="#b24">25)</ref> by <ref type="bibr" target="#b25">(26)</ref>:</p><p>where</p><p>Now, we only need to compare 𝑔 𝑙 (𝜆) and 𝑓 𝑙 (𝜆). Notice that 𝜔 ∈ (0.5, 1) and 𝜆 is the eigenvalue of the normalized graph laplacian 𝑳, so 𝜆 ∈ [0, 2] and 1 − (1 − 𝜔)𝜆 ∈ (0, 1]. Then, 𝑔 𝑙 (𝜆) is positive and decreases as 𝑙 increases:</p><p>The equality holds only if 𝜆 = 0. When it comes to 𝑓 𝑙 (𝜆), we have</p><p>Given Eq. <ref type="bibr" target="#b28">(29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35)</ref>, we can conclude Ω( h(𝑙) ) ≥ Ω(ℎ (𝑙) ).</p><p>Notice that the initial diversity Ω(ℎ (0) ) &gt; 0, so there exists such 𝜆 𝑖 that 𝜆 𝑖 𝑐 2 𝑖 &gt; 0 and 𝜆 𝑖 ≠ 0, and the equality does not hold and the inequality ( <ref type="formula">21</ref>) is proved. Now, we consider the limits of 𝑓 𝑙 (𝜆) and 𝑔 𝑙 (𝜆): </p><p>Eq. ( <ref type="formula">22</ref>) is also proved. □</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Zero Attention Model for Personalized Product Search</title>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">N</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management, CIKM 2019</title>
				<meeting>the 28th ACM International Conference on Information and Knowledge Management, CIKM 2019<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-11-03">2019. November 3-7, 2019</date>
			<biblScope unit="page" from="379" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning a Hierarchical Embedding Model for Personalized Product Search</title>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keping</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Shinjuku, Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017-08-07">2017. August 7-11, 2017</date>
			<biblScope unit="page" from="645" to="654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Explainable Product Search with a Dynamic Relation Embedding Model</title>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keping</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploring Differences in the Impact of Users&apos; Traces on Arabic and English Facebook Search</title>
		<author>
			<persName><forename type="first">Ismail</forename><surname>Badache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/WIC/ACM International Conference on Web Intelligence, WI 2019</title>
				<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-10-14">2019. October 14-17, 2019</date>
			<biblScope unit="page" from="225" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fresh and Diverse Social Signals: Any Impacts on Search</title>
		<author>
			<persName><forename type="first">Ismail</forename><surname>Badache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohand</forename><surname>Boughanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Conference Human Information Interaction and Retrieval</title>
				<meeting>the 2017 Conference on Conference Human Information Interaction and Retrieval<address><addrLine>Oslo, Norway</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-03-07">2017. 2017. March 7-11, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Transformer-based Embedding Model for Personalized Product Search</title>
		<author>
			<persName><forename type="first">Keping</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event</title>
				<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-07-25">2020. July 25-30, 2020</date>
			<biblScope unit="page" from="1521" to="1524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning a Fine-Grained Review-based Transformer Model for Personalized Product Search</title>
		<author>
			<persName><forename type="first">Keping</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;21: The 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, Virtual Event</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-07-11">2021. July 11-15, 2021</date>
			<biblScope unit="page" from="123" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Leverage Implicit Feedback for Context-aware Product Search</title>
		<author>
			<persName><forename type="first">Keping</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Choon</forename><surname>Hui Teo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yesh</forename><surname>Dattatreya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijai</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGIR 2019 Workshop on eCommerce, co-located with the 42st International ACM SIGIR Conference on Research and Development in Information Retrieval, eCom@SIGIR 2019</title>
		<title level="s">CEUR Workshop Proceedings</title>
		<meeting>the SIGIR 2019 Workshop on eCommerce, co-located with the 42st International ACM SIGIR Conference on Research and Development in Information Retrieval, eCom@SIGIR 2019<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-07-25">2019. July 25, 2019</date>
			<biblScope unit="volume">2410</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Personalized product search based on user transaction history and hypergraph learning</title>
		<author>
			<persName><forename type="first">Xuxiao</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jihua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueming</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multim. Tools Appl</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="22157" to="22175" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">How useful is social feedback for learning to rank YouTube videos</title>
		<author>
			<persName><forename type="first">Sergiu</forename><surname>Chelaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Orellana-Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ismail</forename><surname>Sengör</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Altingövde</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note>World Wide Web</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simple and Deep Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
				<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07-18">2020. 13-18 July 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
	<note>Virtual Event (Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Spectral graph theory</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Rk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>American Mathematical Soc</publisher>
			<biblScope unit="volume">92</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Random walks on the click graph</title>
		<author>
			<persName><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Szummer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR 2007: Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007-07-23">2007. July 23-27, 2007</date>
			<biblScope unit="page" from="239" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05">2016. 2016. December 5-10, 2016</date>
			<biblScope unit="page" from="3837" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Graph embeddings for one-pass processing of heterogeneous queries</title>
		<author>
			<persName><forename type="first">Chi Thang</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dung</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minn</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Weidlich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 36th International Conference on Data Engineering (ICDE)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1994">2020. 1994-1997</date>
		</imprint>
	</monogr>
	<note>Quoc Viet Hung Nguyen, and Karl Aberer</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Smoothing clickthrough data for web search ranking</title>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kefeng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009-07-19">2009. 2009. July 19-23, 2009</date>
			<biblScope unit="page" from="355" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">node2vec: Scalable Feature Learning for Networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016-08-13">2016. August 13-17, 2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attentive long short-term preference modeling for personalized product search</title>
		<author>
			<persName><forename type="first">Yangyang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinglong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohan</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-modal preference modeling for product search</title>
		<author>
			<persName><forename type="first">Yangyang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin-Shun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohan</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM international conference on Multimedia</title>
				<meeting>the 26th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1865" to="1873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning Latent Vector Spaces for Product Search</title>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Van Gysel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International Conference on Information and Knowledge Management, CIKM 2016</title>
				<meeting>the 25th ACM International Conference on Information and Knowledge Management, CIKM 2016<address><addrLine>Indianapolis, IN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016-10-24">2016. October 24-28, 2016</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
	<note>Maarten de Rijke, and Evangelos Kanoulas</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deeper Insights Into Graph Convolutional Networks for Semi-Supervised Learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
				<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018-02-02">2018. February 2-7, 2018</date>
			<biblScope unit="page" from="3538" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Label efficient semi-supervised learning via graph filtering</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9582" to="9591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning Better Representations for Neural Information Retrieval with Graph Information</title>
		<author>
			<persName><forename type="first">Xiangsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;20: The 29th ACM International Conference on Information and Knowledge Management</title>
				<meeting><address><addrLine>Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-10-19">2020. October 19-23, 2020</date>
			<biblScope unit="page" from="795" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Structural Relationship Representation Learning with Graph Embedding for Personalized Product Search</title>
		<author>
			<persName><forename type="first">Shang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanli</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;20: The 29th ACM International Conference on Information and Knowledge Management, Virtual Event</title>
				<meeting><address><addrLine>Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-10-19">2020. October 19-23, 2020</date>
			<biblScope unit="page" from="915" to="924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName><forename type="first">Tomás</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st International Conference on Learning Representations, ICLR 2013</title>
				<meeting><address><addrLine>Scottsdale, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-05-02">2013. May 2-4, 2013</date>
		</imprint>
	</monogr>
	<note>Workshop Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning Robust Models for e-Commerce Product Search</title>
		<author>
			<persName><forename type="first">Thanh</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Subbian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05">2020. July 5-10, 2020</date>
			<biblScope unit="page" from="6861" to="6869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Justifying recommendations using distantly-labeled reviews and fine-grained aspects</title>
		<author>
			<persName><forename type="first">Jianmo</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Dual Heterogeneous Graph Attention Network to Improve Long-Tail Performance for Shop Search in E-Commerce</title>
		<author>
			<persName><forename type="first">Xichuan</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bofang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haochuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongbo</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3405" to="3415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">DeepWalk: online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;14</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014-08-24">2014. August 24 -27, 2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">CGTR: Convolution Graph Topology Representation for Document Ranking</title>
		<author>
			<persName><forename type="first">Yuanyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yansong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiran</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</title>
				<meeting>the 29th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2020">Jun Guo. 2020</date>
			<biblScope unit="page" from="2173" to="2176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semantic Entity Retrieval Toolkit</title>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Van Gysel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR 2017 Workshop on Neural Information Retrieval (Neu-IR&apos;17)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Maarten de Rijke, and Evangelos Kanoulas</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Global Context Enhanced Graph Neural Networks for Session-based Recommendation</title>
		<author>
			<persName><forename type="first">Ziyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianling</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event</title>
				<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-07-25">2020. July 25-30, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dynamic Bayesian Metric Learning for Personalized Product Search</title>
		<author>
			<persName><forename type="first">Teng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaiqiao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangsong</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management, CIKM 2019</title>
				<meeting>the 28th ACM International Conference on Information and Knowledge Management, CIKM 2019<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-11-03">2019. November 3-7, 2019</date>
			<biblScope unit="page" from="1693" to="1702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning a Product Relevance Model from Click-Through Data in E-Commerce</title>
		<author>
			<persName><forename type="first">Shaowei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keping</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongbo</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;21: The Web Conference 2021, Virtual Event / Ljubljana</title>
				<meeting><address><addrLine>Slovenia</addrLine></address></meeting>
		<imprint>
			<publisher>ACM / IW3C2</publisher>
			<date type="published" when="2021-04-19">2021. April 19-23, 2021</date>
			<biblScope unit="page" from="2890" to="2899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Neural IR Meets Graph Embedding: A Ranking Model for Product Search</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-05-13">2019. 2019. May 13-17, 2019</date>
			<biblScope unit="page" from="2390" to="2400" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
