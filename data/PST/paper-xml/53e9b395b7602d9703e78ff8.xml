<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feature Selection Algorithms: A Survey and Experimental Evaluation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Luis</forename><forename type="middle">Carlos</forename><surname>Molina</surname></persName>
							<email>lcmolina@lsi.upc.es</email>
							<affiliation key="aff0">
								<orgName type="department">Universitat Politècnica de Catalunya Departament de Llenguatges i Sistemes Informátics</orgName>
								<address>
									<addrLine>Jordi Girona 1-3</addrLine>
									<postCode>C6, 08034</postCode>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lluís</forename><surname>Belanche</surname></persName>
							<email>belanche@lsi.upc.es</email>
							<affiliation key="aff0">
								<orgName type="department">Universitat Politècnica de Catalunya Departament de Llenguatges i Sistemes Informátics</orgName>
								<address>
									<addrLine>Jordi Girona 1-3</addrLine>
									<postCode>C6, 08034</postCode>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Àngela</forename><surname>Nebot</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Universitat Politècnica de Catalunya Departament de Llenguatges i Sistemes Informátics</orgName>
								<address>
									<addrLine>Jordi Girona 1-3</addrLine>
									<postCode>C6, 08034</postCode>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Feature Selection Algorithms: A Survey and Experimental Evaluation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">558163418D944D10C4FFFD31427503B0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In view of the substantial number of existing feature selection algorithms, the need arises to count on criteria that enables to adequately decide which algorithm to use in certain situations. This work assesses the performance of several fundamental algorithms found in the literature in a controlled scenario. A scoring measure ranks the algorithms by taking into account the amount of relevance, irrelevance and redundance on sample data sets. This measure computes the degree of matching between the output given by the algorithm and the known optimal solution. Sample size effects are also studied.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The feature selection problem in terms of supervised inductive learning is: given a set of candidate features select a subset defined by one of three approaches: a) the subset with a specified size that optimizes an evaluation measure, b) the subset of smaller size that satisfies a certain restriction on the evaluation measure and c) the subset with the best commitment among its size and the value of its evaluation measure (general case). The generic purpose pursued is the improvement of the inductive learner, either in terms of learning speed, generalization capacity or simplicity of the representation. It is then possible to understand better the results obtained by the inducer, diminish its volume of storage, reduce the noise generated by irrelevant or redundant features and eliminate useless knowledge.</p><p>A feature selection algorithm (FSA) is a computational solution that is motivated by a certain definition of relevance. However, the relevance of a feature -as seen from the inductive learning perspective-may have several definitions depending on the objective that is looked for. An irrelevant feature is not useful for induction, but not all relevant features are necessarily useful for induction <ref type="bibr" target="#b4">[5]</ref>.</p><p>In this research, several fundamental algorithms found in the literature are studied to assess their performance in a controlled scenario. To this end, a measure to evaluate FSAs is proposed that takes into account the particularities of relevance, irrelevance and redundance on the sample data set. This measure computes the degree of matching between the output given by a FSA and the known optimal solution. Sample size effects are also studied. The results illustrate the strong dependence on the particular conditions of the FSA used and on the amount of irrelevance and redundance in the data set description, relative to the total number of features. This should prevent the use of a single algorithm specially when there is poor knowledge available about the structure of the solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Algorithms for Feature Selection</head><p>A FSA should be seen as a computational approach to a definition of relevance, although in many cases these definitions are followed in a somewhat loose sense. For a review of such definitions, see <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Feature Selection Definition</head><p>Let be the original set of features, with cardinality ¡ ¡ £¢ ¥¤</p><p>. The continuous feature selection problem refers to the assignment of weights ¦ ¨ § to each feature © § in such a way that the order corresponding to its theoretical relevance is preserved. The binary feature selection problem refers to the assignment of binary weights. This can be carried out directly (like many FSAs in machine learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9]</ref>), or filtering the output of the continuous problem solution (see §4.1). These are quite different problems reflecting different design objectives. In the continuous case, one is interested in keeping all the features but in using them differentially in the learning process. On the contrary, in the binary case one is interested in keeping just a subset of the features and using them equally in the learning process.</p><p>The feature selection problem can be seen as a search in a hypothesis space (set of possible solutions). In the case of the binary problem, the number of potential subsets to evaluate is . In this case, a general definition is [13]: Definition 1 (Feature Selection) Let "! be an evaluation measure to be optimized (say to maximize) defined as $# &amp;% ' )( <ref type="formula">10</ref>. The selection of a feature subset can be seen under three considerations: (general case). Notice that, with these definitions, an optimal subset of features is not necessarily unique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Characterization of FSAs</head><p>There exist in the literature several considerations to characterize feature selection algorithms <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14]</ref>. In view of them it is possible to describe this characterization as a search problem in the hypothesis space as follows:</p><p>2 Search Organization. General strategy with which the space of hypothesis is explored. This strategy is in relation to the portion of hypothesis explored with respect to their total number.</p><p>2 Generation of Successors. Mechanism by which pos- sible variants (successor candidates) of the current hypothesis are proposed.</p><p>2 Evaluation Measure. Function by which successor candidates are evaluated, allowing to compare different hypothesis to guide the search process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Search Organization</head><p>A search algorithm is responsible for driving the feature selection process using a specific strategy. Each state in the search space specifies a weighting ¦ RQ TS VU WU VU FS X¦ of the pos- sible features of , with ¡ ¡ Y¢ `¤ . In the binary case, ¦ § ba dc eS Wf hg , whereas in the continuous case ¦ § pi c eS Wf Wq .</p><p>Notice we are stating that relevance should be upper and lower bounded. Also in the binary case a partial order r exists in the search space, with s Q r ts Fu if s Q 9 s &amp;u , whereas in the continuous case s Q r vs Fu if, for all w , ¦ P § x ys Q ! ¦ P § x ys u ! holds. Being a (labeled) list of weighed subsets of features (i.e. states), maintains the (ordered) current list of solutions. The labels indicate the value of the evaluation measure. We consider three types of search: exponential, sequential and random. Most sequential algorithms are characterized by ¡ £¡ ¢ f , whereas exponential and random ones typically use ¡ F¡ I f . Exponential Search: It corresponds to algorithms that carry out searches whose cost is y h !</p><p>. The exhaustive search is an optimal search, in the sense that the best solution is guaranteed. An optimal search need not be exhaustive; for example, if an evaluation measure is monotonic a BRANCH AND BOUND <ref type="bibr" target="#b16">[17]</ref> algorithm is optimal. A measure is monotonic if for any two subsets s Q dS s u and s Q % s u , then A ys Q ! PI A s u !</p><p>. Another example would be an search with an admissible heuristic <ref type="bibr" target="#b17">[18]</ref>.</p><p>Sequential Search: This sort of search selects one among all the successors to the current state. This is done in an iterative manner and once the state is selected it is not possible to go back. Although there is no explicit backtracking the number of such steps must be limited by ¤ ! in order to qualify as a sequential search. The complexity is determined taking into account the number of evaluated sub- sets in each state change. The cost of this search is therefore polynomial ¤ Q !</p><p>. Consequently, these methods do not guarantee an optimal result, since the optimal solution could be in a region of the search space that is not visited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random Search:</head><p>The idea underlying this type of search is to use its randomness to avoid the algorithm to stay on a local minimum and to allow temporarily moving to other states with worse solutions. These are anytime algorithms <ref type="bibr" target="#b13">[14]</ref> and can give several optimal subsets as solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Generation of Successors</head><p>All of the operators act by modifying in some way the weights ¦ ¨ § of the features © § , with ¦ ¨ § 0</p><p>(in the case of the weighting operator), or ¦ ¨ § Ea dc eS Vf g (for the rest).</p><p>Forward: Add features to the current solution , among those that have not been selected yet. In each step, the feature that makes be greater is added to the solution. Starting with</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>¢ fe</head><p>, the forward step consists of:</p><formula xml:id="formula_0"># ¢ hg a d© § ji ¡ hg a V© § g ! is biggerg (1)</formula><p>The stopping criterion can be:</p><formula xml:id="formula_1">¡ ¡ 3¢ 5¤ (if</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>¤</head><p>has been fixed in advance), the value of has not increased in the last k steps, or it surpasses a prefixed value hl . The cost of the operator is ¤ !</p><p>. The main disadvantage is that it is not possible to have in consideration certain basic interactions among features. For example, if © Q S m© u are such that A na V© Q S m© u hg ! Ao A na d© Q g ! S W A na V© u g ! , neither © Q and © u could be selected, in spite of being very useful.</p><p>Backward: Remove features from the current solution , among those that have not been removed yet. In each step, the feature that makes be greater is removed from the solution. Starting with ¢ , the backward step is:</p><formula xml:id="formula_2"># ¢ i a d© § ¡ A i a V© § pg ! is biggerg (2)</formula><p>The stopping criterion can be:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>¡ ¡ ¢ q¤</head><p>, the value of has not increased in the last k steps, or it falls below a prefixed value hl . This operator remedies some problems although there still will be many hidden interactions (in the sense of being unobtainable). The cost is ¤ !</p><p>, although in practice it demands more computation than forward <ref type="bibr" target="#b11">[12]</ref>.</p><p>Both . Weighting: In the weighting operators, the search space is continuous, and all of the features are present in the solution to a certain degree. A successor state is a state with a different weighting. This is typically done by iteratively sampling the available set of instances. Random: This group includes those operators that can potentially generate any other state in a single step. The rest of operators can also have random components, but they are restricted to some criterion of "advance" in the number of features or in improving the measure at each step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Evaluation Measures Probability of error:</head><p>Provided the ultimate goal is to build a classifier able of correctly labelling instances generated by the same probability distribution, minimizing the (bayesian) probability of error of the classifier seems to be the most natural choice. Therefore, it is also a clear choice for .</p><p>Let © { 0 represent the unlabeled instances, and ¢ a W Q S WU WU VU FS p Yg a set of labels (classes), so that # 0 ( . Such probability is defined as <ref type="bibr" target="#b6">[7]</ref>:</p><formula xml:id="formula_3">~ ¢ i Bf ¨ E § r § ¡ © ! q @ V© ! p ©<label>(3)</label></formula><p>where d©</p><formula xml:id="formula_4">! ¢ § H Q V© ¡ § ! r @ § ! is the (unconditional)</formula><p>probability distribution of the instances, and @ § ¡ © ! is the a posteriori probability of @ § being the class of © .</p><p>Since the class-conditional densities are usually unknown, they can either be explicitly modeled (using parametric or non-parametric methods) or implicitly via the design of a classifier that builds the respective decision boundaries between the classes <ref type="bibr" target="#b6">[7]</ref>. Some of these classifiers, like the one-nearest-neighbor rule, have a direct relation to the probability of error. This may require the use of more elaborate methods than a simple holdout procedure (cross validation, bootstrapping) in order to yield a more reliable value. Divergence: These measures compute a probabilistic distance or divergence among the class-conditional probability densities V© ¡ § !</p><p>, using the general formula:</p><formula xml:id="formula_5">¢ 5 t i V© ¡ Q ! S d© ¡ u ! q © (4)</formula><p>To qualify as a valid measure, the function t must be such that the value of satisfies the following conditions: are good ones, the divergence among the conditional probabilities will be significant. Poor features will result in very similar probabilities. Some classical choices are: Chernoff, Bhattacharyya, Kullback-Liebler, Kolmogorov and Matusita <ref type="bibr" target="#b6">[7]</ref>. Dependence: These measures quantify how strongly two features are associated with one another, in the sense that knowing the value of one it is possible to predict the value of the other. In the context of feature selection, a feature is better evaluated the better it predicts the class. Interclass distance: These measures are based on the assumption that instances of a different class are distant in the instance space. It is enough then to define a metric between classes and use it as measure:</p><formula xml:id="formula_6">r § pS p y ! ¢ f § y @ p Q © &amp; § X S X© y m x ! (5) ¢ § H Q @ § ! y m &amp; § Q r &amp;y ! X r § XS X &amp;y ! (6)</formula><p>being © § y the instance k of class @ § , and § the number of instances of the class @ § . The most usual distances belong to the Euclidean family. These measures do not require the modeling of any density function, but their relation to the probability of error can be very loose. Information or Uncertainty: Similarly to the probabilistic dependence, we may observe © and compute the a posteri- ori probabilities r § ¡ © !</p><p>to determine how much information on the class of © has been gained, with respect to its prior probability. If all the classes become roughly equally probable, then the information gain is minimal and the uncertainty (entropy) is maximum.</p><p>Consistency: An inconsistency in and s is defined as two instances in s that are equal when considering only the features in and that belong to different classes. The aim is thus to find the minimum subset of features leading to zero inconsistencies <ref type="bibr" target="#b0">[1]</ref>. The inconsistency count of an instance ¡ s is defined as [14]:</p><formula xml:id="formula_7">¢ s£ ¤ P¥ ! ¢ ! E ! (7</formula><p>) where E ! is the number of instances in s equal to using only the features in and !</p><p>is the number of instances in s of class equal to using only the features in . The inconsistency rate of a feature subset in a sample</p><formula xml:id="formula_8">s is then: ¢ h¦ ! ¢ ¨ § @© hª ¢ D£ ¤ ¥ y ! ¡ s ¡ (8)</formula><p>This is a monotonic measure, in the sense</p><formula xml:id="formula_9">Q 9 « u R¬ ¢ D¦ Q ! I '¢ h¦ u ! U</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">General Schemes for Feature Selection</head><p>The relationship between a FSA and the inducer chosen to evaluate the usefulness of the feature selection process can take three main forms: embedded, filter and wrapper.</p><p>Embedded Scheme: The inducer has its own FSA (either explicit or implicit). The methods to induce logical conjunctions <ref type="bibr" target="#b19">[20]</ref> provide an example of this embedding. Other traditional machine learning tools like decision trees or artificial neural networks are included in this scheme <ref type="bibr" target="#b14">[15]</ref>.</p><p>Filter Scheme: If the feature selection process takes place before the induction step, the former can be seen as a filter of non-useful features prior to induction. In a general sense it can be seen as a particular case of the embedded scheme in which feature selection is used as a pre-processing. The filter schemes are independent of the induction algorithm.</p><p>Wrapper Scheme: In this scheme the relationship is taken the other way around: it is the FSA that uses the learning algorithm as a subroutine <ref type="bibr" target="#b10">[11]</ref>. The general argument in favor of this scheme is to equal the bias of both the FSA and the learning algorithm that will be used later on to assess the goodness of the solution. The main disadvantage is the computational burden that comes from calling the induction algorithm to evaluate each subset of considered features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">General Algorithm for Feature Selection</head><p>An abstract algorithm that unifies the behavior of any FSA is depicted in Fig. <ref type="figure">1</ref>. In particular, being a (weighed) list of weighed subsets of features (i.e. states), keeps the ordered set of solutions in course. Exponential algorithms are typically characterized by ¡ ¡ I f (examples are BRANCH AND BOUND <ref type="bibr" target="#b16">[17]</ref> or <ref type="bibr" target="#b17">[18]</ref>). The presence in the list is a function of the evaluation measure and defines the expansion order. Heuristic search algorithms also keep this list (of open nodes), and the weighting is the value of the heuristic. Random search methods as Evolutionary Algorithms <ref type="bibr" target="#b1">[2]</ref> are characterized by ¡ ¡ I f (the list is the pop- ulation and the weighting is the fitness value of the individuals). Sequential algorithms maintain ¡ ¡ h¢ f , though there are exceptions (e.g., a bidirectional algorithm <ref type="bibr" target="#b7">[8]</ref> would use ¡ ¡ ¢ ). The second weighting (on the features of each solution subset) allows to include the two types of FSA according to their outcome (see §2.1).</p><p>The initial list ® is in general built out of the original set of features and the algorithm maintains the best solution at all times (s T°± ² nw n¯¤ ). At each step, a FSA with a given search organization manipulates the list in a specific way and calls its mechanism for the generation of successors which in turn uses . The result is an updated list and the eventual update of the best solution found so far. Notice that the data sample s is considered global to the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Empirical Evaluation of FSAs</head><p>The first question arising in relation to a feature selection experimental design is: what are the aspects that we would like to evaluate of a FSA solution in a given data set? In this study we decided to evaluate FSA performance with respect to four particularities: relevance, irrelevance, redundance and sample size. To this end, several fundamental FSAs are studied to assess their performance on synthetic data sets with known relevant features. Then sample data sets of different sizes are corrupted with irrelevant and/or redundant features. The experiments are designed to test the endurance of different FSAs (e.g., behaviour against the ratio number-of-irrelevant vs. number-of-relevant features).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Particularities to be evaluated</head><p>Relevance: Different families of problems are generated by varying the number of relevant features ´³ . These are features that, by construction, have an influence on the output and whose role can not be assumed by the rest (i.e., there is no redundance).</p><p>Irrelevance: Irrelevant features are defined as those features not having any influence on the output, and whose values are generated at random for each example. For a problem with ³ relevant features, different numbers of irrelevant features ¶µ are added to the corresponding data sets (thus providing with several subproblems for each choice of ´³ ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Redundance:</head><p>In these experiments, a redundance exists whenever a feature can take the role of another (perhaps the simplest way to model redundance). This is obtained by choosing a relevant feature randomly and replicating it in the data set. For a problem with ³ relevant features, different numbers of redundant features</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>³ F¥</head><p>are added in a way analogous to the generation of irrelevant features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I n p u t :</head><p>• ¸d a t a s a m pl e w i t h f e a t u r e s ¹ º W» ¹ ¼» d½ ¿¾ À ¸e v a l u a t i o n m e a s ur e t o be maximized Á ¨• p¸s u c c e s s o r g e n e r a t i o n o p e r a t o r Output :</p><p>• £Â VÃ HÄ DÅ AE Â ¾ ¸( w e i ghe d ) f e a t u r e s u b s e t Ç È ½ S t a r t _ P o i n t ( ¹ ) ;</p><formula xml:id="formula_10">• £Â VÃ HÄ DÅ ÉAE Â ¾ È ½ { b e s t o f Ç a c c o r d i n g t o À } ;</formula><p>r e p e a t Ç È ½ S e a r c h _ S t r a t e g y ( </p><formula xml:id="formula_11">Ç º Á ¨• Ê À Ë º p¹ ) ; ¹ ÍÌ È ½ { b e s t o f Ç a c c o r d i n g t o À } ; i f À Ê ¹ Ì Ë @Î ÏÀ Ê • £Â dÃ BÄ DÅ ÉAE Â ¾ Ë or Ê À Ê ¹ ÍÌ Ë ½ À Ê • £Â dÃ HÄ hÅ ÉAE Â ¾ Ë and » ¹ Ì » hÐ G» • £Â dÃ HÄ hÅ ÉAE Â ¾ » Ë t h e n</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation of Performance</head><p>The score criterion expresses the degree to which a solution obtained by a FSA matches the correct solution. This criterion behaves as a similarity Õ h © S mÖ ! # Ø× ¼ Ù( i c eS Wf Wq in the classical sense <ref type="bibr" target="#b5">[6]</ref>, satisfying: </p><formula xml:id="formula_12">1. Õ D © S mÖ ! ¢ f RÚ ¬ © ¢ Ö 2. Õ D © S mÖ ! ¢ Õ D Ö £S</formula><formula xml:id="formula_13">2 s ¤ Ü ! ¢ c ´Ú ¬ Ü ¢ µ 2 s ¤ Ü ! ¢ f Ú ¬ Ü ¢ 2 s ¤ Ü ! v ¡s ¤ yÜ r!</formula><p>indicates that Ü is more similar to than Ü . The score is defined in terms of the similarity in that for all Ü % 7 S s ¤ yÜ ! ¢ Õ D Ü S !</p><p>. This scoring measure will also be parameterized, so that it can ponder each type of divergence (in relevance, irrelevance and redundance) to the optimal solution. The set of parameters is expressed as</p><formula xml:id="formula_14">Ñ E¢ a Ñ ³ S Ñ µ S Ñ ³ F¥ g with Ñ Ò I c and Ñ Ò ¢ f .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intuitive Description</head><p>The criterion s ¤ Ü ! penalizes three situations:</p><p>1. There are relevant features lacking in Ü (the solution is incomplete).</p><p>2. There are more than enough relevant features in Ü (the solution is redundant).</p><p>3. There are some irrelevant features in Ü (the solution is incorrect). An order of importance and a weight will be assigned (via the Ñ Ò parameters), to each of these situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formal Description</head><p>The precedent point <ref type="bibr">(3.</ref>) is simple to model: if suffices to check whether ¡ Ü µ ¡ v 5c , being Ü the solution of the FSA.</p><p>Relevance and redundance are strongly related given that, in this context, a feature is redundant or not depending on what other relevant features are present in Ü .</p><p>Notice then that the optimal solution is not unique, though all them should be equally valid for the score. To this end, the features are broken down in equivalence classes, where elements of the same class are redundant to each other (i.e., any optimal solution must comprise only one feature of each equivalence class).</p><p>Being Ü a feature set, we define a binary relation be- tween two features © § mS X© ey Ü as: © § ¶à x© ey $Ú Í¬ `© § and © 3y represent the same information. Clearly à is an equivalence relation. Let Ü ¶á be the quotient set of Ü under à , Ü ´á ¢ a si © âq ¡ © ÔÜ g , any optimal solution Ü ¶ will satisfy:</p><p>1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>¡ Ü ¶ ¡ D¢ w¡ ³ ¡</head><p>2. ã Íi © § q ¼Ü ´á G# hä s© y Ïi © § q # © y ÔÜ ¶ We denote by Ü ¶ any of these solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Construction of the score</head><p>In the present case, the set to be split in equivalence classes is formed by all the relevant features (redundant or not) chosen by a FSA. We define then: </p><formula xml:id="formula_15">Ü á ³ ¢ yÜ ³ g Ü ³ F¥ ! á (equivalence</formula><formula xml:id="formula_16">Let Ü á ³ På á ³ ¢ a si © § q á ³ ¡ ä Fi © ey Wq ÔÜ á ³ # âi © 3y q % i © § q yg</formula><p>and define, for ae quotient set:</p><formula xml:id="formula_17">ç yae ! ¢ è é ê © hë ¡ © ¡ «f !</formula><p>The idea is to express the quotient between the number of redundant features chosen by the FSA and the number it could have chosen, given the relevant features present in its solution. In the precedent notation, this is written (provided the denominator is not null): ç yÜ ´á ³ ! ç yÜ á ³ å á ³ !</p><p>Let us finally build the score, formed by three terms: relevance, irrelevance and redundance. Defining:</p><formula xml:id="formula_18">¢ ¢ f ¡ Ü µ ¡ ¡ µ ¡ S ¦ ¢ ¡ Ü á ³ ¡ ¡ ³ ¡ S with Ü á ³ ¢ yÜ ³ g Ü ³ F¥ ! á ¦ ¢ íì c if ç Ü ´á ³ å á ³ ! ¢ c Q î ¤ ï ¥ î &amp;ð f ¨ ñ Bò ó ï ñ Bò ó ï Fô ¤ ó ï õ otherwiseU the score is s ¤ Ü ! ¢ öÑ ³ ¦ ÷Ó Ñ ³ F¥ ¦ Þ @Ó Ñ µ ¢ S mÜ % ø U</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Restrictions on the</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ñ Ò</head><p>We can establish now the desired restrictions on the behavior of the score. From the more to the less severe: there are relevant features lacking, there are irrelevant features, and there is redundancy in the solution. This is reflected in the following conditions on the Ñ Ò : 1. Choosing an irrelevant feature is better than missing a relevant one: ù ï î ¤ ï î v vù hú î ¤ ú î</p><p>2. Choosing a redundant feature is better than choosing an irrelevant one:</p><formula xml:id="formula_19">ù hú î ¤ ú î v ù ï ¥ î ¤ ï ¥ î We also define Ñ Ò ¢ c if ¡ Ò ¡ P¢ c</formula><p>. Notice that the denominators are important for, as an example, expressing the fact that it is not the same choosing an irrelevant feature when there were only two that when there were three (in the latter case, there is an irrelevant feature that could have been chosen when it was not).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Evaluation</head><p>In this section we detail the experimental methodology and quantify the various parameters of the experiments. The basic idea consists on generating sample data sets with known particularities (synthetic functions t ) and hand them over to the different FSAs to obtained a hypothesis û . The divergence between the defined function and the obtained hypothesis will be evaluated by the score criterion. This experimental design is illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. </p><formula xml:id="formula_20">ü ý ý þ Hÿ þ ¡ £¢ ¥¤ §¦ Hþ þ ¥© § £¤ £© §¢ ¥¤ §¦ Hþ ¢ ¡ ¢ Éþ ¡ © þ ¡ ¦ Bý þ ¡© ! ¤ ÿ " # §" # $ þ ¤ þ ¡¦ Bþ ¡ % %¢ "ý " dý þ Hÿ þ ¡ £¢ ¥¤ § &amp; þ ¡¢ ¡ ý þ ¡ ! &amp; (' ) þ ¡¢ ¡ ý þ Éþ Bÿ þ ¡¦ ¡ ! ¤ 0 ÿ 1 ! ý $ £2 3 4 5¢ ¡2 76 ÿ þ 5 8 Hþ 5¦ ! ý þ ¡9 &amp; @ A B</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Description of the FSAs used</head><p>The ten FSAs used in the experiments were : E-SFG, QBB, LVF, LVI, C-SBG, RELIEF, SFBG, SFFG, W-SBG, and W-SFG (see Table <ref type="table" target="#tab_5">1</ref>). The algorithms E-SFG, W-SFG are versions of SFG using entropy and the accuracy of a C4.5 inducer, respectively. The algorithms C-SBG, W-SBG are versions of SBG using consistency and the accuracy of a C4.5 inducer, respectively. During the course of the experiments the algorithms FOCUS, B&amp;B, ABB and LVW were put aside due to their unaffordable consumption of resources. For a review of all these algorithms, see <ref type="bibr" target="#b15">[16]</ref>.</p><p>Since RELIEF and E-SFG give as output an ordered list of features © § according to their weight ¦ ¨ § , a filtering criterion is necessary to transform this solution to a subset of features. The procedure used here is simple: since the interest is in determining a good cut point, first those ¦ § fur- ther than two variances from the mean are discarded (that is to say, with very high or very low weights). Then define</p><formula xml:id="formula_21">Õ § ¢ ¦ § Ó ¦ § §C FQ and D y ¢ y § H u Õ § . The objective is to search for the feature © y such that: f ¨ D y D ¤ ¼k ¤ is maximum.</formula><p>The cut point is then set between © ey and © 3y Q . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementations of Data Families</head><p>A total of twelve families of data sets were generated studying three different problems and four instances of each, by varying the number of relevant features </p><formula xml:id="formula_22">¢ f if © FQ HG IE FE FE #G © ¥ ! QP © ¥ Q RG SE E FE G © ! , with ¤ ¢ ¨¤ w 5T if ¤ is even and ¤ ¢ ¤ w 5T ! &amp;Ó f if ¤ is odd. GMonks:</formula><p>This problem is a generalization of the classic monks problems <ref type="bibr" target="#b18">[19]</ref>. In its original version, three independent problems were applied on sets of ¤ «¢ VU features that take values of a discrete, finite and unordered set (nominal features). Here we have grouped the three problems in a single one computed on each segment of 6 features. Let ¤ be multiple of 6, ¢ í¤ w 5T U and u ¢ WU ff ! Ó f , for f &amp; . Let us denote for "1" the first value of a feature, for "2" the second, etc. The problems are the following:</p><formula xml:id="formula_23">1. f Þ# â © } ¢ © } Q ! XP © } QY ¢ f 2. Ỹ ¶# two or more © § ¢ f in © } E E FE m© } X3 . ba Í# â © } QY ¢ a cG Þ© } Xd ¢ f ! eP © } QY z ¢ a RG Þ© } Q ´z ¢ !</formula><p>For each segment, the boolean condition Ỹ fG hg P yf iG ba ! is checked. If this condition is satisfied for Õ or more segments with Õ ¢ í¤ qp w 5T (being ¤ qp the number of segments) the function GMonks is 1; otherwise, it is 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Setup</head><p>The experiments were divided in three groups. The first group refers to the relationship between irrelevance vs. relevance. The second refers to the relationship between redundance vs. relevance. The last group refers to sample size. Each group uses three families of problems (Parity, Disjunction and GMonks) with four different instances for each problem, varying the number of relevant features ¶³ </p><formula xml:id="formula_24">Ò ¢ ³ Ó f µ Ó f ³ ¥ , ¢ 3S</formula><p>Ñ E¢ c and µ ¢ ³ F¥ ¢ ³ w 5T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>Due to space reasons, only a sample of the results are presented, in Figs. <ref type="figure" target="#fig_4">3(a</ref>) and (b). Each point is the average of 10 independent runs with different random data samples. The horizontal axis represents the ratios between these particularities as explained above. The vertical axis represents the average results given by the score criterion.</p><p>In Fig. <ref type="figure" target="#fig_4">3</ref>(a) the C-SBG algorithm shows at first a good performance but clearly as the irrelevance ratio increases, it falls dramatically (below the 0.5 level from µ ¢ ³ on). Note that for ´³ ¢ sr performance is always perfect (the plot is on top of the graphic). The plots in Fig. <ref type="figure" target="#fig_4">3(b)</ref> show additional interesting results because we can appreciate the curse of dimensionality effect <ref type="bibr" target="#b9">[10]</ref>. In this figure, W-SBG present an increasingly poor performance (see the figure from top to bottom) with the number of features provided the number of examples is increasing in a linear way. However, in general, as long as more examples are added performance is better (see the figure from left to right).</p><p>A summary of the results is displayed in Fig. <ref type="figure" target="#fig_7">4</ref> for the ten algorithms, allowing for a comparison across all the sample data sets with respect to each studied particularity. Specifically, Figs. 4(a), (b) and (c) show the average score of each algorithm for irrelevance, redundance and sample size, respectively. In each graphic there are two keys: the key to the left shows the algorithms ordered by total average performance, from top to bottom. The key to the right shows the algorithms ordered by average performance on the last abscissa value, also from top to bottom. In other words, the left list is topped by the algorithm that wins on average, while the right list is topped by the algorithm that ends on the lead. This is also useful to help reading the graphics.</p><p>Fig. <ref type="figure" target="#fig_7">4</ref>(a) shows that RELIEF ends up on the lead of the irrelevance vs. relevance problems, while SFFG shows the best average performance. The algorithm W-SFG is also well positioned. Fig. <ref type="figure" target="#fig_7">4</ref>(b) shows that the algorithms LVF and LVI together with C-SBG are the overall best. In fact, there is a bunch of algorithms that also includes the two floating and QBB showing a close performance. Note how RELIEF and the wrappers are very poor performers. Fig. <ref type="figure" target="#fig_7">4(c)</ref> shows that the wrapper algorithms seem to be able to extract the most of the data when there is a shortage of it. Surprisingly,  the backward wrapper is just fairly positioned on average. The SFFG is again quite good on average, together with C-SBG. However, all of the algorithms are quite close and show the same kind of dependency to the data. Note the general poor performance of E-SFG, provided it is the only algorithm that computes its evaluation measure (entropy in this case) independently for each feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>The task of a feature selection algorithm (FSA) is to provide with a computational solution to the feature selection problem motivated by a certain definition of relevance. This algorithm should be reliable and efficient. The many FSAs proposed in the literature are based on quite different principles (as the evaluation measure used, the precise way to explore the search space, etc) and loosely follow different definitions of relevance. In this work a way to evaluate FSAs was proposed in order to understand their general behaviour on the particularities of relevance, irrelevance, redundancy and sample size of synthetic data sets. To achieve this goal, a set of controlled experiments using artificially generated data sets were designed and carried out. The set of optimal solutions is then compared with the output given by the FSAs (the obtained hypotheses). To this end, a scoring measure was defined to express the degree of approximation of    the FSA solution to the real solution. The final outcome of the experiments can be seen as an illustrative step towards gaining useful knowledge that enables to decide which algorithm to use in certain situations. The behaviour of the algorithms to different data particularities is shown and thus the danger in relying in a single algorithm. This points in the direction of using new hybrid algorithms or combinations thereof for a more reliable assessment of feature relevance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>is maximum when they are non-overlapping. If the features used in a solution 9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. FlowChart of Experimental Design.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>.:</head><label></label><figDesc>RelevanceThe different numbers ¶³ vary for each problem, as follows: {4, 8, 16, 32} (for Parity), {5, 10, 15, 20} (for Disjunction) and {6, 12, 18, 24} (for GMonks). Irrelevance: In these experiments, we have µ running from 0 to 2 times the value of ³ , in intervals of 0.2 (that is, eleven different experiments of irrelevance for each ¶³ ). Redundance: Similarly to the generation of irrelevant features, we have ´³ ¥ running from 0 to 2 times the value of ³ , in intervals of 0.2. Sample Size: Given the formula ¡ s ¡ e¢ wÑ Ò (see §3.1), different problems were generated considering G 5a 0.25, 0.5, 0.75, 1.0, 1.25, 1.75, 2.0g ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Sample Size -Parity -W-SBG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Some results to irrelevance, relevance and sample size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Results ordered by total average performance on the data sets (left inset) and by end performance (right inset).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Apply t consecutive forward steps and u con- secutive backward ones. If t wv xu the result is a forward operator, otherwise it is a backward one. An interesting approach is to perform the forward or the backward steps,</figDesc><table><row><cell>selecting the one making A respectively. The cost of the operator is then E g r! or A</cell><cell>ni s H! ¤ ! .</cell><cell>bigger,</cell></row><row><cell cols="3">Compound: depending on the respective values of . This allows to</cell></row><row><cell cols="3">discover new interactions among features. An interesting</cell></row><row><cell cols="3">"backtracking mechanism" is obtained, although other stop-ping conditions should be established if t ¢ u . For exam-ple, for t ¢ u ¢ f , if ©  § is added and © 3y is removed, this</cell></row><row><cell cols="3">could be undone in the following steps. A possible stop-ping criterion is ©  § ¢ © y . In sequential FSA, t {z ¢ u assures a maximum of ¤ steps, with a cost ¤ | } y Q !</cell></row><row><cell></cell><cell></cell><cell cols="2">operators (forward and backward) can be general-</cell></row><row><cell></cell><cell></cell><cell>ized selecting, at each step, subsets of elements</cell><cell>and</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 . FSAs used in the experiments.</head><label>1</label><figDesc></figDesc><table><row><cell>Algorithm</cell><cell>Search</cell><cell>Generation of</cell><cell>Evaluation</cell></row><row><cell></cell><cell>Organization</cell><cell>Succesors</cell><cell>Measure</cell></row><row><cell>LVF</cell><cell>Random</cell><cell>Random</cell><cell>Consistency</cell></row><row><cell>LVI</cell><cell>Random</cell><cell>Random</cell><cell>Consistency</cell></row><row><cell>QBB</cell><cell cols="3">Random/Expon. Random/Backward Consistency</cell></row><row><cell>RELIEF</cell><cell>Random</cell><cell>Weighting</cell><cell>Distance</cell></row><row><cell>C-SBG</cell><cell>Sequential</cell><cell>Backward</cell><cell>Consistency</cell></row><row><cell>E-SFG</cell><cell>Sequential</cell><cell>Forward</cell><cell>Entropy</cell></row><row><cell>SFBG</cell><cell>Exponential</cell><cell>Compound</cell><cell>Consistency</cell></row><row><cell>SFFG</cell><cell>Exponential</cell><cell>Compound</cell><cell>Consistency</cell></row><row><cell>W-SBG</cell><cell>Sequential</cell><cell>Backward</cell><cell>Accuracy(C4.5)</cell></row><row><cell>W-SFG</cell><cell>Sequential</cell><cell>Forward</cell><cell>Accuracy(C4.5)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>WU VU WU FS X© be the relevant features of a problem t .</figDesc><table><row><cell>³ . Let</cell></row><row><cell>© £Q S Parity: This is the classic binary problem of parity the output is t © £Q TS FE E FE &amp;S m© ! ¢ f if the number of ©  § ¤ , where ¢ f is odd and t © FQ dS E FE FE S X© ! ¢ c otherwise. Disjunction: A disjunctive task, with t © FQ TS E FE FE FS m© !</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported by the Spanish CICyT Project TAP99-0747 and by the Mexican Petroleum Institute.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning Boolean Concepts in the Presence of Many Irrelevant Features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Almuallim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="279" to="305" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Evolutionary Algorithms in Theory and Practice</title>
		<author>
			<persName><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Oxford University Press</publisher>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Selection of Relevant Features and Examples in Machine Learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Langley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence on Relevance</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Greiner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Subramanian</surname></persName>
		</editor>
		<imprint>
			<publisher>AI</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="245" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Greedy Attribute Selection</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 11th Int. Conf. on Machine Learning</title>
		<meeting>of the 11th Int. Conf. on Machine Learning<address><addrLine>New Brunswick, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="28" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Freitag</surname></persName>
		</author>
		<title level="m">How Useful is Relevance? Technical report, AAAI Symposium on Relevance</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Chandon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pinson</surname></persName>
		</author>
		<title level="m">Analyse Typologique. Masson</title>
		<imprint>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Pattern Recognition -A Statistical Approach</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Devijver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
			<publisher>Prentice Hall</publisher>
			<pubPlace>London, GB</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">An Evaluation of Feature Selection Methods and their Application to Computer Security</title>
		<author>
			<persName><forename type="first">J</forename><surname>Doak</surname></persName>
		</author>
		<idno>CSE-92-18</idno>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Univ. of California</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Correlation-based Feature Selection for Machine Learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>University of Waikato</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Feature Selection: Evaluation, Application, and Small Sample Performance</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zongker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="158" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Irrelevant Features and the Subset Selection Problem</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pfleger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 11th Int. Conf. on Machine Learning</title>
		<meeting>of the 11th Int. Conf. on Machine Learning<address><addrLine>New Brunswick, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="121" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Toward Optimal Feature Selection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sahami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 13th Int. Conf. on Machine Learning</title>
		<meeting>of the 13th Int. Conf. on Machine Learning<address><addrLine>Bari, IT</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="284" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Comparative Evaluation of medium and large-scale Feature Selectors for Pattern Classifiers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sklansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 1st Int. Workshop on Statistical Techniques in Pattern Recognition</title>
		<meeting>of the 1st Int. Workshop on Statistical Techniques in Pattern Recognition<address><addrLine>Prague</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="91" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Feature Selection for Knowledge Discovery and Data Mining</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Motoda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>London, GB</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Generalization as Search</title>
		<imprint>
			<date type="published" when="1982">1982</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="203" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Feature Selection Algorithms: A Survey and Experimental Evaluation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Belanche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nebot</surname></persName>
		</author>
		<idno>LSI-02-62-R</idno>
		<imprint>
			<date type="published" when="2002">2002</date>
			<pubPlace>Barcelona, Spain</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Universitat Politècnica de Catalunya</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Branch and Bound Algorithm for Feature Subset Selection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Narendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computer</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="917" to="922" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><surname>Heuristics</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>Addison Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The MONK&apos;s Problems: A Performance Comparison of Different Learning Algorithms</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Thrun</surname></persName>
		</author>
		<idno>CS-91-197</idno>
		<imprint>
			<date type="published" when="1991">1991</date>
			<pubPlace>Pittsburgh, PA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>CMU</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning Structural Descriptions from Examples</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Winston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Psychology of Computer Vision</title>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Winston</surname></persName>
		</editor>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>McGraw Hill</publisher>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
