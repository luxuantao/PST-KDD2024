<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Context Autoencoder for Self-Supervised Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of AI</orgName>
								<orgName type="laboratory">Key Lab. of Machine Perception (MoE)</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Hong</orgName>
								<address>
									<settlement>Kong 3 Baidu</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaodi</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ying</forename><surname>Xin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shentong</forename><surname>Mo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yunhao</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shumin</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Hong</orgName>
								<address>
									<settlement>Kong 3 Baidu</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gang</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of AI</orgName>
								<orgName type="laboratory">Key Lab. of Machine Perception (MoE)</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
							<email>&lt;wangjingdong@outlook.com&gt;</email>
						</author>
						<title level="a" type="main">Context Autoencoder for Self-Supervised Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel masked image modeling (MIM) approach, context autoencoder (CAE), for self-supervised learning. We randomly partition the image into two sets: visible patches and masked patches. The CAE architecture consists of: (i) an encoder that takes visible patches as input and outputs their latent representations, (ii) a latent context regressor that predicts the masked patch representations from the visible patch representations that are not updated in this regressor, (iii) a decoder that takes the estimated masked patch representations as input and makes predictions for the masked patches, and (iv) an alignment module that aligns the masked patch representation estimation with the masked patch representations computed from the encoder.</p><p>In comparison to previous MIM methods that couple the encoding and decoding roles, e.g., using a single module in BEiT, our approach attempts to separate the encoding role (content understanding) from the decoding role (making predictions for masked patches) using different modules, improving the content understanding capability. In addition, our approach makes predictions from the visible patches to the masked patches in the latent representation space that is expected to take on semantics. In addition, we present the explanations about why contrastive pretraining and supervised pretraining perform similarly and why MIM potentially performs better. We demonstrate the effectiveness of our CAE through superior transfer performance in downstream tasks: semantic segmentation, and object detection and instance segmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We study the masked image modeling task for selfsupervised representation learning. Masked image modeling (MIM) is a task of masking some patches of the input image and making predictions for the masked patches from the visible patches. It is expected that the resulting encoder network pretrained through solving the MIM task is able to extract the patch representations taking on semantics that are transferred to solving downstream tasks.</p><p>BEiT <ref type="bibr" target="#b3">(Bao et al., 2021)</ref> and the method studied in the ViT paper <ref type="bibr" target="#b21">(Dosovitskiy et al., 2021)</ref>, two MIM methods, learn a ViT (formed with self-attention) to predict the patch tokens and the pixels, respectively, and use the resulting ViT as the pretrained encoder. They take the visible patches and mask tokens representing the masked patches as input, and make predictions for both the visible and masked patches, where the predictions only for masked patches are evaluated during training. The two methods use the single ViT structure simultaneously for both encoding and decoding. Thus, only the partial capacity of the ViT is explored for encoding and representation learning, limiting the representation quality.</p><p>We present a context autoencoder (CAE) approach, illustrated in Figure <ref type="figure">1</ref>, for improving the encoding quality. We randomly partition the image into two sets of patches: visible patches and masked patches. There are four components: arXiv:2202.03026v1 [cs.CV] 7 Feb 2022</p><p>Context Autoencoder for Self-Supervised Representation Learning (a)</p><formula xml:id="formula_0">X v Z v Z m Y m z Zm X m y ?m F H G F Q m (b) X v Y m y ?m R Q m Y v (c) X Z Y X F G y X N</formula><p>Figure <ref type="figure">2</ref>: The computational graphs for (a) a context autoencoder (CAE), (b) BEiT <ref type="bibr" target="#b3">(Bao et al., 2021)</ref>, and (c) a denoising autoencoder (DAE). The parts in cornflower blue are for loss function. (a) The encoder F receives visible patches X v and outputs their latent representations Z v . The latent contextual regressor H predicts the latent representations Z m for masked patches from Z v . The decoder predicts the targets Y m for masked patches from Z m . z and y are the loss functions.</p><p>During training, the gradient is stopped for Zm . See the detail in Section 2. (b) The input includes both visible patches X v and mask queries Q m representing masked patches, and the representations for them are updated within the function R. (c) The function N is a noising function generating the noisy version X from the input X. F and G are the normal encoder and decoder, respectively. For simplicity, the positional embeddings are not included in computational graphs. (a) CAE and (c) DAE perform the encoding and decoding roles explicitly and separately, and (b) BEiT performs the encoding and decoding roles implicitly and simultaneously.</p><p>an encoder, a latent contextual regressor, a decoder, and an alignment module. The encoder, a ViT structure, takes only the visible patches as input and learns the latent representations only for the visible patches. The latent contextual regressor estimates the masked patch representations according to the visible patch representations. The decoder takes the regressed masked patch representations as input and makes predictions for the masked patches. Furthermore, we align the regressed masked patch representations with the masked patch representations computed from the encoder that is the same as the one for encoding visible patches.</p><p>The encoder in the top stream in Figure <ref type="figure">1</ref> operates only on visible patches, only focusing on learning semantic representations. We also want that representation learning is taken only by the encoder through two things: The latent representations of visible patches are not updated in the other modules; and the alignment module expects that the output representations of latent contextual regressor are close to the latent representations for the masked patches computed from the encoder. In comparison to BEiT and the approach in the ViT paper (See Figures <ref type="figure">2 (a</ref>) and (b) (black parts) for the difference), our CAE encoder exploits the whole capability for learning the representation, thus improving the representation quality.</p><p>Second, the prediction from the visible patches to the masked patches, i.e., generating a plausible semantic guess for the masked patches, is performed on the latent space using the latent contextual regressor. The predicted latent representations for the masked patches are constrained to match with the latent representations computed from the encoder. This expects that the latent representations can capture the semantics of the visual content as well as other information for predicting the targets.</p><p>Last, the CAE (including other MIM methods) makes predictions for randomly masked patches, thus caring about the representations for the patches. This indicates that our CAE encoder, e.g., pretrained on ImageNet-1K <ref type="bibr" target="#b15">(Deng et al., 2009)</ref>, learns semantics, not only for the center regions of the original images where the instances of the 1000 classes in ImageNet-1K usually lie, but also for other regions that potentially do not belong to the 1000 classes. This is different from typical contrastive pretraining methods (e.g., MoCo v3 <ref type="bibr">(Chen et al., 2021)</ref> and SimCLR <ref type="bibr">(Chen et al., 2020b)</ref>) that often compare global representations of augmented views and empirically exhibit a tendency to learn semantics mainly from the center patches of the original images.</p><p>We present the empirical performance of our approach on downstream tasks, semantic segmentation, and object detection and instance segmentation. The superior results over supervised pretraining, contrastive pretraining, and other MIM methods validate the aforementioned three points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Approach</head><p>Our context autoencoder (CAE) pretrains the encoder by solving the masked image modeling task. The architecture, shown in Figure <ref type="figure">1</ref>, consists of four parts: an encoder, a latent contextual regressor, a decoder, and an alignment module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Architecture</head><p>We randomly split an image into two sets of patches: visible patches X v and masked patches X m . The computational graph is provided in Figure <ref type="figure">2 (a)</ref>.</p><p>Encoder. The encoder F maps the visible patches X v to the latent representations Z v . It only handles the visible Figure <ref type="figure">3</ref>: Illustration of random block-wise sampling and random cropping. Random block-wise sampling is used in our approach. Random cropping is a key data-augmentation scheme for contrastive pretraining.</p><p>patches. We use the ViT to form our encoder. It first embeds the visible patches by linear projection as patch embeddings, and adds the positional embeddings P v . Then it sends the combined embeddings into a sequence of transformer blocks that are based on self-attention, generating Z m .</p><p>Latent contextual regressor. The latent contextual regressor H predicts the latent representations Z m for the masked patches from the latent representations Z v of the visible patches output from the encoder. We form the latent contextual regressor H using a series of transformer blocks that are based on cross-attention.</p><p>The initial queries Q m , called mask queries, are representations of mask patches that are learned as model parameters and are the same for all the masked patches. The keys and the values consists of the visible patch representations Z v and the output of the previous cross-attention layer (mask queries for the first cross-attention layer). The corresponding positional embeddings are considered when computing the cross-attention weights between the queries and the keys. In this process, the latent representations Z v of the visible patches are not updated.</p><p>Decoder. The decoder G maps the latent representations Z m of the masked patches to some forms Y m of the masked patches, e.g., discrete tokens this paper takes. The decoder, similar to the encoder, is a stack of transformer blocks that are based on self-attention. The decoder only receives the latent representations of the masked patches (the output of the latent contextual regressor), and the positional embeddings of the masked patches as input without directly using the information of the visible patches.</p><p>Latent representation alignment. The latent representation alignment module imposes the constraint on the latent representations Z m of the masked patches predicted by the latent contextual regressor. We feed the masked patches X m into the encoder, which is the same as the one for encoding visible patches, and generate the representations Zm of the masked patches. We then align the two latent representations Z m and Zm for the masked patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Objective Function</head><p>Masking and targets. Following BEiT <ref type="bibr" target="#b3">(Bao et al., 2021)</ref>, we adopt the random block-wise masking strategy (illustrated in Figure <ref type="figure">3</ref>) to split the input image into two sets of patches, visible and masked patches. For each image, 75 of 196 (14 ? 14) patches are masked.</p><p>We use the pre-trained DALL-E <ref type="bibr" target="#b46">(Ramesh et al., 2021)</ref> tokenizer to generate the discrete tokens for forming the targets. The input image is fed into the DALL-E tokenizer, assigning a discrete token to each patch. The target tokens for the masked patches are denoted as ?m .</p><p>Loss function. The loss function (illustrated in Figure <ref type="figure">2</ref> (a), the part in cornflower blue.) consists of a decoding loss: y (Y m , ?m ), and an alignment loss: z (Z m , Zm ). The whole loss is a weighted sum:</p><formula xml:id="formula_1">y (Y m , ?m ) + ? z (Z m , Zm ).</formula><p>(1)</p><p>We use the MSE loss for z (Z m , Zm ) and the cross-entropy loss for y (Y m , ?m ). ? is 2 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Analysis and Conenction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Analysis</head><p>The CAE encoder cares about the patch representations.</p><p>The CAE makes predictions for randomly masked patches from the visible patches. This requires that the CAE encoder cares about the representations for the patches other than only the global representation so that the CAE explores the relations among the patches for making predictions.</p><p>The input and output representations of latent contextual regressor are in the same latent space. Our CAE expects that the representations for the masked patches output from the latent contextual regressor lie in the same space with the input, the representations of the visible patches. We verify this through full image reconstruction where our CAE is trained using the pixel colors as the prediction targets.</p><p>We feed the full patches (without masking, all the image patches are visible) into the encoder, then skip the latent contextual regressor and directly send the encoded patch representations to the decoder for reconstructing the full image. Figure <ref type="figure" target="#fig_0">4</ref> provides reconstruction results for several examples randomly sampled from the ImageNet-1K validation set, implying that the input and output representations of latent contextual regressor are in the same space.</p><p>Probabilistic formulation. The MIM problem can be formulated in the probabilistic form, maximizing the probability of the predictions Y m of the masked patches given the conditions, the visible patches X v , the positions P v of the visible patches, and the positions P m of the masked patches:</p><formula xml:id="formula_2">P (Y m |X v , P v , P m ).</formula><p>It can be solved by introducing latent representations Z m and Z v , with the assumption that Z v </p><formula xml:id="formula_3">P (Y m |X v , P v , P m ) = P (Z v |X v , P v )P (Z m |Z v , P v , P m )P (Y m |Z m , P m ),</formula><p>where the three terms on the right side correspond to three parts of our CAE: the encoder, the latent contextual regressor, and the decoder, respectively.</p><p>The latent representation alignment module can be written as a conditional probability, P (Z m | Zm ), where Zm is the masked patch representations computed from the encoder.</p><p>Intuitive interpretation. Humans are able to hallucinate what appears in the masked regions and how they appear according to the visible regions. We speculate that humans do this possibly in a way similar as the following example: given that only the region of the dog head is visible and the remaining parts are missing, one can (a) recognize the visible region to be about a dog, (b) predict the regions where the other parts of the dog appear, and (c) guess what the other parts look like.</p><p>Our CAE encoder is in some sense like the human recognition step (a). It understands the content by mapping the visual patches into latent representations that lie in the subspace that corresponds to the category dog<ref type="foot" target="#foot_0">1</ref> . We empirically verify this by projecting the latent representations of the patches from the images randomly sampled from the ADE20K set<ref type="foot" target="#foot_1">2</ref> to the 2D space using t-SNE (Van der Maaten <ref type="bibr" target="#b51">&amp; Hinton, 2008)</ref>. The 2D projections shown in Figure <ref type="figure" target="#fig_1">5</ref> implies that the latent representations are clustered to some degree for different categories (though not perfect as our CAE is pretrained on ImageNet-1K).</p><p>The latent contextual regressor is like step (b). It produces a plausible hypothesis for the masked patches, and describes the regions corresponding to the other parts of the dog using latent representations. The CAE decoder is like step (c), mapping the latent representations to the targets. It should be noted that the latent representations might contain other information besides the semantic information, e.g., the part information and the information for making predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Connection</head><p>Relation to autoencoder. The original autoencoder <ref type="bibr">(Le-Cun, 1987;</ref><ref type="bibr" target="#b24">Gallinari et al., 1987;</ref><ref type="bibr" target="#b34">Hinton &amp; Zemel, 1994)</ref> consists of an encoder and a decoder. The encoder maps the input into a latent representation, and the decoder reconstructs the input from the latent representation. The denoising autoencoder (DAE) <ref type="bibr" target="#b53">(Vincent et al., 2010)</ref> (depicted in Figure <ref type="figure">2</ref> (c)), a variant of autoencoder, corrupts the input by adding noises and still reconstructs the non-corrupted input.</p><p>Our CAE encoder (depicted in Figure <ref type="figure">2 (a)</ref>) is similar to the original autoencoder and also contains an encoder and a decoder. Different from the autoencoder where the encoder and the decoder process the whole image, our encoder takes a portion of patches as input and our decoder takes the estimated latent presentations of the other portion of patches as input. Importantly, the CAE introduces a latent contextual regressor that makes predictions in the latent space from the visible patches to the masked patches.</p><p>Relation to BEiT. BEiT <ref type="bibr" target="#b3">(Bao et al., 2021)</ref> (Figure <ref type="figure">2</ref> (b)) feeds both visible patches and masked patches (represented by mask tokens) into a ViT that is based on self-attention, and then predicts the discrete patch tokens, where only the tokens for masked patches are counted in the loss function.</p><p>The ViT in BEiT simultaneously understands the image content and produces a hypothesis for the masked patches. There is no explicit and separate representation extraction module, indicating that the ViT network uses the partial capability for representation learning. In contrast, the CAE encoder is only for content understanding without making predictions for masked patches. The other parts in CAE do not update the representations for visible patches. The output of the latent contextual regressor is expected to align with the masked patch presentations computed from the encoder, constraining that the representation extraction role is only by the encoder. This implies that our CAE encoder exploits the whole capability for representation learning.</p><p>Comparison to contrastive learning. Typical contrastive learning methods, e.g., SimCLR <ref type="bibr">(Chen et al., 2020b)</ref> and MoCo <ref type="bibr" target="#b30">(He et al., 2020;</ref><ref type="bibr">Chen et al., 2021)</ref>, pretrain the networks by solving the pretext task, maximizing the similarities between augmented views (e.g., random crops) from the same image and minimizing the similarities between augmented views from different images.</p><p>It is shown that in <ref type="bibr">(Chen et al., 2020b)</ref> random cropping plays an important role in view augmentation for contrastive learning. Through analyzing random crops (illustrated in Figure <ref type="figure">3</ref>), we observe that the center pixels in the original image space have large chances to belong to random crops. We suspect that the global representation, learned by contrastive learning for a random crop possibly with other augmentation schemes, tends to focus mainly on the center pixels in the original image, so that the representations of different crops from the same image can be possibly similar.</p><p>Figure <ref type="figure" target="#fig_2">6</ref> (the second row) shows that the center region of the original image for the typical contrastive learning approach, MoCo v3, is highly attended.</p><p>In contrast, our MIM approach, CAE, randomly samples the patches from the augmented views to form the visible and masked patches. All the patches are possible to be ran-domly masked for the augmented views and accordingly the original image. Thus, the CAE encoder needs to learn good representations for all the patches, to make good predictions for the masked patches from the visible patches. Figure <ref type="figure" target="#fig_2">6</ref> (the third row) illustrates that almost all the patches in the original images are considered in our CAE encoder.</p><p>Considering that the instances of the 1000 categories in ImageNet-1K locate mainly around the center of the original images, typical contrastive learning methods, e.g., MoCo v3, learn the knowledge mainly about the 1000 categories, which is similar to supervised pretraining. But our CAE and other MIM methods are able to learn more knowledge beyond the 1000 categories from the non-center image regions. This indicates that the CAE has the potential to performs better for downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation</head><p>We study the standard ViT small and base architectures, ViT-S (12 transformer blocks with dimension 384) and ViT-B (12 transformer blocks with dimension 768). The latent contextual regressor consists of 4 transformer blocks based on cross-attention, and the decoder consists of 4 transformer blocks based on self-attention, and an extra linear projection for making predictions.</p><p>We follow BEiT <ref type="bibr" target="#b3">(Bao et al., 2021)</ref> to train the CAE on ImageNet-1K. We partition the image of 224 ? 224 into 14 ? 14 patches with the patch size being 16 ? 16. We use standard random cropping and horizontal flipping for data augmentation. The pretraining settings are almost the same as BEiT <ref type="bibr" target="#b3">(Bao et al., 2021)</ref> (See Appendix A for details). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Pretraining Evaluation</head><p>Linear probing. Linear probing is widely used as a proxy of pretraining quality evaluation for self-supervised representation learning. It learns a linear classifier over the image-level representation output from the pretrained encoder by using the labels of the images, and then tests the performance on the validate set.</p><p>Attentive probing. The output of the CAE encoder is representations for all the patches. It is not suitable to linearly probe the representation, averagely-pooled from patch representations, because the image label in ImageNet-1K only corresponds to a portion of patches. It is also not suitable to use the default class token within the encoder because the default class token serves as a role of aggregating the patch representations for better patch representation extraction and is not merely for the portion of patches corresponding to the image label.</p><p>To use the image-level label as a proxy of evaluating the pretraining quality for our CAE encoder, we need to attend the patches that are related to the label. We introduce a simple modification by using a cross-attention unit with an extra class token (that is different from the class token in the encoder) as the query and the encoder output as the keys and the values, followed by a linear classifier. The introduced cross-attention unit is able to care mainly about the patches belonging to the 1000 classes in ImageNet-1K and remove the interference of other patches. Figure <ref type="figure" target="#fig_3">7</ref> illustrates the effect of the cross-attention unit, showing that the extra cross-attention unit is able to to some degree attend the regions that are related to the 1000 ImageNet-1K classes.</p><p>Results. Table <ref type="table">1</ref> shows the results with three schemes, linear probing (LIN), attentive probing (ATT), and finetuning (FT) for representative contrastive pretraining (MoCo v3 and DINO) and MIM (BEiT and MAE) methods, and our approach CAE. The models of MAE with 300 epochs and BEiT are pretrained by us using the official implementations, and other models are the officially released models.</p><p>Table <ref type="table">1</ref>: Pretraining quality evaluation in terms of finetuning (FT), linear probing (LIN), and attentive probing (ATT). #Epochs refers to the number of pretraining epochs.</p><p>For reference, we report the top-1 accuracy (in the column ATT) of the supervised training approach DeiT <ref type="bibr" target="#b49">(Touvron et al., 2020)</ref>  We highlight a few observations. The fine-tuning performance for these methods are very similar and there is only a minor difference. We think that the reason is that selfsupervised pretraining and fine-tuning are conducted on the same dataset and no extra knowledge is introduced for image classification. The minor difference might come from the optimization aspect: different initialization (provided by pretrained models) for fine-tuning.</p><p>In terms of linear probing, the scores of the contrastive learning methods, MoCo v3 and DINO, are higher than the MIM methods. This is as expected because contrastive learning focuses mainly on learning the representations for 1000 classes (See discussion in Section 3). The pretraining is relatively easier than existing MIM methods as contrastive learning mainly cares about the 1000 classes and MIM methods may care about the classes beyond the 1000 classes.</p><p>For the MIM methods, the scores of attentive probing are much larger than linear probing. This validates our analysis: the MIM methods extract the representations for all the patches, and the classification task needs to attend the corresponding portion of patches.</p><p>The LIN and ATT scores are similar for contrastive pretraining: e.g., with ViT-B, (76.2 vs 77.0) for MoCo v3, and This means that the extra crossattention in attentive probing does not make a big difference, which is one more evidence for our analysis in Section 3 that they already focus mainly on the region where the instance in the 1000 categories lies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>The CAE architecture contains three components for pretraining the encoder: a latent contextual regressor, a decoder, and an alignment module. We cannot remove the latent contextual regressor that is the only unit to make predictions for masked patches from visible patches in our architecture.</p><p>We study the other two components, the decoder and the alignment module.</p><p>Table <ref type="table" target="#tab_1">2</ref> shows the ablation results. We report the scores for attentive probing, and downstream tasks: semantic segmentation on ADE and object detection on COCO. One can see that the downstream task performance is almost the same when only the decoder is added and that the performance increases when the decoder and the alignment module are both added. This also verifies that the alignment module is important for constraining that the input and output representations of the latent contextual regressor lie in the same space and accordingly improving the representation quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Downstream Tasks</head><p>Semantic segmentation on ADE20K <ref type="bibr" target="#b66">(Zhou et al., 2017)</ref>. We follow the code <ref type="bibr" target="#b3">(Bao et al., 2021)</ref> to use UperNet <ref type="bibr" target="#b57">(Xiao et al., 2018)</ref> (See Appendix A for training details). Table <ref type="table" target="#tab_2">3</ref> shows that our CAE with 300 training epochs performs better than DeiT, MoCo v3 and DINO (400 epochs), MAE (300 epochs) and BEiT expect MAE (1600 epochs). Our CAE (800 epochs) further improves the segmentation scores and outperforms BEiT (800 epochs), MAE 1600 epochs, MoCo v3 and DeiT by 2.3, 0.7, 1.6 and 1.8, respectively.</p><p>The superior results over supervised and contrastive pretraining methods, DeiT, MoCo v3 and DINO, stem from that our approach captures the knowledge beyond the 1000 classes in ImageNet-1K. The superior results over BEiT and MAE stem from that our CAE decouples the encoding and decoding roles and achieves a better-pretrained encoder. Object detection and instance segmentation on COCO <ref type="bibr" target="#b42">(Lin et al., 2014)</ref>. We adopt the Mask R-CNN approach <ref type="bibr" target="#b29">(He et al., 2017)</ref> that produces bounding boxes and instance masks simultaneously, with the ViT as the backbone (See Appendix A for training details). We apply the same object detection system to the methods in Table <ref type="table" target="#tab_3">4</ref>. We report the box AP for object detection and the mask AP for instance segmentation. The observations are consistent to those for semantic segmentation in Table <ref type="table" target="#tab_2">3</ref>. Our approach (300 epochs, ViT-B) is superior to all the other models except that a little lower tha MAE (1600 epochs). Our approach (800 epochs) outperforms MAE (1600 epochs), MoCo v3 and DeiT by 0.8, 3.7 and 2.3, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>Self-supervised representation learning has been widely studied in computer vision <ref type="bibr" target="#b19">(Dosovitskiy et al., 2014;</ref><ref type="bibr">2015;</ref><ref type="bibr" target="#b17">Doersch et al., 2015;</ref><ref type="bibr" target="#b58">Xie et al., 2016;</ref><ref type="bibr" target="#b61">Yang et al., 2016;</ref><ref type="bibr" target="#b50">van den Oord et al., 2018;</ref><ref type="bibr" target="#b6">Caron et al., 2018;</ref><ref type="bibr" target="#b0">Asano et al., 2019;</ref><ref type="bibr" target="#b7">Caron et al., 2019;</ref><ref type="bibr" target="#b36">Huang et al., 2019;</ref><ref type="bibr" target="#b67">Zhuang et al., 2019;</ref><ref type="bibr" target="#b23">Ermolov et al., 2021;</ref><ref type="bibr" target="#b41">Li et al., 2020;</ref><ref type="bibr">Gidaris et al., 2020a;</ref><ref type="bibr" target="#b32">Henaff, 2020;</ref><ref type="bibr">Gidaris et al., 2020b;</ref><ref type="bibr" target="#b27">Goyal et al., 2021;</ref><ref type="bibr" target="#b63">Zbontar et al., 2021;</ref><ref type="bibr" target="#b4">Bardes et al., 2021)</ref>. The following mainly reviews closely-related methods.</p><p>Autoencoding. Traditionally, autoencoders were used for dimensionality reduction or feature learning <ref type="bibr" target="#b40">(LeCun, 1987;</ref><ref type="bibr" target="#b24">Gallinari et al., 1987;</ref><ref type="bibr" target="#b34">Hinton &amp; Zemel, 1994;</ref><ref type="bibr" target="#b33">Hinton &amp; Salakhutdinov, 2006;</ref><ref type="bibr" target="#b47">Ranzato et al., 2007;</ref><ref type="bibr" target="#b52">Vincent et al., 2008;</ref><ref type="bibr" target="#b39">Kingma &amp; Welling, 2013)</ref>.</p><p>The denoising autoencoder (DAE) is an autoencoder that receives a corrupted data point as input and is trained to predict the original, uncorrupted data point as its output.</p><p>The variants or modifications of DAE were adopted for self-supervised representation learning, e.g., corruption by masking pixels <ref type="bibr" target="#b53">(Vincent et al., 2010;</ref><ref type="bibr" target="#b45">Pathak et al., 2016;</ref><ref type="bibr">Chen et al., 2020a)</ref>, removing color channels <ref type="bibr" target="#b65">(Zhang et al., 2016)</ref>, shuffling image patches <ref type="bibr" target="#b44">(Noroozi &amp; Favaro, 2016)</ref>, denoising pixel-level noise <ref type="bibr" target="#b1">(Atito et al., 2021)</ref> and so on.</p><p>Contrastive learning. In computer vision, contrastive learning has been popular for self-supervised representation learning <ref type="bibr">(Chen et al., 2020b;</ref><ref type="bibr" target="#b30">He et al., 2020;</ref><ref type="bibr" target="#b48">Tian et al., 2020;</ref><ref type="bibr">Chen et al., 2021;</ref><ref type="bibr" target="#b28">Grill et al., 2020;</ref><ref type="bibr" target="#b9">Caron et al., 2021;</ref><ref type="bibr">Chen &amp; He, 2021;</ref><ref type="bibr" target="#b8">Caron et al., 2020;</ref><ref type="bibr" target="#b56">Wu et al., 2018)</ref>. The basic idea is to maximize the similarity between the views augmented from the same image and minimize the similarity between the views augmented from different images. Random cropping is an important augmentation scheme, and thus typical contrastive learning methods (e.g., MoCo v3) tend to learn knowledge mainly from the center regions of the original images. Some dense variants <ref type="bibr" target="#b54">(Wang et al., 2021;</ref><ref type="bibr">Xie et al., 2021a)</ref> eliminate the tendency in a limited degree by considering an extra contrastive loss with dense patches, Masked image modeling. Motivated by BERT for masked language modeling <ref type="bibr" target="#b16">(Devlin et al., 2019)</ref>, the method studied in <ref type="bibr" target="#b21">(Dosovitskiy et al., 2021)</ref> and BEiT <ref type="bibr" target="#b3">(Bao et al., 2021)</ref>   <ref type="table" target="#tab_2">3</ref> and<ref type="table" target="#tab_3">4</ref>.</p><p>The very recent approach data2vec <ref type="bibr" target="#b2">(Baevski et al., 2022)</ref> is similar to our approach in making predictions in the latent representation space from the visible patches to the masked patches. Similar to BEiT, data2vec couples the prediction and representation extraction processes together and potentially benefits from our approach, separating the two processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>There are two core points for our CAE architecture design: (i) decouple the encoding (image understanding) and decoding (pretext task completion, making predictions for masked patches) roles and (ii) make predictions in the latent semantic representation space from visible patches to masked patches. We will study our CAE for the NLP and speech tasks.</p><p>We give some analysis about contrastive pretraining and masked image modeling for self-supervised representation learning, as well as supervised pretraining. We speculated that due to strong dependence on random cropping augmentation, typical contrastive learning methods (e.g., MoCo and SimCLR) tend to learn semantics mainly from center patches of the original images and little from non-center patches 3 .</p><p>Supervised pretraining is similar and also learns the information from the center patches in ImageNet-1K as the instances of the 1000 classes mainly lie in the center. This explains why contrastive pretraining and supervised pretraining perform similarly for downstream tasks.</p><p>In contrast, masked image modeling methods care about all the patches, having the potential to learn more information and accordingly perform better for downstream tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. How Does Contrastive Pretraining Work?</head><p>We consider the case in ImageNet-1K that the object mainly lies in the center of an image<ref type="foot" target="#foot_2">4</ref> . There are N randomly sampled crops from an image, and each crop I n contains a part of the center object, O n . To maximize the similarity between two crops I m and I n , the pretraining might contain the processes: Select the regions O m and O n from the two crops I m and I n , extract their features f om and f on , and predict the feature of the object, f o , from the part features f om and f on . In this way, the features of the crops from the same image could be similar. Among the N random crops, most crops contain a part of the object in the center, and a few crops that do not contain a part of the center object could be viewed as noises when optimizing the contrastive loss.</p><p>After pretrained on ImageNet-1K (where the object mainly lies in the center) the encoder is able to learn the knowledge of the 1000 classes and localize the region containing the object belonging to the 1000 classes. It is not necessary that the object lies in the center for the testing image. We show the attention maps of MoCo v3 and our CAE for random crops in Figure <ref type="figure" target="#fig_4">10</ref>. This further verifies that MoCo v3 (contrastive pretraining) pretrained on ImageNet-1K tends to attend to the object region, corresponding to the center region of the original image as shown in Figure <ref type="figure" target="#fig_2">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Possible Extensions of Our CAE</head><p>Our CAE can benefit from various schemes and tricks exploited in contrastive learning. For example, we can exploit EMA (exponential moving average), used in MoCo, BYOL, and so on. We may replace the encoder for masked patches with the EMA encoder, possibly removing the necessity of the decoder for target prediction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustrating that the input and output representations of latent contextual regressor are in the same space. We reconstruct the image by feeding the full image (top) into the CAE encoder and then the CAE decoder outputting the reconstructed image (bottom). It can be seen that the image can be constructed with the semantics kept when skipping latent contextual regressor, verifying the same space expectation. and P m (Y m and P v ) are conditionally independent:</figDesc><graphic url="image-15.png" coords="4,308.84,68.46,112.31,112.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5</head><label>5</label><figDesc>Figure 5: t-SNE visualization (one color for one category) of representations extracted from the images in ADE20K. Left: ViT pretrained with our CAE; Right: ViT with random weights. The latent representations from our CAE encoder for each category tend to be grouped together (though not perfect as our CAE encoder is pretrained on ImageNet-1K instead of ADE20K).</figDesc><graphic url="image-16.png" coords="4,427.73,68.46,112.31,112.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Illustrating the attention map averaged over 12 attention heads between the class token and the patch tokens in the last layer of the ViT encoder pretrained on ImageNet-1K. The region inside the blue contour is obtained by thresholding the attention weights to keep 50% of the mass. Top: Input image, Middle: MoCo v3, a typical contrastive learning method, and Bottom: our CAE. One can see that MoCo v3 tends to focus mainly on the centering regions and little on other patches, and our CAE tends to consider almost all the patches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Illustrating the cross-attention unit in attentive probing. The attention map (bottom) is the average of crossattention maps over 12 heads between the extra class token and the patches. One can see that the attended region lies mainly in the object, which helps image classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: The attention maps over two sets of randomly cropped images (the 1st the 4th rows) for MoCo v3 (the 2nd the 5th rows) and our CAE (the 3rd the 6th rows) pretrained on ImageNet-1K. MoCo v3 tends to focus mainly on the object region and little on other regions. Our CAE tends to consider almost all the patches. The attention maps over the original images are shown in Figure 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>to show how far our ATT score is from supervised training. The results for other models and our models are based on our implementations for fine-tuning, linear probing, and attentive probing. MoCo v3 and DINO adopt multi-crop augmentation in each minibatch. MoCo v3: 2 global crops of 224 ? 224. DINO: 2 global crops of 224 ? 224 and 10 local crops of 96 ? 96.</figDesc><table><row><cell>Method</cell><cell>#Epochs</cell><cell>#Crops</cell><cell>FT</cell><cell>LIN</cell><cell>ATT</cell></row><row><cell cols="2">Methods using ViT-S:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DeiT</cell><cell>300</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>79.9</cell></row><row><cell>MoCo v3</cell><cell>300</cell><cell>2</cell><cell>81.7</cell><cell>73.1</cell><cell>73.8</cell></row><row><cell>BEiT</cell><cell>300</cell><cell>1</cell><cell>81.7</cell><cell>15.7</cell><cell>23.6</cell></row><row><cell>CAE</cell><cell>300</cell><cell>1</cell><cell>81.8</cell><cell>50.8</cell><cell>64.8</cell></row><row><cell cols="2">Methods using ViT-B:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DeiT</cell><cell>300</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>81.8</cell></row><row><cell>MoCo v3</cell><cell>300</cell><cell>2</cell><cell>83.0</cell><cell>76.2</cell><cell>77.0</cell></row><row><cell>DINO</cell><cell>400</cell><cell>12</cell><cell>83.3</cell><cell>77.3</cell><cell>77.8</cell></row><row><cell>BEiT</cell><cell>300</cell><cell>1</cell><cell>83.0</cell><cell>37.6</cell><cell>49.4</cell></row><row><cell>MAE</cell><cell>300</cell><cell>1</cell><cell>82.9</cell><cell>61.5</cell><cell>71.1</cell></row><row><cell>MAE</cell><cell>1600</cell><cell>1</cell><cell>83.6</cell><cell>67.8</cell><cell>74.2</cell></row><row><cell>CAE</cell><cell>300</cell><cell>1</cell><cell>83.3</cell><cell>64.2</cell><cell>73.7</cell></row><row><cell>CAE</cell><cell>800</cell><cell>1</cell><cell>83.6</cell><cell>68.3</cell><cell>76.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation studies for the decoder and the alignment module in our CAE. All the models are pretrained on ImageNet-1K with 300 epochs.</figDesc><table><row><cell></cell><cell>Decoder</cell><cell>Align</cell><cell>ATT</cell><cell>ADE</cell><cell>COCO</cell></row><row><cell>CAE CAE CAE</cell><cell>? ? ?</cell><cell>? ? ?</cell><cell>70.7 72.6 73.7</cell><cell>46.2 46.4 47.7</cell><cell>45.6 45.7 48.0</cell></row><row><cell cols="3">(77.3 vs 77.8) for DINO.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Semantic segmentation on ADE20K. All the methods use a ViT-B architecture. All the results are based on the same implementation for semantic segmentation. #Epochs refers to the number of pretraining epochs. * : use multi-crop augmentation (See Table1) and equivalently take a larger number of epochs compared to one-crop augmentation.</figDesc><table><row><cell>Method</cell><cell cols="4">#Epochs Supervised Self-supervised mIoU</cell></row><row><cell>DeiT MoCo v3  *  DINO  *  BEiT BEiT MAE MAE CAE CAE</cell><cell>300 300 400 300 800 300 1600 300 800</cell><cell>? ? ? ? ? ? ? ? ?</cell><cell>? ? ? ? ? ? ? ? ?</cell><cell>47.0 47.2 47.2 45.5 46.5 45.8 48.1 47.7 48.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Object detection and instance segmentation on COCO. Mask R-CNN is adopted and trained with the 1? schedule. All the results are based on the same implementation for object detection and instance segmentation. #Epochs refers to the number of pretraining epochs on ImageNet-1K. * : use multi-crop augmentation (See Table1).</figDesc><table><row><cell>Method</cell><cell>#Epochs</cell><cell>Supervised</cell><cell>Self-supervised</cell><cell>AP b</cell><cell cols="2">Object detection AP b 50 AP b 75</cell><cell cols="3">Instance segmentation AP m AP m 50 AP m 75</cell></row><row><cell cols="2">Methods using ViT-S: DeiT 300 MoCo v3  *  300 BEiT 300 CAE 300</cell><cell>? ? ? ?</cell><cell>? ? ? ?</cell><cell>43.1 43.3 35.6 43.8</cell><cell>65.2 64.9 56.7 64.5</cell><cell>46.6 46.8 38.3 47.1</cell><cell>38.4 38.8 32.6 39.0</cell><cell>61.8 61.6 53.3 61.3</cell><cell>40.6 41.1 34.2 41.7</cell></row><row><cell cols="2">Methods using ViT-B: DeiT 300 MoCo v3  *  300 DINO  *  400 BEiT 300 BEiT 800 MAE 300 MAE 1600 CAE 300 CAE 800</cell><cell>? ? ? ? ? ? ? ? ?</cell><cell>? ? ? ? ? ? ? ? ?</cell><cell>46.9 45.5 46.8 39.5 42.1 45.4 48.4 48.0 49.2</cell><cell>68.9 67.1 68.6 60.6 63.3 66.4 69.4 68.7 70.2</cell><cell>51.0 49.4 50.9 43.0 46.0 49.6 53.1 52.7 53.8</cell><cell>41.5 40.5 41.5 35.9 37.8 40.6 42.6 42.3 43.3</cell><cell>65.5 63.7 65.3 57.7 60.1 63.4 66.1 65.6 67.1</cell><cell>44.4 43.4 44.5 38.5 40.6 43.7 45.9 45.4 46.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>use the ViT structure to solve the masked image modeling task, e.g., predicting the pixels or the discrete tokens. But they do not have explicitly an encoder or a decoder and the ViT structure is essentially a mixture of encoder and decoder, limiting the representation learning quality.</figDesc><table><row><cell>Complementary to our study, MaskFeat and PeCo improve</cell></row><row><cell>the pretraining quality by studying the prediction targets.</cell></row><row><cell>MAE and SplitMask are closely related to BEiT, and can</cell></row><row><cell>be viewed as a modification of BEiT. They prepend an</cell></row><row><cell>extra ViT structure that only receives visible patches as a</cell></row><row><cell>so-called encoder, and then feed the encoded representations</cell></row><row><cell>and the mask tokens of masked patches into a lightweight</cell></row><row><cell>ViT structure for prediction. See the computational graph in</cell></row><row><cell>Appendix C.</cell></row><row><cell>The prepended architecture in MAE and SplitMask is only</cell></row><row><cell>for image understanding (e.g., encoding the image patches),</cell></row><row><cell>but the lightweight ViT decoder (e.g., containing 8 trans-</cell></row><row><cell>former blocks in MAE) might also have a partial role for</cell></row><row><cell>representation extraction besides the decoding role. Differ-</cell></row><row><cell>ently, our approach aims to decouple the two roles. This</cell></row><row><cell>is empirically verified by the superiority of our CAE over</cell></row><row><cell>MAE as given in Tables</cell></row></table><note><p><p><p><p><p><p><p><p><p><p><p>More about recent MIM methods: There are several concurrently-developed methods, such as Masked Autoencoder (MAE)</p><ref type="bibr" target="#b31">(He et al., 2021)</ref></p>, SplitMask</p><ref type="bibr" target="#b22">(El-Nouby et al., 2021)</ref></p>, Masked Feature Prediction (MaskFeat)</p><ref type="bibr" target="#b55">(Wei et al., 2021)</ref></p>, Simple MIM (SimMIM)</p>(Xie et al., 2021b)</p>, Perceptual Codebook for BEiT (PeCo)</p><ref type="bibr" target="#b18">(Dong et al., 2021)</ref></p>.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Our encoder does not know that the subspace is about a dog, and just separates it from the subspaces of other categories.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We choose to use the pixel labels for checking if the representations are correctly clustered.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>There are a few images in which the object does not lie in the center in ImageNet-1K. The images are actually viewed as noises and have little influence for contrastive learning.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to acknowledge <rs type="person">Hangbo Bao</rs>, <rs type="person">Xinlei Chen</rs>, <rs type="person">Li Dong</rs>, <rs type="person">Qi Han</rs>, <rs type="person">Zhuowen Tu</rs>, <rs type="person">Saining Xie</rs>, and <rs type="person">Furu Wei</rs> for the helpful discussions.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training Details</head><p>Pretraining. The settings are almost the same as BEiT <ref type="bibr" target="#b3">(Bao et al., 2021)</ref>. We use AdamW <ref type="bibr" target="#b43">(Loshchilov &amp; Hutter, 2017)</ref> for optimization and train the CAE for 300 (800) epochs with the batch size being 2048. We set the learning rate as 1.5e-3, with cosine learning rate decay and a 10-epoch warmup, and set the weight decay as 0.05. We do not employ drop depth <ref type="bibr" target="#b35">(Huang et al., 2016)</ref> and dropout.</p><p>Fine-tuning on ImageNet. We follow the fine-tuning protocol in BEiT to use layer-wise learning rate decay, weight decay and AdamW. The batch size is 4096, the warmup epoch is 10 and the weight decay is 0.05. For ViT-S, we train 200 epochs with learning rate 1.6e-2 and layer-wise decay rate 0.75. For ViT-B, we train 100 epochs with learning rate 8e-3 and layer-wise decay rate 0.65.</p><p>Linear probing. We use the LARS <ref type="bibr" target="#b38">(Karpathy et al., 2014)</ref> optimizer with momentum 0.9. The model is trained for 90 epochs. The batch size is 16384, the warmup epoch is 10 and the learning rate is 6.4. We adopt an extra BatchNorm layer <ref type="bibr" target="#b37">(Ioffe &amp; Szegedy, 2015)</ref> without affine transformation (affine=False) before the linear classifier. We do not use mixup <ref type="bibr" target="#b64">(Zhang et al., 2017)</ref>, cutmix <ref type="bibr" target="#b62">(Yun et al., 2019)</ref>, drop path <ref type="bibr" target="#b35">(Huang et al., 2016)</ref>, or color jittering, and we set weight decay as zero.</p><p>Attentive probing. The parameters of the encoder are fixed during attentive probing. A cross-attention module, a BatchNorm layer (affine=False), and a linear classifier are appended after the encoder. The extra class token representation in cross-attention is learned as model parameters. The keys and the values are the patch representations output from the encoder. There is no MLP or skip connection operation in the extra cross-attention module. We use the SGD optimizer with momentum 0.9 and train the model for 90 epochs. The batch size is 8192, the warmup epoch is 10 and the learning rate is 0.4. Same as linear probing, we do not use mixup, cutmix, drop path, or color jittering, and we set weight decay as zero.</p><p>Object detection and instance segmentation on COCO. We utilize multi-scale training and resize the image with the size of the short side between 480 and 800 and the longe side no larger than 1333. The batch size is 32, the learning rate is 3e-4, and the layer-wise decay rate is 0.75. We train the network with the 1? schedule: 12 epochs with the learning rate decayed by 10? at epochs 9 and 11. We do not use multi-scale testing. The Mask R-CNN implementation follows MMDetection <ref type="bibr" target="#b10">(Chen et al., 2019)</ref>.</p><p>Semantic segmentation on ADE20K. We use AdamW as the optimizer. The batch size is 16 and the layer-wise decay rate is 0.65. The input resolution is 512 ? 512. We search from two learning rates, 3e-4 and 4e-4, for all the results in Table <ref type="table">3</ref>. We conduct fine-tuning for 160K steps. We do not use multi-scale testing.</p><p>Hyperparameter choice. There is a tradeoff variable ? in the loss function given in Equation 1. We did not do an extensive study and only tried two choices, ? = 1 and ? = 2. The choice ? = 1 works also well, slightly worse than ? = 2. The depths for latent contextual regressor and the decoder are chosen as 4 and 4, which shows better performance than smaller depths. We did not study larger depths that potentially lead to better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Details for t-SNE Visualization in Figure 5</head><p>We adopt t-SNE (Van der <ref type="bibr" target="#b51">Maaten &amp; Hinton, 2008)</ref> to visualize the high-dimensional patch representations output from our CAE encoder on ADE20K <ref type="bibr" target="#b66">(Zhou et al., 2017)</ref>. ADE20K has a total of 150 categories. For each patch in the image, we set its label to be the category that more than half of the pixels belong to. We collect up to 1000 patches for each category from sampled 500 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Computational Graph for MAE and SplitMask</head><p>We provide the computational graph for Masked autoencoder <ref type="bibr" target="#b31">(He et al., 2021)</ref> and SplitMask <ref type="bibr" target="#b22">(El-Nouby et al., 2021)</ref> (one stream) in Figure <ref type="figure">8</ref>. Compared to our CAE, the main issue is that the so-called decoder R might have also the encoding role, i.e., learning semantic representations of the visible patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. More Results for Concurrently-Developed MIM Methods</head><p>Table <ref type="table">5</ref> reports the results of semantic segmentation on ADE20K. The segmentation results are from the corresponding paper. We also report the results of object detection and instance segmentation for MAE under the Cascaded Mask R-CNN</p><p>Figure <ref type="figure">8</ref>: The computational graph for MAE <ref type="bibr" target="#b31">(He et al., 2021)</ref> and the one stream in SplitMask <ref type="bibr" target="#b22">(El-Nouby et al., 2021)</ref>. The two functions, F and R, are both based on self-attention. F (called encoder in MAE) only processes the visible patches X v , and R (called decoder in MAE) processes both the latent representations Z v of the visible patches and the mask queries (Q m ) and updates them simultaneously. framework <ref type="bibr" target="#b5">(Cai &amp; Vasconcelos, 2021)</ref>. This is different from Table <ref type="table">4</ref> that is based on Mask R-CNN. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Image Reconstruction without the Alignment Module</head><p>We visualize the reconstructed image for a modification of CAE by dropping the alignment module during pretraining. The visualizations in Figure <ref type="figure">9</ref> show that the reconstructed images are noisy and meaningless. The reason is that the input and output representations of latent contextual regressor are not in the same space. As a comparison, the reconstructed images in Figure <ref type="figure">4</ref> in the main paper are much better, indicating the importance of the alignment module. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05371</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Sit: Self-supervised vision transformer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Atito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.03602</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">data2vec: A general framework for self-supervised learning in speech, visionand languags</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><surname>Beit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<title level="m">BERT pre-training of image transformers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Vicreg: Variance-3 It is interesting to study how contrastive learning performs if the objects appear at any position in an image other than at the center for ImageNet-1K. invariance-covariance regularization for self-supervised learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bardes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04906</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: High quality object detection and instance segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1483" to="1498" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised pre-training of image features on non-curated data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2959" to="2968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Emerging properties in selfsupervised vision transformers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<idno>CoRR, abs/2104.14294</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1691" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15750" to="15758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno>CoRR, abs/2104.02057</idno>
		<ptr target="https://arxiv.org/abs/2104.02057" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Burstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Doran</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Solorio</surname></persName>
		</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Peco</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12710</idno>
		<title level="m">Perceptual codebook for bert pre-training of vision transformers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="766" to="774" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with exemplar convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1734" to="1747" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In ICLR. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Are large-scale datasets necessary for self-supervised pre</title>
		<author>
			<persName><forename type="first">A</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10740</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Whitening for self-supervised representation learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ermolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3015" to="3024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">M?moires associatives distribu?es: une comparaison (distributed associative memories: a comparison)</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thiria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Soulie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Proceedings of COGNITIVA</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<date type="published" when="1987-05">May 1987. 1987</date>
			<pubPlace>Paris, La Villette</pubPlace>
		</imprint>
	</monogr>
	<note>Cesta-Afcet</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning representations by predicting bags of visual words</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6928" to="6938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Online bag-of-visual-words generation for unsupervised representation learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Puy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.11552</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lefaudeux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<title level="m">Self-supervised pretraining of visual features in the wild. arXiv: Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9726" to="9735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06377</idno>
		<title level="m">Masked autoencoders are scalable vision learners</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">O</forename><surname>Henaff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4182" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">minimum description length, and helmholtz free energy</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><surname>Autoencoders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="3" to="10" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised deep learning by neighbourhood discovery</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2849" to="2858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Mod&apos;eles connexionistes de l&apos;apprentissage</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
		<respStmt>
			<orgName>Universit e de Paris VI</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Prototypical contrastive learning of unsupervised representations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04966</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Zero-shot text-toimage generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</editor>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="8821" to="8831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Efficient learning of sparse representations with an energybased model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Poultney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">1137</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<title level="m">What makes for good views for contrastive learning. arXiv: Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<title level="m">Representation learning with contrastive predictive coding. arXiv: Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Dense contrastive learning for self-supervised visual pretraining</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3024" to="3033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Masked feature prediction for self-supervised visual pre-training</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.09133</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="418" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="16684" to="16693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><surname>Simmim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09886</idno>
		<title level="m">A simple framework for masked image modeling</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Joint unsupervised learning of deep representations and image clusters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5147" to="5156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><surname>Cutmix</surname></persName>
		</author>
		<title level="m">Regularization strategy to train strong classifiers with localizable features. arXiv: Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Barlow twins: Self-supervised learning via redundancy reduction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Deny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03230</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="633" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Local aggregation for unsupervised learning of visual embeddings</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6002" to="6012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
