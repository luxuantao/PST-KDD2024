<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spoken Language Recognition: From Fundamentals to Practice</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><roleName>Member IEEE</roleName><forename type="first">Kong</forename><forename type="middle">Aik</forename><surname>Lee</surname></persName>
							<email>kalee@i2r.a-star.edu.sg</email>
						</author>
						<author>
							<persName><forename type="first">H</forename><surname>Li</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Institute for Infocomm Research</orgName>
								<orgName type="institution" key="instit2">Agency for Science, Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of New South Wales</orgName>
								<address>
									<postCode>2052</postCode>
									<settlement>Kensington</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Agency for Science, Technology, and Research (A*STAR)</orgName>
								<orgName type="institution">Institute for Infocomm Research</orgName>
								<address>
									<postCode>138632</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Spoken Language Recognition: From Fundamentals to Practice</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E8A7D9B6D21057BCCA72DDF13DD74B66</idno>
					<idno type="DOI">10.1109/JPROC.2012.2237151</idno>
					<note type="submission">Manuscript received May 22, 2012; revised September 19, 2012; accepted December 10, 2012. Date of publication February 6, 2013; date of current version April 17, 2013.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Acoustic features</term>
					<term>calibration</term>
					<term>classifier</term>
					<term>fusion</term>
					<term>language recognition evaluation (LRE)</term>
					<term>phonotactic features</term>
					<term>spoken language recognition</term>
					<term>tokenization</term>
					<term>vector space modeling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper provides an introductory tutorial on the fundamentals and the state-of-the-art solutions to automatic spoken language recognition, from both phonological and computational perspectives. It also gives a comprehensive review of current trends and future research directions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Spoken language recognition refers to the automatic process that determines the identity of the language spoken in a speech sample. It is an enabling technology for a wide range of multilingual speech processing applications, such as spoken language translation <ref type="bibr" target="#b141">[141]</ref>, multilingual speech recognition <ref type="bibr" target="#b115">[116]</ref>, and spoken document retrieval <ref type="bibr" target="#b22">[23]</ref>. It is also a topic of great interest in the areas of intelligence and security for information distillation.</p><p>Humans are born with the ability to discriminate between spoken languages as part of human intelligence <ref type="bibr" target="#b147">[147]</ref>. The quest to automate such ability has never stopped <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b88">[89]</ref>, <ref type="bibr" target="#b95">[96]</ref>, <ref type="bibr" target="#b150">[150]</ref>. Just like any other artificial intelligence technologies, spoken language recognition aims to replicate such human ability through computational means. The invention of digital computers has made this possible. A key question is how to scientifically measure the individuality of the diverse spoken languages in the world. Today, automatic spoken language recognition is no longer a part of science fiction. We have seen it being deployed for practical uses <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b80">[81]</ref>, <ref type="bibr" target="#b115">[116]</ref>, <ref type="bibr" target="#b141">[141]</ref>.</p><p>It is estimated that there exist several thousands of spoken languages in the world <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b57">[58]</ref>. The recent edition of the Ethnologue, a database describing all known living languages <ref type="bibr" target="#b70">[71]</ref>, has documented 6909 living spoken languages. Text-based language recognition has traditionally relied on distinct textual features of languages such as words or letter substrings. It was arguably established in 1967 by Gold <ref type="bibr" target="#b40">[41]</ref> as a closed class experiment, in which human subjects were asked to classify a given test document into one of the languages. It was not until the 1990s, however, when people resorted to statistical techniques that formulate the problem as a text categorization problem <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b36">[37]</ref>. For languages that use the Latin alphabet, text-based language recognition has attained reasonably good performance, thus it is considered a solved problem <ref type="bibr" target="#b82">[83]</ref>. As words and letter substrings are the manifestation of lexical-phonological rules of a language, this research has led to the conjecture that a spoken language can be characterized by its lexical-phonological constraints.</p><p>In practice, spoken language recognition is far more challenging than text-based language recognition because there is no guarantee that a machine is able to transcribe speech to text without errors. We know that humans recognize languages through a perceptual or psychoacoustic process that is inherent in the auditory system. Therefore, the type of perceptual cues that human listeners use is always the source of inspiration for automatic spoken language recognition <ref type="bibr" target="#b147">[147]</ref>.</p><p>Human listening experiments suggest that there are two broad classes of language cues: the prelexical information and the lexical semantic knowledge <ref type="bibr" target="#b147">[147]</ref>. Phonetic repertoire, phonotactics, rhythm, and intonation are all parts of the prelexical information <ref type="bibr" target="#b107">[108]</ref>. Although one may consider phones and phonotactics as segmental, and rhythm and intonation as suprasegmental, they are all very different from the lexical semantic knowledge that the words encapsulate, such as semantic meanings and conceptual representations. There is no doubt that both prelexical and lexical semantic knowledge contribute to the human perceptual process for spoken language recognition <ref type="bibr" target="#b107">[108]</ref>.</p><p>Studies in infants' listening experiments have revealed that when infants have not gained a great deal of lexical knowledge, they successfully rely on prelexical cues to discriminate against languages <ref type="bibr" target="#b107">[108]</ref>. In fact, when an adult is dealing with two unfamiliar languages, one can only use prelexical information. But as the infant's language experience enriches or as the adult is handling familiar languages, lexical semantic information will start to play a very important, sometimes determining, role <ref type="bibr" target="#b147">[147]</ref>. While we know that it requires a major effort to command the lexical usage of an entirely new language, in human listening studies, we have observed that human subjects are able to acquire prelexical information rapidly for language recognition purposes.</p><p>The relative importance of perceptual cues for language recognition has always been a subject of debate. Studies in automatic spoken language recognition have confirmed that acoustic and phonotactic features are the most effective language cues <ref type="bibr" target="#b96">[97]</ref>, <ref type="bibr" target="#b150">[150]</ref>. This coincides with the findings from human listening experiments <ref type="bibr" target="#b95">[96]</ref>, <ref type="bibr" target="#b136">[137]</ref>. While the term ''acoustic'' refers to physical sound patterns, the term ''phonotactic'' refers to the constraints that determine permissible syllable structures in a language. We can consider acoustic features as the proxy of phonetic repertoire and call it acoustic-phonetic features. On the other hand, we see phonotactic features as the manifestation of the phonotactic constraints in a language. Therefore, in this paper, we would like to introduce the spoken language recognition techniques that are mainly based on acoustic and phonotactic features, which represent the mainstream research <ref type="bibr" target="#b86">[87]</ref>, <ref type="bibr" target="#b101">[102]</ref>, <ref type="bibr" target="#b150">[150]</ref>. In what follows, we use the term language recognition for brevity.</p><p>We will provide insights into the fundamental principles of the research problem, and a practical applicability analysis of different state-of-the-art solutions to language recognition. We will also make an attempt to establish the connection across different techniques. Advances in feature extraction, acoustic modeling, phone n-gram modeling, phone recognizers, phone lattice generation, vector space modeling, intersession compensation, and score calibration and fusion have contributed to the state-of-the-art performance <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b120">[121]</ref>. To benefit from a wealth of publications related to the National Institute of Standards and Technology (NIST) language recognition evaluations (LREs) <ref type="bibr" target="#b84">[85]</ref>, <ref type="bibr" target="#b85">[86]</ref>, <ref type="bibr" target="#b87">[88]</ref>, <ref type="bibr" target="#b88">[89]</ref>, <ref type="bibr" target="#b101">[102]</ref>, we will demonstrate the working principles of various techniques using the NIST LREs as case studies.</p><p>The rest of the paper is organized as follows. We will elaborate on the principles, problems, and recent advances in language modeling in Section II. We will introduce the phonotactic approaches and acoustic approaches in Sections III and IV, respectively. Advanced topics in system developments related to Gaussian back-ends, multiclass logistic regression, score calibration and fusion, and language detection will be discussed in Section V. Section VI will be devoted to the NIST LRE paradigm, and, finally, an overview of current trends and future research directions will be discussed in Section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. LANGUAGE RECOGNITION PRINCIPLES A. Principles of Language Characterization</head><p>The first perceptual experiment measuring how well human listeners can perform language identification was reported in <ref type="bibr" target="#b95">[96]</ref>. It was concluded that human beings, with adequate training, are the most accurate language recognizers. This observation still holds after 15 years as confirmed again in <ref type="bibr" target="#b136">[137]</ref>, provided that the human listeners speak the languages.</p><p>For languages that they are not familiar with, human listeners can often make subjective judgments with reference to the languages they know, e.g., it sounds like Arabic, it is tonal like Mandarin or Vietnamese, or it has a stress pattern like German or English. Though such judgements are less precise for hard decisions to be made for an identification task, they show how human listeners apply linguistic knowledge at different levels for distinguishing between certain broad language groups, such as tonal versus nontonal languages.</p><p>Studies also revealed that, given only little previous exposure, human listeners can effectively identify a language without much lexical knowledge. In this case, human listeners rely on prominent phonetic, phonotactic, and prosody cues to characterize the languages <ref type="bibr" target="#b95">[96]</ref>, <ref type="bibr" target="#b136">[137]</ref>.</p><p>The set of language cues discussed above can be illustrated according to their level of knowledge abstraction, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Language recognition systems are usually categorized by the features they use, such as the acoustic-phonetic approach, the phonotactic approach, the prosodic approach, and the lexical approach.</p><p>Acoustic phonetics: The human speech apparatus is capable of producing a wide range of sounds. Speech sounds as concrete acoustic events are referred to as phones, whereas speech sounds as entities in a linguistic system are termed as phonemes <ref type="bibr" target="#b57">[58]</ref>. The number of phonemes used in a language ranges from about 15 to 50, with the majority having around 30 phonemes each <ref type="bibr" target="#b27">[28]</ref>. For example, English has a phonetic system that contains 24 consonants and 14 vowels, Mandarin has 21 consonants and ten vowels, and Spanish has 18 consonants and five vowels <ref type="bibr" target="#b147">[147]</ref>. Phonetic repertoires differ from language to language although languages may share some common phonemes. For example, retroflex consonants are used in Mandarin but not in Spanish. The English consonant /8/ (i.e., the voiced th sound) does not appear in Mandarin. These differences between phonetic repertoires imply that each language has its unique set of phonemes <ref type="bibr" target="#b57">[58]</ref>, thus acoustic-phonetic feature distributions.</p><p>Phonotactics: As concluded in phonological studies, each language has its unique set of lexicalphonological rules that govern the combinations of different phonemes. The phonotactic constraints dictate the permissible phone sequences. We note that phonemes can be shared considerably across languages, but the statistics of their sequential patterns differ very much from one language to another. Some phone sequences that occur frequently in one language could be rare in another. For example, consonant clusters, e.g., /fl/, /pr/, and /str/, are commonly observed in English words, while impermissible in Mandarin. Such phonotactic constraints can be characterized by a phone n-gram model <ref type="bibr" target="#b54">[55]</ref>.</p><p>Prosody: Prosody in general refers to suprasegmental features in running speech, such as stress, duration, rhythm, and intonation <ref type="bibr" target="#b1">[2]</ref>. The set of interrelated prosodic features are all important characteristics of spoken languages. For example, the world's languages can be grouped into three rhythm classes: stress-timed languages such as English and other Germanic languages, syllabletimed languages such as French and Hindi, and mora-timed languages such as Japanese <ref type="bibr" target="#b108">[109]</ref>. Lexical tone is the most prominent feature of tonal languages, such as Mandarin, Thai, and Vietnamese. Therefore, prosody appears to be useful for distinguishing between broad language classes (e.g., tonal versus nontonal languages). However, human listening experiments reported in <ref type="bibr" target="#b98">[99]</ref>, and <ref type="bibr" target="#b107">[108]</ref> show that prosodic cues are less informative than the phonotactic one. This observation is consistent with that reported in automatic language recognition <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b126">[127]</ref>, where it was shown that phonotactic features are far more superior to prosodic features. Furthermore, it remains a challenge to reliably extract prosodic features <ref type="bibr" target="#b99">[100]</ref>, <ref type="bibr" target="#b100">[101]</ref>. Therefore, we do not go into details about prosodic approaches in this paper.</p><p>Words and syntax: Languages contain a phonological system that governs how symbols are used to form words or morphemes, and a syntactic system that governs how words and morphemes are combined to form phrases and utterances. Each language has its unique phonological system and syntactic system that can be used for language identification, which characterizes a unique word list or a set of word n-grams <ref type="bibr" target="#b36">[37]</ref>. Therefore, the lexical approach seems to be an obvious choice for language recognition. This is also encouraged by human listening experiments where humans do well when they know the languages. One idea is to run multiple large vocabulary continuous speech recognition (LVCSR) systems for different target languages in parallel <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b91">[92]</ref>, <ref type="bibr" target="#b114">[115]</ref>. The system that gives the highest likelihood score makes the best sense of the speech and is considered as the recognition result. The theory behind this is that, once the system knows what a person is saying, its language is obvious. This study raises an interesting question: If a multilingual LVCSR system has already recognized the language, why do we need a standalone language recognizer? In practice, just like a person would not master a language for the sake of language recognition, we are concerned about the cost effectiveness of using LVCSR for language recognition. As a result, the LVCSR-based lexical approach has not been widely used. The use of phonetic and phonotactic cues is based on the assumption that languages possess partially overlapping sets of phonemes. The same basis was used in constructing the international phonetic alphabet (IPA). Though there are 6909 languages in the world <ref type="bibr" target="#b70">[71]</ref>, the total number of phones required to represent all the sounds of these languages ranges only from 200 to 300 <ref type="bibr" target="#b1">[2]</ref>. Phones that are common in languages are grouped together and given the same symbol in IPA. For instance, the GlobalPhone multilingual text and speech database <ref type="bibr" target="#b116">[117]</ref> uses a phone set consisting of 122 consonants and 114 vowels. Three nonphonetic units (two noise models and one silence model) are also defined for modeling nonspeech events. The same phone set is used for transcribing 20 spoken languages in the GlobalPhone database.</p><p>A systematic study on the extent to which languages are separated in phonetic and phonotactic terms is possible using the GlobalPhone database, in which languages are mapped to a common set of IPA symbols. Fig. <ref type="figure" target="#fig_1">2</ref> compares and contrasts the phonetic histograms of two languages that are arbitrarily selected from the GlobalPhone database in the form of polar plots. The size of the pie shape in the figure indicates the normalized count of a phone in a speech utterance. The three plots in the upper panel were obtained for three different subjects speaking Czech of different contents, while the plots in the lower panel are for another three subjects speaking Portuguese of different contents. A visual inspection easily confirms that the two languages are different in terms of phonetic repertoire and the frequency count of phones, while only slight variations are observed for different instances within the same language despite different speech contents. If we treat the phonetic histogram as an empirical distribution of the language, we may apply similarity measures to provide quantifiable measurements between languages.</p><p>We can also use the GlobalPhone database to study the phonotactic differences between languages by examining how well a phone n-gram model of one language predicts the phone sequence across different languages in terms of perplexity <ref type="bibr" target="#b54">[55]</ref>. A lower perplexity shows that a phone n-gram matches better the phone sequence, in other words, the phone sequence is more predictable. We expect a low perplexity when the n-gram model and the test phone sequence are of the same language, while high perplexity is expected otherwise. To this end, we first build a phone n-gram model <ref type="bibr" target="#b54">[55]</ref> for each of the seven languages selected from the GlobalPhone database that have been transcribed in IPA. We then evaluate the perplexity of the phone n-gram models over some held-out test data for every language pair. Table <ref type="table" target="#tab_0">1</ref> shows the perplexity measures between languages for the cases of bigram and trigram in the upper and lower panels, respectively. The tabulated data clearly indicate that the lowest perplexity values are always observed when the phone n-gram models and the test data are from the same language. This observation confirms the differences between languages in terms of phonotactics and the effectiveness of using phone n-gram models to quantitatively measure such differences.</p><p>While the analysis of the GlobalPhone database was conducted using perfectly transcribed phone sequences, as opposed to an automatic transcription, it illustrates a quantifiable link between phonetic and phonotactic features and language recognition. For the purpose of analysis as described above, phonetic transcriptions were derived from orthographic transcriptions by means of pronunciation dictionaries, as defined in the GlobalPhone database. Furthermore, we use the add-one smoothing method <ref type="bibr" target="#b54">[55]</ref> when training n-gram models to deal with the zero-count problem caused by unseen or out-of-set phones for a particular language (recall that the same phone set is used across different languages in the GlobalPhone database).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Formulation of Language Recognition</head><p>In the following, we state the problem of language recognition from an engineering perspective utilizing the linguistic knowledge sources we described in Section II-A.</p><p>Considering a speech waveform in an unknown language, we convert the audio sample into a sequence of acoustic feature vectors O ¼ foð1Þ; oð2Þ; . . . ; oðTÞg, in which oðtÞ is extracted from the waveform at the discrete frame t and there are T such vectors. Let the set of languages under consideration fL 1 ; L 2 ; . . . ; L N g be equally probable. Identifying a language out of the set of N possible languages involves an assignment of the most likely language label L to the acoustic observation O, such that</p><formula xml:id="formula_0">L ¼ arg max l pðOjL l Þ<label>(1)</label></formula><p>which follows a maximum-likelihood (ML) criterion. For the case where the prior probabilities of observing individual languages are not uniform, the maximum a posteriori (MAP) criterion could be used <ref type="bibr" target="#b63">[64]</ref>. For mathematical tractability, the language-specific density pðojL l Þ is always assumed to follow some functional forms, for which parametric models could be used. In the simplest case, a Gaussian mixture model (GMM) <ref type="bibr" target="#b149">[149]</ref>, <ref type="bibr" target="#b150">[150]</ref> can be used to model directly the distribution of the acoustic features, as we will see in Section IV.</p><p>To deal with phones and phonotactic knowledge sources, we assume that the speech waveform can be segmented into a sequence of phones Y b . Applying the ML criterion, we have now the most likely language given by</p><formula xml:id="formula_1">L ¼ arg max l PðY b jL l Þ:<label>(2)</label></formula><p>In this case, PðY b jL l Þ is the so-called phone n-gram model, which is a discrete probability model describing the phone occurrence and co-occurrences. One subtle difference between (1) and ( <ref type="formula" target="#formula_1">2</ref>) is the stochastic model used, the former being a continuous density function such as a normal distribution, while the latter being a discrete distribution such as a multinomial distribution, depending on the nature of the features we model. In (2), we have assumed that we know the exact sequence of phones in the speech waveform O. In practice, Y b has to be decoded from O by selecting the most likely one from all possible sequences. To this end, we hinge on a set of phone models M, such as a hidden Markov model (HMM), to decode the waveform. The most likely phone sequence could then be obtained using Viterbi decoding</p><formula xml:id="formula_2">Y b ¼ arg max Y PðOjY; MÞ:<label>(3)</label></formula><p>Putting ( <ref type="formula" target="#formula_1">2</ref>) and (3) together, and considering all possible phone sequences instead of the single best hypothesis, we obtain a more general form</p><formula xml:id="formula_3">L ¼ arg max l X 8Y PðOjY; MÞPðYjL l Þ:<label>(4)</label></formula><p>More details can be folded in by having M be language dependent or using lexical constraints in the phone decoding <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b117">[118]</ref>. State-of-the-art language recognizers always resort to simpler cases as in (1)-(3). In Sections III and IV, we will give a detailed mathematical formulation as to how modeling and the decision rule are implemented in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Recent Advances</head><p>Contemporary language recognition systems operate in two phases: training and the runtime test. During the training phase, speech utterances are analyzed and models are built based on the training data given the language labels. The models are intended to represent some language-dependent characteristics seen on the training data. Depending on the information sources they model, these models could be 1) stochastic, e.g., Gaussian mixture model (GMM) <ref type="bibr" target="#b149">[149]</ref>, <ref type="bibr" target="#b150">[150]</ref> and hidden Markov model (HMM) <ref type="bibr" target="#b97">[98]</ref>, <ref type="bibr" target="#b149">[149]</ref>; 2) deterministic, e.g., vector quantization (VQ) <ref type="bibr" target="#b124">[125]</ref>, support vector machine (SVM) <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b64">[65]</ref>, and neural network <ref type="bibr" target="#b6">[7]</ref>; or 3) discrete stochastic, e.g., n-gram <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b66">[67]</ref>. During the test phase, a test utterance is compared to each of the language-dependent models after going through the same preprocessing and feature extraction step. The likelihood of the test utterance to each model is computed. The language associated with the most likely model is hypothesized as the language of the utterance in accordance to the ML criterion as given in (1), (2), and (4) in Section II-B.</p><p>A wide spectrum of approaches has been proposed for modeling the characteristics of languages. In this paper, we are particularly interested in the two most effective ones: the acoustic-phonetic approach and the phonotactic approach. Fig. <ref type="figure" target="#fig_2">3</ref> shows exemplars from these two broad categories in one diagram illustrating their common ground and differences. The advanced techniques have explored the combination of different features and approaches.</p><p>The acoustic-phonetic approach is motivated by the observation that languages differ at a very fundamental level in terms of phones and frequencies of these phones occurring (i.e., the phonetic differences between languages). More importantly, it is assumed that the phonetic characteristics could be captured by some form of spectralbased features. We then proceed to model the distribution of each language in the feature space. Notably, shifteddelta cepstrum (SDC) used in conjunction with GMM has shown to be very successful in this regard <ref type="bibr" target="#b130">[131]</ref>, where a GMM is used to approximate the acoustic-phonetic distribution of a language. It is generally believed that each Gaussian density in a GMM captures some broad phonetic classes <ref type="bibr" target="#b109">[110]</ref>. However, GMM is not intended to model the contextual or dynamic information of speech.</p><p>The phonotactic approach is motivated by the belief that a spoken language can be characterized by its lexicalphonological constraints. With phone transcriptions, we build N phone n-gram models for an N language task. Briefly, a phone n-gram model <ref type="bibr" target="#b54">[55]</ref> is a stochastic model with discrete entries, each describing the probability of a subsequence of n phones (more details in Section III). Given a test utterance, each phone n-gram model produces a likelihood score. The language of the most likely hypothesis represents the classification decision. The method is referred to as the phone recognition followed by language modeling (PRLM) in the literature <ref type="bibr" target="#b150">[150]</ref>. The languages used for the phone recognizers need not be the same, or may even be disjoint with any of those recognized. The idea is analogous to the case where a human listener uses his native language knowledge to describe unknown languages.</p><p>From a system development point of view, it is worth noting that the acoustic-phonetic approach requires only the digitized speech utterances and their language labels, while the phonotactic approach requires the phonetic transcription of speech, which could be expensive to obtain. The two broad categories as described above are by no means encompassing all possible algorithms and approaches available. Variants exist within and between these broad categories, for examples, GMM tokenization <ref type="bibr" target="#b129">[130]</ref>, parallel phone recognition <ref type="bibr" target="#b150">[150]</ref>, universal phone recognition (UPR) <ref type="bibr" target="#b72">[73]</ref>, articulatory attribute-based approach <ref type="bibr" target="#b121">[122]</ref>, <ref type="bibr" target="#b122">[123]</ref>, discriminatively trained GMM using maximum mutual information (MMI) <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b142">[142]</ref> or minimum classification error (MCE) <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b104">[105]</ref>, just to name a few.</p><p>State-of-the-art language recognition systems consist of multiple subsystems in parallel, where individual scores are combined via a postprocessing back-end, as shown in Fig. <ref type="figure" target="#fig_2">3</ref>. The motivation of score fusion is to harness the complementary behavior among subsystems provided that their errors are not completely correlated. Also appearing at the score postprocessing back-end is a calibration stage, the purpose of which is to ensure that individual scores are consistently meaningful across utterances. In particular, score calibration has been shown to be essential for a language detection task where decisions have to be made using a fixed threshold for all target languages given test segments with arbitrary in-set and out-of-set languages <ref type="bibr" target="#b84">[85]</ref>- <ref type="bibr" target="#b86">[87]</ref>. On the other hand, well-calibrated scores also make it easier to map unconstrained scores to a normalized range which can be viewed as confidence scores for downstream consumers. Recent works reported in <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, and <ref type="bibr" target="#b11">[12]</ref> have shown a unified framework where score calibration and fusion are performed jointly using the multiclass logistic regression. We elaborate further on score calibration and fusion in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PHONOTACTIC APPROACHES</head><p>The first sustained effort using the phonotactic patterns was to distinguish languages by comparing the frequency of occurrences of certain reference sounds or sound sequences with that of the target languages <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b69">[70]</ref>.</p><p>To do this, we need to first tokenize the running speech into sound units. This can be achieved by a conventional speech recognizer that employs phonetic classes, phone classes, or phones <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b94">[95]</ref>, <ref type="bibr" target="#b132">[133]</ref>, <ref type="bibr" target="#b143">[143]</ref>, <ref type="bibr" target="#b149">[149]</ref> as the sound units. The first attempt with phonotactic constraints for language recognition was conducted using a Markov process to model sequences of broad phonetic classes generated by a manual phonetic transcription in eight languages <ref type="bibr" target="#b50">[51]</ref>. The Markov process with the broad phone classes was later applied to real speech data for the language identification of five languages <ref type="bibr" target="#b76">[77]</ref>. Next, we discuss the tokenization techniques and two common phonotactic modeling frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Speech Tokenization</head><p>A speech tokenizer converts an utterance into a sequence of sound tokens. As illustrated in Fig. <ref type="figure" target="#fig_3">4</ref>, a token is defined to describe a distinct acoustic-phonetic attribute and can be of different sizes, ranging from a speech frame, a subword unit such as a phone or a syllable, to a lexical word.</p><p>The GMM tokenization technique <ref type="bibr" target="#b129">[130]</ref> operates at the frame level. It converts a sequence of speech frames into a sequence of Gaussian labels, each of which gives rise to the highest likelihood of the speech frame. This is similar to a vector quantization process. As GMM tokenization does not attempt to relate a Gaussian label with an acousticphonetic event, it does not require transcribed speech data for model training. Unfortunately, the interframe phonotactics only describe speech dynamics at a range of tens of milliseconds, which is too short to capture the lexical-phonological information. In general, GMM tokenization is inferior to phone or subword tokenization as far as the system performance is concerned.</p><p>Another method worth mentioning is the attributebased approach <ref type="bibr" target="#b121">[122]</ref>, <ref type="bibr" target="#b122">[123]</ref>, which tokenizes speech utterances into sequences of articulatory attributes instead of phones. An advantage of attribute-based units is that they are defined such that the same set of units is common to all languages. The training data available for different languages can, therefore, be shared to build a universal attribute recognizer (UAR). Similar objective was conceived for the UPR front-end <ref type="bibr" target="#b72">[73]</ref> with a key difference that the UAR uses a much smaller set of speech units. For example, the work reported in <ref type="bibr" target="#b121">[122]</ref> and <ref type="bibr" target="#b122">[123]</ref> uses 15 attributes (five manner-of-articulation attributes, nine place-of-articulation attributes, and one silence unit to indicate the absence of articulation).</p><p>As a tradeoff between the development cost and effectiveness, the phone units are widely used in the state-ofthe-art systems. A phone tokenizer is also referred to as a phone recognizer, where a phone is typically modeled by an HMM <ref type="bibr" target="#b106">[107]</ref>. An open phone loop (or null grammar) configuration <ref type="bibr" target="#b51">[52]</ref>, whereby the transition from one phone to the others is equally probable, is often used in the phone decoding process.</p><p>We have observed in practice that a phone recognizer in a language can be used to tokenize speech of any other language. For example, a high-quality Hungarian phone recognizer based on long temporal context <ref type="bibr" target="#b89">[90]</ref> has been widely used in the community as the phone recognizer for a variety of target languages. The role of a phone recognizer is to provide as accurate as possible phonotactic statistics for language characterization. It is, therefore, not a surprise to know that the accuracy of a phone recognizer has a great impact on the language recognition performance <ref type="bibr" target="#b72">[73]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Phone n-Gram Modeling</head><p>Similar to a word n-gram that describes near-distance word occurrences, a phone n-gram captures the patterns of phone sequences using a multinomial distribution. A phone n-gram model typically employs one or multiple phone recognizers as the front-end and phone n-gram modeling for target languages as the back-end. The frontend tokenizes speech inputs based on a common phoneme inventory, while the back-end language model describes what each target language should look like in terms of phone n-gram statistics. Fig. <ref type="figure" target="#fig_4">5</ref> depicts the block diagram of a PRLM language recognition system with a single phone recognizer and phone n-gram models. To illustrate how it works, we take a single phone recognizer, e.g., English phone recognizer, as an example. During training, for each of the N target languages fL 1 ; L 2 ; . . . ; L N g, the training utterances are tokenized into sequences of English phones, which are then used to train the phone n-gram models f 1 ; 2 ; . . . ; N g. At runtime, the test utterance is tokenized by the same English phone recognizer to give a phone sequence of length J, Y b ¼ w 1 ; w 2 ; . . . ; w J . For each target language l, the log likelihood of the phone sequence can be formed as follows:</p><formula xml:id="formula_4">log PðY b j l Þ ¼ X J j¼1 log P l w j jw jÀ1 . . . w jÀðnÀ1Þ À Á :<label>(5)</label></formula><p>Equation ( <ref type="formula" target="#formula_4">5</ref>) is also interpreted as the cross entropy between the statistics of the phone sequence Y b that represents the empirical distribution of the test sample, with the phone n-gram models log PðY b j l Þ¼ X ŵ CðŵÞ log P l w j jw jÀ1 . . . w jÀðnÀ1Þ</p><formula xml:id="formula_5">À Á<label>(6)</label></formula><p>where ŵ ¼ w j w jÀ1 . . . w jÀðnÀ1Þ and CðŵÞ is the normalized count of the n-gram ŵ in the sequence Y b . The cross entropy measures how well a phone n-gram model predicts a phone sequence. By substituting ( <ref type="formula" target="#formula_5">6</ref>) into (2), we can make a language recognition decision. Working in the same principle as PRLM, parallel phone recognition followed by the phone n-gram language model or parallel phone recognition language modeling (PPR-LM) employs multiple phone recognizers, each of which is trained for a language. The multiple phone recognizers provide the statistics of the test sample from different viewpoints. As shown in Fig. <ref type="figure" target="#fig_5">6</ref>, for each of the F phone recognizers, we train N language models for the N target languages. PPR-LM can be seen as a fusion of multiple PRLM subsystems. Given a test utterance, FN language model scores are generated through the FN phone n-gram models, f ;l for f ¼ 1; 2; . . . ; F and l ¼ 1; 2; . . . ; N. With the scores, one can devise a strategy for language recognition decision making. In the case where we need a single output score, there are many ways to summarize the FN scores into one. One possible way is to fuse the posterior probabilities from F parallel subsystems, as follows:</p><formula xml:id="formula_6">log PðL l jOÞ ¼ X F f ¼1 log PðY b f j f ;l Þ P N i¼1 PðY b f j f ;i Þ<label>(7)</label></formula><p>where Y b f is the phone sequence generated from speech utterance O by the f th phone recognizer. PðY b f j f ;l Þ is the likelihood score for the lth target language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Vector Space Modeling</head><p>The bag-of-sounds framework <ref type="bibr" target="#b71">[72]</ref> marks another successful attempt in language recognition. The idea is to represent the empirical distribution of the test sample in a high-dimensional vector. The bag-of-sounds concept is analogous to the bag-of-words paradigm originally formulated in information retrieval and text categorization <ref type="bibr" target="#b112">[113]</ref>. The bag-of-words paradigm represents a text document as a vector of word counts. It is believed that it is not just the words, but also the co-occurrence of words that distinguish semantic domains of text documents. One can easily draw the analogy between a sound token in the bag-of-sounds and a word in the bag-of-words. The difference is that we deal with a phone sequence instead of a word sequence.</p><p>In the bag-of-sounds framework, we arrange the phone n-gram statistics of both training and test samples into high-dimensional vectors. Fig. <ref type="figure">7</ref> depicts the block diagram of a PPR-VSM architecture, 1 in which PPR is followed by vector space modeling (VSM) <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b74">[75]</ref>. Suppose that we have F phone recognizers with a phone inventory of V ¼ fV 1 ; . . . ; V f ; . . . ; V F g and the number of phonemes in V f is n f . A speech utterance is decoded by these phone recognizers into F phone sequences. Each of these phone sequences can be expressed by a high-dimensional phonotactic feature vector with the phone n-gram statistics. The dimension of the feature vector is equal to the total number of phone n-gram patterns needed to highlight the overall behavior of the utterance. If unigram and bigram are the only concerns, we will have a vector of n f þ n 2 f phonotactic elements, denoted as v f , to represent the utterance by the f th phone recognizer. As shown in Fig. <ref type="figure">7</ref>, all the F phonotactic feature vectors are concatenated into a composite bag-of-sounds vector v ¼ ½v T 1 ; . . .</p><formula xml:id="formula_7">; v T f ; . . . ; v T F T , with a dimension of B ¼ P f ðn f þ n 2 f Þ, if only unigram and bigram features are included.</formula><p>After a spoken utterance is vectorized in this way, language recognition can be cast as a vector-based classification problem. The simplest way is to measure the similarity between two composite vectors, one derived from the test utterance and another derived from all the training data of a target language. The similarity between two vectors in VSM can be approximated by the inner product or cosine distance <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b113">[114]</ref>. If we take a close look at (6), we can find that the cross entropy can be seen as an inner product between the normalized count vector of the test utterance and the vectorized log-probability from the target phone n-gram model.</p><p>Vector space modeling techniques benefit from the recent progress in SVM, which offers a low-cost solution to the classification of high-dimensional vectors. The SVM is optimized based on a structural risk minimization principle <ref type="bibr" target="#b138">[138]</ref>. Due to its distribution-free property, it has the advantage of providing an excellent generalization capability.</p><p>Suppose that we have two bag-of-sounds vectors of B dimensions x ¼ ½x 1 ; x 2 ; . . . ; x B T and y ¼ ½y 1 ; y 2 ; . . . ; y B T , extracted from two speech utterances. Each of the vectors Fig. <ref type="figure">7</ref>. Block diagram of a PPR-VSM language recognition system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>A similar architecture was also studied in speaker recognition <ref type="bibr" target="#b14">[15]</ref>. represents a discrete empirical distribution of phonotactic counts. An L 2 inner product kernel <ref type="bibr" target="#b17">[18]</ref> is given by ðx</p><formula xml:id="formula_8">; yÞ ¼ x T y ¼ X B i¼1 x i Á y i<label>(8)</label></formula><p>which measures the similarity between two bag-of-sounds vectors.</p><p>For each target language, an SVM is trained using the bag-of-sounds vectors pertaining to the target language as the positive set, with those from all other languages as the negative set. In this way, N one-versus-the-rest SVM models are built, one for each target language. Given a test utterance, N SVM output scores will be generated for language recognition. Alternatively, the N SVM scores can be used to produce an N-dimensional score vector <ref type="bibr" target="#b81">[82]</ref>, and thus we are able to project the high-dimension composite feature vectors into a much lower dimension of N. The generated N-dimensional score vectors can be further modeled by a Gaussian back-end for classification decision <ref type="bibr" target="#b151">[151]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Phonotactic Front-End</head><p>By carefully examining Figs. <ref type="figure" target="#fig_5">6</ref> and<ref type="figure">7</ref>, we realize that phone n-gram modeling and vector space modeling are using the same phone recognition front-end, such as a PPR front-end, to derive the n-gram statistics. The difference lies in the way of phone n-gram representation. Therefore, any improvement in phone recognition frontend will benefit both modeling techniques.</p><p>The study of bag-of-sounds has motivated a series of works to further the design of the PPR-VSM front-end. There have been attempts to explore high-order n-grams and to select discriminative phonotactic features <ref type="bibr" target="#b71">[72]</ref>, <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b111">[112]</ref>, <ref type="bibr" target="#b131">[132]</ref>, to redesign phone recognizers that are knowledgeable about the target languages <ref type="bibr" target="#b127">[128]</ref>, and to construct diversified acoustic models for phone recognizers to provide different acoustic perspectives <ref type="bibr" target="#b119">[120]</ref>. While the phone n-grams are typically estimated over the onebest phone transcriptions, it was discovered that the expected n-gram counts derived from a phone lattice outperform the n-gram counts from one-best phone transcriptions. The improvement is attributed to the richer information available in the lattice than in the one-best results <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b117">[118]</ref>.</p><p>In general, it is believed that more parallel phone recognizers, higher order phone n-grams, and more accurate phone recognizers provide more informative phonotactic features and thus lead to better performing systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. ACOUSTIC-PHONETIC APPROACHES</head><p>As discussed in Section II-A, we believe that each language has its unique phonetic repertoire. In the acoustic-phonetic approach, we attempt to model the acoustic-phonetic distribution of a language using the acoustic features.</p><p>The early efforts of acoustic-phonetic approaches to language recognition probably began in the 1980s. A polynomial classifier based on 100 features derived from linear predictive coding (LPC) analysis <ref type="bibr" target="#b26">[27]</ref> was studied for recognition of eight languages. A VQ technique with pitch contour and formant frequency features was proposed for recognition of three languages <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b41">[42]</ref>. A study using multiple language-dependent VQ codebooks and a universal VQ codebook (i.e., general and language independent) was conducted over the LPC derived features for recognition of 20 languages <ref type="bibr" target="#b124">[125]</ref>. This study suggests that acoustic features are effective in different settings. A comparative study among four modeling techniques, VQ, discrete HMM, continuous-density HMM (CDHMM), and GMM with mel-cepstrum coefficients, was carried out over four languages, showing that CDHMM and GMM offer a better performance <ref type="bibr" target="#b97">[98]</ref>.</p><p>In this section, we will discuss feature extraction, two modeling techniques, and intersession variability compensation that all affect the system performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Acoustic Feature Extraction</head><p>Mel-frequency cepstral coefficients (MFCCs) <ref type="bibr" target="#b31">[32]</ref> are effective in most speech recognition tasks because they exploit auditory principles, whereby the mel-scale filter bank is a rough approximation to human auditory system's response <ref type="bibr" target="#b105">[106]</ref>. It is not surprising that they work well in language recognition as well. To overcome undesired variation across sessions, compensation techniques such as mean-variance normalization (MVN) <ref type="bibr" target="#b51">[52]</ref>, RASTA <ref type="bibr" target="#b46">[47]</ref> and vocal tract length normalization (VTLN) <ref type="bibr" target="#b67">[68]</ref> are typically applied after the voice activity detection (VAD) process, by which silence is removed.</p><p>Typically, MFCC features are computed at each short speech segment (e.g., 10 ms) together with their first-and second-order derivatives to capture the short-term speech dynamics. We would like to mention in particular the shifted-delta-cepstral (SDC) coefficients <ref type="bibr" target="#b130">[131]</ref>, which are useful for language recognition because they capture the speech dynamics over a wider range of speech frames than the first-and second-order MFCC derivatives. The computation of the SDC features is illustrated in Fig. <ref type="figure" target="#fig_6">8</ref>. The SDC features are specified by four parameters fZ; d; P; kg, where Z is the number of cepstral coefficients computed at each frame, d represents the time advance and delay for the delta computation, k is the number of delta-cepstral blocks whose delta-cepstral coefficients are stacked to form the final feature vector, and P is the time shift between consecutive blocks. For example, the Z static features are computed as cðtÞ ¼ c 0 ðtÞ; c 1 ðtÞ; . . . ; c ZÀ1 ðtÞ ½ T (9)</p><p>and the ith block of delta-cepstral features is computed as</p><formula xml:id="formula_9">Dcðt; iÞ ¼ cðt þ iP þ dÞ À cðt þ iP À dÞ:<label>(10)</label></formula><p>Finally, each SDC feature vector at time t contains Z parameters of static cepstral features and kZ delta features.</p><p>A commonly adopted SDC configuration is Z À dÀ P À k ¼ 7 À 1 À 3 À 7 leading to kZ ¼ 49 delta features in addition to seven static features. Such SDC features cover temporal information over kP ¼ 21 consecutive frames of cepstral features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Statistical Modeling</head><p>Language recognition and speaker recognition have many similarities in terms of technical formulation, methodologies, and evaluation measurement. The statistical modeling technique, with the universal-backgroundmodel-based GMM (GMM-UBM), brings the same success to language recognition that it has brought to speaker recognition <ref type="bibr" target="#b110">[111]</ref>. One of the attractive attributes of the GMM is its ability to closely approximate any arbitrarily shaped data distributions, and its ability to model the underlying data classes by the individual Gaussian components. In language recognition, we consider the set of spectral frames from the utterances as a collection of independent samples. A GMM is used to approximate the overall acoustic-phonetic distributions of a spoken language. The GMM modeling technique is popular in language recognition due to not only its ability to model a large class of sample distributions, but also its competitive performance in practice.</p><p>For a D-dimensional feature vector o with M Gaussian mixture density functions, the likelihood function of a GMM is a weighted linear combination</p><formula xml:id="formula_10">pðojÞ ¼ X M i¼1 ! i N ðojM i ; 2 i Þ:<label>(11)</label></formula><p>Each of the Gaussian density functions is parameterized by a D-dimensional mean vector M i and a D Â D covariance matrix 2 i . A diagonal covariance matrix is normally adopted for the sake of computation simplicity and practical consideration especially when only a limited amount of training data is available. The Gaussian density function is given as follows:</p><formula xml:id="formula_11">N ðojM i ; 2 i Þ ¼ 1<label>ð2Þ</label></formula><formula xml:id="formula_12">D 2 j2 i j 1 2 exp À 1 2 ðo À M i Þ T 2 À1 i ðo À M i Þ ! (<label>12</label></formula><formula xml:id="formula_13">)</formula><p>with the mixture weights summing to one,</p><formula xml:id="formula_14">P M i¼1 ! i ¼ 1. We denote a GMM as ¼ f! i ; M i ; 2 i ; i ¼ 1; . . . ; Mg.</formula><p>Given a collection of training feature vectors, a GMM is generally trained with the expectation-maximization (EM) algorithm <ref type="bibr" target="#b35">[36]</ref>, where the model parameters are estimated with the ML criterion. In language identification, we train a GMM for each language for the recognition task as formulated in (1).</p><p>In the GMM-UBM paradigm, where UBM is a background model that represents the world's spoken language, we usually start by training a UBM with data from all languages and adapt a GMM model for each language from the UBM using the MAP technique <ref type="bibr" target="#b38">[39]</ref>. In practice, we often suffer from insufficient training data to build a GMM from scratch. The GMM-UBM training process offers a solution to overcome such a problem. With a simple formulation yet competitive performance, the GMM-UBM technique has become a reference system in language recognition, which we will further discuss in Section V-C.</p><p>As the GMM model operates by capturing the underlying acoustic classes as reflected in the spectral feature distributions for each language, it is vulnerable to undesired variability due to nonlanguage effects, such as speaker and channel. Several training techniques have been attempted to address session variations and to improve discriminative ability, for example, constrained maximum-likelihood linear regression (CMLLR) <ref type="bibr" target="#b118">[119]</ref>, soft margin estimation <ref type="bibr" target="#b148">[148]</ref>, and MMI training <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Vector Space Modeling</head><p>Similar to vector space modeling in the phonotactic approaches, there have been effective ways to characterize the spectral features of an utterance as a high-dimensional vector under the SVM paradigm. </p><formula xml:id="formula_15">À d À P À k ¼ 7 À 1 À 3 À 7.</formula><p>The SVM is typically used to separate vectors in a binary classification problem. It projects an input vector x into a scalar value f ðxÞ, as follows:</p><formula xml:id="formula_16">f ðxÞ ¼ X I i¼1 i ðx; x i Þ þ<label>(13)</label></formula><p>where vectors x i are support vectors, I is the number of support vectors, i are the weights, and is a bias. The weights imposed on the support vectors are constrained such that P I i¼1 i ¼ 0 and i 6 ¼ 0. Function ðÁÞ is the kernel, subject to certain properties (the Mercer condition), so that it can be expressed as</p><formula xml:id="formula_17">ðx; yÞ ¼ T ðxÞðyÞ (<label>14</label></formula><formula xml:id="formula_18">)</formula><p>where ðxÞ is a mapping from the input space to a possibly infinite-dimensional space. Now, the question is: How do we represent a speech utterance, thus a language, in highdimensional vector space? We will discuss two different vector space modeling techniques that characterize a speech utterance with spectral features and GMM parameters, respectively.</p><p>After acoustic feature extraction, a speech utterance has become a sequence of spectral feature vectors. Comparing two speech utterances with the SVM, there needs to be a way of taking two such sequences of feature vectors, calculating a sequence kernel operation, and computing the SVM output. A successful approach using the sequence kernel is the generalized linear discriminative sequence (GLDS) <ref type="bibr" target="#b15">[16]</ref>. It takes the explicit polynomial expansion of the input feature vectors and applies the sequence kernel based on generalized linear discriminants. The polynomial expansion includes all the monomials of the features in a feature vector.</p><p>As an example, for two features in the input feature vector o ¼ ½o 1 ; o 2 T , the monomials with degree two are bðoÞ</p><formula xml:id="formula_19">¼ ½1; o 1 ; o 2 ; o 2 1 ; o 1 o 2 ; o 2 2 T .</formula><p>Let O X and O Y be two input sequences of feature vectors. The polynomial discriminant can be obtained using the mean-squared error criterion <ref type="bibr" target="#b15">[16]</ref>. The resulting sequence kernel can be expressed as follows:</p><formula xml:id="formula_20">ðO X ; O Y Þ ¼ bT X R À1 bY<label>(15)</label></formula><p>where the vector</p><formula xml:id="formula_21">b ¼ 1 T X T t¼1 b oðtÞ f g (<label>16</label></formula><formula xml:id="formula_22">)</formula><p>is the average expansion over all the feature vectors of the input utterance O ¼ foð1Þ; oð2Þ; . . . ; oðTÞg. The matrix R is the correlation matrix obtained from a background data set. An approximation of R can be applied to calculate only the diagonal terms <ref type="bibr" target="#b15">[16]</ref>.</p><p>While we consider a GLDS kernel as a nonparametric approach to the language recognition problem, the GMM supervector offers a parametric alternative. Given a UBM and a speech utterance drawn from a language, we derive a GMM supervector m ¼ ½M 1 ; M 2 ; . . . ; M M T by stacking the mean vectors of all the adapted mixture components <ref type="bibr" target="#b17">[18]</ref>. In this way, a speech utterance is mapped to a highdimensional space using the mean parameters of a GMM. It should be noted that the MAP adaptation is performed for each and every utterance available for a particular language. For two speech utterances O X and O Y , two GMMs can be derived using MAP adaptation from the UBM to obtain two supervectors m X ¼ ½M X 1 ; M X 2 ; . . . ; M X M T and</p><formula xml:id="formula_23">m Y ¼ ½M Y 1 ; M Y 2 ; . . . ; M Y M T .</formula><p>There are a number of ways to compare two speech utterances in terms of the derived mean vectors. A natural choice is the Kullback-Leibler (KL) divergence, which can be approximated by the following upper bound <ref type="bibr" target="#b17">[18]</ref>:</p><formula xml:id="formula_24">dðm X ; m Y Þ ¼ 1 2 X M i¼1 w i M X i À M Y i À Á T 2 À1 i M X i À M Y i À Á :<label>(17)</label></formula><p>We can then find the corresponding inner product that serves as a kernel function, i.e., the so-called KL kernel <ref type="bibr" target="#b17">[18]</ref>, as follows:</p><formula xml:id="formula_25">KL ðO X ; O Y Þ ¼ X M i¼1 ffiffiffiffi ffi w i p 2 À1=2 i M X i T ffiffiffiffi ffi w i p 2 À1=2 i M Y i :<label>(18)</label></formula><p>Note that the KL kernel only accommodates adaptation of GMM to mean vectors while leaving the covariance matrices unchanged. A Bhattacharyya kernel <ref type="bibr" target="#b145">[145]</ref> was proposed that allows for adaptation of covariance matrices, showing an improved performance. Similar to the KL kernel, the Bhattacharyya kernel represents speech utterances as the adapted mean vectors of GMMs</p><formula xml:id="formula_26">BHATT ðO X ; O Y Þ ¼ X M i¼1 2 X i þ 2i 2 À1=2 M X i À Mi À Á " # T Â 2 Y i þ 2i 2 À1=2 M Y i À Mi À Á " # :<label>(19)</label></formula><p>The difference lies at the adapted covariance matrices 2 X i and 2 Y i , which appear as the normalization factors to the adapted mean vectors <ref type="bibr" target="#b145">[145]</ref>. Here, Mi and 2i are, respectively, the mean vectors and covariance matrices of the UBM. In <ref type="bibr" target="#b18">[19]</ref>, the KL kernel was extended to include covariance matrices into the supervector. An interesting interaction between the GMM supervector and the SVM is that SVM parameters can be pushed back to GMM models that allow for a more effective scoring.</p><p>Finally, for each target language, a one-versus-the-rest SVM can be trained in the vector space with the target language being the positive set and all other competing languages being the negative set. A decision strategy can be devised to summarize the outputs of multiple one-versusthe-rest SVMs during the runtime test, as suggested in Section III-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Intersession Variability</head><p>In language recognition, we face several sources of nonlanguage variability, such as speaker, gender, channel, and environment, in the speech signals. For simplicity, we refer to all such variabilities as intersession variability, that is, the variability exhibited by a given language from one recording session to another. The class of subspace methods for model compensation, such as joint factor analysis (JFA) <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b140">[140]</ref> and nuisance attribute projection (NAP) <ref type="bibr" target="#b123">[124]</ref>, which originated from speaker recognition research, is useful to address the variability issues. The idea of the subspace techniques is to model and eliminate the nuisance subspace pertaining to intersession variability, thereby reduce the mismatch between training and test. Recent efforts have brought the subspace methods from model domain to feature domain, such as the feature-level latent factor analysis (fLFA) <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b139">[139]</ref> and feature-level NAP (fNAP) <ref type="bibr" target="#b19">[20]</ref> that have shown superior performance in language recognition. Feature domain compensation is not tied to a specific model assumption, thus, it could be used for a wider range of modeling schemes. We explain the working principle of fLFA as follows.</p><p>We assume that the GMM supervector m, derived for a speech utterance, can be decomposed into the sum of two supervectors</p><formula xml:id="formula_27">m ¼ m þ Uz (<label>20</label></formula><formula xml:id="formula_28">)</formula><p>where m is the UBM mean supervector, U is a low-rank matrix projecting the latent factor subspace into the supervector model domain, and z is a low-dimensional vector holding the latent factors for the current speech utterance and language <ref type="bibr" target="#b139">[139]</ref>. Matrix U can be estimated using an EM training based on the principal component analysis <ref type="bibr" target="#b125">[126]</ref>, and the latent factors z are estimated for each session using the probabilistic subspace adaptation method based on MAP estimation <ref type="bibr" target="#b79">[80]</ref>. In order to conduct feature-domain compensation, (20) can be rewritten as</p><formula xml:id="formula_29">M i ¼ Mi þ U i z<label>(21)</label></formula><p>where M i and Mi are the mean vectors of the ith Gaussian component of the GMM and the UBM, respectively. Submatrix U i is the intersession compensation offset related to the ith Gaussian component. The compensation of feature vector oðtÞ is obtained by subtracting a weighted sum of the intersession compensation bias vector</p><formula xml:id="formula_30">o ~ðtÞ ¼ oðtÞ À X M i¼1 i U i z (<label>22</label></formula><formula xml:id="formula_31">)</formula><p>where i is the Gaussian occupation probability.</p><p>Most recently, another factor analysis technique, i-vector <ref type="bibr" target="#b33">[34]</ref>, has become very popular in speaker recognition and has been also introduced to language recognition <ref type="bibr" target="#b34">[35]</ref>. The i-vector paradigm provides an efficient way to compress GMM supervectors by confining all sorts of variabilities (both language and nonlanguage) to a lowdimensional subspace, referred to as the total variability space. The generative equation is given by</p><formula xml:id="formula_32">m ¼ m þ Tw (<label>23</label></formula><formula xml:id="formula_33">)</formula><p>where matrix T is the so-called total variability matrix. The latent variable w is taken to be a low-dimensional random vector with a standard normal distribution. For a speech utterance O, its i-vector is given by the posterior mean of the latent variable w, i.e., EfwjOg. Since T is always a low-rank rectangular matrix, the dimensionality of the i-vector is much smaller compared to that of the supervector m.</p><p>One subtle difference between the total variability space T and the session variability space U, as given in <ref type="bibr" target="#b19">(20)</ref>, is that the former captures both the intersession variability as well the intrinsic variability between languages, which is not represented in the latter. As such, matrix U is used to remove the unwanted intersession variability, whereas the purpose of matrix T is to preserve the important variability in a much more compact form than the original supervector. Compensation techniques are then applied on the i-vectors to cope with the intersession variability. To this end, the linear discriminant analysis has been shown to be effective <ref type="bibr" target="#b34">[35]</ref>. It is worth noting that, though labeled data are not required for training the total variability subspace pertaining to the i-vectors, the subsequent compensation techniques can only be effective when training data for the intended intersession variability are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. TOPICS IN SYSTEM DEVELOPMENTS</head><p>Most state-of-the-art language recognition systems consist of multiple subsystems, which include various forms of acoustic or phonotactic approaches, as described earlier.</p><p>Each subsystem provides an expert view about the input speech. To make a balanced decision that reflects a mixture of expert views, we rely on an effective fusion technique. By combining the individual outputs, we aim at a higher accuracy than that of the best subsystem <ref type="bibr" target="#b58">[59]</ref>. The ways in which subsystems can be combined are numerous <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b58">[59]</ref>. While fusion can take place at the feature, model, or score levels, studies have shown that fusion at the score level is the most effective in delivering increased accuracy <ref type="bibr" target="#b103">[104]</ref>. In this section, we show the latest advances in decision fusion whereby output scores from component subsystems are combined at the score level through the use of Gaussian back-end and multiclass logistic regression. The combined scores for the overall system could then be used for the purpose of language identification and verification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Magic of Gaussian Back-End</head><p>The notion of a Gaussian back-end was first introduced and adopted in <ref type="bibr" target="#b151">[151]</ref> for the fusion of likelihood scores from phone n-grams. It was then extended to a general fusion device taking scores from recognizers of any kind, be it phonotactics or acoustics based. The importance of the Gaussian back-end is twofold: fusion and calibration.</p><p>We consider the output scores from subsystems as elements of a score vector. If there are K subsystems and N target languages, then there are KN elements in the score vector. That is, the score vectors from the component subsystem, each producing N number of scores sðk; lÞ, are stacked to form the score vector s ¼ ½s T 1 ; s T 2 ; . . .</p><formula xml:id="formula_34">; s T K T ,</formula><p>where s k ¼ ½sðk; 1Þ; sðk; 2Þ; . . . ; sðk; NÞ T . During the training phase, the collection of score vectors associated with a given target language are used to train a multivariate normal distribution, one for each of the N target languages. With the ML criterion, the mean vectors of the Gaussian models are given by the sample means of the score vectors based on the language labels. The covariance matrices are estimated in a similar manner, subject to constraints, like, tied, diagonal, full, or structured covariance, by which model complexity could be controlled. Let fM l ; 2 l ; l ¼ 1; . . . ; Ng be the set of Gaussian backend parameters consisting of class-dependent mean vectors and covariance matrices. The back-ended scores, s 0 l for l ¼ 1; . . . ; N, are obtained by evaluating the log Gaussian likelihood log N ðsjM l ; 2 l Þ for each target language L l 2 fL 1 ; L 2 ; . . . ; L N g. For the case where all Gaussians share a common covariance matrix, we form what is called the linear back-end, as follows:</p><formula xml:id="formula_35">s 0 l ¼ log N ðsjM l ; 2Þ À ¼ M T l 2 À1 À Á s À 1 2 M T l 2 À1 M l (24)</formula><p>where terms common to all classes are consolidated as ¼ ðs T 2 À1 sÞ=2 þ C and discarded. Here, C accounts for the normalization factor in the Gaussian function. Clearly, the simplification leads to scaling of the Gaussian likelihoods consistently for all l with a common scaling factor of expðÀÞ, which essentially disappears when the likelihood ratio or posterior probabilities are computed.</p><p>The back-end function in ( <ref type="formula">24</ref>) is made linear with respect to the input raw scores s by tying the Gaussian models with a common covariance matrix. The covariance matrix is trained by pooling data across all languages after removing the mean. Besides parameter tying, we could also resort to diagonal or full covariance leading to a linear back-end with different complexity. To compromise between these two, we may consider using the factor analysis or the probabilistic principle component analysis for modeling the covariance. In <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b131">[132]</ref>, the linear discriminant analysis was used on the score vectors coming out from each core recognizer. The resulting score vector is, therefore, decorrelated justifying the use of a diagonal covariance matrix.</p><p>Another form of the back-end is obtained by imposing a heteroscedastic assumption on the score space, as opposed to the homoscedastic assumption where covariance matrices are tied. As the covariance matrices become different for each target language, an additional quadratic term s T 2 À1 l s appears in the back-end function. A quadratic back-end is particularly useful for modeling out-of-set languages, which generally exhibit larger variation in the score space <ref type="bibr" target="#b10">[11]</ref>.</p><p>The essential element of back-end processing is to recognize languages in the score space by using the output from one or more subsystems (e.g., PPR-LM, SVM, GMM) as features. These subsystems could, therefore, be viewed as score generators <ref type="bibr" target="#b144">[144]</ref>. The scores produced by the subsystems can be in different forms, for instance, loglikelihood, class posterior, or any deterministic distance measure (e.g., Mahalanobis distance). Using a generative score-vector model for each class in the form of a multivariate normal distribution per class, the back-ended scores are calibrated into well-behaved log-likelihood scores from which application-dependent decisions can be made systematically <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b133">[134]</ref>. At the same time, the back-end effectively performs fusion of the recognizers.</p><p>Another interesting byproduct of using the Gaussian back-end is that we can now use it for new languages for which we have only a limited amount of development data. With K subsystems for N target languages, we can train a Gaussian model using score vectors for the ðN þ 1Þth language, a new target language, as long as we are given some developmental data from this language. This is also particularly useful to model out-of-set languages. To this end, the score vectors associated with nontarget languages are pooled and used to train a Gaussian model. Out-of-set languages are then treated as an additional class representing the none-of-the-above hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multiclass Logistic Regression</head><p>Another way of combining likelihood scores from multiple subsystems is the product rule <ref type="bibr" target="#b103">[104]</ref>, <ref type="bibr" target="#b150">[150]</ref>, or equivalently, the sum of log likelihoods as follows:</p><formula xml:id="formula_36">s 0 l ¼ X K k¼1 log SðOj k;l Þ ¼ X K k¼1 sðk; lÞ<label>(25)</label></formula><p>for l ¼ 1; . . . ; N. The underlying assumption in <ref type="bibr" target="#b24">(25)</ref> is that the output scores SðOj k;l Þ from the models k;l can be interpreted as likelihood measures and the subsystems are independent of each other. No training is required where the log-likelihoods are essentially summed with equal weights. A more general form could be obtained by introducing additional control parameters as follows:</p><formula xml:id="formula_37">s 0 l ¼ X K k¼1 k sðk; lÞ þ l<label>(26)</label></formula><p>for l ¼ 1; . . . ; N. Note that the weights k assigned to subsystems are used across all target languages, while biases l are made dependent on the targets. The fusion function is linear with respect to the log-likelihood scores. The weights and biases could be found by an exhaustive grid search optimizing some application-dependent cost functions <ref type="bibr" target="#b73">[74]</ref>. A more systematic way is to use a multiclass logistic regression model <ref type="bibr" target="#b42">[43]</ref> for the class posterior</p><formula xml:id="formula_38">P L l js 0 l À Á ¼ exp s 0 l À Á P N i¼1 exp s 0 i ð Þ<label>(27)</label></formula><p>and to find the parameters that maximize the log-posterior probability on the development data, as follows <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b133">[134]</ref>:</p><formula xml:id="formula_39">Qð 1 ; . . . ; K ; 1 ; . . . ; N Þ¼ X M m¼1 X N l¼1 ml log P L l js 0 l À Á :<label>(28)</label></formula><p>In the above equation, M is the number of training samples and ml is the training label, i.e., ml ¼ w l ; if labeled as language L l 0; otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&amp;</head><p>Here, w l for l ¼ 1; 2; . . . ; N are weighting factors used to normalize the class proportion in the training data. If there are an equal number of training samples for each target language (or normalization is not intended), then we set w l ¼ 1. With w l ¼ M=ðM l NÞ, we normalize out the effect of class population M l from the cost function, where N is the number of target languages. No closed-form solution exists to maximize <ref type="bibr" target="#b27">(28)</ref>. A conjugate gradient-descent optimization technique was found to be the most efficient among several other gradient-based numerical optimization techniques <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b92">[93]</ref>. Notice that calibration is achieved jointly with score fusion via ( <ref type="formula" target="#formula_37">26</ref>)- <ref type="bibr" target="#b28">(29)</ref>, where subsystem scores are combined in the first term on the right-hand side of (26), while score calibration is achieved by scaling and shifting of the input scores. When K ¼ 1, ( <ref type="formula" target="#formula_37">26</ref>) reduces to a score calibration device. Such a form of calibration is referred to as the application-independent calibration in <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, and <ref type="bibr" target="#b134">[135]</ref>, the purpose of which is to allow a threshold to be set on the calibrated scores based on the cost and prior (i.e., the application parameters). Setting of threshold is crucial especially for language detection and open-set language identification tasks. The rationale behind this, as given in <ref type="bibr" target="#b11">[12]</ref>, is as follows. To make a decision with uncalibrated scores, one needs to probe the subsystem to understand the behavior of its output scores and, thus, to learn where to put the threshold. In contrast, given well-calibrated scores, one can calculate the risk expectation in a straightforward way and, thus, make minimum risk decisions with no previous experience of the subsystem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Identification and Verification</head><p>In a similar way as humans perceive the problem, it is intuitive to see automatic language recognition as an identification task. Given a spoken utterance, we match it against a predefined set of target languages and make a decision regarding the identity of the language spoken in the segment of speech. Another closely related recognition problem is language verification or detection. Here, we are given a speech segment and the task is to decide between two hypothesesVwhether the speech segment is from a target (or claimed) language.</p><p>Language recognition could be best manifested as a multiclass recognition problem where the input is known to belong to one from a set of discrete classes. The objective is to recognize the class of the input. In what follows, we adopt the formulation proposed in <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b9">[10]</ref> to further illustrate the problem.</p><p>We are given a list of N target classes, fL 1 ; L 2 ; . . . ; L N g, for each of which a prior probability is given (a flat prior could be assumed in a general application-independent setting). In the closed-set scenario, fL 1 ; L 2 ; . . . ; L N g represents N different explicitly specified languages. In the open-set case, L 1 ; L 2 ; . . . ; L NÀ1 are explicitly specified languages, and L N denotes any of the yet unseen out-of-set languages. Such an open-set scenario could be handled by introducing an artificial ''none-of-the-above'' class into the target set, for instance, at the Gaussian back-end. Given a spoken utterance O and the set of target languages, we have the following different, but closely related, tasks <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>.</p><p>Language identification: Which of the N languages does O belong to? Language verification: Does O belong to language L l or to other languages (i.e., one of the other N À 1 languages)? For the identification task, we compute the likelihood given each language and select the language hypothesis that yields the highest likelihood. The language verification or detection is a binary classification problem, where a decision has to be made between two hypotheses with respect to a decision threshold. In Section VI, we look into more details regarding threshold setting and performance assessment following the language detection protocol as established in NIST LREs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. THE NIST LRE PARADIGM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Corpora for LREs</head><p>The availability of sufficiently large corpora has been the major driving factor in the development of speech technology in recent decades <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b75">[76]</ref>, <ref type="bibr" target="#b86">[87]</ref>, <ref type="bibr" target="#b93">[94]</ref>. To model a spoken language, we need a set of speech data for the language. To account for the within-language variability, which we also call intersession variability, such as speaker, content, recording device, communication channel, and background noise, it is desirable to have sufficient data that include the intended intersession effects.</p><p>The first large-scale speech data collection for the purpose of language recognition research was carried out by OGI in the early 1990s. Their efforts resulted in the OGI-11L and OGI-22L corpora <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b93">[94]</ref>, which are now distributed through the Linguistic Data Consortium (LDC). As its name implies, the OGI-11L is a multilanguage corpus of 11 languages. The number of languages was expanded to 22 in OGI-22L. Similar data collections were organized by LDC leading to the CallHome and CallFriend corpora, consisting of 6 and 12 languages, respectively. The aforementioned corpora were collected over the telephone network, which reaches out to speakers of different languages easily over a wide geographical area <ref type="bibr" target="#b93">[94]</ref>. Both OGI-11L and OGI-22L are monologue speech corpora, where the callers answer to the questions prompted by a machine. The later collection of CallHome, CallFriend, and Mixer <ref type="bibr" target="#b24">[25]</ref> corpora was motivated by a more challenging task aiming at conversational speech using dialog between individuals.</p><p>Another drive that has contributed to advances in speech research is the effort toward standard protocols for performance evaluation. The paradigm of formal evaluation was established by NIST in response to this need by providing the research community with a number of essential elements, such as manually labeled data sets, well-defined tasks, evaluation metrics, and a postevaluation workshop <ref type="bibr" target="#b86">[87]</ref>.</p><p>LREs were conducted by <ref type="bibr">NIST in 1996</ref><ref type="bibr">NIST in , 2003</ref><ref type="bibr">NIST in , 2005</ref><ref type="bibr">NIST in , 2007</ref><ref type="bibr">NIST in , 2009</ref><ref type="bibr">NIST in , and 2011</ref>. It is evident that more languages are being included from year to year, as can be observed in Table <ref type="table" target="#tab_1">2</ref>, which summarizes the language profile. Another highlight is that NIST LRE has set out to focus on a detection task since its inception, as both closed-set and open-set identification problems can be easily formulated as applications of the detection task. Furthermore, performance measures such as identification accuracy, which depends on the number of target languages, could be factored out in the detection task.</p><p>NIST LREs have also sought to examine dialect detection capabilities, besides language detection. While the term dialect may have different linguistic definitions, it means a variety of language that is a particular characteristic of speakers from a specific geographical region. For instance, the dialects of interest include variants of Chinese, English, Hindustani, and Spanish in LRE 2007, as shown in Table <ref type="table" target="#tab_1">2</ref>. Dialect detection is generally believed to be more challenging <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b78">[79]</ref> than language detection, as the precise boundaries between dialects are not always clearly defined. In addition to the specified target languages (and dialects), several out-of-set languages (marked as ''O'') have also been included.</p><p>The emphasis of NIST LREs has been on conversational telephone speech (CTS), since most of the likely applications of the technology involve signals recorded from the public telephone system <ref type="bibr" target="#b86">[87]</ref>. In order to collect speech data of more languages in a cost-effective way, NIST has adopted broadcast narrowband speech (BNBS) lately in LRE 2009 and LRE 2011 <ref type="bibr" target="#b25">[26]</ref>. BNBS data are excerpts of call-in telephone speech embedded in broadcast and webcast. The call-in excerpts are used as we could expect them to cover as many speakers as possible. Broadcast entities like the Voice of America (VOA) broadcasts in more than 45 languages. Alternatively, the British Broadcast Company (BBC) also produces and distributes programs in a large number of languages. They have become an invaluable source of multilingual speech data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Detection Cost and Detection Error Tradeoff (DET) Curve</head><p>In the detection task as defined in NIST LREs, system performance is evaluated by presenting the system with a set of trials, each consisting of a test segment and a hypothesized target language. The system has to decide for each trial whether the target language was spoken in the given segment.</p><p>Let N T be the number of test segments and N be the number of target languages as defined earlier. By presenting each test segment against all target languages, there are N T number of trials for each target and the system under evaluation should produce N Â N T number of true or false decisions, one for each trial. The primary evaluation measure is the average detection cost, defined as follows:</p><formula xml:id="formula_41">C avg ¼ 1 N X N l¼1 C DET ðL l Þ<label>(30)</label></formula><p>where C DET ðL l Þ is the detection cost for the subset of N T trials for which the target language is L l</p><formula xml:id="formula_42">C DET ðL l Þ ¼ C miss P tar P miss ðL l Þ þ C fa ð1 À P tar Þ 1 N À 1 X m6 ¼l P fa ðL l ; L m Þ: (31)</formula><p>The miss probability (or false rejection rate) P miss accounts for the error when a test segment of language L l is rejected as being spoken in that language (i.e., classifying a target trial as a nontarget trial). The false alarm probability (or false acceptance rate) P fa ðL l ; L m Þ accounts for the error when a test segment of language L m is accepted as being spoken in language L l (i.e., classifying a nontarget trial as a target trial). The probabilities are computed as the number of errors divided by the total number of trials in each subset.</p><p>The application parameters fC miss ; C fa ; P tar g were set to the values f1; 1; 0:5g in past evaluations, where the costs for making both types of errors C miss and C fa are set to be equal, and P tar is the prior probability of a target foreseen for a particular application. In Section VI-C, we show how the P tar can be used to set the decision threshold. Putting together these components, the average detection cost can be expressed as</p><formula xml:id="formula_43">C avg ¼ C miss P tar 1 N X N l¼1 P miss ðL l Þ |fflfflfflfflfflfflfflfflfflffl{zfflfflfflfflfflfflfflfflfflffl} P miss ð DET Þ þ C fa ð1 À P tar Þ 1 N X N l¼1 1 N À 1 X m6 ¼l P fa ðL l ; L m Þ " # |fflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflffl{zfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflffl} P fa ð DET Þ :<label>(32)</label></formula><p>Notice that the miss probabilities P miss ðL l Þ are computed separately for each target language, and for each l the false alarm probabilities P fa ðL l ; L m Þ are computed for each target/nontarget language pairs. The direct implication of such a form of averaging computation is that the number of target trials per language is no longer of influence to the resulting C avg measure. This fact was first recognized in LRE 2005 and, since then, it has been used in subsequent LREs. For the case of an open-set test, one or more unseen languages are used as out-of-set languages L oos . This could be accounted for with an addition component P fa ðL l ; L oos Þ in the second term of the detection cost in <ref type="bibr" target="#b31">(32)</ref>. See <ref type="bibr" target="#b87">[88]</ref>, <ref type="bibr" target="#b88">[89]</ref>, and <ref type="bibr" target="#b101">[102]</ref> for details. The hard decisions used for the computation of the detection cost in <ref type="bibr" target="#b31">(32)</ref> are usually obtained based on whether the scores produced by a system exceed a chosen threshold ¼ DET . The intrinsic assumption with such a threshold-based decision is that the higher score supports more the target hypothesis, while the lower score supports the alternative. Another way to assess the system performance is to allow the threshold to change across a range of possible operating points. The resulting plot of P miss ðÞ against P fa ðÞ for different values of is referred to as the detection-error-tradeoff (DET) curve. An example of a DET plot is shown in Fig. <ref type="figure" target="#fig_7">9</ref>. Notice that the axes of the DET plot are warped according to the probit function, i.e., the inverse cumulative density function of a standard Gaussian distribution, making it different from the traditional receiver-operating-characteristic (ROC) curve <ref type="bibr" target="#b42">[43]</ref> in several ways. Notably, the curve becomes a straight line if the target and nontarget scores are normally distributed <ref type="bibr" target="#b83">[84]</ref>. The direct consequence is a less cluttered plot than a set of ROC curves when several DET curves are produced on the same graph.</p><p>The DET curve shows what happens as the decision threshold traverses across different operating points by which the error probabilities P miss ðÞ and P fa ðÞ change in opposite directions. That is, there is an inherent tradeoff between the two errors. Another point to note is that the error probabilities are first computed separately for each target language and averaged to produce P miss ðÞ, similarly for P fa ðÞ. This is not the same as computing directly the error probabilities using the pooled scores (see <ref type="bibr" target="#b135">[136]</ref> for a detailed analysis of why the latter should be avoided). Another visual advantage of DET is that we could show the actual decision operating point on the curve to see how ''far'' it is from the operating point producing the minimum cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance Assessment on NIST LRE Corpora</head><p>Since 1996, NIST has conducted six evaluations of the automatic language recognition technology, most recently in 2011. For the language detection tasks, the participants were required to provide two outputs for each trial, a hard decision and a soft score that represents the degree of support for the target hypothesis with respect to the alternative hypothesis. The hard decisions are used to compute the average detection cost C avg in <ref type="bibr" target="#b31">(32)</ref>, which serves as the primary evaluation measure. The soft scores are used to assess the system performance across a wide range of operating points leading to the DET curve.</p><p>Fig. <ref type="figure" target="#fig_7">9</ref> shows the DET plots for the detection task with test segments of three different durations, namely, 30, 10, and 3 s, as designated in the LRE 2011. The DET plots show the results for a four-subsystem fusion in Fig. <ref type="figure" target="#fig_8">10</ref>, which consists of three acoustic-phonetic subsystems and a phonotactic subsystem using the target-aware lattice rescoring (TALR) technique <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b127">[128]</ref>, <ref type="bibr" target="#b128">[129]</ref>. The three acousticphonetic subsystems are SVM with the Kullback-Leibler kernel (KL-SVM) <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, SVM with Bhattacharyya kernel (BHATT-SVM) <ref type="bibr" target="#b145">[145]</ref>, <ref type="bibr" target="#b146">[146]</ref>; and i-vector <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>. The results as shown are comparable with other top   The KL-SVM, BHATT-SVM, and TALR subsystems were taken from the IIR submission, while the i-vector subsystem was part of Brno276 submission in LRE 2011.</p><p>performing systems reported on the same LRE 2011 data set <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b120">[121]</ref>. 2 Notice that the closer the curve is to the origin, it indicates the better performance. It is apparent that language recognition performance becomes increasingly challenging as the length of the segments decreases. Also marked on the DET plots are the minimum cost and actual decision points indicated, respectively, with a circle and a square on each curve. We obtain the minimum point by sweeping through the DET curve to find the operating point which produces the lowest cost. Ideally, this should coincide with the actual decision point ¼ DET if the threshold was set properly.</p><p>The need to set a threshold is application dependent. Given the application parameters fC miss ; C fa ; P tar g, a threshold is set to minimize the detection cost as in <ref type="bibr" target="#b31">(32)</ref>. Traditionally, this was done by probing the recognizer using a development data set and setting the threshold at the operating point that gives the lowest cost. The drawback of this approach is that the same procedure has to be repeated when a new set of application parameters is given. A more systematic approach, which has been widely accepted in the community, is to postprocess the scores via the so-called application-independent calibration (see Section V-B). Let fs 0 l ; l ¼ 1; . . . ; Ng be the log-likelihood scores obtained from the calibration. For the detection task, we form the detection log-likelihood ratio <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref> </p><formula xml:id="formula_44">s DET ðL l Þ ¼ log exp s 0 l À Á 1 N À 1 X k6 ¼l exp s 0 k À Á :<label>(33)</label></formula><p>The detection threshold can then be set for any particular set of application parameters <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref> DET ¼ À log C miss P tar C fa ð1 À P tar Þ ! :</p><p>In essence, the detection cost is minimized indirectly via the optimization of the surrogate cost in (28) formulated on the multiclass regression model. Fig. <ref type="figure" target="#fig_8">10</ref> shows the schematic diagram of the foursubsystem fusion, the DET curve of which is depicted in Fig. <ref type="figure" target="#fig_7">9</ref>. A common strategy, which was shown to be useful in several reports, is to use a Gaussian back-end (see Section V-A) to summarize each individual subsystem, and to fuse (and calibrate) the resulting Gaussian back-end score together with those from other subsystems via a multiclass logistic regression model. Fig. <ref type="figure" target="#fig_10">11</ref> shows the minimum and actual detection cost for the individual subsystems. In general, the acoustic-phonetic subsystems (i.e., the KL-SVM, BHATT-SVM, and i-vector) are equally competitive as their phonotactic TALR counterparts. It is generally believed that the major factor for attaining classifier fusion with a better accuracy is the diversity in the subsystems <ref type="bibr" target="#b60">[61]</ref>. Such subsystems complement each other through the fusion process to produce the best performance. It is worth noting that the small gap between the actual and minimum costs in Fig. <ref type="figure" target="#fig_10">11</ref> shows the effectiveness and the need of score calibration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. DISCUSSION AND FUTURE DIRECTIONS</head><p>In this paper, we have discussed several key aspects of language recognition, covering language characterization, modeling techniques, and system development strategies. While we have seen tremendous progresses in the past decades, language recognition is still far from perfect. As far as language characterization is concerned, we have not been able to effectively venture beyond acoustic-phonetic and phonotactic knowledge, despite the fact that there exists strong evidence in human listening experiments that prosodic information, syllable structure, and morphology are useful knowledge sources. Nonetheless, recent advances reported in <ref type="bibr" target="#b32">[33]</ref> and <ref type="bibr" target="#b59">[60]</ref> have shown a revival of interest in prosodic features as well as their effective modeling techniques.</p><p>The study of phonetics and linguistics has a long history, which provides insights into the origin of language families and the lexical-phonological relationship between languages. However, we have yet to benefit from a wealth of such discoveries <ref type="bibr" target="#b0">[1]</ref>. Nonetheless, with a better understanding of the fundamental problems, we have seen several encouraging directions in which we could further the research. We briefly name a few in the following.</p><p>In phonotactic approaches, it is clear that good performance relies on effective phonotactic characterization of speech. As shown in Section II-A, perfect phone transcription will provide distinctive phonotactic statistics for languages. An obvious option is to improve the phone recognizers in the tokenization front-end with a better modeling technique, such as the left-context-right-context FeatureNet <ref type="bibr" target="#b89">[90]</ref> and deep neural networks <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b77">[78]</ref>. Nonetheless, we also realize that we will never be able to achieve perfect phone transcription. An alternative is to find a better way in extracting the phone n-gram statistics using existing phone recognizers, while continuing the efforts to improve the phone recognition accuracy. Several attempts have shown positive results along this direction. The lattice-based phone n-gram statistics <ref type="bibr" target="#b39">[40]</ref> make use of the posterior probability as the soft counts as opposed to phone n-gram counts. The target-oriented phone tokenizer <ref type="bibr" target="#b127">[128]</ref> and target-aware lattice rescoring <ref type="bibr" target="#b128">[129]</ref> suggest different ways to make use of the same phone recognizer to create diverse statistics from the same source. The cross-decoder (or cross-recognizer) phone co-occurrences n-gram <ref type="bibr" target="#b102">[103]</ref>, for the first time, explores phonotactic information across multiple phone sequences from parallel phone recognizers. These studies have shown that it is worth the effort to explore phonotactic information from different perspectives.</p><p>In acoustic approaches, the latent factor modeling technique, as used in JFA <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b140">[140]</ref> and i-vector <ref type="bibr" target="#b33">[34]</ref>, is unleashing its potential in language recognition <ref type="bibr" target="#b34">[35]</ref>. We envisage its use for hierarchical modeling of languages. In particular, a latent factor model could be used to capture the majority of useful variability among languages from a distinct language cluster, using separate subspaces for each language cluster. Discrimination between languages can then be done locally in each subspace corresponding to a language cluster. From a modeling perspective, the idea can be seen as an extension to the i-vector approach (see Section IV-D), where we used multiple subspaces, instead of one total variability space, each corresponding to a language cluster.</p><p>In system development, we have shown the usefulness of the Gaussian back-end for decision fusion and calibration. One subtle problem that a system designer may encounter is the duration-dependent nature of the Gaussian back-end. The score vectors produced by the front-end recognizer exhibit larger variation for the test segments with longer durations. This problem is traditionally dealt with by having a separate back-end for each of the nominal durations the system designers could foresee. In <ref type="bibr" target="#b90">[91]</ref>, a simple parametric function is used to scale the log-likelihood scores so that a single Gaussian backend could be used for all durations. A more elegant, but challenging, way could be to model directly the covariance matrix as a function of the test segment duration, so that the covariance of the Gaussian back-end could be updated on the fly according to the test duration.</p><p>The need for fast, efficient, accurate, and robust means of language recognition is of growing importance for commercial, forensic, and government applications. The </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Various levels of perceptual cues used for language recognition.</figDesc><graphic coords="3,77.84,76.53,170.28,148.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Polar histograms showing the phone distributions of (a)-(c) Czech and (d)-(f) Portuguese utterances for three different native speakers in each language. Utterances in the same language are of different contents.</figDesc><graphic coords="4,70.86,404.68,435.96,305.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. General scheme of acoustic-phonetic and phonotactic approaches to automatic language recognition, where N indicates the number of target languages.</figDesc><graphic coords="6,54.42,76.49,469.08,171.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Tokenization of speech at different levels with tokens of different sizes, ranging from a speech frame, a phone, to a lexical word.</figDesc><graphic coords="7,298.43,533.29,216.12,167.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Block diagram of a PRLM language recognition system.</figDesc><graphic coords="8,294.86,75.93,231.72,98.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Block diagram of a PPR-LM language recognition system.</figDesc><graphic coords="9,47.85,76.37,231.00,180.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. SDC feature extraction at frame time t, illustrated for the case of Z À d À P À k ¼ 7 À 1 À 3 À 7.</figDesc><graphic coords="11,47.85,75.91,231.65,169.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. DET curve showing the fusion system performance on the NIST LRE 2011 language detection task for test duration of 30, 10, and 3 s. Circles and squares indicate the minimum cost and actual decision points, respectively.</figDesc><graphic coords="18,55.84,473.59,222.36,217.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Generative Gaussian back-end (GB) followed by multiclass logistic regression for calibration and fusion.</figDesc><graphic coords="18,295.26,75.91,230.40,83.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>2</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Performance of individual acoustic-phonetic and phonotactic subsystems together with their fusion in terms of actual and minimum (average) detection cost (LRE 2011, 30 s).</figDesc><graphic coords="19,290.83,76.25,231.72,177.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Speaker and Language Characterization (SpLC), a special interest group of the International Speech Communication Association (ISCA), started the Odyssey Speaker and Language Recognition Workshop in 1994 as a forum to foster interactions among researchers and to promote language recognition research. Other scientific forums include the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) and the Annual Conference of the International Speech Communication Association (INTERSPEECH), where the latest findings are reported. h</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="17,62.81,115.59,200.28,624.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Perplexity Measured Between Seven Languages Selected Arbitrarily From the GlobalPhone Multilingual Database Based on Bigram and</figDesc><table><row><cell>Trigram Models</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Languages Included in the NIST Series of LRE. Target Languages Are Marked as ''X'' While Out-of-Set Languages Are Marked as ''O''</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Vol. 101, No. 5, May 2013 | Proceedings of the IEEE 1139</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Vol. 101, No. 5, May 2013 | Proceedings of the IEEE 1141</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>Vol. 101, No. 5, May 2013 | Proceedings of the IEEE 1143</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>Vol. 101, No. 5, May 2013 | Proceedings of the IEEE 1147</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4"><p>Vol. 101, No. 5, May 2013 | Proceedings of the IEEE 1149</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_5"><p>Vol. 101, No. 5, May 2013 | Proceedings of the IEEE 1151</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_6"><p>Vol. 101, No. 5, May 2013 | Proceedings of the IEEE 1153</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_7"><p>Vol. 101, No. 5, May 2013 | Proceedings of the IEEE 1159</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ABOUT THE AUTHORS</head><note type="other">Haizhou</note></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Language identification: A tutorial</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ambikairajah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sethu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Circuits Syst. Mag</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="82" to="108" />
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Introducing Phonetic Science</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ashby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Maidment</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Cambridge Univ. Press</publisher>
			<pubPlace>Cambridge, U.K.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language identification: The long and the short of the matter</title>
		<author>
			<persName><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Conf. North Amer. Chapter ACL Human Lang</title>
		<meeting>Annu. Conf. North Amer. Chapter ACL Human Lang<address><addrLine>Los Angeles, CA, USA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="229" to="237" />
		</imprint>
	</monogr>
	<note type="report_type">Technol.</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Analysis of phoneme-based features for language identification</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Berkling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barnard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="page" from="289" to="292" />
			<date type="published" when="1994">1994</date>
			<pubPlace>Adelaide, Australia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language identification of six languages based on a common set of broad phonemes</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Berkling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barnard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Spoken Lang. Process</title>
		<meeting>Int. Conf. Spoken Lang. ess<address><addrLine>Yokohama, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="1891" to="1894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discriminative phonotactics for dialect recognition using context-dependent phone classifiers</title>
		<author>
			<persName><forename type="first">F</forename><surname>Biadsy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Soltauy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Manguy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Navratily</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hirschberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Odyssey: Speaker and Language Recognition Workshop</title>
		<meeting>IEEE Odyssey: Speaker and Language Recognition Workshop<address><addrLine>Brno, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="263" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic language identification with perceptually guided training and recurrent neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Levkowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Spoken Lang. Process</title>
		<meeting>Int. Conf. Spoken Lang. ess<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="289" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On calibration of language recognition scores</title>
		<author>
			<persName><forename type="first">N</forename><surname>Brummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Leeuwen</surname></persName>
		</author>
		<idno type="DOI">10.1109/ODYSSEY.2006.248106</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Odyssey: Speaker Lang. Recognit. Workshop</title>
		<meeting>IEEE Odyssey: Speaker Lang. Recognit. Workshop<address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Application-independent evaluation of speaker detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Brummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Preez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Speech Lang</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="230" to="275" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Focal Multi-ClassVTools for Evaluation, Calibration and Fusion of, and Decision-Making with</title>
		<author>
			<persName><forename type="first">N</forename><surname>Brummer</surname></persName>
		</author>
		<ptr target="http://sites.google.com/site/nikobrummer/" />
	</analytic>
	<monogr>
		<title level="m">Multi-Class Statistical Pattern Recognition Scores</title>
		<imprint>
			<date type="published" when="2007-06">Jun. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BUT-AGNITIO system description for NIST language recognition evaluation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Brummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hubeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karafiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Matejka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Plchot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Strasheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. NIST Lang. Recognit. Eval. Workshop</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<pubPlace>Baltimore, MD, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Measuring, refining and calibrating speaker and language information extracted from speech</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bru ¨mmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ph.D. dissertation, Dept. Electr. Electron. Eng., Stellenbosch Univ</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<pubPlace>Stellenbosch, South Africa</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Description and analysis of the Brno276 system for LRE2011</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bru ¨mmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cumani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karafia ´t</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mate ˇjka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pes ˇa ´n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Plchot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Soufifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Villiers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Odyssey: Speaker Lang. Recognit. Workshop</title>
		<meeting>Odyssey: Speaker Lang. Recognit. Workshop<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Discriminative training techniques for acoustic language identification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Matejka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cernocky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2006">2006</date>
			<pubPlace>Toulouse, France</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">High-level speaker verification with support vector machines</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Leek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="page" from="73" to="76" />
			<date type="published" when="2004">2004</date>
			<pubPlace>Montreal, QC, Canada</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Support vector machines for speaker and language recognition</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Torres-Carrasquillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Speech Lang</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="210" to="229" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Advanced language recognition using cepstra and phonotactics: MITLL system performance on the NIST 2005 language recognition evaluation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gleason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Navratil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torres-Carrasquillo</surname></persName>
		</author>
		<idno type="DOI">10.1109/ODYSSEY.2006.248097</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Odyssey: Speaker Lang. Recognit. Workshop</title>
		<meeting>IEEE Odyssey: Speaker Lang. Recognit. Workshop<address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Support vector machines using GMM supervectors for speaker verification</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Sturim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="308" to="310" />
			<date type="published" when="2006-05">May 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A covariance Kernel for SVM language recognition</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="page" from="4141" to="4144" />
			<date type="published" when="2008">2008</date>
			<pubPlace>Las Vegas, NV, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A comparison of subspace feature-domain methods for language recognition</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Sturim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torres-Carrasquillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech Conf</title>
		<meeting>Interspeech Conf<address><addrLine>Brisbane, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="309" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Compensation of nuisance factors for speaker and language recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Castaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Colibro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dalmasso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laface</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1969" to="1978" />
			<date type="published" when="2007-09">Sep. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">N-gram-based text categorization</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Cavnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Trenkle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Annu. Symp. Document Anal. Inf. Retrieval</title>
		<meeting>3rd Annu. Symp. Document Anal. Inf. Retrieval<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="161" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Retrieval and browsing of spoken content</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hazen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saraclar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="39" to="49" />
			<date type="published" when="2008-05">May 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Vector-based natural language call routing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chu-Carrol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Carpenter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="361" to="388" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The mixer corpus of multilingual, multichannel speaker recognition data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nakasone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Lang. Resources Eval</title>
		<meeting>Int. Conf. Lang. Resources Eval<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="24" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The broadcast narrow band speech corpus: A new resource type for large scale language recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Brandschain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Caruso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Greenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech Conf</title>
		<meeting>Interspeech Conf<address><addrLine>Brighton, U.K</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2867" to="2870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Development of an automatic identification system of spoken languages: Phase I,&apos;&apos; in</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cimarusti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="page" from="1661" to="1663" />
			<date type="published" when="1982">1982</date>
			<pubPlace>Paris, France</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automatic language identification: Performance vs. complexity</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Combrinck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Botha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th Annu. South Africa Workshop Pattern Recognit</title>
		<meeting>8th Annu. South Africa Workshop Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
	<note>Grahams Town, South Africa</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The World&apos;s Major Languages</title>
		<author>
			<persName><forename type="first">B</forename><surname>Comrie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Oxford Univ. Press</publisher>
			<pubPlace>Oxford, U.K.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The Cambridge Factfinder</title>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Cambridge Univ. Press</publisher>
			<pubPlace>Cambridge, U.K.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Analysis of large-scale SVM training algorithms for language and speaker recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cumani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laface</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1585" to="1596" />
			<date type="published" when="2012-07">Jul. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences</title>
		<author>
			<persName><forename type="first">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mermelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="357" to="366" />
			<date type="published" when="1980-08">Aug. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Modeling prosodic features with joint factor analysis for speaker verification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2095" to="2103" />
			<date type="published" when="2007-09">Sep. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Front-end factor analysis for speaker verification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ouellet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="788" to="798" />
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Language recognition via i-vectors and dimensionality reduction</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torres-Carrasquillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dehak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech Conf</title>
		<meeting>Interspeech Conf<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="857" to="860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dumpster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Statistical identification of language</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dunning</surname></persName>
		</author>
		<idno>MCCS-94-273</idno>
	</analytic>
	<monogr>
		<title level="j">Comput. Res. Lab (CRL)</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<pubPlace>New Mexico State Univ., Las Cruces, NM, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Language identification using noisy speech</title>
		<author>
			<persName><forename type="first">J</forename><surname>Foil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="page" from="861" to="864" />
			<date type="published" when="1986">1986</date>
			<pubPlace>Tokyo, Japan</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Maximum a posterior estimation for multivariate Gaussian mixture observations of Markov chains</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Gauvain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Process</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="291" to="298" />
			<date type="published" when="1994-04">Apr. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Language recognition using phone lattices</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Gauvain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Messaoudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Spoken Lang. Process</title>
		<meeting>Int. Conf. Spoken Lang. ess<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1283" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Language identification in the limit</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Gold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Control</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="447" to="474" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improved automatic language identification in noisy speech</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Wohlford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="528" to="531" />
			<date type="published" when="1989">1989</date>
			<pubPlace>Glasgow, Scotland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">The Elements of Statistical Learning: Data Mining, Inference, and Prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Automatic language identification using a segment-based approach</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Hazen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">W</forename><surname>Zue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eurospeech Conf</title>
		<meeting>Eurospeech Conf<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="1303" to="1306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Recent improvements in an approach to segment-based automatic language identification</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Hazen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">W</forename><surname>Zue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="page" from="1883" to="1886" />
			<date type="published" when="1994">1994</date>
			<pubPlace>Adelaide, Australia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Segment-based automatic language identification</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Hazen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">W</forename><surname>Zue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2323" to="2331" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">RASTA processing of speech</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Process</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="578" to="589" />
			<date type="published" when="1994-10">Oct. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Spoken language identification using large vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hieronymus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kadambe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Spoken Lang. Process</title>
		<meeting>Int. Conf. Spoken Lang. ess<address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="1780" to="1783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Products of experts</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th Int. Conf. Artif. Neural Netw</title>
		<meeting>9th Int. Conf. Artif. Neural Netw<address><addrLine>Edinburgh, U.K.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012-11">Nov. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Toward automatic identification of the language of an utterance. I. Preliminary methodological considerations</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>House</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Neuburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="708" to="713" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Hon</surname></persName>
		</author>
		<title level="m">Spoken Language Processing: A Guide to Theory, Algorithm, and System Development</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Hierarchical mixtures of experts and the EM algorithms</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="181" to="214" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Minimum classification error rate methods for speech recognition</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Process</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="257" to="265" />
			<date type="published" when="1997-05">May 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
		<title level="m">Speech and Language Processing</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Experiments in speaker verification using factor analysis likelihood ratios</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Odyssey: Speaker Lang. Recognit. Workshop</title>
		<meeting>Odyssey: Speaker Lang. Recognit. Workshop<address><addrLine>Toledo, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="219" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Joint factor analysis versus eigenchannels in speaker recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ouellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1435" to="1447" />
			<date type="published" when="2007-05">May 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Language characteristics,&apos;&apos; in Multilingual Speech Processing</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kirchhoff</surname></persName>
		</author>
		<editor>T. Schultz and K. Kirchhoff</editor>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Elsevier</publisher>
			<pubPlace>Amsterdam, The Netherlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Combining classifiers: A theoretical framework</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Anal. Appl</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="27" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">iVector fusion of prosodic and cepstral features for speaker verification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kockmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech Conf</title>
		<meeting>Interspeech Conf<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="265" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A theoretical study on six classifier fusion strategies</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Kuncheva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="281" to="286" />
			<date type="published" when="2002-02">Feb. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Language identification using phone-based acoustic likelihoods</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Gauvain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="293" to="296" />
			<date type="published" when="1994">1994</date>
			<pubPlace>Adelaide, Australia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">The OGI 22 language telephone speech corpus</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Oshika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Noel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eurospeech Conf</title>
		<meeting>Eurospeech Conf<address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="817" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Principles of spoken language recognition</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Springer Handbook of Speech Processing and Speech Communication</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Benesty</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Sondhi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Spoken language recognition using support vector machines with generative front-end</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="page" from="4153" to="4156" />
			<date type="published" when="2008">2008</date>
			<pubPlace>Las Vegas, NV, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Using discrete probabilities with Bhattacharyya measure for SVM-based speaker verification</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="861" to="870" />
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Spoken language recognition in the latent topic simplex</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hautama ¨ki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Larcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech Conf</title>
		<meeting>Interspeech Conf<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2933" to="2936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Speaker normalization using efficient frequency warping procedures</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="353" to="356" />
			<date type="published" when="1996">1996</date>
			<pubPlace>Atlanta, GA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Automatic language identification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Doddington</surname></persName>
		</author>
		<idno>RADC-TR-74-200</idno>
	</analytic>
	<monogr>
		<title level="j">Air Force Rome Air Develop. Cntr</title>
		<imprint>
			<date type="published" when="1974-08">Aug. 1974</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Automatic language identification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Doddington</surname></persName>
		</author>
		<idno>RADC-TR-75-264</idno>
	</analytic>
	<monogr>
		<title level="j">Air Force Rome Air Develop. Cntr</title>
		<imprint>
			<date type="published" when="1975-10">Oct. 1975</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Ethnologue: Languages of the World</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>SIL International</publisher>
			<biblScope unit="volume">16</biblScope>
			<pubPlace>Dallas, TX, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">A phonotactic language model for spoken language identification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Assoc. Comput. Linguist</title>
		<imprint>
			<biblScope unit="page" from="515" to="522" />
			<date type="published" when="2005">2005</date>
			<pubPlace>Ann Arbor, MI, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">A vector space modeling approach to spoken language identification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="271" to="284" />
			<date type="published" when="2007-01">Jan. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Institute for Infocomm Research system description for the language recognition evaluation 2007 submission</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. NIST Lang. Recognit. Eval. Workshop</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<pubPlace>Orlando, FL, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Vector-based spoken language classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer Handbook of Speech Processing and Speech Communication</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Benesty</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Sondhi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</editor>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">TechWare: Speaker and spoken language recognition resources</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="139" to="142" />
			<date type="published" when="2010-11">Nov. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Statistical models for automatic language identification</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="page" from="884" to="887" />
			<date type="published" when="1980">1980</date>
			<pubPlace>Denver, CO, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Machine learning paradigms for speech recognition: An overview</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Process</title>
		<imprint/>
	</monogr>
	<note>accepted for publication</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Using local and global phonotactic features in Chinese dialect identification</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="page" from="577" to="580" />
			<date type="published" when="2005">2005</date>
			<pubPlace>Philadelphia, PA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Improved speaker verification through probabilistic subspace adaptation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eurospeech Conf</title>
		<meeting>Eurospeech Conf<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="2021" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Multilingual speech recognition with language identification</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Spoken Lang. Process</title>
		<meeting>Int. Conf. Spoken Lang. ess<address><addrLine>Denver, CO, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="505" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Spoken language recognition with ensemble classifiers</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2053" to="2062" />
			<date type="published" when="2007-09">Sep. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schu ¨tze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Cambridge Univ. Press</publisher>
			<pubPlace>Cambridge, U.K.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">The DET curve in assessment of detection task performance</title>
		<author>
			<persName><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kamm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ordowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Przybocki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eurospeech Conf</title>
		<meeting>Eurospeech Conf<address><addrLine>Rhodes, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1895" to="1898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">NIST 2003 language recognition evaluation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Przybocki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eurospeech Conf</title>
		<meeting>Eurospeech Conf<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1341" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">The current state of language recognition: NIST 2005 evaluation results</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.1109/ODYSSEY.2006.248104</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Odyssey: Speaker Lang. Recognit. Workshop</title>
		<meeting>IEEE Odyssey: Speaker Lang. Recognit. Workshop<address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">NIST speech processing evaluations: LVCSR, speaker recognition, language recognition,&apos;&apos; in Proc</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Garofolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Workshop Signal Process. Appl. Public Security Forensics</title>
		<imprint>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2007">2007</date>
			<pubPlace>Washington, DC, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">NIST 2007 language recognition evaluation,&apos;&apos; presented at the Odyssey: Speaker Lang</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Recognit. Workshop</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<pubPlace>Stellenbosch, South Africa</pubPlace>
		</imprint>
	</monogr>
	<note>paper 016</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">The 2009 NIST language recognition evaluation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Greenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Odyssey: Speaker Lang. Recognit. Workshop</title>
		<meeting>Odyssey: Speaker Lang. Recognit. Workshop<address><addrLine>Brno, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="165" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Phonotactic language identification using high quality phoneme recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Matejka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cernocky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chytil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech Conf</title>
		<meeting>Interspeech Conf<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="2237" to="2240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Beyond frame independent: Parametric modeling of time duration in speaker and language recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mccree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech Conf</title>
		<meeting>Interspeech Conf<address><addrLine>Brisbane, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="767" to="770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Automatic language identification using large vocabulary continuous speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mendoza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="785" to="788" />
			<date type="published" when="1996">1996</date>
			<pubPlace>Atlanta, GA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Algorithms for maximum-likelihood logistic regression</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Carnegie Mellon Univ</title>
		<imprint>
			<biblScope unit="volume">738</biblScope>
			<date type="published" when="2001">2001</date>
			<pubPlace>Pittsburgh, PA, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">The OGI multi-language telephone speech corpus</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Muthusamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Oshika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Spoken Lang. Process., BanffABCanada</title>
		<meeting>Int. Conf. Spoken Lang. ess., BanffABCanada</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="895" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">A comparison of approaches to automatic language identification using telephone speech</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Muthusamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Berkling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Arai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barnard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eurospeech Conf</title>
		<meeting>Eurospeech Conf<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="1307" to="1310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Perceptual benchmarks for automatic language identification</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Muthusamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Cole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="333" to="336" />
			<date type="published" when="1994">1994</date>
			<pubPlace>Adelaide, Australia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Reviewing automatic language identification</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Muthusamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Cole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="33" to="41" />
			<date type="published" when="1994-10">Oct. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Speaker-independent, text-independent language identification by HMM</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nakagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ueda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Seino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Spoken Lang. Process</title>
		<meeting>Int. Conf. Spoken Lang. ess<address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="1011" to="1014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Spoken language recognitionVA step toward multilinguality in speech processing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Navratil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Process</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="678" to="685" />
			<date type="published" when="2001-09">Sep. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Prosodic attribute model for spoken language identification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="page" from="5022" to="5025" />
			<date type="published" when="2010">2010</date>
			<pubPlace>Dallas, TX, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Towards long-range prosodic attribute modeling for language recognition</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hautama ¨ki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech Conf., Chiba, Japan</title>
		<meeting>Interspeech Conf., Chiba, Japan</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1792" to="1795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<ptr target="http://nist.gov/itl/iad/mig/lre.cfm" />
		<title level="m">NIST Language Recognition Evaluations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Improved modeling of cross-decoder phone co-occurrences in SVM-Based phonotactic language recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Penagarikano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Rodriguez-Fuentes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bordel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2348" to="2363" />
			<date type="published" when="2011-11">Nov. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Applying logistic regression to fusion of the NIST&apos;99 1-speaker submissions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pigeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Druyts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Verlinde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Process</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="237" to="248" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Discriminative training of GMM for language identification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISCA IEEE Workshop Spontaneous Speech Process. Recognit</title>
		<meeting>ISCA IEEE Workshop Spontaneous Speech ess. Recognit<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="67" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Quatieri</surname></persName>
		</author>
		<title level="m">Discrete-Time Speech Signal Processing: Principles and Practice</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">A tutorial on hidden Markov models and selected publication in speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="257" to="286" />
			<date type="published" when="1989-02">Feb. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Language identification with suprasegmental cues: A study based on speech re-synthesis</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ramus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="512" to="521" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Correlates of linguistic rhythm in the speech signal</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ramus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nespor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="265" to="292" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Robust text-independent speaker identification using Gaussian mixture speaker models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Process</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="72" to="83" />
			<date type="published" when="1995-01">Jan. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Speaker verification using adapted Gaussian mixture models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Quatieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Dunn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Process</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="19" to="41" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Language recognition with discriminative keyword selection</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="page" from="4145" to="4148" />
			<date type="published" when="2008">2008</date>
			<pubPlace>Las Vegas, NV, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">The SMART Retrieval System</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Term-weighting approaches in automatic text retrieval</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Process. Manage</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="513" to="523" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">LVCSR-based language identification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Rogina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="781" to="784" />
			<date type="published" when="1996">1996</date>
			<pubPlace>Atlanta, GA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Language independent and language adaptive</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="31" to="51" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Globalphone: A multilingual text and speech database developed at Karlsruhe University</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech Conf</title>
		<meeting>Interspeech Conf<address><addrLine>Denver, CO, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="345" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Experiments with lattice-based PPRLM language identification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gleason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Singer</surname></persName>
		</author>
		<idno type="DOI">10.1109/ODYSSEY.2006.248100</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Odyssey: Speaker Lang. Recognit. Workshop</title>
		<meeting>IEEE Odyssey: Speaker Lang. Recognit. Workshop<address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Improved GMM-Based language recognition using constrained MLLR transforms</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Acoust. Speech Signal Process</title>
		<meeting>Int. Conf. Acoust. Speech Signal ess<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="4149" to="4152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">On acoustic diversification front-end for spoken language identification</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1029" to="1037" />
			<date type="published" when="2008-07">Jul. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">The MITLL NIST LRE2011 language recognition system</title>
		<author>
			<persName><forename type="first">E</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torres-Carrasquillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sturim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Odyssey: Speaker Lang. Recognit. Workshop</title>
		<meeting>Odyssey: Speaker Lang. Recognit. Workshop<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="209" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Exploring universal attribute characterization of spoken languages for spoken language recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Siniscalchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Svendsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech Conf</title>
		<meeting>Interspeech Conf<address><addrLine>Brighton, U.K</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="168" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Exploiting context-dependency and acoustic resolution of universal speech attribute models in spoken language recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Siniscalchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Svendsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech Conf., Chiba, Japan</title>
		<meeting>Interspeech Conf., Chiba, Japan</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2718" to="2721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Advances in channel compensation for SVM speaker recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Solomonoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Boardman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="page" from="629" to="632" />
			<date type="published" when="2005">2005</date>
			<pubPlace>Philadelphia, PA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Automatic language recognition using acoustic features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="813" to="816" />
			<date type="published" when="1991">1991</date>
			<pubPlace>Toronto, ON, Canada</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Mixtures of probabilistic principal component analysis</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Tipping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="443" to="482" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Integrating acoustic, prosodic and phonotactic features for spoken language identification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-S</forename><surname>Chng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="page" from="205" to="208" />
			<date type="published" when="2006">2006</date>
			<pubPlace>Toulouse, France</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">A target-oriented phonotactic front-end for spoken language recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1335" to="1347" />
			<date type="published" when="2009-09">Sep. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Target-aware lattice rescoring for dialect recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech Conf</title>
		<meeting>Interspeech Conf<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="733" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Language identification using Gaussian mixture model tokenization</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Torres-Carrasquillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Deller</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="page" from="757" to="760" />
			<date type="published" when="2002">2002</date>
			<pubPlace>Orlando, FL, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Approaches to language identification using Gaussian mixture models and shifted delta cepstral features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Torres-Carrasquillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deller</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Spoken Lang. Process</title>
		<meeting>Int. Conf. Spoken Lang. ess<address><addrLine>Denver, CO, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="89" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Torres-Carrasquillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gleason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sturim</surname></persName>
		</author>
		<title level="m">The MITLL NIST LRE 2007 language recognition system,&apos;&apos; in Proc. Interspeech Conf</title>
		<meeting><address><addrLine>Brisbane, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="719" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Automatic language identification using sub-words models</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Carey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Paris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="page" from="301" to="304" />
			<date type="published" when="1994">1994</date>
			<pubPlace>Adelaide, Australia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Channel-dependent GMM and multi-class logistic regression models for language recognition</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Brummer</surname></persName>
		</author>
		<idno type="DOI">10.1109/ODYSSEY.2006.248094</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Odyssey: Speaker Lang. Recognit. Workshop</title>
		<meeting>IEEE Odyssey: Speaker Lang. Recognit. Workshop<address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">An introduction to application independent evaluation of speaker recognition systems</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bru ¨mmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speaker Classification</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Mu ¨ller</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4343</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">An open-set detection evaluation methodology applied to language and emotion recognition</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Truong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech Conf</title>
		<meeting>Interspeech Conf<address><addrLine>Antwerp, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="338" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<monogr>
		<title level="m" type="main">A human benchmark for the NIST language recognition evaluation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Orr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Speaker Lang</publisher>
		</imprint>
	</monogr>
	<note>Recognit</note>
</biblStruct>

<biblStruct xml:id="b137">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Stellenbosch</forename><surname>Workshop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">South</forename><surname>Africa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>paper 012</note>
</biblStruct>

<biblStruct xml:id="b138">
	<monogr>
		<title level="m" type="main">The Nature of Statistical Learning Theory</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Channel factors compensation in model and feature domain for speaker recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Vair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Colibro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Castaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dalmasso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laface</surname></persName>
		</author>
		<idno type="DOI">10.1109/ODYSSEY.2006.248117</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Odyssey: Speaker Lang. Recognit. Workshop</title>
		<meeting>IEEE Odyssey: Speaker Lang. Recognit. Workshop<address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Explicit modelling of session variability for speaker verification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vogt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Speech Lang</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="17" to="38" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Multilinguality in speech and spoken language systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Geutner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Tomokiyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Woszczyna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1181" to="1190" />
			<date type="published" when="2000-08">Aug. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Large scale discriminative training of hidden Markov models for speech recognition</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Woodland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Speech Lang</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="47" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">An approach to automatic language identification based on language-dependent phone recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barnard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="3511" to="3514" />
			<date type="published" when="1995">1995</date>
			<pubPlace>Detroit, MI, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Development of an approach to language identification based on phone recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Speech Lang</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="37" to="54" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">GMM-SVM kernel with a Bhattacharyya-based distance for speaker recognition</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1300" to="1312" />
			<date type="published" when="2010-08">Aug. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">A GMM-supervector approach to language recognition with adaptive relevance factor</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EUSIPCO</title>
		<meeting>EUSIPCO<address><addrLine>Aalborg, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1993" to="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Cortical competition during language discrimination</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="624" to="633" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Soft margin estimation of Gaussian mixture model parameters for spoken language recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="page" from="4990" to="4993" />
			<date type="published" when="2010">2010</date>
			<pubPlace>Dallas, TX, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Automatic language identification using Gaussian mixture and hidden Markov models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Zissman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="399" to="402" />
			<date type="published" when="1993">1993</date>
			<pubPlace>Minneapolis, MN, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Comparison of four approaches to automatic language identification of telephone speech</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Zissman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Process</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="44" />
			<date type="published" when="1996-01">Jan. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Predicting, diagnosing and improving automatic language identification performance</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Zissman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eurospeech Conf</title>
		<meeting>Eurospeech Conf<address><addrLine>Rhodes, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="51" to="54" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
