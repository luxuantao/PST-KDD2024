<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Application of Discriminative Feature Extraction to Filter-Bank-Based Speech Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><roleName>Member, IEEE</roleName><forename type="first">Alain</forename><surname>Biem</surname></persName>
							<email>biem@watson.ibm.com</email>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Shigeru</forename><surname>Katagiri</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Erik</forename><surname>Mcdermott</surname></persName>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Biing-Hwang</forename><surname>Juang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">IBM Thomas J. Watson Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">NTT Communications Science Lab-oratories</orgName>
								<address>
									<postCode>619-02</postCode>
									<settlement>Kyoto</settlement>
									<country>Japan. B.-H</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Juang is with Bell Laboratories Lucent Technologies</orgName>
								<address>
									<postCode>07974</postCode>
									<settlement>Murray Hill</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Application of Discriminative Feature Extraction to Filter-Bank-Based Speech Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">68C4B7AAD2A1CC03E665B495980B3E30</idno>
					<note type="submission">received May 19, 1997; revised November 15, 1999.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Feature extraction</term>
					<term>filter-bank</term>
					<term>generalized probabilistic descent</term>
					<term>minimun classification error</term>
					<term>pattern recognition</term>
					<term>speech recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A pattern recognizer is usually a modular system which consists of a feature extractor module and a classifier module. Traditionally, these two modules have been designed separately, which may not result in an optimal recognition accuracy. To alleviate this fundamental problem, the authors have developed a design method, named Discriminative Feature Extraction (DFE), that enables one to design the overall recognizer, i.e., both the feature extractor and the classifier, in a manner consistent with the objective of minimizing recognition errors. This paper investigates the application of this method to designing a speech recognizer that consists of a filter-bank feature extractor and a multi-prototype distance classifier. Carefully investigated experiments demonstrate that DFE achieves the design of a better recognizer and provides an innovative recognition-oriented analysis of the filter-bank, as an alternative to conventional analysis based on psychoacoustic expertise or heuristics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>P ATTERN recognition is fundamentally composed of feature extraction and classification, and most recognizers (pattern recognition systems) are modular systems, consisting of a front-end feature extractor (feature extraction module) and a back-end classifier (classification module) <ref type="bibr" target="#b0">[1]</ref>. The pattern to be recognized is first converted to some features, believed to carry the class identity of the pattern, and then the set of features is classified as one of the possible classes.</p><p>To achieve high recognition accuracy, the feature extractor is required to discover salient characteristics suited for classification and the classifier is required to set class boundaries accurately in the feature space. In most conventional cases, the feature extractor is selected based on scientific and/or heuristic knowledge about patterns to recognize; the classifier is statistically designed according to the fundamental Bayes decision theory. In particular, the latter statistical design of classifier has made remarkable progress in recent years due to elegant mathematical formalisms, such as maximum likelihood estimation (or minimum distortion estimation), the discriminant function approach using the minimum squared error (MSE) criterion, and the recent, minimum classification error (MCE) paradigm <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. However, in spite of this progress, the goal of optimal recognizer design has not been fully achieved. The conventional approaches to designing recognition systems have a serious defect. That is, the empirical selection of feature extractors is not guaranteed to be mathematically consistent with the design objective of increasing recognition accuracy. Features selected with no link to the design of the back-end classification stage (i.e., mismatch between distribution of the selected features and the assumed distribution form in the Bayes classifier design), or the final design objective, may make classification more difficult. Overcoming this defect is clearly an important target for further improvement of pattern recognizer design.</p><p>We have proposed a new solution, namely discriminative feature extraction (DFE), to this design consistency problem <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref>. DFE is a general design method for systematically training the overall recognizer in a manner consistent with the minimization of recognition errors, according to the design concept of the recent discriminative training method, i.e., the minimum classification error/generalized probabilistic descent (MCE/GPD) method. The method was first applied to the problem of designing speech recognizers using a lifter, which is an important speech feature extractor mechanism <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>. The task was to classify static (fixed-dimensional) vowel fragments, each represented by an FFT-based high-dimensional cepstrum vector. The task was relatively simple, but it demonstrated that DFE not only contributed to the improvement of the recognition accuracy but also provided reasonable, task-oriented lifter shapes. The success of this experiment marked a remarkable departure from conventional lifter design that was empirically investigated, and thus, not necessarily the best for recognition. Hence, the DFE framework has been applied to various speech recognition tasks <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b8">[9]</ref>.</p><p>For speech recognition, there are still various possibilities for selecting recognizer structures as well as recognition tasks. The preliminary and limited experiments in <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref> are insufficient for a comprehensive evaluation of the general applicability of the method. Therefore, we aim in this paper to elaborate experimental evaluation of DFE, applying it to the design of another recognizer structure. The recognizer used here consists of an important alternative to the liftering feature extractor, namely a filter-bank feature extractor, and a standard distance classifier using multiple prototypes <ref type="bibr" target="#b9">[10]</ref>. Evaluation is done in two experimental tasks: one recognizing static vowel fragments, each being a filter-bank output vector, and one recognizing dynamic (variable-durational) words, each being a sequence of filter-bank output vectors, within the ATR directory assistance task framework <ref type="bibr" target="#b10">[11]</ref>. The first, relatively simple task enables us to carefully investigate the nature of DFE training, e.g., plausibility of designed filter-bank shapes; the second task enables us to study utility of DFE in a more realistic environment using dynamic and distorted telephone speech utterances. In both tasks, DFE-designed recognizers are compared with conventionallydesigned baseline systems, in which the filter-bank is defined based on psychoacoustic findings and the classifier is trained by using -means clustering or MCE/GPD training.</p><p>The use of the prototype-based distance classifier would be considered rather traditional nowadays. However, the classifier is mathematically equivalent, conditioned by the selection of distance measure, to a simplified version of a classifier based on a hidden Markov model (HMM) (see <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b11">[12]</ref> for details).</p><p>This paper is organized as follows. Section II describes the filter-bank-based speech recognizer and also illustrates DFE with its implementation for the selected recognizer structure. Section III presents experimental results for the task of recognizing Japanese five-class, static vowel fragments. Section IV shows experimental results for the directory assistance task. The paper is concluded in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. DFE IMPLEMENTATION FOR FILTER-BANK-BASED SPEECH RECOGNIZER</head><p>A. System Structure 1) Overview: For generality, we consider in this section the task of recognizing dynamic speech patterns of classes such as phonemes and words;</p><p>. The speech recognizer is a modular system, consisting of a front-end filterbank-based feature extractor and a back-end prototype-based distance classifier. Fig. <ref type="figure" target="#fig_0">1</ref> illustrates this modular recognizer.</p><p>In principle, a filter-bank is composed of finite-or infinite-response filters, and one can apply DFE within this framework. However, this requires additional attention to the stability of DFE-adjusted filters. Thus, for simplicity, we emulate filtering by using FFT computation after many recent studies <ref type="bibr" target="#b12">[13]</ref>. <ref type="foot" target="#foot_0">1</ref>An input speech signal is first converted to a sequence of FFT-based power spectrum vectors, where is an FFT-based, -dimensional power spectrum vector at time (time window position) and is the length of ; where is the th element of (the element of at the th frequency index) and is the transpose. Assuming that a filtering function is where is a set of trainable control parameters of the filter-bank, the feature extractor converts to its corresponding filter-bank output pattern in a vector-by-vector mode; where is the (usually )-dimensional filter-bank output vector that corresponds to , i.e., for all possible 's; . The classifier is based on the one in <ref type="bibr" target="#b9">[10]</ref>. It consists of an acoustic model and a language model. The acoustic model uses a state transition structure, similar to an HMM; the language model is a finite state machine structure. The decision rule in this stage is if <ref type="bibr" target="#b0">(1)</ref> where classification operation; trainable classifier parameter set, i.e., a set of prototype vectors; discriminate function, defined in terms of , which indicates the degree to which belongs to . Note here that a smaller value of the discriminate function indicates a higher possibility that an input to the classifier belongs to the corresponding class.</p><p>2) Filter Bank-Based Feature Extractor: In the framework using FFT, filtering is done by weighting of the input power spectrum vector. For a power spectrum vector , the basic formulation of filtering is given as follows:</p><p>(2) where , which is the dimension of the filter-bank output vector, corresponds to the number of filters of the filter-bank; is the -dimensional weight vector of the th channel filter, i.e., where is the weight coefficient of the th channel filter at the th frequency index. This conversion is repeated in the vector-by-vector mode, i.e., for every , thus transforming into . Usually, to make the output vectors smoother, the filters are placed so that they overlap each other, and the set of weights characterizing the filter-bank, i.e., , are determined by using some (small number of) continuous functions. However, in principle, each element of the vector, , itself can be individually treated as a trainable parameter. This latter approach seems attractive because it permits the design of a more general filter-bank. However, this flexible setting also increases sensitivity to training data because of the increase in the number of parameters, which may result in a less robust design for unknown input patterns. The selection of trainable parameters for filter-bank design will be discussed in Section II-B2.</p><p>Most speech recognition systems usually perform the discrete cosine transform (DCT) on the output of the filter-bank to generate cepstral parameters. This widely used transformation produces features that are less correlated than log energies and, subsequently, permits a reduction in the size of feature vector while enabling a higher frequency resolution by appropriate choice of the number of filters <ref type="bibr" target="#b12">[13]</ref>. Here, the DCT transformation can be integrated into the filter-bank function , in which case is a vector of cepstral parameters.</p><p>3) Prototype-Based Distance Classifier: In this classifier, each class (phoneme/word/phrase) is modeled as a string of sub-phonetic states, each being assigned prototypes in the filter-bank output vector space. Let us assume that the model of class has states and the th-state of this model has prototypes;</p><p>, where is the th prototype vector of the th state of model ; is of dimension . Fig. <ref type="figure" target="#fig_1">2</ref> illustrates a model with three states and four prototypes per state.</p><p>According to <ref type="bibr" target="#b9">[10]</ref>, the discriminate function is defined in the following way. First, at every time index , we measure a local distance between a filter-bank output vector and a prototype as follows:</p><p>(3) This distance is simply the squared Euclidean norm of the two corresponding vectors in the filter-bank vector space. The local distance can take the general form of a Mahalanobis distance. However, for reducing the complexity of the system, the local distance measure has been restricted to an Euclidean distance. Next, we embed the local distance in a state distance that indicates the average similarity between the filter-bank output vector and prototypes in a state, i.e., <ref type="bibr" target="#b3">(4)</ref> where represents the state distance of the filter-bank output vector at to the th state of the class model, and is a positive constant. Using a large simplifies (4) as the distance to the closest prototype. The final step of the definition is then to compute an aggregate state distance over possible state sequences as follows: <ref type="bibr" target="#b4">(5)</ref> where is a matrix of state distances, where each element contains ; is the path distance representing an accumulated sum of state distances along the path , one of possible paths within a region of permitted by a dynamic programming procedure for ; is a positive constant. A large implements the discrimination function as the smallest accumulated distance. Accordingly, the classifier recognizes the filter-bank output by applying the discriminate function <ref type="bibr" target="#b4">(5)</ref> to the decision rule <ref type="bibr" target="#b0">(1)</ref>.</p><p>From the above description, it could be noticed that the distance classifier resembles a simplified HMM structure that uses preset transition probabilities, Gaussian-based emission probabilities and the score of the Viterbi path as the discriminate function. This is based on the fact that the Euclidean distance used in (3) is directly derived from the Gaussian probability function by assimilating a prototype to a mean vector and simplifying the corresponding covariance matrix to the identity matrix. This emphasizes the close relationship between the distance-based classifier and HMM-based classifiers and, accordingly, allows us to apply the formalization and discussion in this paper to the general case of using HMM classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. DFE Implementation 1) Fundamentals:</head><p>A traditional design framework simply determines the feature extractor by means which are independent of the design of the classifier, or in other words, the design of the discriminate function set . Obviously, this approach is not the best, and thus some reasonable interaction must be incorporated in the design of these two modules. DFE is a general design method that enables one to overcome this problem in an effective and efficient manner.</p><p>For the system structure described above, the DFE training is implemented as follows. Assume that a design sample ( ) is given at the training time index . Using , the feature extractor converts to . DFE emulates a recognition decision for this sample with a misclassification measure defined over the set of the discriminate functions .</p><p>Among many possibilities, we define the misclassification measure as <ref type="bibr" target="#b5">(6)</ref> where is a positive number which controls the relative contribution of the classes considered. The resulting classification decision is evaluated by using a loss (cost). In accordance with the MCE concept, we use the smooth binary (0-1 step) loss defined as <ref type="bibr" target="#b6">(7)</ref> where is a positive constant. Note that the misclassification measure and the loss function are each a function of both and . Clearly, the interaction between the feature extractor and the classifier is embodied by this definition of functionals. Then, the gradient-based loss minimization of MCE/GPD is simply implemented by using as the trainable parameters of the overall recognizer. A resulting DFE adjustment rule is obtained as follows: <ref type="bibr" target="#b7">(8)</ref> where positive-definite matrix; small, monotonically decreasing, positive number at (the training rate); status of at . The chain rule of differential calculus is used for adjusting the feature extractor module. If the feature extractor parameter set is fixed as constant, the DFE training rule (8) becomes equivalent to that of the original MCE/GPD. The procedural difference between DFE and MCE/GPD is minor, but the conceptual difference is significant.</p><p>The theorem on probabilistic descent optimization shows that an infinite repetition of the adjustment achieves at least a local minimization of the expected loss , where is a dynamic pattern and the probability space is assumed to be defined for such patterns appropriately (see <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>). It should be noted that a local minimum of the expected loss corresponds to a local optimal status of the overall recognizer. However, in practice, since infinite training is obviously impossible, a practical implementation of DFE repeats a finite run of the adjustment (8) over a given finite set of training samples. Consequently, this practical DFE training reduces the empirical loss , or the recognition error that is approximated by this empirical loss, where is the number of given training samples.</p><p>Using the chain rule in (8), we derive the following modular optimization scheme: <ref type="bibr" target="#b8">(9)</ref> where and positive-definite matrices, respectively ( ); and small positive numbers at ; and status of and at , respectively. In ( <ref type="formula">9</ref>), refers to computing the gradient with respect to the input to the classifier module of the recognizer, based on the modularity assumption of recognizer design. The modularity assumption permits the treatment of and as independent trainable parameters in the training phase, even if, in practice, initialization of is based on a preset value of .</p><p>The filter-bank parameters and the classifier parameters are different in nature, resulting in different training behavior for and . The flexible training framework of ( <ref type="formula">9</ref>) is useful for alleviating this problem of training behavior, in particular, the problem of the sensitivity of different parameter adjustments to the training rate.</p><p>The development of DFE was motivated by the modular structure of the recognizer. On the other hand, it has been shown that some artificial-neural-network-based recognizers, such as multi-layer perceptrons involve a feature extraction function within their flat configuration <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref>. By applying MCE/GPD to such systems, one would simultaneously train the feature extractor as well as the classifier for optimal classification decisions. However, the use of such flat systems does not necessarily lead to satisfactory design results in sophisticated pattern recognition task and in particular those involving time-dependent patterns such as speech. Moreover, as shown in many efforts of designing neural-network-based hybrid systems, e.g., <ref type="bibr" target="#b18">[19]</ref>, most patterns that are to be classified still require a particular feature representation framework, usually supported by scientific knowledge concerning the features. Thus, even if the training computation of DFE, based on the chain rule, is mathematically the same as the so-called error back-propagation for multi-layered neural networks, DFE can be applied to a wider variety of recognizer structures.</p><p>2) Trainable Parameter Selection for Filter Bank Design: As mentioned in Section II-A2, there are several possibilities for implementing FFT-based filter-banks. In this paper, we selected two types of implementation, i.e., 1) each filter's frequency response has a Gaussian form determined by three kinds of trainable parameters, center frequency, bandwidth, and gain factor (G-type) and 2) each filter is a set of independent weight coefficients (I-type). The Gaussian form was selected based on its smoothness and tractability. To keep the physical meaning of the parameters, we need to ensure that the values of the parameters remain positive during training. This is done by using the transformation where and the DFE adjustment rule is applied to instead of ( corresponds to either , , , or</p><p>). DFE training is based on gradient search optimization. Consequently, a good initialization of the trainable parameters is advisable. That is, a reasonable initial setting is essential for the shaping and positioning of all of the filters as well as the classifier prototypes. Concerning the filter shape, we initially set each filter to a Gaussian function even in the I-type case where the filter weights were treated as independent coefficients. I-type training generates a new filter shape, even though the filter is initially set to the Gaussian function. Clearly, the number of trainable parameters within the G-type is much smaller than that of the I-type. For generalization capabilities, the number of training parameters must be small compared to the number of available training samples. However, the independent weighting case possesses much larger capability to design different filter shapes, which may lead to a better feature representation. This apparent conflict is a point to be studied in the later comparative experiments, though it may be rather difficult to find a general theoretically-proven solution.</p><p>Concerning the initial filter positioning, we considered two initial settings of frequency scaling: the typical psychoacoustics-based scale called the Mel scale (Mel-scaling) and the linear scale (Linear-scaling). To embody the Mel-scaling, we used the following approximation equation, as provided by <ref type="bibr" target="#b19">[20]</ref> (11)</p><p>The Linear-scaling was simply implemented as <ref type="bibr" target="#b11">(12)</ref> In both the Mel-scaling and the Linear-scaling cases, the filter center frequencies were uniformly spaced on the corresponding frequency scale so as to cover a full range of possible frequency band, which is limited by the Nyquist sampling frequency. Moreover, to avoid discontinuity, the bandwidth of each filter was selected so that adjacent filters crossed at the middle position between the corresponding centers. Gain factors were initialized at value one (1). Fig. <ref type="figure" target="#fig_3">3</ref> depicts two example shapes, each with 16 channels, of the filter-bank defined above: the top shape for the Mel-scaling and the bottom for the Linear-scaling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENT I: VOWEL FRAGMENT RECOGNITION TASK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Task</head><p>This section describes the application of the DFE-trained, filter-bank-based recognizer to the task of recognizing static vowel fragments, each extracted from a Japanese vowel segment by a short Hamming time window; for all of the possible fragment patterns. This simple application was chosen mainly because it provides a tractable framework for the analysis of DFE behavior: the spectral characteristics of vowel sounds are rather well known.</p><p>The number of vowel classes is five, i.e., /a/, /i/, /u/, /e/, /o/;</p><p>. We used a database of 500 phonetically balanced sentences spoken by five speakers (three male and two female speakers) in a soundproof room in order to extract 3500 vowel fragment samples in total. Thus, the vowel patterns came from various spoken contexts with variability such as speaking rate, speaker variability and coarticulation. .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Settings</head><p>1) System: Referring to previous works, e.g., <ref type="bibr" target="#b9">[10]</ref>, we set the number of channels of the filter-bank to 16 (</p><p>). Each filter was initially set to the Gaussian function form. The filter-bank used in the experiment are therefore the same as those illustrated in Fig. <ref type="figure" target="#fig_3">3</ref>.</p><p>Since the input pattern was static, we used a simplified version of the classifier structure comprising only one sub-phonetic state model for every class; , . However, since the number of prototypes within each state often determines the recognition accuracy and since it is rather difficult to determine the optimal number theoretically, we experimentally studied four different settings of the prototype numbers for every class; i.e., , and with , based on preliminary experiments. For MCE implementation, only the best incorrect category was compared against. The prototypes were initialized by using -means clustering techniques as in <ref type="bibr" target="#b9">[10]</ref>, which in this context, can be regarded as a maximum likelihood estimation technique. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results</head><p>We summarize experimental results in Figs. <ref type="figure">4</ref> and<ref type="figure"></ref>  A particularly interesting finding over the discriminative training results is that the error rates on testing data are not clearly linked to either the number of prototypes or the selection of frequency scaling (initialization in the DFE cases). This is probably due to the discriminative nature of MCE/GPD or DFE that made the best use of the available prototypes so as to set class boundaries as accurately as possible in the feature vector spaces, each determined by the initial filter-bank (for the MCE/GPD case) or the DFE-designed filter-bank.</p><p>The differences between the MCE/GPD and the DFE training are not so large, i.e., at most about 2% increase in recognition rate. One possible explanation is that each method has achieved near-to-the lowest possible error rate on the task. This is consistent with the performance insensitivity to the number of prototypes (with the qualification that these are local minima).</p><p>Nevertheless, the results show that DFE is clearly superior to the -means clustering and even to MCE/GPD training in Fig. <ref type="figure">4</ref>. Error rates versus classifier size for various configurations when using an initial filter-bank aligned on the Mel scale in the vowel recognition task. terms of training efficiency, which means making good use of available system parameters in order to achieve as high recognition accuracy as possible. For example, let us compare the smallest Gc-trained (DFE-trained) recognizer case and the largest MCE/GPD-trained recognizer case (Fig. <ref type="figure">4</ref>). The smallest recognizer using 96 trainable parameters [16 filter center frequency parameters and 80 classifier parameters ( prototypes)] achieved an error rate of about 14%, which is slightly better than the rate of the largest MCE/GPD-trained system using 560 trainable parameters ( proto-Fig. <ref type="figure">5</ref>. Error rates versus classifier size for various configurations when using an initial filter-bank aligned on the Linear-frequency scale in the vowel recognition task.</p><p>types). This efficiency can be easily observed for most of the plots on the dotted (testing data) error rate curves. Indeed, DFE succeeded in reducing the system resource to 1/6, while keeping the high recognition accuracy achieved by the MCE/GPD training. Even if recent progress of hardware tech-nology are alleviating the size limitation in system design, the finite nature of system size, and more fundamentally the limited number of available resources, is an inevitable requirement in system development, which may be overcome by the DFE approach.</p><p>As shown above, DFE is more effective in terms of accuracy and training efficiency than the other two training methods. One may expect here that this effectiveness is due to either the DFEs data-driven feature representation or to use of classification information in feature extraction design. In other words, the DFE-trained filter-bank is expected to find new reasonable features, which may differ from conventional ones. Therefore, we analyzed the relation between the trained filter-bank shapes and the available frequency band. In fact, due to the high dimension of the data, a simple observation is clearly insufficient for analyzing the details of the training mechanism, even if, as can be observed, the resulting filter-banks are affected by their initial configuration. Furthermore, due to the local optimality of training, the results are not guaranteed to be truly optimal (in the sense of global optimality over training data). Nevertheless, through careful observations, we can find that the DFE training successfully updated the filter-banks so as to use the class identity information of the spectral vectors more effectively.</p><p>Let us focus on the results of GS-training using the Melscaling initialization (upper-right part of the figure). Most of the filters shift to the region below 3 kHz, and the filters that were originally wide are sharpened, increasing the frequency resolution of filter-bank outputs. These changes must probably have appeared so that the filter-bank could effectively extract features useful for classification, which originally existed in the region. Concerning the error rate over testing data, the GS-training using the Mel-scaling, achieved the lowest error rate of about 14%, within the one prototype per class system structure.</p><p>To verify the scientific validity of the GS-trained filter-bank, we investigated the format location of vowel fragments used for training. Formants are known to be important for vowel classification. Fig. <ref type="figure" target="#fig_8">7</ref> shows a histogram of the lower 3 formants as represented in training data. The formants were estimated by using an LPC-based root finding method, followed by human verification. The values of formants span the 0-4 kHz region, and in particular, most of them exist in the region below 3 kHz. By comparing Figs. <ref type="figure" target="#fig_7">6</ref> and<ref type="figure" target="#fig_8">7</ref>, we may naturally argue that there is a strong correlation between the GS-trained filter-bank shapes and the format distribution.</p><p>The DFE concept carried out using filter-bank-derived cepstrum was reported in <ref type="bibr" target="#b20">[21]</ref> and extensively discussed in <ref type="bibr" target="#b21">[22]</ref>. Two differently sized DCTs were investigated: a 20-order Melbased filter-bank transformed into ten cepstral parameters and a 16-order filter-bank transformed into 15 cepstral parameters. The results of these studies showed that, in the context of dis-criminative training (MCE/GPD or DFE), making use of cepstral parameters instead of log energies as features did not result in fundamental differences: the error rates are of a similar order (around 14.5% error rate). However, in contrast to MCE/GPD and DFE, -means clustering (which is an instanciation of MLE in this specific context), seems to be more sensitive to the use of cepstrum as features instead of filter-bank log energies. For instance, with -means training, using ten cepstral coefficients derived from a 20-channel Mel-scale filter-bank (MFCC) achieved 22.3% error rate whereas, as shown above, using a 16 dimensional feature vector with log energies resulted into an error rate of approximately 38.5% in error rate, in the context of one prototype per class.</p><p>An analysis of the cepstrum-based, DFE-optimized-filterbank, showed that within each training case, specific regions of the available frequency band are emphasized, independently of the size of the DCT. The bottom right figure of Fig. <ref type="figure" target="#fig_7">6</ref> shows the resulting filter-bank after GS-training for the two DCT configurations. Similar to the result obtained when using log energies as features, the filters, across all DCT configurations, tend to gather in the 0-4 kHz region, roughly clustered around the 1.5 kHz, 2.5 kHz and the 3-4 kHz regions. Again, this fact shows a strong correlation between the position of the filter-banks and the distribution of formants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENT II: DIRECTORY ASSISTANCE TASK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Task</head><p>The framework is the design of a system which recognizes Japanese persons' names and forward calls to staff members within the ATR laboratory. Thus, the system is intended to act as an automatic switching operator in charge of accepting spoken names and making connection to the recognized persons. The point of interest is to improve the speech recognition accuracy of the system by applying the DFE training and test the DFE ability on a more challenging and practical task.</p><p>Data were automatically collected in an office environment by a system that periodically called staff members to repeat five randomly selected names. Each name utterance was therefore spoken in the isolated word mode. The process produced 684 utterances in total from 47 speakers (3/4 of whom are male). A total of 570 utterances were used for training and 114 utterances were used for testing. The vocabulary is composed of 64 names.</p><p>The speech signal, coming from the telephone transmitter, was digitized at 8 kHz sampling rate and at 16 bits. To compute acoustic spectral vectors (inputs to the filter-bank feature extractor), a Hamming window of 21 ms was shifted over an input speech utterance every 5 ms. At each window position, a segmented utterance was converted to its corresponding 128-dimensional FFT-based power spectrum-vector ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Settings 1) System:</head><p>In this experiment, we used a 16 channel, Gaussian-shaped filter-bank to produced log energies and a 20 channel filter-bank to generate 10 cepstral parameters. The filters were placed so as to cover the 0-4 kHz frequency range, and their gain values each were set to one (1). Fig. <ref type="figure" target="#fig_7">6</ref>. Resulting filter-banks after DFE optimization of various filter-bank parameters in the context of 1 prototype/per vowel. The left side, from top to bottom, shows the result of center frequency, bandwidth and gain optimization respectively, using log energies. On the right side, the two figures in the top show the result of the simultaneous optimization of center frequency, bandwidth and gain using log energies, the two figures in the middle show weight optimization and the two figures in the bottom show the simultaneous optimization of center frequency, bandwidth and gain using cepstrum.</p><p>For classification, following <ref type="bibr" target="#b9">[10]</ref>, we used the 26 context-independent phoneme models which correspond to the 5 Japanese vowels, 20 consonants (/b/, /ch/, /d/, /f/, /g/, /h/, /j/, /k/, /m/, /n/, /N/, /p/, /r/, /s/, /sh/, /t/, /ts/, /w/, /y/, /z/) and silence. Since the input utterances are essentially dynamic, the length of the input vector sequence is changeable; depends on each input word. Unlike the work in the previous task, which was selected for a detailed comparison study, we used only pre-selected settings for the number of states and the number of prototypes, i.e., each phoneme model consisted of three states, each being associated with two prototypes for log energy-based features and 1 prototype for cepstral features.</p><p>(number of phonemes in class word), and . These selections were based on <ref type="bibr" target="#b9">[10]</ref>. For all experiments, the value of was large, meaning that only the best incorrect category was used in the computation of the misclassification measure.</p><p>2) Training: The initial prototypes of the classifier module produced by ML)-based segmental -means <ref type="bibr" target="#b22">[23]</ref> provided an estimated segmentation of each utterance. This ML-produced baseline system was further trained by DFE. Thus, as in the previous task, we conducted seven types of training: However, for simplicity, we used in this task only the Mel-scaling initialization. In all of the training cases, the filters were therefore initially aligned as shown in the top illustration of Fig. <ref type="figure" target="#fig_3">3</ref>, but covering the 0-4 kHz bandwidth. Note that when using cepstrum as features and Mel scale as the frequency scale, before training begins, the cepstrum are similar to conventional MFCC computed through Gaussian filters. In the baseline MCE/GPD training, the filter-bank was fixed and only the classifier prototypes were re-trained. In the five DFE-based training cases, both the filter-bank parameters and the prototypes were adjusted jointly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results</head><p>Controlling several training factors such as the order of training sample presentation, we repeated training/testing runs several times. All training cases, except the GS-training (when using log energies as features), quite successfully converged to the almost perfect accuracy over training data.</p><p>Fig. <ref type="figure" target="#fig_10">8</ref> shows the lowest error rates for all training cases. The GS-training that jointly adjusted the three kinds of parameters of the Gaussian filters failed to converge appropriately when log energies were used as features. This nonconvergence may be related to the difficulty of selecting right training parameters within gradient-based optimization. However, the results displayed in the bottom of the figure shows that making use of cepstrum reduces the sensitivity to this issue, enabling better learning.</p><p>In this figure, except for the rather limited result of I-type training, which may be due to the problem of the high number of system parameters (over-learning problem), the results clearly showed the utility of DFE as a whole. All of the DFE training cases using the G-type filters, i.e., the Gc-training, the Gg-training, and the Gb-training, improved the result of MCE/GPD training, i.e., 7.9% over testing data in the context of log energies. In particular, the Gg-training achieved around 5.3% when using either log energies or cepstrum coefficients. A comparison between the training and testing data results suggests the validity of the DFE-trained filter-banks. Over training data, the achieved error rates by DFE were almost the same with that of the MCE/GPD training. However, there were the clearer differences in error rates over testing data between the MCE/GPD and DFE training cases. Then, we may naturally argue that this phenomenon was due to the effect of the classification-oriented filter-bank that was realized by the DFE training. DFE succeeded in designing a feature space that was more relevant (essential) to classification, and thus achieved the lower error rates over the unknown testing data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we have described experimental evaluations of the DFE design method in speech pattern recognition tasks using a particular system structure, which consists of the filter-bank feature extractor and a multi-prototype distance classifier with state-transitions. DFE determines all of trainable parameters of the overall recognizer using a single design criterion of reducing a smoothed recognition error counts. First, we evaluated DFE in the vowel fragment pattern recognition task and did analysis of the DFE-trained filter-bank within the context of this simple task. Investigation was performed by controlling experimental settings such as the number of prototypes and different frequency scale initializations. Observation of the DFE-designed filter-banks showed that, in certain cases, the training appropriately focuses on the formant region, which is considered relevant to vowel categorization. Second, the capability of DFE was tested in the more realistic and difficult task, called the ATR directory assistance task, which consists of recognizing names over the telephone. DFE was compared to the segmental -means clustering and the MCE/GPD training, and it successfully achieved the lowest word error rate of 5.3%. The results in this paper therefore clearly confirm the utility of DFE in the common selection of a filter-bank feature extraction and a distance classifier.</p><p>DFE was implemented here using the gradient-based optimization technique. Similar to other MCE's cases, the gradientbased optimization is not essential for the formalization concept of DFE. An important point is to formalize the entire recognition process consistently and directly as a training procedure. However, the gradient method is still useful from the practical viewpoint of computation load, and we actually used in the paper this practical optimization method. As is well known, the method does not guarantee to achieving the global minimum status of the error surface. However, regardless of this defect, experimental results demonstrated that the DFE implementation which aims at achieving at least a local optimal design with the gradient optimization, can be successfully carried out.</p><p>In the design of pattern recognizers, conventionally, recognition accuracy improvement is attempted by increasing the size of trainable parameters used for classification. However, as often criticized and as demonstrated in the previous sections, such an approach cannot be an acceptable solution. A larger size classifier is time-consuming as well as resource-consuming. Moreover, increasing the classification capability often causes the over-learning problem. On the other hand, if samples are represented in a highly separable feature-space, the process of classifying them can be rather simple and easy. A small size classifier would be sufficient for handling such patterns correctly. The experimental results in the paper show that DFE realizes this efficient classifier by finding salient pattern characteristics, as manifested in the training data, in other words, increasing the sample separability, and thus alleviating the over-learning problem.</p><p>DFE can not circumvent, however, the over-learning problem completely: DFE is a data-driven training and its design result relies on the nature of finite samples available in the training stage. Thus, naturally, a careful design manner that entirely covers the task including training data preparation is required even in the use of DFE.</p><p>Since the setting of a classification problem usually assumes that patterns to be classified are represented a priori in the fixed feature space, over-learning in classifier design is often studied in the conventional approach where one attempts to alleviate it by controlling the representation capability of classifier. In contrast, since DFE designs the feature space itself in the datadriven manner, such a conventional approach may be inadequate for further analyzes of the characteristics of DFE, such as its over-learning mechanism. A new mathematical framework is needed for analyzing the feature representation and the classification decision, both determined jointly by DFE. This can be an important future research issue.</p><p>The training effect of DFE is to increase sample separability by transforming the feature-space. The process can be viewed as a way of compensating for the insufficiency of the post-end classification stage. For instance, one can consider that DFE training in the context of the distance classifier compensates for the lower classification power, which is due to using the Euclidean distance as a local distance instead of using a more sophisticated statistical distance measure such as the Mahalanobis distance or the Gaussian probability function. DFE transforms the initial feature space into a simpler space in which samples are more suited for the classification stage, without increasing the trainable classification parameters. In short, DFE attempts to transform the feature-space for an easier classification, while the conventional approach uses a larger-sized classifier or a sophisticated distance measure, thereby increasing classification capability, in order to accomplish difficult classification on a preset feature-space. The latter approach may not be the optimal solution, in particular, when feature-points are highly dispersed in the feature-space. In light of this, DFE can be viewed as compensating for the restricted classification capability by generating features that are based on the minimum classification error criterion. However, it should be noticed that the DFE effect is not limited to this role and goes beyond simple compensation since DFE also adjusts the parameters of the classification stage for better discrimination.</p><p>A fundamental assumption in this work is that the recognizer is made of two separate modules, namely feature extraction and classification. Feature extraction is a particularly required step when the input signal comes from real world environment or is made of high dimensional pattern. Consequently, the modularity assumption has been the foundation of pattern recognition (see <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b23">[24]</ref>), in which a transformation phase, transforming the observed signal into meaningful, information-bearing values, is assumed before classification is carried out. One consequence of the modularity assumption is that the same classification technique can be used in various pattern recognition tasks, and only the front-end feature extraction specifically depends on the task at hand. For instance, speech recognition methods may share the same clustering techniques with character recognition methods, while diverging on the method to use in transforming the input signal from the environment into the data that are to be clustered. This feature extraction phase can take various forms, ranging from digital signal processing schemes to multi-stage, parallel processing as shown in the context of ANN. The modularity assumption is particularly illustrated in the design of most speech recognizers, where many different feature types can be used with many different types of classifiers. For instance, one can optimize HMM or ANN-based classifiers using either MFCC or LPC cepstrum based features.</p><p>The alarming fact is that the modular structure of the recognizer has led to the independent optimization of the corresponding modules, usually using different optimization criteria. Furthermore, the optimization criteria used are typically not directly related to the final target of attaining optimal recognition performance. The merit of DFE is that it preserves the advan-tages of a modular structure, but enables a full collaboration of the modules toward the overall goal of recognition. The modules are designed jointly, using a single optimization criterion that is closely related to classification accuracy.</p><p>The design framework of DFE can also be recognized for a another scientific contribution: DFE can feed-back the knowledge acquired by machine-decision-oriented feature representation to traditional science regarding characteristics of input pattern and/or human capability. This interaction between expertise and statistical design as done through the DFE approach may contribute to the advancement of science and technology in the pattern recognition field.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Block diagram of a modular speech recognizer consisting of a front-end filter-bank-based feature extractor and a back-end multi-prototype distance classifier.</figDesc><graphic coords="2,63.66,62.28,462.96,121.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Prototype-based minimum error classifier structured with three states and four prototypes per state.</figDesc><graphic coords="3,40.74,62.28,248.88,112.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 )</head><label>1</label><figDesc>The weighting function within the G-type implementation is defined ;determines the center frequency, respectively, at the th channel; frequency mapping function that determines frequency scaling. Thus, , and this definition leads to the following four cases of DFE training for the G-type filter implementation, based on the selection of the trainable parameters: Selective training A (Gc-training): Gc-training trains only the center frequencies 's, while keeping the remaining filter parameters fixed. Note that modifying the spacing of the filters, with fixed bandwidth and gain, results in a different coverage of the available frequency range. 2) Selective training B (Gb-training): Gb-training only adjusts the bandwidths 's. A larger value of signifies a narrower filter and vice-versa. 3) Selective training C (Gg-training): Gg-training only adjusts the gain factors 's. A large value of gain factor puts emphasis on the filter output energy. 4) Simultaneous training (GS-training): In GS-training, the three types of adjustable parameters, i.e., center frequencies, bandwidths, and gain factors, are simultaneously trained. In the above training cases, each filter has a Gaussian shape. In the framework of I-type filter implementation, the weight functions are considered independent and are not constrained to be of a pre-chosen filter shape, thus . This then leads to only one training case, namely, I-type training or I-training where one treats each filter weight as an independent trainable parameter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Initial filter bank aligned in the (top) Mel scale and the (bottom) linear scale.</figDesc><graphic coords="5,314.58,62.28,227.28,191.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Half of the samples were used for training (design), and the other half for testing; i.e., 1750 patterns for training and 1750 patterns for testing. Training and testing samples were statistically balanced in terms of speaker and vowel class. That is, 70 samples per speaker and per vowel were used in the training stage as well as the testing stage.Digital representation of the speech waves was made at 12 kHz sampling rate and at 16 bits. Each fragment pattern was extracted by applying a 21 ms Hamming time window to the center position of a vowel segment, labeled by hand. The fragment was then converted to a 128-dimensional, FFT-based power spectrum as input to a filter-bank feature extractor (FFT was performed with 256 points);</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2 )</head><label>2</label><figDesc>Training: For every size of recognizer (every prescribed number of prototypes per class), we conducted the five types of DFE training, namely, Gc-training, Gb-training, Gg-training, GS-training, and I-type training. It should be noted that in all of these cases, DFE training adjusts the classifier prototypes, as in [10], as well as the filter-bank parameters. Moreover, for comparison purposes, we evaluated the performance of two types of baseline training: -means clustering training that corresponds to the initialization cited above and MCE/GPD training that adjusts only the classifier prototypes with MCE/GPD after the initialization. The gradient-based loss minimization of the DFE training and the MCE/GPD training includes several factors/parameters, such as the training rate and the order of presentation of the design samples. These factors must be experimentally or empirically selected and could result in somewhat different recognition performances. For every training case, we conducted several runs by controlling the training factors. However, for presentation clarity, we use in this paper the best recognition accuracy (lowest recognition error rate) for every training case.Concerning the selected training factors, the loss smoothness (the value of ) was 1.0 and all training rates were decreased linearly with the number of iterations (that is, at the same rate). Only the initial value of (classifier training rate) and the ratio of to (feature extractor training rate) were tuned. was initially set to and the ratio was of the order of for the center frequency and bandwidth optimization, for the gain, and in the I-type training case. Regarding parameters sensitivity to learning factors tuning, gain and weighting appeared to be more robust than center frequency and bandwidth. Clearly, these training factors depend on the task, the structure of classifier, and the structure feature extractor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>5 according to the selection of the frequency scale on which the filter-bank is initially designed. Fig. 4 shows recognition error rates for the Mel-scale-based initialization and Fig. 5 for the Linear-scale. The error rates are plotted as a function of the number of prototypes per class. Graphs in the figures illustrate the results of the -means clustering-based training, the MCE/GPD training and the DFE-based training methods. The -means clustering training, which is based on the Minimum Distortion criterion, dramatically reduces the error rate as the number of prototypes increases. Careful observation also shows that the accuracy of the -means clustering training method is affected by the selection of the initial frequency scale: the error rates of the Mel-scaling case are consistently about 5% lower than those of the Linear-scaling. The difference in error rate is rather clear between the -means clustering training and the discriminative training, i.e., the MCE/GPD training or the DFE training. That is, with regards to important results over testing data, both the MCE/GPD training and the DFE training achieved about 13% (in the MCE/GPD training of the largest size classifier using the Linear-scaling initialization and in the DFEs I-type training of the smallest size classifier using the Linear-scaling), while the -means clustering training produced about 19% (in the case of the largest size classifier using the Mel-scaling initialization). These results clearly demonstrate the effect of discriminative training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6</head><label>6</label><figDesc>illustrates the DFE-trained filter-banks in the case of the smallest classifier of one prototype per class. This figure is composed of five parts, with each part showing results of both the Mel-scaling and the Linear-scaling initialization cases. In the figure, the top left part presents the results of Gc-training, drawn as a function of filter center frequencies; the bottom left part presents results of Gb-training, focusing on the bandwidths of individual filters. The upper right part depicts the filter-banks of which all center frequencies, bandwidths, and gains were simultaneously updated (GS-training); the bottom right part depicts the results of I-type training. All these results do not allow an easy analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Formant location in the training data aiming at vowel recognition.</figDesc><graphic coords="11,126.60,62.28,340.08,281.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>1 )</head><label>1</label><figDesc>baseline training using the segmental -means clustering, which, in this context, is based on a minimum distortion estimation; 2) baseline training using the classical MCE/GPD training; 3) Gc-training; 4) Gb-training; 5) Gg-training; 6) GS-training; 7) I-type training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 .</head><label>8</label><figDesc>Fig.8. Top: Experimental results of the directory assistance task or various adjusted parameters using log energies and two prototypes per states. Bottom: Experimental results of the directory assistance task for various adjusted parameters using cepstrum and one prototype per state.</figDesc><graphic coords="12,136.02,62.28,318.24,517.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Shigeru</head><label></label><figDesc>Katagiri (M'88-SM'97-F'00) received the B. E. degree in electrical engineering and the M. E. and Dr. Eng. degrees in information engineering from Tohoku University, Japan, in1977, 1979, and 1982,  respectively. From 1982 to 1986, he was with the Electrical Communication Laboratories, Nippon Telegraph and Telephone Public Corporation (NTT). In 1986, he moved to the Advanced Telecommunications Research Institute International (ATR), where he was engaged in a wide range of speech research, such as speech recognition, acoustic monitoring and spoken language acquisition. During 1989-1990, was a Visiting Researcher with the Speech Research Department, AT&amp;T Bell Laboratories. In 1998, he returned to NTT and currently heads the Intelligent Communication Laboratory, NTT Communication Science Laboratories. He also serves as an Adjunct Professor of the Graduate School of Informatics, Kyoto University, Japan. Dr. Katagiri was a co-recipient of the 22nd Sato Paper Award of the Acoustical Society of Japan (ASJ) in 1982, the 27th Sato Paper Award of the ASJ in 1987, and the IEEE Signal Processing Society (SPS) 1993 Senior Award in 1994. He has served in several academic society functions including an Action Editor of Neural Networks an Associate Editor of the IEEE TRANSACTIONS ON SIGNAL PROCESSING, the Vice-Chair of the Tokyo Chapter of the IEEE-SPS, and the Chair of IEEE-SPS Neural Networks for Signal Processing Technical Committee. Erik McDermott was born on October 16, 1965. He received the B.S. degree in symbolic systems from Stanford University, Stanford, CA, in 1987, and the Ph.D. degree in computer and information science from Waseda University, Tokyo, Japan, in 1997. From 1987 to 1993, he was a Research Associate with the ATR Auditory and Visual Perception Research Laboratories. During this time, his work focused on connectionist approaches to pattern recognition, with an emphasis on speech recognition. From 1993 to 2000, he was a Researcher with ATR Human Information Processing Research Laboratories. His research activities there centered on discriminative approaches to speech recognition. Since 2000, he has been a Research Specialist at NTT Communication Science Laboratories and a Visiting Researcher at ATR Intelligent Systems Division. His interests include telephone-based speech recognition, the development of new approaches to speech recognition based on articulatory models, and the application of speech recognition technology to software for language learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Biing-Hwang Juang (F'92) is Head of Acoustics and Speech Research Department at Bell Laboratories, Lucent Technologies, Murray Hill, NJ. He is engaged in a wide range of communication related research activities, from speech coding, speech recognition to multimedia communications. He has published extensively and holds a number of patents in the area of speech communication and communication services. He is co-author of the book Fundamentals of Speech Recognition (Englewood Cliffs, NJ: Prentice-Hall).Dr. Juang received the 1993 Best Senior Paper Award, the 1994 Best Senior Paper Award, and the 1994 Best Signal Processing Magazine Paper Award, and was co-author of a paper granted the 1994 Best Junior Paper Award, all from the IEEE Signal Processing Society. In 1997, he won the Bell Labs President Award for leading the Bell Labs Automatic Speech Recognition (BLASR) team. He also received the prestigious 1998 Signal Processing Society's Technical Achievement Award and was named the Society's 1999 Distinguished Lecturer. He was an editor for the IEEE TRANSACTIONS ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING(1986)(1987)(1988), the IEEE TRANSACTIONS ON NEURAL NETWORKS (1992-1993), and the Journal of Speech Communication(1992)(1993)(1994). He has served on the Digital Signal Processing and the Speech Technical Committees as well as the Conference Board of the IEEE Signal Processing Society and was(1991)(1992)(1993) Chairman of the Technical Committee on Neural Networks for Signal Processing. He is currently Editor-in-Chief of the IEEE TRANSACTIONS ON SPEECH AND AUDIO PROCESSING and member of the Editorial Board of the PROCEEDINGS OF THE IEEE. He also serves on international advisory boards outside the United States.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>AlainBiem  (M'00) was born in Cameroon in 1967. He received the M.S. degree in telecommunication engineering from the Institut National des Telecommunications (INT), Evry, France, in 1991 and the Ph.D degree in computer science from the University of Paris VI, Paris, France, in 1997. In 1990, he worked on the design of hybrid neural networks and HMM architectures for speech recognition at ATR Interpreting Telephony Research Laboratories, Japan. From 1992 to 1993, he was a Research Associate with ATR Auditory and Visual Perception Research, Laboratories, Japan. In 1993, he joined the ATR Human Information Processing Research Laboratories as a Researcher, involved in various research and development activities on speech parameterization, application of perceptual models to speech recognition, robust speech recognition, and discriminative training. Since 2000, he has been a Research Staff Member with the IBM T. J. Watson Research Center, Yorktown Heights, NY. His current interests include machine learning, handwriting recognition, discriminative training, pen technologies, and document processing. Dr. Biem is a member of the IEEE Signal Processing Society, the IEEE Computer Society, and ISCA.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>This filter-bank implementation forces the time resolution to be equivalent for all channels, because of the use of the short-time window for FFT computation. This also means that there is no control of the time resolution within each channel, a control which is available in time-domain-based digital filter-banks.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The associate editor coordinating the review of this manuscript and approving it for publication was Prof. Joseph Picone.</p><p>A. Biem was with the ATR Human Information Processing Research Laboratories, Kyoto 619-02, Japan. He is now with</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<title level="m">Pattern Classification and Scene Analysis</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Discriminative learning for minimum error classification</title>
		<author>
			<persName><forename type="first">B.-H</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Katagiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Processing</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="3043" to="3054" />
			<date type="published" when="1992-12">Dec. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Discriminative feature extraction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Katagiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-H</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Biem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks for Speech and Vision</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Mammone</surname></persName>
		</editor>
		<meeting><address><addrLine>Ed. London, U.K.</addrLine></address></meeting>
		<imprint>
			<publisher>Chapman &amp; Hall</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Feature extraction based on minimum classification error/generalized probabilistic descent method</title>
		<author>
			<persName><forename type="first">A</forename><surname>Biem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Katagiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, Signal Processing</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, Signal essing</meeting>
		<imprint>
			<date type="published" when="1993-04">Apr. 1993</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="275" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discriminative feature extraction for speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Biem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Katagiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-H</forename><surname>Juang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop Neural Networks Signal Processing</title>
		<meeting>IEEE Workshop Neural Networks Signal essing</meeting>
		<imprint>
			<date type="published" when="1993-09">Sept. 1993</date>
			<biblScope unit="page" from="392" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pattern recognition based on discriminative feature extraction</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="500" to="504" />
			<date type="published" when="1997-02">Feb. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An application of minimum classification error to feature space transformation for speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Peinado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Rubio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="273" to="290" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">HMM-based speech recognition using state-dependent, discriminately derived transforms on mel-warped DFT features</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rathinavelu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Processing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="243" to="256" />
			<date type="published" when="1997-05">May 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discriminative metric design for robust pattern recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Katagiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="2655" to="2662" />
			<date type="published" when="1997-11">Nov. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Prototype-based minimum classification error/generalized probabilistic descent for various speech units</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Katagiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Speech Lang</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="351" to="368" />
			<date type="published" when="1994-10">Oct. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A telephone-based recognition system adaptively trained using minimum classification error/generalized probabilistic descent (MCE/GPD)</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Woudenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Katagiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Acoustical Soc</title>
		<meeting>Acoustical Soc<address><addrLine>Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-03">Mar. 1995</date>
			<biblScope unit="page" from="87" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Discriminative training for speech recognition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mcdermott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
		<respStmt>
			<orgName>Waseda Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Comparison of parametric representation for monosyllabic word recognition in continuously spoken sentences</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mermelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoustics, Speech, Signal Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="357" to="366" />
			<date type="published" when="1980-08">Aug. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A theory of adaptive pattern classifiers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Amari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Electron. Comput</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="299" to="307" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A generalized probabilistic descent method</title>
		<author>
			<persName><forename type="first">S</forename><surname>Katagiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-H</forename><surname>Juang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<publisher>Acoustical Soc. Japan</publisher>
			<date type="published" when="1990-09">Sept. 1990</date>
			<biblScope unit="page" from="141" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning internal representations by error propagation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel Distributed Processing: Explorations in the Microstructure of Cognition</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Bradford Books/MIT Press</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A neural network for feature extraction</title>
		<author>
			<persName><forename type="first">N</forename><surname>Intrator</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Touretzky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufman</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="719" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Optimized feature extraction and the bayes decision in feed-forward classifier networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Patten Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="355" to="364" />
			<date type="published" when="1991-04">Apr. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Continuous speech recognition using multilayer perceptrons with hidden Markov models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, Signal Processing</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, Signal essing</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="413" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Analytical expression for critical band rate and critical bandwidth as a function of frequency</title>
		<author>
			<persName><forename type="first">E</forename><surname>Zwicker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Terhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="1523" to="1525" />
			<date type="published" when="1980-12">Dec. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cepstrum-based filter-bank design using discriminative feature extraction training at various levels</title>
		<author>
			<persName><forename type="first">A</forename><surname>Biem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Katagiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, Signal Processing</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, Signal essing</meeting>
		<imprint>
			<date type="published" when="1997-04">Apr. 1997</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1503" to="1506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Discriminative feature extraction applied to speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Biem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Univ. Paris</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="1997">1997</date>
			<pubPlace>Paris, France</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The segmental k-means algorithm for estimating parameter of hidden Markov models</title>
		<author>
			<persName><forename type="first">B.-H</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoustics, Speech, Signal Processing</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1639" to="1641" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
		<title level="m">Introduction to Statistical Pattern Recognition</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic</publisher>
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
