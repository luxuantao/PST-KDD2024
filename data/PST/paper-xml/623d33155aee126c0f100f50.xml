<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GraphCoCo: Graph Complementary Contrastive Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-24">24 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiawei</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
							<email>yanjuchi@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chentao</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yue</forename><surname>Ding</surname></persName>
							<email>dingyue@sjtu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruoxin</forename><surname>Chen</surname></persName>
							<email>chenruoxin@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Yu</surname></persName>
							<email>yu-xiang@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinyu</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GraphCoCo: Graph Complementary Contrastive Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-24">24 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2203.12821v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Contrastive Learning (GCL) has shown promising performance in graph representation learning (GRL) without the supervision of manual annotations. GCL can generate graph-level embeddings by maximizing the Mutual Information (MI) between different augmented views of the same graph (positive pairs). However, we identify a obstacle that the optimization of InfoNCE loss only concentrates on a few embeddings dimensions, limiting the distinguishability of embeddings in downstream graph classification tasks. This paper proposes an effective graph complementary contrastive learning approach named GraphCoCo to tackle the above issue. Specifically, we set the embedding of the first augmented view as the anchor embedding to localize "highlighted" dimensions (i.e., the dimensions contribute most in similarity measurement). Then remove these dimensions in the embeddings of the second augmented view to discover neglected complementary representations. Therefore, the combination of anchor and complementary embeddings significantly improves the performance in downstream tasks. Comprehensive experiments on various benchmark datasets are conducted to demonstrate the effectiveness of GraphCoCo, and the results show that our model outperforms the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the past a few years, Graph Representation Learning (GRL) has been increasingly popular for ubiquitous graphstructured data in various domains, including traffic <ref type="bibr" target="#b24">[Yu et al., 2018]</ref>, social network <ref type="bibr" target="#b5">[Fan et al., 2019]</ref>, and knowledge graph <ref type="bibr" target="#b14">[Schlichtkrull et al., 2018]</ref>. Graph Neural Networks (GNNs) <ref type="bibr" target="#b9">[Kipf and Welling, 2017;</ref><ref type="bibr" target="#b20">Xu et al., 2019]</ref> are utilized as backbones of GRL to learn low-dimensional embeddings of nodes or graphs while maintaining structure and attribute information. Most GNN models are trained in the (semi-)supervised learning setting requiring abundant manuallyannotated labels. In case of sufficient data labels, recent Contrastive Learning (CL) based on Information Maximization (InfoMax) principle <ref type="bibr" target="#b10">[Linsker, 1988]</ref> has shown promis- ing performance for self-supervised learning with success across fields including computer vision <ref type="bibr">[Chen et al., 2020;</ref><ref type="bibr" target="#b7">He et al., 2020]</ref> and natural language processing <ref type="bibr" target="#b21">[Yang et al., 2021]</ref>. These CL methods maximize the Mutual Information (MI) between different augmented views of the same instance while minimizing the MI between those of the different instances.</p><p>Inspired by the above CL models, Deep Graph InfoMax (DGI) <ref type="bibr" target="#b19">[Veli?kovi? et al., 2019]</ref> applies the InfoMax principle to graph representation learning, which relies on maximizing the mutual information between one graph's the patch-level and global-level representations. Following SimCLR <ref type="bibr">[Chen et al., 2020]</ref>, a series of graph contrastive learning methods <ref type="bibr">[Hassani and Khasahmadi, 2020;</ref><ref type="bibr" target="#b22">You et al., 2020]</ref> enforce the embedding of positive pair (i.e., augmented views of the same graph) to be close and the embedding of negative pair (i.e., augmented views of different graphs) to be distant in Euclidean space. GCC <ref type="bibr" target="#b14">[Qiu et al., 2020]</ref> referring to MoCo <ref type="bibr" target="#b7">[He et al., 2020]</ref> contrasts graph-level embedding with momentum encoder and maintain the queue of data samples.</p><p>However, recent studies <ref type="bibr" target="#b17">[Tschannen et al., 2020;</ref><ref type="bibr" target="#b2">Chen and Li, 2020]</ref> have pointed out that there are gaps between the InfoMax principle and the performance of embeddings in the downstream tasks. Unlike contrastive learning in the computer vision literature, the usage of augmentation can result in a large discrepancy between the embeddings of the two augmented views while keeping the semantic information. Nevertheless, we experimentally observe that this discrepancy can be limited in graph contrastive learning, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. In particular, we further notice that in graph con-trastive learning, a portion of elements in embeddings extracted from two augmented views are much larger than those in other positions, and these elements have the same positions in both embeddings. The similarity between embeddings is mainly controlled by these prominent elements which become more significant during training by influencing the direction of gradient descent. Therefore, the concentration of information on a few dimensions weakens the distinguishability of embeddings in the downstream tasks.</p><p>To tackle the above problem, we propose a novel Graph Complementary Contrastive Learning (GraphCoCo) approach with non-maximum erasing for graph-level selfsupervised representation learning. The key idea of Graph-CoCo is to learn the complementary embedding of graphs with two augmented views, inspired by non-maximum suppression which is widely used in visual object detection <ref type="bibr" target="#b25">[Zhang et al., 2018]</ref>. In particular, we first generate two augmentations of the same graph and encode them into lowdimensional embeddings with GNNs. The first embedding identifies the most significant dimensions and then erases these dimensions in the second embedding to learn complementary information on other dimensions, which boosts the downstream classification tasks. Finally, we optimize the model with a prevalent contrastive loss where the nonmaximum erasing operation is not performed on negative pairs. We conduct a series of experiments on various bioinformatics and social networks datasets to demonstrate the effectiveness of GraphCoCo. Our contributions are as follows:</p><p>1) We identify and theoretically analyze an obstacle of graph contrastive learning that the embeddings of positive pairs share common "highlighted" dimensions whose values can be much larger than others, which limits the expressiveness of embeddings in downstream classification tasks.</p><p>2) We propose a novel graph complementary contrastive learning approach (GraphCoCo) for self-supervised graph classification tasks. Our model effectively encourages the encoder to learn complementary representation in pre-text tasks using non-maximization erasing operation.</p><p>3) Experiments across multiple datasets show that Graph-CoCo outperforms state-of-the-art self-supervised methods on graph classification tasks and achieves competitive performance on node classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Graph neural networks. Graph Neural Networks (GNNs) have attracted growing attention for analyzing graphstructured data in recent years. Generally, GNNs are categorized into spatial-domain and spectral-domain approaches. Based on the spectral graph theory, <ref type="bibr" target="#b1">[Bruna et al., 2014]</ref> first defines the graph convolution in the spectral domain through the eigen-decomposition of the graph Laplacian, defectively causing high computational cost. Graph Convolution Network <ref type="bibr" target="#b9">[Kipf and Welling, 2017]</ref> utilizes the 1-st approximation of the Chebyshev expansion to simplify the calculation. Spatial-based approaches follow a message passing scheme <ref type="bibr" target="#b0">[Abu-El-Haija et al., 2019]</ref>, where each node collects the information from its neighbors iteratively. GraphSAGE <ref type="bibr" target="#b6">[Hamilton et al., 2017]</ref> aggregates the information from randomly sampled neighborhoods to scale to large graphs. GAT <ref type="bibr" target="#b18">[Veli?kovi? et al., 2018]</ref> introduces the attention mechanism to assign scores for each node pair. GIN <ref type="bibr" target="#b20">[Xu et al., 2019]</ref> generalizes the Weisfeiler-Lehman test and reaches the most expressive power among GNNs.</p><p>Graph contrastive learning. As one of the main approaches of self-supervised representation learning, contrastive learning has raised a surge of attraction in computer vision <ref type="bibr" target="#b7">[He et al., 2020;</ref><ref type="bibr">Chen et al., 2020]</ref> and natural language processing <ref type="bibr" target="#b20">[Yang et al., 2019]</ref>. Contrastive learning is based on the mutual information (MI) maximization principle by encouraging the agreement between two augmented views of the same instance and the disagreement between augmented views of different instances. Inspired by visual contrastive learning, a series of graph contrastive learning methods are devised. Deep Graph InfoMax (DGI) <ref type="bibr" target="#b19">[Veli?kovi? et al., 2019]</ref> first applies the InfoMax principle to graph representation learning. DGI relies on maximizing the mutual information between the patch-level and global-level representation of one graph. <ref type="bibr">MVGRL [Hassani and Khasahmadi, 2020]</ref> generates two augmented graph view via graph diffusion and subgraph sampling. Based on SimCLR <ref type="bibr">[Chen et al., 2020]</ref>, GraphCL <ref type="bibr" target="#b22">[You et al., 2020]</ref> enforces the embedding of positive pair (i.e., augmented views of the same graph) to be close and the embedding of negative pair (i.e., augmented views of different graphs) to be distant in Euclidean space. CuCo <ref type="bibr" target="#b4">[Chu et al., 2021]</ref> further utilizes the curriculum learning to select the negative samples. AD-GCL <ref type="bibr" target="#b16">[Susheel et al., 2021]</ref> optimizes adversarial graph augmentation to prevent learning redundant information. JOAO <ref type="bibr" target="#b23">[You et al., 2021]</ref> automatically and adaptively select the augmentation for specific dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Definition</head><p>Let G = (V, E) denote a graph, where</p><formula xml:id="formula_0">V = {v 1 , v 2 , ? ? ? , v N }, E ? V ? V</formula><p>denote the node set and the edge set, respectively. The adjacency matrix containing the connectivity of nodes is denoted as A ? {0, 1} N ?N , where the entry</p><formula xml:id="formula_1">A ij = 1 if (v i , v j ) ? E. The feature matrix is denoted as X ? R N ?F , where the i-th entry x i ? R F is the F -dimensional feature vector of node v i . For self-supervised graph-level representa- tion learning, given a set of graphs G = {G 1 , G 2 , ? ? ? } with- out class information, our objective is to learn a GNN en- coder g ? (X , A) ? R F which encodes each graph G into a F -dimensional vector z G ? R F .</formula><p>These low-dimensional embeddings can be used in downstream tasks, such as node and graph classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Motivation</head><p>Previous work [Chen and <ref type="bibr" target="#b2">Li, 2020]</ref> states that the InfoNCE loss, a widely used objective function in contrastive learning <ref type="bibr">[Chen et al., 2020;</ref><ref type="bibr" target="#b22">You et al., 2020]</ref>, can not guarantee to avoid shortcut solutions that only capture easy-to-learn features. The image augmentation such as color distortion and rotation, can obtain a totally pixel-level different image while keeping semantic information. On the contrary, the graphdata augmentation is local and limited due to the perturbationinvariance of the adjacency matrix and lack of diversity of features. Although augmentation is applied to obtain two views of the same graph, the difference between embeddings of two augmented views is narrow, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Thus the contribution of the encoder when pulling positive pairs close is marginal. Additionally, we observe that a portion of elements are much larger than others in both embeddings and these larger elements share the same positions. We define the position of these much larger elements as "highlighted" dimensions.</p><p>We then analyze how "highlighted" dimensions hurt the performance of the downstream tasks from the perspective of the gradient of InfoNCE shown in Proposition 1 with derivation in Appendix A.</p><p>Proposition 1. Given embeddings of positive pair (u, v + ) and negative pairs (u, v -), the InfoNCE loss is defined as:</p><formula xml:id="formula_2">LNCE = -log exp ( u, v + /? ) exp ( u, v + /? ) + v -exp (( u, v -/? ))</formula><p>.</p><p>(1) Then the gradient of InfoNCE w.r.t embeddings of positive pairs are:</p><formula xml:id="formula_3">?LNCE ?u = -1 ? ((1 -p v + ) ? v + -v -(p v -? v -)) ?LNCE ?v + = - (1-p v + ) ? ? u where p v = exp( u,v /? ) exp ( u,v + /? )+ v -exp( u,v -/?</formula><p>) and ? is the temperature parameter.</p><p>Note that the value of the gradient of InfoNCE in each direction w.r.t. v + is proportional to the value of u in each direction. Minimization of InfoNCE loss mainly lies in "highlighted" dimensions. Intuitively, contrastive learning aiming at maximizing the similarity between positive pairs leads to a shortcut that only a few dimensions to be relatively much larger. Thus, these "highlighted" dimensions are prominent to represent features of augmented graphs and suppress the expressiveness of other dimensions. Consequently, the downstream classification performance mainly depends on "highlighted" dimensions and neglects the leverage of other dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Method</head><p>This section presents the proposed graph complementary contrastive learning (GraphCoCo) approach. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, given the input graph G, we first generate two augmented graph views G1 and G2 . Then each augmented view is encoded into low-dimensional embeddings with one shared encoder. After that, for positive pairs (i.e., two augmented views of the same graph), the embedding of the first augmented view conducts the erasing operation described in Section 5.3 on the embedding of the second one, to prevent the shortcut that focuses on optimizing "highlighted" dimensions to maximize the similarity. Since the negative pairs (i.e., augmented views of the different graph) do not share the same "highlighted" dimensions, the erasing operation is not applied to the negative pairs. Finally, the parameters of the encoder are learned with contrastive objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Graph Augmentation</head><p>One of the critical components in contrastive learning is graph augmentation which generates noise versions of graphs to be passed into the contrastive loss as positive pairs. Given a graph G, we define the i-th augmented view as G(i) = t i (G), where i = {1, 2}, t i is selected from a group of predefined graph augmentation T . Motivated by image augmentations, various graph augmentation are proposed and categorized into two types: structure-based and feature-based. We leverage the optimal combinations reported in GraphCL <ref type="bibr" target="#b22">[You et al., 2020]</ref> for each dataset from following graph augmentation methods: 1) NodeDrop: randomly discards certain portion of nodes with their edges and features. 2) EdgeAdd and EdgeDrop: randomly adds or drops certain portion of edges in graphs. 3) FeatureMasking: randomly masks a portion of dimensions in node features with zero. 4) Subgraph: generates a subgraph with Random Walk. The details for the used augmentation approaches are given in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Encoder</head><p>We leverage the classic GNN <ref type="bibr" target="#b9">[Kipf and Welling, 2017;</ref><ref type="bibr" target="#b20">Xu et al., 2019]</ref> as the encoder to extract the low-dimensional graph-level representation r (i) for each augmented graphs G(i) . Formally, given an augmented graphs G(i) with adjacency matrix A and high-dimensional node features X , where x n = X [n, :] is the feature vector of node v n , the l-th layer of GNN first updates each node's representation in a message passing manner:</p><formula xml:id="formula_4">a (l) n = AGGREGATE (l) h (l-1) m : v m ? N (v n ) , h (l) n = COMBINE (l) h (l-1) n , a (l) n ,<label>(2)</label></formula><p>where h</p><formula xml:id="formula_5">(l)</formula><p>n is the representation of the node v n in the l-th layer of GNN with h (l) (?) can be the sum or average operation, and COMBINE (l) (?) can be concatenation or average operation. Then the graph-level embedding of G i can be obtained through the READOUT function of GNN, which is similar to pooling operation in CNN, as follows:</p><formula xml:id="formula_6">(0) n = x n , N (v n ) is the set of neighbors of node v n , AGGREGATE</formula><formula xml:id="formula_7">r i = READOUT({h k-1 n : v n ? V, k = 1, 2, ? ? ? , K}), (<label>3</label></formula><formula xml:id="formula_8">)</formula><p>where K is the number of layers of the GNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Erasing Operation</head><p>The key innovation of our proposed GraphCoCo approach is the erasing operation which enforces the encoder to learn complementary embeddings. GraphCoCo aims at mining information of inconspicuous dimensions rather than being restricted to scarce common "highlighted" dimensions through an adversarial manner. After the GNN encoder encodes two augmented views { G(1) , G(2) } into embeddings {r (1) , r (2) }, the embedding of the second augmented view r (2) are erased with the guidance of r (1) . The first embedding r (1) recognizes the "highlighted" dimensions whose values are larger than a pre-defined threshold. The corresponding dimensions of r (2) are then erased through masking values with zeros. Note that the erasing operation is performed only on positive pairs but not on negative pairs. Formally, the augmented views of the input graph are transformed by the GNN encoder g ? (?) into a pair of embeddings r (i) ? R K?F , where i ? {1, 2}, K is the number of layer of GNN, and F is the number of output dimensions in each layer. Then normalize the r (1) to the range [0, 1] with minmax scaling and denote it as r (1) , r (1) = r (1) -min (r (1) ) max (r (1) ) -min (r (1) ) .</p><p>We recognize dimensions whose values are larger than the hyper-parameter threshold ? as "highlighted" dimensions. Then a binary mask matrix M ? {0, 1} K?F is created to conduct the erasing operation on the embedding of second augmented view r (2) as follows:</p><formula xml:id="formula_9">M ij = 0, if r (1) ij &gt; ? 1, otherwise<label>(4)</label></formula><p>The erased embedding r(2) as the complementary embedding can be obtained,</p><formula xml:id="formula_10">r(2) = r (2) M , Algorithm 1 Training algorithm of GraphCoCo Input: Training set G = {G j } |G| j=1</formula><p>, GNN encoder g ? (?), augmentation distribution T , threshold ?, mask matrix M , batch size B Output:The pre-trained encoder g ? (?)</p><p>1: Randomly initialize parameters ? of the GNN encoder and set all elements of M to be 0. 2: for each minibatch B sampled from G do 3:</p><formula xml:id="formula_11">for k = 1, 2, ? ? ? , B do 4:</formula><p>Select two augmentation methods t 1 , t 2 from T 5:</p><formula xml:id="formula_12">G(1) k ? t 1 (G k ), G(2) k ? t 2 (G k ) 6: r (1) k ? g ? ( Gk (1)), r<label>(2)</label></formula><formula xml:id="formula_13">k ? g ? ( G(2) k ) 7: M pq = 1 if |r (1)</formula><p>k | pq &gt; ? where p, q are indexes of elements in tensors. 8:</p><formula xml:id="formula_14">r(2) k ? r (2) k M 9: z (1) k,? ? h(r (1) k ), z (2) k,+ ? h(r 2 k ), z<label>(2)</label></formula><p>k,-? h(r 2 k )</p><p>10:</p><p>Computer the loss L via Eq. 5.</p><p>11:</p><p>Update the parameters of g ? (?) and h(?) with adam optimizer by mininizing L.</p><p>12:</p><p>end for 13: end for 14: return The GNN encoder g ? (?)</p><p>where is the Hadamard product. A projection head h(?) composed of 2-layer MLP and ReLU non-linearity is applied on all embeddings for optimization objective, z</p><formula xml:id="formula_15">? = h(r (1) ), z<label>(1)</label></formula><formula xml:id="formula_16">+ = h(r (2) ), z<label>(2)</label></formula><formula xml:id="formula_17">-= h(r (2) ),<label>(2)</label></formula><p>where ? denotes positive and negtive pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Model Training</head><p>We follow <ref type="bibr" target="#b22">[You et al., 2020]</ref> and train the GraphCoCo endto-end by maximizing the agreement between positive pairs {z</p><formula xml:id="formula_18">(1) + , z<label>(2)</label></formula><p>+ } via a contrastive loss. We use the Info Noise-Contrastive Estimation (InfoNCE) <ref type="bibr" target="#b13">[Oord et al., 2018]</ref> which is a lower bound of the mutual information as the training objective. Specifically, given a minibatch of n graphs {G j } n j=1 , let z</p><p>(1) <ref type="table"></ref>and<ref type="table">z</ref> (2) j,-= h(g ? (t 2 (G j ))), where t i (?) is the augmentation function, g ? (?) is the GNN encoder, h(?) is the projection header, M is the masking matrix in Eq. 4, and {+, -} stands for positive or negative pairs. With sim(?, ?) denoting the cosine similarity, the loss for the mini-batch with size B is:</p><formula xml:id="formula_19">j,? = h(g ? (t 1 (G j ))), z (2) j,+ = h(g ? (t 2 (G j )) M ) ,</formula><formula xml:id="formula_20">L = - 1 n B j=1 k?{1,2} log exp(sim(z (1) j,+ , z (2) j,+ )/? ) exp(sim(z (1) j,+ , z (2) j,+ )/? ) + ?- ,<label>(5)</label></formula><p>where ? -= m j =1,j =j,q?{1,2} exp(sim(z</p><formula xml:id="formula_21">(k) j,-, z (q) j ,-)/? ).</formula><p>The training process is described in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Protocol</head><p>Task and datasets. We compare state-of-the-art methods in the settings of unsupervised graph classification tasks and  AD-GCL <ref type="bibr" target="#b16">[Susheel et al., 2021]</ref>, where the last four methods are state-of-the-art contrast-based graph representation learning methods. For node classification tasks, we compare the GraphCoCo with supervised methods including GCN <ref type="bibr" target="#b9">[Kipf and Welling, 2017]</ref> and GIN <ref type="bibr" target="#b20">[Xu et al., 2019]</ref> and contrastbased self-supervised methods including DGI <ref type="bibr" target="#b19">[Veli?kovi? et al., 2019]</ref>, MVGRL [Hassani and Khasahmadi, 2020], GCA <ref type="bibr" target="#b27">[Zhu et al., 2021]</ref> and CCA-SSG <ref type="bibr" target="#b26">[Zhang et al., 2021]</ref>.</p><p>Implementation Details. For graph classification tasks, we leverage the graph isomorphism network (GIN) <ref type="bibr" target="#b20">[Xu et al., 2019]</ref> as the encoder due to its powerful expressiveness in distinguishing the structure of graphs to obtain the graph-level representation. Specifically, we adopt a three-layer GIN with 32 hidden units in each layer and a sum pooling readout function. Then the embeddings generated by the encoder are fed into the downstream SVM classifier. The threshold ? in the erasing operation is set to 0.7. We utilize the 10-fold crossvalidation to train the SVM and record mean accuracy with the standard variation of five-time trials on the test set. Other hyper-parameters remain consistency with the GraphCL <ref type="bibr" target="#b22">[You et al., 2020]</ref>. For node classification tasks, we follow settings of DGI <ref type="bibr" target="#b19">[Veli?kovi? et al., 2019]</ref> which uses the GCN as the encoder and logistic regression downstream classifier. More details of the experimental setup can be found in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experimental Results</head><p>The results of self-supervised graph classification are reported in Tab. 1. We can see contrast-based methods generally exceed both the graph kernel methods and traditional unsupervised methods, indicating the advantages of contrastive learning. GraphCoCo outperforms other unsupervised representation learning baselines with significant improvement across eight of nine datasets, especially on sparsegraph, demonstrating the superiority of our approach. For example, GraphCoCo achieves 82.07% accuracy on densegraph dataset DD, surpassing GraphCL by 3.45% accuracy and CuCo by 2.87% accuracy individually. Meanwhile, the GraphCoCo achieves 73.83% accuracy on sparse-graph dataset IMDB-B, exceeding the GraphCL by 2.69% and AD-GCL by 2.34% accuracy, respectively. The results are attributed to the key component in our approach: the nonmaximization erasing operation, which allows information to be represented in all dimensions of embeddings rather than concentrating on a small number of the highlighted dimensions. The similarity between two embeddings is no longer dominated only by "highlighted" dimensions in both embeddings as in previous works. Finally, with a more uniform distribution of information, all dimensions of the embeddings contribute when performing downstream classification tasks, bringing about a notable improvement. Tab. 2 reports the results of node classification. The GNN encoder in the graph classification task has one readout layer, which is not in the encoder of the node classification task. We design our approach from the gradient of InfoNCE loss w.r.t the graph-level embeddings which is the output of the readout layer. Thus our approach is more suitable for graph-level classification. Even so, results show that our method extended to node classification achieves competitive performance compared to the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Ablation Study</head><p>Comprehensive ablation studies on graph classification are conducted to verify the superiority of our non-maximization erasing operation for learning complementary embeddings. We design following 4 variants of the proposed GraphCoCo:</p><p>(1) GraphCoCo (w/o EO): removes the erasing operation component. Note that this variant is the same as GraphCL which maximizes the similarity between embeddings of positive pairs. (2) GraphCoCo-rand: for positive pairs, randomly mask some dimensions of embedding of the second augmented view with zeros, which is equivalent to applying a dropout function. To make sure the ratio of masked dimensions in the variant is the same as in GraphCoCo with the optimal hyper-parameter threshold ?, as shown in Fig. <ref type="figure">4</ref>, we implement the GraphCoCo-rand by randomly shuffling M in Eq. 4.</p><p>(3) GraphCoCo-non-min: for positive pairs, the dimensions of the second embedding to be erased are the smallest dimensions of the first embedding instead of the largest ones. (4) GraphCoCo-bi: bi-directional erasing operation, i.e., for positive pair, each embedding of augmented view conducts the non-max erasing operation on the other.</p><p>Fig. <ref type="figure">3</ref> compares the results of GraphCoCo and its variants, from which we make the following observations. First, the classification results decrease if the non-maximization erasing operation is removed, verifying the efficacy of leaning the complementary embedding. GraphCoCo-rand improves the GraphCL but does not outperform the GraphCoCo. GraphCoCo-bi has the similar performance with GraphCoCo. Lastly, the non-min erasing operation can hardly improve and even hurts the model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Parameter Sensitivity Study</head><p>Sensitivity study w.r.t. the erasing threshold ?. We vary the values of the import hyper-parameter threshold ? in the erasing operation in Eq. 4 from 0.0 to 1.0 on four datasets, and the results are shown in Fig. <ref type="figure">4</ref>. Surprisingly, even if the second embedding is completely erased when ? = 0, we can still optimize the encoder by repulsing negative pairs and get a barely satisfactory result. In general, we find that when the threshold ? is less than 0.7, the classification accuracy grows with the increase of the threshold. The optimal value of ? is 0.7 or 0.8 for most datasets, with the exception of dataset NCI1, where the optimal value is 0.4. Effect of intensity of augmentations. Since the sparsity of augmented views affects the common highlighted dimensions of both embeddings, we vary the ratio of nodes, edges or features discarded in graph augmentation including NodeDrop, EdgeDrop and FeatureMasking on four datasets. From the results in Figure <ref type="figure">5</ref>, we observe that classification performance degenerates as the intensity of augmentation grows overly high. The optimal modification probability for most datasets is 0.1 to 0.3. These results are in record with the observation that graph-data are sparse and hard to be recovered after discarding information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Transfer Learning Study</head><p>We conduct experiments on four large-scale datasets to evaluate the transferability in predicting the molecular property. The encoder is pre-trained on the ZINC dataset without label and fine-tuned on other datasets, where all settings follow <ref type="bibr" target="#b8">[Hu et al., 2020]</ref>. We select baselines including no-pretrained GIN, GraphCL <ref type="bibr" target="#b22">[You et al., 2020]</ref> and strategies used in <ref type="bibr" target="#b8">[Hu et al., 2020]</ref> including EdgePred, AttrMasking and ContextPred. The experiment result is shown in Tab. 3 evaluated with mean and standard deviation of ROC-AUC score for five trials. GraphCoCo achieves the best performance on three of four datasets and outperforms GraphCL on all datasets. Detailed setup of transfer learning is in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This paper investigates the gaps between contrastive loss and downstream tasks (graph/node classification) performance and proposes the graph complementary contrastive learning in a self-supervised manner named GraphCoCo. In Graph-CoCo, the first embedding of positive pairs discovers the most significant dimensions and erases these dimensions of the second embedding, which is regarded as the complement of the first. The complementary embedding helps the encoder learn neglected information and enhance the distinguishability of the embedding. Extensive experiments show that GraphCoCo significantly outperforms the state-of-the-art self-supervised methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the 64-dimensional embeddings of two augmented views of the first graph in COLLAB dataset extracted by non-trained encoder. Two embeddings have the similar patterns with common "highlighted" dimensions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Overview of GraphCoCo which here involve two augmented views of each input graph. A shared GNN encoder g ? (?) encodes each augmented view as low-dimensional embeddings, where the darker color represents the larger value in that dimension. For positive pairs, the first embedding identifies significant dimensions whose values are larger than a hyper-parameter threshold, and erases these dimensions of the second embedding. While the erasing operation is not performed for negative pairs. Finally, the parameters of the shared encoder ? are optimized by minimizing the objective function with gradient descent.</figDesc><graphic url="image-16.png" coords="3,158.22,123.17,76.46,64.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :Figure 5 :</head><label>345</label><figDesc>Figure 3: Comparison of four variants on five datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Unsupervised graph classification result comparison (% with standard deviation) of GraphCoCo and baselines.</figDesc><table><row><cell>Datasets</cell><cell>PTC-MR</cell><cell>PROTEINS</cell><cell>NCI1</cell><cell>DD</cell><cell>COLLAB</cell><cell>IMDB-B</cell><cell>IMDB-M</cell><cell>RDT-B</cell><cell>RDT-M5K</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Supervised Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GCN</cell><cell>64.2 ? 4.3</cell><cell>76.2 ? 2.8</cell><cell>80.2 ? 2.0</cell><cell>76.2 ? 1.4</cell><cell>79.0 ? 1.8</cell><cell>74.0 ? 3.4</cell><cell>51.9 ? 3.8</cell><cell>&lt; 50.0</cell><cell>&lt; 20.0</cell></row><row><cell>GIN</cell><cell>64.6 ? 7.0</cell><cell>76.6 ? 3.2</cell><cell>82.7 ? 1.7</cell><cell>78.9 ? 1.3</cell><cell>80.2 ? 1.9</cell><cell>75.1 ? 5.1</cell><cell>52.3 ? 2.8</cell><cell>92.4 ? 2.5</cell><cell>57.0 ? 1.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Graph Kernel Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>WL</cell><cell>57.97 ? 0.49</cell><cell>72.92 ? 0.56</cell><cell>80.01 ? 0.50</cell><cell>79.78 ? 0.36</cell><cell>69.30 ? 3.42</cell><cell>72.30 ? 3.44</cell><cell>46.95 ? 0.46</cell><cell>68.82 ? 0.41</cell><cell>46.06 ? 0.21</cell></row><row><cell>DGK</cell><cell>60.08 ? 2.55</cell><cell>73.30 ? 0.82</cell><cell>80.31 ? 0.46</cell><cell>74.85 ? 0.74</cell><cell>64.66 ? 0.50</cell><cell>66.96 ? 0.56</cell><cell>44.55 ? 0.52</cell><cell>78.04 ? 0.39</cell><cell>41.27 ? 0.18</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Unsupervised Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sub2Vec</cell><cell>59.99 ? 6.38</cell><cell>53.03 ? 5.55</cell><cell>52.84 ? 1.47</cell><cell>54.33 ? 2.44</cell><cell>55.26 ? 1.54</cell><cell>55.32 ? 1.52</cell><cell>36.57 ? 0.82</cell><cell>71.48 ? 0.42</cell><cell>36, 68 ? 0.42</cell></row><row><cell>Graph2Vec</cell><cell>60.17 ? 6.86</cell><cell>73.30 ? 2.05</cell><cell>73.22 ? 1.81</cell><cell>79.32 ? 2.32</cell><cell>71.10 ? 0.54</cell><cell>71.12 ? 0.47</cell><cell>50.43 ? 0.88</cell><cell>75.78 ? 1.03</cell><cell>46.86 ? 0.26</cell></row><row><cell>InfoGraph</cell><cell>61.65 ? 1.43</cell><cell>74.44 ? 0.31</cell><cell>76.20 ? 1.06</cell><cell>72.85 ? 0.31</cell><cell>70.65 ? 1.13</cell><cell>73.02 ? 0.93</cell><cell>49.65 ? 9.52</cell><cell>82.50 ? 1.42</cell><cell>53.46 ? 1.03</cell></row><row><cell>GraphCL</cell><cell>63.62 ? 1.83</cell><cell>74.39 ? 0.45</cell><cell>77.87 ? 0.41</cell><cell>78.62 ? 0.40</cell><cell>71.36 ? 1.15</cell><cell>71.14 ? 0.44</cell><cell>50.69 ? 0.43</cell><cell>89.53 ? 0.84</cell><cell>55.99 ? 0.28</cell></row><row><cell>CuCo</cell><cell>64.43 ? 1.57</cell><cell>75.91 ? 0.55</cell><cell>79.24 ? 0.56</cell><cell>79.20 ? 1.12</cell><cell>72.30 ? 0.34</cell><cell>71.98 ? 1.23</cell><cell>51.32 ? 1.89</cell><cell>88.60 ? 0.55</cell><cell>56.49 ? 0.19</cell></row><row><cell>AD-GCL</cell><cell>65.23 ? 1.34</cell><cell>75.04 ? 0.48</cell><cell>75.86 ? 0.62</cell><cell>75.73 ? 0.51</cell><cell>74.89 ? 0.90</cell><cell>71.49 ? 0.98</cell><cell>52.34 ? 1.29</cell><cell>92.35 ? 0.79</cell><cell>56.24 ? 0.43</cell></row><row><cell cols="8">GraphCoCo 68.80 ? 2.12 78.01 ? 1.23 81.04 ? 0.94 82.07 ? 1.25 75.04 ? 0.98 73.83 ? 1.76 54.25 ? 2.13</cell><cell>90.79 ? 1.23</cell><cell>57.58 ? 1.05</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Unsupervised node classification result comparison (% with standard deviation) of GraphCoCo and baselines.</figDesc><table><row><cell>Datasets</cell><cell>Pubmed</cell><cell>CS</cell><cell>Photo</cell><cell>Computer</cell></row><row><cell></cell><cell></cell><cell cols="2">Supervised Methods</cell><cell></cell></row><row><cell>GCN</cell><cell>79.0</cell><cell>91.8 ? 0.1</cell><cell>87.3 ? 1.0</cell><cell>86.5 ? 0.5</cell></row><row><cell>GAT</cell><cell>79.0 ? 0.3</cell><cell>90.5 ? 0.7</cell><cell>86.2 ? 1.5</cell><cell>86.9 ? 0.3</cell></row><row><cell></cell><cell cols="3">Self-supervised Methods</cell><cell></cell></row><row><cell>DGI</cell><cell>77.3 ? 0.6</cell><cell>90.0 ? 0.3</cell><cell>83.1 ? 0.5</cell><cell>83.9 ? 0.5</cell></row><row><cell>MVGRL</cell><cell>80.1 ? 0.7</cell><cell>92.1 ? 0.1</cell><cell>87.3 ? 0.3</cell><cell>87.5 ? 0.1</cell></row><row><cell>GCA</cell><cell>80.7 ? 0.2</cell><cell>92.95 ? 0.12</cell><cell>92.24 ? 0.21</cell><cell>87.85 ? 0.31</cell></row><row><cell>CCA-SSG</cell><cell>81.6 ? 0.4</cell><cell cols="3">93.31 ? 0.22 93.14 ? 0.14 88.74 ? 0.28</cell></row><row><cell cols="2">GraphCoCo 82.23 ? 0.54</cell><cell>91.67 ? 0.33</cell><cell>92.79 ? 0.17</cell><cell>87.49 ? 0.35</cell></row></table><note><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p>further extend to node classification, where we can only access the adjacency matrix and node features of graphs without label information. We conduct experiments on nine benchmark datasets</p><ref type="bibr" target="#b11">[Morris et al., 2020]</ref></p>, including four bioinformatics datasets (PTC-MR, PROTEINS, NCI1, DD) and five social network datasets (COLLAB, IMDB-BINARY, IMDB-MULTI, REDDIT-BINARY, and REDDIT-MULTI-5K) for graph classification tasks and four datasets (Pubmed, Coauthor-CS, Amazon-Photo, Amazon-Computer) for node classification tasks. Details of datasets are in Appendix C. Baselines. We compare the GraphCoCo with three groups of baselines for graph classification tasks. First group is supervised graph neural network including GCN</p><ref type="bibr" target="#b9">[Kipf and Welling, 2017]</ref> </p>and GIN</p><ref type="bibr" target="#b20">[Xu et al., 2019]</ref></p>. Second group includes the graph kernel methods: Weisfeiler-Lehman Subtree kernel (WL)</p><ref type="bibr" target="#b14">[Shervashidze et al., 2011]</ref> </p>and Deep Grph Kernels (DGK)</p><ref type="bibr" target="#b20">[Yanardag and Vishwanathan, 2015]</ref></p>. The last group of baselines are unsupervised graph representation learning methods including Sub2Vec</p><ref type="bibr" target="#b0">[Adhikari et al., 2018]</ref></p>, Graph2Vec</p><ref type="bibr" target="#b12">[Narayanan et al., 2017]</ref></p>, InfoGraph</p><ref type="bibr" target="#b15">[Sun et al., 2020]</ref></p>, GraphCL</p><ref type="bibr" target="#b22">[You et al., 2020]</ref></p>, CuCo</p><ref type="bibr" target="#b4">[Chu et al., 2021],</ref> </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Transfer learning comparison by ROC-AUC. GraphCoCo 70.34 ? 1.21 63.04 ? 0.92 61.45 ? 1.63 76.92 ? 1.91</figDesc><table><row><cell>Pre-train</cell><cell></cell><cell cols="2">ZINC 2M</cell><cell></cell></row><row><cell>Datasets</cell><cell>BBBP</cell><cell>ToxCast</cell><cell>SIDER</cell><cell>ClinTox</cell></row><row><cell>No-pre-train</cell><cell>65.8 ? 4.5</cell><cell>63.4 ? 0.6</cell><cell>57.3 ? 1.6</cell><cell>58.0 ? 4.4</cell></row><row><cell>EdgePred</cell><cell>67.3 ? 2.4</cell><cell>64.1 ? 0.6</cell><cell>60.4 ? 0.7</cell><cell>64.1 ? 3.7</cell></row><row><cell>AttrMasking</cell><cell>64.3 ? 2.8</cell><cell>64.2 ? 0.5</cell><cell>61.0 ? 0.7</cell><cell>71.8 ? 4.1</cell></row><row><cell>ContexPred</cell><cell>68.0 ? 2.0</cell><cell>63.9 ? 0.6</cell><cell>60.9 ? 0.6</cell><cell>65.9 ? 3.8</cell></row><row><cell>GraphCL</cell><cell>69.68 ? 0.67</cell><cell>62.40 ? 0.57</cell><cell>60.53 ? 0.88</cell><cell>75.99 ? 2.65</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName><forename type="first">Abu-El-Haija</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PAKDD</title>
		<imprint>
			<date type="published" when="2018">2019. 2019. 2018</date>
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName><forename type="first">Bruna</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><forename type="middle">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.02803</idno>
		<title level="m">Intriguing properties of contrastive losses</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hinton. A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cuco: Graph representation with curriculum contrastive learning</title>
		<author>
			<persName><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Graph neural networks for social recommendation</title>
		<author>
			<persName><forename type="first">Fan</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hassani and Khasahmadi, 2020] K. Hassani and A. H. Khasahmadi. Contrastive multi-view representation learning on graphs</title>
		<author>
			<persName><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017. 2017. 2020</date>
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Welling ;</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Self-organization in a perceptual network</title>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">R</forename><surname>Linsker</surname></persName>
		</author>
		<author>
			<persName><surname>Linsker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="105" to="117" />
			<date type="published" when="1988">1988. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Tudataset: A collection of benchmark datasets for learning with graphs</title>
		<author>
			<persName><surname>Morris</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08663</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><surname>Narayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05005</idno>
		<title level="m">Learning distributed representations of graphs</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><surname>Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gcc: Graph contrastive coding for graph neural network pre-training</title>
		<author>
			<persName><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESWC</title>
		<imprint>
			<date type="published" when="2011">2020. 2020. 2018. 2011</date>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
	<note>Weisfeilerlehman graph kernels</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adversarial graph augmentation to improve graph contrastive learning</title>
		<author>
			<persName><surname>Susheel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On mutual information maximization for representation learning</title>
		<author>
			<persName><surname>Tschannen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><surname>Veli?kovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep graph infomax</title>
		<author>
			<persName><surname>Veli?kovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reducing word omission errors in neural machine translation: A contrastive learning approach</title>
		<author>
			<persName><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015">2019. 2019. 2015. 2015. 2019. 2019</date>
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">xMoCo: Cross momentum contrastive learning for open-domain question answering</title>
		<author>
			<persName><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Graph contrastive learning automated</title>
		<author>
			<persName><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spatio-temporal graph convolutional networks: a deep learning framework for traffic forecasting</title>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adversarial complementary learning for weakly supervised object localization</title>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">From canonical correlation analysis to self-supervised graph neural networks</title>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Graph contrastive learning with adaptive augmentation</title>
		<author>
			<persName><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
