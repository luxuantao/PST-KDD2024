<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-instance Deep Learning: Discover Discriminative Local Anatomies for Bodypart Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Zhennan</forename><surname>Yan</surname></persName>
							<email>zhennany@cs.rutgers.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yiqiang</forename><surname>Zhan</surname></persName>
							<email>yiqiang.zhan@siemens.com</email>
						</author>
						<author>
							<persName><forename type="first">Zhigang</forename><surname>Peng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shu</forename><surname>Liao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yoshihisa</forename><surname>Shinagawa</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
							<email>szhang16@uncc.edu</email>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiang</forename><forename type="middle">Sean</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<settlement>Healthcare</settlement>
									<region>Malvern, PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Comput-er Science</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<settlement>Piscataway</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of North Carolina</orgName>
								<address>
									<settlement>Charlotte</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-instance Deep Learning: Discover Discriminative Local Anatomies for Bodypart Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A3F35A0AB54C920764A339E38AADBA89</idno>
					<idno type="DOI">10.1109/TMI.2016.2524985</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMI.2016.2524985, IEEE Transactions on Medical Imaging This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMI.2016.2524985, IEEE Transactions on Medical Imaging</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CNN</term>
					<term>Multi-instance</term>
					<term>Multi-stage</term>
					<term>Discriminative Local Information Discovery</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In general image recognition problems, discriminative information often lies in local image patches. For example, most human identity information exists in the image patches containing human faces. The same situation stays in medical images as well. "Bodypart identity" of a transversal slicewhich bodypart the slice comes from -is often indicated by local image information, e.g. a cardiac slice and an aorta arch slice are only differentiated by the mediastinum region. In this work, we design a multi-stage deep learning framework for image classification and apply it on bodypart recognition. Specifically, the proposed framework aims at: 1) discover the local regions that are discriminative and non-informative to the image classification problem, and 2) learn a image-level classifier based on these local regions. We achieve these two tasks by the two stages of learning scheme, respectively. In the pre-train stage, a convolutional neural network (CNN) is learned in a multiinstance learning fashion to extract the most discriminative and and non-informative local patches from the training slices. In the boosting stage, the pre-learned CNN is further boosted by these local patches for image classification. The CNN learned by exploiting the discriminative local appearances becomes more accurate than those learned from global image context. The key hallmark of our method is that it automatically discovers the discriminative and non-informative local patches through multiinstance deep learning. Thus, no manual annotation is required. Our method is validated on a synthetic dataset and a large scale CT dataset. It achieves better performances than state-of-the-art approaches, including the standard deep CNN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(CAD) systems. Since different organ systems have highly diverse characteristics, medical image analysis methods/models are often designed/trained for specific anatomies to incorporate prior knowledge, e.g. organ shape <ref type="bibr" target="#b0">[1]</ref>. To benefit real-world clinical workflows, these algorithms are desired to be invoked automatically for applicable datasets. Therefore, it is important to automatically identify the human bodypart contained in the medical image in the first place. However, compared to the extensively investigated organ segmentation and landmark detection topics, automatic bodypart recognition (identify the human bodypart contained in the medical image) is still less explored.</p><p>In fact, auto-bodypart recognition algorithm may benefit radiological workflow in different aspects. For example, the current imaging workflow requires the planning of the scanning range in topogram or scout scans. With a very reliable and fast bodypart recognition algorithm, this planning step may be conducted on-the-fly to significantly save scanning time. Another example is the bodypart-based query in PACS system. Since the bodypart information in DICOM header is not very reliable [2], an automatic bodypart recognition will enable content-based image retrieval and improve the retrieval precision. Besides the aforementioned "direct" benefits, bodypart recognition algorithm also paves the way to other higher level medical image interpretation tasks. Bodypart recognition can serve as an initialization module for anatomy detection or segmentation algorithms. Given the bodypart information, the search range of the following detection/segmentation algorithms can be reduced, hence, the algorithm speed and robustness are improved. Moreover, with the availability of more and more intelligent medical image analysis algorithms, radiologists hope that medical images could have been "preprocessed" by all applicable auto-algorithms before being loaded for manual reading. In this way, the automatic results can be displayed instantaneously in the reading room to speed up the reading process. In this scenario, a robust bodypart recognition algorithm again becomes important to gate the intelligent algorithms properly for meaningful results.</p><p>It is worth noting that although DICOM header includes bodypart information, text-based retrieval methods still face three major challenges. First, it may contain around 15% errors [2] and thereby limit the accuracy of text-based bodypart recognition. Second, text information in DICOM is highly abstract and may not precisely describe the anatomies contained in the scan. For example, it is difficult to tell if a scan with DICOM tag (0018,0015)="TSPINE" includes the mid-part of the liver. In addition, the multi-language nature of DICOM bodypart information becomes another barrier for text-based retrieval. On the contrary, a reliable image-based bodypart recognition algorithm can tackle all these three challenges by leveraging the intrinsic anatomical appearance information.</p><p>CT and MR are two common forms of medical imaging scans. A CT/MR sequence is usually a 3D volume image consisted of a series of 2D slices. This paper focuses on the bodypart identification in a 2D transversal slice, namely "slicebased bodypart recognition". Specifically, we divide human body into continuous sections according to anatomical context as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Given a 2D transversal slice, the task of a slice-based bodypart recognition algorithm is to identify which section the slice belongs to. Although 3D volume always contain more comprehensive anatomy information, based on which the bodypart recognition can be more accurate, this study only aims at slice-based bodypart recognition for two reasons. First, in some real-world systems, 3D volume is not always accessible. For example, in a client-server application, the server end might only receive the 3D volume data sliceby-slice due to the limited network speed but need to output bodypart information instantaneously. Second, 2D slice-based bodypart recognition provides the foundation of 3D bodypart identification. Given the bodypart identities of all slices of a 3D volume, the 3D bodypart can be straightforwardly derived.</p><p>Slice-based bodypart recognition is essentially a multiclass image classification problem, which has been extensively studied for decades. In general, image classification algorithms consist of feature extraction and classification modules. Based on the different design principles of these two modules, various image classification algorithms can be categorized into three groups. The first group uses carefully hand-crafted features followed by classifiers without feature selection capability, e.g. SVM and logistic regression <ref type="bibr" target="#b1">[3]</ref>, <ref type="bibr" target="#b2">[4]</ref>. The second group extends the feature set to a feature pool derived from some feature basis, e.g., Haar mother functions. Since the feature pool often includes thousands of features, the following classification modules need to have the feature selection capability, e.g., Adaboost <ref type="bibr" target="#b3">[5]</ref>, random forest <ref type="bibr" target="#b4">[6]</ref>. The third group comes from the latest achievements of the deep learning research. Instead of designing any features, those algorithms <ref type="bibr" target="#b5">[7]</ref>, <ref type="bibr" target="#b6">[8]</ref> aim to learn both the features and classifiers jointly from the data. As the features are learned for specific image classification tasks, they often have more discriminative power, hence, achieve better classification performance than those ad-hoc designed ones.</p><p>In slice-based bodypart recognition, it is difficult to "design" common features that work well for different body parts, due to diverse appearances in different body sections and large variability between subjects. Thus, deep learning technology, which learns features and classifiers simultaneously, becomes a promising solution. However, slice-based bodypart recognition has its unique challenge which might not be solved by standard deep learning. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, although image 7 and 8 come from aorta arch and cardiac sections, respectively, their global appearance characteristics are quite similar. For these two slices, the discriminative information only resides in the local mediastinum region (indicated by the yellow boxes). The rest areas are just "non-informative" for classification. Although the standard deep learning framework is able to learn some low-level and abstract features from global context, it cannot learn local patches that are most discriminative for bodypart recognition. The "non-informative" regions here may mislead the classifier to recognize these two sections as identical. Hence, the classification power of the learned deep network may be limited. In fact, this problem also exists in general image classification/recognition applications. For example, in face recognition, Taigman et al. <ref type="bibr" target="#b7">[9]</ref> shows that deep learning can show its power only after the face (the local region of interest) is properly localized. However, while face is a well defined object and can be detected by mature algorithms, the discriminative local regions for bodypart recognition are not easy to define, not to mention that the effort to build these local detectors might be quite large.</p><p>In summary, two key questions need to be answered to tackle the challenge of the slice-based bodypart recognition. First, which local regions are discriminative or noninformative for bodypart recognition? Second, how to learn the local bodypart identifiers without time-consuming manual annotations? We answer these questions using a multi-stage deep learning scheme. In the pre-train stage, a convolutional neural network (CNN) is learned in a multi-instance learning fashion to "discover" the most discriminative local patches. Specifically, each slice is divided into several local patches. The deep network thus receives a set of labeled slices (bags), each containing multiple local patches (instances). The loss function of the CNN is adapted in a way that as long as one local patch (instance) is correctly labeled, the class of corresponding slice (bag) is considered to be correct. In this way, the pre-trained CNN will be more sensitive to the discriminative local patches than others. Based on the responses of the pre-trained CNNs, discriminative and non-informative local patches are selected to further boost the pre-trained CNN. This is the second stage (namely boosting stage) of our learning scheme. At run-time, a sliding window approach is employed to apply the boosted CNN to the subject image. As the CNN only has peaky responses on the discriminative local patches, it essentially identifies bodypart by focusing on the most distinctive local information and discarding noninformative local regions. Thanks to its ability to "discover" local discriminative local patches, this method is expected to identify bodypart more accurately and robust than global image context-based approaches.</p><p>The major contributions of this work include: 1. A multistage deep learning strategy is proposed to identify anatomical body parts by using discriminative local information; 2. The propsoed method does not require annotations of the discriminative local patches. Instead, it automatically discovers these local patches through multi-instance deep learning. Thus, our solution becomes highly scalable. 3. We validate our method on a large number of synthetic images and 7000+ CT slices.</p><p>In both experiments, it shows superior performance than stateof-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we will review relevant studies in two categories. First, we will review different bodypart recognition methods proposed in medical image analysis domain. Second, we will also review representative image recognition methods in general computer vision community, since slice-based bodypart recognition is technically similar to these problems.</p><p>Several bodypart recognition systems in medical imaging domain has been introduced in this decade. Park et al. <ref type="bibr" target="#b8">[10]</ref> proposed an algorithm to determine the body parts using energy information from Wavelet Transform. Look-up tables are designed to classify the imaging modality and body parts. Hong et.al. <ref type="bibr" target="#b9">[11]</ref> proposed a framework to identify different body parts from a whole-body scan. The method starts from establishing global reference frame and the head location. After determining the bounding box of the head, other body parts including neck, thorax cage, abdomen and pelvis, are localized one by one using different algorithms. In general, these approaches employ ad-hoc designed features and algorithms to identify major body parts which have globally variant appearances. Recently, more learning-based approaches are proposed for bodypart recognition. All of these methods essentially resort to the detection of specific organs or landmarks. In <ref type="bibr" target="#b10">[12]</ref>, Zhan et al. trained multiple, organ-specific classifiers and optimize the schedule of organ detections based on information theory. In <ref type="bibr" target="#b11">[13]</ref>, Criminisi et al. utilized regression forests for anatomy detection and localization and obtained better accuracy than their classification approach in <ref type="bibr" target="#b12">[14]</ref>. In <ref type="bibr" target="#b13">[15]</ref>, Donner et al. also trained regressor for anatomical landmark detection. However, since these organ/landmark-based recognition methods rely on a number of organ/landmark detectors, large manual annotation efforts are required in the training stage.</p><p>Technically, slice-based bodypart recognition is an image classification problem, which has been extensively studied in computer vision and machine learning communities. In general, existing image classification methods can be categorized into two groups, global information-based and local informationbased. Global information-based approaches extract features from the whole image. Conventional approaches often rely on carefully designed/selected features, e.g. gist <ref type="bibr" target="#b14">[16]</ref>, SIFT <ref type="bibr" target="#b15">[17]</ref>, Histogram of Oriented Gradients (HOG) <ref type="bibr" target="#b16">[18]</ref> and their variants. These features are extracted on either dense grids or a few interested points, organized as bag of words to provide statistical summary of the spatial scene layouts without any object segmentation <ref type="bibr" target="#b17">[19]</ref>. The framework of global representations followed by classical classifiers has been widely used in scene recognition <ref type="bibr" target="#b17">[19]</ref> and image classification <ref type="bibr" target="#b18">[20]</ref>, <ref type="bibr" target="#b19">[21]</ref>. With the latest advances of machine learning technology, deep learning based algorithms <ref type="bibr" target="#b5">[7]</ref>, <ref type="bibr" target="#b20">[22]</ref> have shown their superior in these tasks due to the ability of learning expressive nonlinear features and classifier simultaneously.</p><p>Roth et.al. <ref type="bibr" target="#b21">[23]</ref> presented a method for anatomy-specific classification of medical images using deep convolutional networks (ConvNets). They applied a trained deep CNN on 2D axial CT images to classify 5 bodyparts (neck, lungs, liver, pelvis and legs) and obtained the state-of-the-art accuracy (5.9% error). Their results demonstrated the power of deep learning in bodypart recognition. However, realworld applications may require a finer grained differentiation beyond these 5 bodyparts, e.g., aortic arch vs cardiac sections. Due to the globally-similar and locally-different appearance characteristics of these body sections, the CNNs trained on the whole axial images may not be able to differentiate them effectively.</p><p>Global information-based approaches achieved good performances in some image classification problems. However, it is not sufficient or appropriate to recognize images whose characteristics are exhibited by local objects, e.g., jumbled image recognition <ref type="bibr" target="#b22">[24]</ref>, multi-label image classification <ref type="bibr" target="#b6">[8]</ref>, <ref type="bibr" target="#b23">[25]</ref>, and scene classification <ref type="bibr" target="#b24">[26]</ref>, <ref type="bibr" target="#b25">[27]</ref>. On the contrary, local information-based approaches can achieve better performance here. In <ref type="bibr" target="#b26">[28]</ref>, Szegedy et al. utilized CNN for local object detection and recognition and achieves state-of-the-art performance on Pascal VOC database. However, the training stage requires manually annotated object bounding boxes, which is often time consuming. To avoid explicit local region or object annotation, different approaches have been emerged. Felzenszwalb et al. <ref type="bibr" target="#b27">[29]</ref> designed a part-based deformable model using local information of object parts for object recognition and detection. They assumed a star-structured model for object, then treated the object's part locations as latent variables during training. In <ref type="bibr" target="#b28">[30]</ref>, Singh et al. used unsupervised clustering method and one-vs-all linear SVM to train classifier for each cluster to discover the discriminative patches which can be used as visual words in spatial pyramid based classification. In another pioneer work, Wei et al. <ref type="bibr" target="#b6">[8]</ref> applied an existing general objectness detection (BING <ref type="bibr" target="#b29">[31]</ref>) to produce some candidate local windows from a given image, which are used to do multi-label. Recently, several studies have emerged to apply multi-instance learning (MIL) <ref type="bibr" target="#b26">[28]</ref>, <ref type="bibr" target="#b30">[32]</ref>- <ref type="bibr" target="#b32">[34]</ref> combined with CNN to better utilize local information in weakly supervised classification or segmentation tasks. For example, Wu et al. proposed a deep multi-instance learning framwork in a weakly supervised setting for image classification and auto-annotation based on object and keyword proposals <ref type="bibr" target="#b33">[35]</ref>. The object proposals are generated by existing methods, e.g. BING <ref type="bibr" target="#b29">[31]</ref>, and the keyword proposals are crawled from web texts using Baidu image search engine. Pinheiro et al. combined CNN and MIL to do pixel labeling in <ref type="bibr" target="#b34">[36]</ref>. They built their segmentation network over the already trained Overfeat <ref type="bibr" target="#b35">[37]</ref> and achieved better performance on Pascal VOC dataset than other weakly supervised methods. Papandreou et al. proposed a semantic image segmentation method in weakly or semi-supervised setting (with bouding boxes or image-level labels) <ref type="bibr" target="#b36">[38]</ref>. They combined CNN with a conditional random field (CRF) <ref type="bibr" target="#b37">[39]</ref> in a EM algorithm to inference the pixel-level labels. Hou et al. extended the EM based label inference method and combine the patch-level classifiers to predict image-level label in gigapixel resolution image classification <ref type="bibr" target="#b38">[40]</ref>.</p><p>Inspired by the latest advances in deep learning research, we propose a multi-instance multi-stage deep learning framework to recognize bodypart in CT slices. Compared to other learning-based approaches, our method only requires "weak" supervision at a global level, i.e., bodypart labels at image level. Our framework is able to discover the discriminative "local" information automatically. In this way, the annotation efforts in the training stage are dramatically reduced. This is in particular meaningful for medical image applications, since the annotations in medical images often require clinical expertise and high cost. This work is extended from our IPMI paper <ref type="bibr" target="#b39">[41]</ref> by fine-tuning the framework and re-organizing data sets for more extensive evaluations to explore the benefits and limitations of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>We design a multi-stage deep learning framework to discover local discriminative patches and build local classifiers. We start this section by the problem statement and notation definitions. The first learning stage is introduced in Sec. III-B, which aims to learn representative local image features in a supervised multi-instance fashion. Then we describe the second learning stage in Sec. III-C, in which some discriminative and non-informative local patches are extracted from images and used to boost learning to obtain a patch-based classifier for image recognition. In Sec. III-D, we discuss the run-time image classification strategy using the learned CNN model. At last, we show details of the implementation in Sec. III-E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Statement</head><p>Definitions: Slice-based bodypart recognition is a typical multi-class image classification problem for a learning algorithm. Denote X as the input slice/image, K as the number of body sections (classes), and l ∈ {1, ..., K} as the corresponding class label of X. The learning algorithm aims to find a function O : X → l. In traditional image classification frameworks, O is often defined as C(F(X)), where F(X) and C(.) denote the feature extractors and classifiers, respectively.</p><p>In the context of convolutional neural network (CNN), O becomes a multi-layer neural network. An example of standard CNN is shown in Fig. <ref type="figure" target="#fig_1">2</ref>, it has two convolutional layers (C1, C3), each followed by a max-pooling layer (S2, S4), one fully connected hidden layer (H5) receiving outputs of the last pooling layer, and one logistic regression (LR) layer </p><formula xml:id="formula_0">L 1 (W) = Xm∈T -log(P(l m |X m ; W)).<label>(1)</label></formula><p>where P(l m |X m ; W) indicates the probability of image X m being correctly classified as class l m using network coefficients W.</p><p>CNN has shown impressive performance in image classification tasks <ref type="bibr" target="#b5">[7]</ref>, <ref type="bibr" target="#b6">[8]</ref>. The success shall be attributed to its capability of modeling complex nonlinear functions and leveraging the context information of neighboring pixels. In these successful applications, standard CNN is conducted as a global learning scheme, which takes the entire image as input. However, in slice-based bodypart recognition, distinctive information often comes from local patches (as shown in Fig. <ref type="figure" target="#fig_0">1</ref>) and these local patches are distributed "inconsistently" at different positions of the slices. The intrinsic conflicts between "global learning scheme" and "local distinctive information" may limit the performance of standard CNN in bodypart recognition. (One may argue that CNN can still learn local features through its convolutional layers. However, this situation only holds while local features always appear at the similar location across different images, which is not the case of bodypart recognition.) We design a toy example to illustrate this problem. As shown in Fig. <ref type="figure" target="#fig_2">3a</ref>, we randomly position and combine 4 types of geometry elements, square, circle, triangle and diamond to synthesize two classes of binary images. While circle and diamond are allowed to appear in any classes, triangle and square are exclusively owned by Class1 and Class2, respectively (ref Sec. IV-A for more details). Using standard CNN that takes the whole image as input, the classification accuracy is ∼ 83% (row "SCNN" of Table (Otherwise, the accuracy shall be much higher due to the significant differences between "triangle" and "square". ) This problem will become trivial if we have the prior knowledge of the discriminative local patches and build local classifiers on them. However, in bodypart recognition, it is not easy to figure out the most discriminative local patches for different body sections. In addition, even with adhoc knowledge, annotating local patches and training local classifiers often takes large effort. The solution thus becomes non-scalable when bodysections are re-defined or the imaging modalities are changed.</p><p>To leverage the local information, more important, to automatically "discover" the discriminative local patches for different body sections, we design a two-stage CNN learning framework. It consists of pre-train and boosting stages, which will be detailed next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Learning stage I: Multi-instance CNN pre-train</head><p>In order to exploit the local information, CNN should take discriminative local patches instead of the entire slice as its input. Here, the key problem is how to automatically discover these local patches through learning. This is the major task of the first stage of our CNN learning framework. A multiinstance learning strategy is designed to achieve this goal.</p><p>Given a training set T = {X m , m = 1, ..., M } with corresponding labels l m . Each training image, X m , is divided into a set of local patches defined as L(X m ) = {x mn , n = 1, ..., N }. These local patches become the basic training samples of the CNN and their labels are inherited from the original images, i.e., all x mn ∈ L(X m ) share the same label l m . While the structure of CNN is still the same as the standard one, the loss function is adapted as:</p><formula xml:id="formula_1">L 2 (W) = Xm∈T -log( max xmn∈L(Xm) P(l m |x mn ; W)),<label>(2)</label></formula><p>where P(l m |x mn ; W) is the probability that the local patch x mn is correctly classified as l m using CNN coefficients W.</p><p>The new loss function is different from Eq. ( <ref type="formula" target="#formula_0">1</ref>) by adopting a multi-instance learning criterion. Here, each original training slice X m is treated as a bag consisting of multiple instances (local patches), {x mn }. Within each bag (slice), only the instance with the highest probability to be correctly classified is counted in the loss function. Such instance is considered as the most discriminative local patch of the image slice. Let R mn be the output vector of the CNN on local patch x mn . The l m th component of R mn represents the probability of x mn being correctly classified. As illustrated in Fig. <ref type="figure" target="#fig_6">4</ref> for the image classification task in toy example. This is exactly in accordance to the fact that these two classes are only distinguishable by "triangle" and "square". It proves that our method is able to discover the key local patches without manual annotation.</p><p>To ensure that the learned CNN will have stable high responses on discriminative local patches, a spatial continuity factor is further incorporated into the loss function as:</p><formula xml:id="formula_2">L 3 (W) = Xm∈T -log( max xmn∈L(Xm) x∈N(xmn) P(l m |x; W)),<label>(3)</label></formula><p>Here, N(x mn ) denotes the local patches in the neighborhood of x mn . Based on Eq. ( <ref type="formula" target="#formula_2">3</ref>), for each training slice, the local patch to be counted in the loss function is not the most individually discriminative one (i.e., with the highest probability of being correctly classified), but the one whose neighboring patches and itself are overall most discriminative. In this way, the selected discriminative local patches will be robust to image translation and artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Learning stage II: CNN boosting</head><p>In the second stage of our learning framework, the main task is to boost the pre-trained CNN using selected local patches, which is illustrated in Fig. <ref type="figure" target="#fig_7">5</ref>.</p><p>The first type of selected local patches are the discriminative ones, i.e., these local patches on which the pre-trained CNN have high responses at the corresponding classes. For each image X m , we select D discriminative local patches as:</p><formula xml:id="formula_3">A m = argmax D xmn∈L(Xm) P(l m |x mn ; Ŵ).<label>(4)</label></formula><p>Here, Ŵ is the coefficients of the pre-trained CNN. P(l m |x mn ; Ŵ) denotes the response of the pre-trained CNN on the local patch x mn corresponding to the correct class l m . argmax D (.) is the operator that returns the arguments of the largest D elements. We noticed that apart from the discriminative local patches, the remaining regions cannot be completely ignored in the boosting stage for two reasons. First, only selecting discriminative patches to boost classifier may lead to overfitting problems. Second, some "confusing" local patches may mislead the bodypart recognition. For example, the patches containing lung regions (green dashed boxes in Fig. <ref type="figure" target="#fig_0">1</ref>) appear in both aortic arch and cardiac sections. For these "confusing" patches, CNN   </p><formula xml:id="formula_4">••• ••• Learning Stage 1 •••••• •••••• ••• •••</formula><formula xml:id="formula_5">•••••• •••••• •••••• R 11 R 12 R 13 R 1n R 1N R M1 R M2 R M3 R Mn R MN x M1 x M2 x M3 x Mn x MN ••• ••• CNN x 11 x 12 x 13 x 1n x 1N ••• ••• Conv Conv MP MP Hidden LR</formula><formula xml:id="formula_6">••• 7 ••• Class 1 ••• 8 ••• K ••• ••• ••• K+1 ••• ••• CNN K + 1 X 1 •••••• X 2 X 3 •••••• X M-2 X M X M-1 ••• ••• ••• ••• ••• ••• Discriminative Non-informative Conv Conv MP</formula><p>MP Hidden LR </p><p>Recall the toy example, Fig. <ref type="figure" target="#fig_2">3b</ref> shows the selected discriminative and non-informative local patches. When the discrimi-native patches from Class1 and Class2 only contain triangle or square, respectively, the non-informative patches may include circle, diamond or background. This is exactly in accordance to the fact that these two classes are only distinguishable by "triangle" and "square". It provides the evidence that our method can discover key local patches without manually annotations.</p><p>After introducing the additional non-informative class, the CNN structure keeps the same as the pre-trained CNN, except the LR layer has an additional output (see shadowed box in the rightmost diagram of Fig. <ref type="figure" target="#fig_7">5</ref>) and the corresponding connections to the hidden layer. Since the pre-trained CNN already captured some discriminative local appearance characteristics, all network layers except the last one are initialized by inheriting their coefficients from the pre-trained CNN. These coefficients are further adapted by minimizing Eq. ( <ref type="formula" target="#formula_8">6</ref>):</p><formula xml:id="formula_8">L 4 (W) = x∈A B -log(P(l|x; W)),<label>(6)</label></formula><p>Here, A = {m=1,...,M } A m and B = {m=1,...,M } B m denote the discriminative and non-informative local patches selected from all training images, respectively. Note that since the non-informative local patches are not belonging to any body section class now, their responses on any body section class can be effectively suppressed during the CNN boosting stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Run-time Classification</head><p>The two-stage CNN learning algorithm is summarized as follows.</p><p>Input:</p><formula xml:id="formula_9">Scalars M , N , K, dataset (X m , l m ), ∀m ∈ {1, • • • , M }, CNN architecture Output:</formula><p>Boosted CNN coefficients W opt 1: Partition X m into N overlapping local regions x mn 2: Pre-train CNN on (x mn , l m ) using multi-instance loss function <ref type="bibr" target="#b1">(3)</ref>, and obtain optimized Ŵ 3: Extract A m and B m according to Eq. ( <ref type="formula" target="#formula_3">4</ref>) and ( <ref type="formula" target="#formula_7">5</ref>) 4: Assign label l m to each instance of A m , and label K + 1 to each of B m 5: Modify pre-trained CNN by adding one unit to LR, layers except LR inherit coefficients 6: Boost CNN on set A ∪ B using loss function <ref type="bibr" target="#b4">(6)</ref>, and obtain optimized W opt At runtime, the boosted CNN is applied for bodypart recognition in a sliding window fashion. The sliding window partitions a testing image X into N overlapping local patches L(X) = {x n , n = 1, ..., N }. For each local patch x n , the boosted CNN outputs a response vector with K + 1 components {P(k|x n ; W opt )|k = 1, ..., K + 1}, where W opt denotes the optimal coefficients of Eq. ( <ref type="formula" target="#formula_8">6</ref>). The class of the local patch x n is then determined as:</p><formula xml:id="formula_10">c(x n ) = argmax k∈{1,...,K+1} P(k|x n ; W opt ).<label>(7)</label></formula><p>Since the class K + 1 is an artificially constructed noninformative one, local patches belong to this class should be ignored in body section determination. Simply, the class (body section) of the testing slice X can be determined by its most discriminative patch x n * as:</p><formula xml:id="formula_11">C(X) = c(x n * ),<label>(8)</label></formula><p>x</p><formula xml:id="formula_12">n * = argmax xn∈L(X);c(xn) =K+1 P(c(x n )|x n ; W opt ).<label>(9)</label></formula><p>However, it is possible that the detected x n * is an outlier with different prediction than its neighbors. It would be more robust to fuse the prediction around its neighborhood to label the image. Therefore, we combine the class probabilities of the neighboring patches around the most discriminative patch to derive the image class as:</p><formula xml:id="formula_13">C(X) = argmax k∈{1,...,K} xn∈N(x n * ) P(k|x n ; W opt ),<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Implementation Details</head><p>In this study, we assume one middle level discriminative patch from an image is enough for the image classification. To discover the patches which are discriminative and representative for their image categories, the patch size should not be too small to include semantic information for the discriminative objects. To simplify analysis, we slide a fixedsize window to extract overlapping patches from images for training and testing. The patch size and step size is specified in experimental settings. We further analyze the sensitivity of patch size later. In learning stage II, since the patches per image are overlapping, the non-informative patches are selected with a spatial constraint that they should not appear neighboring to the discriminative patches.</p><p>In each of the two training stages, we train a CNN model similar to Fig. <ref type="figure" target="#fig_1">2</ref>. The following strategies are employed to improve the performance of the learned CNN. First, Rectified Linear Units (ReLUs) are used to map the neurons' output in convolutional layers. As shown in <ref type="bibr" target="#b5">[7]</ref>, <ref type="bibr" target="#b41">[43]</ref>, ReLUs demonstrates faster convergence and better performance than functions. Second, to incorporate larger variability in our training samples, hence, increase the robustness of the CNN, we augment data using label-preserving transformations <ref type="bibr" target="#b20">[22]</ref>, <ref type="bibr" target="#b42">[44]</ref>. Specifically, we simply apply up to 10% (relate to image size) random translation to increase training data samples. Third, the "dropout" strategy <ref type="bibr" target="#b43">[45]</ref> is employed to reduce the risk of "over-fitting". It forces half of the neurons randomly "dropped out" at each training iteration. In this way, the complex correlation of neurons is reduced and more robust features can be learned. Finally, as the training set may be too large to load into memory at one time, we trained our model using a mini batch of samples at each iteration. The optimization is implemented by stochastic gradient descent with a momentum term β <ref type="bibr" target="#b44">[46]</ref> and a weight decay term γ. For a weight ω ∈ W, its update at iteration t is defined as</p><formula xml:id="formula_14">ω (t) = ω (t-1) + ∆ω (t) ,<label>(11)</label></formula><p>where 1) ). ( <ref type="formula">12</ref>) 1) is the gradient of weight based on current batch of samples.</p><formula xml:id="formula_15">∆ω (t) = β • ∆ω (t-1) -• (δω (t-1) + γ * ω (t-</formula><formula xml:id="formula_16">δω (t-</formula><p>The learning process is conducted on a training subset and a validation subset. It won't stop until the error rate on validation subset is smaller than a threshold ξ or a predefined maximum number of epochs is reached. Besides, the learning may stop earlier if it cannot reach smaller error since the current smallest one after a number of patient iterations. Our algorithm is implemented in Python using Theano library <ref type="bibr" target="#b45">[47]</ref>. To leverage the highly parallelable property of CNN training, we trained our models on a 64-bit desktop with i7-2600 (3.4GHz) CPU, 16GB RAM and NVIDIA GTX-660 3GB GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Image classification on synthetic data</head><p>We first validate our method on a synthetic data set, which has been briefly introduced as a toy example in Sec. III-A. It is constructed by 4 types of geometry elements: triangle, square, circle and diamond. The size of all synthetic images are 60 × 60 with black background (intensity value 0). The basic geometry elements are created within a bounding-box 20×20 and random intensity values in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">255]</ref>. These elements are then resized with random scales up to 10% in height and width, and randomly positioned on the image background. In constructing the two image classes, we ensure that the triangle and square are the "distinctive" element and only appear in Class1 and Class2, respectively. Besides the distinctive element, each image has another element, which is randomly chosen from circle or diamond by coin flipping. </p><formula xml:id="formula_17">F 1score = 2 precision • recall precision + recall . (<label>(13)</label></formula><formula xml:id="formula_18">)<label>14</label></formula><p>The classification accuracy of standard CNN algorithm is 83.3%, as shown in the "SCNN" row in Table I(a). This inferior performance results from the fact that the global CNN learning scheme may not learn the most discriminative local patches. On the contrary, the most discriminative and non-informative local patches are effectively discovered by our two-stage learning framework as shown in Fig. <ref type="figure" target="#fig_2">3b</ref>. The discovered pattern exactly matches the rule of generating these two classes. By leveraging these local patches, our classification accuracy can reach 100% ("BCNN2" row in Table <ref type="table" target="#tab_0">I</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a)).</head><p>A comparison study is conducted using: (1) logistic regression (LR); (2) SVM; (3) standard CNN, similar to LeNet <ref type="bibr" target="#b40">[42]</ref>, trained on whole image (SCNN); (4) local patch-based CNN without boost, i.e., the CNN trained by pre-train stage only (PCNN); (5) local patch-based CNN boosted without additional non-informative class (BCNN1); (6) local patch-based CNN boosted with both discriminative and non-informative patches (BCNN2). Methods (1)-(3) represent conventional learning (using image intensities directly as features) and deep learning approaches. Methods (4),( <ref type="formula" target="#formula_7">5</ref>) are two variants of our proposed one <ref type="bibr" target="#b4">(6)</ref>, which are presented to verify the effects of each component of our method. The parameters of LR and SVM are optimized using grid search with cross-validation. All CNNrelated methods use the same intermediate architecture: one convolutional layer with 10 5 × 5 filters, one max-pooling layer with 2 × 2 kernel, one hidden layer of 300 nodes, and finally followed by a LR layer to output response. The patch size for all patch-based CNNs is 30×30. There are 36 patches extracted from each 60 × 60 image through a sliding window with 6-pixel step size.</p><p>As shown in Table <ref type="table" target="#tab_0">I</ref>(a), standard deep learning method (SCNN) is better than LR, which indicates deep learning can learn good features from raw data. By leveraging the local discriminative information, PCNN gets 16% improvement from SCNN. Among our local patch-based CNNs (PCNN, BCNN1 and BCNN2), BCNN1, which is trained on extracted discriminative (without non-informative) patches, is worse than PCNN due to overfitting. BCNN2, which includes all designed components, achieves the best performance.</p><p>To further prove the adaptivity of our algorithm, we relabeled the synthetic data using Diamond and circle as distinctive elements in class 1 and class 2, respectively (see Fig <ref type="figure" target="#fig_9">6a</ref>). In other words, although the synthetic data are exactly the same, the local patches to distinguish the two classes become different. This is in analogy to real-world problems where the datasets are identical but the classification goal is changed. After conducting the pre-train algorithm, the extracted local patches from the learned model are shown in Fig. <ref type="figure" target="#fig_9">6b</ref>. Again, the extracted local patches contain the most discriminative information, diamond and circle. The classification accuracies are shown in Table I(b). Again, our two-stage learning framework BCNN2 achieves the best performance among all comparison methods. This result demonstrates that our multiinstance CNN learning can adaptively learn discriminative local regions for specific classification tasks without any local level annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Bodypart recognition of CT slices</head><p>In the second experiment, we applied our method in bodypart recognition of transversal CT slices. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, transversal slices of CT scans are categorized into 12 body sections (classes). Our dataset includes 7489 transversal CT slices. They were collected from scans of 675 patients with very different ages (1-90 years old). The imaging protocols were different: 31 different reconstruction kernels, 0.281mm -1.953mm in-slice pixel resolution. We organize such a dataset with large variance to test the robustness of the proposed method. The whole dataset is divided into 2413 (225 patients) training, 656 (56 patients) validation and 4043 (394 patients) testing subsets. In this experiment, we augment data by simply applying up to 10% random translations in training and validation subsets to make them three times larger.</p><p>Our preprocessing includes two different steps: image subsampling and image cropping. First, all images are re-sampled to have 4mm × 4mm pixel resolution and 90 × 90 in size. Then, cropping operation extracts 50 × 50 local patches from each image with 10-pixel step size. Thus, 25 local patches are extracted per image. Our CNN has similar structure as in Fig. <ref type="figure" target="#fig_1">2</ref>. C1 layer has 20 9 × 9 filters. C3 layer has 40 9 × 9 filters. Two sub-sampling layer, S2 and S4, use 2 × 2 max-pooling. 99.9 99.9 99.9 99.9 99.9 99.9 99.9 99.9 H5 layer has 600 hidden nodes. LR layer, O6, has 12 output nodes in pre-train stage, or 13 output nodes in boosting stage.</p><p>As shown in the "BCNN2" row of Table <ref type="table" target="#tab_1">II</ref>, our method can achieve the classification accuracy (F1 score) at 92.23%. Fig. <ref type="figure" target="#fig_10">7</ref> shows more detailed classification performance by the confusion matrix. Most errors appear close to the diagonal line, which means most mis-classifications happen in the neighboring body sections. Quantitatively, 99.2% of the testing cases have "less-than-one neighboring class error" (within the red line corridor of Fig. <ref type="figure" target="#fig_10">7</ref>). In practice, this kind of errors are already acceptable for some use cases and they may be further fixed by post-processing algorithms. For example, the 0.8% gross errors can be further suppressed by a simple label smoothing after classifications of a series of continuous slices for 3D bodypart identification. The learning process takes 440 epochs (≈ 25 hours) in stage I and 70 epochs (≈ 1 hour) in stage II.</p><p>For comparison, tested image classification methods include: (1) LR1, (2) LR2, (3) SVM1, (4) SVM2, (5) CaffeNet, (6) SCNN, (7) SCNN a, (8) PCNN, (9) BCNN1, and (10) our proposed BCNN2. In LR1 and SVM1 methods, we use bag-of-word model with dense SIFT features to train logistic regressor and SVM classifier, respectively. While LR2 and SVM2 methods simply replace SIFT by HOG features. Same as the previous experiment, the LR and SVM parameters were optimized using grid search with cross-validation on the same training and validation sets as other comparison methods. Then, the optimized models were applied on the same testing set to produce results for fair comparisons. SCNN method is the standard CNN that takes the whole slice as input. SCNN a method is the same as SCNN except trained by six times more augmented data samples with random transformations, rotations and scalings. Method ( <ref type="formula" target="#formula_11">8</ref>),( <ref type="formula" target="#formula_12">9</ref>) are the variants of <ref type="bibr" target="#b8">(10)</ref> as described in Sec. IV-A. Similar network structure is used in all CNN-based methods, ( <ref type="formula" target="#formula_8">6</ref>)- <ref type="bibr" target="#b8">(10)</ref>, except different input and output sizes in patch-based CNNs ( <ref type="formula" target="#formula_11">8</ref>)- <ref type="bibr" target="#b8">(10)</ref>. CaffeNet <ref type="bibr" target="#b46">[48]</ref> has the similar structure as AlexNet <ref type="bibr" target="#b5">[7]</ref> with a minor variation, which is trained on whole images without cropping. We noticed that training of CaffeNet with 50×50 cropping doesn't converge. This observation shows that our proposed method is not merely a kind of data augmentation via image cropping. The discriminative and non-informative patches discovered by multi-instance learning are the keys to success. BCNN1 is trained on extracted discriminative (without non-informative) patches from learning stage I. Although the trained classifier focuses more on discriminative patches, ambiguous local patches across different classes (e.g. upholding arms may look similar to neck) are completely ignored and thereby mislead the classifier at runtime. Thus, the performance of BCNN1 is worse than PCNN and close to the SCNN. Compared to its variants, the proposed BCNN2 achieves the best performance (even better than much deeper CNN, CaffeNet), which proves the necessity of using all strategies designed in our method. In addition, we noted that the SCNN a trained with more augmented data is even inferior to the SCNN due to overfitting (training error: SCNN a 4.4% vs. SCNN 5%; testing error: SCNN a 14.7% vs. SCNN 12.3%). It shows that the global CNN cannot learn the anatomy characteristics from more augmented data but overfit them. As shown in Table <ref type="table" target="#tab_1">II</ref>, the overfitting problem is more severe in neck (column 3) and liver upper (column 9) sections. These two sections happen to have subtle global appearance differences compared to their neighboring sections and are thus prone to overfitting. The online classification time of each method is about (1) 4ms, (2) 3ms, (3) 5ms, (4) 4ms, (5) 3ms, (6) 4ms, (7) 4ms, (8) 10ms, (9) 11ms, (10) 11ms per image, respectively. this application of bodypart recognition, the most discriminative patch samples for each class are shown in Fig. <ref type="figure" target="#fig_12">8</ref> as well as the some samples of non-informative (kind of misleading) patches. From this figure, we observe that the proposed method "magically" extracts meaningful local patches for each class without any prior information, and these representative and discriminative local patches can significantly improve the classification task comparing with the global   image information.</p><p>To investigate whether the standard CNNs can discover the required discriminative features at some intermediate layers or they completely miss them, we did extra experiments to train linear SVM classifier on the learned hidden activation on each layer of the baseline CNN (SCNN). Totally 5 classifiers were learned from features on layers (C1, S2, C3, S4, H5). The feature sizes are 134480, 33620, 43560, 10240 and 600, respectively. The F1-scores on testing set are 0.75, 0.77, 0.86, 0.86 and 0.88, respectively. Compared with reported F1-score of SCNN (≈0.88), we conclude that (1) features on higher layers are better for the classification task; (2) although the learned features in SCNN are discriminative to some extent, the more representative and discriminative local features can only be discovered in our proposed patch based learning algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Sensitivity Experiments</head><p>To evaluate the robustness of the trained models, we apply different scales of random linear translation on the testing data and compute the classification error rates. The Fig. <ref type="figure" target="#fig_16">9a</ref> shows the results. From the plots, we can see that our proposed method BCNN2 has the best robustness regarding to the random translation of testing samples. Although the training and validation subsets have been augmented using up to 11%    random translation, the other approaches do not perform as well as the proposed method when the testing samples have larger translations. In this situation, retraining the models on augmented dataset with larger translation could be a solution. However, the re-training costs cannot be overlooked and fixing it after the fact is not efficient in practice.</p><p>As one of the important parameters in BCNN2 method, step size of sliding window testing is investigated regarding to the accuracies (shown in Fig. <ref type="figure" target="#fig_16">9b</ref>). The running times step sizes 1, 5, 10, 15, 20, 25, and 30 pixels are 541.1, 30.6, 11.7, 5.3, 5.2, 3.6 and 3.4 ms per image respectively. Considering the balance of running time and accuracy, step size 10 or 15 should be a reasonable choice in this The effect of patch size the accuracy is also investigated as shown in Fig. <ref type="figure" target="#fig_16">9c</ref>. We can see from the plot that (1) the patch size should not be too small to capture the discriminative information (size 20 or 30); (2) the performance is not very sensitive to the local patch size once it is big enough to include discriminative information (sizes from 40 to 60 in this task).</p><p>We also conducted two extra experiments to test other variants of our proposed method. First, we use Eq. (2) with a bit larger patch size (70x70) rather than the Eq. (3) to accommodate for the neighbors information. The final classification accuracy in terms of F1-score becomes 91.67%, a little worse than that of the proposed BCNN2 (92.23%). Second, instead of using the run-time classification strategy by Eq. ( <ref type="formula" target="#formula_13">10</ref>), we can simply use the Eq. ( <ref type="formula" target="#formula_11">8</ref>) as in <ref type="bibr" target="#b39">[41]</ref>, or majority voting of predictions from all partitioned patches in the slice to predict image classification. The F1 score drops ≈ 2% and ≈ 4%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this paper, a novel multi-stage deep learning framework is presented to tackle the bodypart recognition problem. Its key novelty is to automatically exploit the local information through CNN, and discover the discriminative and noninformative local patches via multi-instance learning. It is worth noting that since no manual annotations are required to label these local patches, our method becomes very scalable. The proposed method is evaluated on a synthetic dataset and a large scale CT dataset. The experimental results show clear improvements compared with state-of-the-art methods. It is proved that the success of the proposed method does not result from more augmented training samples but its capability of discovering local characteristics of different bodyparts. This supervised discriminative patch discovery and classification method can be easily applied to other image classification tasks where local information is critical to distinguish different classes. Our proposed framework can also be extended to 3D cases using 3D convolutional filters. In future, we plan to investigate extracting multi-scale patches from images and exploring some sophisticated algorithms to further improve the performance in the boosting stage.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Definition of body sections. Human body is divided into 12 continuous parts. Each parts may cover different ranges due to the variability of anatomies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of one standard CNN architecture and the outputs of each layer (similar to LeNet [42]).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. A synthetic toy example. (a) Synthetic images of two classes. (b) The discriminative and non-informative local patches selected by the pre-trained CNN model. Note that we never "tell" the algorithm that these two classes are differentiable by triangle and square.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>, for each training image X m , only the local patch that has the highest response at the l m th component of R mn (indicated by the yellow and purple boxes for two training images, respectively), contributes to the loss function and drives the update of network coefficients W during the backward propagation. Accordingly, the learned CNN is expected to have high responses on discriminative local patches. In other words, the most discriminative local patches for each image class are automatically discovered after the CNN training. Fig 3b shows the discovered discriminative patches (containing triangle or square)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Illustration of pre-train learning stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Illustration of boosting learning stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>(</head><label></label><figDesc>Some examples of the synthetic images are shown in Fig. 3a.) Overall, we create 2000 training, 2000 validation and 2000 for testing samples (1000 for each class). Let T P (true positive) denote the number of samples belonging to class k and correctly classified; F N (false negative) denote the number of samples belonging to class k but misclassified; F P (false positive) denote the number of samples not belonging to class k but misclassified as class k. Classification accuracies are reported in terms of recall, precision and F1 score as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The second toy example. (a) Synthetic images of two classes distinguished by diamond and circle. It is important to note that we used the same image samples as in Fig. 3, but re-labeled the images into two classes based on different rules. (b) The discriminative and non-informative local patches discovered by the pre-trained CNN model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Confusion matrix of BCNN2 on CT data. Values are normalized to 0 ∼ 100 in each row.</figDesc><graphic coords="9,55.52,173.47,245.62,184.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Automatically discovered discriminative and non-informative patches from each class through multi-instance learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>Classification errors vs. scales of random translations on testing data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Classification accuracies vs. step size of sliding window in BCNN2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>Classification accuracies vs. patch size in BCNN2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Performance analyses on the sensitivity of parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I CLASSIFICATION</head><label>I</label><figDesc>ACCURACIES ON SYNTHETIC DATA IN TERMS OF RECALL, PRECISION AND F1 SCORE (%).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(a) Triangle and square</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(b) Diamond and circle</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Recall</cell><cell></cell><cell></cell><cell>Precision</cell><cell></cell><cell></cell><cell>F1</cell><cell></cell><cell></cell><cell>Recall</cell><cell></cell><cell></cell><cell>Precision</cell><cell></cell><cell></cell><cell>F1</cell><cell></cell></row><row><cell>Class</cell><cell>1</cell><cell>2</cell><cell>Total</cell><cell>1</cell><cell>2</cell><cell>Total</cell><cell>1</cell><cell>2</cell><cell>Total</cell><cell>1</cell><cell>2</cell><cell>Total</cell><cell>1</cell><cell>2</cell><cell>Total</cell><cell>1</cell><cell>2</cell><cell>Total</cell></row><row><cell>LR</cell><cell>78.7</cell><cell>83.4</cell><cell>81.1</cell><cell cols="2">82.6 79.7</cell><cell>81.1</cell><cell cols="2">80.6 81.5</cell><cell>81.1</cell><cell>70.3</cell><cell>62.5</cell><cell>66.4</cell><cell>65.2</cell><cell>67.8</cell><cell>66.5</cell><cell>67.7</cell><cell>65.0</cell><cell>66.5</cell></row><row><cell>SVM</cell><cell>84.5</cell><cell>81.2</cell><cell>82.9</cell><cell cols="2">81.8 84.0</cell><cell>82.9</cell><cell cols="2">83.1 82.6</cell><cell>82.9</cell><cell>69.0</cell><cell>63.1</cell><cell>66.1</cell><cell>65.2</cell><cell>67.1</cell><cell>66.1</cell><cell>67.0</cell><cell>65.0</cell><cell>66.1</cell></row><row><cell>SCNN</cell><cell>84.2</cell><cell>82.4</cell><cell>83.3</cell><cell cols="2">82.7 83.9</cell><cell>83.3</cell><cell cols="2">83.5 83.2</cell><cell>83.3</cell><cell>91.8</cell><cell>94.2</cell><cell>93</cell><cell>94.1</cell><cell>92.0</cell><cell>93.0</cell><cell>92.9</cell><cell>93.1</cell><cell>93.0</cell></row><row><cell>PCNN</cell><cell>99.6</cell><cell>99.7</cell><cell>99.7</cell><cell cols="2">99.7 99.6</cell><cell>99.7</cell><cell cols="2">99.7 99.7</cell><cell>99.7</cell><cell>99.6</cell><cell>100</cell><cell>99.8</cell><cell>100</cell><cell>99.6</cell><cell>99.8</cell><cell>99.8</cell><cell>99.8</cell><cell>99.8</cell></row><row><cell>BCNN1</cell><cell>98.4</cell><cell>99.7</cell><cell>99.1</cell><cell cols="2">99.7 98.4</cell><cell>99.1</cell><cell cols="2">99.0 99.1</cell><cell>99.1</cell><cell>95.6</cell><cell>100</cell><cell>97.8</cell><cell>100</cell><cell>95.8</cell><cell>97.9</cell><cell>97.8</cell><cell>97.9</cell><cell>97.9</cell></row><row><cell>BCNN2</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>99.9</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>99.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II CLASSIFICATION</head><label>II</label><figDesc>ACCURACIES ON CT DATA IN TERMS OF RECALL, PRECISION AND F1 SCORE (%).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Recall</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Class</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>Total</cell></row><row><cell>LR1</cell><cell>48.54</cell><cell cols="2">63.64 33.33</cell><cell>69.95</cell><cell>39.09</cell><cell>43.37</cell><cell>47.66</cell><cell>81.43</cell><cell>25.82</cell><cell>53.68</cell><cell>41.62</cell><cell>88.79</cell><cell>63.37</cell></row><row><cell>LR2</cell><cell>67.96</cell><cell cols="2">64.50 42.59</cell><cell>74.24</cell><cell>38.64</cell><cell>42.17</cell><cell>56.25</cell><cell>78.51</cell><cell>37.09</cell><cell>65.79</cell><cell>60.41</cell><cell>91.48</cell><cell>68.71</cell></row><row><cell>SVM1</cell><cell>41.75</cell><cell cols="2">64.07 46.30</cell><cell>76.26</cell><cell>36.36</cell><cell>45.18</cell><cell>47.27</cell><cell>78.66</cell><cell>31.46</cell><cell>52.90</cell><cell>50.25</cell><cell>88.57</cell><cell>64.63</cell></row><row><cell>SVM2</cell><cell>76.70</cell><cell cols="2">81.39 39.82</cell><cell>79.55</cell><cell>54.09</cell><cell>63.86</cell><cell>69.92</cell><cell>84.06</cell><cell>44.60</cell><cell>64.47</cell><cell>75.64</cell><cell>96.53</cell><cell>76.75</cell></row><row><cell>CaffeNet</cell><cell>71.85</cell><cell cols="2">64.94 87.96</cell><cell>85.10</cell><cell>74.09</cell><cell>80.72</cell><cell>57.81</cell><cell>94.59</cell><cell>80.75</cell><cell>78.95</cell><cell>85.28</cell><cell>97.53</cell><cell>84.74</cell></row><row><cell>SCNN</cell><cell>84.47</cell><cell cols="2">93.51 72.22</cell><cell>88.89</cell><cell>80.46</cell><cell>80.12</cell><cell>86.72</cell><cell>95.47</cell><cell>77.93</cell><cell>77.63</cell><cell>78.43</cell><cell>96.30</cell><cell>87.73</cell></row><row><cell>SCNN a</cell><cell>81.55</cell><cell cols="2">96.54 43.52</cell><cell>92.68</cell><cell>79.09</cell><cell>90.96</cell><cell>93.75</cell><cell>88.45</cell><cell>51.17</cell><cell>64.47</cell><cell>90.36</cell><cell>92.60</cell><cell>84.76</cell></row><row><cell>PCNN</cell><cell>87.38</cell><cell cols="2">92.21 87.96</cell><cell>93.18</cell><cell>90.00</cell><cell>74.70</cell><cell>94.53</cell><cell>95.47</cell><cell>81.69</cell><cell>81.05</cell><cell>90.36</cell><cell>92.49</cell><cell>90.21</cell></row><row><cell>BCNN1</cell><cell>54.37</cell><cell cols="2">83.12 69.44</cell><cell>94.95</cell><cell>75.91</cell><cell>82.53</cell><cell>93.36</cell><cell>95.03</cell><cell>84.98</cell><cell>72.63</cell><cell>85.53</cell><cell>96.75</cell><cell>87.78</cell></row><row><cell>BCNN2</cell><cell>88.35</cell><cell cols="2">96.97 80.56</cell><cell>91.67</cell><cell>86.82</cell><cell>87.35</cell><cell>93.75</cell><cell>95.61</cell><cell>79.81</cell><cell>87.11</cell><cell>87.82</cell><cell>99.33</cell><cell>92.21</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Precision</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Class</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>Total</cell></row><row><cell>LR1</cell><cell>72.46</cell><cell cols="2">63.09 63.16</cell><cell>64.42</cell><cell>55.48</cell><cell>54.14</cell><cell>54.46</cell><cell>72.15</cell><cell>55.00</cell><cell>57.14</cell><cell>47.40</cell><cell>67.87</cell><cell>62.21</cell></row><row><cell>LR2</cell><cell>65.42</cell><cell cols="2">64.78 61.33</cell><cell>68.53</cell><cell>53.80</cell><cell>46.36</cell><cell>56.92</cell><cell>76.06</cell><cell>59.40</cell><cell>64.43</cell><cell>62.80</cell><cell>78.92</cell><cell>67.74</cell></row><row><cell>SVM1</cell><cell>75.44</cell><cell cols="2">61.93 65.79</cell><cell>64.81</cell><cell>53.69</cell><cell>51.37</cell><cell>53.78</cell><cell>74.00</cell><cell>56.30</cell><cell>58.26</cell><cell>49.62</cell><cell>72.15</cell><cell>63.72</cell></row><row><cell>SVM2</cell><cell>89.77</cell><cell cols="2">72.03 72.88</cell><cell>72.25</cell><cell>56.94</cell><cell>57.92</cell><cell>72.76</cell><cell>82.62</cell><cell>62.50</cell><cell>70.81</cell><cell>71.64</cell><cell>90.54</cell><cell>76.39</cell></row><row><cell>CaffeNet</cell><cell>98.67</cell><cell cols="2">98.04 41.49</cell><cell>86.41</cell><cell>93.68</cell><cell>83.23</cell><cell>87.57</cell><cell>84.25</cell><cell>66.15</cell><cell>77.32</cell><cell>84.42</cell><cell>99.09</cell><cell>86.84</cell></row><row><cell>SCNN</cell><cell>87.88</cell><cell cols="2">87.45 82.11</cell><cell>87.35</cell><cell>87.62</cell><cell>83.13</cell><cell>84.73</cell><cell>92.36</cell><cell>76.50</cell><cell>75.06</cell><cell>83.51</cell><cell>96.73</cell><cell>87.72</cell></row><row><cell>SCNN a</cell><cell>96.55</cell><cell cols="2">78.80 77.05</cell><cell>81.19</cell><cell>89.69</cell><cell>79.06</cell><cell>80.54</cell><cell>94.38</cell><cell>83.85</cell><cell>74.70</cell><cell>66.17</cell><cell>98.33</cell><cell>85.75</cell></row><row><cell>PCNN</cell><cell>96.77</cell><cell cols="2">93.83 81.20</cell><cell>87.86</cell><cell>85.35</cell><cell>96.88</cell><cell>90.30</cell><cell>95.33</cell><cell>78.73</cell><cell>83.92</cell><cell>77.73</cell><cell>99.76</cell><cell>90.69</cell></row><row><cell>BCNN1</cell><cell>100.00</cell><cell cols="2">94.58 54.75</cell><cell>76.27</cell><cell>91.26</cell><cell>93.20</cell><cell>91.92</cell><cell>95.59</cell><cell>77.68</cell><cell>82.64</cell><cell>78.74</cell><cell>97.08</cell><cell>88.62</cell></row><row><cell>BCNN2</cell><cell>96.81</cell><cell cols="2">91.80 88.78</cell><cell>90.30</cell><cell>90.52</cell><cell>89.51</cell><cell>91.95</cell><cell>95.75</cell><cell>80.95</cell><cell>82.13</cell><cell>91.78</cell><cell>98.66</cell><cell>92.25</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>F1 score</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Class</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>Total</cell></row><row><cell>LR1</cell><cell>58.14</cell><cell cols="2">63.36 43.64</cell><cell>67.07</cell><cell>45.87</cell><cell>48.16</cell><cell>50.83</cell><cell>76.51</cell><cell>35.14</cell><cell>55.36</cell><cell>44.32</cell><cell>76.93</cell><cell>62.78</cell></row><row><cell>LR2</cell><cell>66.67</cell><cell cols="2">64.64 50.27</cell><cell>71.27</cell><cell>44.97</cell><cell>44.16</cell><cell>56.58</cell><cell>77.27</cell><cell>45.67</cell><cell>65.10</cell><cell>61.58</cell><cell>84.74</cell><cell>68.22</cell></row><row><cell>SVM1</cell><cell>53.75</cell><cell cols="2">62.98 54.35</cell><cell>70.07</cell><cell>43.36</cell><cell>48.08</cell><cell>50.31</cell><cell>76.26</cell><cell>40.36</cell><cell>55.45</cell><cell>49.94</cell><cell>79.52</cell><cell>64.17</cell></row><row><cell>SVM2</cell><cell>82.72</cell><cell cols="2">76.42 51.50</cell><cell>75.72</cell><cell>55.48</cell><cell>60.75</cell><cell>71.32</cell><cell>83.33</cell><cell>52.06</cell><cell>67.49</cell><cell>73.58</cell><cell>93.44</cell><cell>76.57</cell></row><row><cell>CaffeNet</cell><cell>83.15</cell><cell cols="2">78.13 56.38</cell><cell>85.75</cell><cell>82.74</cell><cell>81.96</cell><cell>69.65</cell><cell>89.12</cell><cell>72.73</cell><cell>78.13</cell><cell>84.85</cell><cell>98.31</cell><cell>85.78</cell></row><row><cell>SCNN</cell><cell>86.14</cell><cell cols="2">90.38 76.85</cell><cell>88.11</cell><cell>83.89</cell><cell>81.60</cell><cell>85.71</cell><cell>93.89</cell><cell>77.21</cell><cell>76.33</cell><cell>80.89</cell><cell>96.52</cell><cell>87.73</cell></row><row><cell>SCNN a</cell><cell>88.42</cell><cell cols="2">86.77 55.62</cell><cell>86.56</cell><cell>84.06</cell><cell>84.59</cell><cell>86.64</cell><cell>91.32</cell><cell>63.56</cell><cell>69.21</cell><cell>76.39</cell><cell>95.38</cell><cell>85.25</cell></row><row><cell>PCNN</cell><cell>91.84</cell><cell cols="2">93.01 84.44</cell><cell>90.44</cell><cell>87.61</cell><cell>84.35</cell><cell>92.37</cell><cell>95.40</cell><cell>80.18</cell><cell>82.46</cell><cell>83.57</cell><cell>95.99</cell><cell>90.45</cell></row><row><cell>BCNN1</cell><cell>70.44</cell><cell cols="2">88.48 61.22</cell><cell>84.59</cell><cell>82.88</cell><cell>87.54</cell><cell>92.64</cell><cell>95.31</cell><cell>81.17</cell><cell>77.31</cell><cell>82.00</cell><cell>96.91</cell><cell>88.20</cell></row><row><cell>BCNN2</cell><cell>92.39</cell><cell cols="2">94.32 84.47</cell><cell>90.98</cell><cell>88.63</cell><cell>88.42</cell><cell>92.84</cell><cell>95.68</cell><cell>80.38</cell><cell>84.55</cell><cell>89.75</cell><cell>98.99</cell><cell>92.23</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards robust and effective shape modeling: Sparse shape composition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Zhou ; Gueld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kohnen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Wein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bredno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="280" to="287" />
			<date type="published" when="2002">2002. 2002</date>
			<publisher>International Society for Optics and Photonics</publisher>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Early detection of breast cancer using SVM classifier technique</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Selvi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0912.2314</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Computer-aided detection of polyps in CT colonography using logistic regression</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">F</forename><surname>Van Ravesteijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Van Wijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Vos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Truyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stoker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><surname>Vliet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="120" to="131" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A desicion-theoretic generalization of online learning and an application to boosting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Learning Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="23" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.5726</idno>
		<title level="m">Cnn: Single-label to multi-label</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision Pattern Recognition</title>
		<meeting>IEEE Int. Conf. Computer Vision Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A novel algorithm for identification of body parts in medical images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fuzzy Systems and Knowledge Discovery</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1148" to="1158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Methods and apparatus for automatic body part identification and localization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<idno>App. 11/933</idno>
		<imprint>
			<date type="published" when="2008-05-15">May 15 2008</date>
			<biblScope unit="page">518</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">US Patent</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Active scheduling of organ detection and segmentation in whole-body medical images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Med. Image Comput. Comput.-Assist. Intervent</title>
		<imprint>
			<biblScope unit="page" from="313" to="321" />
			<date type="published" when="2008">2008</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Regression forests for efficient anatomy detection and localization in CT studies</title>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Computer Vision. Recognition Techniques and Applications in Medical Imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="106" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Decision forests with longrange spatial context for organ localization in ct volumes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bucciarelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Med. Image Comput. Comput.-Assist. Intervent</title>
		<imprint>
			<biblScope unit="page" from="69" to="80" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Global localization of 3D anatomical structures by pre-filtered Hough Forests and discrete optimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Donner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1304" to="1314" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Building the gist of a scene: The role of global image features in recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progress in Brain Research</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page" from="23" to="36" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision Pattern Recognition</title>
		<meeting>IEEE Int. Conf. Computer Vision Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision Pattern Recognition</title>
		<meeting>IEEE Int. Conf. Computer Vision Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Linear spatial pyramid using sparse coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision Pattern Recognition</title>
		<meeting>IEEE Int. Conf. Computer Vision Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1794" to="1801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">High-dimensional signature compression for large-scale image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision Pattern Recognition</title>
		<meeting>IEEE Int. Conf. Computer Vision Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1665" to="1672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision Pattern Recognition</title>
		<meeting>IEEE Int. Conf. Computer Vision Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Anatomy-specific classification of medical images using deep convolutional nets</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intl. Symp. Biomed. Imag</title>
		<meeting>IEEE Intl. Symp. Biomed. Imag</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="101" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recognizing jumbled images: the role of local and global information in image classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Computer Vision. IEEE</title>
		<meeting>Int. Conf. on Computer Vision. IEEE</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="519" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Joint multi-label multi-instance learning for image classification</title>
		<author>
			<persName><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision Pattern Recognition</title>
		<meeting>IEEE Int. Conf. Computer Vision Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multiple-instance learning for natural scene classification</title>
		<author>
			<persName><forename type="first">O</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Ratan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Machine Learning</title>
		<meeting>Int. Conf. on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Blocks that shout: Distinctive parts for scene classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Juneja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision Pattern Recognition</title>
		<meeting>IEEE Int. Conf. Computer Vision Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="923" to="930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep neural networks for object detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="2553" to="2561" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of midlevel discriminative patches</title>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Computer Vision</title>
		<meeting>European Conf. on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="73" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">BING: Binarized normed gradients for objectness estimation at 300fps</title>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision Pattern Recognition</title>
		<meeting>IEEE Int. Conf. Computer Vision Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3286" to="3293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A framework for multiple-instance learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lozano-Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="570" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Support vector machines for multiple-instance learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="561" to="568" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">EM-DD: An improved multiple-instance learning technique</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="1073" to="1080" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep multiple instance learning for image classification and auto-annotation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision Pattern Recognition</title>
		<meeting>IEEE Int. Conf. Computer Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3460" to="3469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">From image-level to pixel-level labeling with convolutional networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision Pattern Recognition</title>
		<meeting>IEEE Int. Conf. Computer Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1713" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Le-Cun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Weaklyand semi-supervised learning of a DCNN for semantic image segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02734v2</idno>
		<ptr target="http://arxiv.org/abs/1502.02734" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1210.5644</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Efficient multiple instance convolutional neural networks for gigapixel resolution image classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Kurc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Saltz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.07947</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bodypart recognition using multi-stage deep learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shinagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Information Processing in Medical Imaging</title>
		<meeting>Int. Conf. Information essing in Medical Imaging</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="449" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998-11">November 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Machine Learning</title>
		<meeting>Int. Conf. on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Best practices for convolutional neural networks applied to visual document analysis</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Steinkraus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th Int. Conf. on Document Analysis and Recognition</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="958" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On the momentum term in gradient descent learning algorithms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="151" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Python for Scientific Computing Conference (SciPy)</title>
		<meeting>of the Python for Scientific Computing Conference (SciPy)</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
