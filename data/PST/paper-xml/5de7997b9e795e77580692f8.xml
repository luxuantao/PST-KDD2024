<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Conformalized Quantile Regression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yaniv</forename><surname>Romano</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Evan</forename><surname>Patterson</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Emmanuel</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Departments of Mathematics and of Statistics</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Conformalized Quantile Regression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B3E9F5C19BB1ADC700F5864D60C8B187</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conformal prediction is a technique for constructing prediction intervals that attain valid coverage in finite samples, without making distributional assumptions. Despite this appeal, existing conformal methods can be unnecessarily conservative because they form intervals of constant or weakly varying length across the input space. In this paper we propose a new method that is fully adaptive to heteroscedasticity. It combines conformal prediction with classical quantile regression, inheriting the advantages of both. We establish a theoretical guarantee of valid coverage, supplemented by extensive experiments on popular regression datasets. We compare the efficiency of conformalized quantile regression to other conformal methods, showing that our method tends to produce shorter intervals.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In many applications of regression modeling, it is important not only to predict accurately but also to quantify the accuracy of the predictions. This is especially true in situations involving high-stakes decision making, such as estimating the efficacy of a drug or the risk of a credit default. The uncertainty in a prediction can be quantified using a prediction interval, giving lower and upper bounds between which the response variable lies with high probability. An ideal procedure for generating prediction intervals should satisfy two properties. First, it should provide valid coverage in finite samples, without making strong distributional assumptions, such as Gaussianity. Second, its intervals should be as short as possible at each point in the input space, so that the predictions will be informative. When the data is heteroscedastic, getting valid but short prediction intervals requires adjusting the lengths of the intervals according to the local variability at each query point in predictor space. This paper introduces a procedure that performs well on both criteria, being distribution-free and adaptive to heteroscedasticity.</p><p>Our work is heavily inspired by conformal prediction, a general methodology for constructing prediction intervals <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>. Conformal prediction has the virtue of providing a nonasymptotic, distribution-free coverage guarantee. The main idea is to fit a regression model on the training samples, then use the residuals on a held-out validation set to quantify the uncertainty in future predictions. The effect of the underlying model on the length of the prediction intervals, and attempts to construct intervals with locally varying length, have been studied in numerous recent works <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>. Nevertheless, existing methods yield conformal intervals of either fixed length or length depending only weakly on the predictors, as argued in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>In conformal prediction to date, there has been a mismatch between the primary inferential focusconditional mean estimation-and the ultimate inferential goal-prediction interval estimation. Statistical efficiency is lost by estimating a mean when an interval is needed. A more direct approach to interval estimation is offered by quantile regression <ref type="bibr" target="#b17">[18]</ref>. Take any algorithm for quantile regression, i.e., for estimating conditional quantile functions from data. To obtain prediction intervals with, say, nominal 90% coverage, simply fit the conditional quantile function at the 5% and 95% levels and form the corresponding intervals. Even for highly heteroscedastic data, this methodology has been shown to be adaptive to local variability <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref>. However, the validity of the estimated intervals is guaranteed only for specific models, under certain regularity and asymptotic conditions <ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref>.</p><p>In this work, we combine conformal prediction with quantile regression. The resulting method, which we call conformalized quantile regression (CQR), inherits both the finite sample, distribution-free validity of conformal prediction and the statistical efficiency of quantile regression. <ref type="foot" target="#foot_1">1</ref> On one hand, CQR is flexible in that it can wrap around any algorithm for quantile regression, including random forests and deep neural networks <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref>. On the other hand, a key strength of CQR is its rigorous control of the miscoverage rate, independent of the underlying regression algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary and outline</head><p>Suppose we are given n training samples {(X i , Y i )} n i=1 and we must now predict the unknown value of Y n+1 at a test point X n+1 . We assume that all the samples {(X i , Y i )} n+1 i=1 are drawn exchangeably-for instance, they may be drawn i.i.d.-from an arbitrary joint distribution P XY over the feature vectors X ∈ R p and response variables Y ∈ R. We aim to construct a marginal distribution-free prediction interval C(X n+1 ) ⊆ R that is likely to contain the unknown response Y n+1 . That is, given a desired miscoverage rate α, we ask that</p><formula xml:id="formula_0">P{Y n+1 ∈ C(X n+1 )} ≥ 1 -α<label>(1)</label></formula><p>for any joint distribution P XY and any sample size n. The probability in this statement is marginal, being taken over all the samples {(X i , Y i )} n+1 i=1 . To accomplish this, we build on the method of conformal prediction <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">8]</ref>. We first split the training data into two disjoint subsets, a proper training set and a calibration set. <ref type="foot" target="#foot_2">2</ref> We fit two quantile regressors on the proper training set to obtain initial estimates of the lower and upper bounds of the prediction interval, as explained in Section 2. Then, using the calibration set, we conformalize and, if necessary, correct this prediction interval. Unlike the original interval, the conformalized prediction interval is guaranteed to satisfy the coverage requirement (1) regardless of the choice or accuracy of the quantile regression estimator. We prove this in Section 4.</p><p>Our method differs from the standard method of conformal prediction <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15]</ref>, recalled in Section 3, in that we calibrate the prediction interval using conditional quantile regression, while the standard method uses only classical, conditional mean regression. The result is that our intervals are adaptive to heteroscedasticity whereas the standard intervals are not. We evaluate the statistical efficiency of our framework by comparing its miscoverage rate and average interval length with those of other methods. We review existing state-of-the-art schemes for conformal prediction in Section 5 and we compare them with our method in Section 6. Based on extensive experiments across eleven datasets, we conclude that conformal quantile regression yields shorter intervals than the competing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Quantile regression</head><p>The aim of conditional quantile regression <ref type="bibr" target="#b17">[18]</ref> is to estimate a given quantile, such as the median, of Y conditional on X. Recall that the conditional distribution function of Y given X = x is</p><formula xml:id="formula_1">F (y | X = x) := P{Y ≤ y | X = x},</formula><p>and that the αth conditional quantile function is</p><formula xml:id="formula_2">q α (x) := inf{y ∈ R : F (y | X = x) ≥ α}.</formula><p>Fix the lower and upper quantiles to be equal to α lo = α/2 and α hi = 1 -α/2, say. Given the pair q αlo (x) and q αhi (x) of lower and upper conditional quantile functions, we obtain a conditional prediction interval for Y given X = x, with miscoverage rate α, as</p><formula xml:id="formula_3">C(x) = [q αlo (x), q αhi (x)].<label>(2)</label></formula><p>By construction, this interval satisfies</p><formula xml:id="formula_4">P{Y ∈ C(X)|X = x} ≥ 1 -α.<label>(3)</label></formula><p>Notice that the length of the interval C(X) can vary greatly depending on the value of X. The uncertainty in the prediction of Y is naturally reflected in the length of the interval. In practice we cannot know this ideal prediction interval, but we can try to estimate it from the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimating quantiles from data</head><p>Classical regression analysis estimates the conditional mean of the test response Y n+1 given the features X n+1 =x by minimizing the sum of squared residuals on the n training points:</p><formula xml:id="formula_5">μ(x) = µ(x; θ), θ = argmin θ 1 n n i=1 (Y i -µ(X i ; θ)) 2 + R(θ).</formula><p>Here θ are the parameters of the regression model, µ(x; θ) is the regression function, and R is a potential regularizer.</p><p>Analogously, quantile regression estimates a conditional quantile function q α of Y n+1 given X n+1 =x. This can be cast as the optimization problem</p><formula xml:id="formula_6">qα (x) = f (x; θ), θ = argmin θ 1 n n i=1 ρ α (Y i , f (X i ; θ)) + R(θ),</formula><p>where f (x; θ) is the quantile regression function and the loss function ρ α is the "check function" or "pinball loss" <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24]</ref>, defined by</p><formula xml:id="formula_7">ρ α (y, ŷ) := α(y -ŷ) if y -ŷ &gt; 0, (1 -α)(ŷ -y) otherwise.</formula><p>The simplicity and generality of this formulation makes quantile regression widely applicable. As in classical regression, one can leverage the great variety of machine learning methods to design and learn qα <ref type="bibr">[19-21, 23, 30]</ref>.</p><p>All this suggests an obvious strategy to construct a prediction band with nominal miscoverage rate α: estimate qαlo (x) and qαhi (x) using quantile regression, then output Ĉ(X n+1 ) = [q αlo (X n+1 ), qαhi (X n+1 )] as an estimate of the ideal interval C(X n+1 ) from equation ( <ref type="formula" target="#formula_3">2</ref>). This approach is widely applicable and often works well in practice, yielding intervals that are adaptive to heteroscedasticity. However, it is not guaranteed to satisfy the coverage statement (3) when C(X) is replaced by the estimated interval Ĉ(X n+1 ). Indeed, the absence of any finite sample guarantee can sometimes be disastrous. This worry is corroborated by our experiments, which show that the intervals constructed by neural networks can substantially undercover.</p><p>Under regularity conditions and for specific models, estimates of conditional quantile functions via the pinball loss or related methods are known to be asymptotically consistent <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>. Certain methods that do not minimize the pinball loss, such as quantile random forests <ref type="bibr" target="#b21">[22]</ref>, are also asymptotically consistent. But to get valid coverage in finite samples, we must draw on a different set of ideas, from conformal prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Conformal Prediction</head><p>We now describe how conformal prediction <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref> constructs prediction intervals that satisfy the finitesample coverage guarantee (1). To be carried out exactly, the original, or full, conformal procedure effectively requires the regression algorithm to be invoked infinitely many times. In contrast, the method of split, or inductive, conformal prediction <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8]</ref> avoids this problem, at the cost of splitting the data. While our proposal is applicable to both versions of conformal prediction, in the interest of space we will restrict our attention to split conformal prediction and refer the reader to <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15]</ref> for a more detailed comparison between the two methods.</p><p>Under the assumptions of Section 1, the split conformal method begins by splitting the training data into two disjoint subsets: a proper training set {(X i , Y i ) : i ∈ I 1 } and calibration set {(X i , Y i ) : i ∈ I 2 }. Then, given any regression algorithm A,<ref type="foot" target="#foot_3">3</ref> a regression model is fit to the proper training set:</p><formula xml:id="formula_8">μ(x) ← A ({(X i , Y i ) : i ∈ I 1 }) .</formula><p>Next, the absolute residuals are computed on the calibration set, as follows:</p><formula xml:id="formula_9">R i = |Y i -μ(X i )|, i ∈ I 2 . (<label>4</label></formula><formula xml:id="formula_10">)</formula><p>For a given level α, we then compute a quantile of the empirical distribution <ref type="foot" target="#foot_4">4</ref> of the absolute residuals,</p><formula xml:id="formula_11">Q 1-α (R, I 2 ) := (1 -α)(1 + 1/|I 2 |)-th empirical quantile of {R i : i ∈ I 2 } .</formula><p>Finally, the prediction interval at a new point X n+1 is given by</p><formula xml:id="formula_12">C(X n+1 ) = [μ(X n+1 ) -Q 1-α (R, I 2 ), μ(X n+1 ) + Q 1-α (R, I 2 )] . (<label>5</label></formula><formula xml:id="formula_13">)</formula><p>This interval is guaranteed to satisfy (1), as shown in <ref type="bibr" target="#b2">[3]</ref>. For related theoretical studies, see <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>A closer look at the prediction interval (5) reveals a major limitation of this procedure: the length of C(X n+1 ) is fixed and equal to 2Q 1-α (R, I 2 ), independent of X n+1 . Lei et al <ref type="bibr" target="#b14">[15]</ref> observe that the intervals produced by the full conformal method also vary only slightly with X n+1 , provided the regression algorithm is moderately stable. This brings us to our proposal, which offers a principled approach to constructing variable-width conformal prediction intervals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conformalized quantile regression (CQR)</head><p>In this section we introduce our procedure, beginning with a small experiment on simulated data to show how it improves upon standard conformal prediction. Figure <ref type="figure" target="#fig_1">1</ref> compares the prediction intervals produced by (a) the split conformal method, (b) its locally adaptive variant (described later in Section 5), and (c) our method, conformalized quantile regression (CQR). The heteroskedasticity of the data is evident, as the dispersion of Y varies considerably with X. The data also contains outliers, shown in the supplementary material. For all three methods, we construct 90% prediction intervals on the test data. From Figures <ref type="figure" target="#fig_1">1a</ref> and<ref type="figure" target="#fig_1">1d</ref>, we see that the lengths of the split conformal intervals are fixed and equal to 2.91. The prediction intervals of the locally weighted variant, shown in Figure <ref type="figure" target="#fig_1">1b</ref>, are partially adaptive, resulting in slightly shorter intervals, of average length 2.86. Our method, shown in Figure <ref type="figure" target="#fig_1">1c</ref>, is also adaptive, but its prediction intervals are considerably shorter, of average length 1.99, due to better estimation of the lower and upper quantiles. We refer the reader to the supplementary material for further details about this experiment, as well as a second simulation demonstrating the advantage of CQR on heavy-tailed data.</p><p>We now describe CQR itself. As in split conformal prediction, we begin by splitting the data into a proper training set, indexed by I 1 , and a calibration set, indexed by I 2 . Given any quantile regression algorithm A, we then fit two conditional quantile functions qαlo and qαhi on the proper training set:</p><formula xml:id="formula_14">{q αlo , qαhi } ← A({(X i , Y i ) : i ∈ I 1 }).</formula><p>In the essential next step, we compute conformity scores that quantify the error made by the plug-in prediction interval Ĉ(x) = [q αlo (x), qαhi (x)]. The scores are evaluated on the calibration set as</p><formula xml:id="formula_15">E i := max{q αlo (X i ) -Y i , Y i -qαhi (X i )}<label>(6)</label></formula><p>for each i ∈ I 2 . The conformity score E i has the following interpretation.</p><formula xml:id="formula_16">If Y i is below the lower endpoint of the interval, Y i &lt; qαlo (X i ), then E i = |Y i -qαlo (X i )|</formula><p>is the magnitude of the error incurred by this mistake. Similarly, if Y i is above the upper endpoint of the interval, Y i &gt; qαhi (X i ),</p><formula xml:id="formula_17">then E i = |Y i -qαhi (X i )|. Finally, if Y i correctly belongs to the interval, qαlo (X i ) ≤ Y i ≤ qαhi (X i ),</formula><p>then E i is the larger of the two non-positive numbers qαlo (X i ) -Y i and Y i -qαhi (X i ) and so is itself non-positive. The conformity score thus accounts for both undercoverage and overcoverage.</p><p>Finally, given new input data X n+1 , we construct the prediction interval for Y n+1 as  The target coverage rate is 90%. The broken black curve in (a) and (b) is the pointwise prediction from the random forest estimator. In (c), we show two curves, representing the lower and upper quantile regression estimates based on random forests <ref type="bibr" target="#b21">[22]</ref>. Observe how in this example the quantile regression estimates closely match the adjusted estimates-the boundary of the blue region-obtained by conformalization.</p><formula xml:id="formula_18">C(X n+1 ) = [q αlo (X n+1 ) -Q 1-α (E, I 2 ), qαhi (X n+1 ) + Q 1-α (E, I 2 )] ,<label>(7)</label></formula><p>Algorithm 1: Split Conformal Quantile Regression. Input: Data (X i , Y i ), 1 ≤ i ≤ n; miscoverage level α ∈ (0, 1); quantile regression algorithm A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Process:</head><p>Randomly split {1, . . . , n} into two disjoint sets I 1 and I 2 . Fit two conditional quantile functions:</p><formula xml:id="formula_19">{q αlo , qαhi } ← A({(X i , Y i ) : i ∈ I 1 }). Compute E i for each i ∈ I 2 , as in equation (6). Compute Q 1-α (E, I 2 ), the (1 -α)(1 + 1/|I 2 |)-th empirical quantile of {E i : i ∈ I 2 }. Output: Prediction interval C(x) = [q αlo (x) -Q 1-α (E, I 2 ), qαhi (x) + Q 1-α (E, I 2 )] for X n+1 = x.</formula><p>where</p><formula xml:id="formula_20">Q 1-α (E, I 2 ) := (1 -α)(1 + 1/|I 2 |)-th empirical quantile of {E i : i ∈ I 2 }<label>(8)</label></formula><p>conformalizes the plug-in prediction interval.</p><p>For ease of reference, the CQR procedure is summarized in Algorithm 1. The following theorem, establishing its validity, is proved in the supplementary material.</p><p>Theorem 1. If (X i , Y i ), i = 1, . . . , n + 1 are exchangeable, then the prediction interval C(X n+1 ) constructed by the split CQR algorithm satisfies P{Y n+1 ∈ C(X n+1 )} ≥ 1 -α. Moreover, if the conformity scores E i are almost surely distinct, then the prediction interval is nearly perfectly calibrated:</p><formula xml:id="formula_21">P{Y n+1 ∈ C(X n+1 )} ≤ 1 -α + 1/(|I 2 | + 1).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Practical considerations and extensions</head><p>Conformalized quantile regression can accommodate a wide range of quantile regression methods <ref type="bibr">[18-23, 25, 30]</ref> to estimate the conditional quantile functions, q αlo and q αhi . The estimators can be even be aggregates of different quantile regression algorithms. Recently, new deep learning techniques have been proposed <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref> for constructing prediction intervals. These methods could be wrapped by our framework and would then immediately enjoy rigorous coverage guarantees. In our experiments, we focus on quantile neural networks <ref type="bibr" target="#b19">[20]</ref> and quantile regression forests <ref type="bibr" target="#b21">[22]</ref>.</p><p>Because the underlying quantile regression algorithm may process the proper training set in arbitrary ways, our framework affords broad flexibility in hyper-parameter tuning. Consider, for instance, the tuning of typical hyper-parameters of neural networks, such as the batch size, the learning rate, and the number of epochs. The hyperparameters may be selected, as usual, by cross validation, where we minimize the average interval length over the folds.</p><p>In this vein, we record two specific implementation details that we have found to be useful.</p><p>1. Quantile regression is sometimes too conservative, resulting in unnecessarily wide prediction intervals. In our experience, quantile regression forests <ref type="bibr" target="#b21">[22]</ref> are often overly conservative and quantile neural networks <ref type="bibr" target="#b19">[20]</ref> are occasionally so. We can mitigate this problem by tuning the nominal quantiles of the underlying method as additional hyper-parameters in cross validation. Notably, this tuning does not invalidate the coverage guarantee, but it may yield shorter intervals, as our experiments confirm. 2. To reduce the computational cost, instead of fitting two separate neural networks to estimate the lower and upper quantile functions, we can replace the standard one-dimensional estimate of the unknown response by a two-dimensional estimate of the lower and upper quantiles. In this way, most of the network parameters are shared between the two quantile estimators.</p><p>We adopt this approach in the experiments of Section 6.</p><p>Another avenue for extension is the conformalization step. The conformalization implemented by equations ( <ref type="formula" target="#formula_18">7</ref>) and ( <ref type="formula" target="#formula_20">8</ref>) allows coverage errors to be spread arbitrarily over the left and right tails. Using a method reminiscent of <ref type="bibr" target="#b33">[34]</ref>, we can control the left and right tails independently, yielding a stronger coverage guarantee. It is stated below and proved in the supplementary material. As we will see in Section 6, the price paid for the stronger coverage guarantee is slightly longer intervals. Theorem 2. Define the prediction interval</p><formula xml:id="formula_22">C(X n+1 ) := [q αlo (X n+1 ) -Q 1-αlo (E lo , I 2 ), qαhi (X n+1 ) + Q 1-αhi (E hi , I 2 )],</formula><p>where</p><formula xml:id="formula_23">Q 1-αlo (E lo , I 2 ) is the (1 -α lo )-th empirical quantile of {q αlo (X i ) -Y i : i ∈ I 2 } and Q 1-αhi (E hi , I 2 ) is the (1 -α hi )-th empirical quantile of {Y i -qαhi (X i ) : i ∈ I 2 }. If the samples (X i , Y i ), i = 1, . . . , n + 1 are exchangeable, then P{Y n+1 ≥ qαlo (X n+1 ) -Q 1-αlo (E lo , I 2 )} ≥ 1 -α lo and P{Y n+1 ≤ qαhi (X n+1 ) + Q 1-αhi (E hi , I 2 )} ≥ 1 -α hi .</formula><p>Consequently, assuming α = α lo + α hi , we also have P{Y n+1 ∈ C(X n+1 )} ≥ 1 -α.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work: locally adaptive conformal prediction</head><p>Locally adaptive split conformal prediction, first proposed in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref> and later studied in <ref type="bibr" target="#b14">[15]</ref>, is an earlier approach to making conformal prediction adaptive to heteroskedascity. Like our method, it starts from the observation that one can replace the absolute residuals in equation ( <ref type="formula" target="#formula_9">4</ref>) by any other loss function that treats the data exchangeably. In this case, the absolute residuals R i are replaced by the scaled residuals Ri :</p><formula xml:id="formula_24">= |Y i -μ(X i )|/σ(X i ) = R i /σ(X i ), i ∈ I 2 ,</formula><p>where σ(X i ) is a measure of the dispersion of the residuals at X i . Usually σ(x) is an estimate of the conditional mean absolute deviation (MAD) of |Y -μ(x)| given X = x. Finally, the prediction interval at X n+1 is computed</p><formula xml:id="formula_25">as C(X n+1 ) = μ(X n+1 ) -σ(X n+1 )Q 1-α ( R, I 2 ), μ(X n+1 ) + σ(X n+1 )Q 1-α ( R, I 2 )</formula><p>. Both μ and σ are fit only on the proper training set. Consequently, μ and σ satisfy the assumptions of conformal prediction and, hence, locally adaptive conformal prediction inherits the coverage guarantee of standard conformal prediction.</p><p>In practice, locally adaptive conformal prediction requires fitting two functions, in sequence, on the proper training set. (Thus is more computationally expensive than standard conformal prediction.) First, one fits the conditional mean function μ(x), as described in Section 3. Then one fits σ(x) to the pairs {(X i , R i ) : i ∈ I 1 }, using a regression model that predicts the residuals R i given the inputs X i . As an example, the intervals in Figure <ref type="figure" target="#fig_1">1b</ref> above are created by locally adaptive split conformal prediction, where both μ and σ are random forests.</p><p>Locally adaptive conformal prediction is limited in several ways, some more important than others. A first limitation, already noted in <ref type="bibr" target="#b14">[15]</ref>, appears when the data is actually homoskedastic. In this case, the locally adaptive method suffers from inflated prediction intervals compared to the standard method. This is presumably due to the extra variability introduced by estimating σ as well as μ.</p><p>The locally adaptive method faces a more fundamental statistical limitation. There is an essential difference between the residuals on the proper training set and the residuals on the calibration set: the former are biased by an optimization procedure designed to minimize them, while the latter are unbiased. Because it uses the proper training residuals (as it must to ensure valid coverage), the locally adaptive method tends to systematically underestimate the prediction error. In general, this forces the correction constant Q 1-α ( R, I 2 ) to be large and the intervals to be less adaptive.</p><p>To press this point further, suppose the conditional mean function μ is a deep neural network. It is well attested in the deep learning literature that, given enough training samples, the best prediction error is attained by "over-fitting" to the training data, in the sense that the training error is nearly zero.</p><p>The training residuals are then very poor estimates of the true prediction error, resulting in severe loss of adaptivity. Our method, in contrast, does not suffer from this problem because the original training objective is to estimate the lower and upper conditional quantiles, not the conditional mean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In this section we systematically compare our method, conformalized quantile regression, to the standard and locally adaptive versions of split conformal prediction. Among preexisting conformal prediction algorithms, we select leading variants that use random forests <ref type="bibr" target="#b9">[10]</ref> and neural networks <ref type="bibr" target="#b34">[35]</ref> for conditional mean regression. Specifically, we evaluate the original version of split conformal prediction (Section 3) using three regression algorithms: Ridge, Random Forests and Neural Net.</p><p>We evaluate locally adaptive conformal prediction (Section 5) using the same three underlying regression algorithms: Ridge Local, Random Forests Local, and Neural Net Local. Likewise, we configure our method (Algorithm 1) to use quantile random forests <ref type="bibr" target="#b21">[22]</ref>, CQR Random Forests, and quantile neural networks <ref type="bibr" target="#b19">[20]</ref>, CQR Neural Net. Finally, as a baseline, we also include the previous two quantile regression algorithms, but without any conformalization: Quantile Random Forests and Quantile Neural Net. The last two methods, in contrast to the others, do not have finite-sample coverage guarantees. All implementation details are available in the supplementary material.</p><p>We conduct the experiments on eleven benchmark datasets for regression, listed in the supplementary material. In each case, we standardize the features to have zero mean and unit variance and we rescale the response by dividing it by its mean absolute value. <ref type="foot" target="#foot_5">5</ref> The performance metrics are averaged over  Table <ref type="table" target="#tab_0">1</ref> summarizes our 2,200 experiments, showing the average performance across all the datasets and training-test splits. On average, our method achieves shorter prediction intervals than both standard and locally adaptive conformal prediction. It may seem surprising that our method also outperforms non-conformalized quantile regression, which is permitted more training data. There are several possible explanations for this. First, the non-conformalized methods sometimes overcover, but that is mitigated by our signed conformity scores <ref type="bibr" target="#b5">(6)</ref>. In addition, by using CQR, we can tune the quantiles of the underlying quantile regression algorithms using cross-validation (Section 4). Interestingly, CQR selects quantiles below the nominal level.</p><p>Turning to the issue of valid coverage, all methods based on conformal prediction successfully construct prediction bands at the nominal coverage rate of 90%, as the theory suggests they should. One of the non-conformalized methods, based on random forests, is slightly conservative, while the other, based on neural networks, tends to undercover. In fact, other authors have shown that the coverage of quantile neural networks depends greatly on the tuning of the hyper-parameters, with, for instance, the actual coverage in [25, Figure <ref type="figure">3</ref>] ranging from the nominal 95% to well below 50%. Such volatility demonstrates the importance of the conformal prediction's finite-sample guarantee.</p><p>When estimating a lower and an upper quantile by two separate quantile regressions, there is no guarantee that the lower estimate will actually be smaller than the upper estimate. This is known as the quantile crossing problem <ref type="bibr" target="#b36">[37]</ref>. Quantile crossing can affect quantile neural networks, but not quantile regression forests. When the two quantiles are far apart, as in the 5% and 95% quantiles, we should expect the estimates to cross very infrequently and that is indeed what we find in the experiments. Nevertheless, we also evaluated a post-processing method to eliminate crossings <ref type="bibr" target="#b37">[38]</ref>. It yields a slight improvement in performance: the average interval length of the CQR neural networks drops from 1.40 to 1.35, while the coverage rate remains the same. The average interval length of the unconformalized quantile neural networks drops from 1.50 to 1.40, with a decrease in the average coverage rate, from 88.87 to 87.99.</p><p>As expected, adopting the two-tailed, asymmetric conformalization proposed in Theorem 2 causes an increase in average interval length compared to the symmetric conformalization of Theorem 1. Specifically, the average length for CQR neural networks increases from 1.40 to 1.58, while the coverage rate stays about the same. The average length for the CQR random forests increases from 1.40 to 1.57, accompanied by a slight increase in the average coverage rate, from 90.34 to 90.94.</p><p>In a series of figures, provided in the supplementary material, we break down the performance of the different methods on each of the benchmark datasets. The performance on individual datasets confirms the overall trend in Table <ref type="table" target="#tab_0">1</ref>. Locally adaptive conformal prediction generally outperforms standard conformal prediction, and, on ten out of eleven datasets, conformalized quantile regression outperforms both. As a representative example, Figure <ref type="figure" target="#fig_2">2</ref> shows our results on a dataset (bio) about the physicochemical properties of protein tertiary structure <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Conformal quantile regression is a new way of constructing prediction intervals that combines the advantages of conformal prediction and quantile regression. It provably controls the miscoverage rate in finite samples, under the mild distributional assumption of exchangeability, while adapting the interval lengths to heteroskedasticity in the data.</p><p>We expect the ideas behind conformal quantile regression to be applicable in the related setting of conformal predictive distributions <ref type="bibr" target="#b38">[39]</ref>. In this extension of conformal prediction, the aim is to estimate a predictive probability distribution, not just an interval. We see intriguing connections between our work and a very recent, independently written paper on conformal distributions <ref type="bibr" target="#b16">[17]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Split: Avg. coverage 91.4%; Avg. length 2.91. (b) Local: Avg. coverage 91.7%; Avg. length 2.86. (c) CQR: Avg. coverage 91.06%; Avg. length 1.99. (d) Length of prediction intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Prediction intervals on simulated heteroscedastic data with outliers (see the supplementary material for a full range display): (a) the standard split conformal method, (b) its locally adaptive variant, and (c) CQR (our method). The length of the interval as a function of X is shown in (d).The target coverage rate is 90%. The broken black curve in (a) and (b) is the pointwise prediction from the random forest estimator. In (c), we show two curves, representing the lower and upper quantile regression estimates based on random forests<ref type="bibr" target="#b21">[22]</ref>. Observe how in this example the quantile regression estimates closely match the adjusted estimates-the boundary of the blue region-obtained by conformalization.</figDesc><graphic coords="5,309.96,231.98,194.04,135.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Average length (left) and coverage (right) of prediction intervals (α = 0.1) on the bio dataset<ref type="bibr" target="#b35">[36]</ref>. The numbers in the colored boxes are the average lengths, shown in red for split conformal, in gray for locally adapative split conformal, and in light blue for our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>20 different training-test splits; 80% of the examples are used for training and the remaining 20% Length and coverage of prediction intervals (α = 0.1), averaged across 11 datasets and 20 random training-test splits. Our methods are shown in bold font. The methods marked by an asterisk are not supported by finite-sample coverage guarantees.</figDesc><table><row><cell>Method</cell><cell cols="2">Avg. Length Avg. Coverage</cell></row><row><cell>Ridge</cell><cell>3.07</cell><cell>90.08</cell></row><row><cell>Ridge Local</cell><cell>2.93</cell><cell>90.14</cell></row><row><cell>Random Forests</cell><cell>2.24</cell><cell>90.00</cell></row><row><cell>Random Forests Local</cell><cell>1.82</cell><cell>89.99</cell></row><row><cell>Neural Net</cell><cell>2.20</cell><cell>89.95</cell></row><row><cell>Neural Net Local</cell><cell>1.79</cell><cell>90.02</cell></row><row><cell>CQR Random Forests</cell><cell>1.40</cell><cell>90.34</cell></row><row><cell>CQR Neural Net</cell><cell>1.40</cell><cell>90.02</cell></row><row><cell>*Quantile Random Forests</cell><cell>*2.21</cell><cell>*92.62</cell></row><row><cell>*Quantile Neural Net</cell><cell>*1.50</cell><cell>*88.87</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>An implementation of CQR is available online at https://github.com/yromano/cqr.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>Like conformal regression, CQR has a variant that does not require data splitting.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>In full conformal prediction, the regression algorithm must treat the data exchangeably, but no such restrictions apply to split conformal prediction.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>The explicit formula for empirical quantiles is recalled in the supplementary material.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5"><p>In the experiments, we compute the needed sample means and variances only on the proper training set. This ensures that if the original data is exchangeable, then the rescaled data remains so. That being said, we could also rescale using sample means and variances computed on the test data, because it would preserve exchangeability even while it destroys independence.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements E. C. was partially supported by the Office of Naval Research (ONR) under grant N00014-16-1-2712, by the Army Research Office (ARO) under grant W911NF-17-1-0304, by the Math + X award from the Simons Foundation and by a generous gift from TwoSigma. E. P. and Y. R. were partially supported by the ARO grant. Y. R. was also supported by the same Math + X award. Y. R. thanks the Zuckerman Institute, ISEF Foundation and the Viterbi Fellowship, Technion, for providing additional research support. We thank Chiara Sabatti for her insightful comments on a draft of this paper and Ryan Tibshirani for his crucial remarks on our early experimental findings.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Machine-learning applications of algorithmic randomness</title>
		<author>
			<persName><forename type="first">Volodya</forename><surname>Vovk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Gammerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Saunders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="444" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Inductive confidence machines for regression</title>
		<author>
			<persName><forename type="first">Harris</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kostas</forename><surname>Proedrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volodya</forename><surname>Vovk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Gammerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="345" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Algorithmic learning in a random world</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vovk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Gammerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glenn</forename><surname>Shafer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On-line predictive linear regression</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vovk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilia</forename><surname>Nouretdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Gammerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1566" to="1590" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distribution-free prediction sets</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Robins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Wasserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">501</biblScope>
			<biblScope unit="page" from="278" to="287" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Distribution-free prediction bands for non-parametric regression</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Wasserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="96" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Normalized nonconformity measures for regression conformal prediction</title>
		<author>
			<persName><forename type="first">Harris</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Gammerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volodya</forename><surname>Vovk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Applications</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="64" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inductive conformal prediction: Theory and application to neural networks</title>
		<author>
			<persName><forename type="first">Harris</forename><surname>Papadopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tools in artificial intelligence. IntechOpen</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Regression conformal prediction with nearest neighbours</title>
		<author>
			<persName><forename type="first">Harris</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vovk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Gammerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="815" to="840" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Regression conformal prediction with random forests</title>
		<author>
			<persName><forename type="first">Ulf</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><surname>Boström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuve</forename><surname>Löfström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><surname>Linusson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="155" to="176" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Regression trees for streaming data with local performance guarantees</title>
		<author>
			<persName><forename type="first">Ulf</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cecilia</forename><surname>Sönströd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><surname>Linusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><surname>Boström</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Big Data</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="461" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient conformal regressors using bagged neural nets</title>
		<author>
			<persName><forename type="first">Ulf</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cecilia</forename><surname>Sönströd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><surname>Linusson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cross-conformal predictors</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vovk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematics and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="9" to="28" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Accelerating difficulty estimation for conformal regression forests</title>
		<author>
			<persName><forename type="first">Henrik</forename><surname>Boström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><surname>Linusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuve</forename><surname>Löfström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulf</forename><surname>Johansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematics and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="125" to="144" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distribution-free predictive inference for regression</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G'</forename><surname>Max</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">J</forename><surname>Rinaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><surname>Wasserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">523</biblScope>
			<biblScope unit="page" from="1094" to="1111" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Discretized conformal prediction for efficient distribution-free inference</title>
		<author>
			<persName><forename type="first">Wenyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelli-Jean</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rina</forename><forename type="middle">Foygel</forename><surname>Barber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">173</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vovk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Petej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Toccaceli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Gammerman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06579</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Conformal calibrators. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Regression quantiles</title>
		<author>
			<persName><forename type="first">Roger</forename><surname>Koenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilbert</forename><surname>Bassett</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica: Journal of the Econometric Society</title>
		<imprint>
			<biblScope unit="page" from="33" to="50" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Quantile regression via an MM algorithm</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Lange</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="60" to="77" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A quantile regression neural network approach to estimating the conditional density of multiperiod returns</title>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="299" to="311" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Quantile regression</title>
		<author>
			<persName><forename type="first">Roger</forename><surname>Koenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">F</forename><surname>Hallock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Perspectives</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="143" to="156" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Quantile regression forests</title>
		<author>
			<persName><forename type="first">Nicolai</forename><surname>Meinshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="983" to="999" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Nonparametric quantile estimation</title>
		<author>
			<persName><forename type="first">Ichiro</forename><surname>Takeuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">D</forename><surname>Sears</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1231" to="1264" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Estimating conditional quantiles with the help of the pinball loss</title>
		<author>
			<persName><forename type="first">Ingo</forename><surname>Steinwart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Christmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Natasa</forename><surname>Tagasovska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00908</idno>
		<title level="m">Frequentist uncertainty estimates for deep learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dropout as a Bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Landslide displacement prediction with uncertainty based on neural networks with random hidden weights</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun</forename><surname>Lung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2683" to="2695" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Balaji Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6402" to="6413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">High-quality prediction intervals for deep learning: A distribution-free, ensembled approach</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Brintrup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Neely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6473" to="6482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Greedy function approximation: A gradient boosting machine</title>
		<author>
			<persName><forename type="first">Jerome</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Direct use of regression quantiles to construct confidence sets in linear models</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>Portnoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="287" to="306" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Statistical inference on heteroscedastic models based on regression quantiles</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>Portnoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Nonparametric Statistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="239" to="260" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Trimmed conformal prediction for high-dimensional models</title>
		<author>
			<persName><forename type="first">Wenyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaokai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wooseok</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rina</forename><forename type="middle">Foygel</forename><surname>Barber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09933</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Signed-error conformal regression</title>
		<author>
			<persName><forename type="first">Henrik</forename><surname>Linusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulf</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuve</forename><surname>Löfström</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="224" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Reliable prediction intervals with regression neural networks</title>
		<author>
			<persName><forename type="first">Harris</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haris</forename><surname>Haralambous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="842" to="851" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Physicochemical properties of protein tertiary structure data set</title>
		<ptr target="https://archive.ics.uci.edu/ml/datasets/Physicochemical+Properties+of+Protein+Tertiary+Structure" />
		<imprint>
			<date type="published" when="2019-01">January, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An empirical quantile function for linear models with iid errors</title>
		<author>
			<persName><forename type="first">Gilbert</forename><surname>Bassett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jr</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Koenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">378</biblScope>
			<biblScope unit="page" from="407" to="415" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Quantile and probability curves without crossing</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Chernozhukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iván</forename><surname>Fernández-Val</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfred</forename><surname>Galichon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1093" to="1125" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Nonparametric predictive distributions based on conformal prediction</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vovk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieli</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valery</forename><surname>Manokhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Ge</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="30" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
