<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Tambet</forename><surname>Matiisen</surname></persName>
							<email>tambet.matiisen@ut.ee</email>
						</author>
						<author>
							<persName><forename type="first">Avital</forename><surname>Oliver</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Taco</forename><surname>Cohen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
						</author>
						<author>
							<persName><surname>Matiisen</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science</orgName>
								<orgName type="institution">University of Tartu</orgName>
								<address>
									<postCode>51005</postCode>
									<settlement>Tartu</settlement>
									<country key="EE">Estonia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">A. Oliver was with OpenAI</orgName>
								<address>
									<postCode>94110</postCode>
									<settlement>San Francisco</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Google Brain</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Informatics Institute</orgName>
								<orgName type="department" key="dep2">Schulman is with OpenAI</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<postCode>1012 WX, 94110</postCode>
									<settlement>Amsterdam, San Francisco</settlement>
									<region>CA</region>
									<country>The Netherlands. J., USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">033914FEF9407BDD9C10D88E73C75DBD</idno>
					<idno type="DOI">10.1109/TNNLS.2019.2934906</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Active learning</term>
					<term>curriculum learning</term>
					<term>deep reinforcement learning</term>
					<term>learning progress</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose Teacher-Student Curriculum Learning (TSCL), a framework for automatic curriculum learning, where the Student tries to learn a complex task, and the Teacher automatically chooses subtasks from a given set for the Student to train on. We describe a family of Teacher algorithms that rely on the intuition that the Student should practice more those tasks on which it makes the fastest progress, i.e., where the slope of the learning curve is highest. In addition, the Teacher algorithms address the problem of forgetting by also choosing tasks where the Student's performance is getting worse. We demonstrate that TSCL matches or surpasses the results of carefully hand-crafted curricula in two tasks: addition of decimal numbers with long shortterm memory (LSTM) and navigation in Minecraft. Our automatically ordered curriculum of submazes enabled to solve a Minecraft maze that could not be solved at all when training directly on that maze, and the learning was an order of magnitude faster than a uniform sampling of those submazes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Deep reinforcement learning algorithms have been used to solve difficult tasks in video games <ref type="bibr" target="#b0">[1]</ref>, locomotion <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, and robotics <ref type="bibr" target="#b3">[4]</ref>. However, tasks with sparse rewards like "Robot, arrange chairs in the meeting room as a circle" remain challenging to solve with direct application of these algorithms. The main reason for this is the large number of timesteps to achieve the reward, which is challenging both for credit assignment (which of the actions contributed to getting the reward) and exploration (which actions to try next). For example, it is known that the number of samples needed to solve a task with random exploration increases exponentially with the number of steps to get a reward <ref type="bibr" target="#b4">[5]</ref>. One approach to overcome this problem is to use curriculum learning <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b8">[9]</ref>, where tasks are ordered by increasing difficulty and training only proceeds to harder tasks once easier ones are mastered. Curriculum learning helps when, after mastering a simpler task, the policy for a harder task is discoverable through local exploration.</p><p>To use curriculum learning, the researcher must:</p><p>1) be able to order subtasks by difficulty; 2) decide on a "mastery" threshold. This can be based on achieving a certain score <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, which requires a priori knowledge of acceptable performance of each task. Alternatively, this can be based on a plateau of performance, which can be hard to detect given the noise in the learning curve; 3) continuously mix in easier tasks while learning harder ones to avoid forgetting. Designing these mixtures is effectively challenging <ref type="bibr" target="#b6">[7]</ref>. In this brief, we describe a new approach called Teacher-Student Curriculum Learning (TSCL). The Student is the model being trained. The Teacher monitors the Student's training progress and determines the tasks on which the Student should train at each training step, in order to maximize the Student's progression through the curriculum. The Student can be any machine learning model. The Teacher is itself learning about the Student as it is giving tasks, all as part of a single training session.</p><p>We describe several Teacher algorithms based on the notion of learning progress <ref type="bibr" target="#b9">[10]</ref>. The main idea is that the Student should practice more tasks on which it is making the fastest progress, i.e., the learning curve slope is highest. To counter forgetting, the Student should also practice tasks where the performance is getting worse, i.e., the learning curve slope is negative.</p><p>The main contributions of this brief are as follows.</p><p>1) We formalize TSCL, a Teacher-Student framework for curriculum learning as partially observable Markov decision process (POMDP) <ref type="bibr" target="#b10">[11]</ref>. 2) We propose a family of algorithms based on the notion of learning progress. The algorithms also address the problem of forgetting previous tasks. 3) We evaluate the algorithms on supervised and reinforcement learning tasks: addition of decimal numbers with long shortterm memory (LSTM) and navigation in Minecraft. The code is available at https://github.com/tambetm/ TSCL.</p><p>II. TEACHER-STUDENT SETUP Fig. <ref type="figure" target="#fig_0">1</ref> illustrates the Teacher-Student interaction. At each timestep, the Teacher chooses tasks for the Student to practice on. The Student trains on those tasks and returns back a score. The Teacher's goal is for the Student to succeed on a final task with as few training steps as possible. Generally, the task is parameterized by a categorical value representing one of N tasks, but one can also imagine multi-dimensional or continuous task parameterization. The score can be, for example, episode total reward in reinforcement learning or validation set accuracy in supervised learning.</p><p>We formalize the Teacher's goal of helping the Student to learn a final task as solving a POMDP <ref type="bibr" target="#b10">[11]</ref>. POMDP extends the Markov decision process (MDP) <ref type="bibr" target="#b11">[12]</ref> to cases where the full system state is not visible and decisions have to be made based on a partial view of the system state-the observations. We present two POMDP formulations: 1) simple, best suited for reinforcement learning and 2) batch, best suited for supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Simple POMDP Formulation</head><p>The simple POMDP formulation exposes the score of the Student on a single task and is well-suited for reinforcement learning problems.</p><p>1) The state s t represents the entire state of the Student (i.e., neural network parameters and optimizer state) and is not observable to the Teacher.</p><p>2) The action a t corresponds to the parameters of the task chosen by Teacher. In the following, we only consider a discrete task parameterization, meaning that Teacher chooses one of N subtasks. Taking an action means training Student on that task for a certain number of iterations.</p><p>3) The observation o t is the score x (a t ) t of the task a t the Student trained on at timestep t, e.g., the episode total reward. Although in theory, the Teacher could also observe other aspects of the Student state like network weights, for simplicity, we choose to expose only the score. 4) Reward r t is the change in score for the task the Student trained on at timestep t:</p><formula xml:id="formula_0">r t = x (a t ) t -x (a t ) t i</formula><p>, where t i is the previous timestep when the same task was trained on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Batch POMDP Formulation</head><p>In supervised learning, a training batch can include multiple tasks. This motivates the batch formulation of the POMDP.</p><p>1) The state s t represents the training state of the Student.</p><p>2) The action a t represents a probability distribution over N tasks.</p><p>Each training batch is sampled according to the distribution:</p><formula xml:id="formula_1">a t = ( p (1) t , . . . , p (N) t ), where p (i)</formula><p>t is the probability of task i at timestep t.</p><p>3) The observation o t is the scores of all tasks after the training step:</p><formula xml:id="formula_2">o t = (x (1) t , . . . , x (N) t )</formula><p>In the simplest case, the scores could be accuracies of the tasks calculated on the training set. However, in the case of minibatch training, the model evolves during training, and therefore, additional evaluation pass is needed anyway after training step to produce consistent results. Therefore, we use a separate validation set that contains a uniform mix of all tasks to produce these scores. 4) The reward r t is the sum of changes in evaluation scores from the previous timestep:</p><formula xml:id="formula_3">r t = N i=1 x (i) t -x (i)</formula><p>t -1 . This setup could also be used with reinforcement learning by performing training in batches of episodes. However, because scoring one sample (one episode) in reinforcement learning is usually much more computationally expensive than in supervised learning, it makes sense to use simple POMDP formulation and make decision about the next task immediately after each training step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Optimization Criteria</head><p>For either of the POMDP formulations, maximizing the Teacher episode total reward is equivalent to maximizing the score of all tasks at the end of the training session:</p><formula xml:id="formula_4">T t =1 r t = N i=1 x (i) T i</formula><p>, where T i is the last training step where task i was being trained on 1 .</p><p>Although an obvious choice for optimization criteria would have been the performance in the final task, initially, the Student might not have any success in the final task and this does not provide any meaningful feedback signal to the Teacher. Therefore, we choose to maximize the sum of performances in all tasks. The assumption, here, is that in curriculum learning, the final task includes the elements of all previous tasks; therefore, good performance in the intermediate tasks usually leads to good performance in the final task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. ALGORITHMS</head><p>POMDPs are typically solved using reinforcement learning algorithms. However, those require many training episodes, while we aim 1 Due to telescoping summation canceling out all x (i) t terms but the T i th .</p><p>Fig. <ref type="figure" target="#fig_1">2</ref>. Idealistic curriculum learning. Left: task scores improve over time, the next task starts improving once the previous task has been mastered. Right: task sampling probability depends on the slope (or derivative) of the learning curve on the left-the task is sampled more as long as its performance is improving.</p><p>to train the Student in one Teacher episode. Therefore, we resort to simpler heuristics. The basic intuition is that the Student should practice those tasks more for which it is making most progress <ref type="bibr" target="#b9">[10]</ref>, while also practicing tasks that are at risk of being forgotten. Fig. <ref type="figure" target="#fig_1">2</ref> shows a demonstration of the ideal training progress in a curriculum learning setting as follows.</p><p>1) At first, the Teacher has no knowledge so it samples from all tasks uniformly (iteration 0). 2) When the Student starts making progress on task 1, the Teacher allocates more probability mass to this task (iterations 250-750). 3) When the Student masters task 1, its learning curve flattens, and the Teacher samples the task less often. At this point, Student also starts making progress on task 2, so the Teacher samples more from task 2 (iterations 750-1250). 4) This continues until the Student masters all tasks (iteration 2000). As all task learning curves flatten in the end, the Teacher returns to uniform sampling of the tasks (iterations 2000-3000). The process shown in Fig. <ref type="figure" target="#fig_1">2</ref> is idealistic, since, in practice, some unlearning often occurs, i.e., when most of the probability mass is allocated to task 2, performance on task 1 might get worse. To counter this, the Student should also practice all learned tasks, especially those where unlearning occurs. For this reason, we sample tasks according to the absolute value of the slope of the learning curve instead. If the change in scores is negative, this must mean that unlearning occurred and this task should be practiced more.</p><p>This intuitive description alone does not prescribe an algorithm. We need to propose a method of estimating learning curve slope from noisy task scores and a way to balance exploration and exploitation. We take inspiration from algorithms for the nonstationary multi-armed bandit problem <ref type="bibr" target="#b12">[13]</ref> and adapt them to TSCL. For brevity, we only give the descriptions of simple formulation algorithms here, the batch formulation algorithms can be found in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Online Algorithm</head><p>The online algorithm (see Algorithm 1) is inspired by the basic nonstationary bandit algorithm <ref type="bibr" target="#b12">[13]</ref>. It uses exponentially weighted moving average to track the expected return Q from different tasks</p><formula xml:id="formula_5">Q t +1 (a t ) = αr t + (1 -α)Q t (a t )</formula><p>where α is learning rate and r t = x</p><formula xml:id="formula_6">(a t ) t -x (a t ) t</formula><p>is the difference in score from the previous timestep t when the same task was practiced. The next task can be chosen by -greedy exploration: sample a random task with probability or use arg max a Q t (a) otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Online Algorithm</head><p>Initialize STUDENT learning algorithm Initialize expected return Q(a) = 0 for all N tasks for t = 1, . . . , T do Choose task a t based on |Q(a)| using -greedy or Boltzmann policy Train STUDENT using task a t and observe reward r t = x </p><formula xml:id="formula_7">(a t ) t - x (a t ) t Update expected return Q(a t ) = αr t + (1 -α)Q(a t )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. , K do</head><p>Train STUDENT using task a t and observe score</p><formula xml:id="formula_8">o t = x (a t ) t</formula><p>Store score o t in list D Apply linear regression to D and extract the coefficient as</p><formula xml:id="formula_9">r t Update expected return Q(a t ) = αr t + (1 -α)Q(a t )</formula><p>Alternatively, the next task can be chosen using the Boltzmann distribution</p><formula xml:id="formula_10">p(a) = e Q t (a)/τ N i=1 e Q t (i)/τ</formula><p>where τ is the temperature of the Boltzmann distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Naive Algorithm</head><p>To estimate the learning progress more reliably, one should practice the task several times. The Naive algorithm (see Algorithm 2) trains each task K times, observes the resulting scores, and estimates the slope of the learning curve using linear regression. The regression coefficient is used as the reward in the above-mentioned nonstationary bandit algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Window Algorithm</head><p>Repeating the task, a fixed number of times is expensive, when clearly no progress is made. The window algorithm (see Algorithm 3) keeps FIFO buffer of last K scores for each task, and timesteps when these scores were recorded. Linear regression is performed to estimate the slope of the learning curve for each task, with the timesteps as the input variables. The regression coefficient is used as the reward in the above-mentioned nonstationary bandit algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Sampling Algorithm</head><p>The previous algorithms require tuning of hyperparameters to balance exploration. To get rid of exploration hyperparameters, we take inspiration from Thompson sampling. The sampling algorithm (see Algorithm 4) keeps a buffer of last K rewards for each task. To choose the next task, a recent reward is sampled from each task's K -last-rewards buffer. Then, whichever task yielded the highest sampled reward is chosen. This makes exploration a natural part of the algorithm: tasks that have recently had high rewards are sampled more often.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Keylock MDP</head><p>Keylock MDP <ref type="bibr" target="#b4">[5]</ref> is a simple task that is challenging for current reinforcement learning methods. N-step keylock MDP (see Fig. <ref type="figure" target="#fig_2">3</ref>) is basically a chain of n states plus final state. From each nonfinal state, the agent can take two actions: one that moves the agent forward to the next state and the other that moves the agent back to the beginning of the chain. The agent is rewarded with +1 when it reaches the final state.</p><p>As the agent does not get any rewards until it reaches the final state, for the first time, it can only do random exploration at best. It is easy to see that, for the general case, it needs 2 n episodes to try all possible combinations of actions in all states before it reaches the reward. Therefore, the time required to solve this MDP is exponential with respect to the length n of the chain.</p><p>We created a curriculum for keylock MDP by treating solving the MDP from each state s i as a separate task i. It is expected that solving the MDP from later states is easier than from early states. We used tabular Q-learning <ref type="bibr" target="#b13">[14]</ref> with the Boltzmann exploration as the Student algorithm (see Appendix B for details). Our goal with this experiment was to show that the algorithms recover the natural ordering of curriculum tasks-backwards from the final state. In addition, it showcases the main intuition behind the algorithm-it starts with easy tasks where the solution can be found with limited  exploration and proceeds with gradually more complex tasks where only local exploration is needed to ground the solution to previously solved tasks.</p><p>Fig. <ref type="figure" target="#fig_3">4</ref> shows the average reward for ten-step keylock MDP after 1000 episodes. This task was unsolvable in 1000 episodes when trained only from the first state s 1 . Our algorithms all performed significantly better than a uniform sampling of initial states. Fig. <ref type="figure" target="#fig_4">5</ref> shows an example of chosen tasks (initial states) during one training session. The diagonal on the figure shows that the curriculum discovered matches the intuition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Decimal Number Addition</head><p>Addition of decimal numbers with LSTM is a well-known task that requires a curriculum to be learned in reasonable time <ref type="bibr" target="#b6">[7]</ref>. It is implemented as sequence-to-sequence model <ref type="bibr" target="#b14">[15]</ref>, where the input to the network is two decimal-coded numbers separated by a "plus" sign, and output of the network is the sum of those numbers, also in decimal coding. The curriculum is based on the number of digits in the input numbers-it is easier to learn the addition of short numbers and then move on to longer numbers.</p><p>Number addition is a supervised learning problem and, therefore, can be trained more efficiently by including several curriculum tasks in the minibatch. Therefore, we adopt batch training scheme as outlined in Section II-B. The score we use is the accuracy of each task calculated on the validation set. The results shown are means Fig. <ref type="figure">6</ref>. Results of nine-digit 1-D addition experiment. The Y -axis shows the number of epochs to achieve 99% validation accuracy, lower is better. Variants using the absolute value of the slope surpass the best manual curriculum ("combined"). Fig. <ref type="figure">7</ref>. Progression of the task distribution over time for nine-digit 1-D addition (sampling algorithm). The algorithm progresses from simpler tasks to more complicated. Harder tasks take longer to learn, and the algorithm keeps training on easier tasks to counter unlearning. and standard deviations of three runs with different random seeds. Full experiment details can be found in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Addition With 1-D Curriculum:</head><p>We started with a similar setup to <ref type="bibr" target="#b6">[7]</ref>, where the curriculum task determines the maximum number of digits in both added numbers. The results are shown in Fig. <ref type="figure">6</ref>. Our algorithms outperformed uniform sampling and the best manual curriculum ("combined") for nine-digit addition from <ref type="bibr" target="#b6">[7]</ref>. An example of the task distribution during training is given in Fig. <ref type="figure">7</ref>.</p><p>2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>) Addition With 2-D Curriculum:</head><p>We also experimented with a curriculum where the ordering of tasks is not obvious. We used the same decimal addition task, but in this case, the length of each number is chosen separately, making the task-space 2-D. Each training batch is modeled as a probability distribution over the length of both numbers P(l 1 , l 2 ). We also tried making this distribution independent such that P(l 1 , l 2 ) = P(l 1 )P(l 2 ), but that did not work as well.</p><p>There is no equivalent experiment in <ref type="bibr" target="#b6">[7]</ref>, so we created a manual curriculum inspired by their best 1-D curriculum. In particular, we increase difficulty by increasing the maximum length of both two numbers, which reduces the problem to a 1-D curriculum. Fig. <ref type="figure" target="#fig_5">8</ref> shows the results for nine-digit 2-D addition. Fig. <ref type="figure">9</ref> illustrates different approaches taken by the manual and automated curriculum.</p><p>3) Observations: 1) Using an absolute value of Q boosts the performance of almost all the algorithms, which means that it is efficient in countering forgetting. 2) There is no universal best algorithm. For 1-D, the window algorithm, and for 2-D, the Naive algorithm performed the best. The task seems easier, the manual curriculum is hard to beat, and uniform sampling is competitive.</p><p>Fig. <ref type="figure">9</ref>. Accuracy progress for four-digit 2-D addition, X-and Y -axes show the length of two numbers, darker color means higher accuracy. Top: TSCL. Bottom: the best manual curriculum. Our algorithm takes distinctively different approaches by training on shorter numbers first. Nine-digit videos can be found at https://youtu.be/y_QIcQ6spWk and https:// youtu.be/fB2kx-esjgw.</p><p>Sampling is competitive in both and has the least hyperparameters.</p><p>3) Whether -greedy or Boltzmann exploration works better depends on the algorithm. 4) Uniform sampling is surprisingly efficient, especially in 2D case. 5) The 2-D task is solved faster, and the manual curriculum is hard to beat in 2-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Minecraft</head><p>Minecraft is a popular 3-D video game where players can explore, craft tools, and build arbitrary structures, making it a potentially rich environment for reinforcement learning research. We used the Malmo platform <ref type="bibr" target="#b15">[16]</ref> with OpenAI Gym wrapper 2 to interact with Minecraft in our reinforcementlearning experiments. In particular, we used ClassroomDecorator from Malmo to generate random mazes for the agent to solve. The mazes contain sequences of rooms separated by the following obstacles.</p><p>1) Wall: The agent has to locate a doorway in the wall.</p><p>2) Lava: The agent has to cross a bridge over lava. We created a simple curriculum with the following five steps. 1) Maze 1: A single room with a target.</p><p>2) Maze 2: Two rooms separated by lava.</p><p>3) Maze 3: Two rooms separated by wall. 4) Maze 4: Three rooms separated by lava and wall, in random order.</p><p>2 https://github.com/tambetm/gym-minecraft Fig. <ref type="figure" target="#fig_0">10</ref>. Mazes in five-step curriculum. The starting position of the agent and the location of the target were randomized within the room for each episode.</p><p>5) Maze 5: Four rooms separated by lava and walls, in random order. Fig. <ref type="figure" target="#fig_0">10</ref> shows the room layout in mazes. Manual curriculum trained first task for 200 000 steps, second, third and fourth tasks for 400 000 steps, and fifth task for 600 000 steps.</p><p>We only implemented the window algorithm for the Minecraft task, because other algorithms rely on score change, which is not straightforward to calculate for parallel training scheme. We use three baselines: uniform sampling, training only on the last task, and a manually tuned curriculum. Full experimental details can be found in Appendix D.</p><p>Fig. <ref type="figure" target="#fig_0">11</ref> shows learning curves for Minecraft five-step curriculum. The mean curve and standard deviation are based on three runs with different random seeds.</p><p>Video of the trained agent can be found here: https://youtu. be/cada0d_aDIc. The learned policy is robust to the number of rooms, given that obstacles are of the same type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RELATED WORK</head><p>Work by Bengio et al. <ref type="bibr" target="#b5">[6]</ref> sparked general interest in curriculum learning. More recent results include learning to execute short programs <ref type="bibr" target="#b6">[7]</ref>, finding shortest paths in graphs <ref type="bibr" target="#b7">[8]</ref>, and learning to play first-person shooter <ref type="bibr" target="#b8">[9]</ref>. All those works rely on manually designed curricula and do not attempt to produce it automatically.</p><p>The idea of using learning progress as the reward could be traced back to <ref type="bibr" target="#b16">[17]</ref>. It has been successfully applied in the context of developmental robotics to learn object manipulation <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> and also in actual classroom settings to teach primary school students <ref type="bibr" target="#b19">[20]</ref>. Using learning progress as the reward can be linked to the concept of intrinsic motivation <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b20">[21]</ref>.</p><p>More recently, Sukhbaatar et al. <ref type="bibr" target="#b21">[22]</ref> proposed a method to generate incremental goals and, therefore, curricula automatically. The setup consists of two agents, Alice and Bob, where Alice is generating trajectories and Bob is trying to either repeat or reverse them. Similar work in <ref type="bibr" target="#b22">[23]</ref> uses the generative adversarial network to generate goal states for an agent. Compared to TSCL, they are able to generate new subtasks on the go, but this mainly aids in exploration and is not guaranteed to help in learning the final task.</p><p>The most similar work to ours was done concurrently in <ref type="bibr" target="#b23">[24]</ref>. Although the problem statement is strikingly similar, our approaches differ. They apply the automatic curriculum learning only to supervised sequence learning tasks, while we consider also reinforcement learning tasks. They use the EXP3.S algorithm for adversarial bandits while we propose alternative algorithms inspired by nonstationary bandits. They consider other learning progress metrics based on complexity gain while we focus only on prediction gain (which performed overall best in their experiments). They do not attempt to counter unlearning, which we do by using the absolute value of the prediction gain. Moreover, their work only uses uniform sampling of tasks as a baseline, whereas ours compares the best known manual curriculum for the given tasks. In summary, they arrive at very similar conclusions.</p><p>Decimal addition has also been explored in <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b26">[27]</ref>, sometimes improving results over original work in <ref type="bibr" target="#b6">[7]</ref>. Our goal was not to improve the addition results but to evaluate different curriculum approaches; therefore, there is no direct comparison.</p><p>Minecraft is a relatively recent addition to reinforcement learning environments. The work in <ref type="bibr" target="#b27">[28]</ref> evaluates memory-based architectures for Minecraft. They use cognition-inspired tasks in visual gridworld. Our tasks differ in that they do not need explicit memory, and the movement is continuous, not grid-world. Another work in <ref type="bibr" target="#b28">[29]</ref> uses tasks similar to ours but they take different approaches: they learn a Deep Skill Module for each subtask, freeze weights of those modules, and train hierarchical deep reinforcement learning network to pick either single actions or subtask policies. In contrast, our approach uses simple policy network and relies on the TSCL to learn (and not forget) the subtasks.</p><p>Although exploration bonuses <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b31">[32]</ref> solve the same problem of sparse rewards, they apply to Student algorithms, while we were considering different Teacher approaches. For this reason, we leave the comparison to future work.</p><p>Curriculum learning has also been successfully applied to semisupervised learning. It has been shown <ref type="bibr" target="#b32">[33]</ref> that label propagation algorithms benefit from simple-to-hard ordering of unlabeled samples. The simplicity of a sample can be estimated from its label reliability and discriminability. These, in turn, can be estimated using learned model <ref type="bibr" target="#b32">[33]</ref>, by assessing noisyness of labels <ref type="bibr" target="#b33">[34]</ref>, by using multiple input modalities <ref type="bibr" target="#b34">[35]</ref>, or by using ensemble of models <ref type="bibr" target="#b35">[36]</ref>. These approaches are not directly applicable in supervised or reinforcement learning settings, and they do not make use of learning progress or attempt to counter unlearning, which are the main points of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We presented a framework for automatic curriculum learning that can be used for supervised and reinforcement learning tasks. We proposed a family of algorithms within that framework based on the concept of learning progress. Although many of the algorithms performed equally well, it was crucial to rely on the absolute value of the slope of the learning curve when choosing the tasks. This guarantees the retraining on tasks which the network is starting to forget. In our LSTM decimal addition experiments, the sampling algorithm outperformed the best manually designed curriculum as well as the uniform sampling. On the challenging five-task Minecraft navigation problem, our window algorithm matched the performance of a carefully designed manual curriculum and significantly outperformed uniform sampling. For problems where curriculum learning is necessary, TSCL can avoid the tedium of ordering the difficulty of subtasks and hand-designing the curriculum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. FUTURE WORK</head><p>This work can be seen as the first step in solving full Minecraft benchmark suite<ref type="foot" target="#foot_0">3</ref> , which involves additional obstacles such as doors, jumps, ladders, and stairs. Curriculum learning proved to be a suitable paradigm for solving this task, and TSCL shows a promise in helping to automatically construct curriculum of different room sizes, maze lengths, and obstacle types.</p><p>For simplicity, we only considered discrete task parameterizations in this work. The same idea can be applied to continuous task parameterizations by replacing discrete categorical distribution with Gaussian distribution or a mixture of Gaussians. This is left as future work.</p><p>Another promising idea to explore is the usage of automatic curriculum learning in contexts where the subtasks have not been predefined. For example, subtasks can be sampled from a generative model or taken from different initial states in the same environment.  <ref type="bibr" target="#b13">[14]</ref> with the Boltzmann exploration was used as the Student algorithm. The Boltzmann exploration was chosen over -greedy to make the learning curve smoother-with -greedy policy, the learning curve would be almost a step function, and application of those algorithms would not make sense. After 1000 train episodes, 100 test episodes were used to calculate the average reward. The Boltzmann exploration was also used during those test episodes, so that the average reward would better represent the convergence level of the Q-table. Tabular Q-learning agent used temperature τ = 0.2, learning rate α = 0.02, and discount rate γ = 0.99. These numbers were chosen to make figures more illustrative, they do not significantly affect the behavior of algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><formula xml:id="formula_11">= α r t + (1 -α) Q APPENDIX B KEYLOCK MDP TRAINING DETAILS Tabular Q-learning</formula><p>Hyperparameters for Teacher algorithms were chosen using a grid search. Table <ref type="table" target="#tab_2">I</ref> shows scanned hyperparameter values for each algorithm, the chosen value is in bold. The average reward over ten training runs was used to compare different hyperparameter settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C DECIMAL NUMBER ADDITION TRAINING DETAILS</head><p>Our reimplementation of decimal addition is based on Keras <ref type="bibr" target="#b36">[37]</ref>. The encoder and decoder are both LSTMs with 128 units. In contrast Fig. <ref type="figure" target="#fig_0">11</ref>. Minecraft five-step curriculum results, the Y -axis shows mean episode reward per 10 000 timesteps for the current task. Left: training performance, note the manual curriculum task switches after 200 000, 600 000, 1 000 000, and 1 400 000 steps. For automatic and uniform curriculum, the training score has no clear interpretation. Right: evaluation training on the last task. When training only on the last task, the agent did not make any progress at all. When training on a uniform mix of the tasks, the progress was slow, and the performance on the last task was poor. Manual curriculum allowed the agent to learn the last task to an acceptable level. TSCL is comparable to the manual curriculum in performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX D MINECRAFT TRAINING DETAILS</head><p>The Minecraft task consisted of navigating through randomly generated mazes. The maze ends with a target block and the agent gets 1 000 points by touching it. Each move costs -0.1 and dying Fig. <ref type="figure" target="#fig_1">12</ref>. Network architecture used for Minecraft. Input to the network was 40 × 30 color image, outputs of the network were state value (used as a baseline), move action, and turn action. Actions produced mean for Gaussian sampling, the variance was learned. In preliminary experiments, we used additional outputs for use and jump actions. Those used Bernoulli sampling, where action value was the probability of action. Fig. <ref type="figure" target="#fig_2">13</ref>. Training scheme used for Minecraft. There were ten runner processes interacting with ten Minecraft environments. All runners used the same shared network weights to predict actions. The states, actions, rewards, and some auxiliary information were sent to the trainer process via FIFO queues. Trainer collects this data from queues and forms minibatch for training the policy network. After training the new policy network, weights are shared to all runners. in lava or getting a timeout yields -1 000 points. Timeout is 30 s (1 500 steps) in the first task and 45 s (2 250 steps) in the subsequent tasks.</p><p>For learning, we used the proximal policy optimization (PPO) algorithm <ref type="bibr" target="#b38">[39]</ref> implemented using Keras <ref type="bibr" target="#b36">[37]</ref> and optimized for real-time environments. The policy network used four convolutional layers and one LSTM layer. Input to the network was 40 × 30 color image, and outputs were two Gaussian actions: move forward/backward and turn left/right. In addition, the policy network had state value output, which was used as the baseline. Fig. <ref type="figure" target="#fig_1">12</ref> shows the network architecture.</p><p>For training, we used a setup with ten parallel Minecraft instances, as shown in Fig. <ref type="figure" target="#fig_2">13</ref>. The agent code was separated into runners, which interact with the environment, and a trainer that performs batch training on GPU, similar to <ref type="bibr" target="#b39">[40]</ref>. Runners regularly update their snapshot of the current policy weights, but they only perform prediction (forward pass), never training. After a fixed number of steps, they use FIFO buffers to send collected states, actions, and rewards to the trainer. Trainer collects those experiences from all runners, assembles them into batches, and performs training. FIFO buffers allow the runners to run asynchronously from trainer. This also means that the trainer is not completely on-policy, but this problem is handled by the importance of sampling in PPO.</p><p>During training, we also used frame skipping, i.e., processed only every fifth frame. This sped up the learning considerably, and surprisingly, the resulting policy also worked without frameskip. Also, we used the auxiliary loss for predicting the depth as suggested in <ref type="bibr" target="#b40">[41]</ref>. Surprisingly, this resulted only in minor improvements.</p><p>For automatic curriculum learning, we only implemented the window algorithm for the Minecraft task, because other algorithms rely on score change, which is not straightforward to calculate for parallel training scheme. The window size was defined in timesteps and fixed to 10 000 in the experiments, exploration rate was set to 0.1.</p><p>The idea of the first task in the curriculum was to make the agent associate the target with a reward. In practice, this task proved to be too simple-the agent could achieve almost the same reward by doing backward circles in the room. For this reason, we added a penalty for moving backwards to the policy loss function. This fixed the problem in most cases, but we occasionally still had to discard some unsuccessful runs. Results only reflect the successful runs.</p><p>We also had some preliminary success combining continuous (Gaussian) actions with binary (Bernoulli) actions for "jump" and "use" controls, as shown in Fig. <ref type="figure" target="#fig_1">12</ref>. This allowed the agent to learn to cope also with rooms that involve doors, switches, or jumping obstacles, see https://youtu.be/e1oKiPlAv74.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Teacher-student setup.</figDesc><graphic coords="1,312.47,113.81,250.10,53.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 2</head><label>2</label><figDesc>Naive Algorithm Initialize STUDENT learning algorithm Initialize expected return Q(a) = 0 for all N tasks for t = 1, . . . , T do Choose task a t based on |Q(a)| using -greedy or Boltzmann policy Reset D = ∅ for k = 1, . .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Three-step keylock MDP. One action from each state moves the agent forward toward the final state, and the other action moves it back to the beginning of the chain. The final state gives reward +1.</figDesc><graphic coords="3,321.47,352.37,231.38,69.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.4. Results of ten-step keylock MDP experiment. The Y -axis is the average reward after 1000 learning episodes, higher is better. This task could not be solved at all by training only from the first state. Our algorithms did significantly better than a uniform sampling of initial states. The results shown are based on ten runs.</figDesc><graphic coords="4,60.47,54.77,232.10,154.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Sampled tasks (initial states) from window Boltzmann algorithm. The algorithm starts with simpler later tasks (top left corner) and proceeds to more complex early tasks (bottom right corner).</figDesc><graphic coords="4,72.77,273.54,203.41,141.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Fig.8. Results of nine-digit 2-D addition experiment. The Y -axis shows the number of epochs to achieve 99% validation accuracy, lower is better. The task seems easier, the manual curriculum is hard to beat, and uniform sampling is competitive.</figDesc><graphic coords="5,46.07,55.49,255.86,170.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>in D(a) for each task a exploration coefficient was fixed to 0.1, the temperature τ was fixed to 0.0004, the learning rate α was 0.1, and the window size K was 10 in all experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="8,100.79,57.39,411.38,177.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="8,84.84,275.83,442.45,237.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 3</head><label>3</label><figDesc>Window Algorithm Initialize STUDENT learning algorithm Initialize FIFO buffers D(a) and E(a) with length K for all N tasks Initialize expected return Q(a) = 0 for all N tasks for t = 1, . . . , T do Choose task a t based on |Q(a)| using -greedy or Boltzmann policy Train STUDENT using task a t and observe score o t = x Store score o t in D(a t ) and timestep t in E(a t ) Use linear regression to predict D(a t ) from E(a t ) and use the coef. as r t Update expected return Q(a t ) = αr t + (1 -α)Q(a t )</figDesc><table><row><cell>(a t ) t</cell></row><row><cell>Algorithm 4 Sampling Algorithm</cell></row><row><cell>Initialize STUDENT learning algorithm</cell></row></table><note><p>Initialize FIFO buffers D(a) with length K for all N tasks for t = 1, . . . , T do Sample reward ra from D(a) for each task (if D(a) = ∅ then ra = 1) Choose task a t = arg max a |r a | Train STUDENT using task a t and observe reward r t = x (a t ) t x (a t ) t Store reward r t in D(a t )</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I SCANNED</head><label>I</label><figDesc>HYPERPARAMETER VALUES FOR DIFFERENT ALGORITHMS IN KEYLOCK MDP EXPERIMENT. THE USED VALUE IS SHOWN IN BOLD. BEST RESULT SHOWS AVERAGE REWARD AFTER 1000 LEARNING EPISODES, CALCULATED OVER TEN TRAINING SESSIONSto the original implementation, the hidden state is not passed from encoder to decoder; instead, the last output of the encoder is provided to all inputs of the decoder. One curriculum training step consists of training on 40 960 samples. Validation set consists of 4 096 samples, and 4 096 is also the batch size. Adam optimizer<ref type="bibr" target="#b37">[38]</ref> is used for training with the default learning rate of 0.001. Both input and output are padded to a fixed size.</figDesc><table><row><cell>Algorithm 7 Window Algorithm Initialize STUDENT learning algorithm Initialize FIFO buffers D(a) with length K for all N tasks Initialize expected return Q(a) = 0 for all N tasks for t=1,…,T do Create probability distribution a t = ( p (1) t , . . . , p (N) t ) based on | Q| using -greedy or Boltzmann policy Train STUDENT using probability distribution a t and observe scores o t = (x (1) (N) t ) Store score o (a) t in D(a) for all tasks a Apply linear regression to each D(a) and extract the coefficients as vector r t Update expected return Q = α r Algorithm 8 Sampling Algorithm Initialize STUDENT learning algorithm Initialize FIFO buffers D(a) with length K for all N tasks for t=1,…,T do Sample reward ra from D(a) for each task (if D(a) = ∅ then ra = 1) Create one-hot probability distribution ˜t a = ( p (1) t , . . . , p (N) t ) based on arg max a |r a | Mix in uniform dist. : a t = (1 -) ˜t a + /N Train STUDENT using probability distribution a t and observe scores o t = (x (1) t , . . . , x (N) t ) Calculate score changes r t = o t -o t -1 Store reward r</cell></row></table><note><p><p>t , . . . , x t + (1 -α) Q</p>In the experiments, we used the number of steps until 99% validation set accuracy is reached as a comparison metric. The</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>https://github.com/Microsoft/malmo/blob/master/ sample_missions/MalmoMissionTable_CurrentTasks_2016_ 06_14.pdf</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank Microsoft for their excellent Malmö environment for Minecraft, J. Tobin and P. Abbeel for suggestions and comments, V. Cheung, J. Schneider, B. Mann, and A. Chaidarun for always being helpful with OpenAI infrastructure. They would also like to thank R. Vicente, A. Tampuu and I. Kuzovkin from the University of Tartu for their comments and discussion. The results from this brief were presented as a poster at Deep Reinforcement Learning Symposium, NIPS 2017, Long Beach, CA, USA. T. Matiisen, A. Oliver, and T. Cohen did the work as part of their internship in OpenAI.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Trust region policy optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Continuous control with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02971</idno>
		<ptr target="https://arxiv.org/abs/1509.02971" />
		<imprint>
			<date type="published" when="2015-09">Sep. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">End-to-end training of deep visuomotor policies</title>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00702</idno>
		<ptr target="https://arxiv.org/abs/1504.00702" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient exploration in reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<ptr target="http://hunch.net/images/Efficient_Reinforcement_Learning.pdf" />
	</analytic>
	<monogr>
		<title level="m">Encyclopedia Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011-08-27">2011. Aug. 27, 2019</date>
			<biblScope unit="page" from="309" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th Annu</title>
		<meeting>26th Annu</meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning to execute</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.4615</idno>
		<ptr target="https://arxiv.org/abs/1410.4615" />
		<imprint>
			<date type="published" when="2014-10">Oct. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hybrid computing using a neural network with dynamic external memory</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="issue">7626</biblScope>
			<biblScope unit="page" from="471" to="476" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Training agent for first-person shooter game with actor-critic curriculum learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Submitted Conf. Learn. Represent</title>
		<meeting>Submitted Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">What is intrinsic motivation? A typology of computational approaches</title>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Oudeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kaplan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers Neurorobot</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2009-11">Nov. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Planning and acting in partially observable stochastic domains</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Cassandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="134" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Markovian decision process</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Mech</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="684" />
			<date type="published" when="1957">1957</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1054</biblScope>
			<date type="published" when="1998-09">Sep. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Q-learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J C H</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="279" to="292" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The malmo platform for artificial intelligence experimentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bignell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf</title>
		<meeting>Int. Joint Conf</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4246" to="4247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Curious model-building control systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Joint Conf. Neural Netw</title>
		<meeting>IEEE Int. Joint Conf. Neural Netw</meeting>
		<imprint>
			<date type="published" when="1991-11">Nov. 1991</date>
			<biblScope unit="page" from="1458" to="1463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Intrinsic motivation systems for autonomous mental development</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Oudeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">V</forename><surname>Hafner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Evol. Comput</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="265" to="286" />
			<date type="published" when="2007-04">Apr. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Active learning of inverse models with intrinsically motivated goal exploration in robots</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baranes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Oudeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robot. Auton. Syst</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="73" />
			<date type="published" when="2013-01">Jan. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-armed bandits for intelligent tutoring systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Clement</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Oudeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lopes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Educ. Data Mining</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="20" to="48" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Formal theory of creativity, fun, and intrinsic motivation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Auton. Mental Develop</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="230" to="247" />
			<date type="published" when="1990">1990-2010. Sep. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Intrinsic motivation and automatic curricula via asymmetric self-play</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05407</idno>
		<ptr target="https://arxiv.org/abs/1703.05407" />
		<imprint>
			<date type="published" when="2017-03">Mar. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Automatic goal generation for reinforcement learning agents</title>
		<author>
			<persName><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Florensa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06366</idno>
		<ptr target="https://arxiv.org/abs/1705.06366" />
		<imprint>
			<date type="published" when="2017-05">May 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Automated curriculum learning for neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03003</idno>
		<ptr target="https://arxiv.org/abs/1704.03003" />
		<imprint>
			<date type="published" when="2017-04">Apr. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Grid long shortterm memory</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.01526</idno>
		<ptr target="https://arxiv.org/abs/1507.01526" />
		<imprint>
			<date type="published" when="2015-07">Jul. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Neural programmer-interpreters</title>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06279</idno>
		<ptr target="https://arxiv.org/abs/1511.06279" />
		<imprint>
			<date type="published" when="2015-11">Nov. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Neural GPUs learn algorithms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08228</idno>
		<ptr target="https://arxiv.org/abs/1511.08228" />
		<imprint>
			<date type="published" when="2015-11">Nov. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Control of memory, active perception, and action in minecraft</title>
		<author>
			<persName><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chockalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09128</idno>
		<ptr target="https://arxiv.org/abs/1605.09128" />
		<imprint>
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A deep hierarchical approach to lifelong learning in minecraft</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Givony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zahavy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Mankowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07255</idno>
		<ptr target="https://arxiv.org/abs/1604.07255" />
		<imprint>
			<date type="published" when="2016-11">Nov. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unifying count-based exploration and intrinsic motivation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="1471" to="1479" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">VIME: Variational information maximizing exploration</title>
		<author>
			<persName><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1109" to="1117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Incentivizing exploration in reinforcement learning with deep predictive models</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Stadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.00814</idno>
		<ptr target="https://arxiv.org/abs/1507.00814" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Label propagation via teaching-to-learn and learning-to-teach</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1452" to="1465" />
			<date type="published" when="2017-06">Jun. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Progressive stochastic learning for noisy labels</title>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Celina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5136" to="5148" />
			<date type="published" when="2018-10">Oct. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multimodal curriculum learning for semi-supervised image classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3249" to="3260" />
			<date type="published" when="2016-07">Jul. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ensemble teaching for hybrid label propagation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="388" to="402" />
			<date type="published" when="2019-02">Feb. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Keras</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<ptr target="https://arxiv.org/abs/1412.6980" />
		<imprint>
			<date type="published" when="2014-12">Dec. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Proximal policy optimization algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<ptr target="https://arxiv.org/abs/1707.06347" />
		<imprint>
			<date type="published" when="2017-08">Aug. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Reinforcement learning through asynchronous advantage actor-critic on a GPU</title>
		<author>
			<persName><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Frosio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clemons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06256</idno>
		<ptr target="https://arxiv.org/abs/1611.06256" />
		<imprint>
			<date type="published" when="2016-11">Nov. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning to navigate in complex environments</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mirowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03673</idno>
		<ptr target="https://arxiv.org/abs/1611.03673" />
		<imprint>
			<date type="published" when="2016-11">Nov. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
