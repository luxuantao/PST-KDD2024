<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ultrasound Standard Plane Detection Using a Composite Neural Network Framework</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hao</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">L</forename><surname>Wu</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">J.-Z</forename><surname>Cheng</surname></persName>
							<email>jzcheng@szu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">D</forename><surname>Ni</surname></persName>
						</author>
						<author>
							<persName><forename type="first">S</forename><surname>Li</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Dong Ni</orgName>
								<address>
									<addrLine>Jie-Zhi Cheng</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<postCode>518060</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<postCode>518060</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">School of Nursing</orgName>
								<orgName type="institution">Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department" key="dep1">Department of Ultrasound</orgName>
								<orgName type="department" key="dep2">Affiliated Shenzhen Maternal</orgName>
								<orgName type="institution" key="instit1">Child Healthcare Hospital</orgName>
								<orgName type="institution" key="instit2">Nanfang Medical University</orgName>
								<address>
									<postCode>518000</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department" key="dep1">Department of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">Guangdong Provincial Key Laboratory of Computer Vision and Virtual Reality Technology</orgName>
								<orgName type="institution">Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>518052</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Ultrasound Standard Plane Detection Using a Composite Neural Network Framework</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FEB7159E787CB29743FC9E33A81C4EA9</idno>
					<idno type="DOI">10.1109/TCYB.2017.2685080</idno>
					<note type="submission">received December 6, 2016; revised March 12, 2017; accepted March 16, 2017.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Convolutional neural network (CNN)</term>
					<term>deep learning</term>
					<term>knowledge transfer</term>
					<term>recurrent neural network (RNN)</term>
					<term>standard plane</term>
					<term>ultrasound (US)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Ultrasound (US) imaging is a widely used screening tool for obstetric examination and diagnosis. Accurate acquisition of fetal standard planes with key anatomical structures is very crucial for substantial biometric measurement and diagnosis. However, the standard plane acquisition is a labor-intensive task and requires operator equipped with a thorough knowledge of fetal anatomy. Therefore, automatic approaches are highly demanded in clinical practice to alleviate the workload and boost the examination efficiency. The automatic detection of standard planes from US videos remains a challenging problem due to the high intraclass and low interclass variations of standard planes, and the relatively low image quality. Unlike previous studies which were specifically designed for individual anatomical standard planes, respectively, we present a general framework for the automatic identification of different standard planes from US videos. Distinct from conventional way that devises handcrafted visual features for detection, our framework explores in-and between-plane feature learning with a novel composite framework of the convolutional and recurrent neural networks. To further address the issue of limited training data, a multitask learning framework is implemented to exploit common knowledge across detection tasks of distinctive standard planes for the augmentation of feature learning. Extensive experiments have been conducted on hundreds of US fetus videos to corroborate the better efficacy of the proposed framework on the difficult standard plane detection problem.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>U LTRASOUND (US) is a widely used obstetric exam- ination tool for its advantages of low cost, mobility, and the capability of real time imaging <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. In general, the clinical obstetric US examination involves the procedures of manual scanning, standard plane selection, biometric measurement, and diagnosis <ref type="bibr" target="#b2">[3]</ref>. Particularly, the accurate acquisition and selection of the US planes that can clearly depict the key anatomic structures of fetus is very crucial for the subsequent biometric measurement and diagnosis. For example, the prebirth weight of baby can be estimated from the US measurements of head circumference, biparietal diameter, abdominal circumference, and femur length. Therefore, the selection of US planes that can depict the corresponding organs with good quality will be very important for the accurate estimation of fetus weight <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. In terms of diagnostic purpose, the US views that can visualize the detailed facial and cardiac structures of fetus deem to be very important for the timely prenatal diagnosis of facial dysmorphism and congenital heart diseases. These US planes that can depict key anatomic structures clearly for either biometric measurement or disease diagnosis are generally recommended by professional organizations for the standard fetal US examination and are often denoted as US standard planes <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b8">[9]</ref>.</p><p>In clinical practice, the US standard plane is commonly acquired by hand with laborious maneuver of the probe for searching the desirable view that can concurrently present the key anatomical structures, see Fig. <ref type="figure" target="#fig_0">1</ref>. Specifically, three standard planes: 1) fetal abdominal standard plane (FASP); 2) fetal face axial standard plane (FFASP); and 3) fetal four-chamber view standard plane (FFVSP) of heart are shown in Fig. <ref type="figure" target="#fig_0">1</ref>. The FFASP is determined with the presence of three key organs of: 1) nose bone; 2) lens; and 3) eyes in the US view, whereas the FASP is expected to include stomach bubble (SB), umbilical vein (UV), and spine (SP). The definition of FFVSP is the US plane that can clearly visualize five key cardiac structures of: 1) left atrium; 2) right atrium; 3) left ventricle; 4) right ventricle; and 5) descending aorta in the same image. The FASP can be used for the estimation of fetal weight, while the FFASP and FFVSP can be informative for the diagnosis of facial dysmorphism and congenital heart diseases, respectively. Since the clinically needed US standard planes can be very diverse and the overall number of planes can be several dozens for a thorough examination <ref type="bibr" target="#b9">[10]</ref>, it usually takes around tens of minutes or more to acquire and define the US standard planes, even for a very experienced obstetrician. Therefore, the selection of necessary US standard planes can be one of the most time consuming procedure in the obstetric examination. On the other aspect, the process of acquisition and selection of the correct US standard planes requires the operator being proficient in maternal and fetal anatomy and highly depends on operator's experience. As a consequence, it would be very challenging for an inexperienced operator or novice to fulfill the whole task of US standard plane acquisition. Meanwhile, since the standard plane acquisition is a knowledge-intensive task and required planes are very diverse, the learning curve of this procedure can be very long <ref type="bibr" target="#b10">[11]</ref>. In such a case, the manpower shortage can be expected in highly populated regions as the training of a ready operator for the US fetal examination can be costly and take a long time. Motivated by the aforementioned issues, the computerized scheme with automatic plane detection and selection capability will be highly welcome to alleviate the routinely obstetric workload <ref type="bibr" target="#b11">[12]</ref> and address the issues of medical manpower shortage on underserved populations and areas <ref type="bibr" target="#b10">[11]</ref>. The computer-aided scheme can also help to facilitate the training of medical novices with computerized feedback from a score-based quality control system <ref type="bibr" target="#b12">[13]</ref>.</p><p>The topic of computer-aided US frame detection and selection is relatively new and has recently received more and more attention in these years <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b17">[18]</ref>. The computerized scheme can help to lower down the operator dependency in US scanning and improve the efficiency of post-processing procedures with automatic mechanisms. Kwitt et al. <ref type="bibr" target="#b10">[11]</ref> developed a template-based method equipped with dynamic texture model to retrieve frames containing key structures from US video. The efficacy of the template-based method was merely verified in phantom studies, and hence, the applicability to the real data may need to be further explored. In the obstetric application, quite a few computerized methods had also been proposed to identify specific standard planes from freehand US videos. Zhang et al. <ref type="bibr" target="#b7">[8]</ref> adopted the cascade AdaBoost to locate the plane with gestational sac. To automatically select the FASP from the US video, Ni et al. <ref type="bibr" target="#b11">[12]</ref> used the radial component descriptor to encode the spatial co-presence relation of the SB, UV, and SP to retrieve the target plane. Generally speaking, most previous methods have to find out useful features and exploit the mathematical and spatial priors for the detection of each specific US plane. In such a case, the detection method designed for one standard plane, e.g., FASP, may not be easily generalized to another standard plane, say FFASP.</p><p>By and large, the challenges of developing the detection algorithm for US standard planes can be summarized in fourfold. First, the US standard plane often has high intraclass appearance variation caused by various factors like imaging artifacts of acoustic shadows and speckles, deformation of soft tissues, fetal development, transducer poses <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref>, etc. Second, the key anatomical structures in the standard plane may possibly appear similar to other structures. For instance, shadows, the abdominal aorta and the inferior vena cava are often mistakenly identified as the SB or UV in the FASP of Fig. <ref type="figure" target="#fig_0">1</ref>, as the shape and echogenicity of these structures resemble to each other. Accordingly, even for experienced obstetricians, the plane selection results can be possibly misled by the low interclass variation. The third challenge lies in that the available US fetus training image data and expert annotations are significantly more limited and less accessible than the image data for many computer vision problems. To obtain the US fetus data, it has to get the local institutional review board (IRB) approval and consent from subjects. Meanwhile, the annotation on the US standard planes from long US fetus videos requires professional obstetric knowledge and is a very time consuming task. With limited training data and annotation, the capability of any US standard plane method based on machine learning will be constrained. The potential over-fitting issue may also be difficult to avoid. The fourth challenge consists in that the US fetus standard planes can be very diverse for their own diagnostic purposes, see Fig. <ref type="figure" target="#fig_0">1</ref>. In such a case, it will be very hard to devise a general method that can retrieve multiple standard planes from US fetus videos. These four challenges will impose great difficulty on any off-the-shelf pattern recognition techniques, e.g., the template-based <ref type="bibr" target="#b10">[11]</ref>, geometrical shape-based <ref type="bibr" target="#b11">[12]</ref>, and feature-based methods <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b18">[19]</ref>, and hence, the algorithm for each standard plane may need to be specifically designed.</p><p>The deep learning techniques have made breakthroughs in the field of computer vision <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b24">[25]</ref> and medical image computing <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b31">[32]</ref>. Instead of elaboration on hand-crafted features on each respective problem in the conventional pattern recognition pipeline, the deep learning techniques are able to automatically discover important features and exploit the feature relation from training data <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>. However, the deep learning techniques may demand a large number of training data, which is usually not feasible in medical image analysis problems, to construct an effective model. To address the issue of training data size, the transfer learning scheme has recently been introduced into the deep learning techniques, particularly with the deep convolutional neural networks (CNNs), to leverage the knowledge across different domains <ref type="bibr" target="#b34">[35]</ref>- <ref type="bibr" target="#b36">[37]</ref>. Specifically, in the application of US fetus standard plane detection, Chen et al. <ref type="bibr" target="#b37">[38]</ref> exploited to transfer the knowledge from the natural scene images toward the domain of fetus for the identification of FASP with the CNN model. The experimental results suggested that the low level image cues like corner, edges, etc., learned from the natural scene domain can serve as good network initialization for CNN to effectively boost FASP detection performance than the random initialization setting. Although relatively satisfactory performance had been achieved with knowledge-transferred CNN scheme in <ref type="bibr" target="#b37">[38]</ref>, the gap between the natural scene domain and the fetus US domain remains significant. Accordingly, the performance improvement may be thus limited. Meanwhile, the study of <ref type="bibr" target="#b37">[38]</ref> only considered the image cues within single plane, which may not be sufficient to address the high intraclass and low interclass variation issues. As an extension of our previous work <ref type="bibr" target="#b38">[39]</ref>, in this paper, we will explore the interframe contextual clues, which are very informative for human experts during the manual screening, for the US standard plane detection problem.</p><p>To address the four challenges discussed above, this paper attempts to leverage the framework of multitask learning, deep learning technique, and the sequence learning model (RNN) to detect three standard planes, i.e., FASP, FFASP, and FFVSP, from US fetus videos. Specifically, we treat the detection of the three standard planes as three individual tasks and jointly learn the spatial features with the deep CNN. The shared spatial features across the three tasks extracted from individual frame are further transferred to the RNN for the modeling of temporal relation. The multitask learning framework aims to uncover the common knowledge shared across different tasks. With such a framework, the training data on each individual task can be helpful to other tasks, and hence, the demand on large data size for all tasks can be potentially eased. The training of the deep CNN is based on the multitask learning framework to identify the useful common in-plane spatial features at the supervised learning phase. With the consideration of learning the three detection tasks in the same architecture, the generalization capability of the constructed deep CNN can be thus augmented and the issues of low interclass and high interclass variations can also be handled properly. The RNN <ref type="bibr" target="#b39">[40]</ref>- <ref type="bibr" target="#b41">[42]</ref> had been widely applied to address many machine learning problems for various sequential data, e.g., speech recognition <ref type="bibr" target="#b42">[43]</ref>, video recognition <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, and machine translation <ref type="bibr" target="#b45">[46]</ref>, with promising results. In this paper, we specifically exploit the long short-term memory (LSTM) model to harness the interframe contexts. The LSTM model has a good capability to solve issues of exploding or vanishing gradients that could be possibly caused by temporal signal drops, serious noise corruption, and occlusion <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>. The training of the LSTM model is based on the extracted features from the multitask deep CNN. Since the contextual cues are also commonly used by medical experts in the clinical US scanning and plane selection, the modeling of interframe contexts may be helpful to tackle the issue of low interclass variation for better detection performance.</p><p>The performance of the proposed multitask deep and temporal learning framework will be evaluated with extensive experiments by comparing our performance with other stateof-the-art methods in the literature. The outperformance of the proposed method over other baseline methods corroborates the efficacy of multitask learning and the exploit of temporal features on this new US standard plane detection problem. Since the proposed method does not explicitly elaborate on the feature design, it is also easy to apply our multitask deep and temporal learning framework for the detection of other standard US planes.</p><p>The remainder of this paper is organized as follows. Section II describes the proposed method in details. Experimental results are evaluated qualitatively and quantitatively in Section III. Section IV discusses the advantages and disadvantages of our proposed method, as well as future research directions. Finally, the conclusions are drawn in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHOD</head><p>The left part of Fig. <ref type="figure" target="#fig_1">2</ref> illustrates the overview of the proposed model, which is a composite neural network framework with the specialized deep CNN and RNN to exploit the inand between-plane features from fetus US videos. The composite neural network is denoted as T-RNN throughout this paper for short. The deep CNN model of the T-RNN framework aims to uncover useful spatial features from individual US planes. To address the issue of limited training data, the multitask learning is implemented for the training of the deep CNN models by treating the detection of FASP, FFVSP, and FFASP as three individual tasks. The goal of multitask learning is to leverage the limited training data of each detection task for better model generalization and avoidance of potential over-fitting problem. Comparing to the large domain gap between the natural scene images and US fetus images <ref type="bibr" target="#b37">[38]</ref>, the training data of the three detection tasks are of the same image modality and relatively relevant. Therefore, the common knowledge shared by the three detection tasks may be more easily explored by the CNN model, and would be served as a more reliable basis for the task-oriented fine-tuning. Based on the in-plane knowledge of the CNN models learned with the multitask learning, the between-plane relation is further exploited with the specific RNN of LSTM model. The complex contextual knowledge discovered by the LSTM model will help to deal with the issues of low interclass variation for the boosting of detection capability.</p><p>The whole T-RNN framework is realized in three major steps. First, a regions of interest (ROI) classifier is jointly trained with CNN models, named as J-CNN, across three detection tasks of FASP, FFVSP, and FFASP. The ROI classifier of J-CNN models is expected to locate the informative regions in each US plane of the three detection tasks. The features extracted from the identified ROI at each frame by the J-CNN model are further forwarded to the LSTM model that is imparted with the between-plane knowledge to yield </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE I ARCHITECTURE OF J-CNN MODEL</head><p>the corresponding task prediction scores on each US frame. Finally, the score of each frame is further inferred by averaging all prediction scores from the LSTM model. A US plane will be identified as the standard plane when the inferred score is larger than a defined threshold T 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Joint Learning Across Multitasks</head><p>The basic structure of CNN is composed of several pairs of alternating convolutional (C) and max-pooling (M) layers, followed by fully connected (F) layers <ref type="bibr" target="#b48">[49]</ref>. Previous studies have suggested that the knowledge learned from one task domain via CNN can benefit the training of another task domain where annotated data are limited <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>. Therefore, the CNN model can be very suitable for multitask learning. Specifically, a joint learning scheme with CNN across multiple detection tasks of US standard planes is carried out, as illustrated in the right part of Fig. <ref type="figure" target="#fig_1">2</ref>. The matrix W s represents the synaptic parameters of layers from C1 to M5 and can be adjusted in training process of the CNN model.</p><p>Via the co-training process from the annotated data of the FASP, FFVSP, and FFASP, the common knowledge across the three distinctive tasks can be further encoded in the matrix W s . The W m (m = 1, 2, and 3 represents the task of FFASP, FFVSP, and FASP, respectively) stands for the synaptic parameters of F6 and F7 layers to learn the task-specific knowledge at the supervised training of the CNN models. The whole learning problem is then formulated as a cost minimization process of the joint max-margin loss function L 1</p><formula xml:id="formula_0">L 1 = λ 2 m W m 2 2 + W s 2 2 + m k max 0, 1 -y mk F m f s mk ; W m 2 (1)</formula><formula xml:id="formula_1">f s mk = F s (I mk ; W s ) (2)</formula><p>where the first component of L 1 is the regularization penalty term and the second component is the data loss term. The cost minimization can be realized by adjusting the synaptic matrices of W s and W m . The importance weighting between the two terms in (1) is controlled by the hyper-parameter λ, which is empirically defined as 1.0 throughout this paper. In (2), the function F s indicates the common feature function specified by W s across the three tasks, whereas the function F m is the task-specific discriminant function controlled by matrix W m . The I mk in (2) stands for the kth image plane with respect to the mth task, and the f s mk is the output of the function F s , i.e., the neuron activations of M5 layer. The y mk ∈ {-1, 1} specifies the corresponding ground truth label for the input frame I mk . The detailed architecture configuration of the J-CNN models in this paper can be found in Table <ref type="table">I</ref>, where padding and nonlinear activation layers are not shown for simplified presentation. Meanwhile, the rectified linear units are implemented in the nonlinear activation layer <ref type="bibr" target="#b51">[52]</ref> and the dropout strategy is employed in the fully connected layers for better generalization capability <ref type="bibr" target="#b52">[53]</ref>. The learning rate is set as 0.01 initially and gradually decreased by factor of 10, whenever the training loss stops to decrease. The constructed J-CNN models can help to manifest the informative ROIs with respect to each task and the corresponding extracted features will be fed into the latter LSTM model for further processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. US Standard Plane Detection via T-RNN</head><p>During the clinical US fetal examination, the contextual cues between two consecutive scanning frames are intuitively used by the operator for the searching of anatomical targets, as the in-plane visual cues sometimes may not be sufficient to support the clinical judgement. Motivated by this, the special RNN model, i.e., the LSTM <ref type="bibr" target="#b46">[47]</ref>, is adopted here to exploit the between-plane cues from the recorded US fetus videos. The training of the LSTM model is based on the manifested inplane ROIs from the J-CNN model. Because the J-CNN model can filter out most irrelevant image cues to the three detection tasks, the LSTM model can further focus on the polished taskrelated ROI for more efficient and effective establishment of contextual relations between US planes.</p><p>Given the input frame I mk , the probability map of the ROI is computed by the J-CNN model with the sliding window technique. Specifically, for robustness of computation, each subimage by the sliding window from the original image is augmented into ten input samples by cropping the patches of its center and four corners, as well as the corresponding mirrored five patches <ref type="bibr" target="#b37">[38]</ref>. The final score of each sliding window subimage can then be defined with the averaged J-CNN score over its 10 varied replications. With the robust sliding window scheme, the center of the final ROI identified by J-CNN can be regarded as the location with maximal value in the computed probability score map. Following that, the features from the penultimate layer (i.e., the activations of F6 layer) of the J-CNN model are extracted from the estimated ROI of each frame as the inputs of the LSTM model. A preprocessing of the US videos is implemented to facilitate the training of LSTM model. Specifically, the long US videos are clipped into shorter montages of fixed T frames. Accordingly, the input video can be thus treated as consecutive samples of montages. Each montage is denoted by a sequential feature vector: x = {x 1 , . . . , x t , . . . , x T } and x t ∈ R q (q = 100 in our experiments) with the corresponding label vector of y = {y 1 , . . . , y t , . . . , y T }, where y t ∈ {0, 1}. It is worth noting that the consecutive clipped montages share overlapping US frames for the robustness of computation.</p><p>In the traditional RNN, the back-propagation algorithm is commonly adopted for the training. However, the backpropagation algorithm may fall short of dealing with the vanishing or exploding gradients <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, and thus could be sensitive to noisy or corruption in the data sequence. The LSTM model on the other hand is able to tackle this problem by incorporating the so-called memory cells into the network architecture. The memory cell equips the network with better abilities to find and exploit long range context with the arrival of sequential inputs <ref type="bibr" target="#b42">[43]</ref>, hence, it endows the LSTM model the capability and flexibility on handling the intermittent noise, data corruption and error. With these advantages, the LSTM model will be quite suitable for the processing of US fetus videos, where the image quality of some frames can possibly be very bad and not informative.</p><p>A basic architecture of LSTM model can be constituted with units of input gate, memory cell wired with self-recurrent connection, forget gate and output gate, see Fig. <ref type="figure" target="#fig_2">3</ref> for illustration. Specifically, the element-wise nonlinear functions shown in Fig. <ref type="figure" target="#fig_2">3</ref> can be either the sigmoid function in the form of σ (x) = [1/(1 + e -x )] or the hyperbolic tangent function, φ(x) = [(e xe -x )/(e x + e -x )], that can squash the range of input x into the respective range of [0, 1] and [-1, 1]. The gates serve to modulate the interactions between the memory cell c t and its environment <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>. The input gate i t can control incoming input x t whether to alter the state of the memory cell or block it instead. The output gate o t is in charge of the memory cell state to have an effect on hidden neurons or not. The forget gate f t can modulate the self-recurrent connection of the memory cell to steer the memory cell whether to remember or forget the previous state c t-1 . All the gates and memory cell have the same vector size with hidden state h t ∈ R H (H is the number of hidden units). The update mechanisms of the gates and memory cells can be realized with the following equations:</p><formula xml:id="formula_2">i t = σ (W xi x t + W hi h t-1 + b i ) f t = σ W xf x t + W hf h t-1 + b f o t = σ (W xo x t + W ho h t-1 + b o ) c t = f t c t-1 + i t φ(W xc x t + W hc h t-1 + b c ) h t = o t φ(c t ) (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>where h 0 = 0, and all W denote the weighting matrices. For examples, W xi is the input-input gate matrix, whereas W hi is the matrix of hidden-input gate. In the (3), all b stand for the bias terms with respect to each unit, and the operator represents the element-wise multiplication. The final predictions can be obtained by feeding h t into a softmax classification layer over the three tasks. Thus, the parameters θ (including all W and b) of the LSTM model can be trained by minimizing the negative logarithm loss function L 2 with stochastic gradient descent method <ref type="bibr" target="#b55">[56]</ref>. The L 2 is defined as where N is the total number of the clipped montages, and p n (y t |x t , h t-1 ; θ) is the correctly predicted probability function for tth frame of one training montage, given the current input x t and previous hidden state h t-1 .</p><formula xml:id="formula_4">L 2 = - N n=1 T t=1 log p n (y t |x t , h t-1 ; θ ) (4) IEEE TRANSACTIONS ON CYBERNETICS</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Materials</head><p>All the US images and videos involved in this paper were acquired from the Shenzhen Maternal and Child Healthcare Hospital during September 2011 to February 2013. The study protocol was reviewed and approved by the ethics committee of the same institution. Meanwhile, all participating subjects agreed the data usage for scientific research and relevant algorithm development. The US videos were recorded with conventional hand-held 2-D US probe on pregnant women in the supine position, by following the standard obstetric examination protocol. All US videos were acquired with a Siemens Acuson Sequoia 512 US scanner, and the fetal gestational age of all subjects ranges from 18 to 40 weeks. Each video was obtained from one subject with 17-48 US frames for the purpose of searching one US standard plane. More specifically, one US video can be recorded from the region of either fetal face, abdomen, or chest to enclose the respective FFASP, FASP, or FFVSP. The ground truths of videos were manually annotated by an experienced obstetrician with more than five years of clinical experience.</p><p>For the training of the ROI classifier with J-CNN, the training samples with respect to FASP, FFASP and FFVSP were drawn from respective 300 US videos. Therefore, there are totally 900 US videos in which each of them exclusively contains one type of the three standard planes. For the performance evaluation, the tasks of FASP and FFASP are tested with 219 videos and 52 videos, respectively, whereas the testing data for the FFVSP task are 60 videos. The overall involved testing US images for the FASP and FFASP are 8718 and 2278, respectively, and the number of US images for the FFVSP is 2252. All the training and testing data were collected by following the rigorous scanning protocol for quality assurance. Details of the used US dataset in this paper can be found in Table <ref type="table" target="#tab_0">II</ref>. In summary, there are a total of 1231 US videos for the training and testing of the proposed T-RNN, whereas the overall number of involved images is 50 624. To the best of our knowledge, this is the largest real clinical dataset available for US standard plane detection study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Visualization of Intermediate Results</head><p>To give insight on the interaction between the models of J-CNN and LSTM during the processing of US fetus videos, Fig. <ref type="figure" target="#fig_3">4</ref> demonstrates the feature maps of J-CNN for the video frames and the task prediction result on each frame by the LSTM. The left column of Fig. <ref type="figure" target="#fig_3">4</ref> shows the final detection results of three US standard planes by the proposed method T-RNN. It can be observed that all identified standard planes by our algorithm can clearly depict the corresponding key anatomic structures. The predicted scores by the LSTM model of the three identified planes in the left column of Fig. <ref type="figure" target="#fig_3">4</ref> are above the threshold T 0 (determined by testing on a set of samples from the training set in our experiments). To further provide the visual assessment of the detection efficacy of key anatomical structures by the J-CNN, the middle column of Fig. <ref type="figure" target="#fig_3">4</ref> lists the C1 feature maps <ref type="bibr" target="#b56">[57]</ref> of the US frame by our J-CNN model. Specifically, it can be found that the regions with large responses of C1 layer in the feature maps mostly match with the key anatomical structures, and thus corroborate the effectiveness of the J-CNN model. The right column of Fig. <ref type="figure" target="#fig_3">4</ref> demonstrates the sequential prediction results by the LSTM model over the video frames. In the detection of all FASP, FFASP, and FFVSP in Fig. <ref type="figure" target="#fig_3">4</ref>, the prediction curves share a good consistency with the corresponding ground truths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison of Quantitative Performance</head><p>To quantitatively illustrate the efficacy of the proposed T-RNN framework, two most relevant approaches <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b37">[38]</ref> are considered here for performance comparison. The first baseline method is a feature-based approach that exploited the geometric relation and the dedicated features for the task of FASP detection. Specifically, a radial component model and vessel probability map was developed in <ref type="bibr" target="#b11">[12]</ref>, denoted as RVD, to model the anatomical prior and the geometrical relationship of structures for the plane identification. The second baseline method proposed in <ref type="bibr" target="#b37">[38]</ref> is the most related work to this paper. The work of <ref type="bibr" target="#b37">[38]</ref> attempted to leverage the transferred knowledge from natural image domains on the detection of FASP in 2-D US videos and this method is called as T-CNN for short, whereas the neural network trained with random initialization is denoted as R-CNN. To further illustrate the effectiveness of the LSTM model on the three detection tasks, we also report the detection performance that is attained solely with J-CNN. The performance report of standard plane detection with only J-CNN model can also help to elucidate that the effect of knowledgetransferring by the multitask framework can mostly yield better boosting of performance than the knowledge learned from natural images. In this paper, we employ four assessment metrics <ref type="bibr" target="#b57">[58]</ref> including recall: R = N tp /(N tp + N fn ), precision: P = N tp /(N tp + N fp ), F 1 score: F 1 = 2RP/(R + P), and accuracy: A = (N tp + N tn )/(N tp + N tn + N fp + N fn ), where N tp , N tn , N fp , and N fn represents the number of true positives, true negatives, false positives, and false negatives, respectively.</p><p>Two comparison schemes are implemented with the basic units of US images and videos. The image-based comparison scheme aims to illustrate the capability of different methods on the differentiation of standard and nonstandard planes over all participating testing images. The second video-based comparison scheme is to see whether the detection algorithms can effectively retrieve the standard plane from an acquired US video. Since the clinical demand for the subsequent biometric measurements and disease diagnosis is to identify the specific standard plane from the scanned US video, the videobased comparison scheme may help to illustrate the clinical applicability of each detection algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Image-Based Evaluation:</head><p>The image-based comparison results with the four assessment metrics over all comparing methods are shown in Table <ref type="table" target="#tab_1">III</ref>. Specifically, the deep learningbased methods of T-RNN, J-CNN, T-CNN, and R-CNN achieve better detection results than the method <ref type="bibr" target="#b11">[12]</ref> does on the FASP detection. This may suggest that the engineering of task-specific features may sometimes turn out to be not as useful as the features automatically underlaid by deep learning models. Meanwhile, it can also be observed from Table <ref type="table" target="#tab_1">III</ref> that J-CNN and T-CNN <ref type="bibr" target="#b37">[38]</ref> outperforms the R-CNN <ref type="bibr" target="#b37">[38]</ref> in most assessment metrics. Accordingly, the efficacy of knowledgetransferring on the issues of over-fitting and limited data can be properly substantiated. Furthermore, in most assessment metrics, the J-CNN attains better performance than T-CNN does. This may suggest that the knowledge shared by the three tasks can provide more effective model initialization and learning, as the image domain is the same and the data of the three tasks are relatively relevant (though still quite different). The improvement by the knowledge derived from natural images from ImageNet <ref type="bibr" target="#b58">[59]</ref> is relatively limited, probably because the underlying domain gap may be too large to boost the detection performance significantly.</p><p>Compared with other methods, our T-RNN method achieves the best performance for the detection of three standard planes. Particularly, for the FASP detection task, a significant outperformance can be observed in Table <ref type="table" target="#tab_1">III</ref>, and hence, further suggests the effectiveness of our composite neural network framework with the exploration of in-and between-plane cues from US videos. To give more quantitative comparison, the precision-recall plane and receiver operating characteristic curves of all methods on FASP detection task are shown in Fig. <ref type="figure" target="#fig_4">5</ref>. The scores of area under the curve obtained by the method of T-RNN, J-CNN, T-CNN, R-CNN, and RVD were 0.95, 0.94, 0.93, 0.90, and 0.80, respectively, further support the outperformance of the proposed T-RNN method.</p><p>2) Video-Based Evaluation: To quantitatively assess the capability of standard plane detection from US video by the comparing methods, we follow the same evaluation protocols in <ref type="bibr" target="#b11">[12]</ref> for the definition the true positives and true negatives. Specifically, each video is regarded as one testing sample. A true positive identification is defined as the case that a correct standard plane can be successfully detected from a video which encloses at least one standard plane. The true negative case will be confirmed when no standard plane is detected from a video that contains no standard planes. For the methods of T-RNN, J-CNN, T-CNN, and R-CNN, a US video is regarded to have a standard plane if the highest computed score of all member frames is larger than the defined threshold.</p><p>The quantitative results of the video-based scheme with respect to the four assessment metrics are reported in Table <ref type="table" target="#tab_2">IV</ref>. It can be found that the proposed composite neural network model of T-RNN outperforms other methods in most assessment metrics for the three detection tasks. Specifically, the attained F1 scores are 0.969, 0.832, and 0.917 for the detection of FASP, FFASP, and FFVSP, respectively, whereas the corresponding recall values are all larger than 0.9. Therefore, it can be suggested that most of standard planes can be effectively identified by the T-RNN method from the US videos. Accordingly, the potential applicability of the proposed method to meet the clinical demand can be bright.</p><p>The detection system was implemented with the mixed programming technology of Python and C++ based on the open source tool Caffe <ref type="bibr" target="#b35">[36]</ref>. It took about 15 h to train the T-RNN model once for all. During the testing, the T-RNN method generally took less than 1 min to identify the standard planes from a video with 40 frames on a workstation equipped with a 2.50 GHz Intel X-eon E5-2609 CPU and an NVIDIA Titan GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DISCUSSION</head><p>In this paper, we proposed a composite neural network framework that can effectively discover and fuse the in-and between-plane features to identify desirable standard planes in the US videos. The experimental results corroborate the effectiveness of the usage of multitask framework and the between-plane contextual relation on the detection problems of the FASP, FFASP, and FFVSP. Specifically, by comparing the performances between J-CNN and T-CNN in Table <ref type="table" target="#tab_1">III</ref>, the J-CNN model can achieve better performance on the detection of three types of standard planes with the evaluation of all four assessment metrics. Similarly, the J-CNN can mostly achieve better performance as well as in the video-based comparison scheme, see Table <ref type="table" target="#tab_2">IV</ref>. It is worth noting that the J-CNN here is co-trained with 900 US videos (37 376 US images), which is significantly less than the millions of natural images in the ImageNet dataset. Although the margin is not large, the outperformance of J-CNN suggests that the multitask framework can leverage the knowledge of thousands of US images as a more effective CNN model initialization than the cross-domain transferring learning does from millions of natural images.</p><p>Since the multitask learning is to explore sharable features across different tasks for better generalization, it could help those tasks which are slightly under sampled. However, learning from extremely imbalanced data remains a challenge for most learning techniques. For those tasks with less samples, the sharable features may need to be augmented with the task-specific features to achieve better classification/regression performance. For examples, in the context of semantic characterization of pulmonary nodules, the studies explored the sharable features <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref> across different tasks and taskspecific features to address the data imbalance issues for the different semantic characteristics of lung nodules in the annotations. With such exploration scheme, the prediction performance can be improved. We happen to have balanced training data for the three tasks in this paper, and hence, the data imbalance problem may not affect our learning scheme seriously. Since the data imbalance issue is a difficult problem in many machine learning contexts, we will explore this issue in the future study.</p><p>Referring to the performance comparison between the T-RNN and J-CNN in Tables III and IV, the T-RNN averagely achieves higher scores in both image-and video-based schemes with perceivable margins. It thus can prove that the contextual knowledge learned by the LSTM model can effectively boost the detection performance over all three tasks.</p><p>In Table <ref type="table" target="#tab_1">III</ref>, the accuracy scores are significantly higher than their corresponding precision and recall scores with respect to all algorithms. Referring to the equations of measurements, it can be found that the computation of accuracy score includes the number of true negatives in the numerator, whereas the calculation of precision and recall scores does not. Since the number of true negative images, i.e., the nonstandard planes, is significantly larger than the number of the true positive images (standard planes), the accuracy scores are expected to be larger than the scores of precision and recall. Therefore, the assessment metrics of precision, recall, and F1 can be more referential for the evaluation of all comparing algorithms. Fig. <ref type="figure" target="#fig_5">6</ref> lists some examples of false positives and false negatives by the T-RNN in the FFASP and FFVSP tasks to illustrate the difficulty of these two tasks. The false-positively detected planes may be similar to the standard planes but fail to depict some key structures of each task clearly, e.g., the ocular regions in Fig. <ref type="figure" target="#fig_5">6</ref>(a) and ventricular valves in Fig. <ref type="figure" target="#fig_5">6(c</ref>). The false negative detections may be due to the confusion with the weak reconstructed acoustic signals, e.g., the left cardiac walls in Fig. <ref type="figure" target="#fig_5">6(d</ref>) or the presence of other structures, e.g., the bright structures below the eyes and nose in Fig. <ref type="figure" target="#fig_5">6(b)</ref>. Generally speaking, the tasks of FFASP and FFVSP are relatively hard as the head and chest regions contain more bone structures and hence the shadowing effect will be more frequently occur.</p><p>Although the efficacy of the proposed composite neural network has been well demonstrated in this paper, the developed T-RNN model still has several limitations to be addressed in the future studies. First, the current shape of the T-RNN model may still fall short of satisfying the goal of real-time application. The T-RNN generally takes less than 1 min to identify the standard plane when processing a US video with 40 images. In other words, the time to process one frame takes around 1-2 s, and hence, the operator may easily feel the computational lag with such a processing speed. As a consequence, it is probably not able to generate real-time feedback in the clinical US examination. The computational bottleneck of the T-RNN model lies in the sliding window scanning of the J-CNN model. One potential solution to address the high computational cost of the sliding window scheme may be the replacement of the fully connected layers with the fully convolutional layers <ref type="bibr" target="#b61">[62]</ref>.</p><p>Instead of convolving the image with a small window, the fully convolutional network operates on the whole image with the result in the form of probability map <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b62">[63]</ref>. In this way, the detection process can be possibly sped up as only one pass of forward propagation is carried out and the exhaustive scanning can be prevented. Furthermore, the computation for the standard plane detection may also be accelerated to meet the real-time constraint with the substantial code optimization and parallelization. In this paper, we mainly focus on the algorithm design as well as evaluate the efficacy of the proposed method. We leave the acceleration issue for future studies. The second limitation of the proposed T-RNN model consists in that the current data were acquired from healthy babies and mothers. The generalization to the pathological cases remains unknown. To see the capability of the T-RNN model on the identification of standard planes with abnormalities, we shall continue to collect more clinical data with further IRB approvals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>The proposed composite neural network model, i.e., T-RNN, aims to address four major challenges, i.e., high intraclass variation, low interclass variation, limited data, diversity of standard planes, for the computerized detection of fetus standard planes. The T-RNN is able to address the three detection tasks of FASP, FFASP, and FFVSP with the same architecture. With this advantage, the effort to specifically design the detection model for each type of standard plane can be alleviated. The multitask learning framework is introduced here to exploit the shared knowledge across different tasks for reliable model learning and leverage the usage of limited data we have. Meanwhile, with the integration of in-and between-plane cues, the high intraclass and low interclass variation can be further tackled to achieve the current detection performance. The computerized detection of US fetus standard planes is a relatively new topic but crucial to boost the clinical practice. Most previous methods were specifically devised on one dedicated type of standard plane and neglected the contextual cues. This paper proposes a new composite framework with better task generalization and higher identification capability with the fusing of automatically discovered in-and betweenplane cues. Accordingly, this paper would shed a light on the potential applicability of composite neural network models on the processing of difficult US image data. Meanwhile, it may be referential to the future studies on the generalization of other US fetus standard planes, and even the plane selection problems of other organs for the US adult examination.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of different fetal standard planes for FFASP, FASP, and FFVSP, respectively (left column illustrates the anatomical structures, right column illustrates the corresponding US image examples, and the green rectangles denote the ROI).</figDesc><graphic coords="2,88.49,52.72,172.08,192.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Left: overview of the proposed T-RNN model. Right: architecture of the proposed J-CNN model.</figDesc><graphic coords="4,210.59,211.85,70.10,54.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of LSTM model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Left: typical US standard plane detection results. Middle: several feature maps of ROIs in C1 layer. Right: sequenced predictions in the video.</figDesc><graphic coords="6,129.11,227.69,106.13,79.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. PR plane and ROC curves of different methods on FASP detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Examples of false detection results. (a) False positive of FFASP. (b) False negative of FFASP. (c) False positive of FFVSP. (d) False negative of FFVSP.</figDesc><graphic coords="9,56.39,53.33,125.06,85.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE II DETAILS</head><label>II</label><figDesc></figDesc><table /><note><p>OF US DATASET</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE III RESULTS</head><label>III</label><figDesc>OF STANDARD PLANE DETECTION ON US IMAGES</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV RESULTS</head><label>IV</label><figDesc>OF STANDARD PLANE DETECTION ON US VIDEOS</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Basic Research Program of China, 973 Program under Project 2015CB351706, in part by the National Natural Science Foundation of China under Grant 61571304, Grant 61501305, and Grant 61233012, and in part by the Research Grants Council of Hong Kong Special Administrative Region under Grant CUHK 14202514. This paper was recommended by Associate Editor M. Shin.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Utility of portable ultrasound in a community in ghana</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Spencer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Adler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Ultrasound Med</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1735" to="1743" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The practice of ultrasound: A step-by-step guide to abdominal scanning</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Gangarosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gastroenterology</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1357</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">in Webb&apos;s Physics of Medical Imaging</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bamber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tristam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>CRC Press</publisher>
			<biblScope unit="page">351</biblScope>
			<pubPlace>Boca Raton, FL, USA</pubPlace>
		</imprint>
	</monogr>
	<note>Diagnostic ultrasound</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Feasibility and reproducibility of an image-scoring method for quality control of fetal biometry in the second trimester</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Salomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ultrasound Obstetrics Gynecol</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="40" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Detection and measurement of fetal anatomies from ultrasound images using a constrained probabilistic boosting tree</title>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Good</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1342" to="1355" />
			<date type="published" when="2008-09">Sep. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The importance of quality management in fetal measurement</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Dudley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chapman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ultrasound Obstetrics Gynecol</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="190" to="196" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Iterative multi-domain regularized deep learning for anatomical structure detection and segmentation from ultrasound images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Med</title>
		<meeting>Int. Conf. Med<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Intelligent scanning: Automated standard plane selection and biometric measurement of early gestational sac in routine ultrasound examination</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="5015" to="5027" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">FUIQA: Fetal ultrasound image quality assessment with deep convolutional networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCYB.2017.2671898</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern., to be published</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">AIUM practice guideline for the performance of obstetric ultrasound examinations</title>
		<author>
			<persName><surname>Amer</surname></persName>
		</author>
		<author>
			<persName><surname>Inst</surname></persName>
		</author>
		<author>
			<persName><surname>Ultrasound Med</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Ultrasound Med. Official J. Amer. Inst. Ultrasound Med</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Localizing target structures in ultrasound video-A phantom study</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kwitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Razzaque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aylward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="712" to="722" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Standard plane localization in ultrasound by radial component model and selective search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ultrasound Med. Biol</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2728" to="2742" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Quality control of fetal ultrasound images: Detection of abdomen anatomical landmarks using AdaBoost</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rahmatullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sarris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Papageorghiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Biomed. Imag. Nano Macro</title>
		<meeting>IEEE Int. Symp. Biomed. Imag. Nano Macro<address><addrLine>Chicago, IL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="6" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automated retrieval of standard diagnostic fetal cardiac ultrasound planes in the second trimester of pregnancy: A prospective evaluation of software</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abuhamad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Falkensammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Reichartseder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ultrasound Obstetrics Gynecol</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="36" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Integration of local and global features for anatomical object detection in ultrasound</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rahmatullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Papageorghiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Med</title>
		<meeting>Med<address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="402" to="409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic detection and measurement of structures in fetal head ultrasound volumes using sequential estimation and integrated detection network (IDN)</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sofka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Good</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1054" to="1070" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Selective search and sequential detection for standard plane localization in ultrasound</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Abdominal Imaging. Computation and Clinical Applications</title>
		<meeting><address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="203" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fetal abdominal standard plane localization through representation learning with knowledge transfer</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning in Medical Imaging</title>
		<meeting><address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="125" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Searching for structures of interest in an ultrasound video sequence</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Maraci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Napolitano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Papageorghiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning in Medical Imaging</title>
		<meeting><address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="133" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Three-dimensional fetal sonography: Use and misuse</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Benacerraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Ultrasound Med</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1063" to="1067" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stacked deep polynomial network based representation learning for tumor classification with small ultrasound image dataset</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="87" to="94" />
			<date type="published" when="2016-06">Jun. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Biologically inspired model for visual cognition achieving unsupervised episodic and semantic feature learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2335" to="2347" />
			<date type="published" when="2016-10">Oct. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning in medical image analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-I</forename><surname>Suk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="248" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatic detection of cerebral microbleeds from MR images via 3D convolutional neural networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1182" to="1195" />
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Histopathological image classification with color pattern random binary hashing based PCANet and matrix-form classifier</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ying</surname></persName>
		</author>
		<idno type="DOI">10.1109/JBHI.2016.2602823</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform., to be published</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">3D fully convolutional networks for intervertebral disc localization and segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Med. Imag. Virtual Reality</title>
		<meeting>Int. Conf. Med. Imag. Virtual Reality<address><addrLine>Bern, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="375" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for computeraided detection: CNN architectures, dataset characteristics and transfer learning</title>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1285" to="1298" />
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3D deeply supervised network for automatic liver segmentation from CT volumes</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Med</title>
		<meeting>Int. Conf. Med<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="149" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Med</title>
		<meeting>Int. Conf. Med<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013-08">Aug. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">CNN features off-the-shelf: An astounding baseline for recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. Workshops<address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="512" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia</title>
		<meeting>ACM Int. Conf. Multimedia<address><addrLine>Orlando, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Standard plane localization in fetal ultrasound via domain transferred deep neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1627" to="1636" />
			<date type="published" when="2015-09">Sep. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Automatic fetal ultrasound standard plane detection using knowledge transferred recurrent neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015</title>
		<meeting><address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="507" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Recurrent neural networks,&quot; in Design and Applications</title>
		<author>
			<persName><forename type="first">L</forename><surname>Medsker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>CRC Press</publisher>
			<pubPlace>Washington, DC, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A novel connectionist system for unconstrained handwriting recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="855" to="868" />
			<date type="published" when="2009-05">May 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Output convergence analysis for a class of delayed recurrent neural networks with time-varying inputs</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. B, Cybern</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="95" />
			<date type="published" when="2006-02">Feb. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Supervised Sequence Labelling With Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">385</biblScope>
			<pubPlace>Berlin, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null<address><addrLine>Montreal, QC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Gradient Flow in Recurrent Nets: The Difficulty of Learning Long-Term Dependencies</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>IEEE Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1998-11">Nov. 1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Natural image bases to represent neuroimaging data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Ayhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Maida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 30th Int. Conf. Mach. Learn. (ICML)</title>
		<meeting>30th Int. Conf. Mach. Learn. (ICML)<address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="987" to="994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">DCAN: Deep contour-aware networks for object instance segmentation from histology images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017-02">Feb. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th Int. Conf</title>
		<meeting>14th Int. Conf<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Dropout training as adaptive regularization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="351" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno>abs/1308.0850</idno>
		<ptr target="http://arxiv.org/abs/1308.0850" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="page" from="1" to="43" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning to execute</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>abs/1410.4615</idno>
		<ptr target="http://arxiv.org/abs/1410.4615" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Gradient-based learning algorithms for recurrent networks and their computational complexity</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Back-Propagation: Theory, Architectures and Applications</title>
		<meeting><address><addrLine>Hillsdale, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Lawrence Erlbaum Associates</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="433" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<meeting><address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A probabilistic interpretation of precision, recall and f-score, with implication for evaluation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Goutte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gaussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval</title>
		<meeting><address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="345" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Bridging computational features toward multiple semantic features with multi-task regression: A study of CT pulmonary nodules</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Med</title>
		<meeting>Int. Conf. Med<address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="53" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Automatic scoring of multiple semantic attributes with multi-task feature leverage: A study on pulmonary nodules in CT images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Imag</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="802" to="814" />
			<date type="published" when="2017-03">Mar. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Mitosis detection in breast cancer histology images via deep cascaded networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th AAAI Conf</title>
		<meeting>13th AAAI Conf<address><addrLine>Phoenix, AZ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1160" to="1166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
