<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Multi-label Text Classification: An Attention-based Recurrent Network Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wei</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
							<email>cheneh@ustc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuying</forename><surname>Chen</surname></persName>
							<email>yuying.cyy@antfin.com</email>
						</author>
						<author>
							<persName><forename type="first">Zai</forename><surname>Huang</surname></persName>
							<email>huangzai@mail.ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
							<email>zhaozhou@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dan</forename><surname>Zhang</surname></persName>
							<email>danzhang@iflytek.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">iFLYTEK Research</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">iFLYTEK Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shijin</forename><surname>Wang</surname></persName>
							<email>sjwang3@iflytek.com</email>
							<affiliation key="aff2">
								<orgName type="institution">iFLYTEK Research</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">iFLYTEK Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="middle">2019</forename><surname>Hierarchical</surname></persName>
						</author>
						<title level="a" type="main">Hierarchical Multi-label Text Classification: An Attention-based Recurrent Network Approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">58DB8049DD6B5E0D41DBE86A1A6BF9FD</idno>
					<idno type="DOI">10.1145/3357384.3357885</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T05:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Hierarchical Multi-label Text Classification</term>
					<term>Attention Mechanism</term>
					<term>Hierarchical Attention Networks Chemistry | Physics Inorganic Chemistry | Nuclear Physics Non-metallic Elements | Radioactive Sources | Nuclear Reactors</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hierarchical multi-label text classification (HMTC) is a fundamental but challenging task of numerous applications (e.g., patent annotation), where documents are assigned to multiple categories stored in a hierarchical structure. Categories at different levels of a document tend to have dependencies. However, the majority of prior studies for the HMTC task employ classifiers to either deal with all categories simultaneously or decompose the original problem into a set of flat multi-label classification subproblems, ignoring the associations between texts and the hierarchical structure and the dependencies among different levels of the hierarchical structure. To that end, in this paper, we propose a novel framework called H ierarchical Attention-based Recurrent N eural N etwork (HARNN) for classifying documents into the most relevant categories level by level via integrating texts and the hierarchical category structure. Specifically, we first apply a documentation representing layer for obtaining the representation of texts and the hierarchical structure. Then, we develop an hierarchical attention-based recurrent layer to model the dependencies among different levels of the hierarchical structure in a top-down fashion. Here, a hierarchical attention strategy is proposed to capture the associations between texts and the hierarchical structure. Finally, we design a hybrid method which is capable of predicting the categories of each level while classifying all categories in the entire hierarchical structure precisely. Extensive experimental results on two real-world datasets demonstrate the effectiveness and explanatory power of HARNN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Many real-world applications organize documents in a hierarchical structure, where classes are specialized into subclasses or grouped into superclasses <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32]</ref>. For example, an electronic document (e.g., web-page, patent and e-mail) is associated with multiple categories and all these categories are stored hierarchically in a tree or a Direct Acyclic Graph (DAG) <ref type="bibr" target="#b15">[16]</ref>. Figure <ref type="figure" target="#fig_0">1</ref> shows a toy example of a patent document with a four-level hierarchical category structure, the top part is a detailed text description of the patent, and its corresponding hierarchical categories with a tree graph representation are shown below. The root node is considered as level 0, and the parent nodes represent more general than the child nodes. It is an elegant way to show the characteristics of data and a multi-dimensional perspective to tackle the classification problem via the hierarchical structure. Thus, this kind of problem, known as hierarchical multilabel text classification (HMTC), has aroused widespread attraction in both the industry and the academia <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>In the literature, there are many efforts for HMTC problem. Initially, the flat-based methods (e.g., Naive Bayes) have been proposed to predict only the categories of the last level by reducing the HMTC problem into a flat multi-label problem <ref type="bibr" target="#b11">[12]</ref>. Unfortunately, these simple approaches ignore the hierarchical category structure information (e.g., nuclear physics is the subclass of physics in Figure <ref type="figure" target="#fig_0">1</ref>). To tackle the problem above, some works have taken the hierarchical structure into consideration <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27]</ref>, which can be further compartmentalized into two approaches: <ref type="bibr" target="#b0">(1)</ref> Local approaches generate hierarchical classifiers and each classifier is responsible for predicting either corresponding categories <ref type="bibr" target="#b1">[2]</ref> or corresponding category levels <ref type="bibr" target="#b26">[27]</ref>; <ref type="bibr" target="#b1">(2)</ref> Global approaches gather all the levels of categories together and predict them with a single classifier <ref type="bibr" target="#b9">[10]</ref>. However, these studies only focus on either the local regions or the overall structure of the category hierarchy, while ignoring the dependencies among different levels of the hierarchical structure.</p><p>In summary, there are still many unique challenges inherent in designing an effective HMTC solution. First, in a text classification task, different parts of the document are associated with different</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Categories:</head><p>A gas injection system of a nuclear power plant including a reactor and a gaseous waste disposal system, comprises a hydrogen injection unit for injecting hydrogen into a reactor of a nuclear power plant, an oxygen injection unit for … categories. For instance, in Figure <ref type="figure" target="#fig_0">1</ref>, the "red" words of document D focus more on physics while the "blue" words concentrate more on chemistry in C 1 (level-1 category), and the words on the "red" underline further describe nuclear physics in C 2 (level-2 category). Therefore, when understanding the semantic of each document, it is necessary to capture these associations between texts and the hierarchical structure. Second, there are dependencies among different levels in the hierarchical category structure (i.e., the determination of a category not only is influenced by its parent category but also will affect its child categories). As shown in Figure <ref type="figure" target="#fig_0">1</ref>, document D concentrates on the nuclear physics in C 2 since its parent category physics in C 1 is focused. And nuclear reactors in C 3 is the subclass of nuclear physics, which should also be highly heeded. Thus, it is also critical to classify level by level via considering the dependencies among different levels of the hierarchical structure. Third, when assigning the document into hierarchical categories, not only the local regions but also the overall structure of the category hierarchy should be considered. Hence, how to predict the categories of each level while classifying all categories in the entire hierarchical structure precisely is a nontrivial problem.</p><p>To address the challenges mentioned above, in this paper, we propose a novel framework named Hierarchical Attention-based Recurrent Neural Network (HARNN), which can automatically annotate a document with the most relevant categories level by level. Specifically, given the documents and the whole hierarchical category structure, we first apply a Documentation Representing Layer (DRL) for obtaining the representation of texts and the hierarchical structure. Then, we devise an Hierarchical Attention-based Recurrent Layer (HARL) to model the dependencies among different levels by leveraging the hierarchical structure gradually in a top-down fashion. To be specific, at each level, the attention mechanism qualifies the contribution of each text word to each category, and the textcategory association information will affect the next category level. Throughout the recurrence, the Hierarchical Attention-based Memory (HAM) unit we designed can model the dependencies among different levels of the hierarchical structure. After that, we utilize a Hybrid Predicting Layer (HPL) for combining the predictions of each level in the category hierarchy and the overall hierarchical structure. Thus, the proposed model is capable of predicting the categories of each level while classifying all categories in the entire hierarchical structure precisely. Finally, extensive experiments on two large-scale real-world datasets demonstrate the effectiveness and explanatory power of our model. To the best of our knowledge, this is the first comprehensive attempt to employ hierarchical attentive neural networks for HMTC problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Generally, the related works can be grouped into two research aspects, i.e., studies on hierarchical multi-label text classification and attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Studies on HMTC</head><p>There have been several efforts for HMTC in the literature. Initially, flat-based methods were widely used in HMTC task, such as Decision Tree (DT) and Naive Bayes (NB) <ref type="bibr" target="#b11">[12]</ref>, which learned discriminant classifiers only for the categories of the last level in the category hierarchy. However, the hierarchical structure information was ignored in these approaches, which caused the inefficiency. Thus, some hierarchical approaches were proposed by taking the hierarchical structure information into consideration and could be compartmentalized into local and global approaches according to the strategy adopted. Regarding local approaches, Cesa-Bianchi et al. <ref type="bibr" target="#b7">[8]</ref> proposed a classification method using hierarchical SVM, where SVM learning was applied to a category only if its parent category was labeled as positive. Afterwards, a large margin method was proposed by Rousu et al. <ref type="bibr" target="#b25">[26]</ref> to calculate the maximum structural margin for the output categories. As for global approaches, Vens et al. <ref type="bibr" target="#b30">[31]</ref> developed a tree-based approach called Clus-HMC to employ a single decision tree to deal with the entire hierarchical category structure. Recently, neural networks have been utilized for HMTC problems and have shown its effectiveness. Cerri et al. <ref type="bibr" target="#b6">[7]</ref> attempted to use HMC-LMLP for incrementally training a set of neural networks and each being responsible for predicting the categories in a given level. Borges et al. <ref type="bibr" target="#b3">[4]</ref> proposed a global approach based on a competitive artificial neural network to predict all categories in the hierarchical structure.</p><p>Nevertheless, the above works mainly focus on either the local regions or the overall structure of the category hierarchy. Moreover, they neglect the dependencies among the different levels of the hierarchical structure, which leads to error propagation and well-known class-membership inconsistency <ref type="bibr" target="#b27">[28]</ref>. Along this line, methods combing the advantages of local and global approaches have been applied <ref type="bibr" target="#b32">[33]</ref> in many domains (e.g., text classification, protein function prediction), where a neural network HMCN was proposed for integrating the predictions of each level in the category hierarchy and the overall hierarchical structure. Unfortunately, they failed to capture the associations between texts and the hierarchical structure, which are important for understanding the semantics of each document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Attention Mechanism</head><p>In our framework, one of the most significant steps is to reveal the associations between texts and each category in the hierarchical structure from top to down and level by level, which is related to the attention mechanism <ref type="bibr" target="#b36">[37]</ref>. In text classification, attention mechanism is a powerful approach to highlight different parts of the text semantic representation by assigning different weights. For example, Huang et al. <ref type="bibr" target="#b17">[18]</ref> used attention mechanism on the top of a CNN model to introduce an extra source of information for guiding the extraction of the sentence embedding. Tao et al. <ref type="bibr" target="#b29">[30]</ref> utilized an attention method which created a weighted vector representation by using the encodings of either RNNs hidden states. Lin et al. <ref type="bibr" target="#b19">[20]</ref> designed a self-attention mechanism for the sequential models (e.g., RNN) to replace the max pooling or averaging step, which enabled their model pay attention to the different aspects of the sentence.</p><p>In the aforementioned works, the attention weights are usually calculated by the correspondence between texts and each category in particular levels, which treats the different levels of the hierarchical structure independently thereby ignores the dependencies among the different levels. In this work, we propose a novel hierarchical attention recurrent structure to capture the associations between texts and each category gradually from top to down, which integrates the dependencies among different levels. Specifically, in the hierarchical attention mechanism, the attention weights of texts and each category in a level not only are influenced by its previous level but also will affect the next level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES 3.1 Problem Definition</head><p>In HMTC problem, there are a set of documents (e.g., patent) and each document contains the text description and its expected categories which are organized in a hierarchical structure. Before defining the HMTC problem, we start with a detailed description of the hierarchical structure and documents.</p><formula xml:id="formula_0">Definition 3.1. (Hierarchical Structure γ ). Given the defined possible categories in H hierarchical levels C = (C 1 , C 2 , . . . , C H ), where C i = {c 1 , c 2 , . . . } ∈ {0, 1} |C i |</formula><p>is the set of possible categories in the i-th hierarchical level and |C i | is the number of categories in the i-th hierarchical level and K is the total number of categories. We define the hierarchical category structure γ over C as a partial order set (C, ≺). ≺ is a partial order representing the PARENT-OF relationship, which is asymmetric, anti-reflexive and transitive <ref type="bibr" target="#b33">[34]</ref>:</p><formula xml:id="formula_1">• ∀c x ∈ C i , c y ∈ C j , c x ≺ c y then c y ⊀ c x • ∀c x ∈ C i , c x ⊀ c x • ∀c x ∈ C i , c y ∈ C j , c z ∈ C k , if c x ≺ c y and c y ≺ c z then c x ≺ c z</formula><p>A set of M documents with expected hierarchical categories can be denoted as X = {(D 1 , L 1 ), (D 2 , L 2 ) . . . , (D M , L M )}, where D i = {w 1 , w 2 , . . . , w N } can usually be represented as a sequence of N words, and L i = {ℓ 1 , ℓ 2 , . . . , ℓ H } is the set of excepted hierarchical categories assigned to the document D i , with ℓ i ⊂ C i in the corresponding hierarchical structure γ . Then, the HMTC problem can be formulated as: Definition 3.2. (HMTC Problem). Given a set of documents and the corresponding hierarchical category structure, our goal is to integrate the document texts D and the corresponding hierarchical category structure γ to learn a classification model Ω, which can be used to predict the hierarchical categories L for documents:</p><formula xml:id="formula_2">Ω(D, γ , Θ) → L, (<label>1</label></formula><formula xml:id="formula_3">)</formula><p>where Θ is the parameters of Ω.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MODEL ARCHITECTURE</head><p>In this section, we will introduce the technical details of HARNN framework. As shown in Figure <ref type="figure" target="#fig_2">2</ref>, HARNN mainly contains three parts, i.e., Documentation Representing Layer (DRL), Hierarchical Attention-based Recurrent Layer (HARL) and Hybrid Predicting Layer (HPL). Specifically, we utilize DRL to obtain the unified representation of each document text and the hierarchical category structure. Then, we design HARL to model the dependencies among different levels via capturing the associations between texts and each category of the hierarchical structure in a top-down fashion. Finally, HPL is applied to predict the hierarchical categories of documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Documentation Representing Layer</head><p>In the first stage of HARNN, DRL aims to generate the unified representation of the document text and the hierarchical category structure. To this end, we first apply an Embedding Layer to encode text and the hierarchical category structure. Then a Bi-LSTM Layer is utilized to enhance the encodings of text semantic representation.</p><p>Embedding Layer is used to encode the text and the hierarchical category structure. DRL receives the text tokens of a document D and the hierarchical category structure γ as input, as shown in Figure <ref type="figure" target="#fig_2">2</ref>. Intuitively, the document text D is formalized as a sequence of N words D = (w 1 , w 2 , . . . , w N ), where w i ∈ R k is initialized by a k-dimensional pre-trained word embedding with Word2vec <ref type="bibr" target="#b22">[23]</ref>. Like Word2vec operation in text tokens, we embed the hierarchical category structure γ into a matrix S = (S 1 , S 2 , . . . , S H ) for training, where S i ∈ R |C i |×d a is a randomly initialized matrix which represents the embedding of the i-th hierarchical category level with the d a -dimension. After the initialization from Embedding Layer, we apply the subsequent Bi-LSTM Layer to enhance the semantic representations of the document text.</p><p>Bi-LSTM Layer targets at enhancing the encodings of the text semantic representation. For the text tokens in D, we utilize a Bi-LSTM architecture, which is a variant of the traditional LSTM architecture <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17]</ref>. Bi-LSTM can learn not only long-range dependencies across the input sequence but also context information from forward and backward simultaneously, which is beneficial for enhancing the encodings of the text semantic representation. Specifically, the input of the Bi-LSTM network is a sequence D = (w 1 , w 2 , . . . , w N ), and the hidden vector of a Bi-LSTM is calculated as follows:</p><formula xml:id="formula_4">-→ h n = LST M( - → h n-1 , w n ), ← - h n = LST M( ← - h n+1 , w n ), h n = [ -→ h n , ← - h n ],<label>(2)</label></formula><p>where -→ h n and ←h n are the forward hidden vector and the backward hidden vector respectively at the n-th word step in the Bi-LSTM. And h n ∈ R 2u is the hidden output of Bi-LSTM at the n-th word step, which is the concatenation of -→ h n and ←h n , where u is the hidden unit number of each unidirectional LSTM.</p><p>For simplicity, we note all h n as V = {h 1 , h 2 , . . . , h N } ∈ R N ×2u . After that, to obtain the whole semantic representation of the document D, we exploit the word-wise average pooling operation to merge N words contextual representations into an average embedding </p><formula xml:id="formula_5">V as V = avд(h 1 , h 2 , . . . , h N ) ∈ R 2u .</formula><formula xml:id="formula_6">P 1 L &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 H W K j 4 E l n Z B x 1 G G S t a q I G W 0 P o p M = " &gt; A A A B 8 H i c b V A 9 S w N B E J 2 L X z F + R S 1 t F o N g F e 6 i o G X Q x s I i g v m Q 5 A x 7 m 7 1 k y e 7 e s b s n h O N + h Y 2 F I r b + H D v / j Z v k C k 1 8 M P B 4 b 4 a Z e U H M m T a u + + 0 U V l b X 1 j e K m 6 W t 7 Z 3 d v f L + Q U t H i S K 0 S S I e q U 6 A N e V M 0 q Z h h t N O r C g W A a f t Y H w 9 9 d t P V G k W y X s z i a k v 8 F C y k B F s r P T Q 6 K e 3 2 W P q Z f 1 y x a 2 6 M 6 B l 4 u W k A j k a / f J X b x C R R F B p C M d a d z 0 3 N n 6 K l W G E 0 6 z U S z S N M R n j I e 1 a K r G g 2 k 9 n B 2 f o x C o D F E b K l j R o p v 6 e S L H Q e i I C 2 y m w G e l F b y r + 5 3 U T E 1 7 6 K Z N x Y q g k 8 0 V h w p G J 0 P R 7 N G C K E s M n l m C i m L 0 V k R F W m B i b U c m G 4 C 2 + v E x a t a p 3 V q 3 d n V f q V 3 k c R T i C Y z g F D y 6 g D j f Q g C Y Q E P A M r / D m K O f F e X c + 5 q 0 F J 5 8 5 h D 9 w P n 8 A q + 2 Q U g = = &lt; / l a t e x i t &gt; P 2 L &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F X T V M X f L R Q J 2 U 4 V r I j J A e v y d u B o = " &gt; A A A B 8 H i c b V A 9 S w N B E J 2 L X z F + R S 1 t F o N g F e 6 i o G X Q x s I i g v m Q 5 A x 7 m 7 1 k y e 7 e s b s n h O N + h Y 2 F I r b + H D v / j Z v k C k 1 8 M P B 4 b 4 a Z e U H M m T a u + + 0 U V l b X 1 j e K m 6 W t 7 Z 3 d v f L + Q U t H i S K 0 S S I e q U 6 A N e V M 0 q Z h h t N O r C g W A a f t Y H w 9 9 d t P V G k W y X s z i a k v 8 F C y k B F s r P T Q 6 K e 3 2 W N a y / r l i l t 1 Z 0 D L x M t J B X I 0 + u W v 3 i A i i a D S E I 6 1 7 n p u b P w U K 8 M I p 1 m p l 2 g a Y z L G Q 9 q 1 V G J B t Z / O D s 7 Q i V U G K I y U L W n Q T P 0 9 k W K h 9 U Q E t l N g M 9 K L 3 l T 8 z + s m J r z 0 U y b j x F B J 5 o v C h C M T o e n 3 a M A U J Y Z P L M F E M X s r I i O s M D E 2 o 5 I N w V t 8 e Z m 0 a l X v r F q 7 O 6 / U r / I 4 i n A E x 3 A K H l x A H W 6 g A U 0 g I O A Z X u H N U c 6 L 8 + 5 8 z F s L T j 5 z C H / g f P 4 A r X K Q U w = = &lt; / l a t e x i t &gt; P H L &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E q R d w B p C d G O O W J a 2 y 1 k n 9 t u 7 X h A = " &gt; A A A B 8 H i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y h C s w l 0 U t A z a p L C I Y D 4 k O c P e Z i 9 Z s r t 3 7 O 4 J 4 b h f Y W O h i K 0 / x 8 5 / 4 y a 5 Q h M f D D z e m 2 F m X h B z p o 3 r f j s r q 2 v r G 5 u F r e L 2 z u 7 e f u n g s K W j R B H a J B G P V C f A m n I m a d M w w 2 k n V h S L g N N 2 M L 6 Z + u 0 n q j S L 5 L 2 Z x N Q X e C h Z y A g 2 V n p o 9 N P b 7 D G t Z / 1 S 2 a 2 4 M 6 B l 4 u W k D D k a / d J X b x C R R F B p C M d a d z 0 3 N n 6 K l W G E 0 6 z Y S z S N M R n j I e 1 a K r G g 2 k 9 n B 2 f o 1 C o D F E b K l j R o p v 6 e S L H Q e i I C 2 y m w G e l F b y r + 5 3 U T E 1 7 5 K Z N x Y q g k 8 0 V h w p G J 0 P R 7 N G C K E s M n l m C i m L 0 V k R F W m B i b U d G G 4 C 2 + v E x a 1 Y p 3 X q n e X Z R r 1 3 k c B T i G E z g D D y 6 h B n V o Q B M I C H i G V 3 h z l P P i v D s f 8 9 Y V J 5 8 5 g j 9 w P n 8 A z u C Q a Q = = &lt; / l a t e x i t &gt; ⊗(1 ↵) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P 0 J s b z Y 0 3 c F J 5 T H x h a K Y 1 F Q x b 2 k = " &gt; A A A C A H i c b V C 7 T s M w F H X K q 5 R X g I G B x W q F V I S o k j L A W M H C W C T 6 k J q o c h y n t X C c y H a Q o i g L 3 8 A f s D C A E C u f w d a / w W k 7 Q M u R L B + d c 6 / u v c e L G Z X K s i Z G a W V 1 b X 2 j v F n Z 2 t 7 Z 3 T P 3 D 7 o y S g Q m H R y x S P Q 9 J A m j n H Q U V Y z 0 Y 0 F Q 6 D H S 8 x 5 u C r / 3 S I S k E b 9 X a U z c E I 0 4 D S h G S k t D 8 8 j x I u b L N N R f V r f P H c T i M T r N h 2 b N a l h T w G V i z 0 m t V X X O n i e t t D 0 0 v x 0 / w k l I u M I M S T m w r V i 5 G R K K Y k b y i p N I E i P 8 g E Z k o C l H I Z F u N j 0 g h y d a 8 W E Q C f 2 4 g l P 1 d 0 e G Q l n s q C t D p M Z y 0 S v E / 7 x B o o I r N 6 M 8 T h T h e D Y o S B h U E S z S g D 4 V B C u W a o K w o H p X i M d I I K x 0 Z h U d g r 1 4 8 j L p N h v 2 R a N 5 p 9 O 4 B j O U w T G o g j q w w S V o g V v Q B h 2 A Q Q 5 e</formula><p>w B t 4 N 5 6 M V + P D + J y V l o x 5 z y H 4 A + P r B 2 H k m T w = &lt; / l a t e x i t &gt; ⊗↵ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y M 8 + 9 f 8 g W l R f g q x 2 6 z L p m T a X F 7 c = "</p><formula xml:id="formula_7">&gt; A A A B 7 X i c b Z C 7 S g N B F I Z n 4 y 2 u t 6 i l z W I Q r M J u L L Q R g z a W E c w F k i W c n c w m Y 2 Z n h p l Z I S x 5 B x s L R W w s f B R 7 G / F t n F w K T f x h 4 O P / z 2 H O O Z F k V B v f / 3 Z y S 8 s r q 2 v 5 d X d j c 2 t 7 p 7 C 7 V 9 c i V Z j U s G B C N S P Q h F F O a o Y a R p p S E U g i R h r R 4 G q c N + 6 J 0 l T w W z O U J E y g x 2 l M M R h r 1 d v A Z B 8 6 h a J f 8 i f y F i G Y Q f H i w z 2 X b 1 9 u t V P 4 b H c F T h P C D W a g d S v w p Q k z U I Z i R k Z u O 9 V E A h 5 A j 7 Q s c k i I D r P J t C P v y D p d L x b K P m 6 8 i f u 7 I 4 N E 6 2 E S 2 c o E T F / P Z 2 P z v 6 y V m v g s z C i X q S E c T z + K U + Y Z 4 Y 1 X 9 7 p U E W z Y 0 A J g R e 2 s H u 6 D A m z s g V x 7 h G B + 5 U W o l 0 v B S</formula><p>a l 8 4 x c r l 2 i q P D p A h + g Y B e g U V d A 1 q q I a w u g O P a A n 9 O w I 5 9 F 5 c V 6 n p T l n 1 r O P / s h 5 / w H s V p J Z &lt; / l a t e x i t &gt; P G &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T I T </p><formula xml:id="formula_8">I C D T d m S m c i v O e y z o P 1 / m O 2 Y M = " &gt; A A A B 7 H i c b V C 7 S g N B F L 0 b X z G + o o K N z W A Q r M J u L L Q M s d A y A T c J x C X M T m a T I b M z y 8 y s E J Z 8 g 4 2 F I r Z 2 / o V f Y G f j t z h 5 F J p 4 4 M L h n H u 5 9 5 4 w 4 U w b 1 / 1 y c i u r a + s b + c 3 C 1 v b O 7 l 5 x / 6 C p Z a o I 9 Y n k U r V D r C l n g v q G G U 7 b i a I 4 D j l t h c O r i d + 6 p 0 o z K W 7 N K K F B j P u C R Y x g Y y W / 3 s 2 u x 9 1 i y S 2 7 U 6 B l 4 s 1 J q X r U + G b v t Y 9 6 t / h 5 1 5 M k j a k w h G O t O 5 6 b m C D D y j D C 6 b h w l 2 q a Y D L E f d q x V O C Y 6 i C b H j t G p 1 b p o U g q W 8 K g q f p 7 I s O x 1 q M 4 t J 0 x N g O 9 6 E 3 E / 7 x O a q L L I G M i S Q 0 V Z L Y o S j k y E k 0 + R z 2 m K D F 8 Z A k m i t l b E R l g h Y m x + R R s C N 7 i y 8 u k W S l 7 5 + V K w 6 Z R g x n y c A w n c A Y e X E A V b q A O P h B</formula><formula xml:id="formula_9">Q o 0 L z d r 8 d T x D + G F n E O w 7 g = " &gt; A A A B 7 H i c b V C 7 S g N B F L 0 b X z G + o o K N z W A Q r M J u L L Q M E c Q y A T c J x C X M T m a T I b M z y 8 y s E J Z 8 g 4 2 F I r Z 2 / o V f Y G f j t z h 5 F J p 4 4 M L h n H u 5 9 5 4 w 4 U w b 1 / 1 y c i u r a + s b + c 3 C 1 v b O 7 l 5 x / 6 C p Z a o I 9 Y n k U r V D r C l n g v q G G U 7 b i a I 4 D j l t h c O r i d + 6 p 0 o z K W 7 N K K F B j P u C R Y x g Y y W / 3 s 2 u x 9 1 i y S 2 7 U 6 B l 4 s 1 J q X r U + G b v t Y 9 6 t / h 5 1 5 M k j a k w h G O t O 5 6 b m C D D y j D C 6 b h w l 2 q a Y D L E f d q x V O C Y 6 i C b H j t G p 1 b p o U g q W 8 K g q f p 7 I s O x 1 q M 4 t J 0 x N g O 9 6 E 3 E / 7 x O a q L L I G M i S Q 0 V Z L Y o S j k y E k 0 + R z 2 m K D F 8 Z A k m i t l b E R l g h Y m x + R</formula><p>R s C N 7 i y 8 u k W S l 7 5 + V K w 6 Z R g x n y c A w n c A Y e X E A V b q A O P h B g 8 A B P 8 O w I 5 9 F 5 c V 5 n r T l n P n M I f + C 8 / Q C S 5 Z I 6 &lt; / l a t e x i t &gt; ! 0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n 6 n f 7 D l m m g y j g</p><formula xml:id="formula_10">x B Z r K c 5 k / E 8 L D Q = " &gt; A A A B 8 X i c b Z D L S g M x F I Y z X u t 4 q 7 p 0 E y y C q z J T F 7 o R i 2 5 c V r A X b M e S S c + 0 o U l m S D J C G f o W b l w o o k s f x L 0 b 8 W 1 M L w t t / S H w 8 f / n k H N O m H C m j e d 9 O w u L S 8 s r q 7 k 1 d 3 1 j c 2 s 7 v 7 N b 0 3 G q K F R p z G P V C I k G z i R U D T M c G o k C I k I O 9 b B / O c r r 9 6 A 0 i + W N G S Q Q C N K V L G K U G G v d t m I B X X K X e c N 2 v u A V v b H w P P h T K J x / u G f J 2 5 d b a e c / W 5 2 Y p g K k o Z x o 3 f S 9 x A Q Z U Y Z R D k O 3 l W p I C O 2 T L j Q t S i J A B 9 l 4 4 i E + t E 4 H R 7 G y T x o 8 d n 9 3 Z E R o P R C h r R T E 9 P R s N j L / y 5 q p i U 6 D j M k k N S D p 5 K M o 5 d j E e L Q + 7 j A F 1 P C B B U I V s 7 N i 2 i O K U G O P 5 N o j + L M r z 0 O t V P S P i 6 V r r 1 C + Q B P l 0 D 4 6 Q E f I R y e o j K 5 Q B V U R R R I 9 o C f 0 7 G j n 0 X l x X i e l C 8 6 0 Z w / 9 k f P + A 9 / j l A o = &lt; / l a t e x i t &gt; ! 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W i K 2 U B R l 0 t e 1 u 2 j + i z 2 I m c Q F 6 0 Q = " &gt; A A A B 8 X i c b Z D L S g M x F I Y z X u t 4 q 7 p 0 E y y C q z J T F 7 o R i 2 5 c V r A X b M e S S c + 0 o U l m S D J C G f o W b l w o o k s f x L 0 b 8 W 1 M L w t t / S H w 8 f / n k H N O m H C m j e d 9 O w u L S 8 s r q 7 k 1 d 3 1 j c 2 s 7 v 7 N b 0 3 G q K F R p z G P V C I k G z i R U D T M c G o k C I k I O 9 b B / O c r r 9 6 A 0 i + W N G S Q Q C N K V L G K U G G v d t m I B X X K X + c N 2 v u A V v b H w P P h T K J x / u G f J 2 5 d b a e c / W 5 2 Y p g K k o Z x o 3 f S 9 x A Q Z U Y Z R D k O 3 l W p I C O 2 T L j Q t S i J A B 9 l 4 4 i E + t E 4 H R 7 G y T x o 8 d n 9 3 Z E R o P R C h r R T E 9 P R s N j L / y 5 q p i U 6 D j M k k N S D p 5 K M o 5 d j E e L Q + 7 j A F 1 P C B B U I V s 7 N i 2 i O K U G O P 5 N o j + L M r z 0 O t V P S P i 6 V r r 1 C + Q B P l 0 D 4 6 Q E f I R y e o j K 5 Q B V U R R R I 9 o C f 0 7 G j n 0 X l x X i e l C 8 6 0 Z w / 9 k f P + A + F o l A s = &lt; / l a t e x i t &gt; ! 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t m D Y B a h F l z r Q Y w f t a h f y z J F t o 1 U = " &gt; A A A B 8 X i c b Z D L S g M x F I b P e K 3 j r e r S T b A I r s p M X e h G L L p x W c F e s B 1 L J s 2 0 o U l m S D J C G f o W b l w o o k s f x L 0 b 8 W 1 M L w t t / S H w 8 f / n k H N O m H C m j e d 9 O w u L S 8 s r q 7 k 1 d 3 1 j c 2 s 7 v 7 N b 0 3 G q C K 2 S m M e q E W J N O Z O 0 a p j h t J E o i k X I a T 3 s X 4 7 y + j 1 V m s X y x g w S G g j c l S x i B B t r 3 b Z i Q b v 4 L i s N 2 / m C V / T G Q v P g T 6 F w / u G e J W 9 f b q W d / 2 x 1 Y p I K K g 3 h W O u m 7 y U m y L A y j H A 6 d F u p p g k m f d y l T Y s S C 6 q D b D z x E B 1 a p 4 O i W N k n D R q 7 v z s y L L Q e i N B W C m x 6 e j Y b m f 9 l z d R E p 0 H G Z J I a K s n k o y j l y M R o t D 7 q M E W J 4 Q M L m C h m Z 0 W k h x U m x h 7 J t U f w Z 1 e e h 1 q p 6 B 8 X S 9 d e o X w B E + V g H w 7 g C H w 4 g T J c Q Q W q Q E D C A z z B s 6 O d R + f F e Z 2 U L j j T n j 3 4 I + f 9 B + L t l A w = &lt; / l a t e x i t &gt; ! H 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 8 V 6 U M 7 T m g g 4 O a C u i c t m h a s D R C c c = " &gt; A A A B 8 3 i c b V C 7 S g N B F J 2 N r 7 i + o p Y 2 g 0 G w M e z G Q h s x a J M y g n l A d g 2 z k 9 l k y D y W m V k h L P k N G w t F b f 0 O e x v x b 5 w 8 C k 0 8 c O F w z r 3 c e 0 + U M K q N 5 3 0 7 u a X l l d W 1 / L q 7 s b m 1 v V P Y 3 W t o m S p M 6 l g y q V o R 0 o R R Q e q G G k Z a i S K I R 4 w 0 o 8 H 1 2 G / e E 6 W p F L d m m J C Q o 5 6 g M c X I W C k I J C c 9 d J d V T / x R p 1 D 0 S t 4 E c J H 4 M 1 K 8 / H A v k t c v t 9 Y p f A Z d i V N O h M E M a d 3 2 v c S E G V K G Y k Z G b p B q k i A 8 Q D 3 S t l Q g T n S Y T W 4 e w S O r d G E s l S 1 h 4 E T 9 P Z E h r v W Q R 7 a T I 9 P X 8 9 5 Y / M 9 r p y Y + D z M q k t Q Q g a e L 4 p R B I + E 4 A N i l i m D D h p Y g r K i 9 F e I + U g g b G 5 N r Q / D n X 1 4 k j X L J P y 2 V b 7 x i 5 Q p M k Q c H 4 B A c A x + c g Q q o g h q o A w w S 8 A C e w L O T O o / O i / M 2 b c 0 5 s 5 l 9 8 A f O + w / k Z 5 S U &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f J + x I O N H l 3 e q L G r 8 e w G g f e 3 / x 4 E = " &gt; A A A B 7 n i c b V D L S g N B E J z 1 G e M r 6 l G R w S B 4 C r v x o M e g F 4 8 J m A c k S + i d z C Z D Z m a X m V k h L D n 6 A V 4 8 K O L V T 8 h 3 e P M b / A l n k x w 0 s a C h q O q m u y u I O d P G d b + c l d W 1 9 Y 3 N 3 F Z + e 2 d 3 b 7 9 w c N j Q U a I I r Z O I R 6 o V g K a c S V o 3 z H D a i h U F E X D a D I a 3 m d 9 8 o E q z S N 6 b U U x 9 A X 3 J Q k b A W K n Z 6 Y M Q k O 8 W i m 7 J n Q I v E 2 9 O i p W T S e 3 7 8 X R S 7 R Y + O 7 2 I J I J K Q z h o 3 f b c 2 P g p K M M I p + N 8 J 9 E 0 B j K E P m 1 b K k F Q 7 a f T c 8 f 4 3 C o 9 H E b K l j R 4 q v 6 e S E F o P R K B 7 R R g B n r R y 8 T / v H Z i w m s / Z T J O D J V k t i h M O D Y R z n 7 H P a Y o M X x k C R D F 7 K 2 Y D E A B M T a h L A R v 8 e V l 0 i i X v M t S u W b T u E E z 5 N A x O k M X y E N X q I L u U B X V E U F D 9 I R e 0 K s T O 8 / O m / M + a 1 1 x 5 j N H 6 A + c j x + r r Z L U &lt; / l a t e x i t &gt; V &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z 7 W M 4 r 9 Z 0 K z t G 5 O K w L i E I j + q g W 4 = " &gt; A A A B 6 H i c b Z C 7 S g N B F I b P x l t c b 1 F L m 8 U g W I X d W G g j B m 0 s E z A X S J Y w O z m b j J m d X W Z m h b D k C W w s F L H V h 7 G 3 E d / G y a X Q 6 A 8 D H / 9 / D n P O C R L O l H b d L y u 3 t L y y u p Z f t z c 2 t 7 Z 3 C r t 7 D R W n k m K d x j y W r Y A o 5 E x g X T P N s Z V I J F H A s R k M r y Z 5 8 w 6 l Y r G 4 0 a M E / Y j 0 B Q s Z J d p Y t U a 3 U H R L 7 l T O X / D m U L x 4 t 8 + T t 0 + 7 2 i 1 8 d H o x T S M U m n K i V N t z E + 1 n R G p G O Y 7 t T q o w I X R I + t g 2 K E i E y s + m g 4 6 d I + P 0 n D C W 5 g n t T N 2 f H R m J l B p F g a m M i B 6 o x W x i / p e 1 U x 2 e + R k T S a p R 0 N l H Y c o d H T u T r Z 0 e k 0 g 1 H x k g V D I z q 0 M H R B K q z W 1 s c w R v c e W / 0 C i X v J N S u e Y W K 5 c w U x 4 O 4 B C O w Y N T q M A 1 V K E O F B D u 4 R G e r F v r w X q 2 X m a l O W v e s w + / Z L 1 + A x O V k B s = &lt; / l a t e x i t &gt; H &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " O m n s B I c I 7 u L 9 x T 1 n n c l a c U z B 0 H k = " &gt; A A A B 6 H i c b V D L T g J B E O z F F + I L 9 e h l I j H x R H b R R I 9 E L x w h k U c C G z I 7 9 M L I 7 O x m Z t a E E L 7 A i w e N 8 e o n e f N v H G A P C l b S S a W q O 9 1 d Q S K 4 N q 7 7 7 e Q 2 N r e 2 d / K 7 h b 3 9 g 8 O j 4 v F J S 8 e p Y t h k s Y h V J 6 A a B Z f Y N N w I 7 C Q K a R Q I b A f j + 7 n f f k K l e S w f z C R B P 6 J D y U P O q L F S o 9 Y v l t y y u w B Z J 1 5 G S p C h 3 i 9 + 9 Q Y x S y O U h g m q d d d z E + N P q T K c C Z w V e q n G h L I x H W L X U k k j 1 P 5 0 c e i M X F h l Q M J Y 2 Z K G L N T f E 1 M a a T 2 J A t s Z U T P S q 9 5 c / M / r p i a 8 9 a d c J q l B y Z a L w l Q Q E 5 P 5 1 2 T A F T I j J p Z Q p r i 9 l b A R V Z Q Z m 0 3 B h u C t v r x O W p W y d 1 W u N K 5 L 1 b s s j j y c w T l c g g c 3 U I U a 1 K E J D B C e 4 R X e n E f n x X l 3 P p a t O S e b O Y U / c D 5 / A J 7 H j N A = &lt; / l a t e x i t &gt; S &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G a A j 9 C g g s i M S Y T 4 U 8 1 a v q C Q z m e k = " &gt; A A A B 6 H i c b Z C 7 S g N B F I b P x l t c b 1 F L m 8 E g W I X d W G g j B m 0 s E z Q X S J Y w O z m b j J m 9 M D M r h J A n s L F Q x F Y f x t 5 G f B s n l 0 I T f x j 4 + P 9 z m H O O n w i u t O N 8 W 5 m l 5 Z X V t e y 6 v b G 5 t b 2 T 2 9 2 r q T i V D K s s F r F s + F S h 4 B F W N d c C G 4 l E G v o C 6 3 7 / a p z X 7 1 E q H k e 3 e p C g F 9 J u x A P O q D Z W 5 a a d y z s F Z y K y C O 4 M 8 h c f 9 n n y / m W X 2 7 n P V i d m a Y i R Z o I q 1 X S d R H t D K j V n A k d 2 K 1 W Y U N a n X W w a j G i I y h t O B h 2 R I + N 0 S B B L 8 y J N J u 7 v j i E N l R q E v q k M q e 6 p + W x s / p c 1 U x 2 c e U M e J a n G i E 0 / C l J B d E z G W 5 M O l 8 i 0 G B i g T H I z K 2 E 9 K i n T 5 j a 2 O Y I 7 v / I i 1 I o F 9 6 R Q r D j 5 0 i V M l Y U D O I R j c O E U S n A N Z a g C A 4 Q H e I J n 6 8 5 6 t F 6 s 1 2 l p x p r 1 7 M M f W W 8 / D w m Q G A = = &lt; / l a t e x i t &gt; d a &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F X i n 9 o 2 1 0 j c H w 3 W z a l J R n 9 n g j A s = " &gt; A A A B 7 H i c b V A 9 S w N B E J 2 L X z F + R Q U b m 8 U g W I W 7 W G g Z Y m O Z g J c E k i P s 7 e 0 l S / b 2 j t 0 9 I R z 5 D T Y W i t j a + S / 8 B X Y 2 / h Y 3 l x S a + G D g 8 d 4 M M / P 8 h D O l b f v L K q y t b 2 x u F b d L O 7 t 7 + w f l w 6 O 2 i l N J q E t i H s u u j x X l T F B X M 8 1 p N 5 E U R z 6 n H X 9 8 M / M 7 9 1 Q q F o s 7 P U m o F + G h Y C E j W B v J D Q Y Z n g 7 K F b t q 5 0 C r x F m Q S v 2 k 9 c 3 e G x / N Q f m z H 8 Q k j a j Q h G O l e o 6 d a C / D U j P C 6 b T U T x V N M B n j I e 0 Z K n B E l Z f l x 0 7 R u V E C F M b S l N A o V 3 9 P Z D h S a h L 5 p j P C e q S W v Z n 4 n 9 d L d X j t Z U w k q a a C z B e F K U c 6 R r P P U c A k J Z p P D M F E M n M r I i M s M d E m n 5 I J w V l + e Z W 0 a 1 X n s l p r m T Q a M E c R T u E M L s C B K 6 j D L T T B B Q I M H u A J n i 1 h P V o v 1 u u 8 t W A t Z o 7 h D 6 y 3 H 9 q M k m k = &lt; / l a t e x i t &gt; 2 ⇤ u &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R s t V x p j s t V n W q B m 1 O C M Y K A 0 j j 9 Y = " &gt; A A A B 6 n i c b Z D L S g M x F I b P e K 3 j r e r S T b A I 4 q L M 1 I V u x K I b l x X t B d q h Z N J M G 5 r J h C Q j l K G P 4 M a F I i 7 1 X d y 7 E d / G 9 L L Q 1 h 8 C H / 9 / D j n n h J I z b T z v 2 1 l Y X F p e W c 2 t u e s b m 1 v b + Z 3 d m k 5 S R W i V J D x R j R B r y p m g V c M M p w 2 p K I 5 D T u t h / 2 q U 1 + + p 0 i w R d 2 Y g a R D j r m A R I 9 h Y 6 7 Z 0 n L b z B a / o j Y X m w Z 9 C 4 e L D P Z d v X 2 6 l n f 9 s d R K S x l Q Y w r H W T d + T J s i w M o x w O n R b q a Y S k z 7 u 0 q Z F g W O q g 2 w 8 6 h A d W q e D o k T Z J w w a u 7 8 7 M h x r P Y h D W x l j 0 9 O z 2 c j 8 L 2 u m J j o L M i Z k a q g g k 4 + i l C O T o N H e q M M U J Y Y P L G C i m J 0 V k R 5 W m B h 7 H d c e w Z 9 d e R 5 q p a J / U i z d e I X y J U y U g 3 0 4 g C P w 4 R T K c A 0 V q A K B L j z A E z w 7 3 H l 0 X p z X S e m C M + 3 Z g z 9 y 3 n 8 A G G i Q q g = = &lt; / l a t e x i t &gt; D &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f U n O Q a Z v k 7 h S S l u A P U O 2 o U i N i n A = " &gt; A A A B 6 H i c b Z C 7 S g N B F I b P x l t c b 1 F L m 8 E g W I X d W G g j B r W w T M B c I F n C 7 O R s M m b 2 w s y s E E K e w M Z C E V t 9 G H s b 8 W 2 c X A p N / G H g 4 / / P Y c 4 5 f i K 4 0 o 7 z b W W W l l d W 1 7 L r 9 s b m 1 v Z O b n e v p u J U M q y y W M S y 4 V O F g k d Y 1 V w L b C Q S a e g L r P v 9 q 3 F e v 0 e p e B z d 6 k G C X k i 7 E Q 8 4 o 9 p Y l e t 2 L u 8 U n I n I I r g z y F 9 8 2 O f J + 5 d d b u c + W 5 2 Y p S F G m g m q V N N 1 E u 0 N q d S c C R z Z r V R h Q l m f d r F p M K I h K m 8 4 G X R E j o z T I U E s z Y s 0 m b i / O 4 Y 0 V G o Q + q Y y p L q n 5 r O x + V / W T H V w 5 g 1 5 l K Q a I z b 9 K E g F 0 T E Z b 0 0 6 X C L T Y m C A M s n N r I T 1 q K R M m 9 v Y 5 g j u / M q L U C s W 3 J N C s e L k S 5 c w V R Y O 4 B C O w Y V T K M E N l K E K D B A e 4 A m e r T v r 0 X q x X q e l G W v W s w 9 / Z L 3 9 A P g + k A k = &lt; / l a t e x i t &gt; N &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P O W S q R 0 d 1 E a e C w y L H g w M N x E E C A 0 = " &gt; A A A B 6 H i c b Z C 7 S g N B F I b P x l t c b 1 F L m 8 E g W I X d W G g j B m 2 s J A F z g W Q J s 5 O z y Z j Z C z O z Q g h 5 A h s L R W z 1 Y e x t x L d x c i k 0 8 Y e B j / 8 / h z n n + I n g S j v O t 5 V Z W l 5 Z X c u u 2 x u b W 9 s 7 u d 2 9 m o p T y b D K Y h H L h k 8 V C h 5 h V X M t s J F I p K E v s O 7 3 r 8 Z 5 / R 6 l 4 n F 0 q w c J e i H t R j z g j G p j V W 7 a u b x T c C Y i i + D O I H / x Y Z 8 n 7 1 9 2 u Z 3 7 b H V i l o Y Y a S a o U k 3 X S b Q 3 p F J z J n B k t 1 K F C W V 9 2 s W m w Y i G q L z h Z N A R O T J O h w S x N C / S Z O L + 7 h j S U K l B 6 J v K k O q e m s / G 5 n 9 Z M 9 X B m T f k U Z J q j N j 0 o y A V R M d k v D X p c I l M i 4 E B y i Q 3 s x L W o 5 I y b W 5 j m y O 4 8 y s v Q q 1 Y c E 8 K x Y q T L 1 3 C V F k 4 g E M 4 B h d O o Q T X U I Y q M E B 4 g C d 4 t u 6 s R + v F e p 2 W Z q x Z z z 7 8 k f X 2 A w d 1 k B M = &lt; / l a t e x i t &gt; k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x 8 u O x Q M G s z h N T b i 4 D H L M y I L 4 z M 4 = " &gt; A A A B 6 H i c b Z C 7 S g N B F I b P x l t c b 1 F L m 8 E g W I X d W G g j B m 0 s E z A X S J Y w O z m b j J m 9 M D M r h C V P Y G O h i K 0 + j L 2 N + D Z O L o V G f x j 4 + P 9 z m H O O n w i u t O N 8 W b m l 5 Z X V t f y 6 v b G 5 t b 1 T 2 N 1 r q D i V D O s s F r F s + V S h 4 B H W N d c C W 4 l E G v o C m / 7 w a p I 3 7 1 A q H k c 3 e p S g F 9 J + x A P O q D Z W b d g t F J 2 S M x X 5 C + 4 c i h f v 9 n n y 9 m l X u 4 W P T i 9 m a Y i R Z o I q 1 X a d R H s Z l Z o z g W O 7 k y p M K B v S P r Y N R j R E 5 W X T Q c f k y D g 9 E s T S v E i T q f u z I 6 O h U q P Q N 5 U h 1 Q O 1 m E 3 M / 7 J 2 q o M z L + N R k m q M 2 O y j I B V E x 2 S y N e l x i U y L k Q H K J D e z E j a g k j J t b m O b I 7 i L K / + F R r n k n p T K N a d Y u Y S Z 8 n A A h 3 A M L p x C B a 6 h C n V g g H A P j / B k 3 V o P 1 r P 1 M i v N W f O e f f g l 6 / U b M 2 m Q M A = = &lt; / l a t e x i t &gt; S h</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n r l m s c a 9 7 c X D + e q / p l 8 r </p><formula xml:id="formula_11">3 o o e z Q I = " &gt; A A A C A X i c b V C 7 S g N B F J 3 1 G e N r 1 U a w G Q y C N m E 3 F l o G b S w j m g d k 1 z A 7 u Z s M m X 0 w M y v E N T Z + g K 0 f I I K F I r b + h Z 1 / 4 2 y S Q h M P X D h z z r 3 M v c e L O Z P K s r 6 N m d m 5 + Y X F 3 F J + e W V 1 b d 3 c 2 K z J K B E U q j T i k W h 4 R A J n I V Q V U x w a s Q A S e B z q X u 8 0 8 + v X I C S L w k v V j 8 E N S C d k P q N E a a l l b j s c f I V v M b 6 4 S r s D 7 A j W 6 W b v l l m w i t Y Q e J r Y Y 1 I o H z z c x L n n x 0 r L / H L a E U 0 C C B X l R M q m b c X K T Y l Q j H I Y 5 J 1 E Q k x o j 3 S g q W l I A p B u O r x g g P e 0 0 s Z + J H S F C g / V 3 x M p C a T s B 5 7 u D I j q y k k v E / / z m o n y j 9 2 U h X G i I K S j j / y E Y x X h L A 7 c Z g K o 4 n 1 N C B V M 7 4 p p l w h C l Q 4 t r 0 O w J 0 + e J r V S 0 T 4 s l s 5 1 G i d o h B z a Q b t o H 9 n o C J X R G a q g K q L o D j 2 h V / R m 3 B s v x r v x M W q d M c Y z W + g P j M 8 f K 0 W Z D Q = = &lt; / l a t e x i t &gt;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hierarchical Attention-based Recurrent Layer</head><p>After obtaining the unified representation of the text and the hierarchical category structure, we propose HARL, a recurrent architecture, to model the dependencies among different levels by leveraging the hierarchical structure gradually in a top-down fashion. At each category level, the core repeating unit, i.e., Hierarchical Attention-based Memory (HAM) unit is explicitly designed to capture the associations between texts and categories. Meanwhile, it can transfer the corresponding hierarchical semantic representation to the next layer. This hierarchical semantic representation is consistent with the human reading habit that people usually understand the concept of a document from shallow to deep. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, the "red" words in D pay more attention to physics concept in C 1 (level-1 category) while the words on "red" underline focus more on nuclear physics concept in C 2 . Obviously, nuclear physics is the subclass of physics. Therefore, it is necessary to qualify the contributions of the text semantic representation to the category hierarchy from top to down and learn the attention representations for each category level. Figure <ref type="figure" target="#fig_6">3</ref> shows the details of an HAM unit. An HAM unit mainly consists of three components, i.e., Text-Category Attention (TCA), Class Prediction Module (CPM) and Class Dependency Module (CDM). Specifically, TCA first captures the associations between texts and categories of the hierarchical structure. Based on the textcategory associations, CPM generates the unified representation and predictions of the corresponding category level. Next, CDM is utilized to model the dependencies among different levels of the hierarchical structure. For the h-th category level in γ , the input to corresponding HAM unit includes three parts, i.e., the whole text semantic representation V , the corresponding category level representation S h , and the information of previous level ω h-1 . The    unified representation A h L and information ω h at the h-th level are updated as following formulas:</p><formula xml:id="formula_12">TCA CDM CPM Avg V &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z 7 W M 4 r 9 Z 0 K z t G 5 O K w L i E I j + q g W 4 = " &gt; A A A B 6 H i c b Z C 7 S g N B F I b P x l t c b 1 F L m 8 U g W I X d W G g j B m 0 s E z A X S J Y w O z m b j J m d X W Z m h b D k C W w s F L H V h 7 G 3 E d / G y a X Q 6 A 8 D H / 9 / D n P O C R L O l H b d L y u 3 t L y y u p Z f t z c 2 t 7 Z 3 C r t 7 D R W n k m K d x j y W r Y A o 5 E x g X T P N s Z V I J F H A s R k M r y Z 5 8 w 6 l Y r G 4 0 a M E / Y j 0 B Q s Z J d p Y t U a 3 U H R L 7 l T O X / D m U L x 4 t 8 + T t 0 + 7 2 i 1 8 d H o x T S M U m n K i V N t z E + 1 n R G p G O Y 7 t T q o w I X R I + t g 2 K E i E y s + m g 4 6 d I + P 0 n D C W 5 g n t T N 2 f H R m J l B p F g a m M i B 6 o x W x i / p e 1 U x 2 e + R k T S a p R 0 N l H Y c o d H T u T r Z 0 e k 0 g 1 H x k g V D I z q 0 M H R B K q z W 1 s c w R v c e W / 0 C i X v J N S u e Y W K 5 c w U x 4 O 4 B C O w Y N T q M A 1 V K E O F B D u 4 R G e r F v r w X q 2 X m a l O W v e s w + / Z L 1 + A x O V k B s = &lt; / l a t e x i t &gt; ! h 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / k L D y O I 0 z 7 B n 7 J m j t m r t f j c J A 0 4 = " &gt; A A A B 8 3 i c b V C 7 S g N B F J 3 1 G d d X 1 N J m M A g 2 h t 1 Y a C M G b S w j m A c k a 5 i d z C Z D 5 s X M r B C W / I a N h a K 2 f o e 9 j f g 3 T h 6 F J h 6 4 c D j n X u 6 9 J 1 a M G h s E 3 9 7 C 4 t L y y m p u z V / f 2 N z a z u / s 1 o x M N S Z V L J n U j R g Z w q g g V U s t I w 2 l C e I x I / W 4 f z X y 6 / d E G y r F r R 0 o E n H U F T S h G F k n t V q S k y 6 6 y 3 r H 4 b C d L w T F Y A w 4 T 8 I p K V x 8 + O f q 9 c u v t P O f r Y 7 E K S f C Y o a M a Y a B s l G G t K W Y k a H f S g 1 R C P d R l z Q d F Y g T E 2 X j m 4 f w 0 C k d m E j t S l g 4 V n 9 P Z I g b M + C x 6 + T I 9 s y s N x L / 8 5 q p T c 6 i j A q V W i L w Z F G S M m g l H A U A O 1 Q T b N n A E Y Q 1 d b d C 3 E M a Y e t i 8 l 0 I 4 e z L 8 6 R W K o Y n x d J N U C h f g g l y Y B 8 c g C M Q g l N Q B t e g A q o A A w U e w B N 4 9 l L v 0 X v x 3 i a t C 9 5 0 Z g / 8 g f f + A x V W l L Q = &lt; / l a t e x i t &gt; ! h &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 C B C U 5 z P r 7 o v d c S s + D s s 3 c H s A P s = " &gt; A A A B 8 X i c b Z D L S g M x F I b P e K 3 j r e r S T b A I r s p M X e h G L L p x W c F e s B 1 L J s 2 0 o U l m S D J C G f o W b l w o o k s f x L 0 b 8 W 1 M L w t t / S H w 8 f / n k H N O m H C m j e d 9 O w u L S 8 s r q 7 k 1 d 3 1 j c 2 s 7 v 7 N b 0 3 G q C K 2 S m M e q E W J N O Z O 0 a p j h t J E o i k X I a T 3 s X 4 7 y + j 1 V m s X y x g w S G g j c l S x i B B t r 3 b Z i Q b v 4 L u s N 2 / m C V / T G Q v P g T 6 F w / u G e J W 9 f b q W d / 2 x 1 Y p I K K g 3 h W O u m 7 y U m y L A y j H A 6 d F u p p g k m f d y l T Y s S C 6 q D b D z x E B 1 a p 4 O i W N k n D R q 7 v z s y L L Q e i N B W C m x 6 e j Y b m f 9 l z d R E p 0 H G Z J I a K s n k o y j l y M R o t D 7 q M E W J 4 Q M L m C h m Z 0 W k h x U m x h 7 J t U f w Z 1 e e h 1 q p 6 B 8 X S 9 d e o X w B E + V g H w 7 g C H w 4 g T J c Q Q W q Q E D C A z z B s 6 O d R + f F e Z 2 U L j j T n j 3 4 I + f 9 B z U K l E I = &lt; / l a t e x i t &gt; P h L &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " y n C N z A t V i C d j A X F F Y 7 z M O e 2 P U V g = " &gt; A A A B 8 H i c b V C 7 S g N B F L 0 b X z G + o p Y 2 Q 4 K Q K u z G I p Z B G w u L C O Y h y R p m J 7 P J k N n Z Z W Z W C M t + h Y 2 g I r b + i a 2 d 6 M c 4 e R S a e O D C 4 Z x 7 u f c e L + J M a d v + t D I r q 2 v r G 9 n N 3 N b 2 z u 5 e f v + g q c J Y E t o g I Q 9 l 2 8 O K c i Z o Q z P N a T u S F A c e p y 1 v d D 7 x W 3 d U K h a K a z 2 O q B v g g W A + I 1 g b 6 a b e S y 7 T 2 2 S Y 9 v J F u 2 x P g Z a J M y f F W q H 0 / V V 9 f 6 z 3 8 h / d f k j i g A p N O F a q 4 9 i R d h M s N S O c p r l u r G i E y Q g P a M d Q g Q O q 3 G R 6 c I q O j d J H f i h N C Y 2 m 6 u + J B A d K j Q P P d A Z Y D 9 W i N x H / 8 z q x 9 k / d h I k o 1 l S Q 2 S I / 5 k i H a P I 9 6 j N J i e Z j Q z C R z N y K y B B L T L T J K G d C c B Z f X i b N S t k 5 K V e u T B p n M E M W j q A A J X C g C j W 4 g D o 0 g E A A 9 / A E z 5 a 0 H q w X 6 3 X W m r H m M 4 f w B 9 b b D y 0 y l G E = &lt; / l a t e x i t &gt; S h &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d p y k 1 F y Y 5 h 3 R q u W 1 3 J 8 V 6 2 N i p H s = " &gt; A A A B 7 H i c b Z C 7 S g N B F I b P x l t c b 1 F L m 8 E g W I X d W M R G D N p Y R n S T Q L K G 2 c l s M m R 2 d p m Z F c K S Z 7 C x U M R K 8 F X s b c S 3 c X I p N P G H g Y / / P 4 c 5 5 w Q J Z 0 o 7 z r e V W 1 p e W V 3 L r 9 s b m 1 v b O 4 X d v b q K U 0 m o R 2 I e y 2 a A F e V M U E 8 z z W k z k R R H A a e N Y H A 5 z h v 3 V C o W i 1 s 9 T K g f 4 Z 5 g I S N Y G 8 u 7 u c v 6 o 0 6 h 6 J S c i d A i u D M o n n / Y Z 8 n b l 1 3 r F D 7 b 3 Z i k E R W a c K x U y 3 U S 7 W d Y a k Y 4 H d n t V N E E k w H u 0 Z Z B g S O q / G w y 7 A g d G a e L w l i a J z S a u L 8 7 M h w p N Y w C U x l h 3 V f z 2 d j 8 L 2 u l O j z 1 M y a S V F N B p h + F K U c 6 R u P N U Z d J S j Q f G s B E M j M r I n 0 s M d H m P r Y 5 g j u / 8 i L U y y X 3 p F S + d o r V C 5 g q D w d w C M f g Q g W q c A U</formula><formula xml:id="formula_13">I R l K H n B K w E i u 6 q c E I L t N R 1 m / W L a r 9 g x 4 l T g L U m 6 U K t 9 f 9 f f H Z r / 4 0 R t E N A m Z B C q I 1 q 5 j x + C l R A G n g m W F X q J Z T O i Y D J l r q C Q h 0 1 4 6 O z n D p 0 Y Z 4 C B S p i T g m f p 7 I i W h 1 p P Q N 5 0 h g Z F e 9 q b i f 5 6 b Q H D u p V z G C T B J 5 4 u C R G C I 8 P R / P O C K U R A T Q w h V 3 N y K 6 Y g o Q s G k V D A h O M s v r 5 J 2 r e q c V W v X J o 0 L N E c e n a A S q i A H 1 V E D X</formula><formula xml:id="formula_14">n x / 1 d 8 f m / 3 i R 2 8 Q 0 S R k E q g g W r u O H Y O X E g W c C p Y V e o l m M a F j M m S u o Z K E T H v p 7 O Q M n x p l g I N I m Z K A Z + r v i Z S E W k 9 C 3 3 S G B E Z 6 2 Z u K / 3 l u A s G 5 l 3 I Z J 8 A k n S 8 K E o E h w t P / 8 Y A r R k F M D C F U c X M r p i O i C A W T U s G E 4 C y / v E r a t a p z V q 1 d m z Q u 0 B x 5 d I J K q I I c V E c N d I W a q I U o i t A 9 e k L P F l g P 1 o v 1 O m / N W Y u Z Y / Q H 1 t s P C g u V e Q = = &lt; / l a t e x i t &gt; P h L &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " y n C N z A t V i C d j A X F F Y 7 z M O e 2 P U V g = " &gt; A A A B 8 H i c b V C 7 S g N B F L 0 b X z G + o p Y 2 Q 4 K Q K u z G I p Z B G w u L C O Y h y R p m J 7 P J k N n Z Z W Z W C M t + h Y</formula><formula xml:id="formula_15">f F W q H 0 / V V 9 f 6 z 3 8 h / d f k j i g A p N O F a q 4 9 i R d h M s N S O c p r l u r G i E y Q g P a M d Q g Q O q 3 G R 6 c I q O j d J H f i h N C Y 2 m 6 u + J B A d K j Q P P d A Z Y D 9 W i N x H / 8 z q x 9 k / d h I k o 1 l S Q 2 S I / 5 k i H a P I 9 6 j N J i e Z j Q z C R z N y K y B B L T L T J K G d C c B Z f X i b N S t k 5 K V e u T B p n M E M W j q A A J X C g C j W 4 g D o 0 g E A A 9 / A E z 5 a 0 H q w X 6 3 X W m</formula><formula xml:id="formula_16">V = avд(V ), r h at t ,W h at t = TCA(ω h-1 , V , S h ), P h L , A h L = CPM([ V ⊕ r h at t ]), ω h = CDM(W h at t , P h L ),<label>(3)</label></formula><p>where V is the whole text semantic representation, r h at t is the associated text-category representation of h-th level, W h at t is the textcategory attention matrix at h-th level, P h L denotes the predicted probability vector of h-th level, ⊕ denotes vector concatenation operation and avg() is the average pooling operation. Note that all elements in ω 0 are initialized as 1.0 since the level 0 is the root of the hierarchical structure which contains no extra information.</p><p>Next, we introduce each component of the HAM unit.</p><p>Text-Category Attention targets at capturing the associations among texts and categories, and outputs the associated text-category representation r h at t and text-category attention matrix W h at t at hth category level. As shown in Figure <ref type="figure" target="#fig_8">4</ref>, given the information of previous category level ω h-1 ∈ R N ×2u , we integrate it with the whole text semantic representation V ∈ R N ×2u to generate the V h :</p><formula xml:id="formula_17">V h = ω h-1 ⊗ V ,<label>(4)</label></formula><p>where V h ∈ R N ×2u denotes the representation which introduces the hierarchical information of previous category level, and ⊗ denotes entry-wise product operation.</p><p>Inspired by <ref type="bibr" target="#b19">[20]</ref>, the document text is usually formed by multiple components which focus on different aspects, especially for the long sentences. For instance, in Figure <ref type="figure" target="#fig_0">1</ref>, the "red" words of document D focus more on physics while the "blue" words concentrate more on chemistry. Thus, for the h-th category level, we need |C h | aspects for focusing on different categories to represent the overall semantic of the whole sentence. We use</p><formula xml:id="formula_18">S h ∈ R |C h |×d a which is the embedding of h-th category level to perform |C h | different categories of attention. Formally, O h = tanh(W h s • V T h ), W h at t = so f tmax(S h • O h ),<label>(5)</label></formula><p>where W h s ∈ R d a ×2u is a randomly initialized weight matrix, O h denotes the activations which can be viewed as one MLP without bias, d a is a hyperparameter we can set arbitrarily, the softmax() ensures all the computed weights sum up to 1 for each category. And</p><formula xml:id="formula_19">W h at t = (W h 1 ,W h 2 , . . . ,W h |C h | ) ∈ R |C h |×N is the annotation matrix,</formula><p>where W h i ∈ R N represents the attention score of the text with i-th category in h-th level after normalization and each element in this vector represents the contribution of each word token on its corresponding position contributes to this i-th category.</p><p>Then, we compute |C h | weighted sums by multiplying the annotation matrix W h at t and the text semantic representation V h . The resulting matrix M h ∈ R |C h |×2u is the associated text-category representation with each category in h-th level. And r h at t ∈ R 2u which is the associated text-category representation for whole h-th level can be modeled as a vector by averaging M h in category-dims:</p><formula xml:id="formula_20">M h = W h at t • V h , r h at t = avд(M h ).<label>(6)</label></formula><p>With the help of Text-Category Attention, we can get the associated text-category representation r h at t and the annotation matrix W h at t for the h-th category level. Class Prediction Module aims at generating the unified representation and predicting the categories for each level by integrating the original text semantic representation and the associated textcategory representation which introduces the information of its previous level. Formally, let A h L denote the representation of h-th level and A h L is given by:  </p><formula xml:id="formula_21">A h L = φ(W h T • [ V ⊕ r h at t ] + b h T ),<label>(7)</label></formula><formula xml:id="formula_22">L f g = " &gt; A A A B 6 X i c b Z D L S g M x F I b P 1 F s d b 1 W X b o J F c F V m 6 k I 3 Y t G N y y r 2 A u 1 Q M m m m D c 0 k Q 5 I R y t A 3 c O N C E b d 9 G P d u x L c x v S y 0 9 Y f A x / + f Q 8 4 5 Y c K Z N p 7 3 7 e R W V t f W N / K b 7 t b 2 z u 5 e Y f + g r m W q C K 0 R y a V q h l h T z g S t G W Y 4 b S a K 4 j j k t B E O b i Z 5 4 5 E q z a R 4 M M O E B j H u C R Y x g o 2 1 7 s t p p 1 D 0 S t 5 U a B n 8 O R S v P t z L Z P z l V j u F z 3 Z X k j S m w h C O t W 7 5 X m K C D C v D C K c j t 5 1 q m m A y w D 3 a s i h w T H W Q T S c d o R P r d F E k l X 3 C o K n 7 u y P D s d b D O L S V M T Z 9 v Z h N z P + y V m q i i y B j I k k N F W T 2 U Z R y Z C S a r I 2 6 T F F i + N A C J o r Z W R H p Y 4 W J s c d x 7 R H 8 x Z W X o V 4 u + W e l 8 p 1 X r F z D T H k 4 g m M 4 B R / O o Q K 3 U I U a E I j g C V 7 g 1 R k 4 z 8 6 b 8 z 4 r z T n z n k P 4 I 2 f 8 A 7 N v k H Y = &lt; / l a t e x i t &gt; 2u &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 d i 7 B N J F K j x 8 Z S P Q 2 z + S 0 p o k L f g = " &gt; A A A B 6 X i c b Z D L S g M x F I b P 1 F s d b 1 W X b o J F c F V m 6 k I 3 Y t G N y y r 2 A u 1 Q M m m m D c 0 k Q 5 I R y t A 3 c O N C E b d 9 G P d u x L c x v S y 0 9 Y f A x / + f Q 8 4 5 Y c K Z N p 7 3 7 e R W V t f W N / K b 7 t b 2 z u 5 e Y f + g r m W q C K 0 R y a V q h l h T z g S t G W Y 4 b S a K 4 j j k t B E O b i Z 5 4 5 E q z a R 4 M M O E B j H u C R Y x g o 2 1 7 s t p p 1 D 0 S t 5 U a B n 8 O R S v P t z L Z P z l V j u F z 3 Z X k j S m w h C O t W 7 5 X m K C D C v D C K c j t 5 1 q m m A y w D 3 a s i h w T H W Q T S c d o R P r d F E k l X 3 C o K n 7 u y P D s d b D O L S V M T Z 9 v Z h N z P + y V m q i i y B j I k k N F W T 2 U Z R y Z C S a r I 2 6 T F F i + N A C J o r Z W R H p Y 4 W J s c d x 7 R H 8 x Z W X o V 4 u + W e l 8 p 1 X r F z D T H k 4 g m M 4 B R / O o Q K 3 U I U a E I j g C V 7 g 1 R k 4 z</formula><formula xml:id="formula_23">K Y h H L h k 8 V C h 5 h V X M t s J F I p K E v s O 7 3 r 8 Z 5 / R 6 l 4 n F 0 q w c J e i H t R j z g j G p j V W 7 a u b x T c C Y i i + D O I H / x Y Z 8 n 7 1 9 2 u Z 3 7 b H V i l o Y Y a S a o U k 3 X S b Q 3 p F J z J n B k t 1 K F C W V 9 2 s W m w Y i G q L z h Z N A R O T J O h w S x N C / S Z O L + 7 h j S U K l B 6 J v K k O q e m s / G 5 n 9 Z M 9 X B m T f k U Z J q j N j 0 o y A V R M d k v D X p c I l M i 4 E B y i Q 3 s x L W o 5 I y b W 5 j m y O 4 8 y s v Q q 1 Y c E 8 K x Y q T L 1 3 C V F k 4 g E M 4 B h d O o Q T X U I Y q M E B 4 g C d 4 t u 6 s R + v F e p 2 W Z q x Z z z 7 8 k f X 2 A w d 1 k B M = &lt; / l a t e x i t &gt; V &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z 7 W M 4 r 9 Z 0 K z t G 5 O K w L i E I j + q g W 4 = " &gt; A A A B 6 H i c b Z C 7 S g N B F I b P x l t c b 1 F L m 8 U g W I X d W G g j B m 0 s E z A X S J Y w O z m b j J m d X W Z m h b D k C W w s F L H V h 7 G 3 E d / G y a X Q 6 A 8 D H / 9 / D n P O C R L O l H b d L y u 3 t L y y u p Z f t z c 2 t 7 Z 3 C r t 7 D R W n k m K d x j y W r Y A o 5 E x g X T P N s Z V I J F H A s R k M r y Z 5 8 w 6 l Y r G 4 0 a M E / Y j 0 B Q s Z J d p Y t U a 3 U H R L 7 l T O X / D m U L x 4 t 8 + T t 0 + 7 2 i 1 8 d H o x T S M U m n K i V N t z E + 1 n R G p G O Y 7 t T q o w I X R I + t g 2 K E i E y s + m g 4 6 d I + P 0 n D C W 5 g n t T N 2 f H R m J l B p F g a m M i B 6 o x W x i / p e 1 U x 2 e + R k T S a p R 0 N l H Y c o d H T u T r Z 0 e k 0 g 1 H x k g V D I z q 0 M H R B K q z W 1 s c w R v c e W / 0 C i X v J N S u e Y W K 5 c w U x 4 O 4 B C O w Y N T q M A 1 V K E O F B D u 4 R G e r F v r w X q 2 X m a l O W v e s w + / Z L 1 + A x O V k B s = &lt; / l a t e x i t &gt; ! h 1</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / k L D y O I 0 z 7 B n 7 J m j t m r t f j c J A 0   </p><formula xml:id="formula_24">4 = " &gt; A A A B 8 3 i c b V C 7 S g N B F J 3 1 G d d X 1 N J m M A g 2 h t 1 Y a C M G b S w j m A c k a 5 i d z C Z D 5 s X M r B C W / I a N h a K 2 f o e 9 j f g 3 T h 6 F J h 6 4 c D j n X u 6 9 J 1 a M G h s E 3 9 7 C 4 t L y y m p u z V / f 2 N z a z u / s 1 o x M N S Z V L J n U j R g Z w q g g V U s t I w 2 l C e I x I / W 4 f z X y 6 / d E G y r F r R 0 o E n H U F T S h G F k n t V q S k y 6 6 y 3 r H 4 b C d L w T F Y A w 4 T 8 I p K V x 8 + O f q 9 c u v t P O f r Y 7 E K S f C Y o a M a Y a B s l G G t K W Y k a H f S g 1 R C P d R l z Q d F Y g T E 2 X j m 4 f w 0 C k d m E j t S l g 4 V n 9 P Z I g b M + C x 6 + T I 9 s y s N x L / 8 5 q p T c 6 i j A q V W i L w Z F G S M m g l H A U A O 1 Q T b N n A E Y Q 1 d b d C 3 E M a Y e t i 8 l 0 I 4 e z L 8 6 R W K o Y n x d J N U C h f g g l y Y B 8 c g C M Q g l N Q B t e g A q o A A w U e w B N 4 9 l L v 0 X v x 3 i a t C 9 5 0 Z g / 8 g f f + A x V W l L Q = &lt; /</formula><formula xml:id="formula_25">K Y h H L h k 8 V C h 5 h V X M t s J F I p K E v s O 7 3 r 8 Z 5 / R 6 l 4 n F 0 q w c J e i H t R j z g j G p j V W 7 a u b x T c C Y i i + D O I H / x Y Z 8 n 7 1 9 2 u Z 3 7 b H V i l o Y Y a S a o U k 3 X S b Q 3 p F J z J n B k t 1 K F C W V 9 2 s W m w Y i G q L z h Z N A R O T J O h w S x N C / S Z O L + 7 h j S U K l B 6 J v K k O q e m s / G 5 n 9 Z M 9 X B m T f k U Z J q j N j 0 o y A V R M d k v D X p c I l M i 4 E B y i Q 3 s x L W o 5 I y b W 5 j m y O 4 8 y s v Q q 1 Y c E 8 K x Y q T L 1 3 C V F k 4 g E M 4 B h d O o Q T X U I Y q M E B</formula><formula xml:id="formula_26">K Y h H L h k 8 V C h 5 h V X M t s J F I p K E v s O 7 3 r 8 Z 5 / R 6 l 4 n F 0 q w c J e i H t R j z g j G p j V W 7 a u b x T c C Y i i + D O I H / x Y Z 8 n 7 1 9 2 u Z 3 7 b H V i l o Y Y a S a o U k 3 X S b Q 3 p F J z J n B k t 1 K F C W V 9 2 s W m w Y i G q L z h Z N A R O T J O h w S x N C / S Z O L + 7 h j S U K l B 6 J v K k O q e m s / G 5 n 9 Z M 9 X B m T f k U Z J q j N j 0 o y A V R M d k v D X p c I l M i 4 E B y i Q 3 s x L W o 5 I y b W 5 j m y O 4 8 y s v Q q 1 Y c E 8 K x Y q T L 1 3 C V F k 4 g E M 4 B h d O o Q T X U I Y q M E B 4 g C d 4 t u 6 s R + v F e p 2 W Z q x Z z z 7 8 k f X 2 A w d 1 k B M = &lt; / l a t e x i t &gt; 2u &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 d i 7 B N J F K j x 8 Z S P Q 2 z + S 0 p o k L f g = " &gt; A A A B 6 X i c b Z D L S g M x F I b P 1 F s d b 1 W X b o J F c F V m 6 k I 3 Y t G N y y r 2 A u 1 Q M m m m D c 0 k Q 5 I R y t A 3 c O N C E b d 9 G P d u x L c x v S y 0 9 Y f A x / + f Q 8 4 5 Y c K Z N p 7 3 7 e R W V t f W N / K b 7 t b 2 z u 5 e Y f + g r m W q C K 0 R y a V q h l h T z g S t G W Y 4 b S a K 4 j j k t B E O b i Z 5 4 5 E q z a R 4 M M O E B j H u C R Y x g o 2 1 7 s t p p 1 D 0 S t 5 U a B n 8 O R S v P t z L Z P z l V j u F z 3 Z X k j S m w h C O t W 7 5 X m K C D C v D C K c j t 5 1 q m m A y w D 3 a s i h w T H W Q T S c d o R P r d F E k l X 3 C o K n 7 u y P D s d b D O L S V M T Z 9 v Z h N z P + y V m q i i y B j I k k N F W T 2 U Z R y Z C S a r I 2 6 T F F i + N A C J o r Z W R H p Y 4 W J s c d x 7 R H 8 x Z W X o V 4 u + W e l 8 p 1 X r F z D T H k 4 g m M 4 B R / O o Q K 3 U I U a E I j g C V 7 g 1 R k 4 z 8 6 b 8 z 4 r z T n z n k P 4 I 2 f 8 A 7 N v k H Y = &lt; / l a t e x i t &gt; r h att &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W 8 7 D M 6 K d F J r 0 8 M q W k i 4 u d H s x U s 4 = " &gt; A A A B 8 n i c b V D L S s N A F J 3 U V 6 2 v q k s 3 Q 4 v Q V U n q o i</formula><formula xml:id="formula_27">I R l K H n B K w E i u 6 q c E I L t N R 1 m / W L a r 9 g x 4 l T g L U m 6 U K t 9 f 9 f f H Z r / 4 0 R t E N A m Z B C q I 1 q 5 j x + C l R A G n g m W F X q J Z T O i Y D J l r q C Q h 0 1 4 6 O z n D p 0 Y Z 4 C B S p i T g m f p 7 I i W h 1 p P Q N 5 0 h g Z F e 9 q b i f 5 6 b Q H D u p V z G C T B J 5 4 u C R G C I 8 P R / P O C K U R A T Q w h V 3 N y K 6 Y g o Q s G k V D A h O M s v r 5 J 2 r e q c V W v X J o 0 L N E c e n a A S q i A H 1 V E D X a E m a i G K I n S P n t C z B d a D 9 W K 9 z l t z 1 m L m G P 2 B 9 f Y D N A W V l A = = &lt; / l a t e x i t &gt; V h</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 M b U q 3 E q V 6 6 q 2 C b K T y H 4 z r w I 8   </p><formula xml:id="formula_28">2 g = " &gt; A A A B 7 H i c b V C 7 S g N B F L 0 T X z G + o o K N z W A Q r M J u L L Q M s b F M w E 0 C y R J m J 7 P J k N n Z Z W Z W C E u + w c Z C E V s 7 / 8 I v s L P x W 5 w 8 C k 0 8 c O F w z r 3 c e 0 + Q C K 6 N 4 3 y h 3 N r 6 x u Z W f r u w s 7 u 3 f 1 A 8 P G r q O F W U e T Q W s W o H R D P B J f M M N 4 K 1 E 8 V I F A j W C k Y 3 U 7 9 1 z 5 T m s b w z 4 4 T 5 E R l I H n J K j J W 8 Z i 8 b T n r F k l N 2 Z s C r x F 2 Q U v W k 8 c 3 f a x / 1 X v G z 2 4 9 p G j F p q C B a d 1 w n M X 5 G l O F U s E m h m 2 q W E D o i A 9 a x V J K I a T + b H T v B 5 1 b p 4 z B W t q T B M / X 3 R E Y i r c d R Y D s j Y o Z 6 2 Z u K / 3 m d 1 I T X f s Z l k h o m 6 X x R m A p s Y j z</formula><formula xml:id="formula_29">m K h c d p 2 x t f T v 3 2 H Z W K h U F D T y L q C j w M m M 8 I 1 k a 6 a f W T U X q b N N J + v m i X 7 R n Q K n E W p F g r l L 6 / q u + P 9 X 7 + o z c I S S x o o A n H S n U d O 9 J u g q V m h N M 0 1 4 s V j T A Z 4 y H t G h p g Q Z W b z A 5 O 0 a l R B s g P p a l A o 5 n 6 e y L B Q q m J 8 E y n w H q k l r 2 p + J / X j b V / 7 i Y s i G J N A z J f 5 M c c 6 R B N v 0 c D J i n R f G I I J p K Z W x E Z Y Y m J N h n l T A j O 8 s u r p F U p O 2 f l y r V J 4 w L m y M I J F K A E D l S h B l d Q h y Y Q E H A P T / B s S e v B e</formula><formula xml:id="formula_30">L S F V 9 m b j h A R M J b c C 8 = " &gt; A A A B 8 n i c b V D L S s N A F J 3 U V 6 2 v q k s 3 Q 4 v Q V U n q o i</formula><formula xml:id="formula_31">I R l K H n B K w E h u p 5 8 S g O w 2 H W X 9 Y t m u 2 j P g V e I s S L l R q n x / 1 d 8 f m / 3 i R 2 8 Q 0 S R k E q g g W r u O H Y O X E g W c C p Y V e o l m M a F j M m S u o Z K E T H v p 7 O Q M n x p l g I N I m Z K A Z + r v i Z S E W k 9 C 3 3 S G B E Z 6 2 Z u K / 3 l u A s G 5 l 3 I Z J 8 A k n S 8 K E o E h w t P / 8 Y A r R k F M D C F U c X M r p i O i C A W T U s G E 4 C y / v E r a t a p z V q 1 d m z Q u 0 B x 5 d I J K q I I c V E c N d I W a q I U o i t A 9 e k L P F l g P 1 o v 1 O m / N W Y u Z Y / Q H 1 t s P C g u V e Q = = &lt; / l a t e x i t &gt; M h &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E b 2 f X 0 3 d T f S 1 q N d A R W S A c I m s 6 S A = " &gt; A A A B 7 H i c b V C 7 S g N B F L 0 b X z G + o o K N z W A Q r M J u L L Q M s b E R E n C T Q F z C 7 G Q 2 G T I 7 s 8 z M C m H J N 9 h Y K G J r 5 1 / 4 B X Y 2 f o u T R 6 G J B y 4 c z r m X e + 8 J E 8 6 0 c d 0 v J 7 e y u r a + k d 8 s b G 3 v 7 O 4 V 9 w + a W q a K U J 9 I L l U 7 x J p y J q h v m O G 0 n S i K 4 5 D T V j i 8 m v i t e 6 o 0 k + L W j B I a x L g v W M Q I N l b y b 7 r Z Y N w t l t y y O w V a J t 6 c l K p H j W / 2 X v u o d 4 u f d z 1 J 0 p g K Q z j W u u O 5 i Q k y r A w j n I 4 L d 6 m m C S Z D 3 K c d S w W O q Q 6 y 6 b F j d G q V H o q k s i U M m q q / J z I c a z 2 K Q 9 s Z Y z P Q i 9 5 E / M / r p C a 6 D D I m k t R Q Q W a L o p Q j I 9 H k c 9 R j i h L D R 5 Z g o p i 9 F Z E B V p g Y m 0 / B h u A t v r x M m p W y d 1 6 u N G w a N Z g h D 8 d w A m f g w Q V U 4 R r q 4 A M B B g / w B M + O c B 6 d F + d 1 1 p p z 5 j O H 8 A f O 2 w / B 9 5 J Z &lt; / l a t e x i t &gt; 1</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E u L u S 3 r s / h 5 7 2 p J u a J 5 w / 9 C T 5 w w  </p><formula xml:id="formula_32">= " &gt; A A A B 6 X i c b Z D L S s N A F I Z P 6 q 3 G W 9 W l m 8 E i u C p J X e h G L L p x W c V e o A 1 l M p 2 0 Q y e T M D M R S u g b u H G h i N s + j H s 3 4 t s 4 S b v Q 1 h 8 G P v 7 / H O a c 4 8 e c K e 0 4 3 1 Z h Z X V t f a O 4 a W 9 t 7 + z u l f Y P m i p K J K E N E v F I t n 2 s K G e C N j T T n L Z j S X H o c 9 r y R z d Z 3 n q k U r F I P O h x T L 0 Q D w Q L G M H a W P e u 3 S u V n Y q T C y 2 D O 4 f y 1 Y d 9 G U + / 7 H q v 9 N n t R y Q J q d C E Y 6 U 6 r h N r L 8 V S M 8 L p x O 4 m i s a Y j P C A d g w K H F L l p f m k E 3 R i n D 4 K I m m e 0 C h 3 f 3 e k O F R q H P q m M s R 6 q B a z z P w v 6 y Q 6 u P B S J u J E U 0 F m H w U J R z p C 2 d q o z y Q l m o 8 N Y C K Z m R W R I Z a Y a H O c 7 A j u 4 s r L 0 K x W 3 L N K 9 c 4 p 1 6 5 h p i I c w T G c g g v n U I N b q E M D C A T w B C / w a o 2 s Z + v N e p + V F q x 5 z y H 8 k T X 9 A Q + + k A o = &lt; / l a t e x i t &gt; 2u &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 d i 7 B N J F K j x 8 Z S P Q 2 z + S 0 p o k L f g = " &gt; A A A B 6 X i c b Z D L S g M x F I b P 1 F s d b 1 W X b o J F c F V m 6 k I 3 Y t G N y y r 2 A u 1 Q M m m m D c 0 k Q 5 I R y t A 3 c O N C E b d 9 G P d u x L c x v S y 0 9 Y f A x / + f Q 8 4 5 Y c K Z N p 7 3 7 e R W V t f W N / K b 7 t b 2 z u 5 e Y f + g r m W q C K 0 R y a V q h l h T z g S t G W Y 4 b S a K 4 j j k t B E O b i Z 5 4 5 E q z a R 4 M M O E B j H u C R Y x g o 2 1 7 s t p p 1 D 0 S t 5 U a B n 8 O R S v P t z L Z P z l V j u F z 3 Z X k j S m w h C O t W 7 5 X m K C D C v D C K c j t 5 1 q m m A y w D 3 a s i h w T H W Q T S c d o R P r d F E k l X 3 C o K n 7 u y P D s d b D O L S V M T Z 9 v Z h N z P + y V m q i i y B j I k k N F W T 2 U Z R y Z C S a r I 2 6 T F F i + N A C J o r Z W R H p Y 4 W J s c d x 7 R H 8 x Z W X o V 4 u + W e l 8 p 1 X r F z D T H k 4 g m M 4 B R / O o Q K 3 U I U a E I j g C V 7 g 1 R k 4 z</formula><formula xml:id="formula_33">L f g = " &gt; A A A B 6 X i c b Z D L S g M x F I b P 1 F s d b 1 W X b o J F c F V m 6 k I 3 Y t G N y y r 2 A u 1 Q M m m m D c 0 k Q 5 I R y t A 3 c O N C E b d 9 G P d u x L c x v S y 0 9 Y f A x / + f Q 8 4 5 Y c K Z N p 7 3 7 e R W V t f W N / K b 7 t b 2 z u 5 e Y f + g r m W q C K 0 R y a V q h l h T z g S t G W Y 4 b S a K 4 j j k t B E O b i Z 5 4 5 E q z a R 4 M M O E B j H u C R Y x g o 2 1 7 s t p p 1 D 0 S t 5 U a B n 8 O R S v P t z L Z P z l V j u F z 3 Z X k j S m w h C O t W 7 5 X m K C D C v D C K c j t 5 1 q m m A y w D 3 a s i h w T H W Q T S c d o R P r d F E k l X 3 C o K n 7 u y P D s d b D O L S V M T Z 9 v Z h N z P + y V m q i i y B j I k k N F W T 2 U Z R y Z C S a r I 2 6 T F F i + N A C J o r Z W R H p Y 4 W J s c d x 7 R H 8 x Z W X o V 4 u + W e l 8 p 1 X r F z D T H k 4 g m M 4 B R / O o Q K 3 U I U a E I j g C V 7 g 1 R k 4 z</formula><formula xml:id="formula_34">f v L K q y t b 2 x u F b d L O 7 t 7 + w f l w 6 O 2 i l N J q E t i H s u u j x X l T F B X M 8 1 p N 5 E U R z 6 n H X 9 8 M / M 7 9 1 Q q F o s 7 P U m o F + G h Y C E j W B v J D Q Y Z n g 7 K F b t q 5 0 C r x F m Q S v 2 k 9 c 3 e G x / N Q f m z H 8 Q k j a j Q h G O l e o 6 d a C / D U j P C 6 b T U T x V N M B n j I e 0 Z K n B E l Z f l x 0 7 R u V E C F M b S l N A o V 3 9 P Z D h S a h L 5 p j P C e q S W v Z n 4 n 9 d L d X j t Z U w k q a a C z B e F K U c 6 R r P P U c A k J Z p P D M F E M n M r I i M s M d E m n 5 I J w V l + e Z W 0 a 1 X n s l p r m T Q a M E c R T u E M L s C B K 6 j D L T T B B Q I M H u A J n i 1 h P V o v 1 u u 8 t W A t Z o 7 h D 6 y 3 H 9 q M k m k = &lt; / l a t e x i t &gt; d a &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F X i n 9 o 2 1 0 j c H w 3 W z a l J R n 9 n g j A s = " &gt; A A A B 7 H i c b V A 9 S w N B E J 2 L X z F + R Q U b m 8 U g W I W 7 W G g Z Y m O Z g J c E k i P s 7 e 0 l S / b 2 j t 0 9 I R z 5 D T Y W i t j a + S / 8 B X Y 2 / h Y 3 l x S a + G D g 8 d 4 M M / P 8 h D O l b f v L K q y t b 2 x u F b d L O 7 t 7 + w f l w 6 O 2 i l N J q E t i H s u u j x X l T F B X M 8 1 p N 5 E U R z 6 n H X 9 8 M / M 7 9 1 Q q F o s 7 P U m o F + G h Y C E j W B v J D Q Y Z n g 7 K F b t q 5 0 C r x F m Q S v 2 k 9 c 3 e G x / N Q f m z H 8 Q k j a j Q h G O l e o 6 d a C / D U j P C 6 b T U T x V N M B n j I e 0 Z K n B E l Z f l x 0 7 R u V E C F M b S l N A o V 3 9 P Z D h S a h L 5 p j P C e q S W v Z n 4 n 9 d L d X j t Z U w k q a a C z B e F K U c 6 R r P P U c A k J Z p P D M F E M n M r I i M s M d E m n 5 I J w V l + e Z W 0 a 1 X n s l p r m T Q a M E c R T u E M L s C B K 6 j D L T T B B Q I M H u A J n i 1 h P V o v 1 u u 8 t W A t Z o 7 h D 6 y 3 H 9 q M k m k = &lt; / l a t e x i t &gt; d a &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F X i n 9 o 2 1 0 j c H w 3 W z a l J R n 9 n g j A s = " &gt; A A A B 7 H i c b V A 9 S w N B E J 2 L X z F + R Q U b m 8 U g W I W 7 W G g Z Y m O Z g J c E k i P s 7 e 0 l S / b 2 j t 0 9 I R z 5 D T Y W i t j a + S / 8 B X Y 2 / h Y 3 l x S a + G D g 8 d 4 M M / P 8 h D O l b f v L K q y t b 2 x u F b d L O 7 t 7 + w f l w 6 O 2 i l N J q E t i H s u u j x X l T F B X M 8 1 p N 5 E U R z 6 n H X 9 8 M / M 7 9 1 Q q F o s 7 P U m o F + G h Y C E j W B v J D Q Y Z n g 7 K F b t q 5 0 C r x F m Q S v 2 k 9 c 3 e G x / N Q f m z H 8 Q k j a j Q h G O l e o 6 d a C / D U j P C 6 b T U T x V N M B n j I e 0 Z K n B E l Z f l x 0 7 R u V E C F M b S l N A o V 3 9 P Z D h S a h L 5 p j P C e q S W v Z n 4 n 9 d L d X j t Z U w k q a a C z B e F K U c 6 R r P P U c A k J Z p P D M F E M n M r I i M s M d E m n 5 I J w V l + e Z W 0 a 1 X n s l p r m T Q a M E c R T u E M L s C B K 6 j D L T T B B Q I M H u A J n i 1 h P V o v 1 u u 8 t W A t Z o 7 h D 6 y 3 H 9 q M k m k = &lt; / l a t e x i t &gt; S h &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d p y k 1 F y Y 5 h 3 R q u W 1 3 J 8 V 6 2 N i p H s = " &gt; A A A B 7 H i c b Z C 7 S g N B F I b P x l t c b 1 F L m 8 E g W I X d W M R G D N p Y R n S T Q L K G 2 c l s M m R 2 d p m Z F c K S Z 7 C x U M R K 8 F X s b c S 3 c X I p N P G H g Y / / P 4 c 5 5 w Q J Z 0 o 7 z r e V W 1 p e W V 3 L r 9 s b m 1 v b O 4 X d v b q K U 0 m o R 2 I e y 2 a A F e V M U E 8 z z W k z k R R H A a e N Y H A 5 z h v 3 V C o W i 1 s 9 T K g f 4 Z 5 g I S N Y G 8 u 7 u c v 6 o 0 6 h 6 J S c i d A i u D M o n n / Y Z 8 n b l 1 3 r F D 7 b 3 Z i k E R W a c K x U y 3 U S 7 W d Y a k Y 4 H d n t V N E E k w H u 0 Z Z B g S O q / G w y 7 A g d G a e L w l i a J z S a u L 8 7 M h w p N Y w C U x l h 3 V f z 2 d j 8 L 2 u l O j z 1 M y a S V F N B p h + F K U c 6 R u P N U Z d J S j Q f G s B E M j M r I n 0 s M d H m P r Y 5 g j u / 8 i L U y y X 3 p F S + d o r V C 5 g q D w d w C M f g Q g W q c A U 1 8 I A A g w d 4 g m d L W I / W i / U 6 L c 1 Z s 5 5 9 + C P r / Q d K j p H + &lt; / l a t e x i t &gt; O h &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " s Q 0 J r T z x D u G m 5 P X W E z 1 2 F k r d S N M = " &gt; A A A B 7 H i c b V C 7 S g N B F L 0 b X z G + o o K N z W A Q r M J u L L Q M s b E z A T c J x C X M T m a T I b M z y 8 y s E J Z 8 g 4 2 F I r Z 2 / o V f Y G f j t z h 5 F J p 4 4 M L h n H u 5 9 5 4 w 4 U w b 1 / 1 y c i u r a + s b + c 3 C 1 v b O 7 l 5 x / 6 C p Z a o I 9 Y n k U r V D r C l n g v q G G U 7 b i a I 4 D j l t h c O r i d + 6 p 0 o z K W 7 N K K F B j P u C R Y x g Y y X / p p s N x t 1 i y S 2 7 U 6 B l 4 s 1 J q X r U + G b v t Y 9 6 t / h 5 1 5 M k j a k w h G O t O 5 6 b m C D D y j D C 6 b h w l 2 q a Y D L E f d q x V O C Y 6 i C b H j t G p 1 b p o U g q W 8 K g q f p 7 I s O x 1 q M 4 t J 0 x N g O 9 6 E 3 E / 7 x O a q L L I G M i S Q 0 V Z L Y o S j k y E k 0 + R z 2 m K D F 8 Z A k m i t l b E R l g h Y m x + R R s C N 7 i y 8 u k W S l 7 5 + V K w 6 Z R g x n y c A w n c A Y e X E A V r q E O P h B g 8 A B P 8 O w I 5 9 F 5 c V 5 n r T l n P n M I f + C 8 / Q D F B 5 J b &lt; / l a t e x i t &gt; W h s &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L r V w a d 3 s t C + X Y 6 n 8 d R z y D O 7 9 W w k = " &gt; A A A B 8 H i c b V D L S g N B E O z 1 G e M r 6 t H L k C D k F H b j I R 6 D X j x G M A 9 J Y p i d z C Z D Z m a X m V k h L P s V X g Q V 8 e q f e P U m + j F O H g d N L G g o q r r p 7 v I j z r R x 3 U 9 n Z X V t f W M z s 5 X d 3 t n d 2 8 8 d H D Z 0 G C t C 6 y T k o W r 5 W F P O J K 0 b Z j h t R Y p i 4 X P a 9 E c X E 7 9 5 R 5 V m o b w 2 4 4 h 2 B R 5 I F j C C j Z V u m r 1 E p 7 f J M O 3 l C m 7 J n Q I t E 2 9 O C t V 8 8 f u r 8 v 5 Y 6 + U + O v 2 Q x I J K Q z j W u u 2 5 k e k m W B l G O E 2 z n V j T C J M R H t C 2 p R I L q r v J 9 O A U n V i l j 4 J Q 2 Z I G T d X f E w k W W o + F b z s F N k O 9 6 E 3 E / 7 x 2 b I K z b s J k F B s q y W x R E H N k Q j T 5 H v W Z o s T w s S W Y K G Z v R W S I F S b G Z p S 1 I X i L L y + T R r n k n Z b K V z a N c 5 g h A 8 e Q h y J 4 U I E q X E I N 6 k B A w D 0 8 w b O j n A f n x X m d t a 4 4 8 5 k j + A P n 7 Q d z 5 Z S P &lt; / l a t e x i t &gt; W h att &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " y 3 8 c 8 m a / G y L S F V 9 m b j h A R M J b c C 8 = " &gt; A A A B 8 n i c b V D L S s N A F J 3 U V 6 2 v q k s 3 Q 4 v Q V U n q o i 6 L b l x W s A 9 I Y 5 l M J + 3 Q y S T M 3 A g l 5 D P c K C j i 1 h 9 x 6 0 7 0 Y 5 w + F t p 6 4 M L h n H u 5 9 x 4 / F l y D b X 9 a u b X 1 j c 2 t / H Z h Z 3 d v / 6 B 4 e N T W U a I o a 9 F I R K r r E 8 0 E l 6 w F H A T r x o q R 0 B e s 4 4 8 v p 3 7 n j i n N I 3 k D k 5 h 5 I R l K H n B K w E h u p 5 8 S g O w 2 H W X 9 Y t m u 2 j P g V e I s S L l R q n x / 1 d 8 f m / 3 i R 2 8 Q 0 S R k E q g g W r u O H Y O X E g W c C p Y V e o l m M a F j M m S u o Z K E T H v p 7 O Q M n x p l g I N I m Z K A Z + r v i Z S E W k 9 C 3 3 S G B E Z 6 2 Z u K / 3 l u A s G 5 l 3 I Z J 8 A k n S 8 K E o E h w t P / 8 Y A r R k F M D C F U c X M r p i O i C A W T U s G E 4 C y / v E r a t a p z V q 1 d m z Q u 0 B x 5 d I J K q I I c V E c N d I W a q I U o i t A 9 e k L P F l g P 1 o v 1 O m / N W Y u Z Y / Q H 1 t s P C g u V e Q = = &lt; / l a t e x i t &gt; C h &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F V a S I 6 U R f 3 O 6 j 1 q U / k w P q J j 5 9 y Y = " &gt; A A A C A X i c b V C 7 S g N B F L 3 r M 8 b X q o 1 g M x g E b c J u L L Q M p r G M Y B 6 Q X c P s Z D Y Z M v t g Z l a I a 2 z 8 A F s / Q A Q L R W z 9 C z v / x t k k h S Y e u H D m n H u Z e 4 8 X c y a V Z X 0 b c / M L i 0 v L u Z X 8 6 t r 6 x q a 5 t V 2 X U S I I r Z G I R 6 L p Y U k 5 C 2 l N M c V p M x Y U B x 6 n D a 9 f y f z G N R W S R e G l G s T U D X A 3 Z D 4 j W G m p b e 4 6 n P o K 3 S J U u U p 7 Q + Q I 1 u 1 l 7 7 Z Z s I r W C G i W 2 B N S K B 8 9 3 M S 5 5 8 d q 2 / x y O h F J A h o q w r G U L d u K l Z t i o R j h d J h 3 E k l j T P q 4 S 1 u a h j i g 0 k 1 H F w z R g V Y 6 y I + E r l C h k f p 7 I s W B l I P A 0 5 0 B V j 0 5 7 W X i f 1 4 r U f 6 p m 7 I w T h Q N y f g j P + F I R S i L A 3 W Y o E T x g S a Y C K Z 3 R a S H B S Z K h 5 b X I d j T J 8 + S e q l o H x d L F z q N M x g j B 3 u w D 4 d g w w m U 4 R y q U A M C d / A E r / B m 3 B s v x r v x M W 6 d M y Y z O / A H x u c P E i W Y / Q = = &lt; / l a t e x i t &gt; C h &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F V a S I 6 U R f 3 O 6 j 1 q U / k w P q J j 5 9 y Y = " &gt; A A A C A X i c b V C 7 S g N B F L 3 r M 8 b X q o 1 g M x g E b c J u L L Q M p r G M Y B 6 Q X c P s Z D Y Z M v t g Z l a I a 2 z 8 A F s / Q A Q L R W z 9 C z v / x t k k h S Y e u H D m n H u Z e 4 8 X c y a V Z X 0 b c / M L i 0 v L u Z X 8 6 t r 6 x q a 5 t V 2 X U S I I r Z G I R 6 L p Y U k 5 C 2 l N M c V p M x Y U B x 6 n D a 9 f y f z G N R W S R e G l G s T U D X A 3 Z D 4 j W G m p b e 4 6 n P o K 3 S J U u U p 7 Q + Q I 1 u 1 l 7 7 Z Z s I r W C G i W 2 B N S K B 8 9 3 M S 5 5 8 d q 2 / x y O h F J A h o q w r G U L d u K l Z t i o R j h d J h 3 E k l j T P q 4 S 1 u a h j i g 0 k 1 H F w z R g V Y 6 y I + E r l C h k f p 7 I s W B l I P A 0 5 0 B V j 0 5 7 W X i f 1 4 r U f 6 p m 7 I w T h Q N y f g j P + F I R S i L A 3 W Y o E T x g S a Y C K Z 3 R a S H B S Z K h 5 b X I d j T J 8 + S e q l o H x d L F z q N M x g j B 3 u w D 4 d g w w m U 4 R y q U A M C d / A E r / B m 3 B s v x r v x M W 6 d M y Y z O / A H x u c P E i W Y / Q = = &lt; / l a t e x i t &gt; C h</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F V a S I 6 U R f 3 O 6 j 1 q U / k w P q J j 5 9 y Y = "  where W h T ∈ R v×4u is a randomly initialized weight matrix and b h T ∈ R v×1 is the corresponding bias vector, φ is a non-linear activation function (e.g., RELU). Then, the local prediction of h-th category level P h L is calculated by:</p><formula xml:id="formula_35">&gt; A A A C A X i c b V C 7 S g N B F L 3 r M 8 b X q o 1 g M x g E b c J u L L Q M p r G M Y B 6 Q X c P s Z D Y Z M v t g Z l a I a 2 z 8 A F s / Q A Q L R W z 9 C z v / x t k k h S Y</formula><formula xml:id="formula_36">K Y h H L h k 8 V C h 5 h V X M t s J F I p K E v s O 7 3 r 8 Z 5 / R 6 l 4 n F 0 q w c J e i H t R j z g j G p j V W 7 a u b x T c C Y i i + D O I H / x Y Z 8 n 7 1 9 2 u Z 3 7 b H V i l o Y Y a S a o U k 3 X S b Q 3 p F J z J n B k t 1 K F C W V 9 2 s W m w Y i G q L z h Z N A R O T J O h w S x N C / S Z O L + 7 h j S U K l B 6 J v K k O q e m s / G 5 n 9 Z M 9 X B m T f k U Z J q j N j 0 o y A V R M d k v D X p c I l M i 4 E B y i Q 3 s x L W o 5 I y b W 5 j m y O 4 8 y s v Q q 1 Y c E 8 K x Y q T L 1 3 C V F k 4 g E M 4 B h d O o Q T X U I Y q M E B 4 g C d 4 t u 6 s R + v F e p 2 W Z q x Z z z 7 8 k f X 2 A w d 1 k B M = &lt; / l a t e x i t &gt;</formula><formula xml:id="formula_37">P h L = σ (W h L • A h L + b h L ),<label>(8)</label></formula><p>where W h L ∈ R |C h |×v is the weighted matrix that connects the activations of h-th level with |C h | output units, b h L ∈ R |C h |×1 is the corresponding bias vector, and σ is the sigmoid activation.</p><p>Class Dependency Module is utilized to model the dependencies between different levels of the hierarchical structure by keeping the hierarchical information for each level. For the h-th category level, different categories have different contributions to the prediction, which can be used as the trade-off parameter for revising text-category attention matrix. Therefore, we first utilize an entrywise product operation to combine the predicted category values P h L and the text-category attention matrix W h at t to generate the weighted text-category attention matrix K h :</p><formula xml:id="formula_38">K h = broadcast(P h L ) ⊗ W h at t ,<label>(9)</label></formula><p>where</p><formula xml:id="formula_39">K h = (k h 1 , . . . , k h |C h | ) ∈ R |C h |×N</formula><p>is the weighted textcategory attention matrix of h-th level considering the different categories should have the different weight, k h i denotes weighted attention score of the i-th category with the text semantic representation in h-th level, and broadcast() is the process of making matrixes with different shapes have compatible shapes for arithmetic operations (i.e., entry-wise product).</p><p>Then we exploit the category-wise average pooling operation to merge |C h | categories into an average representation K h :</p><formula xml:id="formula_40">K h = avд(K h ),<label>(10)</label></formula><p>where K h ∈ R N is the weighted attention vector of h-th level which holds the attention associations of the whole category level with the text. Next, we broadcast the average representation into ω h :     </p><formula xml:id="formula_41">C Q K 0 Z B T A 0 h V H F z K 6 Z j o g g F E 1 f Z h O A u v 7 x K O v W a e 1 G r 3 z W q z e s i j h I 6 Q a f o H L n o E j X R L W q h N q L o C T 2 j V / R</formula><formula xml:id="formula_42">C Q K 0 Z B T A 0 h V H F z K 6 Z j o g g F E 1 f Z h O A u v 7 x K O v W a e 1 G r 3 z W q z e s i j h I 6 Q a f o H L n o E j X R L W q h N q L o C T 2 j V / R</formula><formula xml:id="formula_43">ω h = broadcast( K h ),<label>(11)</label></formula><p>where</p><formula xml:id="formula_44">ω h = (ω h 1 , ω h 2 , . . . , ω h N ) ∈ R N ×2u</formula><p>is a matrix holding the hierarchy information and the associations between the text and the whole category level at h-th level, and ω h i ∈ R 2u measures the association weights between the whole previous category level and the i-th word token in D.</p><p>Finally, we bring ω h to the next category level since the information of each category level not only is influenced by the previous level but also will affect the next level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Hybrid Predicting Layer</head><p>Throughout HARL, we can obtain the unified representation A h L and the predictions P h L of each category level. It is important to predict all categories in the entire hierarchy, and predicting the categories of each level in HMTC problem is also critical. Thus, we apply a Hybrid Predicting Layer, a hybrid prediction method to generate the final predictions by considering both local and global prediction information. Specifically, we note all the A h L as</p><formula xml:id="formula_45">A L = A 1 L , A 2 L , . . . , A H L ∈ R H ×v ,</formula><p>and then exploit the level-wise average pooling operation to merge H levels representations into a global embedding A L = avд(A 1 L , A 2 L , . . . , A H L ) ∈ R v . Let P G denote the global predictions of the entire category hierarchy and P G is given by:</p><formula xml:id="formula_46">A G = φ(W G • A L + b G ), P G = σ (W M • A G + b M ),<label>(12)</label></formula><p>where W G ∈ R v×v , W M ∈ R K ×v are randomly initialized weight matries, b G ∈ R v×1 and b M ∈ R K ×1 are corresponding bias vectors. P G is a continuous vector and each element in this vector P i G denotes the probability P(c i |x) for c i ∈ C.</p><p>In order to integrate both local and global predictions, the final predictions P F is calculated by:</p><formula xml:id="formula_47">P F = (1 -α) • (P 1 L ⊕ P 2 L ⊕, . . . , P H L ) + α • P G ,<label>(13)</label></formula><p>where α ∈ [0, 1] is a balance parameter, which is used to regulate the trade-off regarding the local and global predictions. We set α = 0.5 as the default option in order to give equal importance to both local and global predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Loss Function</head><p>In this subsection, we specify a loss function which is the sum of the local and global loss functions for each document to train HARNN. In the training stage, for each document D, the local (L L ) and global (L G ) loss are calculated as: </p><formula xml:id="formula_48">L L = H h=1 [ε(P h L , Y h L )], L G = ε(P G , Y G ),<label>(14)</label></formula><p>where Y G is the binary label vector (expected output) containing all categories of the hierarchical structure and Y h L is the binary label vector (expected output) containing only the categories of the h-th level. Since categories are not mutually exclusive, we use ε(•, •) the binary cross-entropy to minimize the local and global loss function of a document. The final loss function we optimize is thus given by:</p><formula xml:id="formula_49">L(Θ) = L L + L G + λ Θ ||Θ|| 2 ,<label>(15)</label></formula><p>where Θ denotes all parameters of HARNN and λ Θ is the regularization hyperparameter. In this way, we can learn HARNN by directly minimizing the loss function L(Θ) using Adam <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we first introduce the datasets and our experimental setups. Then, we conduct extensive experiments on HARNN model and the other state-of-the-art methods to answer the following questions:</p><p>• RQ1: How does HARNN perform in predicting total categories of the entire hierarchical structure compared to the state-of-the-art methods? • RQ2: How does HARNN perform in predicting categories of each level in the hierarchical structure compared to the other hierarchical approaches? • RQ3: How does the trade-off coefficient (α) between local and global predictions influences the performance ? • RQ4: Is the proposed hierarchical attention mechanism helpful and explanatory in HTMC problem? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset Description</head><p>We conduct experiments on two real-world datasets: educational exercises dataset and patent documents dataset, which contain document text records and hierarchical category structures. Education Dataset is collected from an online education system which provides a series of exercise-based applications for high school students in China. The dataset contains 240,309 real-world math exercises and each math exercise contains multiple categories which are organized into a tree structure with three levels.</p><p>Patent Dataset is collected from the USPTO 1 which is a patent system grating U.S. patents. The dataset includes 100,000 real-world granted US patents and each patent document includes textual information (e.g., title, abstract) and multiple hierarchical categories. Figure <ref type="figure" target="#fig_0">1</ref> shows a toy example of a patent document with a four-level hierarchical category structure.</p><p>Table <ref type="table" target="#tab_1">1</ref> and Figure <ref type="figure" target="#fig_23">5</ref> show the basic statistics and the documentation distribution of the two datasets respectively, and we can observe that: (1) Education dataset has total 459 categories which are organized into a three-level hierarchical structure while Patent dataset has total 9,162 categories with a four-level hierarchical structure; (2) Each document consists of about 3.64 and 7.67 categories in Education and Patent dataset respectively. These categories are distributed in different levels, and each level basically averages one or two categories; (3) About 95% documents of Education dataset contain fewer than 350 words, while 95% documents of Patent dataset contain fewer than 150 words.</p><p>The average number of category per instance is quite small compared to the total number of category in the whole hierarchical structure. Thus, when predicting the multiple categories for each document, it is critical to integrate the dependencies between different levels of the hierarchical structure to reduce the influence of the other irrelevant category information. Moreover, it is also necessary to capture the associations between texts and the hierarchical structure, especially for the long sentences, which has been fully discussed in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Setup</head><p>For validating the effectiveness of HARNN, we employ 10-fold cross-validation on labeled documents in two datasets, where one 1 https://www.uspto.gov/ of ten folds is targeted to construct the testing set and the rest for the training set.</p><p>Word Embedding Pre-training. We use the Word2vec tool <ref type="bibr" target="#b22">[23]</ref> with a dimension (k) 100 to generate the word embedding for each word in a document, which is trained on texts of documents in the datasets.</p><p>HARNN Setting. <ref type="foot" target="#foot_0">2</ref> We set the size (d a ) of embedding representations for categories as 200, the dimension (u) of hidden states in Bi-LSTM as 256, the dimension (v) of the all full-connected layer units as 512 and the parameter (α) of local &amp; global information trade-off regulation as 0.5.</p><p>Training Details. In HARNN, we set the maximum length N of words in a document as 350 in Education dataset while 150 in Patent dataset (zero padded when necessary) according to our observation in Figure <ref type="figure" target="#fig_23">5</ref>. We initialize parameters in HARNN with a truncated normal distribution and the standard deviation is set as 0.1. For training HARNN, we use the Adam optimizer <ref type="bibr" target="#b18">[19]</ref> with a learning rate of 1 × 10 -3 , and set the mini-batches as 256, λ Θ = 0.00004 , β = 0.1 in Eq. <ref type="bibr" target="#b14">(15)</ref>. All parameters of HARNN can be tuned during the training process. Note that all the fully-connected layers comprise 512 ReLU neurons. We also use dropout <ref type="bibr" target="#b28">[29]</ref> with a droprate of 0.5 to prevent overfitting and gradient clipping to avoid the gradient explosion problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Baseline Approaches</head><p>In order to demonstrate the effectiveness of HARNN, we compare it with several HMTC methods including four state-of-the-art models and some variants of HARNN:</p><p>• Clus-HMC <ref type="bibr" target="#b30">[31]</ref>: Clus-HMC is a global approach that builds a single decision tree to classify all categories simultaneously. • HMC-LMLP <ref type="bibr" target="#b6">[7]</ref>: HMC-LMLP is a local approach which trains a set of neural networks and each is responsible for the prediction of the categories belonging to a given level. • HMCN-F <ref type="bibr" target="#b32">[33]</ref>: HMCN-F is a feed-forward hybrid approach which combines the prediction of each level in the category hierarchy and the overall hierarchy structure. • HMCN-R <ref type="bibr" target="#b32">[33]</ref>: HMCN-R is a recurrent hybrid approach which combines the prediction of each level in the category hierarchy and the overall hierarchy structure. of the local regions in the category hierarchy and the overall hierarchical structure. (4) HMCN-F and HMCN-R are generally better than HMC-LMLP and Clus-HMC. This once again implies that it is effective for the HTMC task by integrating the predictions of each level in the category hierarchy and the overall hierarchical structure. In summary, these evidences all indicate that the dependencies among different levels of the hierarchical structure and textcategory associations are important for classifying documents in HMTC task. Moreover, these clues also reveal that HARNN can classify the documents into the multiple categories more effectively by integrating the predictions of each level in the category hierarchy and the overall hierarchical structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Performance on different levels (RQ2).</head><p>In HMTC task, it is important to predict all categories in the entire hierarchy, and annotating categories of each level precisely is also critical. Thus, we conduct an experiment for comparing HARNN with HMC-LMLP, HMCN-F and HMCN-R on each level of the hierarchical structure separately. From Figure <ref type="figure">6</ref>, we can see that HARNN outperforms all the other methods on all the category levels of the hierarchical structure, since HARNN leverages the information of hierarchical structure. Moreover, we also notice that the performance of all models tend to decrease when the hierarchy deepens, and HARNN retains superior performance compared to the other baselines. That is, as the hierarchical level increases, the categories of the level increase rapidly (e.g., the Patent dataset has 661, 8364 categories in level-3 and level-4 respectively, as shown in Table <ref type="table" target="#tab_1">1</ref>), which influences the model performance a lot. And HARNN considers the associations between texts and the hierarchical structure and dependencies among different levels of the hierarchical structure while the other three baselines do not. These results once again indicate that HARNN is more effective and powerful for HMTC task by considering both the associations between texts and the hierarchical structure and modeling the dependencies between different levels of the hierarchical structure. the importance of the local prediction and global prediction in <ref type="bibr" target="#b12">(13)</ref>. When α is smaller, the network tends to prioritize local predictions during learning. Conversely, as α is larger, the network is allowed to focus on global prediction more easily. We conduct an experiment by assigning different α from set {0, 0.1, 0.2, . . . , 1.0}. As shown in Figure <ref type="figure" target="#fig_24">7</ref>, as α increases, the performance of HARNN increases at the beginning, but it decreases afterwards both in two datasets. These results indicate that combining the local and global predictions properly is vital for achieving more accurate classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.4">Hierarchical Attention Visualization (RQ4</head><p>). An important characteristic of HARNN is that it can capture hierarchical attention information between text semantic representations and the hierarchical structure through visual attention vector K h in Eq. <ref type="bibr" target="#b9">(10)</ref> for each category level. The heatmap of the learned attention weights for a patent document is illustrated in Figure <ref type="figure">8</ref>, where the document sample is the one in Figure <ref type="figure" target="#fig_0">1</ref>. Please note that, we only show the part of the document text for illustration purposes. The color in Figure <ref type="figure">8</ref> changes from white to red while the value of attention weights increases. From Figure <ref type="figure">8</ref>, we find that the HARNN model majorly learns to capture some key words in the text. For instance, the attention weights of the words "nuclear" and "plant" are obviously higher than the other words since these words dominate the prediction of the category Physics in Level-1. And we also observe that the attention weights of the words "nuclear" and "reactor" gradually increase when the hierarchy deepens, and meanwhile, the attention weights of the words "plant" and "injection" decrease. That is, the key words in different levels might be different due to the different concept description. Specifically, the words "nuclear" and "reactor" are more appropriate for describing the concepts of subsequent levels (e.g., the Nuclear Reactors concept in level-3), while the words "plant" and "injection" seem less relevant. That once again indicates that HARNN is capable of understanding the concept of a document text from shallow to deep properly. These results imply that HARNN can provide a good way to capture the hierarchical attention information in a given document via the HAM unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS AND FUTURE WORK</head><p>In this paper, we proposed a model named HARNN for classifying documents into the most relevant categories level by level via combing the text and hierarchical structure. Specifically, we first applied a Documentation Representing Layer for obtaining the semantic encodings of text and categories. Then, we devised a Hierarchical Attention-based Recurrent Layer to model the dependencies between different levels by capturing the associations among text and hierarchical structure in a top-down fashion. After that, we designed a hybrid approach which is capable of predicting categories of each level while classifying the all categories in the entire hierarchy precisely. Finally, extensive experiments on two real-world datasets clearly demonstrated the effectiveness and explanatory power of HARNN.</p><p>In the future, there are still some directions for exploration. First, we would like to leverage the explicit constraints of the hierarchical structure (e.g., the hierarchical violation <ref type="bibr" target="#b27">[28]</ref>) as regularization to improve classification performance. Second, we would like to address the issue of incomplete labels <ref type="bibr" target="#b34">[35]</ref> and use more effective embedding for representing the hierarchical structure <ref type="bibr" target="#b20">[21]</ref>. Third, as HAM is LSTM-like structure for encoding the category hierarchy information, so we will also try to design Bi-HAM (like Bi-LSTM) to capture the better hierarchy information from upward and downward simultaneously with our HARNN. Finally, as our HARNN is a general framework, we will test its performance on the other domains, such as the protein function prediction in biochemistry <ref type="bibr" target="#b6">[7]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A toy example of a patent document in HTMC problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>g 8 A B P 8 O w I 5 9 F 5 c V 5 n r T l n P n M I f + C 8 / Q C U a p I 7 &lt; / l a t e x i t &gt; P F &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l e b C M 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The HARNN framework takes a given document text D and the corresponding hierarchical category structure γ as inputs, and outputs the hierarchical categories L for the given document.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>1 8 I A A g w d 4 g m d L W I / W i / U 6 L c 1 Z s 5 5 9 + C P r / Q d K j p H + &lt; / l a t e x i t &gt; r h att &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W 8 7 D M 6 K d F J r 0 8 M q W k i 4 u d H s x U s 4 = " &gt; A A A B 8 n i c b V D L S s N A F J 3 U V 6 2 v q k s 3 Q 4 v Q V U n q o i 6 L b l x W s A 9 I Y 5 l M J + 3 Q y S T M 3 A g l 5 D P c K C j i 1 h 9 x 6 0 7 0 Y 5 w + F t p 6 4 M L h n H u 5 9 x 4 / F l y D b X 9 a u b X 1 j c 2 t / H Z h Z 3 d v / 6 B 4 e N T W U a I o a 9 F I R K r r E 8 0 E l 6 w F H A T r x o q R 0 B e s 4 4 8 v p 3 7 n j i n N I 3 k D k 5 h 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>a E m a i G K I n S P n t C z B d a D 9 W K 9 z l t z 1 m L m G P 2 B 9 f Y D N A W V l A = = &lt; / l a t e x i t &gt; W h att &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " y 3 8 c 8 m a / G y L S F V 9 m b j h A R M J b c C 8 = " &gt; A A A B 8 n i c b V D L S s N A F J 3 U V 6 2 v q k s 3 Q 4 v Q V U n q o i 6 L b l x W s A 9 I Y 5 l M J + 3 Q y S T M 3 A g l 5 D P c K C j i 1 h 9 x 6 0 7 0 Y 5 w + F t p 6 4 M L h n H u 5 9 x 4 / F l y D b X 9 a u b X 1 j c 2 t / H Z h Z 3 d v / 6 B 4 e N T W U a I o a 9 F I R K r r E 8 0 E l 6 w F H A T r x o q R 0 B e s 4 4 8 v p 3 7 n j i n N I 3 k D k 5 h 5 I R l K H n B K w E h u p 5 8 S g O w 2 H W X 9 Y t m u 2 j P g V e I s S L l R q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>2 g I r b + i a 2 d 6 M c 4 e R S a e O D C 4 Z x 7 u f c e L + J M a d v + t D I r q 2 v r G 9 n N 3 N b 2 z u 5 e f v + g q c J Y E t o g I Q 9 l 2 8 O K c i Z o Q z P N a T u S F A c e p y 1 v d D 7 x W 3 d U K h a K a z 2 O q B v g g W A + I 1 g b 6 a b e S y 7 T 2 2 S Y 9 v J F u 2 x P g Z a J M y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Hierarchical attention-based memory unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " 2 d i 7 B N J F K j x 8 Z S P Q 2 z + S 0 p o k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>8 6 b 8 z 4</head><label>4</label><figDesc>r z T n z n k P 4 I 2 f 8 A 7 N v k H Y = &lt; / l a t e x i t &gt; N &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "P O W S q R 0 d 1 E a e C w y L H g w M N x E E C A 0 = " &gt; A A A B 6 H i c b Z C 7 S g N B F I b P x l t c b 1 F L m 8 E g W I X d W G g j B m 2 s J A F z g W Q J s 5 O z y Z j Z C z O z Q g h 5 A h s L R W z 1 Y e x t xL d x c i k 0 8 Y e B j / 8 / h z n n + I n g S j v O t 5 V Z W l 5 Z X c u u 2 x u b W 9 s 7 u d 2 9 m o p T y b D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>l a t e x i t &gt; N &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P O W S q R 0 d 1 E a e C w y L H g wM N x E E C A 0 = " &gt; A A A B 6 H i c b Z C 7 S g N B F I b P x l t c b 1 F L m 8 E g W I X d W G g j B m 2 s J A F z g W Q J s 5 O z y Z j Z C z O z Q g h 5 A h s L R W z 1 Y e x t xL d x c i k 0 8 Y e B j / 8 / h z n n + I n g S j v O t 5 V Z W l 5 Z X c u u 2 x u b W 9 s 7 u d 2 9 m o p T y b D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>4 g C d 4 t u 6 s R + v F e p 2 W Z q x Z z z 7 8 k f X 2 A w d 1 k B M = &lt; / l a t e x i t &gt; N &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "P O W S q R 0 d 1 E a e C w y L H g w M N x E E C A 0 = " &gt; A A A B 6 H i c b Z C 7 S g N B F I b P x l t c b 1 F L m 8 E g W I X d W G g j B m 2 s J A F z g W Q J s 5 O z y Z j Z C z O z Q g h 5 A h s L R W z 1 Y e x tx L d x c i k 0 8 Y e B j / 8 / h z n n + I n g S j v O t 5 V Z W l 5 Z X c u u 2 x u b W 9 s 7 u d 2 9 m o p T y b D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>6 L b l x W s A 9 I Y 5 l M J + 3 Q y S T M 3 A g l 5 D P c K C j i 1 h 9 x 6 0 7 0 Y 5 w + F t p 6 4 M L h n H u 5 9 x 4 / F l y D b X 9 a u b X 1 j c 2 t / H Z h Z 3 d v / 6 B 4 e N T W U a I o a 9 F I R K r r E 8 0 E l 6 w F H A T r x o q R 0 B e s 4 4 8 v p 3 7 n j i n N I 3 k D k 5 h 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>9 H P e 5 Y t S I s S W E K m 5 v x X R I F K H G 5 l O w I b j L L 6 + S Z q X s X p Y r D Z t G D e b I w y m c w Q W 4 c A V V u I U 6 e E C B w w M 8 w T O S 6 B G 9 o N d 5 a w 4 t Z o 7 h D 9 D b D 8 + / k m I = &lt; / l a t e x i t &gt; V T h &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B f S y O V b Y P 5 G Q N 5 6 6 z e e U T e / p B w Q = " &gt; A A A B 8 H i c b V D L S g N B E O y N r x h f U Y 9 e h g Q h p 7 A b D / E Y 9 O I x Q l 6 S r G F 2 M p s M m d l d Z m a F s O x X e B F U x K t / 4 t W b 6 M c 4 e R w 0 s a C h q O q m u 8 u L O F P a t j + t z N r 6 x u Z W d j u 3 s 7 u 3 f 5 A / P G q p M J a E N k n I Q 9 n x s K K c B b S p m e a 0 E 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>r F e 5 6 0 Z a z F z D H 9 g v f 0 A Q x K U b w = = &lt; / l a t e x i t &gt; W h att &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " y 3 8 c 8 m a / G y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>6 L b l x W s A 9 I Y 5 l M J + 3 Q y S T M 3 A g l 5 D P c K C j i 1 h 9 x 6 0 7 0 Y 5 w + F t p 6 4 M L h n H u 5 9 x 4 / F l y D b X 9 a u b X 1 j c 2 t / H Z h Z 3 d v / 6 B 4 e N T W U a I o a 9 F I R K r r E 8 0 E l 6 w F H A T r x o q R 0 B e s 4 4 8 v p 3 7 n j i n N I 3 k D k 5 h 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>8 6 b 8 z 4</head><label>4</label><figDesc>r z T n z n k P 4 I 2 f 8 A 7 N v k H Y = &lt; / l a t e x i t &gt; 2u &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 d i 7 B N J F K j x 8 Z S P Q 2 z + S 0 p o k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>8 6 b 8 z 4</head><label>4</label><figDesc>r z T n z n k P 4 I 2 f 8 A 7 N v k H Y = &lt; / l a t e x i t &gt; d a &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F X i n 9 o 2 1 0 j c H w 3 W z a l J R n 9 n g j A s = "&gt; A A A B 7 H i c b V A 9 S w N B E J 2 L X z F + R Q U b m 8 U g W I W 7 W G g Z Y m O Z g J c E k i Ps 7 e 0 l S / b 2 j t 0 9 I R z 5 D T Y W i t j a + S / 8 B X Y 2 / h Y 3 l x S a + G D g 8 d 4 M M / P 8 h D O l b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>e u H D m n H u Z e 4 8 X c y a V Z X 0 b c / M L i 0 v L u Z X 8 6 t r 6 x q a 5 t V 2 X U S I I r ZG I R 6 L p Y U k 5 C 2 l N M c V p M x Y U B x 6 n D a 9 f y f z G N R W S R e G l G s T U D X A 3 Z D 4 j W G m p b e 4 6 n P o K 3 S J U u U p 7 Q + Q I 1 u 1 l 7 7 Z Z s I r W C G i W 2 B N S K B 8 9 3 M S 5 5 8 d q 2 / x y O h F J A h o q w r G U L d u K l Z t i o R j h d J h 3 E k l j T P q 4 S 1 u a h j i g 0 k 1 H F w z R g V Y 6 y I + E r l C h k f p 7 I s W B l I P A 0 5 0 B V j 0 5 7 W X i f 1 4 r U f 6 p m 7 I w T h Q N y f g j P + F I R S i L A 3 W Y o E T x g S a Y C K Z 3 R a S H B S Z K h 5 b X I d j T J 8 + S e q l o H x d L F z q N M x g j B 3 u w D 4 d g w w m U 4 R y q U A M C d / A E r / B m 3 B s v x r v x M W 6 d M y Y z O / A H x u c P E i W Y / Q = = &lt; / l a t e x i t &gt; N &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P O W S q R 0 d 1 E a e C w y L H g w M N x E E C A 0 = " &gt; A A A B 6 H i c b Z C 7 S g N B F I b P x l t c b 1 F L m 8 E g W I X d W G g j B m 2 s J A F z g W Q J s 5 O z y Z j Z C z O z Q g h 5 A h s L R W z 1 Y e x t xL d x c i k 0 8 Y e B j / 8 / h z n n + I n g S j v O t 5 V Z W l 5 Z X c u u 2 x u b W 9 s 7 u d 2 9 m o p T y b D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Text-Category Attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>⇥10 4 &lt;</head><label>4</label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " W 5 L w Q H q O P NM I + H z Q G / U T G V W 5 p 6 k = " &gt; A A A B + 3 i c b V B N S 8 N A E N 3 4 W e t X r E c v i 0 X w V J J a 0 G P R i 8 c K 9 g O a W D b b T b t 0 s w m 7 E 7 G E / B U v H h T x 6 h / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S A T X 4 D j f 1 t r 6 x u b W d m m n v L u 3 f 3 B o H 1 U 6 O k 4 V Z W 0 a i 1 j 1 A q K Z 4 J K 1 g Y N g v U Q x E g W C d Y P J z c z v P j K l e S zv Y Z o w P y I j y U N O C R h p Y F e 8 I M w 8 4 B H T 2 H U e s k a e D + y q U3 P m w K v E L U g V F W g N 7 C 9 v G N M 0 Y h K o I F r 3 X S c B P y M K O B U s L 3 u p Z g m h E z J i f U M l M c v 8 bH 5 7 j s + M M s R h r E x J w H P 1 9 0 R G I q 2 n U W A 6 I w J j v e z N x P + 8 f g r h l Z 9 x m a T A J F 0 s C l O B I c a z I P</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>3 &lt; 4 &lt;</head><label>34</label><figDesc>m 5 d a L 9 W 5 9 L F r X r G L m G P 2 B 9 f k D a n 6 U C A = = &lt; / l a t e x i t &gt; ⇥10 l a t e x i t s h a 1 _ b a s e 6 4 = " v L I Z t n w F q Y Y m Q /i j B t f 6 L A t x g J c = " &gt; A A A B + 3 i c b V B N S 8 N A E J 3 4 W e t X r E c v i 0 X w V J J W 0 G P R i 8 c K 9 g O a W D b b T b t 0 s w m 7 G 7 G E / B U v H h T x 6 h / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S D h T 2 n G + r b X 1 j c 2 t 7 d J O e X d v / + D Q P q p 0 V J x K Q t s k 5 r H s B V h R z g R t a 6 Y 5 7 S W S 4 i j g t B t M b m Z + 9 5 F K x W J x r 6 c J 9 S M 8 E i x k B G s j D e y K F 4 S Z p 1 l E F X K d h 6 y R 5 w O 7 6 t S c O d A q c Q t S h Q K t g f 3 l D W O S R l R o w r F S f d d J t J 9 h q R n h N C 9 7 q a I J J h M 8 o n 1 D B T b L / G x + e 4 7 O j D J E Y S x N C Y 3 m 6 u + J D E d K T a P A d E Z Y j 9 W y N x P / 8 / q p D q / 8 j I k k 1 V S Q x a I w 5 U j H a B Y E G j J J i e Z T Q z C R z N y K y B h L T L S J q 2 x C c J d f X i W d e s 1 t 1 O p 3 F 9 X m d R F H C U 7 g F M 7 B h U t o w i 2 0 o A 0 E n u A Z X u H N y q 0 X 6 9 3 6 W L S u W c X M M f y B 9 f k D a P i U B w = = &lt; / l a t e x i t &gt; l a t e x i t s h a 1 _ b a s e 6 4 = " W 5 L w Q H q O P N M I + H z Q G / U T G V W 5 p 6 k = " &gt; A A A B + 3 i c b V B N S 8 N A E N 3 4 W e t X r E c v i 0 X w V J J a 0 G P R i 8 c K 9 g O a W D b b T b t 0 s w m 7 E 7 G E / B U v H h T x 6 h / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S A T X 4 D j f 1 t r 6 x u b W d m m n v L u 3 f 3 B o H 1 U 6 O k 4 V Z W 0 a i 1 j 1 A q K Z 4 J K 1 g Y N g v U Q x E g W C d Y P J z c z v P j K l e S z v Y Z o w P y I j y U N O C R h p Y F e 8 I M w 8 4 B H T 2 H U e s k a e D + y q U 3 P m w K v E L U g V F W g N 7 C 9 v G N M 0 Y h K o I F r 3 X S c B P y M K O B U s L 3 u p Z g m h E z J i f U M l M cv 8 b H 5 7 j s + M M s R h r E x J w H P 1 9 0 R G I q 2 n U W A 6 I w J j v e z N x P + 8 f g r h l Z 9 x m a T A J F 0 s C l O B I c a z I P</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>3 &lt;</head><label>3</label><figDesc>m 5 d a L 9 W 5 9 L F r X r G L m G P 2 B 9 f k D a n 6 U C A = = &lt; / l a t e x i t &gt; ⇥10 l a t e x i t s h a 1 _ b a s e 6 4 = " v L I Zt n w F q Y Y m Q / i j B t f 6 L A t x g J c = " &gt; A A A B + 3 i c b V B N S 8 N A E J 3 4 W e t X r E c v i 0 X w V J J W 0 G P R i 8 c K 9 g O a W D b b T b t 0 s w m 7 G 7 G E / B U v H h T x 6 h / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S D h T 2 n G + r b X 1 j c 2 t 7 d J O e X d v / + D Q P q p 0 V J x K Q t s k 5 r H s B V h R z g R t a 6 Y 5 7 S W S 4 i j g t B t M b m Z + 9 5 F K x W J x r 6 c J 9 S M 8 E i x k B G s j D e y K F 4 S Z p 1 l E F X K d h 6 y R 5 w O 7 6 t S c O d A q c Q t S h Q K t g f 3 l D W O S R l R o w r F S f d d J t J 9 h q R n h N C 9 7 q a I J J h M 8 o n 1 D B T b L / G x + e 4 7 O j D J E Y S x N C Y 3 m 6 u + J D E d K T a P A d E Z Y j 9 W y N x P / 8 / q p D q / 8 j I k k 1 V S Q x a I w 5 U j H a B Y E G j J J i e Z T Q z C R z N y K y B h L T L S J q 2 x C c J d f X i W d e s 1 t 1 O p 3 F 9 X m d R F H C U 7 g F M 7 B h U t o w i 2 0 o A 0 E n u A Z Xu H N y q 0 X 6 9 3 6 W L S u W c X M M f y B 9 f k D a P i U B w = = &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Number distributions of observed records.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Model performance with different α.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>5. 5 . 3 Figure 8 :</head><label>538</label><figDesc>Figure 8: Illustration of learned attention weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>based Recurrent Layer Documentation Representating Layer Hybrid Predicting Layer gas injection system waste … disposal</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>……</cell></row><row><cell></cell><cell></cell><cell></cell><cell>HAM</cell><cell>HAM</cell><cell>……</cell><cell>HAM</cell></row><row><cell cols="4">Hierarchical Attention-</cell></row><row><cell>ROOT</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Level 1</cell><cell></cell><cell></cell><cell>Embedding</cell><cell>Bi-lstm</cell><cell>Embedding</cell></row><row><cell>Level 2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>…</cell><cell>…</cell><cell></cell><cell>…</cell></row><row><cell>Level H</cell><cell>…</cell><cell>…</cell><cell>…</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The statistics of the datasets</figDesc><table><row><cell>Statistics</cell><cell cols="2">Education Patent</cell></row><row><cell># instances</cell><cell>240,309</cell><cell>100,000</cell></row><row><cell># hierarchical levels</cell><cell>3</cell><cell>4</cell></row><row><cell># categories in level-1</cell><cell>18</cell><cell>9</cell></row><row><cell># categories in level-2</cell><cell>75</cell><cell>128</cell></row><row><cell># categories in level-3</cell><cell>366</cell><cell>661</cell></row><row><cell># categories in level-4</cell><cell>-</cell><cell>8364</cell></row><row><cell># total categories</cell><cell>459</cell><cell>9162</cell></row><row><cell># average categories per instance in level-1</cell><cell>1.07</cell><cell>1.46</cell></row><row><cell># average categories per instance in level-2</cell><cell>1.21</cell><cell>1.62</cell></row><row><cell># average categories per instance in level-3</cell><cell>1.36</cell><cell>1.82</cell></row><row><cell># average categories per instance in level-4</cell><cell>-</cell><cell>2.78</cell></row><row><cell># average categories per instance</cell><cell>3.64</cell><cell>7.67</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison on the HMTC task.</figDesc><table><row><cell></cell><cell></cell><cell>(a) Education</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b) Patent</cell><cell></cell><cell></cell></row><row><cell>Baseline</cell><cell cols="4">Metrics Precision Recall micro-F1 AU (PRC)</cell><cell>Baseline</cell><cell cols="4">Metrics Precision Recall micro-F1 AU (PRC)</cell></row><row><cell>Clus-HMC</cell><cell>0.806</cell><cell>0.704</cell><cell>0.752</cell><cell>0.685</cell><cell>Clus-HMC</cell><cell>0.419</cell><cell>0.345</cell><cell>0.379</cell><cell>0.145</cell></row><row><cell>HMC-LMLP</cell><cell>0.841</cell><cell>0.732</cell><cell>0.783</cell><cell>0.861</cell><cell>HMC-LMLP</cell><cell>0.692</cell><cell>0.380</cell><cell>0.490</cell><cell>0.526</cell></row><row><cell>HMCN-R</cell><cell>0.844</cell><cell>0.733</cell><cell>0.785</cell><cell>0.867</cell><cell>HMCN-R</cell><cell>0.684</cell><cell>0.395</cell><cell>0.501</cell><cell>0.528</cell></row><row><cell>HMCN-F</cell><cell>0.843</cell><cell>0.739</cell><cell>0.787</cell><cell>0.865</cell><cell>HMCN-F</cell><cell>0.704</cell><cell>0.376</cell><cell>0.491</cell><cell>0.524</cell></row><row><cell>HARNN-LG</cell><cell>0.853</cell><cell>0.744</cell><cell>0.795</cell><cell>0.876</cell><cell>HARNN-LG</cell><cell>0.738</cell><cell>0.420</cell><cell>0.535</cell><cell>0.579</cell></row><row><cell>HARNN-LH</cell><cell>0.860</cell><cell>0.740</cell><cell>0.795</cell><cell>0.879</cell><cell>HARNN-LH</cell><cell>0.733</cell><cell>0.418</cell><cell>0.532</cell><cell>0.578</cell></row><row><cell>HARNN-GH</cell><cell>0.845</cell><cell>0.747</cell><cell>0.793</cell><cell>0.874</cell><cell>HARNN-GH</cell><cell>0.740</cell><cell>0.405</cell><cell>0.523</cell><cell>0.572</cell></row><row><cell>HARNN</cell><cell>0.860</cell><cell>0.767</cell><cell>0.811</cell><cell>0.893</cell><cell>HARNN</cell><cell>0.742</cell><cell>0.425</cell><cell>0.541</cell><cell>0.583</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>The code is available at https://github.com/RandolphVI/Hierarchical-Multi-Label-Text-Classification</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ACKNOWLEDGEMENTS</head><p>This research was partially supported by grants from the National Natural Science Foundation of China (Grants No. U1605251, 61727809, 61922073, 61602405), and the Young Elite Scientist Sponsorship Program of CAST.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>label Text Classification: An Attention-based Recurrent Network Approach. In The 28th ACM International Conference on Information and Knowledge Management (CIKM '19), November 3-7, 2019, Beijing, China. ACM, New York, NY, USA, 10 pages.</p><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x V u p m G 9 v h q R t J</p><p>r m F p o a 9 l s J + 3 S z S b s b o Q S + h u 8 e F D E q z / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5</p><p>H S / D g G h p w B y 3 w g Q C D Z 3 i F N 0 c 6 L 8 6 7 8 7 F s L T j 5 z C n 8 g f P 5 A 4 H v j n w = &lt; / l a t e x i t &gt; C 4 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D q I j 3 C i o G 5 I S i W r l m y R C d S W 9 r T</p><p>r e 2 d 4 q 7 p b 3 9 g 8 O j 8 v F J W 8 e p o s y j s Y h V N y C a C S 6 Z Z 7 g R r J s o R q J A s E 4 w a c 7 9 z h N T m s f y w U w T 5 k d k J H n I K T F W 8 p q P W X 0 2 K F e c q r M A X i d u T i q Q o z U o f / W H M U 0 j J g 0 V R O u e 6 y T G z 4 g y n A o 2 K / V T z R J C J 2 T E e p Z K E j H t Z 4 t j Z / j C K k M c x s q W N H i h / p 7 I S K T 1 N A p s Z 0 T M W K 9 6 c / E / r 5 e a 8 M b P u E x S w y R d L g p T g U 2 M 5 5 / j I V e M G j G 1 h F D F 7 a 2 Y j o k i 1 N h 8 S j Y E d / X l d d K u V d 2 r a u 2</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HMC-LMLP</head><note type="other">HMCN</note><p>The variants of HARNN are listed as follows:</p><p>• HARNN-LG: HARNN-LG is a variant of HARNN without considering the dependencies between different levels of the hierarchical structure. • HARNN-LH: HARNN-LH is a variant of HARNN without considering the overall hierarchical structure information. • HARNN-GH: HARNN-GH is a variant of HARNN without considering the information of the local regions in the hierarchical structure.</p><p>Concretely, the chosen baselines can be categorized into local approach (HMC-LMLP), global approach (Clus-HMC) and hybrid approachs (HMCN-F, HMCN-R). All baselines ignore the associations between texts and the hierarchical structure. Note that we design three variants of HARNN to highlight the effectiveness of our proposed HAM unit and hybrid prediction method. In the following experiments, all methods are implemented by TensorFlow <ref type="bibr" target="#b0">[1]</ref>, and all the experiments are conducted on a Linux server with four 2.0GHz Intel Xeon E5-2620 CPUs and a Tesla K80 GPU. For fair comparisons, all parameters in these baselines are tuned to achieve the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluation Metrics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Threshold based evaluation.</head><p>In HMTC task, we first adopt three widely used evaluation metrics <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>: precision, recall and F1 measure. Given a category i ∈ C, let T P i , F P i , F N i , be the number of true positives, false positives, false negatives, respectively. The precision and recall for the whole output hierarchical structure are then defined as <ref type="bibr" target="#b30">[31]</ref>:</p><p>And we also evaluate the performance using micro -F 1 <ref type="bibr" target="#b12">[13]</ref> which is the combination of precision and recall. The micro -F 1 measures the classification effectiveness by counting the true positives, false negatives and false positives globally, which takes class imbalance into account <ref type="bibr" target="#b35">[36]</ref>. For the three threshold based evaluation metrics, the larger, the better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Area under the average PR curve.</head><p>The outputs of our model HARNN and the other baselines are probability values of each category in the entire hierarchical structure. Hence, the final predictions are generated after thresholding those probabilities. The choice of an optimal threshold is difficult and often subjective, thus we follow the trend of HMTC research <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b32">33]</ref> to employ Area Under the Average Precision-Recall Curve (AU (PRC)) <ref type="bibr" target="#b8">[9]</ref> for avoiding choosing thresholds. The higher the AU (PRC) of a method is, the better its predictive performance is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">Performance Comparison (RQ1).</head><p>To demonstrate practical significance of our proposed model, we compare HARNN with all the baselines on HMTC task. The results of all methods on both Education and Patent datasets are shown in Table <ref type="table">2</ref>. From the results, we can get several observations:</p><p>(1) HARNN achieves the best performance on all evaluation metrics in two datasets, which indicates that HARNN is more capable for HMTC tasks with an advantage of tackling hierarchical category structure effectively and accurately. (2) All the variants of HARNN perform better than HMCN-F and HMCN-R on all evaluation metrics in two datasets. The reason is that the variants of HARNN could capture the associations between texts and the hierarchical structure via TCA while HMCN-F and HMCN-R cannot. (3) HARNN beats HARNN-LG by additionally leveraging the HAM unit which considers the dependencies among different levels of the hierarchical structure. And HARNN performs better than HARNN-LH and HARNN-GH by combining the information</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Refined experts: improving classification in large taxonomies</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 32nd international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-label classification on tree-and dag-structured hierarchies</title>
		<author>
			<persName><forename type="first">W</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-label hierarchical classification using a competitive neural network for protein function prediction</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Borges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Nievola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2012 International Joint Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Neural Networks (IJCNN)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-label feature selection using correlation information</title>
		<author>
			<persName><forename type="first">A</forename><surname>Braytee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Catchpoole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1649" to="1656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical document categorization with support vector machines</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth ACM international conference on Information and knowledge management</title>
		<meeting>the thirteenth ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="78" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reduction strategies for hierarchical multi-label classification in protein function prediction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cerri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Barros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>De Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">373</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Incremental algorithms for hierarchical classification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gentile</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zaniboni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="31" to="54" />
			<date type="published" when="2006-01">Jan. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The relationship between precision-recall and roc curves</title>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goadrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large margin hierarchical classification</title>
		<author>
			<persName><forename type="first">O</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Keshet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-first international conference on Machine learning</title>
		<meeting>the twenty-first international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Boosting multi-label hierarchical text categorization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fagni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="287" to="313" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automated categorization in the international patent classification</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Fall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Törcsvári</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Benzineb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karetka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acm Sigir Forum</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="10" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-label learning: a review of the state of the art and ongoing research</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gibaja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ventura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="411" to="444" />
			<date type="published" when="2014">2014</date>
			<publisher>Wiley Interdisciplinary Reviews</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A survey of automated hierarchical classification of patents</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-F</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Professional Search in the Modern World</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="215" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bringing structure to text: mining phrases, entities, topics, and hierarchies</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>El-Kishky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1968" to="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Question difficulty prediction for reading problems in standard tests</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N D</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03130</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hierarchical taxonomy aware network embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1920" to="1929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchically classifying documents with multiple labels</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Perry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Intelligence and Data Mining</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="133" to="139" />
		</imprint>
	</monogr>
	<note>CIDM&apos;09. IEEE Symposium on</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hierarchical multilabel classification of social text streams</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Peetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Van Dolen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</title>
		<meeting>the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="213" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning hierarchical multi-category text classification models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rousu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on Machine learning</title>
		<meeting>the 22nd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="744" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Kernel-based learning of hierarchical multilabel classification models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rousu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1601" to="1626" />
			<date type="published" when="2006-07">Jul. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hierarchical text categorization using neural networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="118" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A survey of hierarchical classification across different application domains</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Silla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="31" to="72" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A radical-aware attention-based model for chinese text classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Third AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Decision trees for hierarchical multi-label classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Vens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Struyf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schietgat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Džeroski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Blockeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">185</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-label relational neighbor classification using social context features</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hierarchical multi-label classification networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cerri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5225" to="5234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning classifiers using hierarchically structured class taxonomies</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Honavar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Abstraction, Reformulation, and Approximation</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="313" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning low-rank label correlations for multi-label classification with missing labels</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Data Mining</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1067" to="1072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Effective multi-label active learning for text classification</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="917" to="926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Modeling social attention for stock analysis: An influence propagation perspective</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Data Mining</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="609" to="618" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
