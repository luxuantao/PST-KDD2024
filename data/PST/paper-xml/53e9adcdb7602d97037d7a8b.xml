<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Map-Reduce for Machine Learning on Multicore</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Cheng-Tao</forename><surname>Chu</surname></persName>
							<email>chengtao@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">CS. Department</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<addrLine>353 Serra Mall</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305-9025. †</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Rexee Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sang</forename><forename type="middle">Kyun</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CS. Department</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<addrLine>353 Serra Mall</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305-9025. †</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Rexee Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi-An</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CS. Department</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<addrLine>353 Serra Mall</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305-9025. †</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Rexee Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuanyuan</forename><surname>Yu</surname></persName>
							<email>yuanyuan@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">CS. Department</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<addrLine>353 Serra Mall</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305-9025. †</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Rexee Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gary</forename><surname>Bradski</surname></persName>
							<email>garybradski@gmail</email>
							<affiliation key="aff0">
								<orgName type="department">CS. Department</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<addrLine>353 Serra Mall</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305-9025. †</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Rexee Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CS. Department</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<addrLine>353 Serra Mall</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305-9025. †</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Rexee Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kunle</forename><surname>Olukotun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CS. Department</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<addrLine>353 Serra Mall</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305-9025. †</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Rexee Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Map-Reduce for Machine Learning on Multicore</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">364EB7D2DB9495EAD4FB6E4C7C5DE668</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We are at the beginning of the multicore era. Computers will have increasingly many cores (processors), but there is still no good programming framework for these architectures, and thus no simple and unified way for machine learning to take advantage of the potential speed up. In this paper, we develop a broadly applicable parallel programming method, one that is easily applied to many different learning algorithms. Our work is in distinct contrast to the tradition in machine learning of designing (often ingenious) ways to speed up a single algorithm at a time. Specifically, we show that algorithms that fit the Statistical Query model <ref type="bibr" target="#b14">[15]</ref> can be written in a certain "summation form," which allows them to be easily parallelized on multicore computers. We adapt Google's map-reduce <ref type="bibr" target="#b6">[7]</ref> paradigm to demonstrate this parallel speed up technique on a variety of learning algorithms including locally weighted linear regression (LWLR), k-means, logistic regression (LR), naive Bayes (NB), SVM, ICA, PCA, gaussian discriminant analysis (GDA), EM, and backpropagation (NN). Our experimental results show basically linear speedup with an increasing number of processors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Frequency scaling on silicon-the ability to drive chips at ever higher clock rates-is beginning to hit a power limit as device geometries shrink due to leakage, and simply because CMOS consumes power every time it changes state <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. Yet Moore's law <ref type="bibr" target="#b19">[20]</ref>, the density of circuits doubling every generation, is projected to last between 10 and 20 more years for silicon based circuits <ref type="bibr" target="#b9">[10]</ref>. By keeping clock frequency fixed, but doubling the number of processing cores on a chip, one can maintain lower power while doubling the speed of many applications. This has forced an industrywide shift to multicore.</p><p>We thus approach an era of increasing numbers of cores per chip, but there is as yet no good framework for machine learning to take advantage of massive numbers of cores. There are many parallel programming languages such as Orca, Occam ABCL, SNOW, MPI and PARLOG, but none of these approaches make it obvious how to parallelize a particular algorithm. There is a vast literature on distributed learning and data mining <ref type="bibr" target="#b17">[18]</ref>, but very little of this literature focuses on our goal: A general means of programming machine learning on multicore. Much of this literature contains a long and distinguished tradition of developing (often ingenious) ways to speed up or parallelize individual learning algorithms, for instance cascaded SVMs <ref type="bibr" target="#b10">[11]</ref>. But these yield no general parallelization technique for machine learning and, more pragmatically, specialized implementations of popular algorithms rarely lead to widespread use. Some examples of more general papers are: Caregea et. al. <ref type="bibr" target="#b4">[5]</ref> give some general data distribution conditions for parallelizing machine learning, but restrict the focus to decision trees; Jin and Agrawal <ref type="bibr" target="#b13">[14]</ref> give a general machine learning programming approach, but only for shared memory machines. This doesn't fit the architecture of cellular or grid type multiprocessors where cores have local cache, even if it can be dynamically reallocated.</p><p>In this paper, we focuses on developing a general and exact technique for parallel programming of a large class of machine learning algorithms for multicore processors. The central idea of this approach is to allow a future programmer or user to speed up machine learning applications by "throwing more cores" at the problem rather than search for specialized optimizations. This paper's contributions are:</p><p>(i) We show that any algorithm fitting the Statistical Query Model may be written in a certain "summation form." This form does not change the underlying algorithm and so is not an approximation, but is instead an exact implementation. (ii) The summation form does not depend on, but can be easily expressed in a map-reduce <ref type="bibr" target="#b6">[7]</ref> framework which is easy to program in. (iii) This technique achieves basically linear speed-up with the number of cores.</p><p>We attempt to develop a pragmatic and general framework. What we do not claim:</p><p>(i) We make no claim that our technique will necessarily run faster than a specialized, one-off solution. Here we achieve linear speedup which in fact often does beat specific solutions such as cascaded SVM <ref type="bibr" target="#b10">[11]</ref> (see section 5; however, they do handle kernels, which we have not addressed). (ii) We make no claim that following our framework (for a specific algorithm) always leads to a novel parallelization undiscovered by others. What is novel is the larger, broadly applicable framework, together with a pragmatic programming paradigm, map-reduce. (iii) We focus here on exact implementation of machine learning algorithms, not on parallel approximations to algorithms (a worthy topic, but one which is beyond this paper's scope).</p><p>In section 2 we discuss the Statistical Query Model, our summation form framework and an example of its application. In section 3 we describe how our framework may be implemented in a Googlelike map-reduce paradigm. In section 4 we choose 10 frequently used machine learning algorithms as examples of what can be coded in this framework. This is followed by experimental runs on 10 moderately large data sets in section 5, where we show a good match to our theoretical computational complexity results. Basically, we often achieve linear speedup in the number of cores. Section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Statistical Query and Summation Form</head><p>For multicore systems, Sutter and Larus <ref type="bibr" target="#b24">[25]</ref> point out that multicore mostly benefits concurrent applications, meaning ones where there is little communication between cores. The best match is thus if the data is subdivided and stays local to the cores. To achieve this, we look to Kearns' Statistical Query Model <ref type="bibr" target="#b14">[15]</ref>.</p><p>The Statistical Query Model is sometimes posed as a restriction on the Valiant PAC model <ref type="bibr" target="#b25">[26]</ref>, in which we permit the learning algorithm to access the learning problem only through a statistical query oracle. Given a function f (x, y) over instances, the statistical query oracle returns an estimate of the expectation of f (x, y) (averaged over the training/test distribution). Algorithms that calculate sufficient statistics or gradients fit this model, and since these calculations may be batched, they are expressible as a sum over data points. This class of algorithms is large; We show 10 popular algorithms in section 4 below. An example that does not fit is that of learning an XOR over a subset of bits. <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b14">15]</ref>. However, when an algorithm does sum over the data, we can easily distribute the calculations over multiple cores: We just divide the data set into as many pieces as there are cores, give each core its share of the data to sum the equations over, and aggregate the results at the end. We call this form of the algorithm the "summation form."</p><p>As an example, consider ordinary least squares (linear regression), which fits a model of the form y = θ T x by solving: </p><formula xml:id="formula_0">θ * = min θ m i=1 (θ T x i -y i ) 2</formula><formula xml:id="formula_1">* = (X T X) -1 X T y.</formula><p>To put this computation into summation form, we reformulate it into a two phase algorithm where we first compute sufficient statistics by summing over the data, and then aggregate those statistics and solve to get</p><formula xml:id="formula_2">θ * = A -1 b. Concretely, we compute A = X T X and b = X T y as follows: A = m i=1 (x i x T i ) and b = m i=1 (x i y i ).</formula><p>The computation of A and b can now be divided into equal size pieces and distributed among the cores. We next discuss an architecture that lends itself to the summation form: Map-reduce.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Architecture</head><p>Many programming frameworks are possible for the summation form, but inspired by Google's success in adapting a functional programming construct, map-reduce <ref type="bibr" target="#b6">[7]</ref>, for wide spread parallel programming use inside their company, we adapted this same construct for multicore use. Google's map-reduce is specialized for use over clusters that have unreliable communication and where individual computers may go down. These are issues that multicores do not have; thus, we were able to developed a much lighter weight architecture for multicores, shown in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows a high level view of our architecture and how it processes the data. In step 0, the map-reduce engine is responsible for splitting the data by training examples (rows). The engine then caches the split data for the subsequent map-reduce invocations. Every algorithm has its own engine instance, and every map-reduce task will be delegated to its engine (step 1). Similar to the original map-reduce architecture, the engine will run a master (step 1.1) which coordinates the mappers and the reducers. The master is responsible for assigning the split data to different mappers, and then collects the processed intermediate data from the mappers (step 1.1.1 and 1.1.2). After the intermediate data is collected, the master will in turn invoke the reducer to process it (step 1.1.3) and return final results (step 1.1.4). Note that some mapper and reducer operations require additional scalar information from the algorithms. In order to support these operations, the mapper/reducer can obtain this information through the query info interface, which can be customized for each different algorithm (step 1.1.1.1 and 1.1.3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Adopted Algorithms</head><p>In this section, we will briefly discuss the algorithms we have implemented based on our framework. These algorithms were chosen partly by their popularity of use in NIPS papers, and our goal will be to illustrate how each algorithm can be expressed in summation form. We will defer the discussion of the theoretical improvement that can be achieved by this parallelization to Section 4.1. In the following, x or x i denotes a training vector and y or y i denotes a training label.</p><p>• Locally Weighted Linear Regression (LWLR) LWLR <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b2">3]</ref> is solved by finding the solution of the normal equations Aθ = b, where</p><formula xml:id="formula_3">A = m i=1 w i (x i x T i ) and b = m i=1 w i (x i y i ).</formula><p>For the summation form, we divide the computation among different mappers. In this case, one set of mappers is used to compute subgroup w i (x i x T i ) and another set to compute subgroup w i (x i y i ). Two reducers respectively sum up the partial values for A and b, and the algorithm finally computes the solution θ = A -1 b. Note that if w i = 1, the algorithm reduces to the case of ordinary least squares (linear regression).</p><p>• Naive Bayes (NB) In NB <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21]</ref>, we have to estimate P (x j = k|y = 1), P (x j = k|y = 0), and P (y) from the training data. In order to do so, we need to sum over x j = k for each y label in the training data to calculate P (x|y). We specify different sets of mappers to calculate the following: subgroup 1{x j = k|y = 1}, subgroup 1{x j = k|y = 0}, subgroup 1{y = 1} and subgroup 1{y = 0}. The reducer then sums up intermediate results to get the final result for the parameters.</p><p>• Gaussian Discriminative Analysis (GDA) The classic GDA algorithm <ref type="bibr" target="#b12">[13]</ref> needs to learn the following four statistics P (y), µ 0 , µ 1 and Σ. For all the summation forms involved in these computations, we may leverage the map-reduce framework to parallelize the process. Each mapper will handle the summation (i.e. Σ 1{y i = 1}, Σ 1{y i = 0}, Σ 1{y i = 0}x i , etc) for a subgroup of the training samples. Finally, the reducer will aggregate the intermediate sums and calculate the final result for the parameters. • k-means In k-means <ref type="bibr" target="#b11">[12]</ref>, it is clear that the operation of computing the Euclidean distance between the sample vectors and the centroids can be parallelized by splitting the data into individual subgroups and clustering samples in each subgroup separately (by the mapper).</p><p>In recalculating new centroid vectors, we divide the sample vectors into subgroups, compute the sum of vectors in each subgroup in parallel, and finally the reducer will add up the partial sums and compute the new centroids. • Logistic Regression (LR) For logistic regression <ref type="bibr" target="#b22">[23]</ref>, we choose the form of hypothesis as</p><formula xml:id="formula_4">h θ (x) = g(θ T x) = 1/(1 + exp(-θ T x))</formula><p>Learning is done by fitting θ to the training data where the likelihood function can be optimized by using Newton-Raphson to update θ := θ -H -1 ∇ θ (θ). ∇ θ (θ) is the gradient, which can be computed in parallel by mappers summing up subgroup (y (i) -h θ (x (i) ))x (i) j each NR step i. The computation of the hessian matrix can be also written in a summation form of H(j, k) := H(j, k)</p><formula xml:id="formula_5">+ h θ (x (i) )(h θ (x (i) ) -1)x (i) j x (i)</formula><p>k for the mappers. The reducer will then sum up the values for gradient and hessian to perform the update for θ.</p><p>• Neural Network (NN) We focus on backpropagation <ref type="bibr" target="#b5">[6]</ref> By defining a network structure (we use a three layer network with two output neurons classifying the data into two categories), each mapper propagates its set of data through the network. For each training example, the error is back propagated to calculate the partial gradient for each of the weights in the network. The reducer then sums the partial gradient from each mapper and does a batch gradient descent to update the weights of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Principal Components Analysis (PCA) PCA [29] computes the principle eigenvectors of the covariance matrix</head><formula xml:id="formula_6">Σ = 1 m m i=1 x i x T i</formula><p>-µµ T over the data. In the definition for Σ, the term m i=1 x i x T i is already expressed in summation form. Further, we can also express the mean vector µ as a sum, µ = 1 m m i=1 x i . The sums can be mapped to separate cores, and then the reducer will sum up the partial results to produce the final empirical covariance matrix.</p><p>• Independent Component Analysis (ICA) ICA <ref type="bibr" target="#b0">[1]</ref> tries to identify the independent source vectors based on the assumption that the observed data are linearly transformed from the source data. In ICA, the main goal is to compute the unmixing matrix W. We implement batch gradient ascent to optimize the W 's likelihood. In this scheme, we can independently calculate the expression</p><formula xml:id="formula_7">1 -2g(w T 1 x (i) ) . . .</formula><p>x (i) T in the mappers and sum them up in the reducer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Expectation Maximization (EM)</head><p>For EM <ref type="bibr" target="#b7">[8]</ref> we use Mixture of Gaussian as the underlying model as per <ref type="bibr" target="#b18">[19]</ref>. For parallelization: In the E-step, every mapper processes its subset of the training data and computes the corresponding w (i) j (expected pseudo count). In Mphase, three sets of parameters need to be updated: p(y), µ, and Σ. For p(y), every mapper will compute subgroup (w (i) j ), and the reducer will sum up the partial result and divide it by m. For µ, each mapper will compute subgroup (w (i) j * x (i) ) and subgroup (w (i) j ), and the reducer will sum up the partial result and divide them. For Σ, every mapper will compute subgroup (w (i) j * (x (i) -µ j ) * (x (i) -µ j ) T ) and subgroup (w (i) j ), and the reducer will again sum up the partial result and divide them.</p><p>• Support Vector Machine (SVM) Linear SVM's <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b21">22]</ref> primary goal is to optimize the following primal problem min w,b w 2 + C i:ξi&gt;0 ξ p i s.t. y (i) (w T x (i) + b) ≥ 1ξ i where p is either 1 (hinge loss) or 2 (quadratic loss). <ref type="bibr" target="#b1">[2]</ref> has shown that the primal problem for quadratic loss can be solved using the following formula where sv are the support vectors:</p><formula xml:id="formula_8">∇ = 2w + 2C i∈sv (w • x i -y i )x i &amp; Hessian H = I + C i∈sv x i x T i</formula><p>We perform batch gradient descent to optimize the objective function. The mappers will calculate the partial gradient subgroup(i∈sv) (w • x i -y i )x i and the reducer will sum up the partial results to update w vector. Some implementations of machine learning algorithms, such as ICA, are commonly done with stochastic gradient ascent, which poses a challenge to parallelization. The problem is that in every step of gradient ascent, the algorithm updates a common set of parameters (e.g. the unmixing W matrix in ICA). When one gradient ascent step (involving one training sample) is updating W , it has to lock down this matrix, read it, compute the gradient, update W , and finally release the lock. This "lock-release" block creates a bottleneck for parallelization; thus, instead of stochastic gradient ascent, our algorithms above were implemented using batch gradient ascent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Algorithm Time Complexity Analysis</head><p>Table <ref type="table">1</ref> shows the theoretical complexity analysis for the ten algorithms we implemented on top of our framework. We assume that the dimension of the inputs is n (i.e., x ∈ R n ), that we have m training examples, and that there are P cores. The complexity of iterative algorithms is analyzed for one iteration, and so their actual running time may be slower. <ref type="foot" target="#foot_0">1</ref> A few algorithms require matrix inversion or an eigen-decomposition of an n-by-n matrix; we did not parallelize these steps in our experiments, because for us m &gt;&gt; n, and so their cost is small. However, there is extensive research in numerical linear algebra on parallelizing these numerical operations <ref type="bibr" target="#b3">[4]</ref>, and in the complexity analysis shown in the table, we have assumed that matrix inversion and eigen-decompositions can be sped up by a factor of P on P cores. (In practice, we expect P ≈ P .) In our own software implementation, we had P = 1. Further, the reduce phase can minimize communication by combining data as it's passed back; this accounts for the log(P ) factor.</p><p>As an example of our running-time analysis, for single-core LWLR we have to compute A = m i=1 w i (x i x T i ), which gives us the mn 2 term. This matrix must be inverted for n 3 ; also, the reduce step incurs a covariance matrix communication cost of n 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>To provide fair comparisons, each algorithm had two different versions: One running map-reduce, and the other a serial implementation without the framework. We conducted an extensive series of experiments to compare the speed up on data sets of various sizes (table 2), on eight commonly used machine learning data sets from the UCI Machine Learning repository and two other ones from a [anonymous] research group (Helicopter Control and sensor data). Note that not all the experiments make sense from an output view -regression on categorical data -but our purpose was to test speedup so we ran every algorithm over all the data.</p><p>The first environment we conducted experiments on was an Intel X86 PC with two Pentium-III 700 MHz CPUs and 1GB physical memory. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results and Discussion</head><p>Table <ref type="table" target="#tab_2">3</ref> shows the speedup on dual processors over all the algorithms on all the data sets. As can be seen from the table, most of the algorithms achieve more than 1.9x times performance improvement. For some of the experiments, e.g. gda/covertype, ica/ipums, nn/colorhistogram, etc., we obtain a greater than 2x speedup. This is because the original algorithms do not utilize all the cpu cycles efficiently, but do better when we distribute the tasks to separate threads/processes.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> shows the speedup of the algorithms over all the data sets for 2,4,8 and 16 processing cores.</p><p>In the figure, the thick lines shows the average speedup, the error bars show the maximum and minimum speedups and the dashed lines show the variance.   of cores, but with a slope &lt; 1.0. The reason for the sub-unity slope is increasing communication overhead. For simplicity and because the number of data points m typically dominates reduction phase communication costs (typically a factor of n<ref type="foot" target="#foot_1">2</ref> but n &lt;&lt; m), we did not parallelize the reduce phase where we could have combined data on the way back. Even so, our simple SVM approach gets about 13.6% speed up on average over 16 cores whereas the specialized SVM cascade <ref type="bibr" target="#b10">[11]</ref> averages only 4%.</p><p>Finally, the above are runs on multiprocessor machines. We finish by reporting some confirming results and higher performance on a proprietary multicore simulator over the sensor dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>As the Intel and AMD product roadmaps indicate <ref type="bibr" target="#b23">[24]</ref>, the number of processing cores on a chip will be doubling several times over the next decade, even as individual cores cease to become significantly faster. For machine learning to continue reaping the bounty of Moore's law and apply to ever larger datasets problems, it is important to adopt a programming architecture which takes advantage of multicore. In this paper, by taking advantage of the summation form in a map-reduce framework, we could parallelize a wide range of machine learning algorithms and achieve a 1.9 times speedup on a dual processor on up to 54 times speedup on 64 cores. These results are in line with the complexity analysis in Table <ref type="table">1</ref>. We note that the speedups achieved here involved no special optimizations of the algorithms themselves. We have demonstrated a simple programming framework where in the future we can just "throw cores" at the problem of speeding up machine learning code.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Multicore map-reduce framework</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a)-(i) show the speedup from 1 to 16 processors of all the algorithms over all the data sets. The Bold line is the average, error bars are the max and min speedups and the dashed lines are the variance.</figDesc><graphic coords="7,108.00,298.01,130.58,98.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The operating system was Linux RedHat 8.0 Kernel 2.4.20-</figDesc><table><row><cell></cell><cell>single</cell><cell>multi</cell></row><row><cell>LWLR LR NB NN GDA PCA ICA k-means EM SVM</cell><cell cols="3">O(mn 2 + n 3 ) O( mn 2 P + n 3 P + n 2 log(P )) O(mn 2 + n 3 ) O( mn 2 P + n 3 P + n 2 log(P )) O(mn + nc) O( mn P + nc log(P )) O(mn + nc) O( mn P + nc log(P )) O(mn 2 + n 3 ) O( mn 2 P + n 3 P + n 2 log(P )) O(mn 2 + n 3 ) O( mn 2 P + n 3 P + n 2 log(P )) O(mn 2 + n 3 ) O( mn 2 P + n 3 P + n 2 log(P )) O(mnc) O( mnc P + mn log(P )) O(mn 2 + n 3 ) O( mn 2 P + n 3 P + n 2 log(P )) O(m 2 n) O( m 2 n P + n log(P ))</cell></row><row><cell></cell><cell cols="2">Table 1: time complexity analysis</cell></row><row><cell cols="2">Data Sets</cell><cell cols="2">samples (m) features (n)</cell></row><row><cell>Adult</cell><cell></cell><cell>30162</cell><cell>14</cell></row><row><cell cols="2">Helicopter Control</cell><cell>44170</cell><cell>21</cell></row><row><cell cols="2">Corel Image Features</cell><cell>68040</cell><cell>32</cell></row><row><cell cols="2">IPUMS Census</cell><cell>88443</cell><cell>61</cell></row><row><cell cols="2">Synthetic Time Series</cell><cell>100001</cell><cell>10</cell></row><row><cell cols="2">Census Income</cell><cell>199523</cell><cell>40</cell></row><row><cell cols="2">ACIP Sensor</cell><cell>229564</cell><cell>8</cell></row><row><cell cols="2">KDD Cup 99</cell><cell>494021</cell><cell>41</cell></row><row><cell cols="2">Forest Cover Type</cell><cell>581012</cell><cell>55</cell></row><row><cell cols="2">1990 US Census</cell><cell>2458285</cell><cell>68</cell></row><row><cell></cell><cell cols="3">Table 2: data sets size and description</cell></row><row><cell cols="4">8smp. In addition, we also ran extensive comparison experiments on a 16 way Sun Enterprise 6000,</cell></row><row><cell cols="4">running Solaris 10; here, we compared results using 1,2,4,8, and 16 cores.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Speedup is basically linear with number</figDesc><table><row><cell></cell><cell>lwlr</cell><cell>gda</cell><cell>nb</cell><cell>logistic</cell><cell>pca</cell><cell>ica</cell><cell>svm</cell><cell>nn</cell><cell>kmeans</cell><cell>em</cell></row><row><cell>Adult</cell><cell>1.922</cell><cell>1.801</cell><cell>1.844</cell><cell>1.962</cell><cell>1.809</cell><cell>1.857</cell><cell>1.643</cell><cell>1.825</cell><cell>1.947</cell><cell>1.854</cell></row><row><cell>Helicopter</cell><cell>1.93</cell><cell>2.155</cell><cell>1.924</cell><cell>1.92</cell><cell>1.791</cell><cell>1.856</cell><cell>1.744</cell><cell>1.847</cell><cell>1.857</cell><cell>1.86</cell></row><row><cell>Corel Image</cell><cell>1.96</cell><cell>1.876</cell><cell>2.002</cell><cell>1.929</cell><cell>1.97</cell><cell>1.936</cell><cell>1.754</cell><cell>2.018</cell><cell>1.921</cell><cell>1.832</cell></row><row><cell>IPUMS</cell><cell>1.963</cell><cell>2.23</cell><cell>1.965</cell><cell>1.938</cell><cell>1.965</cell><cell>2.025</cell><cell>1.799</cell><cell>1.974</cell><cell>1.957</cell><cell>1.984</cell></row><row><cell>Synthetic</cell><cell>1.909</cell><cell>1.964</cell><cell>1.972</cell><cell>1.92</cell><cell>1.842</cell><cell>1.907</cell><cell>1.76</cell><cell>1.902</cell><cell>1.888</cell><cell>1.804</cell></row><row><cell>Census Income</cell><cell>1.975</cell><cell>2.179</cell><cell>1.967</cell><cell>1.941</cell><cell>2.019</cell><cell>1.941</cell><cell>1.88</cell><cell>1.896</cell><cell>1.961</cell><cell>1.99</cell></row><row><cell>Sensor</cell><cell>1.927</cell><cell>1.853</cell><cell>2.01</cell><cell>1.913</cell><cell>1.955</cell><cell>1.893</cell><cell>1.803</cell><cell>1.914</cell><cell>1.953</cell><cell>1.949</cell></row><row><cell>KDD</cell><cell>1.969</cell><cell>2.216</cell><cell>1.848</cell><cell>1.927</cell><cell>2.012</cell><cell>1.998</cell><cell>1.946</cell><cell>1.899</cell><cell>1.973</cell><cell>1.979</cell></row><row><cell>Cover Type</cell><cell>1.961</cell><cell>2.232</cell><cell>1.951</cell><cell>1.935</cell><cell>2.007</cell><cell>2.029</cell><cell>1.906</cell><cell>1.887</cell><cell>1.963</cell><cell>1.991</cell></row><row><cell>Census</cell><cell>2.327</cell><cell>2.292</cell><cell>2.008</cell><cell>1.906</cell><cell>1.997</cell><cell>2.001</cell><cell>1.959</cell><cell>1.883</cell><cell>1.946</cell><cell>1.977</cell></row><row><cell>avg.</cell><cell>1.985</cell><cell>2.080</cell><cell>1.950</cell><cell>1.930</cell><cell>1.937</cell><cell>1.944</cell><cell>1.819</cell><cell>1.905</cell><cell>1.937</cell><cell>1.922</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note><p>Speedups achieved on a dual core processor, without load time. Numbers reported are dualcore time / single-core time. Super linear speedup sometimes occurs due to a reduction in processor idle time with multiple threads.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>2 NN speedup was[16 cores, 15.5x], [32 cores, 29x], [64 cores, 54x]. LR speedup was [16 cores, 15x], [32 cores, 29.5x], [64 cores, 53x]. Multicore machines are generally faster than multiprocessor machines because communication internal to the chip is much less costly.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>If, for example, the number of iterations required grows with m. However, this would affect single-and multi-core implementations equally.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>This work was done in collaboration with Intel Corporation.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Skip Macy from Intel for sharing his valuable experience in VTune performance analyzer. Yirong Shen, Anya Petrovskaya, and Su-In Lee from Stanford University helped us in preparing various data sets used in our experiments. This research was sponsored in part by the Defense Advanced Research Projects Agency (DARPA) under the ACIP program and grant number NBCH104009.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An information-maximization approach to blind separation and blind deconvolution</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Bell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Computation</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Training a support vector machine in the primal</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>submitted</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Locally weighted regression: An approach to regression analysis by local fitting</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Cleveland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Devlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="596" to="610" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast parallel matrix inversion algorithms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Csanky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Comput</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="618" to="623" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A framework for learning from distributed data using sufficient statistics and its application to learning decision trees</title>
		<author>
			<persName><forename type="first">A</forename><surname>Silvescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Caragea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Honavar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Hybrid Intelligent Systems</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning representation by back-propagating errors</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mapreduce: Simplified data processing on large clusters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="137" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rubin</forename><forename type="middle">D B</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Power-constrained cmos scaling limits</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Microprocessors for the new millennium: Challenges, opportunities and new frontiers</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gelsinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ISSCC Tech. Digest</title>
		<imprint>
			<biblScope unit="page" from="22" to="25" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Eric Cosatto and Vladimire Vapnik. Parallel support vector machines: The cascade svm</title>
		<author>
			<persName><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><forename type="middle">Durdanovic</forename><surname>Hans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Clustering Algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hartigan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discriminant analysis by gaussian mixtures</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society B</title>
		<imprint>
			<biblScope unit="page" from="155" to="176" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Shared memory parallelization of data mining algorithms: Techniques, programming interface, and performance</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second SIAM International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Efficient noise-tolerant learning from statistical queries</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kearns</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="392" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">An Introduction to Computational Learning Theory</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Umesh</forename><forename type="middle">V</forename><surname>Vazirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Naive (bayes) at forty: The independence asssumption in information retrieval</title>
		<author>
			<persName><forename type="first">David</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML98: Tenth European Conference On Machine Learning</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Distributed data mining bibliography</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hillow</forename><surname>Kargupta</surname></persName>
		</author>
		<ptr target="http://www.cs.umbc.edu/hillol/DDMBIB/" />
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The expectation-maximization algorithm</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="page" from="47" to="59" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Progress in digital integrated electronics</title>
		<author>
			<persName><forename type="first">G</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IEDM Tech. Digest</title>
		<imprint>
			<biblScope unit="page" from="11" to="13" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An analysis of bayesian classifiers</title>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Iba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pat</forename><surname>Langley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Fast training of support vector machines using sequential minimal optimization</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="185" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Logistic regression diagnostics</title>
		<author>
			<persName><forename type="first">Daryl</forename><surname>Pregibon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Annals of Statistics</title>
		<imprint>
			<date type="published" when="1981">1981</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="705" to="724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">There&apos;s a multicore in your future</title>
		<author>
			<persName><forename type="first">T</forename><surname>Studt</surname></persName>
		</author>
		<ptr target="http://tinyurl.com/ohd2m" />
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Software and the concurrency revolution</title>
		<author>
			<persName><forename type="first">Herb</forename><surname>Sutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Larus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Queue</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="54" to="62" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A theory of the learnable</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1134" to="1142" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Estimation of Dependencies Based on Empirical Data</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
			<publisher>Springer Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Linear regression diagnostics</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Welsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kuh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Paper</title>
		<imprint>
			<date type="published" when="1977">1977</date>
			<biblScope unit="volume">173</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Principal component analysis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Esbensen Wold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Geladi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
		<respStmt>
			<orgName>Chemometrics and Intelligent Laboratory Systems</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
