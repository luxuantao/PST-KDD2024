<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Meta Relational Learning for Few-Shot Link Prediction in Knowledge Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mingyang</forename><surname>Chen</surname></persName>
							<email>mingyangchen@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wen</forename><surname>Zhang</surname></persName>
							<email>wenzhang2015@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group 3 AZFT Joint Lab for Knowledge Engine</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qiang</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group 3 AZFT Joint Lab for Knowledge Engine</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
							<email>huajunsir@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Meta Relational Learning for Few-Shot Link Prediction in Knowledge Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Link prediction is an important way to complete knowledge graphs (KGs), while embedding-based methods, effective for link prediction in KGs, perform poorly on relations that only have a few associative triples. In this work, we propose a Meta Relational Learning (MetaR) framework to do the common but challenging few-shot link prediction in KGs, namely predicting new triples about a relation by only observing a few associative triples. We solve few-shot link prediction by focusing on transferring relation-specific meta information to make model learn the most important knowledge and learn faster, corresponding to relation meta and gradient meta respectively in MetaR. Empirically, our model achieves stateof-the-art results on few-shot link prediction KG benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A knowledge graph is composed by a large amount of triples in the form of (head entity, relation, tail entity) ((h, r, t) in short), encoding knowledge and facts in the world. Many KGs have been proposed <ref type="bibr" target="#b18">(Vrandei and Krtzsch, 2014;</ref><ref type="bibr" target="#b0">Bollacker et al., 2008;</ref><ref type="bibr" target="#b3">Carlson et al., 2010)</ref> and applied to various applications <ref type="bibr" target="#b2">(Bordes et al., 2014;</ref><ref type="bibr" target="#b24">Zhang et al., 2016</ref><ref type="bibr" target="#b25">Zhang et al., , 2019a))</ref>.</p><p>Although with huge amount of entities, relations and triples, many KGs still suffer from incompleteness, thus knowledge graph completion is vital for the development of KGs. One of knowledge graph completion tasks is link prediction, predicting new triples based on existing ones. For link prediction, KG embedding methods <ref type="bibr" target="#b1">(Bordes et al., 2013;</ref><ref type="bibr" target="#b13">Nickel et al., 2011;</ref><ref type="bibr" target="#b16">Trouillon et al., 2016;</ref><ref type="bibr" target="#b23">Yang et al., 2015)</ref> are * Equal contribution. † Corresponding author. promising ways. They learn latent representations, called embeddings, for entities and relations in continuous vector space and accomplish link prediction via calculation with embeddings.</p><p>The effectiveness of KG embedding methods is promised by sufficient training examples, thus results are much worse for elements with a few instances during training <ref type="bibr" target="#b27">(Zhang et al., 2019c)</ref>. However, few-shot problem widely exists in KGs. For example, about 10% of relations in Wikidata <ref type="bibr" target="#b18">(Vrandei and Krtzsch, 2014)</ref> have no more than 10 triples. Relations with a few instances are called few-shot relations. In this paper, we devote to discuss few-shot link prediction in knowledge graphs, predicting tail entity t given head entity h and relation r by only observing K triples about r, usually K is small. Figure <ref type="figure" target="#fig_0">1</ref> depicts an example of 3-shot link prediction in KGs.</p><p>To do few-shot link prediction, <ref type="bibr" target="#b22">Xiong et al. (2018)</ref> made the first trial and proposed GMatching, learning a matching metric by considering both learned embeddings and one-hop graph structures, while we try to accomplish few-shot link prediction from another perspective based on the intuition that the most important information to be transferred from a few existing instances to incomplete triples should be the common and shared knowledge within one task. We call such information relation-specific meta information and propose a new framework Meta Relational Learning (MetaR) for few-shot link prediction. For example, in Figure <ref type="figure" target="#fig_0">1</ref>, relation-specific meta information related to the relation CEOof or CountryCapital will be extracted and transferred by MetaR from a few existing instances to incomplete triples.</p><p>The relation-specific meta information is helpful in the following two perspectives: 1) transferring common relation information from observed triples to incomplete triples, 2) accelerating the learning process within one task by observing only a few instances. Thus we propose two kinds of relation-specific meta information: relation meta and gradient meta corresponding to afore mentioned two perspectives respectively. In our proposed framework MetaR, relation meta is the highorder representation of a relation connecting head and tail entities. Gradient meta is the loss gradient of relation meta which will be used to make a rapid update before transferring relation meta to incomplete triples during prediction.</p><p>Compared with GMatching <ref type="bibr" target="#b22">(Xiong et al., 2018</ref>) which relies on a background knowledge graph, our MetaR is independent with them, thus is more robust as background knowledge graphs might not be available for few-shot link prediction in real scenarios.</p><p>We evaluate MetaR with different settings on few-shot link prediction datasets. MetaR achieves state-of-the-art results, indicating the success of transferring relation-specific meta information in few-shot link prediction tasks. In summary, main contributions of our work are three-folds:</p><p>• Firstly, we propose a novel meta relational learning framework (MetaR) to address fewshot link prediction in knowledge graphs.</p><p>• Secondly, we highlight the critical role of relation-specific meta information for fewshot link prediction, and propose two kinds of relation-specific meta information, relation meta and gradient meta. Experiments show that both of them contribute significantly.</p><p>• Thirdly, our MetaR achieves state-of-the-art results on few-shot link prediction tasks and we also analyze the facts that affect MetaR's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>One target of MetaR is to learn the representation of entities fitting the few-shot link prediction task and the learning framework is inspired by knowledge graph embedding methods. Furthermore, using loss gradient as one kind of meta information is inspired by MetaNet <ref type="bibr" target="#b12">(Munkhdalai and Yu, 2017)</ref> and MAML <ref type="bibr" target="#b5">(Finn et al., 2017)</ref> which explore methods for few-shot learning by metalearning. From these two points, we regard knowledge graph embedding and meta-learning as two main kinds of related work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Knowledge Graph Embedding</head><p>Knowledge graph embedding models map relations and entities into continuous vector space.</p><p>They use a score function to measure the truth value of each triple (h, r, t). Same as knowledge graph embedding, our MetaR also need a score function, and the main difference is that representation for r is the learned relation meta in MetaR rather than embedding of r as in normal knowledge graph embedding methods. One line of work is started by TransE <ref type="bibr" target="#b1">(Bordes et al., 2013)</ref> with distance score function. TransH <ref type="bibr" target="#b20">(Wang et al., 2014)</ref> and TransR <ref type="bibr" target="#b10">(Lin et al., 2015b)</ref> are two typical models using different methods to connect head, tail entities and their relations. DistMult <ref type="bibr" target="#b23">(Yang et al., 2015)</ref> and Com-plEx <ref type="bibr" target="#b16">(Trouillon et al., 2016)</ref> are derived from RESCAL <ref type="bibr" target="#b13">(Nickel et al., 2011)</ref>, trying to mine latent semantics in different ways. There are also some others like ConvE <ref type="bibr" target="#b4">(Dettmers et al., 2018)</ref> using convolutional structure to score triples and models using additional information such as entity types <ref type="bibr" target="#b21">(Xie et al., 2016)</ref> and relation paths <ref type="bibr" target="#b9">(Lin et al., 2015a)</ref>. <ref type="bibr" target="#b19">Wang et al. (2017)</ref> comprehensively summarize the current popular knowledge graph embedding methods.</p><p>Traditional embedding models are heavily rely on rich training instances <ref type="bibr" target="#b26">(Zhang et al., 2019b;</ref><ref type="bibr" target="#b22">Xiong et al., 2018)</ref>, thus are limited to do few-shot link prediction. Our MetaR is designed to fill this vulnerability of existing embedding models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Meta-Learning</head><p>Meta-learning seeks for the ability of learning quickly from only a few instances within the same concept and adapting continuously to more concepts, which are actually the rapid and incremental learning that humans are very good at.</p><p>Several meta-learning models have been proposed recently. Generally, there are three kinds of meta-learning methods so far: (1) Metric-based meta-learning <ref type="bibr" target="#b7">(Koch et al., 2015;</ref><ref type="bibr" target="#b17">Vinyals et al., 2016;</ref><ref type="bibr" target="#b15">Snell et al., 2017;</ref><ref type="bibr" target="#b22">Xiong et al., 2018)</ref>, which tries to learn a matching metric between query and support set generalized to all tasks, where the idea of matching is similar to some nearest neighbors algorithms. Siamese Neural Network <ref type="bibr" target="#b7">(Koch et al., 2015)</ref> is a typical method using symmetric twin networks to compute the metric of two inputs. GMatching <ref type="bibr" target="#b22">(Xiong et al., 2018)</ref>, the first trial on one-shot link prediction in knowledge graphs, learns a matching metric based on entity embeddings and local graph structures which also can be regarded as a metric-based method. (2) Model-based method <ref type="bibr" target="#b14">(Santoro et al., 2016;</ref><ref type="bibr" target="#b12">Munkhdalai and Yu, 2017;</ref><ref type="bibr" target="#b11">Mishra et al., 2018)</ref>, which uses a specially designed part like memory to achieve the ability of learning rapidly by only a few training instances.</p><p>MetaNet <ref type="bibr" target="#b12">(Munkhdalai and Yu, 2017)</ref>, a kind of memory augmented neural network (MANN), acquires meta information from loss gradient and generalizes rapidly via its fast parameterization. (3) Optimization-based approach <ref type="bibr" target="#b5">(Finn et al., 2017;</ref><ref type="bibr" target="#b8">Lee and Choi, 2018)</ref>, which gains the idea of learning faster by changing the optimization algorithm. Model-Agnostic Meta-Learning <ref type="bibr" target="#b5">(Finn et al., 2017)</ref> abbreviated as MAML is a model-agnostic algorithm. It firstly updates parameters of task-specific learner, and meta-optimization across tasks is performed over parameters by using above updated parameters, it's like "a gradient through a gradient".</p><p>As far as we know, work proposed by <ref type="bibr" target="#b22">Xiong et al. (2018)</ref> is the first research on few-shot learning for knowledge graphs. It's a metric-based model which consists of a neighbor encoder and a matching processor. Neighbor encoder enhances the embedding of entities by their one-hop neighbors, and matching processor performs a multistep matching by a LSTM block.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Formulation</head><p>In this section, we present the formal definition of a knowledge graph and few-shot link prediction task. A knowledge graph is defined as follows:</p><formula xml:id="formula_0">Definition 3.1 (Knowledge Graph G) A knowl- edge graph G = {E, R, T P}. E is the entity set. R is the relation set. And T P = {(h, r, t) ∈ E × R × E} is the triple set.</formula><p>And a few-shot link prediction task in knowledge graphs is defined as:</p><formula xml:id="formula_1">Definition 3.2 (Few-shot link prediction task T ) With a knowledge graph G = {E, R, T P}, given a support set S r = {(h i , t i ) ∈ E × E|(h i , r, t i ) ∈ T P} about relation r ∈ R</formula><p>, where |S r | = K, predicting the tail entity linked with relation r to head entity h j , formulated as r : (h j , ?), is called Kshot link prediction.</p><p>As defined above, a few-shot link prediction task is always defined for a specific relation. During prediction, there usually is more than one triple to be predicted, and with support set S r , we call the set of all triples to be predicted as query set Q r = {r : (h j , ?)}.</p><p>The goal of a few-shot link prediction method is to gain the capability of predicting new triples about a relation r with only observing a few triples about r. Thus its training process is based on a set of tasks T train = {T i } M i=1 where each task T i = {S i , Q i } corresponds to an individual fewshot link prediction task with its own support and query set. Its testing process is conducted on a set of new tasks T test = {T j } N j=1 which is similar to T train , other than that T j ∈ T test should be about relations that have never been seen in T train .</p><p>Embedding Learner  </p><formula xml:id="formula_2">R ′ T r R ′ T r Relation-Meta Learner G T r G T r Embedding Learner Support Step Query Step R T r R T r</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head><p>To make one model gain the few-shot link prediction capability, the most important thing is transferring information from support set to query set and there are two questions for us to think about:</p><p>(1) what is the most transferable and common information between support set and query set and (2) how to learn faster by only observing a few instances within one task. For question (1), within one task, all triples in support set and query set are about the same relation, thus it is naturally to suppose that relation is the key common part between support and query set. For question (2), the learning process is usually conducted by minimizing a loss function via gradient descending, thus gradients reveal how the model's parameters should be changed. Intuitively, we believe that gradients are valuable source to accelerate learning process.</p><p>Based on these thoughts, we propose two kinds of meta information which are shared between support set and query set to deal with above problems:</p><p>• Relation Meta represents the relation connecting head and tail entities in both support and query set and we extract relation meta for each task, represented as a vector, from support set and transfer it to query set.</p><p>• Gradient Meta is the loss gradient of relation meta in support set. As gradient meta shows how relation meta should be changed in order to reach a loss minima, thus to accelerate the learning process, relation meta is updated through gradient meta before being transferred to query set. This update can be viewed as the rapid learning of relation meta.</p><p>In order to extract relation meta and gradient mate and incorporate them with knowledge graph embedding to solve few-shot link prediction, our proposal, MetaR, mainly contains two modules:</p><p>• Relation-Meta Learner generates relation meta from heads' and tails' embeddings in the support set.</p><p>• Embedding Learner calculates the truth values of triples in support set and query set via entity embeddings and relation meta. Based on the loss function in embedding learner, gradient meta is calculated and a rapid update for relation meta will be implemented before transferring relation meta to query set.</p><p>The overview and algorithm of MetaR are shown in Figure <ref type="figure" target="#fig_1">2</ref> and Algorithm 1. Next, we introduce each module of MetaR via one few-shot link prediction task T r = {S r , Q r }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Relation-Meta Learner</head><p>To extract the relation meta from support set, we design a relation-meta learner to learn a mapping from head and tail entities in support set to relation meta. The structure of this relation-meta learner can be implemented as a simple neural network.</p><p>In task T r , the input of relation-meta learner is head and tail entity pairs in support set {(h i , t i ) ∈ S r }. We firstly extract entity-pair specific relation Update φ and emb by loss in Q r 9: end while meta via a L-layers fully connected neural network,</p><formula xml:id="formula_3">x 0 = h i ⊕ t i x l = σ(W l x l−1 + b l ) R (h i ,t i ) = W L x L−1 + b L (1)</formula><p>where h i ∈ R d and t i ∈ R d are embeddings of head entity h i and tail entity t i with dimension d respectively. L is the number of layers in neural network, and l ∈ {1, . . . , L − 1}. W l and b l are weights and bias in layer l. We use LeakyReLU for activation σ. x⊕y represents the concatenation of vector x and y. Finally, R (h i ,t i ) represent the relation meta from specific entity pare h i and t i .</p><p>With multiple entity-pair specific relation meta, we generate the final relation meta in current task via averaging all entity-pair specific relation meta in current task,</p><formula xml:id="formula_4">R Tr = K i=1 R (h i ,t i ) K (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Embedding Learner</head><p>As we want to get gradient meta to make a rapid update on relation meta, we need a score function to evaluate the truth value of entity pairs under specific relations and also the loss function for current task. We apply the key idea of knowledge graph embedding methods in our embedding learner, as they are proved to be effective on evaluating truth value of triples in knowledge graphs.</p><p>In task T r , we firstly calculate the score for each entity pairs (h i , t i ) in support set S r as follows:</p><formula xml:id="formula_5">s (h i ,t i ) = h i + R Tr − t i (3)</formula><p>where x represents the L2 norm of vector x.</p><p>We design the score function inspired by TransE <ref type="bibr" target="#b1">(Bordes et al., 2013)</ref> which assumes the head entity embedding h, relation embedding r and tail entity embedding t for a true triple (h, r, t) satisfying h+r = t. Thus the score function is defined according to the distance between h + r and t.</p><p>Transferring to our few-show link prediction task, we replace the relation embedding r with relation meta R Tr as there is no direct general relation embeddings in our task and R Tr can be regarded as the relation embedding for current task T r . With score function for each triple, we set the following loss,</p><formula xml:id="formula_6">L(S r ) = (h i ,t i )∈Sr [γ + s (h i ,t i ) − s (h i ,t ′ i ) ] + (4)</formula><p>where [x] + represents the positive part of x and γ represents margin which is a hyperparameter.</p><formula xml:id="formula_7">s (h i ,t ′ i )</formula><p>is the score for negative sample (h i , t ′ i ) corresponding to current positive entity pair (h i , t i ) ∈ S r , where (h i , r, t ′ i ) / ∈ G. L(S r ) should be small for task T r which represents the model can properly encode truth values of triples. Thus gradients of parameters indicate how should the parameters be updated. Thus we regard the gradient of R Tr based on L(S r ) as gradient meta G Tr :</p><formula xml:id="formula_8">G Tr = ∇ R Tr L(S r )<label>(5)</label></formula><p>Following the gradient update rule, we make a rapid update on relation meta as follows:</p><formula xml:id="formula_9">R ′ Tr = R Tr − βG Tr (6)</formula><p>where β indicates the step size of gradient meta when operating on relation meta.</p><p>When scoring the query set by embedding learner, we use updated relation meta. After getting the updated relation meta R ′ , we transfer it to samples in query set Q r = {(h j , t j )} and calculate their scores and loss of query set, following the same way in support set:</p><formula xml:id="formula_10">s (h j ,t j ) = h j + R ′ Tr − t j (7) L(Q r ) = (h j ,t j )∈Qr [γ + s (h j ,t j ) − s (h j ,t ′ j ) ] + (8)</formula><p>where L(Q r ) is our training objective to be minimized. We use this loss to update the whole model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training Objective</head><p>During training, our objective is to minimize the following loss L which is the sum of query loss for all tasks in one minibatch:</p><formula xml:id="formula_11">L = (Sr,Qr)∈T train L(Q r ) (9)</formula><p>5 Experiments</p><p>With MetaR, we want to figure out following things: 1) can MetaR accomplish few-shot link prediction task and even perform better than previous model? 2) how much relation-specific meta information contributes to few-shot link prediction? 3) is there any requirement for MetaR to work on few-shot link prediction? To do these, we conduct the experiments on two few-shot link prediction datasets and deeply analyze the experiment results<ref type="foot" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and Evaluation Metrics</head><p>We use two datasets, NELL-One and Wiki-One which are constructed by <ref type="bibr" target="#b22">Xiong et al. (2018)</ref>. NELL-One and Wiki-One are derived from NELL <ref type="bibr" target="#b3">(Carlson et al., 2010)</ref> and Wikidata <ref type="bibr" target="#b18">(Vrandei and Krtzsch, 2014)</ref>   graph originally, we can make use of such background graph by fitting it into training tasks or using it to train embeddings to initialize entity representations. Overall, we have three kinds of dataset settings, shown in Table <ref type="table" target="#tab_5">3</ref>. For setting of BG:In-Train, in order to make background graph included in training tasks, we sample tasks from triples in background graph and original training set, rather than sampling from only original training set.</p><p>Note that these three settings don't violate the task formulation of few-shot link prediction in KGs. The statistics of NELL-One and Wiki-One are shown in Table <ref type="table">2</ref>.</p><p>We use two traditional metrics to evaluate different methods on these datasets, MRR and Hits@N. MRR is the mean reciprocal rank and Hits@N is the proportion of correct entities ranked in the top N in link prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation</head><p>During training, mini-batch gradient descent is applied with batch size set as 64 and 128 for NELL-One and Wiki-One respectively. We use Adam <ref type="bibr" target="#b6">(Kingma and Ba, 2015)</ref> with the initial learning rate as 0.001 to update parameters. We set γ = 1 and β = 1. The number of positive and negative triples in query set is 3 and 10 in NELL-One and Wiki-One. Trained model will be applied on validation tasks each 1000 epochs, and the current model parameters and corresponding performance will be recorded, after stopping, the model that has the best performance on Hits@10 will be treated as final model. For number of training epoch, we use early stopping with 30 patient epochs, which means that we stop the training when the performance on Hits@10 drops 30 times continuously. Following GMatching, the embedding dimension of NELL-One is 100 and Wiki-One is 50. The MRR Hits@10 Hits@5 Hits@1 NELL-One 1-shot 5-shot 1-shot 5-shot 1-shot 5-shot 1-shot 5-shot sizes of two hidden layers in relation-meta learner are 500, 200 and 250, 100 for NELL-One and Wiki-One.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GMatching</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>The results of two few-shot link prediction tasks, including 1-shot and 5-shot, on NELL-One and Wiki-One are shown in Table <ref type="table" target="#tab_6">4</ref>. The baseline in our experiment is GMatching <ref type="bibr" target="#b22">(Xiong et al., 2018)</ref>, which made the first trial on few-shot link prediction task and is the only method that we can find as baseline. In this table, results of GMatching with different KG embedding initialization are copied from the original paper. Our MetaR is tested on different settings of datasets introduced in Table <ref type="table" target="#tab_5">3</ref>.</p><p>In Table <ref type="table" target="#tab_6">4</ref>, our model performs better with all evaluation metrics on both datasets. Specifically, for 1-shot link prediction, MetaR increases by 33%, 28.1%, 29.2% and 27.8% on MRR, Hits@10, Hits@5 and Hits@1 on NELL-One, and 41.4%, 18.8%, 37.9% and 62.2% on Wiki-One, with average improvement of 29.53% and 40.08% respectively. For 5-shot, MetaR increases by 29.9%, 40.5%, 32.6% and 17.5% on MRR, Hits@10, Hits@5 and Hits@1 on NELL-One with average improvement of 30.13%.</p><p>Thus for the first question we want to ex- Table <ref type="table">5</ref>: Results of ablation study on Hits@10 of 1-shot link prediction in NELL-One.</p><p>plore, the results of MetaR are no worse than GMatching, indicating that MetaR has the capability of accomplishing few-shot link prediction.</p><p>In parallel, the impressive improvement compared with GMatching demonstrates that the key idea of MetaR, transferring relation-specific meta information from support set to query set, works well on few-shot link prediction task.</p><p>Furthermore, compared with GMatching, our MetaR is independent with background knowledge graphs. We test MetaR on 1-shot link prediction in partial NELL-One and Wiki-One which discard the background graph, and get the results of 0.279 and 0.348 on Hits@10 respectively. Such results are still comparable with GMatching in fully datasets with background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study</head><p>We have proved that relation-specific meta information, the key point of MetaR, successfully contributes to few-shot link prediction in previous section. As there are two kinds of relation-specific meta information in this paper, relation meta and gradient meta, we want to figure out how these two kinds of meta information contribute to the performance. Thus, we conduct an ablation study with three settings. The first one is our complete MetaR method denoted as standard. The second one is removing the gradient meta by transferring un-updated relation meta directly from support set to query set without updating it via gradient meta, denoted as -g. The third one is removing the relation meta further which makes the model rebase to a simple TransE embedding model, denoted as -g -r. The result under the third setting is copied from <ref type="bibr" target="#b22">Xiong et al. (2018)</ref>. It uses the triples from background graph, training tasks and one-shot training triples from validation/test set, so it's neither BG:Pre-Train nor BG:In-Train. We conduct the ablation study on NELL-one with metric Hit@10 and results are shown in Table <ref type="table">5</ref>.</p><p>Table <ref type="table">5</ref> shows that removing gradient meta decreases 29.3% and 15% on two dataset settings, and further removing relation meta continuous decreases the performance with 55% and 72% compared to the standard results. Thus both relation meta and gradient meta contribute significantly and relation meta contributes more than gradient meta. Without gradient meta and relation meta, there is no relation-specific meta information transferred in the model and it almost doesn't work. This also illustrates that relationspecific meta information is important and effective for few-shot link prediction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Facts That Affect MetaR's Performance</head><p>We have proved that both relation meta and gradient meta surely contribute to few-shot link prediction. But is there any requirement for MetaR to ensure the performance on few-shot link prediction? We analyze this from two points based on the results, one is the sparsity of entities and the other is the number of tasks in training set.</p><p>The sparsity of entities We notice that the best result of NELL-One and Wiki-One appears in different dataset settings. With NELL-One, MetaR performs better on BG:In-Train dataset setting, while with Wiki-One, it performs better on BG:Pre-Train. Performance difference between two dataset settings is more significant on Wiki-One.</p><p>Most datasets for few-shot task are sparse and the same with NELL-One and Wiki-One, but the entity sparsity in these two datasets are still significantly different, which is especially reflected in the proportion of entities that only appear in one triple in training set, 82.8% and 37.1% in Wiki-One and NELL-One respectively. Entities only have one triple during training will make MetaR unable to learn good representations for them, because entity embeddings heavily rely on triples related to them in MetaR. Only based on one triple, the learned entity embeddings will include a lot of bias. Knowledge graph embedding method can learn better embeddings than MetaR for those oneshot entities, because entity embeddings can be corrected by embeddings of relations that connect to it, while they can't in MetaR. This is why the best performance occurs in BG:Pre-train setting on Wiki-One, pre-train entity embeddings help MetaR overcome the low-quality on one-shot entities.</p><p>The number of tasks From the comparison of MetaR's performance between with and without background dataset setting on NELL-One, we find that the number of tasks will affect MetaR's performance significantly. With BG:In-Train, there are 321 tasks during training and MetaR achieves 0.401 on Hits@10, while without background knowledge, there are 51, with 270 less, and MetaR achieves 0.279. This makes it reasonable that why MetaR achieves best performance on BG:In-Train with NELL-One. Even NELL-One has 37.1% one-shot entities, adding background knowledge into dataset increases the number of training tasks significantly, which complements the sparsity problem and contributes more to the task.</p><p>Thus we conclude that both the sparsity of entities and number of tasks will affect performance of MetaR. Generally, with more training tasks, MetaR performs better and for extremely sparse dataset, pre-train entity embeddings are preferred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose a meta relational learning framework to do few-shot link prediction in KGs, and we design our model to transfer relation-specific meta information from support set to query set. Specif-ically, using relation meta to transfer common and important information, and using gradient meta to accelerate learning. Compared to GMatching which is the only method in this task, our method MetaR gets better performance and it is also independent with background knowledge graphs. Based on experimental results, we analyze that the performance of MetaR will be affected by the number of training tasks and sparsity of entities. We may consider obtaining more valuable information about sparse entities in few-shot link prediction in KGs in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of 3-shot link prediction in KGs. One task represents observing only three instances of one specific relation and conducting link prediction on this relation. Our model focuses on extracting relationspecific meta information by a kind of relational learner which is shared across tasks and transferring this meta information to do link prediction within one task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of MetaR. T r = {S r , Q r }, R Tr and R ′ Tr represent relation meta and updated relation meta, and G Tr represents gradient meta.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1</head><label>1</label><figDesc>Learning of MetaR Require: Training tasks T train Require: Embedding layer emb; Parameter of relation-meta learner φ 1: while not done do 2: Sample a task T r = {S r , Q r } from T train 3: Get R from S r (Equ. 1, Equ. 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The training and testing examples of 1-shot link prediction in KGs.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>gives a concrete example of the data during learning and testing for few-shot link prediction.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Three forms of datasets in our experiments.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Results of few-shot link prediction in NELL-One and Wiki-One. Bold numbers are the best results of all and underline numbers are the best results of GMatching. The contents of (bracket) after MetaR illustrate the form of datasets we use for MetaR.</figDesc><table><row><cell>RESCAL</cell><cell>.188</cell><cell>-</cell><cell>.305</cell><cell>-</cell><cell>.243</cell><cell>-</cell><cell>.133</cell><cell>-</cell></row><row><cell>GMatching TransE</cell><cell>.171</cell><cell>-</cell><cell>.255</cell><cell>-</cell><cell>.210</cell><cell>-</cell><cell>.122</cell><cell>-</cell></row><row><cell>GMatching DistMult</cell><cell>.171</cell><cell>-</cell><cell>.301</cell><cell>-</cell><cell>.221</cell><cell>-</cell><cell>.114</cell><cell>-</cell></row><row><cell>GMatching ComplEx</cell><cell>.185</cell><cell>.201</cell><cell>.313</cell><cell>.311</cell><cell>.260</cell><cell>.264</cell><cell>.119</cell><cell>.143</cell></row><row><cell>GMatching Random</cell><cell>.151</cell><cell>-</cell><cell>.252</cell><cell>-</cell><cell>.186</cell><cell>-</cell><cell>.103</cell><cell>-</cell></row><row><cell cols="2">MetaR (BG:Pre-Train) .164</cell><cell>.209</cell><cell>.331</cell><cell>.355</cell><cell>.238</cell><cell>.280</cell><cell>.093</cell><cell>.141</cell></row><row><cell>MetaR (BG:In-Train)</cell><cell>.250</cell><cell>.261</cell><cell>.401</cell><cell>.437</cell><cell>.336</cell><cell>.350</cell><cell>.170</cell><cell>.168</cell></row><row><cell>Wiki-One</cell><cell cols="8">1-shot 5-shot 1-shot 5-shot 1-shot 5-shot 1-shot 5-shot</cell></row><row><cell>GMatching RESCAL</cell><cell>.139</cell><cell>-</cell><cell>.305</cell><cell>-</cell><cell>.228</cell><cell>-</cell><cell>.061</cell><cell>-</cell></row><row><cell>GMatching TransE</cell><cell>.219</cell><cell>-</cell><cell>.328</cell><cell>-</cell><cell>.269</cell><cell>-</cell><cell>.163</cell><cell>-</cell></row><row><cell>GMatching DistMult</cell><cell>.222</cell><cell>-</cell><cell>.340</cell><cell>-</cell><cell>.271</cell><cell>-</cell><cell>.164</cell><cell>-</cell></row><row><cell>GMatching ComplEx</cell><cell>.200</cell><cell>-</cell><cell>.336</cell><cell>-</cell><cell>.272</cell><cell>-</cell><cell>.120</cell><cell>-</cell></row><row><cell>GMatching Random</cell><cell>.198</cell><cell>-</cell><cell>.299</cell><cell>-</cell><cell>.260</cell><cell>-</cell><cell>.133</cell><cell>-</cell></row><row><cell cols="2">MetaR (BG:Pre-Train) .314</cell><cell>.323</cell><cell>.404</cell><cell>.418</cell><cell>.375</cell><cell>.385</cell><cell>.266</cell><cell>.270</cell></row><row><cell>MetaR (BG:In-Train)</cell><cell>.193</cell><cell>.221</cell><cell>.280</cell><cell>.302</cell><cell>.233</cell><cell>.264</cell><cell>.152</cell><cell>.178</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">The source code of experiments is available at https://github.com/AnselCmy/MetaR</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We want to express gratitude to the anonymous reviewers for their hard work and kind comments, which will further improve our work in the future.</p><p>This work is funded by NSFC 91846204/61473260, national key research program YS2018YFB140004, and Alibaba CangJingGe(Knowledge Engine) Research Plan.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Freebase: A collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data</title>
				<meeting>the 2008 ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Open question answering with weakly supervised embedding models</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European conference on machine learning and knowledge discovery in databases</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="165" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Toward an architecture for never-ending language learning</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Estevam R Hruschka</surname></persName>
		</author>
		<author>
			<persName><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Fourth AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML deep learning workshop</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gradient-based meta-learning with learned layerwise metric and subspace</title>
		<author>
			<persName><forename type="first">Yoonho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungjin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2933" to="2942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modeling relation paths for representation learning of knowledge bases</title>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siwei</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015a</date>
			<biblScope unit="page" from="705" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2015">2015b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A simple neural attentive metalearner</title>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Meta networks</title>
		<author>
			<persName><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2554" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Metalearning with memory-augmented neural networks</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Éric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Denny</forename><surname>Vrandei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Krtzsch</surname></persName>
		</author>
		<title level="m">Wikidata: A free collaborative knowledge base. Communications of the ACM</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="78" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Eighth AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Representation learning of knowledge graphs with hierarchical types</title>
		<author>
			<persName><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2965" to="2971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">One-shot relational learning for knowledge graphs</title>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1980" to="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Collaborative knowledge base embedding for recommender systems</title>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Jing Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="353" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Long-tail relation extraction via knowledge graph embeddings and graph convolution networks</title>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanlin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3016" to="3025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Iteratively learning embeddings and rules for knowledge graph reasoning</title>
		<author>
			<persName><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bibek</forename><surname>Paudel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaoyan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abraham</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019b</date>
			<biblScope unit="page" from="2366" to="2377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Interaction embeddings for prediction and explanation in knowledge graphs</title>
		<author>
			<persName><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bibek</forename><surname>Paudel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abraham</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining</title>
				<meeting>the Twelfth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019c</date>
			<biblScope unit="page" from="96" to="104" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
