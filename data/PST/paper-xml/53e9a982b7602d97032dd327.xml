<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">and leads the Web Information Retrieval/ Natural Language Processing Group (WING)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Yee</forename><forename type="middle">Fan</forename><surname>Tan</surname></persName>
							<email>tanyeefa@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">and leads the Web Information Retrieval/ Natural Language Processing Group (WING)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">79E9EB3A9F812446A6E394E0DEDF5370</idno>
					<idno type="DOI">10.1145/1314215.1314231</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>W hen data stores grow large, data quality, cleaning, and integrity become issues. The commercial sector spends a massive amount of time and energy canonicalizing customer and product records as their lists of products and consumers expand. An Accenture study in 2006 found that a high-tech equipment manufacturer saved $6 million per year by removing redundant customer records used in customer mailings. In 2000, the U.K. Ministry of Defence embarked on the massive "The Cleansing Project," solving key problems with its inventory and logistics and saving over $25 million over four years.</p><p>In digital libraries, such problems manifest most urgently not in the customer, product, or item records, but in the metadata that describes the library's holdings. Several well-known citation lists of computer science research contain over 50% duplicate citations, although none of these duplicates are exact string matches <ref type="bibr" target="#b1">[2]</ref>. Without metadata cleaning, libraries might end up listing multiple records for the same item, causing circulation problems, and skewing the distribution of their holdings. In addition, when different authors share the same name (for example, Wei Wang, J. Brown), author disambiguation must be performed to correctly link authors to their respective monographs and articles, and not to others. Metadata inconsistencies can be due to problems with varying ordering of fields, type of delimiters used, omission of fields, multiple representations of names of people and organizations, and typographical errors.</p><p>When libraries import large volumes of metadata from sources that follow a metadata standard, a manually compiled set of rules called a crosswalk may be used to transform the metadata into the library's own format. However, such crosswalks are expensive to create manually, and public ones exist only for a few, well-used formats. Crucially, they also do not address how to detect and remove inexact duplicates. As digital libraries mine and incorporate data from a wider variety of sources, especially noisy sources, such as the Web, finding a suitable and scalable matching solution becomes critical.</p><p>Here, we examine this problem and its solutions. The de-duplication task takes a list of metadata records as input and returns the list with duplicate records removed. For example, the search results shown in the figure here are identical and should have been combined into a single entry. It should be noted that many disciplines of computer science have instances of similar inexact match-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Technical Opinion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Record Matching in Digital Library Metadata</head><p>Using evidence from external sources to create more accurate matching systems.</p><p>Min-Yen Kan and Yee Fan Tan ing problems, and as such this problem has many names, including de-duplication, data cleaning, disambiguation, record linkage, entity resolution, attribution, and plagiarism detection. While these variant problems differ in specifics, a common key operation is to determine whether two data records match. We explain this problem and generate awareness of the approaches espoused by different communities. For a detailed review, we urge readers to consult the individual papers and a more detailed survey paper <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UNINFORMED STRING MATCHING</head><p>In its most basic form, record matching can be simplified as string matching, which decides whether a pair of observed strings refer to the same underlying item. In such cases, we use the similarity between the strings to calculate whether they are coreferential. When such pairwise similarity measures are viewed as kernel-comparison operations, the record-matching problem can be cast as a clustering problem. If two records' similarity exceeds a threshold, they are considered two variants of the same item. String similarity measures can be classified as either set-or sequence-based, depending on whether or not ordering information is used.</p><p>Set-based similarity considers the two strings as independent sets of characters S and T, such as the Jaccard measure, defined as the ratio of the intersection of the sets over the union (see Equation <ref type="formula">1</ref>).</p><p>Cosine similarity, borrowed from information retrieval, views both sets as vectors and calculates the angle between the vectors, where a smaller angle indicates higher similarity. Alternatively, asymmetric measures, such as degree of similarity (see Equation <ref type="formula">2</ref>) may be more appropriate when one string is more important to match than the other.</p><p>Sequence-based measures can be generally cast as edit distances. They measure the cost of transforming one ordered string into the other. Typically, the transformation cost is measured by summing the cost of simple incremental operations, such as insertion, deletion, and substitution.</p><p>Hybrids of both set-and sequence-based measures are often used. For example, when the string is a series of words, a sequence-based measure may be employed for individual tokens, but the string as a whole may be modeled as a set of tokens <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INFORMED SIMILARITY AND RECORD MATCHING</head><p>Library metadata records contain a wide variety of data-personal names, URLs, controlled subject headers, publication names, and years. Viewing the list as a database table, each of these columns may have its own notions for what is considered acceptable variation ("Liz" = "Elizabeth"; "Comm. of the ACM" = "CACM"; 1996 ≠ 1997). Knowing what type of data exists in a column can inform us of what constitutes similarity and duplication. As such, string similarity measures are usually weighted differently per column.</p><p>Certain data types have been studied in depth. In fact, the need to consolidate records of names and addresses in government and industry pioneered research to find reliable rules and weights for record matching. In set-based similarity, tokens may be weighted with respect to their (log) frequency, as is done in information retrieval models. In sequence-based edit operations, a spectrum of weighting schemes has been used to capture regularities in the data, basically by varying the edit cost based on the position and input. For example, in genomic data, sequences often match even when a whole substring is inserted or deleted; the same is true when matching abbreviations to their full forms. In census data, the initial letters of people's names are rarely incorrect.</p><p>Such models need to set parameters, such as the cost for each type of edit operation in a principled way. Fortunately, data-driven methods have emerged to learn optimal weights from training data (see <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GRAPHICAL FORMALISMS FOR RECORD MATCHING</head><p>Graphical formalisms are becoming popular for record matching. Typically, columns or whole records are viewed as nodes in a graph with edges connecting similar nodes, allowing global information to be incorporated in the disambiguation process. One may assign similarity values to edges and identify cliques of high weights as matching nodes.</p><p>A common manifestation of graphical formalisms in disambiguation tasks is social networks, such as collaboration networks. Social network analysis methods, such as centrality and betweeness, can be applied. For example, in author disambiguation we may be able to attribute two papers to the same "Wei Wang" when the coauthor lists do not have common names but share names with a third paper; the two nodes are connected by a path through a third node. Yet another work uses network cuts and random walks in the collaboration network of actors to disambiguate names in the Internet Movie Database <ref type="bibr" target="#b6">[7]</ref>.</p><p>Consolidating records using one column of data can sometimes cascade and benefit matching on other columns of the same data. This incremental approach can resolve duplicates when true matching records do not exceed a global similarity threshold before individual fields in the records are merged. Graphical formalisms, such as dependency graphs <ref type="bibr" target="#b3">[4]</ref> or conditional random fields <ref type="bibr" target="#b10">[11]</ref>, nicely model incremental record matching, enabling the propagation of contextual similarity.</p><p>Graphical formalisms in the form of generative probabilistic models have also been suggested. In the author-disambiguation problem, we can view authors as members of collaborative groups. This model first picks out collaborative groups and then assigns authors within these groups to generate references. We can then run this model in the opposite direction to infer which collaborative group (thus which disambiguated author) is responsible for a particular work <ref type="bibr" target="#b0">[1]</ref>. Such graphical models have outperformed methods using pairwise compar-isons in accuracy but have yet to demonstrate efficiency on large data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REDUCING COMPLEXITY</head><p>Since digital libraries often contain large numbers of records, bruteforce pairwise comparison is often infeasible. As of 2005, the estimated number of independent articles and monographs in computer science research alone exceeded 2.2 million <ref type="bibr" target="#b8">[9]</ref>, an amount unsuited for O(n 2 ) complexity. (Log) linear time matching algorithms are needed to scale to such large metadata sets.</p><p>Observations show the ratio of true record matches to nonmatches is very low; it is very unlikely two randomly picked records refer to the same item. Thus, a computationally cheap similarity measure is often used to first separate such implausible matches. These blocking (or canopy) methods map records into a set of blocks in linear time. For example, we can construct a block for all records that have the token "J" and another block for all records that have the token "Brown." Records containing both tokens would be members of both blocks. More computationally expensive similarity measures can then be confined to run only within each block, where records have a non-zero probability of matching.</p><p>Constructing an optimal block algorithm requires tuning parameters for the proper number of blocks, overlap between blocks, and size of the blocks. These parameters can be either rigorously controlled to bound the complexity of the inner comparison <ref type="bibr" target="#b5">[6]</ref> or learned from data or sampling <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSION</head><p>Matching problems matured to become a research issue as early as the 1940s, probably due to the analysis of census data or medical records <ref type="bibr" target="#b5">[6]</ref>. Since then, advances have been made both on better theoretical models for weighted matching and proofs for error bounds and optimality.</p><p>One promising direction lies with graphical models, which can be tailored to model the underlying structure of the specific record-matching scenario. A difficulty in applying these models is in complexity; modeling the structure more accurately requires a more complex graphical model, which in turn creates complexity in the inference procedure. A way of reducing this complexity further would help propel these models for large-scale data sets.</p><p>Bringing more knowledge to bear on the problem may also help. Noisy sources, such as the Web, can be seen as a treasure trove of statistical information for matching-if carefully cleaned and utilized. This is especially fruitful for library metadata, as information about authors, titles, and publishers is readily available on the Web. Motivated by similar approaches in information retrieval research, we have leveraged Web search results when disambiguating author names in citation lists <ref type="bibr" target="#b9">[10]</ref>. Our study showed that using evidence from such external sources alone can achieve the same disambiguation performance as using the record data itself. We can also ask humans for help directly-by distinguishing which parts of the matching process are easier for humans to solve than machines. The classic Fellegi-Sunter model <ref type="bibr" target="#b11">[12]</ref> defines a check zone where uncertain matches are given to human experts to manually check. Similar to approaches used in computer vision, active learning based on manual disambiguation can help create more accurate matching systems. Elusive, domain-specific matching knowledge may be easier to capture by having human experts solve example problems rather than asking them to code explicit rules.</p><p>It is unclear whether advances in record matching have kept up with the pace at which information is becoming widely available today. In the world of digital libraries, metadata inconsistencies are a significant barrier to locating and collating knowledge that end users and reference librarians have had to adapt to. In some cases, humans resort to using external sources of information to (in)validate a possible match. As more information becomes Web accessible, we expect mining such external sources for knowledge will play an increasingly useful role in matching problems. Incorporating such external yet accessible knowledge gathering as an active component of matching algorithms will be a valuable research direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>92Equation 1 .Equation 2 .</head><label>12</label><figDesc>February 2008/Vol. 51, No. 2 COMMUNICATIONS OF THE ACM Technical Opinion Jaccard measure. Degree of similarity.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Searching for "computers and intractability" on Google Scholar.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A latent dirichlet model for unsupervised entity resolution</title>
		<author>
			<persName><forename type="first">I</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIAM International Conference on Data Mining</title>
		<meeting>the SIAM International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2006-04">Apr. 2006</date>
			<biblScope unit="page" from="47" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive duplicate detection using learnable string similarity measures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bilenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2003-08">Aug. 2003</date>
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A comparison of string distance metrics for name-matching tasks</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Fienberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Integration on the Web (IIWeb)</title>
		<imprint>
			<date type="published" when="2003-08">Aug. 2003</date>
			<biblScope unit="page" from="73" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reference reconciliation in complex information spaces</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Madhavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2005-06">June 2005</date>
			<biblScope unit="page" from="85" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Duplicate record detection: A survey</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Elmagarmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Verykios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering (TKDE)</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2007-01">Jan. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A generalization of band joins and the merge/purge problem</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hernández</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996-03">Mar. 1996</date>
		</imprint>
		<respStmt>
			<orgName>Columbia University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A network analysis model for disambiguation of names in lists</title>
		<author>
			<persName><forename type="first">B</forename><surname>Malin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Airoldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carley</forename></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational and Mathematical Organization Theory</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="139" />
			<date type="published" when="2005-07">July 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning blocking schemes for record linkage</title>
		<author>
			<persName><forename type="first">M</forename><surname>Michelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Knoblock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the National Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2006-07">July 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A comparison of on-line computer science citation databases</title>
		<author>
			<persName><forename type="first">V</forename><surname>Petricek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Research and Advanced Technology for Digital Libraries (ECDL)</title>
		<meeting>the European Conference on Research and Advanced Technology for Digital Libraries (ECDL)</meeting>
		<imprint>
			<date type="published" when="2005-09">Sept. 2005</date>
			<biblScope unit="page" from="438" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Search engine driven author disambiguation</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">F</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/IEEE Joint Conference on Digital Libraries (JCDL)</title>
		<meeting>the ACM/IEEE Joint Conference on Digital Libraries (JCDL)</meeting>
		<imprint>
			<date type="published" when="2006-06">June 2006</date>
			<biblScope unit="page" from="314" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An integrated, conditional model of information extraction and coreference with application to citation matching</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wellner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI)</title>
		<meeting>the Conference on Uncertainty in Artificial Intelligence (UAI)</meeting>
		<imprint>
			<date type="published" when="2004-07">July 2004</date>
			<biblScope unit="page" from="593" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">An application of the Fellegi-Sunter Model of record linkage to the 1990 U.S. Decennial Census</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Thibaudeau</surname></persName>
		</author>
		<idno>RR91/09</idno>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>U.S. Bureau of the Census</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
