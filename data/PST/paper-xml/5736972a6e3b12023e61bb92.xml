<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sentiment Analysis for Social Media Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yilin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<settlement>Tempe, Arizona</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Baoxin</forename><surname>Li</surname></persName>
							<email>baoxin.li@asu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<settlement>Tempe, Arizona</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sentiment Analysis for Social Media Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B315C2225E1D936F801797198393CB4F</idno>
					<idno type="DOI">10.1109/ICDMW.2015.142</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this proposal, we study the problem of understanding human sentiments from large scale collection of Internet images based on both image features and contextual social network information (such as friend comments and user description). Despite the great strides in analyzing user sentiment based on text information, the analysis of sentiment behind the image content has largely been ignored. Thus, we extend the significant advances in text-based sentiment prediction tasks to the higherlevel challenge of predicting the underlying sentiments behind the images. We show that neither visual features nor the textual features are by themselves sufficient for accurate sentiment labeling. Thus, we provide a way of using both of them, and formulate sentiment prediction problem in two scenarios: supervised and unsupervised. We develop an optimization algorithm for finding a local-optima solution under the proposed framework. With experiments on two large-scale datasets, we show that the proposed method improves significantly over existing state-of-the-art methods. In the future, we are going to incorporating more information on the social network and explore sentiment on signed social network.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A picture is worth a thousand words. It is surely worth even more when it comes to convey human emotions and sentiments. Examples that support this are abundant: great captivating photos often contain rich emotional cues that help viewers easily connect with those photos. With the advent of social media, an increasing number of people start to use photos to express their joy, grudge, and boredom on social media platforms like Flickr and Instagram. Automatic inference of the emotion and sentiment information from such ever-growing, massive amounts of user-generated photos is of increasing importance to many applications in health-care, anthropology, communication studies, marketing, and many sub-areas within computer science such as computer vision. Think about this: Emotional wellness impacts several aspects of people's lives. For example, it introduces self-empathy, giving an individual greater awareness of their feelings. It also improves one's self-esteem and resilience, allowing them to bounce back with ease, from poor emotional health, and physical stress and difficulty. As people are increasingly using photos to record their daily lives <ref type="foot" target="#foot_0">1</ref> , we can assess a person's emotional wellness based on the emotion and sentiment inferred from her photos on social media platforms (in addition to existing emotion/sentiment analysis effort, e.g., see <ref type="bibr" target="#b0">[1]</ref> on text-based social media).</p><p>In this paper, our goal is to automatically infer human sentiments (positive, neutral and negative) from photos shared on Flickr and Instagram. While sentiment analysis of photos is still in its infancy, a number of tools have been proposed during past two years. A popular approach is to identify visual features from a photo that are related to human sentiments, such as objects (e.g., toys, birthday cakes, gun), human actions (e.g., crying or laughing), and many other features like color temperature. However, such an approach is often insufficient because the same objects/actions may convey different sentiments in different photo contexts. For example, consider Figure <ref type="figure" target="#fig_0">1</ref>: one can easily detect the crying lady and girl (using computer vision algorithms such as face detection and expression recognition). However, the same "crying" action conveys two clearly different sentiments: the "crying" in Figure <ref type="figure" target="#fig_0">1a</ref> is obviously positive as the result of a successful marriage proposal. In contrast, the tearful girl in Figure <ref type="figure" target="#fig_0">1b</ref> looks quite unhappy thus expresses negative sentiment. In other words, the so-called "visual affective gap" <ref type="bibr" target="#b1">[2]</ref> exists between rudimentary visual features and human sentiment embedded in a photo. On the other hand, one may also consider inferring the sentiment of a photo via its textual descriptions (e.g., titles) using existing off-theshelf text-based sentiment analysis tools <ref type="bibr" target="#b2">[3]</ref>. Although these descriptions can provide very helpful context information of the photos, solely relying on them while ignoring the visual features of the photos can lead to poor performance as well. Consider Figure <ref type="figure" target="#fig_0">1</ref> again: by analyzing only the text description, we can conclude that both Figure <ref type="figure" target="#fig_0">1a</ref> and 1b convey negative sentiment as the keyword "crying" is often classified as negative sentiment in standard sentiment lexicon . Last, both visual feature-based and text-based sentiment analysis approaches require massive amounts of training data in order to learn high quality models. However, manually annotating the sentiment of a vast amount of photos and/or their textual descriptions is time consuming and error-prone, presenting a bottleneck in learning good models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we review the related work on sentiment analysis and the methods for matrix factorization. Sentiment analysis on text and images: Recently, sentiment analysis has shown its success in opinion mining on textual data, including product review <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, newspaper articles <ref type="bibr" target="#b2">[3]</ref>, and movie rating <ref type="bibr" target="#b5">[6]</ref>. Besides, there have been increasing interests in social media data <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b9">[10]</ref>, such as Twitter and Weibo data. Unlike text-based sentiment prediction approaches, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref> employed mid-level attributes of visual feature to model visual content for sentiment analysis. <ref type="bibr" target="#b7">[8]</ref> provides a method based on low-level visual features and social information via a topic model. While <ref type="bibr" target="#b8">[9]</ref> tries to solve the problem by a graphical model which is based on friend interactions. In contrast to our approach, all such methods restrict sentiment prediction to the specific data domain. For example, in Figure <ref type="figure" target="#fig_0">1</ref>, we can see that approaches using pure visual information <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref> may be confused by the subtle sentiment embedded in the image. e.g., two crying people convey totally different sentiment. <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> assume that the images belong to the same sentiment share the same low-level visual features is often not true, because positive and negative images may have similar low-level visual features, e.g., two black-white images contain smiling and sad faces respectively. Recent, deep learning has shown its success in feature learning for many computer vision problem, <ref type="bibr" target="#b10">[11]</ref> provides a transfer deep neutral network structure for sentiment analysis. However, for deep learning framework, millions of images with associated sentiment labels are needed for network training.In real world, such label information is not available and how to deal with overfitting for small training data remains a challenging problem.</p><p>Non-negative matrix factorization(NMF): Our proposed framework is also inspired by recent progress in matrix factorization algorithms. NMF has been shown to be useful in computer vision and data mining applications including face recognition <ref type="bibr" target="#b11">[12]</ref>, object detection <ref type="bibr" target="#b12">[13]</ref> and feature selection <ref type="bibr" target="#b13">[14]</ref>, etc. Specifically, the work in <ref type="bibr" target="#b14">[15]</ref> brings more attention to NMF in the research community, where the author proposed a simple multiplicative rule to solve the problem and showed the factor coherence of original image data. <ref type="bibr" target="#b15">[16]</ref> shows that if adding orthogonal constrains, the NMF is equivalent to K-means clustering. Further, <ref type="bibr" target="#b16">[17]</ref> presents a work that shows, when incorporating freedom control factors, the non-negative factors will achieve a better performance on classification. In this paper, motivated by previous NMF framework for learning the latent factors, we extend these efforts significantly and propose a comprehensive formulation which incorporates more physicallymeaningful constraints for regularizing the learning process in order to find a proper solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE PROPOSED FRAMEWORKS</head><p>The weaknesses discussed in the foregoing motivate the need for a more accurate automated framework to infer the sentiment of photos, with 1) considering the photo context to bridge the "visual affective gap", 2) considering a photo's visual features to augment text-based sentiment, and 3) considering the availability of textual information, thus a photo may have little or no social context (e.g., friend comments, user description). Here we provide two frameworks based on supervised learning and unsupervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supervised Sentiment Analysis</head><p>We propose an efficient and effective framework <ref type="bibr" target="#b17">[18]</ref>, named RSAI (Robust Sentiment Analysis for Images), for inferring human sentiment from photos that leverages these partial solutions. Specifically, to fill the visual affective gap, we first extract visual features from a photo using lowlevel visual features (e.g., color histograms) and a large number of mid-level (e.g., objects) visual attribute/object detectors. It is formed by merging the low-level visual features to the detected mid-level objects and mapping them to a dictionary. On the other hand, to learn the image's context, we analyze the image's textual description and capture its sentiment based on sentiment lexicons. Finally, with the help from ANPs and image context, RSAI infers the image's sentiment by factorizing an input image-features matrix into three factors corresponding to image-term, termsentiment and sentiment-features. Similarly, the learnt image context can be used to constrain image-term and termsentiment factors. Last, the availability of labeled sentiment of the images can be used to regulate the product of imageterm, term-sentiment factors. We pose this factorization as an optimization problem where, in addition to minimizing the reconstruction error, we also require that the factors respect the prior knowledge to the extent possible. We derive a set of multiplicative update rules that efficiently produce this factorization, and provide empirical comparisons with</p><formula xml:id="formula_0">Table I: Notations Notation Dimension Description X n × m Input data matrix T n × t Data-term matrix S t × k Term-sentiment matrix V m × k Feature-sentiment matrix T 0 n × t Prior knowledge on T S 0 t × k Prior knowledge on S V 0 m × k Prior knowledge on V R 0 n × k</formula><p>Prior knowledge on the labels several competing methodologies on two real datasets of photos from Flickr and Instagram. We examine the results both quantitatively and qualitatively to demonstrate that our method improves significantly over baseline approaches.</p><p>We assume that all the images can be partitioned into K sentiment (K = 3 in this paper as we focus on positive, neutral and negative. Our goal is to model the sentiment for each image based on visual features and available text features. Let n be the number of images and the size of contextual vocabulary is t. Table <ref type="table">1</ref> lists the mathematical notation used in this section.</p><p>Since, we can then easily cluster the images with similar word frequencies and predict the cluster's sentiment based on its word sentiment. Meanwhile, for each image, which has m-dimensional visual features, we can cluster the images and predict the sentiment based on the feature probability., our framework takes these n data points and decomposes them simultaneously into three factors: photo-text, textsentiment and visual feature-sentiment. In other words, our model tries to solve the following optimization problem:</p><formula xml:id="formula_1">min T SV X -T SV T 2 F + α V -V 0 2 F + β T -T 0 2 F + γ S -S 0 2 F + δ T S -R 0 2 F subject to T ≥ 0, S ≥ 0, V ≥ 0 (1)</formula><p>where X ∈ R n×m represents input data matrix, and T ∈ R n×t indicates the text features. That is, the ith row of matrix T corresponds to the posterior probability of the ith image's contextual social network information referring to the t text terms (vocabulary). Similarly, S ∈ R t×k indicates the posterior probability of a text belonging to k sentiments. Finally, V ∈ R m×k represents the sentiment for each ANP. The regularization term T 0 is the term-frequency matrix for the whole word vocabulary (which is built based on textual descriptions of all photos). It is worth noting that the nonnegativity makes the latent components easy to interpret. Besides, we also introduce three types of prior knowledge for model regularization: (1) sentiment-lexicon of textual words, (2) the normalized sentiment strength for each ANP, and (3) sentiment labels for each image. The first prior knowledge is from a public sentiment lexicon named MPQA corpus <ref type="foot" target="#foot_2">2</ref> . Since this corpus is constructed without respect to any specific domain, it provides a domain independent prior on word-sentiment association. It should be noted that the English usage in social network is very casual and irregular, we employ a stemmer technique proposed in <ref type="bibr" target="#b18">[19]</ref>. As a result, the ill-formed words can be detected and corrected based on morphophonemic similarity, for example "good" is a correct version of "goooooooooooood". Besides some abbreviation of popular words such as "lol"(means laughing out loud) is also added as prior knowledge. We encode the prior knowledge in a word sentiment matrix S 0 where if the i th word belongs to j th sentiment, then S 0 (i, j) = 1, otherwise it equals to zero. In addition to the prior knowledge on lexicon, our second prior knowledge comes from the Visual Sentiment Ontology (VSO) <ref type="bibr" target="#b6">[7]</ref>, which is based on the well known previous researches on human emotions and sentiments <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. The sample ANP sentiment scores are shown in Table <ref type="table">2</ref>. Similar to the word sentiment matrix S 0 , the prior knowledge on ANPs V 0 is the sentiment indicator matrix. Our last prior knowledge focuses on the prior knowledge on the sentiment label associated with the image itself. As our framework essentially is a semisupervised learning approach, this leads to a domain adapted model that has the capability to handle some domain specific data. The partial label is given by the image sentiment matrix R 0 where R 0 ∈ R n×k .</p><p>To solve the optimization problem above, we employ the alternating multiplicative updating scheme shown in <ref type="bibr" target="#b16">[17]</ref> to find the optimal solutions. First, we use fixed V and S to update T as follows:</p><formula xml:id="formula_2">T ij ← T ij [XV S T + βT 0 + δR 0 S T ] ij [T SV T V S T + βT + δT SS T ] ij (2)</formula><p>Next, we use the similar update rule to update S and V :</p><formula xml:id="formula_3">S ij ← S ij [T T XV + γS 0 + δT T R 0 ] ij [T T T SV T V + γS + δT T T S] ij (3) V ij ← V ij [X T T S + αV 0 ] ij [V S T T T T S + αV ] ij (4)</formula><p>The learning process consists of an iterative procedure using Eq (3), Eq (4) and Eq (5) until convergence. The description of the process is shown in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Unsupervised Sentiment Analysis</head><p>The vast majority of existing methods are supervised, relying on labeled images to train sentiment classifiers. Unfortunately, sentiment labels are in general unavailable for social media images, and it is too labor-and time-intensive to obtain labeled sets large enough for robust training. In order Algorithm 1 Multiplicative Updating Algorithm Input: X, T 0 , S 0 , V 0 , R 0 , α, β, γ, δ Output: T, S, V Initialization: T, S, V while Not Converge do Update T using Eq(2) with fixed S,V Update S using Eq(3) with fixed T,V Update V using Eq(4) with fixed T,S End to utilize the vast amount of unlabeled social media images, an unsupervised approach would be much more desirable. This paper studies unsupervised sentiment analysis.</p><p>Typically, visual features such as color histogram, brightness, the presence of objects and visual attributes lack the level of semantic meanings required by sentiment prediction. In supervised case, label information could be directly utilized to build the connection between the visual features and the sentiment labels. Thus, unsupervised sentiment analysis for social media images is inherently more challenging than its supervised counterpart. As images from social media sources are often accompanied by textual information, intuitively such information may be employed. However, textual information accompanying images is often incomplete (e.g., scarce tags) and noisy (e.g., irrelevant comments), and thus often inadequate to support independent sentiment analysis. On the other hand, such information can provide muchneeded additional semantic information about the underlying images, which may be exploited to enable unsupervised sentiment analysis. How to achieve this is the objective of our approach.</p><p>In this paper, we study unsupervised sentiment analysis for social media images with textual information by investigating two related challenges: (1) how to model the interaction between images and textual information systematically so as to support sentiment prediction using both sources of information, and (2) how to use textual information to enable unsupervised sentiment analysis for social media images. In addressing these two challenges, we propose a novel Unsupervised SEntiment Analysis (USEA) <ref type="bibr" target="#b21">[22]</ref> framework, which performs sentiment analysis for social media images in an unsupervised fashion.</p><p>Without label information, it is challenging for unsupervised sentiment analysis to connect visual features with sentiment labels. Textual information associated with social media images may be exploited to help, as it provides semantics about the underly images and in particular rich sentiment signals such as sentiment words and emotion symbols may be found in the textual fields. Hence, to exploit textual information, we investigate (1) how to incorporate textual information into visual information; and (2) how to model sentiment signals in textual information.</p><p>Since visual and textual information are two views about the same set of images, it is reasonable to assume that they share the same sentiment label space. More specifically, the sentiment of I i should be consistent with that of its associated textual information p i . Let U 0 ∈ R n×k be the sentiment label space where U 0 (i, j) = 1 if the i-th data instance belongs to c j , and U 0 (i, j) = 0 otherwise. We propose the following formulation to incorporate visual information with textual information based on nonnegative matrix factorization:</p><formula xml:id="formula_4">min UV X v -U v V T v 2 F + α X t -U t V T t 2 F + β( U v -U 0 2 F + U t -U 0 2 F ) subject to U v ≥ 0, U t ≥ 0; ||U 0 (i, :)|| 0 = 1, i ∈ {1, 2, ..n} U 0 (i, j) ∈ {0, 1} j ∈ {1, 2, ..k} (5)</formula><p>where α controls how textual information contributes to the model and || • || 0 is 0 , which counts the number of nonzero entries in the vector. U v ∈ R n×k and U t ∈ R n×k are the sentiment label spaces learned from visual information and textual information, respectively. The term of</p><formula xml:id="formula_5">β( U v -U 0 2 F + U t -U 0 2 F )</formula><p>ensures that these two types of information should share the sentiment label space U 0 . V v ∈ R mv×k and V t ∈ R mt×k indicate the sentiment polarities of visual and textual features, respectively.</p><p>Since visual and textual information are two views about the same set of images, it is reasonable to assume that they share the same sentiment label space. More specifically, the sentiment of I i should be consistent with that of its associated textual information p i . Let U 0 ∈ R n×k be the sentiment label space where U 0 (i, j) = 1 if the i-th data instance belongs to c j , and U 0 (i, j) = 0 otherwise. We propose the following formulation to incorporate visual information with textual information based on nonnegative matrix factorization:</p><formula xml:id="formula_6">min UV X v -U v V T v 2 F + α X t -U t V T t 2 F + β( U v -U 0 2 F + U t -U 0 2 F ) subject to U v ≥ 0, U t ≥ 0; ||U 0 (i, :)|| 0 = 1, i ∈ {1, 2, ..n} U 0 (i, j) ∈ {0, 1} j ∈ {1, 2, ..k} (6)</formula><p>where α controls how textual information contributes to the model and || • || 0 is 0 , which counts the number of nonzero entries in the vector. U v ∈ R n×k and U t ∈ R n×k are the sentiment label spaces learned from visual information and textual information, respectively. The term of</p><formula xml:id="formula_7">β( U v -U 0 2 F + U t -U 0 2 F )</formula><p>ensures that these two types of information should share the sentiment label space U 0 . V v ∈ R mv×k and V t ∈ R mt×k indicate the sentiment polarities of visual and textual features, respectively.</p><p>Textual information contains rich sentiment signals. First, some words may contain sentiment polarities. For example, some words are positive such as "happy" and "terrific"; while others are negative such as "gloomy" and "disap-pointed". The sentiment polarities of words can be obtained via some public sentiment lexicons. For example, the sentiment lexicon MPQA contains 7,504 human labeled words which are commonly used in the daily life with 2,721 positive words and 4,783 negative words. Second, some abbreviations and emoticons are strong sentiment indicators. For example, "lol"(means laughing out loud) is a positive indicator while ":(" is a negative indicator. Let V t0 ∈ R mv×k be the matrix coding sentiment signals in textual information where V t0 (i, j) = 1 if i-th word belongs to c j and V t0 (i, j) = 0 otherwise. To model sentiment signals, we force the learned sentiment polarities of textual features to be consistent with those indicated by sentiment signals. Furthermore, not all textual features in F t contain sentiment polarities and V t should be sparse. We propose the following formulation to achieve these two goals as:</p><formula xml:id="formula_8">min V t -V t0 2,1<label>(7)</label></formula><p>X 2,1 is the 2,1 of the matrix X, which ensures the row sparsity of X <ref type="bibr">[?]</ref>.</p><p>The significance of textual information in unsupervised sentiment analysis for social media images is two-fold. First, textual information bridges the semantic gap between visual features and sentiment labels. Second, we are allowed to do sentiment analysis for social media images in an unsupervised scenarios by modeling textual information via Eqs. ( <ref type="formula">6</ref>) and <ref type="bibr" target="#b6">(7)</ref>.</p><p>By combining the above discussion, we can have the following initial framework, which provides a potential solution to inferring sentiments by jointly considering visual information and corresponding contextual information:</p><formula xml:id="formula_9">min UV X v -U v V T v 2 F + α X t -U t V T t 2 F + β( U v -U 0 2 F + U t -U 0 2 F ) + γ V t -V t0 2,1 s.t. U v ≥ 0, U t ≥ 0, U T 0 U 0 = I; U 0 ≥ 0 (8)</formula><p>There are 5 components, i.e. U v , V v , U t , V t and U 0 , in Eq. ( <ref type="formula">4</ref>). Thus it is difficult to optimize all the components simultaneously. In the following parts, we demonstrate an alternating algorithm to optimize the objective function by updating each component iteratively.</p><p>Update V t : If U 0 , U v , V v and U t are fixed, then the objective function is decoupled and the constrains are independent of V t . Thus we can optimize V t separately and ignore the term without V t , leading to the following:</p><formula xml:id="formula_10">min Vv J (V t ) = X t -U t V T t 2 F + δ V t -V t0 2 F (9)</formula><p>where δ = γ α . Taking the derivation of J (V t ) and setting it to zero, we can obtain the following form:</p><formula xml:id="formula_11">(-X T t U t + V t U T t U t ) + δD t (V t -V t0 ) = 0 (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>where D t is a diagonal matrix with jth element on the diagonal D(j, j) = 1 2 Vt(j,:)-Vt0(j,:) 2 . In Eq. ( <ref type="formula">6</ref>), solving V t directly is intractable. Since D t and U T t U t are symmetric and positive definite, we employ eigen decomposition for them as:</p><formula xml:id="formula_13">U T t U t = U 1 Λ 1 U T 1 D t = U 2 Λ 2 U T 2<label>(11)</label></formula><p>where U 1 , U 2 are eigen vectors and Λ 1 , Λ 2 are diagonal matrices with eigen values on the diagonal. Substituting U T t U t and D t in Eq. ( <ref type="formula">6</ref>), we have:</p><formula xml:id="formula_14">V t U 1 Λ 1 U T 1 + δU 2 Λ 2 U T 2 V t = X T t U t + δD t V t0 (12) Multiplying U T</formula><p>2 and U 1 from left to right on both sides: <ref type="formula">9</ref>) becomes V t Λ 1 + δΛ 2 V t = Q, then we can obtain the V t and V t as:</p><formula xml:id="formula_15">U T 2 V t U 1 Λ 1 + δΛ 2 U T 2 V t U 1 = U T 2 (X t U t + δD t V t0 )U 1 (13) Let V t = U T 2 V t U 1 and Q = U T 2 (X t U t + δD t V t0 )U 1 , Eq. (</formula><formula xml:id="formula_16">V t (s, l) = Q(s, l) δλ s 2 + λ l 1 V t = U 2 V t U T 1 (<label>14</label></formula><formula xml:id="formula_17">)</formula><p>where λ s 2 is the s-th eigen value of D t and λ l 1 is l-th eigen value of U T t U t . Update V v . If U 0 , U t , V t and U v are fixed, by setting the derivation of the objective function to zero, V v can be easily obtained as</p><formula xml:id="formula_18">V v = X T v U v (U T v U v ) -1</formula><p>. Moreover, we can easily verify updating V v will monotonically decrease the objective function.</p><p>Update U v : If V v , U t , V t and U 0 are fixed, U v can be obtained by the following optimization problem:</p><formula xml:id="formula_19">min Uv J (U v ) = X v -U v V T v 2 F + β U v -U 0 2 F s.t. U v ≥ 0<label>(15)</label></formula><p>The Lagrangian function of Eq. ( <ref type="formula" target="#formula_13">11</ref>) is :</p><formula xml:id="formula_20">min Uv L(U v ) = X v -U v V T v 2 F + β U v -U 0 2 F -T r(ΓU v )<label>(16)</label></formula><p>where Γ is Lagrangian multiplier. Taking the deviation of J (U v ) and using the KKT condition (Γ(s, l)U v (s, l) = 0), we can obtain:</p><formula xml:id="formula_21">(-X v V v + U v V T v V v + βU v -βU 0 ) sl (U v ) sl = 0<label>(</label></formula><p>17) which leads to the following update rule for U v :</p><formula xml:id="formula_22">(U v ) sl ← (U v ) sl ((X v V v ) + + U v (V T v V v ) -+ βU 0 ) sl ((X v V v ) -+ U v (V T v V v ) + + βU v ) sl (18) where (X(s, l)) + = (|X(s, l)| + X(s, l))/2, (X(s, l)) -= (|X(s, l)| -X(s, l))/2 and X = X + -X -.</formula><p>Update U t : It is worth noting that the procedure of solving U t is exactly the same as that of U v . Thus, we omit the solution of U t here.</p><p>Update U 0 : With U v , U t , V t and V v fixed, the sentiment label U 0 can be obtained by solving the following optimization problem:</p><formula xml:id="formula_23">min U J (U 0 ) = U v -U 0 2 F + U t -U 0 2 F s.t. U T 0 U 0 = I; U 0 ≥ 0<label>(19)</label></formula><p>The Lagrangian function of Eq. ( <ref type="formula">17</ref>) is:</p><formula xml:id="formula_24">min U J (U 0 ) = U v -U 0 2 F + U t -U 0 2 F + T r(Λ(U T 0 U 0 -I)) -T r(ΓU 0 )<label>(20)</label></formula><p>where Λ and Γ are Lagrangian multipliers. Taking the derivation of J (U 0 ) and using KKT conditions we can obtain</p><formula xml:id="formula_25">(U 0 -U v + U 0 -U t + U 0 Λ) sl (U 0 ) sl = 0<label>(21)</label></formula><p>which leads the following update rule for U 0 :</p><formula xml:id="formula_26">(U 0 ) sl ← (U 0 ) sl (U v + U t + (U 0 Λ) -) sl ((U 0 Λ) + + 2U 0 ) sl (<label>22</label></formula><formula xml:id="formula_27">)</formula><p>Note that updating U 0 needs updating the Lagrangian multiplier Λ as well. To obtain Λ, we sum over s and get Λ(s, s) = (U T 0 U v -I + U T 0 U t -I) s,s . The offdiagonal elements of Λ are approximately obtained from non-negative value of U 0 , leading to Λ(s, t) = (U T 0 U v -I +U T 0 U t -I) st . Overall, we can obtain Λ by combining the diagonal values and off-diagonal values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2</head><p>The proposed USEA Input: {X v , X t , V t0 } α, β, γ Output: k sentiment label for each data instance. Initialization: U t , U v , V v , V t while Not Converge do Update V t using Eq.( <ref type="formula" target="#formula_16">14</ref>) and compute <ref type="figure"></ref>and<ref type="figure">(V T</ref> t V t ) +,- Update U v using Eq. ( <ref type="formula">18</ref>), similarly update U t Computing Λ Update U 0 End Using max-pooling for U 0 to predict sentiment labels.</p><formula xml:id="formula_28">V v = X T v U v (U T v U v ) -1 . Computing (X v V v ) +,-, (X t V t ) +,-, (V T v V v ) +,-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PRELIMINARY RESULTS</head><p>We now quantitatively and qualitatively compare the proposed model on image sentiment prediction with other candidate methods. We also evaluate the robustness of the proposed model with respect to various training samples and different combinations of prior knowledge. Finally, we perform a deeper analysis of our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiment Settings</head><p>We perform the evaluation on two large scale image datasets collected from Flickr and Instagram respectively. The collection of Flickr dataset is based on the image IDs provided by <ref type="bibr" target="#b7">[8]</ref>, which contains 3,504,192 images from 4,807 users. Because some images are unavailable now, and without loss of generality, we limit the number of images from each user. Thus, we get 120,221 images from 3921 users. For the collection of the Instagram dataset, we randomly pick 10 users as seed nodes and collect images by traversing the social network based on breadth first search. The total number of images from Instagram is 130,230 from 3,451 users.</p><p>Establishing Ground Truth: For training and evaluating the proposed method, we need to know the sentiment labels. Thus, 20,000 Flickr images are labeled by three human subjects, the majority voting is employed. However, manually acquiring the labels for these two large scale datasets is expensive and time consuming. Consequently, the rest of more than 230,000 images are labeled by the tags, which was suggested by the previous works <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b22">[23]</ref> <ref type="foot" target="#foot_3">3</ref> . Since labeling the images based on the tags may cause noise issue, and for better reliability we only label the images with primary sentiment labels, which include: positive, neutral and negative. It is worth noting that the human labeled images have both primary sentiment labels and fine grained sentiment labels. The fine grained labels, including: happiness, amusement, anger, fear, sad and disgust, are used to for fine grained sentiment prediction.</p><p>Feature extraction: the proposed method has the ability to incorporate visual and textual information. For visual information, we follow the recent approaches <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref> by using mid-level visual features. The visual features are extracted by a large-scale visual attribute detectors <ref type="bibr" target="#b6">[7]</ref> and the feature dimension is 1200. Text-based features are formed by the term frequency in user profiles, image captions and comments. It is worth noting that textual features, which contain user descriptions, friends' comments and image captions, are preprocessed by stop word removing and stemming Since RSAI is supervised approach, the comparison methods for include: Senti API <ref type="foot" target="#foot_4">4</ref> , SentiBank <ref type="bibr" target="#b6">[7]</ref>, EL <ref type="bibr" target="#b7">[8]</ref> and the baseline method.</p><p>• Senti API is a text based sentiment prediction API, it measures the text sentiment by counting the sentiment strength for each text term. • SentiBank is a state-of-the-art visual based sentiment prediction method. The method extracts a large number of visual attributes and associates them with a sentiment score. Similar to Senti API, the sentiment prediction is based on the sentiment of each visual attributes. • EL is a graphical model based approach, it infers the sentiment based on the friend interactions and several low level visual features. • Baseline: The baseline method comes from our basic model. To compare it fairly, we also introduce R 0 with the basic model which makes the baseline method have the ability to learn from training data. The results of comparison are shown in Table <ref type="table">2</ref>. We employ 30% data for training and remaining for testing. To verify the reliability of tags labeled images, we also included 20000 labeled Flickr images with primary sentiment label. Especially, the classifier setting for SentiBank and EL followed the original papers. The classifier of Sentibank is logistic regression and for EL it is SVM. From the results we can see that, the proposed method performs best in both datasets. Noting that proposed method improved 10% and 6% over state-of-the-art methods <ref type="bibr" target="#b6">[7]</ref>. Noting that the number we reported in Table <ref type="table">2</ref> is the precision accuracy for each method.</p><p>From the table, we can see that, even though noise exists in the Flickr and Instagram dataset, the results are similar to the performance on human labeled dataset. Another interesting observation is that the performance of EL on Instagram is worse than on Flickr, one reason could be that the wide usage of "picture filters" lowers discriminative ability of the low level visual features, while the models based on the mid level attributes can easily avoid this filter ambiguity.</p><p>For unsupervised approach USEA, we compared with the following sentiment analysis algorithms:</p><p>• Senti API: 5 . This API is natural language processing API that performs unsupervised sentiment prediction using word-based sentiment. The method only uses textual information. • USEA-T: A variant of the proposed method that only considers the textual information including user profiles, image captions and friends' comments. • Random: It predicts sentiment labels of images by randomly guessing. Moreover, noting that SentiBank <ref type="bibr" target="#b6">[7]</ref> and EL <ref type="bibr" target="#b7">[8]</ref> are originally proposed for supervised sentiment analysis. We extend them to unsupervised scenarios by replacing original classifiers such as SVM or logistic regression with Kmeans. However, the clusters identified by K-means have 5 http://sentistrength.wlv.ac.uk/ no sentiment labels and we determine their sentiment labels with the Euclidean distance to the ground truth. We use SentiBank-K, and EL-K to represent these modifications.</p><p>Table <ref type="table">3</ref> lists the comparison results and we make several key observations:</p><p>• Most of the time, textual based approaches obtain slight better performance than Random. These results support -(1) textual information is often incomplete and noisy and thus often inadequate to support independent sentiment analysis; and (2) textual information contains important cues for sentiment analysis. • The proposed framework often obtains better performance than baseline methods. There are two major reasons. First textual information provides semantic meanings and sentiment signals for images. Second we combine visual and textual information for sentiment analysis. The impact of textual information on the proposed framework will be discussed in the following subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. FUTURE WORK</head><p>In the future, we will employ crowdsourcing tools, such as AmazonTurk <ref type="foot" target="#foot_5">6</ref> , to obtain high-quality, manually-labeled data to test the proposed method. Furthermore, inspired by the recent development of advanced deep learning algorithms and their success in image classification and detection tasks, we will follow this research direction to perform the sentiment analysis via deep learning. In order to have a robust trained architecture and network parameters, we will focus on the deep learning models that work for smaller dataset. Moreover, beyond sentiment analysis, we will study social event and social response <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> via visual data in the social media. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example shows affective gap.</figDesc><graphic coords="2,62.99,73.00,116.40,104.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table II :</head><label>II</label><figDesc>The comparison results of different supervised methods for sentiment analysis.</figDesc><table><row><cell></cell><cell>Senti API</cell><cell>SentiBank</cell><cell>EL</cell><cell cols="2">Baseline RSAI</cell></row><row><cell>20000 Flickr</cell><cell>0.32</cell><cell>0.42</cell><cell>0.47</cell><cell>0.48</cell><cell>0.52</cell></row><row><cell>Flickr</cell><cell>0.34</cell><cell>0.47</cell><cell>0.45</cell><cell>0.48</cell><cell>0.57</cell></row><row><cell>Instagram</cell><cell>0.27</cell><cell>0.56</cell><cell>0.37</cell><cell>0.54</cell><cell>0.62</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table III :</head><label>III</label><figDesc>The comparison results of different unsupervised methods for sentiment analysis.</figDesc><table><row><cell>Method</cell><cell>Flickr (#20,000)</cell><cell>Flickr (#140,221)</cell><cell>Instagram (#131,224)</cell></row><row><cell>Senti API</cell><cell>32.30%</cell><cell>34.15%</cell><cell>37.80%</cell></row><row><cell>SentiBank-K</cell><cell>41.32%</cell><cell>41.12%</cell><cell>46.31%</cell></row><row><cell>EL-K</cell><cell>36.39%</cell><cell>42.90%</cell><cell>43.21%</cell></row><row><cell>USEA-T</cell><cell>37.90%</cell><cell>40.22%</cell><cell>36.41%</cell></row><row><cell>USEA</cell><cell>55.22%</cell><cell>56.18%</cell><cell>59.94%</cell></row><row><cell>Random</cell><cell>32.81%</cell><cell>33.12%</cell><cell>33.05%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://www.pewinternet.org/2015/01/09/social-media-update-2014/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>978-1-4673-8493-3/15 $31.00 © 2015 IEEE DOI 10.1109/ICDMW.2015.142</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>http://mpqa.cs.pitt.edu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>More details can be found in<ref type="bibr" target="#b7">[8]</ref> and<ref type="bibr" target="#b22">[23]</ref> </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>http://sentistrength.wlv.ac.uk/,a text based sentiment prediction API</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://www.mturk.com/mturk/welcome</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Not all moods are created equal! exploring human emotional states in social media</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">De</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Counts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth International AAAI Conference on Weblogs and Social Media</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Affective image classification using features inspired by psychology and art theory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Machajdik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international conference on Multimedia</title>
		<meeting>the international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="83" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Thumbs up?: sentiment classification using machine learning techniques</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 conference on Empirical methods in natural language processing</title>
		<meeting>the ACL-02 conference on Empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sentiment analysis and opinion mining</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="167" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd annual meeting on Association for Computational Linguistics</title>
		<meeting>the 42nd annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">271</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Largescale visual sentiment ontology and detectors using adjective noun pairs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM international conference on Multimedia</title>
		<meeting>the 21st ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="223" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">How do your friends on social media disclose your emotions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Can we understand van gogh&apos;s mood?: learning to infer affects from images in social networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM international conference on Multimedia</title>
		<meeting>the 20th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="857" to="860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sentribute: image sentiment analysis from a mid-level perspective</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mcdonough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Workshop on Issues of Sentiment Discovery and Opinion Mining</title>
		<meeting>the Second International Workshop on Issues of Sentiment Discovery and Opinion Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Robust image sentiment analysis using progressively trained and domain transferred deep networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Non-negative matrix factorization framework for face recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="495" to="511" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning the parts of objects by non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">401</biblScope>
			<biblScope unit="issue">6755</biblScope>
			<biblScope unit="page" from="788" to="791" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Non-negative matrix factorization as a feature selection tool for maximum margin classifiers</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Das</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2841" to="2848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Algorithms for non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="556" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the equivalence of nonnegative matrix factorization and spectral clustering</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM</title>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="606" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Orthogonal nonnegative matrix t-factorizations for clustering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="126" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Inferring sentiment from web images with joint inference on visual and social cues: A regulated matrix factorization approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kambhampati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICWSM</title>
		<imprint>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Lexical normalisation of short text messages: Makn sens a# twitter</title>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="368" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The expression of the emotions in man and animals</title>
		<author>
			<persName><forename type="first">C</forename><surname>Darwin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Emotion: A psychoevolutionary synthesis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Plutchik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Harper &amp; Row</title>
		<imprint>
			<date type="published" when="1980">1980</date>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised sentiment analysis for social media images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Twitter sentiment classification using distant supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Go</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bhayani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Et-lda: Joint topic modeling for aligning events and their twitter feedback</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kambhampati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th AAAI Conference</title>
		<meeting>the 6th AAAI Conference</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">What we instagram: A first analysis of instagram photo content and user types</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Manikonda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kambhampati</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
