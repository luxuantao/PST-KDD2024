<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Kernel Methods for Deep Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Youngmin</forename><surname>Cho</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>9500 Gilman Drive, Mail Code</addrLine>
									<postCode>0404, 92093-0404</postCode>
									<settlement>San Diego, La Jolla</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Lawrence</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
							<email>saul@cs.ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>9500 Gilman Drive, Mail Code</addrLine>
									<postCode>0404, 92093-0404</postCode>
									<settlement>San Diego, La Jolla</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Kernel Methods for Deep Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a new family of positive-definite kernel functions that mimic the computation in large, multilayer neural nets. These kernel functions can be used in shallow architectures, such as support vector machines (SVMs), or in deep kernel-based architectures that we call multilayer kernel machines (MKMs). We evaluate SVMs and MKMs with these kernel functions on problems designed to illustrate the advantages of deep architectures. On several problems, we obtain better results than previous, leading benchmarks from both SVMs with Gaussian kernels as well as deep belief nets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent work in machine learning has highlighted the circumstances that appear to favor deep architectures, such as multilayer neural nets, over shallow architectures, such as support vector machines (SVMs) <ref type="bibr" target="#b0">[1]</ref>. Deep architectures learn complex mappings by transforming their inputs through multiple layers of nonlinear processing <ref type="bibr" target="#b1">[2]</ref>. Researchers have advanced several motivations for deep architectures: the wide range of functions that can be parameterized by composing weakly nonlinear transformations, the appeal of hierarchical distributed representations, and the potential for combining unsupervised and supervised methods. Experiments have also shown the benefits of deep learning in several interesting applications <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>Many issues surround the ongoing debate over deep versus shallow architectures <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref>. Deep architectures are generally more difficult to train than shallow ones. They involve difficult nonlinear optimizations and many heuristics. The challenges of deep learning explain the early and continued appeal of SVMs, which learn nonlinear classifiers via the "kernel trick". Unlike deep architectures, SVMs are trained by solving a simple problem in quadratic programming. However, SVMs cannot seemingly benefit from the advantages of deep learning.</p><p>Like many, we are intrigued by the successes of deep architectures yet drawn to the elegance of kernel methods. In this paper, we explore the possibility of deep learning in kernel machines. Though we share a similar motivation as previous authors <ref type="bibr" target="#b6">[7]</ref>, our approach is very different. Our paper makes two main contributions. First, we develop a new family of kernel functions that mimic the computation in large neural nets. Second, using these kernel functions, we show how to train multilayer kernel machines (MKMs) that benefit from many advantages of deep learning.</p><p>The organization of this paper is as follows. In section 2, we describe a new family of kernel functions and experiment with their use in SVMs. Our results on SVMs are interesting in their own right; they also foreshadow certain trends that we observe (and certain choices that we make) for the MKMs introduced in section 3. In this section, we describe a kernel-based architecture with multiple layers of nonlinear transformation. The different layers are trained using a simple combination of supervised and unsupervised methods. Finally, we conclude in section 4 by evaluating the strengths and weaknesses of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Arc-cosine kernels</head><p>In this section, we develop a new family of kernel functions for computing the similarity of vector inputs x, y ∈ d . As shorthand, let Θ(z) = 1  2 (1 + sign(z)) denote the Heaviside step function. We define the nth order arc-cosine kernel function via the integral representation:</p><formula xml:id="formula_0">k n (x, y) = 2 dw e − w 2 2 (2π) d/2 Θ(w • x) Θ(w • y) (w • x) n (w • y) n<label>(1)</label></formula><p>The integral representation makes it straightforward to show that these kernel functions are positivesemidefinite. The kernel function in eq. ( <ref type="formula" target="#formula_0">1</ref>) has interesting connections to neural computation <ref type="bibr" target="#b7">[8]</ref> that we explore further in sections 2.2-2.3. However, we begin by elucidating its basic properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Basic properties</head><p>We show how to evaluate the integral in eq. ( <ref type="formula" target="#formula_0">1</ref>) analytically in the appendix. The final result is most easily expressed in terms of the angle θ between the inputs:</p><formula xml:id="formula_1">θ = cos −1 x • y x y .<label>(2)</label></formula><p>The integral in eq. ( <ref type="formula" target="#formula_0">1</ref>) has a simple, trivial dependence on the magnitudes of the inputs x and y, but a complex, interesting dependence on the angle between them. In particular, we can write:</p><formula xml:id="formula_2">k n (x, y) = 1 π x n y n J n (θ)<label>(3)</label></formula><p>where all the angular dependence is captured by the family of functions J n (θ). Evaluating the integral in the appendix, we show that this angular dependence is given by:</p><formula xml:id="formula_3">J n (θ) = (−1) n (sin θ) 2n+1 1 sin θ ∂ ∂θ n π − θ sin θ .<label>(4)</label></formula><p>For n = 0, this expression reduces to the supplement of the angle between the inputs. However, for n &gt; 0, the angular dependence is more complicated. The first few expressions are:</p><formula xml:id="formula_4">J 0 (θ) = π − θ (5) J 1 (θ) = sin θ + (π − θ) cos θ (6) J 2 (θ) = 3 sin θ cos θ + (π − θ)(1 + 2 cos 2 θ)<label>(7)</label></formula><p>We describe eq. ( <ref type="formula" target="#formula_2">3</ref>) as an arc-cosine kernel because for n = 0, it takes the simple form k 0 (x, y) = 1− 1 π cos −1 x•y x y . In fact, the zeroth and first order kernels in this family are strongly motivated by previous work in neural computation. We explore these connections in the next section.</p><p>Arc-cosine kernels have other intriguing properties. From the magnitude dependence in eq. ( <ref type="formula" target="#formula_2">3</ref>), we observe the following: (i) the n = 0 arc-cosine kernel maps inputs x to the unit hypersphere in feature space, with k 0 (x, x) = 1; (ii) the n = 1 arc-cosine kernel preserves the norm of inputs, with k 1 (x, x) = x 2 ; (iii) higher order (n &gt; 1) arc-cosine kernels expand the dynamic range of the inputs, with k n (x, x) ∼ x 2n . Properties (i)-(iii) are shared respectively by radial basis function (RBF), linear, and polynomial kernels. Interestingly, though, the n = 1 arc-cosine kernel is highly nonlinear, also satisfying k 1 (x, −x) = 0 for all inputs x. As a practical matter, we note that arccosine kernels do not have any continuous tuning parameters (such as the kernel width in RBF kernels), which can be laborious to set by cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Computation in single-layer threshold networks</head><p>Consider the single-layer network shown in Fig. <ref type="figure" target="#fig_0">1</ref> (left) whose weights W ij connect the jth input unit to the ith output unit. The network maps inputs x to outputs f (x) by applying an elementwise nonlinearity to the matrix-vector product of the inputs and the weight matrix: f (x) = g(Wx). The nonlinearity is described by the network's so-called activation function. Here we consider the family of one-sided polynomial activation functions g n (z) = Θ(z)z n illustrated in the right panel of Fig. <ref type="figure" target="#fig_0">1</ref>. For n = 0, the activation function is a step function, and the network is an array of perceptrons. For n = 1, the activation function is a ramp function (or rectification nonlinearity <ref type="bibr" target="#b8">[9]</ref>), and the mapping f (x) is piecewise linear. More generally, the nonlinear (non-polynomial) behavior of these networks is induced by thresholding on weighted sums. We refer to networks with these activation functions as single-layer threshold networks of degree n.</p><p>Computation in these networks is closely connected to computation with the arc-cosine kernel function in eq. ( <ref type="formula" target="#formula_0">1</ref>). To see the connection, consider how inner products are transformed by the mapping in single-layer threshold networks. As notation, let the vector w i denote ith row of the weight matrix W. Then we can express the inner product between different outputs of the network as:</p><formula xml:id="formula_5">f (x) • f (y) = m i=1 Θ(w i • x)Θ(w i • y)(w i • x) n (w i • y) n , (<label>8</label></formula><formula xml:id="formula_6">)</formula><p>where m is the number of output units. The connection with the arc-cosine kernel function emerges in the limit of very large networks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b7">8]</ref>. Imagine that the network has an infinite number of output units, and that the weights W ij are Gaussian distributed with zero mean and unit variance. In this limit, we see that eq. ( <ref type="formula" target="#formula_5">8</ref>) reduces to eq. ( <ref type="formula" target="#formula_0">1</ref>) up to a trivial multiplicative factor:</p><formula xml:id="formula_7">lim m→∞ 2 m f (x) • f (y) = k n (x, y).</formula><p>Thus the arc-cosine kernel function in eq. ( <ref type="formula" target="#formula_0">1</ref>) can be viewed as the inner product between feature vectors derived from the mapping of an infinite single-layer threshold network <ref type="bibr" target="#b7">[8]</ref>.</p><p>Many researchers have noted the general connection between kernel machines and neural networks with one layer of hidden units <ref type="bibr" target="#b0">[1]</ref>. The n = 0 arc-cosine kernel in eq. (1) can also be derived from an earlier result obtained in the context of Gaussian processes <ref type="bibr" target="#b7">[8]</ref>. However, we are unaware of any previous theoretical or empirical work on the general family of these kernels for degrees n ≥ 0.</p><p>Arc-cosine kernels differ from polynomial and RBF kernels in one especially interesting respect. As highlighted by the integral representation in eq. ( <ref type="formula" target="#formula_0">1</ref>), arc-cosine kernels induce feature spaces that mimic the sparse, nonnegative, distributed representations of single-layer threshold networks. Polynomial and RBF kernels do not encode their inputs in this way. In particular, the feature vector induced by polynomial kernels is neither sparse nor nonnegative, while the feature vector induced by RBF kernels resembles the localized output of a soft vector quantizer. Further implications of this difference are explored in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Computation in multilayer threshold networks</head><p>A kernel function can be viewed as inducing a nonlinear mapping from inputs x to feature vectors Φ(x). The kernel computes the inner product in the induced feature space: k(x, y) = Φ(x)•Φ(y). In this section, we consider how to compose the nonlinear mappings induced by kernel functions. Specifically, we show how to derive new kernel functions</p><formula xml:id="formula_8">k ( ) (x, y) = Φ(Φ(...Φ times (x))) • Φ(Φ(...Φ times (y)))<label>(9)</label></formula><p>which compute the inner product after successive applications of the nonlinear mapping Φ(•). Our motivation is the following: intuitively, if the base kernel function k(x, y) = Φ(x) • Φ(y) mimics the computation in a single-layer network, then the iterated mapping in eq. ( <ref type="formula" target="#formula_8">9</ref>) should mimic the computation in a multilayer network. test set. SVMs with arc cosine kernels have error rates from 22.36-25.64%. Results are s kernels of varying degree (n) and levels of recursion ( ). The best previous results are 24 SVMs with RBF kernels and 22.50% for deep belief nets <ref type="bibr" target="#b1">[2]</ref>. See text for details.  2000 training examples as a validation set to choose the margin penalty parameter; after this parameter by cross-validation, we then retrained each SVM using all the training exam reference, we also report the best results obtained previously from three layer deep belief ne 3) and SVMs with RBF kernels (SVM-RBF). These references are representative of th state-of-the-art for deep and shallow architectures on these data sets.</p><p>The right panels of figures 2 and 3 show the test set error rates from arc cosine kernels o degree (n) and levels of recursion ( ). We experimented with kernels of degree n = 0 corresponding to single layer threshold networks with "step", "ramp", and "quarter-pipe" functions. We also experimented with the multilayer kernels described in section 2.3, c from one to six levels of recursion. Overall, the figures show that on these two data se different arc cosine kernels outperform the best results previously reported for SVMs w kernels and deep belief nets. We give more details on these experiments below. At a h though, we note that SVMs with arc cosine kernels are very straightforward to train; unli with RBF kernels, they do not require tuning a kernel width parameter, and unlike deep b they do not require solving a difficult nonlinear optimization or searching over possible arch</p><p>In our experiments, we quickly discovered that the multilayer kernels only performed w n = 1 kernels were used at higher ( &gt; 1) levels in the recursion. Figs. 2 and 3 therefore s these sets of results; in particular, each group of bars shows the test error rates when a kernel (of degree n = 0, 1, 2) was used at the first layer of nonlinearity, while the n = 1 k used at successive layers. We do not have a formal explanation for this effect. However, r only the n = 1 arc cosine kernel preserves the norm of its inputs: the n = 0 kernel maps onto a unit hypersphere in feature space, while higher-order (n &gt; 1) kernels may induc spaces with severely distorted dynamic ranges. Therefore, we hypothesize that only n = 1 a kernels preserve sufficient information about the magnitude of their inputs to work effe composition with other kernels.</p><p>Finally, the results on both data sets reveal an interesting trend: the multilayer arc cosin often perform better than their single layer counterparts. Though SVMs are shallow arch The best previous results are 24.04% for SVMs with RBF kernels and 22.50% for deep belief nets <ref type="bibr" target="#b10">[11]</ref>. See text for details.</p><note type="other">5</note><p>We first examine the results of this procedure for widely used kernels. Here we find that the iterated mapping in eq. ( <ref type="formula" target="#formula_8">9</ref>) does not yield particularly interesting results. Consider the two-fold composition that maps x to Φ(Φ(x)). For linear kernels k(x, y) = x • y, the composition is trivial: we obtain the identity map Φ(Φ(x)) = Φ(x) = x. For homogeneous polynomial kernels k(x, y) = (x • y) d , the composition yields:</p><formula xml:id="formula_9">Φ(Φ(x)) • Φ(Φ(y)) = (Φ(x) • Φ(y)) d = ((x • y) d ) d = (x • y) d 2 . (<label>10</label></formula><formula xml:id="formula_10">)</formula><p>The above result is not especially interesting: the kernel implied by this composition is also polynomial, just of higher degree (d 2 versus d) than the one from which it was constructed. Likewise, for RBF kernels k(x, y) = e −λ x−y 2 , the composition yields:</p><formula xml:id="formula_11">Φ(Φ(x)) • Φ(Φ(y)) = e −λ Φ(x)−Φ(y) 2 = e −2λ(1−k(x,y)) .<label>(11)</label></formula><p>Though non-trivial, eq. ( <ref type="formula" target="#formula_11">11</ref>) does not represent a particularly interesting computation. Recall that RBF kernels mimic the computation of soft vector quantizers, with k(x, y) 1 when x−y is large compared to the kernel width. It is hard to see how the iterated mapping Φ(Φ(x)) would generate a qualitatively different representation than the original mapping Φ(x).</p><p>Next we consider the -fold composition in eq. ( <ref type="formula" target="#formula_8">9</ref>) for arc-cosine kernel functions. We state the result in the form of a recursion. The base case is given by eq. ( <ref type="formula" target="#formula_2">3</ref>) for kernels of depth = 1 and degree n. The inductive step is given by:</p><formula xml:id="formula_12">k (l+1) n (x, y) = 1 π k (l) n (x, x) k (l) n (y, y) n/2 J n θ ( ) n ,<label>(12)</label></formula><p>where θ</p><formula xml:id="formula_13">( )</formula><p>n is the angle between the images of x and y in the feature space induced by the -fold composition. In particular, we can write:</p><formula xml:id="formula_14">θ ( ) n = cos −1 k ( ) n (x, y) k ( ) n (x, x) k ( ) n (y, y) −1/2 . (<label>13</label></formula><formula xml:id="formula_15">)</formula><p>The recursion in eq. ( <ref type="formula" target="#formula_12">12</ref>) is simple to compute in practice. The resulting kernels mimic the computations in large multilayer threshold networks. Above, for simplicity, we have assumed that the arc-cosine kernels have the same degree n at every level (or layer) of the recursion. We can also use kernels of different degrees at different layers. In the next section, we experiment with SVMs whose kernel functions are constructed in this way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Experiments on binary classification</head><p>We evaluated SVMs with arc-cosine kernels on two challenging data sets of 28 × 28 grayscale pixel images. These data sets were specifically constructed to compare deep architectures and kernel machines <ref type="bibr" target="#b10">[11]</ref>. In the first data set, known as rectangles-image, each image contains an occluding rectangle, and the task is to determine whether the width of the rectangle exceeds its height; examples are shown in Fig. <ref type="figure" target="#fig_4">2</ref> (left). In the second data set, known as convex, each image contains a white region, and the task is to determine whether the white region is convex; examples are shown test set. SVMs with arc cosine kernels have error rates from 22.36-25.64%. Results are s kernels of varying degree (n) and levels of recursion ( ). The best previous results are 24 SVMs with RBF kernels and 22.50% for deep belief nets <ref type="bibr" target="#b1">[2]</ref>. See text for details.  2000 training examples as a validation set to choose the margin penalty parameter; after this parameter by cross-validation, we then retrained each SVM using all the training exam reference, we also report the best results obtained previously from three layer deep belief ne 3) and SVMs with RBF kernels (SVM-RBF). These references are representative of th state-of-the-art for deep and shallow architectures on these data sets.</p><p>The right panels of figures 2 and 3 show the test set error rates from arc cosine kernels o degree (n) and levels of recursion ( ). We experimented with kernels of degree n = 0 corresponding to single layer threshold networks with "step", "ramp", and "quarter-pipe" a functions. We also experimented with the multilayer kernels described in section 2.3, c from one to six levels of recursion. Overall, the figures show that on these two data se different arc cosine kernels outperform the best results previously reported for SVMs w kernels and deep belief nets. We give more details on these experiments below. At a h though, we note that SVMs with arc cosine kernels are very straightforward to train; unli with RBF kernels, they do not require tuning a kernel width parameter, and unlike deep be they do not require solving a difficult nonlinear optimization or searching over possible arch</p><p>In our experiments, we quickly discovered that the multilayer kernels only performed w n = 1 kernels were used at higher ( &gt; 1) levels in the recursion. Figs. 2 and 3 therefore s these sets of results; in particular, each group of bars shows the test error rates when a kernel (of degree n = 0, 1, 2) was used at the first layer of nonlinearity, while the n = 1 k used at successive layers. We do not have a formal explanation for this effect. However, r only the n = 1 arc cosine kernel preserves the norm of its inputs: the n = 0 kernel maps onto a unit hypersphere in feature space, while higher-order (n &gt; 1) kernels may induc spaces with severely distorted dynamic ranges. Therefore, we hypothesize that only n = 1 a kernels preserve sufficient information about the magnitude of their inputs to work effe composition with other kernels.</p><p>Finally, the results on both data sets reveal an interesting trend: the multilayer arc cosin often perform better than their single layer counterparts. Though SVMs are shallow arch 5  <ref type="bibr" target="#b10">[11]</ref>. Our experiments in binary classification focused on these data sets because in previously reported benchmarks, they exhibited the biggest performance gap between deep architectures (e.g., deep belief nets) and traditional SVMs.</p><p>We followed the same experimental methodology as previous authors <ref type="bibr" target="#b10">[11]</ref>. SVMs were trained using libSVM (version 2.88) <ref type="bibr" target="#b11">[12]</ref>, a publicly available software package. For each SVM, we used the last 2000 training examples as a validation set to choose the margin penalty parameter; after choosing this parameter by cross-validation, we then retrained each SVM using all the training examples.</p><p>For reference, we also report the best results obtained previously from three-layer deep belief nets (DBN-3) and SVMs with RBF kernels (SVM-RBF). These references appear to be representative of the current state-of-the-art for deep and shallow architectures on these data sets.</p><p>Figures <ref type="figure" target="#fig_16">2 and 3</ref> show the test set error rates from arc-cosine kernels of varying degree (n) and levels of recursion ( ). We experimented with kernels of degree n = 0, 1 and 2, corresponding to threshold networks with "step", "ramp", and "quarter-pipe" activation functions. We also experimented with the multilayer kernels described in section 2.3, composed from one to six levels of recursion.</p><p>Overall, the figures show that many SVMs with arc-cosine kernels outperform traditional SVMs, and a certain number also outperform deep belief nets. In addition to their solid performance, we note that SVMs with arc-cosine kernels are very straightforward to train; unlike SVMs with RBF kernels, they do not require tuning a kernel width parameter, and unlike deep belief nets, they do not require solving a difficult nonlinear optimization or searching over possible architectures.</p><p>Our experiments with multilayer kernels revealed that these SVMs only performed well when arccosine kernels of degree n = 1 were used at higher ( &gt; 1) levels in the recursion. Figs. <ref type="figure" target="#fig_14">2 and</ref> 3 therefore show only these sets of results; in particular, each group of bars shows the test error rates when a particular kernel (of degree n = 0, 1, 2) was used at the first layer of nonlinearity, while the n = 1 kernel was used at successive layers. We hypothesize that only n = 1 arc-cosine kernels preserve sufficient information about the magnitude of their inputs to work effectively in composition with other kernels. Recall that only the n = 1 arc-cosine kernel preserves the norm of its inputs: the n = 0 kernel maps all inputs onto a unit hypersphere in feature space, while higherorder (n &gt; 1) kernels induce feature spaces with different dynamic ranges.</p><p>Finally, the results on both data sets reveal an interesting trend: the multilayer arc-cosine kernels often perform better than their single-layer counterparts. Though SVMs are (inherently) shallow architectures, this trend suggests that for these problems in binary classification, arc-cosine kernels may be yielding some of the advantages typically associated with deep architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Deep learning</head><p>In this section, we explore how to use kernel methods in deep architectures <ref type="bibr" target="#b6">[7]</ref>. We show how to train deep kernel-based architectures by a simple combination of supervised and unsupervised methods. Using the arc-cosine kernels in the previous section, these multilayer kernel machines (MKMs) perform very competitively on multiclass data sets designed to foil shallow architectures <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multilayer kernel machines</head><p>We explored how to train MKMs in stages that involve kernel PCA <ref type="bibr" target="#b12">[13]</ref> and feature selection <ref type="bibr" target="#b13">[14]</ref> at intermediate hidden layers and large-margin nearest neighbor classification <ref type="bibr" target="#b14">[15]</ref> at the final output layer. Specifically, for -layer MKMs, we considered the following training procedure:</p><p>1. Prune uninformative features from the input space. The individual steps in this procedure are well-established methods; only their combination is new. While many other approaches are worth investigating, our positive results from the above procedure provide a first proof-of-concept. We discuss each of these steps in greater detail below.</p><p>Kernel PCA. Deep learning in MKMs is achieved by iterative applications of kernel PCA <ref type="bibr" target="#b12">[13]</ref>. This use of kernel PCA was suggested over a decade ago <ref type="bibr" target="#b15">[16]</ref> and more recently inspired by the pretraining of deep belief nets by unsupervised methods. In MKMs, the outputs (or features) from kernel PCA at one layer are the inputs to kernel PCA at the next layer. However, we do not strictly transmit each layer's top principal components to the next layer; some components are discarded if they are deemed uninformative. While any nonlinear kernel can be used for the layerwise PCA in MKMs, arc-cosine kernels are natural choices to mimic the computations in large neural nets.</p><p>Feature selection. The layers in MKMs are trained by interleaving a supervised method for feature selection with the unsupervised method of kernel PCA. The feature selection is used to prune away uninformative features at each layer in the MKM (including the zeroth layer which stores the raw inputs). Intuitively, this feature selection helps to focus the unsupervised learning in MKMs on statistics of the inputs that actually contain information about the class labels. We prune features at each layer by a simple two-step procedure that first ranks them by estimates of their mutual information, then truncates them using cross-validation. More specifically, in the first step, we discretize each real-valued feature and construct class-conditional and marginal histograms of its discretized values; then, using these histograms, we estimate each feature's mutual information with the class label and sort the features in order of these estimates <ref type="bibr" target="#b13">[14]</ref>. In the second step, considering only the first w features in this ordering, we compute the error rates of a basic kNN classifier using Euclidean distances in feature space. We compute these error rates on a held-out set of validation examples for many values of k and w and record the optimal values for each layer. The optimal w determines the number of informative features passed onto the next layer; this is essentially the width of the layer. In practice, we varied k from 1 to 15 and w from 10 to 300; though exhaustive, this cross-validation can be done quickly and efficiently by careful bookkeeping. Note that this procedure determines the architecture of the network in a greedy, layer-by-layer fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distance metric learning.</head><p>Test examples in MKMs are classified by a variant of kNN classification on the outputs of the final layer. Specifically, we use large margin nearest neighbor (LMNN) classification <ref type="bibr" target="#b14">[15]</ref> to learn a Mahalanobis distance metric for these outputs, though other methods are equally viable <ref type="bibr" target="#b16">[17]</ref>. The use of LMNN is inspired by the supervised fine-tuning of weights in the training of deep architectures <ref type="bibr" target="#b17">[18]</ref>. In MKMs, however, this supervised training only occurs at the final layer (which underscores the importance of feature selection in earlier layers). LMNN learns a distance metric by solving a problem in semidefinite programming; one advantage of LMNN is that the required optimization is convex. Test examples are classified by the energy-based decision rule for LMNN <ref type="bibr" target="#b14">[15]</ref>, which was itself inspired by earlier work on multilayer neural nets <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiments on multiway classification</head><p>We evaluated MKMs on the two multiclass data sets from previous benchmarks <ref type="bibr" target="#b10">[11]</ref> that exhibited the largest performance gap between deep and shallow architectures. The data sets were created from the MNIST data set <ref type="bibr" target="#b19">[20]</ref> of 28 × 28 grayscale handwritten digits. The mnist-back-rand data set was generated by filling the image background by random pixel values, while the mnist-back-image data set was generated by filling the image background with random image patches; examples are shown in Figs. <ref type="figure" target="#fig_18">4 and 5</ref>. Each data set contains 12000 and 50000 training and test examples, respectively. test set. SVMs with arc cosine kernels have error rates from 22.36-25.64%. Resul kernels of varying degree (n) and levels of recursion ( ). The best previous results SVMs with RBF kernels and 22.50% for deep belief nets <ref type="bibr" target="#b1">[2]</ref>. See text for details.  with RBF kernels and 18.63% for deep belief nets <ref type="bibr" target="#b1">[2]</ref>. See text for details.</p><p>2000 training examples as a validation set to choose the margin penalty parameter this parameter by cross-validation, we then retrained each SVM using all the trainin reference, we also report the best results obtained previously from three layer deep be 3) and SVMs with RBF kernels (SVM-RBF). These references are representativ state-of-the-art for deep and shallow architectures on these data sets.</p><p>The right panels of figures 2 and 3 show the test set error rates from arc cosine ke degree (n) and levels of recursion ( ). We experimented with kernels of degree n corresponding to single layer threshold networks with "step", "ramp", and "quarterfunctions. We also experimented with the multilayer kernels described in section from one to six levels of recursion. Overall, the figures show that on these two different arc cosine kernels outperform the best results previously reported for S kernels and deep belief nets. We give more details on these experiments below. though, we note that SVMs with arc cosine kernels are very straightforward to trai with RBF kernels, they do not require tuning a kernel width parameter, and unlike d they do not require solving a difficult nonlinear optimization or searching over possib</p><p>In our experiments, we quickly discovered that the multilayer kernels only perfor n = 1 kernels were used at higher ( &gt; 1) levels in the recursion. Figs. 2 and 3 ther these sets of results; in particular, each group of bars shows the test error rates w kernel (of degree n = 0, 1, 2) was used at the first layer of nonlinearity, while the n used at successive layers. We do not have a formal explanation for this effect. How only the n = 1 arc cosine kernel preserves the norm of its inputs: the n = 0 kernel onto a unit hypersphere in feature space, while higher-order (n &gt; 1) kernels may spaces with severely distorted dynamic ranges. Therefore, we hypothesize that only kernels preserve sufficient information about the magnitude of their inputs to wo composition with other kernels.</p><p>Finally, the results on both data sets reveal an interesting trend: the multilayer ar often perform better than their single layer counterparts. Though SVMs are shallo 5     with RBF kernels and 18.63% for deep belief nets <ref type="bibr" target="#b1">[2]</ref>. See text for details.</p><p>2000 training examples as a validation set to choose the margin penalty parameter; af this parameter by cross-validation, we then retrained each SVM using all the training ex reference, we also report the best results obtained previously from three layer deep belief 3) and SVMs with RBF kernels (SVM-RBF). These references are representative of state-of-the-art for deep and shallow architectures on these data sets.</p><p>The right panels of figures 2 and 3 show the test set error rates from arc cosine kernel degree (n) and levels of recursion ( ). We experimented with kernels of degree n = corresponding to single layer threshold networks with "step", "ramp", and "quarter-pip functions. We also experimented with the multilayer kernels described in section 2.3 from one to six levels of recursion. Overall, the figures show that on these two data different arc cosine kernels outperform the best results previously reported for SVM kernels and deep belief nets. We give more details on these experiments below. At though, we note that SVMs with arc cosine kernels are very straightforward to train; u with RBF kernels, they do not require tuning a kernel width parameter, and unlike deep they do not require solving a difficult nonlinear optimization or searching over possible a</p><p>In our experiments, we quickly discovered that the multilayer kernels only performe n = 1 kernels were used at higher ( &gt; 1) levels in the recursion. Figs. <ref type="figure" target="#fig_16">2 and 3</ref> therefor these sets of results; in particular, each group of bars shows the test error rates when kernel (of degree n = 0, 1, 2) was used at the first layer of nonlinearity, while the n = 1 used at successive layers. We do not have a formal explanation for this effect. Howeve only the n = 1 arc cosine kernel preserves the norm of its inputs: the n = 0 kernel ma onto a unit hypersphere in feature space, while higher-order (n &gt; 1) kernels may in spaces with severely distorted dynamic ranges. Therefore, we hypothesize that only n = kernels preserve sufficient information about the magnitude of their inputs to work e composition with other kernels.</p><p>Finally, the results on both data sets reveal an interesting trend: the multilayer arc co often perform better than their single layer counterparts. Though SVMs are shallow a 5  We trained MKMs with arc-cosine kernels and RBF kernels in each layer. For each data set, we initially withheld the last 2000 training examples as a validation set. Performance on this validation set was used to determine each MKM's architecture, as described in the previous section, and also to set the kernel width in RBF kernels, following the same methodology as earlier studies <ref type="bibr" target="#b10">[11]</ref>. Once these parameters were set by cross-validation, we re-inserted the validation examples into the training set and used all 12000 training examples for feature selection and distance metric learning. For kernel PCA, we were limited by memory requirements to processing only 6000 out of 12000 training examples. We chose these 6000 examples randomly, but repeated each experiment five times to obtain a measure of average performance. The results we report for each MKM are the average performance over these five runs.</p><p>The right panels of Figs. <ref type="figure" target="#fig_18">4 and 5</ref> show the test set error rates of MKMs with different kernels and numbers of layers . For reference, we also show the best previously reported results <ref type="bibr" target="#b10">[11]</ref> using traditional SVMs (with RBF kernels) and deep belief nets (with three layers). MKMs perform significantly better than shallow architectures such as SVMs with RBF kernels or LMNN with feature selection (reported as the case = 0). Compared to deep belief nets, the leading MKMs obtain slightly lower error rates on one data set and slightly higher error rates on another.</p><p>We can describe the architecture of an MKM by the number of selected features at each layer (including the input layer). The number of features essentially corresponds to the number of units in each layer of a neural net. For the mnist-back-rand data set, the best MKM used an n = 1 arc-cosine kernel and 300-90-105-136-126-240 features at each layer. For the mnist-back-image data set, the best MKM used an n = 0 arc-cosine kernel and 300-50-130-240-160-150 features at each layer.</p><p>MKMs worked best with arc-cosine kernels of degree n = 0 and n = 1. The kernel of degree n = 2 performed less well in MKMs, perhaps because multiple iterations of kernel PCA distorted the dynamic range of the inputs (which in turn seemed to complicate the training for LMNN). MKMs with RBF kernels were difficult to train due to the sensitive dependence on kernel width parameters. It was extremely time-consuming to cross-validate the kernel width at each layer of the MKM. We only obtained meaningful results for one and two-layer MKMs with RBF kernels.</p><p>We briefly summarize many results that we lack space to report in full. We also experimented on multiclass data sets using SVMs with single and multi-layer arc-cosine kernels, as described in section 2. For multiclass problems, these SVMs compared poorly to deep architectures (both DBNs and MKMs), presumably because they had no unsupervised training that shared information across examples from all different classes. In further experiments on MKMs, we attempted to evaluate the individual contributions to performance from feature selection and LMNN classification. Feature selection helped significantly on the mnist-back-image data set, but only slightly on the mnist-backrandom data set. Finally, LMNN classification in the output layer yielded consistent improvements over basic kNN classification provided that we used the energy-based decision rule <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>In this paper, we have developed a new family of kernel functions that mimic the computation in large, multilayer neural nets. On challenging data sets, we have obtained results that outperform previous SVMs and compare favorably to deep belief nets. More significantly, our experiments validate the basic intuitions behind deep learning in the altogether different context of kernel-based architectures. A similar validation was provided by recent work on kernel methods for semi-supervised embedding <ref type="bibr" target="#b6">[7]</ref>. We hope that our results inspire more work on kernel methods for deep learning.</p><p>There are many possible directions for future work. For SVMs, we are currently experimenting with arc-cosine kernel functions of fractional and (even negative) degree n. For MKMs, we are hoping to explore better schemes for feature selection <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> and kernel selection <ref type="bibr" target="#b22">[23]</ref>. Also, it would be desirable to incorporate prior knowledge, such as the invariances modeled by convolutional neural nets <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b3">4]</ref>, though it is not obvious how to do so. These issues and others are left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Derivation of kernel function</head><p>In this appendix, we show how to evaluate the multidimensional integral in eq. ( <ref type="formula" target="#formula_0">1</ref>) for the arc-cosine kernel. Let θ denote the angle between the inputs x and y. Without loss of generality, we can take x to lie along the w 1 axis and y to lie in the w 1 w 2 -plane. Integrating out the orthogonal coordinates of the weight vector w, we obtain the result in eq. (3) where J n (θ) is the remaining integral:</p><formula xml:id="formula_16">J n (θ) = dw 1 dw 2 e − 1 2 (w 2 1 +w<label>2</label></formula><p>2 ) Θ(w 1 ) Θ(w 1 cos θ + w 2 sin θ) w n 1 (w 1 cos θ + w 2 sin θ) n . <ref type="bibr" target="#b13">(14)</ref> Changing variables to u = w 1 and v = w 1 cos θ+w 2 sin θ, we simplify the domain of integration to the first quadrant of the uv-plane:</p><formula xml:id="formula_17">J n (θ) = 1 sin θ ∞ 0 du ∞ 0 dv e −(u 2 +v 2 −2uv cos θ)/(2 sin 2 θ) u n v n . (<label>15</label></formula><formula xml:id="formula_18">)</formula><p>The prefactor of (sin θ) −1 in eq. ( <ref type="formula" target="#formula_17">15</ref>) is due to the Jacobian. To simplify the integral further, we adopt polar coordinates u = r cos( ψ 2 + π 4 ) and v = r sin( ψ 2 + π 4 ). Then, integrating out the radius coordinate r, we obtain:</p><formula xml:id="formula_19">J n (θ) = n! (sin θ) 2n+1 π 2 0 dψ cos n ψ (1 − cos θ cos ψ) n+1 .<label>(16)</label></formula><p>To evaluate eq. ( <ref type="formula" target="#formula_19">16</ref>), we first consider the special case n = 0. The following result can be derived by contour integration in the complex plane <ref type="bibr" target="#b24">[25]</ref>:</p><formula xml:id="formula_20">π/2 0 dψ 1 − cos θ cos ψ = π − θ sin θ . (<label>17</label></formula><formula xml:id="formula_21">)</formula><p>Substituting eq. ( <ref type="formula" target="#formula_20">17</ref>) into our expression for the angular part of the kernel function in eq. ( <ref type="formula" target="#formula_19">16</ref>), we recover our earlier claim that J 0 (θ) = π − θ. Related integrals for the special case n = 0 can also be found in earlier work <ref type="bibr" target="#b7">[8]</ref>.For the case n &gt; 0, the integral in eq. ( <ref type="formula" target="#formula_19">16</ref>) can be performed by the method of differentiating under the integral sign. In particular, we note that: </p><p>Substituting eq. ( <ref type="formula" target="#formula_22">18</ref>) into eq. ( <ref type="formula" target="#formula_19">16</ref>), then appealing to the previous result in eq. ( <ref type="formula" target="#formula_20">17</ref>), we recover the expression for J n (θ) in eq. ( <ref type="formula" target="#formula_3">4</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Single layer network and activation functions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>17</head><label>17</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Left: examples from the convex data set. Right: classification error rates on th SVMs with arc cosine kernels have error rates from 17.15-20.51%. Results are shown fo of varying degree (n) and levels of recursion ( ). The best previous results are 19.13% f with RBF kernels and 18.63% for deep belief nets [2]. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Left: examples from the rectangles-image data set. Right: classification error rates on the test set. SVMs with arc-cosine kernels have error rates from 22.36-25.64%. Results are shown for kernels of varying degree (n) and levels of recursion ( ).The best previous results are 24.04% for SVMs with RBF kernels and 22.50% for deep belief nets<ref type="bibr" target="#b10">[11]</ref>. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>17</head><label>17</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Left: examples from the convex data set. Right: classification error rates on th SVMs with arc cosine kernels have error rates from 17.15-20.51%. Results are shown fo of varying degree (n) and levels of recursion ( ). The best previous results are 19.13% f with RBF kernels and 18.63% for deep belief nets [2]. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Left: examples from the convex data set. Right: classification error rates on the test set.SVMs with arc-cosine kernels have error rates from 17.15-20.51%. Results are shown for kernels of varying degree (n) and levels of recursion ( ). The best previous results are 19.13% for SVMs with RBF kernels and 18.63% for deep belief nets<ref type="bibr" target="#b10">[11]</ref>. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>2 .</head><label>2</label><figDesc>Repeat times: (a) Compute principal components in the feature space induced by a nonlinear kernel. (b) Prune uninformative components from the feature space. 3. Learn a Mahalanobis distance metric for nearest neighbor classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>17</head><label>17</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Left: examples from the convex data set. Right: classification error rates SVMs with arc cosine kernels have error rates from 17.15-20.51%. Results are sh of varying degree (n) and levels of recursion ( ). The best previous results are 19with RBF kernels and 18.63% for deep belief nets<ref type="bibr" target="#b1">[2]</ref>. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Left: examples from the mnist-back-rand data set. Right: classification error rates on the test set for MKMs with different kernels and numbers of layers .MKMs with arc-cosine kernel have error rates from 6.36-7.52%. The best previous results are 14.58% for SVMs with RBF kernels and 6.73% for deep belief nets<ref type="bibr" target="#b10">[11]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Left: examples from the rectangles-image data set. Right: classification error test set. SVMs with arc cosine kernels have error rates from 22.36-25.64%. Results a kernels of varying degree (n) and levels of recursion ( ). The best previous results are SVMs with RBF kernels and 22.50% for deep belief nets [2]. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>17</head><label>17</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Left: examples from the convex data set. Right: classification error rates on SVMs with arc cosine kernels have error rates from 17.15-20.51%. Results are shown of varying degree (n) and levels of recursion ( ). The best previous results are 19.13%with RBF kernels and 18.63% for deep belief nets<ref type="bibr" target="#b1">[2]</ref>. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Left: examples from the mnist-back-image data set. Right: classification error rates on the test set for MKMs with different kernels and numbers of layers . MKMs with arc-cosine kernel have error rates from 18.43-29.79%. The best previous results are 22.61% for SVMs with RBF kernels and 16.31% for deep belief nets [11].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>π 2 0 dψ cos n ψ ( 1 −dψ 1 −</head><label>011</label><figDesc>cos θ cos ψ) n+1 = 1 n! ∂ n ∂(cos θ) n π/2 0 cos θ cos ψ .</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Scaling learning algorithms towards AI</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006-07">July 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised learning of invariant feature hierarchies with applications to object recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 IEEE Conference on Computer Vision and Pattern Recognition (CVPR-07)</title>
				<meeting>the 2007 IEEE Conference on Computer Vision and Pattern Recognition (CVPR-07)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: deep neural networks with multitask learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning (ICML-08)</title>
				<meeting>the 25th International Conference on Machine Learning (ICML-08)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning deep architectures for AI. Foundations and Trends in Machine Learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep learning via semi-supervised embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning (ICML-08)</title>
				<meeting>the 25th International Conference on Machine Learning (ICML-08)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1168" to="1175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Computation with infinite neural networks</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1203" to="1216" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Permitted and forbidden sets in symmetric thresholdlinear networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H R</forename><surname>Hahnloser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Slotine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="621" to="638" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Bayesian Learning for Neural Networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Springer-Verlag New York, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An empirical evaluation of deep architectures on problems with many factors of variation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning (ICML-07)</title>
				<meeting>the 24th International Conference on Machine Learning (ICML-07)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="473" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">LIBSVM: a library for support vector machines</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/˜cjlin/libsvm" />
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Nonlinear component analysis as a kernel eigenvalue problem</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1299" to="1319" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An introduction to variable and feature selection</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elisseeff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1157" to="1182" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nonlinear component analysis as a kernel eigenvalue problem</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technical Report</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>Max-Planck-Institut für biologische Kybernetik</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neighbourhood components analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 19</title>
				<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hoffman</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 IEEE Conference on Computer Vision and Pattern Recognition (CVPR-05)</title>
				<meeting>the 2005 IEEE Conference on Computer Vision and Pattern Recognition (CVPR-05)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<title level="m">The MNIST database of handwritten digits</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sparse kernel principal component analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tipping</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 13</title>
				<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Sparse kernel feature analysis</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">L</forename><surname>Mangasarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<idno>99-04</idno>
		<imprint>
			<date type="published" when="1999">1999</date>
			<pubPlace>Madison</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin, Data Mining Institute</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning the kernel matrix with semidefinite programming</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="27" to="72" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Functions of a Complex Variable: Theory and Technique</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Carrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Pearson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Society for Industrial and Applied Mathematics</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
