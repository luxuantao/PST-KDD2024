<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning-Based Classification of Hyperspectral Data</title>
				<funder ref="#_GPKbejG #_WEnk2xC">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_75e32Z5">
					<orgName type="full">Fundamental Research Funds for the Central Universities</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Yushi</forename><forename type="middle">Chen</forename><surname>Chen</surname></persName>
							<email>chenyushi@hit.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
							<email>lin.zhouhan@gmail.com</email>
						</author>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Xing</forename><surname>Zhao</surname></persName>
							<email>zhaoxing@hit.edu.cn</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Gang</forename><surname>Wang</surname></persName>
							<email>wanggang@ntu.edu.sg</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Yanfeng</forename><surname>Gu</surname></persName>
							<email>yfgu@hit.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of Image and Information Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<postCode>150001</postCode>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School Electrics and Electronics Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<postCode>639798</postCode>
									<settlement>Singapore</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Learning-Based Classification of Hyperspectral Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/JSTARS.2014.2329330</idno>
					<note type="submission">received October 14, 2013; revised April 24, 2014; accepted May 30, 2014. Date of publication June 25, 2014; date of current version August 01, 2014.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Autoencoder (AE)</term>
					<term>deep learning</term>
					<term>feature extraction</term>
					<term>hyperspectral data classification</term>
					<term>logistic regression</term>
					<term>stacked autoencoder (SAE)</term>
					<term>support vector machine (SVM)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Classification is one of the most popular topics in hyperspectral remote sensing. In the last two decades, a huge number of methods were proposed to deal with the hyperspectral data classification problem. However, most of them do not hierarchically extract deep features. In this paper, the concept of deep learning is introduced into hyperspectral data classification for the first time. First, we verify the eligibility of stacked autoencoders by following classical spectral information-based classification. Second, a new way of classifying with spatial-dominated information is proposed. We then propose a novel deep learning framework to merge the two features, from which we can get the highest classification accuracy. The framework is a hybrid of principle component analysis (PCA), deep learning architecture, and logistic regression. Specifically, as a deep learning architecture, stacked autoencoders are aimed to get useful high-level features. Experimental results with widely-used hyperspectral data indicate that classifiers built in this deep learning-based framework provide competitive performance. In addition, the proposed joint spectral-spatial deep neural network opens a new window for future research, showcasing the deep learning-based methods' huge potential for accurate hyperspectral data classification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>B Y COMBINING imaging and spectroscopy technology, hyperspectral remote sensing can get spatially and spectrally continuous data simultaneously. Hyperspectral data are becoming a valuable tool for monitoring the Earth's surface <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, and are used in a wide array of applications. An incomplete list includes agriculture <ref type="bibr" target="#b2">[3]</ref>, mineralogy <ref type="bibr" target="#b3">[4]</ref>, surveillance <ref type="bibr" target="#b4">[5]</ref>, physics <ref type="bibr" target="#b5">[6]</ref>, astronomy <ref type="bibr" target="#b6">[7]</ref>, chemical imaging <ref type="bibr" target="#b7">[8]</ref>, and environmental sciences <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. A common technique in these applications is the classification of each pixel in hyperspectral data. If successfully exploited, the hyperspectral data can yield higher classification accuracies and more detailed class taxonomies <ref type="bibr" target="#b10">[11]</ref>. However, there are several critical problems in the classification of hyperspectral data: 1) curse of dimensionality, because of the high number of spectral channels; 2) limited number of labeled training samples; and 3) large spatial variability of spectral signature <ref type="bibr" target="#b11">[12]</ref>.</p><p>A lot of different classification methods have been proposed to deal with hyperspectral data classification. Traditional hyperspectral data classification methods use spectral information only, and the classification algorithms typically include parallelepiped classification, k-nearest-neighbors, maximumlikelihood, minimum distance, and logistic regression <ref type="bibr" target="#b12">[13]</ref>. The majority of these above algorithms suffer a lot from the "curse of dimensionality." To deal with the high dimensionality and limited training samples of hyperspectral data <ref type="bibr" target="#b13">[14]</ref>, some dimensionality reduction-based classification methods were proposed. Transformation is one method available to deal with high dimensionality <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref>. Band selection is another method available to mitigate this "curse" <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b19">[20]</ref>.</p><p>In <ref type="bibr" target="#b20">[21]</ref>, a promising classification method, support vector machine (SVM), is introduced for hyperspectral data classification. SVM exhibits low sensitivity to high dimensionality and is unlikely to suffer from the Hughes phenomenon <ref type="bibr" target="#b21">[22]</ref>. In most cases, SVM-based classifiers can obtain better classification accuracy than other widely used pattern recognition techniques <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b21">[22]</ref>. For a long time, these classifiers were the state-of-theart methods <ref type="bibr" target="#b22">[23]</ref>.</p><p>Spatial information has been growing more and more important for hyperspectral data classification in recent years <ref type="bibr" target="#b29">[30]</ref>. Spatial-spectral classification methods provide significant advantages in terms of improving performance <ref type="bibr" target="#b9">[10]</ref>. To deal with spatial variability of spectral signature, some recent approaches try to incorporate spatial information into consideration <ref type="bibr" target="#b30">[31]</ref>- <ref type="bibr" target="#b32">[33]</ref>. In <ref type="bibr" target="#b31">[32]</ref>, the proposed method based on the fusion of morphological information and original data followed by SVM provides good classification results. In <ref type="bibr" target="#b32">[33]</ref>, a new classification framework is proposed to exploit the spatial and spectral information using loopy belief propagation and active learning. In recent years, sparse representation-based methods have been widely used in many fields. In <ref type="bibr" target="#b33">[34]</ref>, spatial-spectral kernel sparse representation is proposed to deal with hyperspectral data classification.</p><p>Considering the machine learning task of classification, classifiers like linear SVM and logistic regression can be attributed to single-layer classifiers, whereas decision tree or SVM with kernels are believed to have two layers <ref type="bibr" target="#b23">[24]</ref>. As is confirmed in neuroscience, human brains perform well in tasks like object recognition because of its multiple stages of processing from retina to cortex <ref type="bibr" target="#b24">[25]</ref>. Similarly, machine learning systems with multiple layers of processing extract more abstract, invariant features of data, and thus are believed to have the ability of yielding higher classification accuracy than those traditional, shallower classifiers. These deep architectures have been shown to yield promising performance in many field including classification or regression tasks that involve image <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b46">[47]</ref>, language <ref type="bibr" target="#b27">[28]</ref>, and speech <ref type="bibr" target="#b28">[29]</ref>.</p><p>In this paper, we introduce deep learning-based feature extraction for hyperspectral data classification for the first time. Our work focuses on applying autoencoder (AE), which is one of the deep architecture-based models, to learn deep features of hyperspectral data in an unsupervised manner. Our methods exploit single-layer AE and multi-layer stacked AE (SAE) to learn shallow and deep features of hyperspectral data, respectively. Furthermore, we propose a new way of extracting spatialdominated information for classification. At last, we propose a novel classification framework dealing with joint spectralspatial information, which utilizes all of the features extracted in the former two sections.</p><p>The rest of this paper is organized into six sections. Section II is a description of deep learning, AE, and SAE models used in this paper. In Section III, we focus on classifying with spectral features, whereas Section IV details a new way of incorporating spatial information by extracting spatial-dominated features. In Section V, we further merge the former two spectral and spatial approaches and propose a novel joint spectral-spatial deep learning framework, which yields the highest classification accuracy. Experimental results are shown in Section VI. Section VII summarizes the observations and completes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. DEEP LEARNING, AE, AND SAE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deep Learning</head><p>As early as 1989, the universal expressive power of three-layer nets was proved via bumps and Fourier ideas <ref type="bibr" target="#b34">[35]</ref>. The proof showed surprisingly that any continuous function from input to output can be implemented in a three-layer net, given sufficient number of hidden units and proper nonlinearities in activation function and weights. However, due to the lack of proper training algorithms in early years, people could not harness this powerful model until Hinton proposed his deep learning idea in 2006 <ref type="bibr" target="#b26">[27]</ref>.</p><p>Deep learning involves a class of models which try to hierarchically learn deep features of input data with very deep neural networks, typically deeper than three layers. The network is first layer-wise initialized via unsupervised training and then tuned in a supervised manner. In this scheme, high-level features can be learned from low-level ones, whereas the proper features can be formulated for pattern classification in the end. Deep models can potentially lead to progressively more abstract and complex features at higher layers, and more abstract features are generally invariant to most local changes of the input. According to some recent papers <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, deep models can give better approximation to nonlinear functions than shallow models.</p><p>Typical deep neural network architectures include deep belief networks (DBNs) <ref type="bibr" target="#b37">[38]</ref>, deep Boltzmann machines (DBMs) <ref type="bibr" target="#b38">[39]</ref>, SAEs <ref type="bibr" target="#b39">[40]</ref>, and stacked denoising AEs (SDAEs) <ref type="bibr" target="#b40">[41]</ref>.</p><p>The layer-wise training models have a bunch of alternatives such as restricted Boltzmann machines (RBMs) <ref type="bibr" target="#b41">[42]</ref>, pooling units <ref type="bibr" target="#b42">[43]</ref>, convolutional neural networks (CNNs) <ref type="bibr" target="#b43">[44]</ref>, AEs, and denoising AEs (DAE) <ref type="bibr" target="#b39">[40]</ref>. In this paper, we adopt one of the above deep learning models, AE, for hyperspectral data classification and choose SAEs as the corresponding deep architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Autoencoders</head><p>An AE has one visible layer of inputs, one hidden layer of units, one reconstruction layer of d units, and an activation function (Fig. <ref type="figure" target="#fig_0">1</ref>).</p><p>During training, it first maps the input to the hidden layer and produces the latent activity . The network corresponding to this step is shown in the boxed part of Fig. <ref type="figure" target="#fig_0">1</ref> and is called an "encoder." Then, is mapped by a "decoder" to an output layer that has the same size of the input layer, which is called "reconstruction." The reconstructed values are denoted as . Mathematically, these two steps can be formulated as where and denote the input-to-hidden and the hidden-tooutput weights, respectively, and denote the bias of hidden and output units, and denotes the activation function. Conventionally, the nonlinearity is provided in . There are a lot of alternatives for such as sigmoid function, hyperbolic tangent, and rectified linear function.</p><p>In our paper, the following constraint holds</p><p>We say that the AE has tied weights, which helps to halve model parameters. Thus, we have three groups of parameters remaining to learn: , , .</p><p>The goal of training is to minimize the "error" between input and reconstruction, i.e., where is dependent on parameters , , while is given.</p><p>stands for the "error," which can be defined in a variety of ways. Thus, the weight updating rule can be defined as (where denotes the learning rate)</p><p>After training the network, the reconstruction layer together with its parameters are removed and the learned feature lies in the hidden layer, which can subsequently be used for classification or used as the input of a higher layer to produce a deeper feature.</p><p>The power of AE lies in this form of reconstruction-oriented training. Note that during reconstruction, it only uses the information in hidden layer activity , which is encoded as features from input. If the model can recover original input perfectly from , it means that retains enough information of the input. And the learned nonlinear transformation, which is defined by those weights and biases, can be deemed as a good feature extraction step. So, stacking the encoders trained in this manner minimizes information loss. At the meantime, they preserve abstract and invariant information in deeper feature. This is the reason why we choose AE to progressively extract deep features for hyperspectral data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Stacked AE</head><p>Stacking the input and hidden layers of AEs together layer by layer constructs a SAE. The model is used to generate deep features of hyperspectral data. Fig. <ref type="figure" target="#fig_1">2</ref> shows a typical instance of a SAE connected with a subsequent logistic regression classifier.</p><p>The first AE maps inputs in 0th layer to a first layer feature in first layer. It is trained using the same method introduced in Section II-B. After we finish training the first layer AE, subsequent layers of AEs are trained via the output of its previous layer. For example, although we are training the AE between the second and third layer, we try to reconstruct the output of the second layer according to the activity of the third layer. After this layer of training, the decoder of the third layer AE is cast away and only the input-to-hidden parameters are incorporated as weights between the second and the third layer.</p><p>If the subsequent classifier is implemented as a neural network too, parameters throughout the whole network can be adjusted slightly while we are training the classifier. This step is called fine-tuning. For logistic regression, the training is simply back propagation, searching for a minimum in a peripheral region of parameters initialized by the former step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. CLASSIFYING WITH SPECTRAL FEATURES</head><p>There exist some motivations to extract robust deep spectral features. First, because of the complex situation of lighting in the large scene, objects of the same class show different spectral characteristics in different locations. For example, a lawn exposed to direct sunlight shows different spectral characteristics from a similar lawn eclipsed from the sunlight by a high building. Also, scattering from other peripheral ground objects tilts the spectra of the lawn and changes its characteristics too. Other factors involve rotations of the sensor, different atmospheric scattering conditions, and so on. According to these factors, the probability distribution of a certain class is hard to be one-hot and has variations over multiple directions in the feature space. These complex variations of spectra make it hopeless to analyze pixel by pixel how they are affected by their tangent pixels in the complicated real situation, thus they demand more robust and invariant features. It is believed that deep architectures can potentially lead to progressively more abstract features at higher layers of feature, and more abstract features are generally invariant to most local changes of the input <ref type="bibr" target="#b23">[24]</ref>.</p><p>To get more generally invariant features and tackle these problems, a deep spectral feature of hyperspectral data can be learned progressively layer by layer with the aforementioned AE models. Generally speaking, we first compute features via a SAE and deem them as the features of data, then construct a logistic regression classifier on top of the neural network to finish the classification phase. By adjusting different numbers of layers of AEs, both shallow and deep features can be learned. Fig. <ref type="figure" target="#fig_2">3</ref> shows a typical instance of the deep architecture used in our paper. The training procedure will be detailed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Hierarchal Pretraining</head><p>The first stage is to learn a deep feature of spectra via pretraining a SAE in a hierarchal manner, which is outlined in  Section II-C. Here, we derive the detailed training criterion while training an AE within each layer.</p><p>To get a nonlinear mapping, the activation function in (1)-( <ref type="formula">2</ref>) is set to be a sigmoidal function in both the encoder and decoder Tied weights are used while training the AE. Thus, there remain three groups of parameters to learn: , , and .</p><p>Before setting the updating rule for the weights, we need to properly set the "error" rule-a cost function-first. There are a lot of ways to define such a cost function for reconstruction. Since sigmoid activation is used, the derivative tends to asymptotically close to 0 as the output of the neuron draws near 0 or 1. If we use an ordinary cost like mean-square error, the gradient of the cost will also suffer from the same problem, which results in an unacceptably slow training speed. However, it can be found from the following derivation that cross-entropy tends to allow errors to change weights even when nodes saturate (i.e., outputs are close to 0 or 1.). So we always use cross-entropy when activation is set to be sigmoidal.</p><p>In our implementation, the cost is actually computed on a mini-batch of inputs since we adopt a mini-batch update strategy for the large dataset where denotes the input vector size and denotes the minibatch size.</p><p>denotes th element of the th input (reconstruction) in the mini-batch. The inner summation is over the input dimension, whereas the outer is over a whole mini-batch.</p><p>We optimize (9) using the mini-batch stochastic gradient descent method. We will now derive the partial differentials of cost with respect to parameters , , and . First, we will rewrite the reconstruction in a scalar form where denotes the net input of the th hidden (output) unit, given the th sample in the mini-batch.</p><p>It is explicit that the first-order and second-order derivative of (8) are Having deducted ( <ref type="formula">10</ref>)-( <ref type="formula">12</ref>), we can compute the partial derivatives of the reconstruction over parameters , , and . To simplify notations, we still show these equations in a scalar form where means the weight connecting the th input and the th hidden unit.</p><p>stands for bias of the th unit in the hidden (reconstruction) layer.</p><p>Putting them ( <ref type="formula">10</ref>)-( <ref type="formula">17</ref>) all together, we have partial differentials of cost (9) over parameters , and Substituting them into (5)-( <ref type="formula">7</ref>), the weight updating rules are determined.</p><p>After training the network, we remove the reconstruction layer and deem the hidden activity to be the learned feature. Subsequent layers are trained in the same manner, but their inputs are instead the outputs of their former layers. The SAE is thus constructed with encoders layer by layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Fine-Tuning and Classification</head><p>To integrate the layers of neural networks and perform classification by utilizing the learned feature, we need to fine-tune the whole pretrained network with a logistic regression classifier, which uses soft-max as its output-layer activation. Soft-max ensures the activation of each output unit sums to 1, so that we can deem the output as a set of conditional probabilities. For example, given input vector , which is an output of former layers of AEs, the probability that the input belongs to category equals where and are weights and biases of the logistic regression layer, and the summation is over all the output units.</p><p>The output-layer size is set to be the same as the total number of classes, and the input has the same size as the dimension of last-layer features. Since the logistic regression is implemented as a single-layer neural network, it can be merged with the former layers of networks to get a deep classifier. The fitting of the classifier is conducted over the whole architecture, but with very slight learning rates on former layer AEs. Expressions for the partial derivatives can be very complicated, but deducing them is similar to those in Section III-A.</p><p>In a nutshell, the whole flowchart of the proposed SAE-LR algorithm is as follows. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>for every layer ( )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>Construct an autoencoder with d_vis input neurons, d_hid hidden neurons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>if is the first layer (i.e., ) 6.</p><p>7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8.</head><p>Set input of the autoencoder to be initial data. 9. else 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11.</head><p>12.</p><p>Set input of the AE to be the output of its former layer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CLASSIFYING WITH SPATIAL-DOMINATED FEATURES</head><p>Unlike other hyperspectral data spatial information extraction methods which only use the four or eight tangent neighbors or simple filtering, our deep framework takes all the pixels in a flat neighbor region into consideration, and lets the AEs learn the feature by itself. The overall flowchart of our proposed method is detailed in Fig. <ref type="figure" target="#fig_3">4</ref>.</p><p>We propose to take all voxels in a neighborhood region of a certain pixel in the original data into consideration. Due to the hundreds of channels along the spectral dimension, it always has tens of thousands of dimensions. A large neighborhood region will result in too large input dimension for the classifier, containing too large amount of redundancy. So, in the first layer, PCA is introduced to condense the whole image, to reduce the data dimension to an acceptable scale and in the meantime reserving spatial information. Since we mainly care about incorporating spatial information in this method, we use PCA along the spectral dimension and only retain the first several principle components. The PCA transformation matrix is fitted on the whole image, both for tagged and untagged pixels. This step does cast away part of the spectral information, but since PCA is conducted for pixel vectors, the spatial information remains intact. Then, in the second layer, we extract a neighborhood region of the pixel in this condensed data, which has only several principle components in its spectral dimension. This layer yields a total dimension of several hundreds and that is acceptable.</p><p>After these processes, we "flatten" the data in the third layer, i.e., stretch it to a 1-D vector, and feed it into a SAE. Subsequent layers include a layer-wise training SAE and fine-tune the whole model with logistic regression. These steps are similar to the former subsection which deals with deep spectral feature, and thus we will not repeat describing them here.</p><p>The procedures that our algorithm consists of are detailed below. 3. PCA transform the image.</p><p>4. Retain the first principle components. Thus we have an image of size.</p><p>5. for each pixel</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6.</head><p>Crop a neighboring region for each point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7.</head><p>For those points near the edge that don't have enough surrounding pixels, fill in with its mirror.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8.</head><p>Flatten the array into a vector of size. 12. end</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. JOINT SPECTRAL-SPATIAL CLASSIFICATION FRAMEWORK</head><p>In this section, we integrate the spectral and spatial-dominated features together to construct a joint spectral-spatial classification framework. The whole flowchart is shown in Fig. <ref type="figure" target="#fig_7">5</ref>.</p><p>The spectrum of a pixel should first be taken into consideration, since it contains the most important information for discriminating different kinds of ground categories. For spatial information, we extract the first several principle components of a neighborhood region to get the spatial-dominated information, which helps improve classification accuracy as verified in Section IV. This procedure corresponds to the first three steps of processing in Section IV (Fig. <ref type="figure" target="#fig_3">4</ref>). These coefficients are then concatenated to the spectrum of that pixel, forming a hybrid set of features consisting of both spectral and spatial information.</p><p>Following training and fine-tuning steps mentioned above, we can eventually get class labels for each pixel. The whole flowchart of this final framework is shown in the following chart.</p><p>Algorithm 3: Joint Spectral-Spatial Classification 1. begin 2. Extract Spatial-dominated feature for each pixel according to Algorithm 2 to form a matrix .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Scale into unit interval.</head><p>4. Normalize the whole initial image onto unit interval.</p><p>5. for each pixel</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6.</head><p>Add spectrum of each pixel on tail of each pixel's feature vector, (i.e., rows in ). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">end</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Description and Experiment Design</head><p>In our study, two hyperspectral datasets with different environmental settings are used to validate our proposed methods. They are the mixed vegetation site over Kennedy Space Center (KSC), FL, USA, and an urban site over the city of Pavia, Italy.</p><p>The first study site lies around KSC, FL, USA (Fig. <ref type="figure" target="#fig_8">6</ref>). The image data was acquired by the National Aeronautics and Space Administration (NASA) Airborne Visible/Infrared Imaging Spectrometer instrument, on <ref type="bibr">March 23, 1996</ref>. AVIRIS acquires data in a range of 224 bands with wavelengths ranging from 0.4 to ? . The KSC data with has a spatial resolution of 18 m. Given water absorption and low signal-tonoise ratio (SNR) bands, 176 spectral bands are used for classification with 48 bands discarded. 13 different land-cover classes available in the original dataset are displayed in Table <ref type="table" target="#tab_2">I</ref>. The second dataset is gathered by a sensor known as the reflective optics system imaging spectrometer (ROSIS-3) over the city of Pavia, Italy, with (Fig. <ref type="figure">7</ref>). 115 bands are collected in the ? range of the electromagnetic spectrum. The high spatial resolution of 1.3 m per pixel aims to avoid a high fraction of mixed pixels. In the experiment, some bands have been removed due to noise; the remaining 103 channels are used for the classification. Nine land cover classes are selected, which are shown in Fig. <ref type="figure">7</ref> and the numbers of samples for each class are displayed in Table <ref type="table" target="#tab_3">II</ref>.</p><p>In both images, we split the tagged parts of the image into three sets, i.e., training, validation, and testing data, with a split ratio 6:2:2. That is, we randomly choose 60% of the tagged samples as the training set, and 20% and 20% for the validation and testing sets, respectively. During training, we use the training set to learn weights and biases of each neuron and use the validation set to tune the best super-parameters like hidden unit sizes or hidden layer numbers. The test set is used to produce final classification results. Thus, small classes will be trained and tested with a smaller number of pixels in contrast with large classes.</p><p>Experiments were organized into four parts. The first aims at analyzing the behavior of AEs, which are the building blocks of our proposed methods. In the second experiment, the effectiveness of deep architecture is tested in comparison to the SVMbased method. In the third part, we test the classification accuracy of spatial-dominated features of hyperspectral data. Finally, the effectiveness of joint spatial-spectral feature, which is the best of all proposed models, is inspected.</p><p>In order to quantitatively compare and estimate the capabilities of the proposed models, overall accuracy (OA), average accuracy (AA), and Kappa coefficient <ref type="bibr" target="#b44">[45]</ref> are used as performance measurement. In the experiments, we split the dataset into three parts, i.e., training, validation and testing data, and apply crossvalidation analysis with the KSC and Pavia datasets <ref type="bibr" target="#b45">[46]</ref>.</p><p>To perform statistical evaluation, we conduct 100 independent replications of the whole process and use the average Kappa coefficient to compare the performance between different methods. As mentioned above, for each replication, the training, validation, and testing data are randomly selected with a ratio of 6:2:2. A paired t-test is performed to test whether the observed increase in the mean Kappa coefficient is statistically significant (at the level of 95%) <ref type="bibr" target="#b45">[46]</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. AEs: Behavior and Analysis</head><p>Single layer AEs are basic building blocks of our proposed models, so we investigate the behavior of AEs in this section before we present classification accuracies of more complicated models.</p><p>1) Reconstruction: First, we examine the quality of the extracted features by checking the quality of the reconstructed spectra. We use a single-layer AE with 100 hidden units and train it on the KSC dataset. It is shown that the AE do progressively learn better reconstruction during training. Since the AE restitutes a rather perfect reconstruction from hundreds of iterating epochs (Fig. <ref type="figure" target="#fig_10">8</ref>) and computing the reconstruction need only the hidden activity (Section II-B), we can say that the learned hidden activity retains enough information from the input. Thus, it can be thought as a good feature set for the input data.</p><p>2) Filters Learned: Suppose the dataset to be processed has N spectral bands, we are using an AE with N input neurons and H hidden neurons. The input-to-hidden layer of an AE is fully connected, so every single hidden unit has its connections to every input neuron. For each hidden unit, it has a fan-in of N connections. The N connections as a whole can be viewed as a "filter" since they behave by filtering away information from some input which represent certain wavelengths and at the same time exaggerating others. In this way, an AE learning with H hidden units can be viewed as learning with H such filters.</p><p>There is a convenient way of visualizing these filters. We truncate the weight vector into pieces of equal length, and vertically concatenate them to form a matrix M. So the matrix M has N entries and for the whole network, we have H such matrixes. Then, for each matrix M, we use the intensities of N pixels of a tiny image patch which has the same size as M (called "filter image") to reflect the N connection. By plotting a filter image for each hidden unit, we can observe some interesting features of these learned filters more conveniently (Fig. <ref type="figure" target="#fig_9">9</ref>). Fig. <ref type="figure" target="#fig_9">9</ref>(a) and (b) shows the filters acquired after training AE s on KSC and Pavia datasets, respectively. Some hidden units have large weights over a small portion of input units and small weights over others, which suggest that a certain wavelength interval is informative and discriminative and others' weights have more complex connecting patterns, having ripples over different input units or showing Gaussian-like noises in some bands. To make the visualization more direct, these 1-D connections are horizontally folded into pixels corresponding with the 176 and 103 input sizes of the KSC and Pavia data. That is why we find all filters are extracting "horizontal" features in all of the plotted filter images. In Fig. <ref type="figure" target="#fig_9">9(a)</ref>, there are 20 hidden units in the trained AE, thus we can see 20 tiny filter images in the plot. For the Pavia data, the situations are similar [Fig. <ref type="figure" target="#fig_9">9(b)</ref>], but with 60 hidden units.</p><p>3) Running Time: We concede that neural networks take longer time to train compared with other machine learning algorithms like KNN or SVM, and so does deep learning. In this section, we focus on how the running time changes with respect to the scale of AE model.</p><p>First, we inspect the training time. We use 3100 training samples for each AE on a NVIDIA GT750M graphics card. The pretraining epochs are set to be 5000, whereas fine-tuning epochs are set to be 50 000. Experimental results [Fig. <ref type="figure" target="#fig_11">10(a)</ref>] show that training time generally grows with the increase of input and hidden sizes. On the contrary, if we keep fix hidden size or input size, training time grows proportionally with respect to training epochs.  On the other hand, an advantage of deep learning algorithms is that they are super-fast on testing. In Table <ref type="table" target="#tab_4">III</ref>, an AE of hidden size 20 on the KSC dataset and 60 on the Pavia dataset with logistic regression is compared with radial basis function (RBF), kernel SVM, linear SVM, and k nearest neighbors. We take all 314 368 pixels in the KSC dataset and 207 400 pixels in the Pavia dataset for classification and compare the running time of all the mentioned classifiers. Experiments in both the two dataset have confirmed that AE runs much faster than other classification algorithms in the control group.</p><p>4) Comparing With Other Feature Extraction Methods: By comparing the AEs with other feature extraction methods, involving principle component analysis (PCA), kernel PCA (KPCA), independent component analysis (ICA), nonnegative matrix factorization (NMF), and factor analysis (FA), we verify the effectiveness of these AE features from the sense of classification.</p><p>First, we substitute the AE in the SAE-LR scheme with these feature extraction methods. All the logistic regression classifiers are set to have learning rate 0.1 and are iterated on the training data for 10 000 epochs. The SAE only consists of one layer of AE. Experiments show that by combining with logistic regression, AE outperforms all other feature extraction methods and gets the highest accuracy.</p><p>To be fair, we also combine the aforementioned feature extraction methods with SVM to verify if AEs bring more benefits for classification. Results show that although logistic regression as a neural network tends to be more sensitive to dimensions, AEs help improve the accuracies of both classifiers. The only exception lies in the AE-SVM case, where factor analysis outperforms AE with 20 extracted features [Fig. <ref type="figure" target="#fig_12">11(b)]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Classification With Spectral Feature</head><p>In Section VI-B, we have examined various characteristics of AEs. In this part of experiment, we begin to exploit their potential by applying them purely to spectral information. Here, we mainly focus on the effect of depths in order to compare with the typical classifier SVM.   In Table <ref type="table" target="#tab_2">IV</ref>, we tried several SAEs with different depths. For the KSC data, it has 176 spectral channels and each hidden layer size is set to 20, and also a logistic regression layer is used on top of the SAE. So the neural networks are constructed as . For the Pavia dataset which has 103 spectral channels and 9 classes, the performance reaches its best when using 60 as the size of hidden layers. The neural networks are like . "Depth" corresponds to the number of 20 or 60-sized layers in the deep neural network. Experiments show that depth does help to increase classification accuracy. Note that in this part of experiment, the AE is not fully tuned, with only 5000 epochs of pretraining and 50 000 epochs of fine-tuning. If we continue training the model, it will yield higher accuracies.</p><p>2) Comparison With SVMs: For comparison with the classical SVM models, we conduct SVM with linear kernel, RBF-SVM with PCA feature extraction, and SVM with RBF kernel on the KSC and Pavia data. The SVM parameters are tuned to achieve the best performance, as is the SAE-LR classifier. To elaborate, the SAE-LR trained for KSC data has 20 hidden units and 1 hidden layer and is trained with 3300 epochs of pretraining and 400 000 epochs of fine-tuning with a very slight learning rate. SAE-LR for the Pavia dataset is constructed similarly, but with 4 layers and 60 hidden units per layer.</p><p>We performed the experiments with same parameter settings as described above 100. Table V shows the mean value of the OA, AA, and Kappa coefficients for the 100 replications. We can see that out of all the methods in the control group, RBF-SVM yields the highest accuracy in terms of mean OA. However, the SAE-LR method turns out to be better than RBF-SVM on all three measurements. It has shown a significant gain in accuracy for both the KSC and Pavia datasets. Further, we performed the paired t-test (as described in Section VI-A) between the trained SAE-LR and the other three SVM models. The detailed statistics of the Kappa coefficients of the four methods are shown in Fig. <ref type="figure" target="#fig_13">12</ref>. Paired t-test results show that improvements on Kappa coefficients are statistically significant (at the level of 95%).</p><p>In the following sections VI-D and VI-E of experiments, we will show that if spatial information is incorporated, classification results will grow much higher, and thus further exceed RBF-SVM's accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Classification With Spatial-Dominated Feature</head><p>If we directly apply SVM on the spatial-dominated information collected by cropping adjacent patches, the accuracy will be slightly higher than that yielded by spectral features (Table <ref type="table" target="#tab_6">VI</ref>). What is more, our deep neural networks confirm that these kinds of features can lead to higher accuracy for classification in terms of mean performance. We inspect our spatial information extraction method by varying the number of retained principle components and depth of neural network.</p><p>1) Differing Principle Components: Although the proposed spatial-dominated method majorly focuses on extracting spatial information of hyperspectral data, using how much spectral information to retain still plays a role in the completeness of  its features. The amount of spectral information can be adjusted by varying the number of principle components (i.e., the in Algorithm 2). Here, we vary the retained principle components from 1 to 8, and check how the final classification accuracy is affected. In Fig. <ref type="figure" target="#fig_15">13</ref>, an SAE-LR model with one hidden layer is constructed. It shows that as the number of principle components grows, the classification accuracies of both images become higher. As a trade-off between accuracy and data size, we empirically choose 4 as the number.</p><p>2) Effect of Depth: Depth also plays an important role in spatial-dominated classification. We train a series of SAEs with different depths, but with fixed principle component numbers and hidden unit numbers to see how the depth of the features effect classification accuracies. Results are show in Fig. <ref type="figure" target="#fig_3">14</ref>. Compared with spectral information, deeper features are required for spatial-dominated information to get the best classification accuracy. This helps us to determine how many layers are needed to get an optimal configuration of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Joint Spectral-Spatial Classification Framework</head><p>This section of experimentation culminates all of our previous methods. By putting both the spectral and spatial information together to form a hybrid input and utilizing the deep classification framework detailed in Section IV-B, we get the highest classification accuracy we have ever attained.</p><p>1) Comparison With Spatial-Dominated Methods and SVMs: Here, we compare joint spectral-spatial classification with the aforementioned spatial methods. We also perform RBF-SVM on both to form a control group (Table <ref type="table" target="#tab_6">VI</ref>). Similarly, experiments are also performed for 100 times. Compared with Table V, we can figure that for both the SAE-LR and RBF-SVM methods, joint features yield higher accuracy than spectral features in terms of mean performance, and while comparing the two methods within each feature set, SAE-LR is more precise. As in the last Section VI-E2, statistical evaluations of Kappa coefficients are plotted as the left four boxes in Fig. <ref type="figure" target="#fig_16">15(a)</ref> and (b). We also performed paired t-tests between SAE-LRs and their corresponding control group SVMs. The results have shown that SAE-LR does achieve higher accuracy.</p><p>2) Comparing With Other Spatial Methods: Spatial information is very important in hyperspectral data classification. Some methods such as extended morphological profile (EMP) try to integrate spatial information into spectral-based classifiers. In the EMP method, principle components of hyperspectral data are computed and then the morphological profiles are used to extract spatial information on the first several components. EMP followed by SVM is a successful spatial-spectral classification method of hyperspectral data. We searched a range of c and g configurations for the SVM used in the EMP RBF-SVM method, and for the KSC data, they are configured as and , whereas those in the Pavia data are , and performed 100 replications [the rightmost column in Fig. <ref type="figure" target="#fig_17">16</ref> 3) Whole Image Classification: In this section, we examine the classification accuracy from a visual perspective. We choose the best SAE-LR models for the spectral, spatial-dominated, and joint sets of information to classify the whole images of KSC and Pavia. All parameters in these models are optimized. From    increase the accuracy of SVM and logistic regression while obtaining the highest accuracy when compared with other feature extraction methods like PCA, KPCA, and NMF.</p><p>For hyperspectral data classification, our proposed SAE-LR method has been proven to provide statistically higher accuracy than RBF-SVM, a classical classifier previously considered to be state-of-the-art in this field. In addition, we also inspected the impact that the depth of feature has on classifying hyperspectral data. Our experimental results suggest that deeper features always lead to higher classification accuracies, though too deep structure will act inversely. Based on our results, we suggest using 4-6 hidden layers of AEs with 20-60 hidden units per layer for hyperspectral data classification tasks. The disadvantage of SAE-LR is its training time, but in compensate, the testing time efficiency is much faster than other methods like SVM or KNN.</p><p>For our proposed spatial-dominated information-based classification, both SAE-LR and SVM have proved the effectiveness of the PCA-window spatial information extraction method. The SAE-LR classifier succeeds in classifying datasets and yields a higher accuracy than traditional spectral information-based methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Single layer AE for hyperspectral data classification. The model learns a hidden feature " " from input " " by reconstructing it on " ." Corresponding parameters are denoted in the network.</figDesc><graphic url="image-1.png" coords="2,360.23,65.65,133.44,102.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Instance of a SAE connected with a logistic regression layer. It has five layers: one input layer, three hidden layers, and an output layer.</figDesc><graphic url="image-2.png" coords="3,58.28,65.88,210.48,160.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Classifying with spectral feature. The classification scheme shown here has five layers: one input layer, three hidden layers of AEs, and an output layer of logistic regression. If we want to learn a shallower feature set, we just remove the higher layers of AE.</figDesc><graphic url="image-3.png" coords="3,302.46,65.65,248.88,93.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Spatial-dominated information classification scheme. The first step of procesing is PCA compressing over spectral dimension, then after flatening the data, AEs are introduced to extract layer-wise deep features.</figDesc><graphic url="image-4.png" coords="5,110.15,65.88,370.32,89.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 2 :</head><label>2</label><figDesc>Classification With Spatial-Dominated Feature 1. begin 2. initialize neighborhood region size , number of principle components , image height , width .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>9 . end 10 .</head><label>910</label><figDesc>Concatenate all vectors to form a matrix . 11. Train a SAE-LR with as the input. Training procedures are the same to Algorithm 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>7 . end 8 .</head><label>78</label><figDesc>Train a SAE-LR with as the input. Training procedures are the same to Algorithm 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Joint spectral-spatial classification framework. Spectral and spatial information are extracted separately via the former mentioned schemes, and feature extraction is conducted via a deep architecture like SAEs. Final classification is implemented as the final layer of the neural network, using classical neural network classifiers like logistic regression.</figDesc><graphic url="image-5.png" coords="6,301.66,65.65,250.56,144.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. NASA data, KSC. Band 20 and corresponding ground truth areas representing 13 land cover classes.</figDesc><graphic url="image-6.png" coords="7,37.98,65.88,251.04,151.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Filter images learned by an AE on (a) KSC dataset and (b) Pavia dataset.Each N-pixel tiny rectangle stands for N input-to-hidden weights that connects each input unit to a same hidden unit. The intensity of each pixel stands for the absolute value of corresponding weights.</figDesc><graphic url="image-10.png" coords="8,356.94,65.65,139.92,191.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Reconstructions of a same input in different iteration epochs. (a) Input spectrum. (b)-(f) Reconstructions of (a) in epoch 1, 10, 100, 1000, and 3500, respectively. Vertical axis stands for normalized reflectance, whereas horizontal axis stands for band numbers.</figDesc><graphic url="image-11.png" coords="8,38.15,65.88,250.80,285.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Factors influencing training time. (a) Training time of an AE with different hidden and input sizes. (b) Training time elapsed on each epoch whereas varying hidden sizes. (c) Training time elapsed on each epoch whereas varying input size.</figDesc><graphic url="image-12.png" coords="9,43.77,65.88,503.04,127.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. (a) AE-LR and (b) AE-SVM performance with respect to hidden sizes on the KSC dataset. Dashed lines stands for performance of the control group and the red solid line stands for AE-based methods. Horizontal axis stands for the number of features we extract in the control group and number of hidden units we use while training an AE. In ICA, we choose the parallel fast ICA algorithm and use initial whitening as the preprocessing step, and the maximum iteration step is set to be 200. In NMF, we use the projected gradient method and we use RBF kernel in KPCA.</figDesc><graphic url="image-14.png" coords="9,326.49,244.97,200.88,329.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Box plot of Kappa coefficients of different methods on (a) KSC and (b) Pavia datasets. Numbers in the abscissa corresponding to 1) SAE-LR; 2) Linear SVM; 3) PCA RBF-SVM; and 4) RBF-SVM. We plot these boxes by doing 100 independent replications. The red line through the center of each box indicates the median value of the Kappa coefficients. The edges of boxes are the 25th and 75th percentiles. Whiskers extend to the maximum and minimum points. Abnormal outliers shown as red " "s.</figDesc><graphic url="image-17.png" coords="10,326.95,65.99,199.92,304.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>(a) and (b)]. Paired t-tests also show that the joint information-based SAE-LR does consistently reach a higher accuracy than the EMP RBF-SVM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Effect of principle components. Fig. 14. Effect of depth.</figDesc><graphic url="image-20.png" coords="11,326.95,199.11,199.92,160.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Box plot of Kappa coefficients of spectral, spatial-dominated and joint classification scheme on (a) KSC and (b) Pavia datasets. Numbers in the abscissa corresponding to 1) SAE-LR on spatial-dominated information; 2) RBF-SVM on spatial-dominated information; 3) SAE-LR on joint information; 4) RBF-SVM on joint information; and 5) EMP RBF-SVM. The meanings of the indicators in the box are the same as Fig. 12.</figDesc><graphic url="image-21.png" coords="12,95.36,65.88,399.84,147.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. Spectral (left), spatial-dominated (middle), and joint (right) classification results of the whole image on (a) KSC and (b) Pavia datasets. Results are generated with learned SAE-LR models.</figDesc><graphic url="image-22.png" coords="12,43.65,279.55,503.04,294.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I LAND</head><label>I</label><figDesc>COVER CLASSES AND NUMBERS OF PIXELS IN KSC DATASET</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II LAND</head><label>II</label><figDesc>COVER CLASSES AND NUMBERS OF PIXELS IN PAVIA DATASET</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III</head><label>III</label><figDesc></figDesc><table><row><cell>TESTING TIME COMPARISON</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI SPATIAL</head><label>VI</label><figDesc>-DOMINATED AND JOINT CLASSIFICATION OF SAE-LR AND SVM MODEL</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Authorized licensed use limited to: Tsinghua University. Downloaded on January 01,2024 at 03:42:48 UTC from IEEE Xplore. Restrictions apply.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>The authors would like to thank <rs type="person">Prof. R. Memisevic</rs> and <rs type="person">Y. Bengio</rs> for their suggestion of using Theano, which accelerates the implementation of the models significantly. They would also like to thank the Editor who handled our paper and the three anonymous reviewers for providing truly outstanding comments and suggestions that significantly helped us improve the technical quality and presentation of our paper.</p></div>
			</div>
			<div type="funding">
<div><p>This work was supported in part by the <rs type="funder">Fundamental Research Funds for the Central Universities</rs> under Grant <rs type="grantNumber">HIT. NSRIF.2013028</rs> and in part by <rs type="funder">National Natural Science Foundation of China</rs> under Grant <rs type="grantNumber">61301206</rs> and Grant <rs type="grantNumber">61371180</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_75e32Z5">
					<idno type="grant-number">HIT. NSRIF.2013028</idno>
				</org>
				<org type="funding" xml:id="_GPKbejG">
					<idno type="grant-number">61301206</idno>
				</org>
				<org type="funding" xml:id="_WEnk2xC">
					<idno type="grant-number">61371180</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hyperspectral image data analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Landgrebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="28" />
			<date type="published" when="2002-01">Jan. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Remote Sensing Digital Image Analysis: An Introduction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Richards</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Use of hyperspectral imagery for mapping grape varieties in the Barossa Valley, South Australia</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Lacar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T</forename><surname>Grierson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Geosci. Remote Sens. Symp. (IGARSS)</title>
		<meeting>IEEE Int. Geosci. Remote Sens. Symp. (IGARSS)<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="2875" to="2877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Analysis of spectral absorption features in hyperspectral imagery</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">V D</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Appl. Earth Observ. Geoinf</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="68" />
			<date type="published" when="2004-01">Jan. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An introduction to hyperspectral imaging and its application for security, surveillance and target acquisition</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Imaging Sci. J</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="241" to="253" />
			<date type="published" when="2010-05">May 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Egerton</surname></persName>
		</author>
		<title level="m">Electron Energy-Loss Spectroscopy in the Electron Microscope</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Plenum</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hyperspectral imaging for astronomy and space surveillance</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Hege</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE&apos;s 48th Annu</title>
		<meeting>SPIE&apos;s 48th Annu<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="380" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hyperspectral imaging-An emerging process analytical tool for food quality and safety control</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Gowen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Food Sci. Technol</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="590" to="598" />
			<date type="published" when="2007-12">Dec. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Remote sensing of the coastal zone: An overview and priorities for future research</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Malthus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Mumby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="2805" to="2815" />
			<date type="published" when="2003-11">Nov. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hyperspectral remote sensing data analysis and future challenges</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bioucas-Dias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Geosci. Remote Sens. Mag</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="6" to="36" />
			<date type="published" when="2013-02">Feb. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An active learning approach to hyperspectral data classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Crawford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1231" to="1242" />
			<date type="published" when="2008-04">Apr. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Kernel-based methods for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Camps-Valls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1351" to="1362" />
			<date type="published" when="2005-06">Jun. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A relative evaluation of multiclass image classification by support vector machines</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Foody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mathur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1335" to="1343" />
			<date type="published" when="2004-06">Jun. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convex geometry based outlier-insensitive estimation of number of endmembers in hyperspectral images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ambikapathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Whispers</title>
		<meeting>IEEE Whispers<address><addrLine>Gainesville, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">Jun. 25-28, 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dimensionality reduction of hyperspectral data using discrete wavelet transform feature extraction</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Koger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2331" to="2338" />
			<date type="published" when="2002-10">Oct. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hyperspectral data analysis and supervised feature reduction via projection pursuit</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Landgrebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2653" to="2667" />
			<date type="published" when="1999-06">Jun. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hyperspectral image classification and dimensionality reduction: An orthogonal subspace projection approach</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Harsanyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="779" to="785" />
			<date type="published" when="1994-07">Jul. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A joint band prioritization and band-decorrelation approach to band selection for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L G</forename><surname>Althouse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2631" to="2641" />
			<date type="published" when="1999-06">Jun. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A new search algorithm for feature selection in hyperspectral remote sensing images</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Serpico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1360" to="1367" />
			<date type="published" when="2001-07">Jul. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Simultaneous feature selection and SVM parameter determination in classification of hyperspectral imagery using Ant Colony Optimization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Samadzadegan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Can. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="139" to="156" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Classification of hyperspectral remote sensing images with support vector machines</title>
		<author>
			<persName><forename type="first">F</forename><surname>Melgani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lorenzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1778" to="1790" />
			<date type="published" when="2004-08">Aug. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Support vector machines for classification of hyperspectral data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Gualtieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chettri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Geosci. Remote Sens. Symp. (IGARSS)</title>
		<meeting>IEEE Geosci. Remote Sens. Symp. (IGARSS)<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="813" to="815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A genetic algorithm based wrapper feature selection method for classification of hyperspectral images using support vector machine</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Geoinformat. Joint Conf. GIS Built Environ</title>
		<meeting>Geoinformat. Joint Conf. GIS Built Environ</meeting>
		<imprint>
			<date type="published" when="2008-11">Nov. 2008</date>
			<biblScope unit="page" from="71471J" to="71471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013-08">Aug. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep hierarchies in primate visual cortex what can we learn for computer vision?</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kruger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1847" to="1871" />
			<date type="published" when="2013-08">Aug. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Inf</title>
		<meeting>Neural Inf<address><addrLine>Lake Tahoe, Nevada, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006-07">Jul. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning in the deep structured conditional random fields</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Inf. Process. Syst. Workshop</title>
		<meeting>Neural Inf. ess. Syst. Workshop<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-12">Dec. 2009</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep belief networks using discriminative features for phone recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Acoust. Speech Signal Process. (ICASSP)</title>
		<meeting>Acoust. Speech Signal ess. (ICASSP)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="5060" to="5063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Incorporation of spatial constraints into spectral mixture analysis of remotely sensed hyperspectral data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int</title>
		<meeting>IEEE Int<address><addrLine>Grenoble, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Spectral-spatial classification of hyperspectral imagery based on partitional clustering techniques</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tarabalka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2973" to="2987" />
			<date type="published" when="2009-08">Aug. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Spectral and spatial classification of hyperspectral data using SVMs and morphological profiles</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fauvel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3804" to="3814" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spectral-spatial classification of hyperspectral data using loopy belief propagation and active learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bioucas-Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Plaza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="844" to="856" />
			<date type="published" when="2013-02">Feb. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Spatial-spectral kernel sparse representation for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2462" to="2471" />
			<date type="published" when="2013-06">Jun. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Theory of the backpropagation neural network</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hecht-Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. IEEE Neural Netw</title>
		<meeting>Int. Joint Conf. IEEE Neural Netw<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="593" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep, narrow sigmoid belief networks are universal approximators</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2629" to="2636" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep belief networks are compact universal approximators</title>
		<author>
			<persName><forename type="first">N</forename><surname>Leroux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2192" to="2207" />
			<date type="published" when="2010-08">Aug. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006-07">Jul. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep Boltzmann machines</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf</title>
		<meeting>Int. Conf<address><addrLine>Clearwater Beach, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="448" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Inf</title>
		<meeting>Neural Inf<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010-12">Dec. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A practical guide to training restricted Boltzmann machines</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>UTML TR2010-003</idno>
	</analytic>
	<monogr>
		<title level="j">Dept. Comput. Sci., Univ. Toronto</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<pubPlace>Toronto, ON, Canada</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989-04">Apr. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biol. Cybern</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="1980-04">Apr. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A reappraisal of the kappa coefficient</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Clin. Epidemiol</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="949" to="958" />
			<date type="published" when="1988-10">Oct. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Assessment of spectral, polarimetric, temporal, and spatial dimensions for urban and peri-urban land cover classification using Landsat and SAR data</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Woodcock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kellndorfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens. Environ</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="72" to="82" />
			<date type="published" when="2012-02">Feb. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning discriminative hierarchical features for object recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1159" to="1163" />
			<date type="published" when="2014-09">Sep. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
