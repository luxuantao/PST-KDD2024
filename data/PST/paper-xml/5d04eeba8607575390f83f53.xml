<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SoftSKU: Optimizing Server Architectures for Microservice Diversity @Scale</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Akshitha</forename><surname>Sriraman</surname></persName>
							<email>akshitha@umich.edu</email>
							<affiliation key="aff1">
								<orgName type="department">ISCA &apos;19</orgName>
								<address>
									<addrLine>June 22-26</addrLine>
									<postCode>2019</postCode>
									<settlement>Phoenix</settlement>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Abhishek</forename><surname>Dhanotia</surname></persName>
							<email>abhishekd@fb.com</email>
							<affiliation key="aff1">
								<orgName type="department">ISCA &apos;19</orgName>
								<address>
									<addrLine>June 22-26</addrLine>
									<postCode>2019</postCode>
									<settlement>Phoenix</settlement>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
							<email>twenisch@umich.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan ?</orgName>
								<address>
									<country>Facebook</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SoftSKU: Optimizing Server Architectures for Microservice Diversity @Scale</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3307650.3322227</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Microservice</term>
					<term>resource fungibility</term>
					<term>soft SKU</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The variety and complexity of microservices in warehousescale data centers has grown precipitously over the last few years to support a growing user base and an evolving product portfolio. Despite accelerating microservice diversity, there is a strong requirement to limit diversity in underlying server hardware to maintain hardware resource fungibility, preserve procurement economies of scale, and curb qualification/test overheads. As such, there is an urgent need for strategies that enable limited server CPU architectures (a.k.a "SKUs") to provide performance and energy efficiency over diverse microservices. To this end, we first undertake a comprehensive characterization of the top seven microservices that run on the compute-optimized data center fleet at Facebook.</p><p>Our characterization reveals profound diversity in OS and I/O interaction, cache misses, memory bandwidth utilization, instruction mix, and CPU stall behavior. Whereas customizing a CPU SKU for each microservice might be beneficial, it is prohibitive. Instead, we argue for "soft SKUs", wherein we exploit coarse-grain (e.g., boot time) configuration knobs to tune the platform for a particular microservice. We develop a tool, ?SKU, that automates search over a soft-SKU design space using A/B testing in production and demonstrate how it can obtain statistically significant gains (up to 7.2% and 4.5% performance improvement over stock and production servers, respectively) with no additional hardware requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS Computer systems organization ? Cloud computing</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The increasing user base and feature portfolio of web applications is driving precipitous growth in the diversity and complexity of the back-end services comprising them <ref type="bibr" target="#b0">[1]</ref>. There is a growing trend towards microservice implementation models <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>, wherein a complex application is decomposed into distributed microservices <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref> that each provide specialized functionality <ref type="bibr" target="#b10">[11]</ref>, such as HTTP connection termination, key-value serving <ref type="bibr" target="#b11">[12]</ref>, protocol routing <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, or ad serving <ref type="bibr" target="#b14">[15]</ref>. This deployment model enables application components' independent scalability by ramping the number of physical servers/cores dedicated to each in response to diurnal and long-term load trends <ref type="bibr" target="#b4">[5]</ref>.</p><p>At global user population scale, important microservices can grow to account for an enormous installed base of physical hardware. Across Facebook's global server fleet, seven key microservices in four service domains run on hundreds of thousands of servers and occupy a large portion of the compute-optimized installed base. These microservices' importance begs the question: do our existing server platforms serve them well? Are there common bottlenecks across microservices that we might address when selecting a future server CPU architecture?</p><p>To this end, we undertake comprehensive system-level and architectural characterizations of these microservices on Facebook production systems serving live traffic. We find that application functionality disaggregation across microservices has yielded enormous diversity in system and CPU architectural requirements, as shown in Fig. <ref type="figure">1</ref>. For example, caching microservices <ref type="bibr" target="#b15">[16]</ref> require intensive I/O and microsecondscale response latency and frequent OS context switches can comprise 18% of CPU time. In contrast, a Feed <ref type="bibr" target="#b16">[17]</ref> microservice computes for seconds per request with minimal OS interaction. Our Web <ref type="bibr" target="#b17">[18]</ref> microservice entails massive instruction footprints, leading to astonishing instruction cache and ITLB misses and branch mispredictions, while others execute much smaller instruction footprints. Some microservices depend heavily on floating-point performance while others have no floating-point instructions. The microarchitectural trends we discover differ markedly from those of SPEC CPU2006/2017 <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, academic cloud workloads <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>, and even some of Google's major services <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Such diversity might suggest a strategy to specialize CPU architectures to suit each microservice's distinct needs. Optimizing one or more of these microservices to achieve even single-digit percent speedups can yield immense performanceper-watt benefits. Indeed, we report observations that might inform future hardware designs. However, large-scale internet operators have strong economic incentives to limit hardware platforms' diversity to (1) maintain fungibility of hardware resources, <ref type="bibr" target="#b1">(2)</ref> preserve procurement advantages that arise from economies of scale, and (3) limit the overhead of qualifying/testing myriad hardware platforms. As such, there is an immediate need for strategies that enable a limited set of server CPU architectures (often called "SKUs," short for "Stock Keeping Units") to provide performance and energy efficiency over microservices with diverse characteristics.</p><p>Rather than diversify the hardware portfolio, we argue for "soft SKUs," a strategy wherein we exploit coarse-grain (e.g., boot time) OS and hardware configuration knobs to tune limited hardware SKUs to better support their presently assigned microservice. Unlike data centers that co-locate services via virtualization, Facebook's microservices run on dedicated bare metal servers, allowing us to easily create microservice-specific soft SKUs. As microservice allocation needs vary, servers can be redeployed to different soft SKUs through reconfiguration and/or reboot. Our OS and CPUs provide several specialization knobs; in this study, we focus on seven: (1) core frequency, (2) uncore frequency, (3) active core count, (4) code vs. data prioritization in the last-level cache ways, (5) hardware prefetcher configuration, (6) use of transparent huge pages, and (7) use of static huge pages.</p><p>Identifying the best microservice-specific soft-SKU configuration is challenging: the design space is large, service code evolves quickly, synthetic load tests do not necessarily capture production behavior, and the effects of tuning a particular knob are often small (a few percent performance change). To this end, we develop ?SKU-a design tool that automates search within the seven-knob soft-SKU design space using A/B testing in production systems on live traffic. ?SKU automatically varies soft-SKU configuration while collecting numerous fine-grain performance measurements to obtain sufficient statistical confidence to detect even small performance improvements. We evaluate a prototype of ?SKU and demonstrate that the soft SKUs it designs outperform stock and production server configurations by up to 7.2% and 4.5% respectively, with no additional hardware requirement.</p><p>In summary, we contribute:</p><p>? A comprehensive characterization of system-level bottlenecks experienced by key production microservices in one of the largest social media platforms today.</p><p>? A detailed study of microservices' architectural bottle-necks, highlighting potential design optimizations.</p><p>? ?SKU: A design tool that automatically tunes important configurable server parameters to create microservicespecific "soft" server SKUs on existing hardware.</p><p>? A detailed performance study of configurable server parameters tuned by ?SKU.</p><p>The rest of the paper is organized as follows: We describe and measure these seven production microservices' performance traits in Sec. 2. We argue the need for Soft SKUs in Sec. 3. We describe ?SKU's design in Sec. 4 and we discuss the methodology used to evaluate ?SKU in Sec. 5. We evaluate ?SKU in Sec. 6, discuss limitations in Sec. 7, compare against related work in Sec. 8, and conclude in Sec. 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Understanding Microservice Performance</head><p>We aim to identify software and hardware bottlenecks faced by Facebook's key production microservices to see if they share common bottlenecks that might be addressed in future server CPU architectures. In this section, we (1) describe each microservice, (2) explain our characterization methodology, (3) discuss system-level characteristics to provide insights into how each microservice is operated, (4) report on the architectural characteristics and bottlenecks faced by each microservice, and (5) summarize our characterization's most important conclusions. A key theme that emerges throughout our characterization is diversity; the seven microservices differ markedly in their performance constraints' time-scale, instruction mix, cache behavior, CPU utilization, bandwidth requirements, and pipeline bottlenecks. Unfortunately, this diversity calls for sometimes conflicting optimization choices, motivating our pursuit of "soft SKUs" (Section 3) rather than custom hardware for each microservice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Production Microservices</head><p>We characterize seven microservices in four diverse service domains running on Facebook's compute-optimized data center fleet. The workloads with longer work-per-request (e.g. Feed2, Ads1) might be called "services" by some readers; we use "microservice" since none of these systems is entirely stand-alone. We characterize on production systems serving live traffic. We first detail each microservice's functionality.</p><p>Web. Web implements the HipHop Virtual Machine, a Just-In-Time (JIT) compilation and runtime system for PHP and Hack <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>, to serve web requests originating from end-users. Web employs request-level parallelism: an incoming request is assigned to one of a fixed pool of PHP worker threads, which services the request until completion. If all workers are busy, arriving requests are enqueued. Web makes frequent requests to other microservices, and the corresponding worker thread blocks waiting on the responses.</p><p>Feed1 and Feed2. Feed1 and Feed2 are key microservices in our News Feed service. Feed2 aggregates various leaf microservices' responses into discrete "stories." These stories are then characterized into dense feature vectors by feature extractors and learned models <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref>. The feature vectors are then sent to Feed1, which calculates and returns a predicted user relevance vector. Stories are then ranked and selected for display based on the relevance vectors. Ads1 and Ads2. Ads1 and Ads2 maintain user-specific and ad-specific data, respectively <ref type="bibr" target="#b14">[15]</ref>. When Ads1 receives an ad request, it extracts user data from the request and sends targeting information to Ads2. Ads2 maintains a sorted ad list, which it traverses to return ads meeting the targeting criteria to Ads1. Ads1 then ranks the returned ads.</p><p>Cache1 and Cache2. Cache is a large distributed-memory object caching service (like, e.g., <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>) that reduces throughput requirements of various backing stores. Cache1 and Cache2 correspond to two tiers within each geographic region for this service. Client microservices contact the Cache2 tier. If a request misses in Cache2, it is forwarded to the Cache1 tier. Cache1 misses are then sent to an underlying database cluster in that region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Characterization Approach</head><p>We characterize the seven microservices by profiling each in production while serving real-world user queries. We next describe the characterization methodology.</p><p>Hardware platforms. We perform our characterization on 18-and 20-core Intel Skylake processor platforms <ref type="bibr" target="#b30">[31]</ref>, Skylake18 and Skylake20. Characteristics of each are summarized in Table <ref type="table" target="#tab_0">1</ref>. Web, Feed1, Feed2, Ads1, and Cache2 run on Skylake18. Ads2 and Cache1 are deployed on Sky-lake20. Both platforms support Intel Resource Director Technology (RDT) <ref type="bibr" target="#b31">[32]</ref>. RDT facilitates tunable Last-Level Cache (LLC) size configurations using Cache Allocation Technology (CAT) <ref type="bibr" target="#b32">[33]</ref>, and allows prioritizing code vs. data in the LLC ways using Code Data Prioritization (CDP) <ref type="bibr" target="#b33">[34]</ref>.</p><p>Experimental setup. We measure each microservice in Facebook's production environment's default deploymentstand-alone with no co-runners on bare metal hardware. Therefore, there are no cross-service contention or interference effects in our data. We measure each system at peak load to stress performance bottlenecks and characterize the system's maximum throughput capabilities. Facebook's production microservice codebases evolve rapidly; we repeat experiments across updates to ensure that results are stable. We collect most system-level performance data using an internal tool called Operational Data Store (ODS) <ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref>. ODS enables retrieval, processing, and visualization of sampling data collected from all machines in the data center. ODS provides functionality similar to Google-Wide-Profiling <ref type="bibr" target="#b37">[38]</ref>.</p><p>To analyze microservices' interactions with the underlying hardware, we use myriad processor performance counters. We collect data with Intel's EMON <ref type="bibr" target="#b38">[39]</ref>-a performance monitoring and profiling tool that time multiplexes sampling of a vast number of processor-specific hardware performance counters with minimal error. For each experiment, we use this tool to collect tens of thousands of hardware performance events. We report 95% confidence intervals on mean results. We contrast our measurements with some CloudSuite <ref type="bibr" target="#b20">[21]</ref>, SPEC CPU2006 <ref type="bibr" target="#b18">[19]</ref>, SPEC CPU2017 <ref type="bibr" target="#b19">[20]</ref>, and Google services <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref> where possible. We measured SPEC CPU2006 performance on Skylake20. We reproduce selected data from published reports on SPEC CPU2017 <ref type="bibr" target="#b19">[20]</ref>, Cloud-Suite <ref type="bibr" target="#b20">[21]</ref>, and Google's services <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref> measured on Haswell, Westmere, and Haswell, respectively. These results are not directly comparable with our measurements as they are measured on different hardware. Nevertheless, they provide context for the greater bottleneck diversity we observe in our microservices relative to commonly studied benchmark suites.</p><p>We present our characterization in two parts. We first discuss system-level characteristics observed over the entire fleet. We then present performance-counter measurements and their implications on architectural bottlenecks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">System-Level Characterization</head><p>We first present key system-level metrics, such as request latency, achieved throughput, and path length (instructions per query), to provide insight into how the microservices behave and how these traits may impact architectural bottlenecks. Throughout, we call attention to key axes of diversity.</p><p>2.3.1 Request throughput, request latency, and path length. We report approximate peak-load throughput, average request latency, and path length (instructions per query) in Table <ref type="table" target="#tab_1">2</ref>. The amount of work per query varies by six orders of magnitude across the microservices, resulting in throughputs ranging from tens of Queries Per Second (QPS) to 100,000s of QPS with average request latencies ranging from tens of microseconds to single-digit seconds.</p><p>Microservices' differing time scales imply that per-query overheads that may pose major bottlenecks for some microservices are negligible for others. For example, microsecondscale overheads that arise from accesses to Flash <ref type="bibr" target="#b39">[40]</ref>, emerging memory technologies like 3D XPoint by Intel and Micron <ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref>, or 40-100 Gb/s Infiniband and Ethernet network interactions <ref type="bibr" target="#b43">[44]</ref> can significantly degrade the request latency of microsecond-scale microservices <ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref> like Cache1 or Cache2. However, such microsecond-scale overheads have negligible impact on the request latency of secondsscale microservices like Feed2. The request latency diversity motivates our choice to include several microservices in our detailed performance-counter investigation.</p><p>2.3.2 Request latency breakdown. We next characterize request latency in greater detail to determine the relative contribution of computation and queuing/stalls on an average request's end-to-end latency. We report the average fraction of time a request is "running" (executing instruc-  tions) vs. "blocked" (stalled, e.g., on I/O) in Fig. <ref type="figure" target="#fig_0">2 (a)</ref>. We omit Cache1 and Cache2 from this measurement since their queries follow concurrent execution paths and time cannot easily be apportioned as "running" or "blocked".</p><p>Feed1 and Ads2 are almost entirely compute-bound throughout a request's life as they are leaves and do not block on requests to other microservices in the common case. They will benefit directly from architectural features that enhance instruction throughput. In contrast, Web, Feed2, and Ads1 emit requests to other microservices and hence their queries spend considerable time blocked. These can benefit from architectural/OS features that support greater concurrency <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b48">49]</ref>, fast thread switching, and better I/O performance <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref>.</p><p>We further break down Web's "blocked" component in Fig. <ref type="figure" target="#fig_0">2</ref> (b) into queuing latency (while a query awaits a worker thread's availability), scheduler latency (where a worker is ready but not running), and I/O latency (where a query is blocked on a request to another microservice). Although Web's scheduler delays are surprisingly high, these delays are not due to inefficient system design, and are instead triggered by thread over-subscription. To improve Web's throughput, load balancing schemes continue spawning worker threads until adding another worker begins degrading throughput.</p><p>2.3.3 CPU utilization at peak load. The microservices also vary in their CPU utilization profile. Fig. <ref type="figure" target="#fig_1">3</ref> shows the CPU utilization and its user-and kernel-mode breakdown when each microservice is operated at the maximum load it can sustain without violating Quality of Service (QoS) constraints. We make two observations: (1) CPU resources are not always fully utilized. (2) Most microservices exhibit a relatively small fraction of kernel/IO wait utilization. Each microservice faces latency, quality, and reliability constraints, which impose QoS requirements that in turn impose constraints on how high CPU utilization may rise before a constraint is violated. Our load balancers modulate load to ensure constraints are met. More specifically, Cache1, Cache2, Feed1, Feed2, Ads1, and Ads2 under-utilize the CPU due to strict latency constraints enforced to maintain user experience. These services might benefit from tail latency optimizations,  which might allow them to operate at higher CPU utilization. Cache1 and Cache2 exhibit higher kernel-mode utilization due to frequent context switches, which we inspect next.</p><p>2.3.4 Context switch penalty. We report the fraction of a CPU-second each microservice spends context switching in Fig. <ref type="figure" target="#fig_2">4</ref>. We estimate context switch penalty by first aggregating non-voluntary and voluntary context switch counts reported by Linux's time utility. We then estimate upper and lower context switch penalty bounds using switching latencies reported by prior works <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref>.</p><p>Cache1 and Cache2 incur context switches far more frequently than other microservices, and may spend as much as 18% of CPU time in switching. These frequent context switches also lead to worse cache locality, as we will show in our architectural characterization. Software/hardware optimizations <ref type="bibr" target="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref><ref type="bibr" target="#b55">[56]</ref><ref type="bibr" target="#b56">[57]</ref><ref type="bibr" target="#b57">[58]</ref><ref type="bibr" target="#b58">[59]</ref><ref type="bibr" target="#b59">[60]</ref><ref type="bibr" target="#b60">[61]</ref><ref type="bibr" target="#b61">[62]</ref> that reduce context switch latency or counts might considerably improve Cache performance.</p><p>2.3.5 Instruction mix. We report our microservices' instruction mix and contrast with SPEC CPU2006 benchmarks in Fig. <ref type="figure" target="#fig_3">5</ref>. Instruction mix varies substantially across our microservices, especially with respect to store-intensity and the presence/absence of floating-point operations. The microservices that include ranking models that operate on real-valued feature vectors, Ads1, Ads2, Feed1, and Feed2, all include floating-point operations, and Feed1 is dominated by them. These microservices can likely benefit from optimizations for dense computation, such as SIMD instructions.</p><p>Prior work has reported that key-value stores, like Cache1 and Cache2, are typically memory intensive <ref type="bibr" target="#b15">[16]</ref>. However,  we note that Cache requires substantial arithmetic and control flow instructions for parsing requests and marshalling or unmarshalling data; their load-store intensity does not differ from other services as much as the literature might suggest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Architectural Characterization</head><p>We next turn to performance-counter-based analysis of the architectural bottlenecks of our microservice suite, and examine opportunities it reveals for future hardware SKU design.</p><p>2.4.1 IPC and stall causes. We report each microservice's overall Instructions Per Cycle (IPC) in Fig. <ref type="figure" target="#fig_4">6</ref>. We contrast our results with IPCs for commonly studied benchmark suites <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> and published results for comparable Google services <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref>. Prior works' IPCs are measured on other platforms as shown in Fig. <ref type="figure" target="#fig_4">6</ref>; although absolute IPCs may not be directly comparable, it is nevertheless useful to compare variability and spreads.</p><p>None of our microservices use more than half of the theoretical execution bandwidth of a Skylake CPU (theoretical peak IPC of 5.0), and Cache1 uses only 20%. As such, simul-taneous multithreading is effective for these services and is enabled in our platforms. Relative to alternative benchmarks, our microservices exhibit (1) a greater IPC diversity than Google's services <ref type="bibr" target="#b0">[1]</ref> and (2) a lower IPC than most widelystudied SPEC CPU2006 benchmarks. Given our production workloads' larger codebase, larger working set, and more varied memory access patterns, we do not find our lower typical IPC surprising. When accounting for Skylake's enhanced performance over Haswell, we find the range of IPC values we report to be comparable to the Google services <ref type="bibr" target="#b22">[23]</ref>.</p><p>We provide insight into the root causes of relatively low IPC using the Top-down Microarchitecture Analysis Method (TMAM) <ref type="bibr" target="#b62">[63]</ref> to categorize processor pipelines' execution stalls, as reported in Fig. <ref type="figure" target="#fig_5">7</ref>. TMAM exposes architectural bottlenecks despite the many latency-masking optimizations of modern out-of-order processors. The methodology reports bottlenecks in terms of "instruction slots"-the fraction of the peak retirement bandwidth that is lost due to stalls each cycle. Slots are categorized as: front-end stalls due to instruction fetch misses, back-end stalls due to pipeline dependencies and load misses, bad speculation due to recovery from branch mispredictions, and retiring of useful work.</p><p>As suggested by the IPC results, our microservices retire instructions in only 22%-40% of possible retirement slots. However, the nature of the stalls in our applications varies substantially across microservices and differs markedly from the other suites. We make several observations. First, our microservices tend to have greater front-end stalls than SPEC workloads. In particular, Web, Cache1, and Cache2 lose ?37% of retirement slots due to front-end stalls; only Google's Gmail-FE and search exhibit comparable front-end stalls. In Web, front-end stalls arise due to its enormous code footprint due to a rich feature set and the many URL endpoints it implements. In Cache, frequent context switches and OS activity cause high front-end stalls. As we will show, these microservices could benefit from larger Icache and ITLB and other techniques that address instruction misses <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b64">65]</ref>. In contrast, microservices like Ads1, Ads2, or Feed1 do not stand to gain much from greater instruction capacity, leading to conflicting SKU optimization goals.</p><p>Second, mispredicted branches make up 3% -13% of wasted slots. Branch mispredictions are more rare in datacrunching microservices like Feed1 and more common when instruction footprint is large, as in Web, where aliasing in the Branch Target Buffer contributes a large fraction of branch misspeculations. SKU optimization goals diverge, with some microservices calling for simple branch predictors while others call for higher capacity and more sophisticated prediction.</p><p>Third, back-end stalls, largely due to data cache misses, occupy up to 48% of slots, implying that several microservices can benefit from memory hierarchy enhancements. However, microservices like Web or Feed2, which have fewer back-end stalls, likely gain more from chip area/power dedicated to additional computation resources rather than cache.</p><p>2.4.2 Cache misses. We provide greater nuance to our front-end and back-end stall breakdown by measuring instruction and data misses in the cache hierarchy. We present code and data Misses Per Kilo Instruction (MPKI) across all cache levels-L1, L2, and LLC in Figs. <ref type="figure" target="#fig_6">8</ref> and<ref type="figure" target="#fig_7">9</ref>, to analyze the overall effectiveness of each cache level. We also show  cache MPKI reported by prior work <ref type="bibr" target="#b22">[23]</ref> for Google search and our measurements of SPEC CPU2006 on Skylake20.</p><p>We make the following observations: (1) Our L1 MPKI are drastically higher than the comparison applications, especially for code, and particularly for Cache1 and Cache2.</p><p>(2) LLC data misses are commonly high in all microservices, especially in Feed1, which traverses large data structures. (3) Web incurs 1.7 LLC instruction MPKI. These misses are quite computationally expensive, since out-of-order mechanisms do not hide instruction stalls. It is unusual for applications to incur non-negligible LLC instruction misses at all in steady state; few such applications are reported in the academic literature.</p><p>Prior works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b65">66]</ref> typically find current LLC sizes to be sufficient to encompass server applications' entire code footprint. In Web, the large code footprint and high instruction miss rates arise due to the large code cache, frequent JIT code generation, and a large and complex control flow graph. Cache1 and Cache2 incur frequent context switches (see Fig. <ref type="figure" target="#fig_2">4</ref>) among distinct thread pools executing different code, which leads to code thrashing in L1 and, to a lesser degree, L2. We conclude many microservices can benefit from larger I-caches, instruction prefetching, or prioritizing code over data in the LLC using techniques like Intel's CDP <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b66">67]</ref>.</p><p>2.4.3 LLC capacity sensitivity. Using CAT <ref type="bibr" target="#b33">[34]</ref>, we inspect sensitivity to LLC capacity. We vary capacity by enabling LLC ways two at a time, up to the maximum of 11 ways. We report LLC MPKI broken down by code and data in Fig. <ref type="figure" target="#fig_8">10</ref>. We omit Cache as it fails to meet QoS constraints with reduced LLC capacity. For most microservices, a knee (8 ways) emerges where the LLC is large enough to capture a primary working set without degrading IPC, and further  capacity increases provide diminishing returns. For some microservices (e.g., Ads2 and Feed1), the largest working set is too large to be captured. Hence, some services might benefit from trading LLC capacity for additional cores <ref type="bibr" target="#b67">[68]</ref>.</p><p>2.4.4 TLB misses. We report instruction and data TLB MPKI in Fig. <ref type="figure" target="#fig_9">11</ref>. For the DTLB, we break down misses due to loads and stores. The ITLB miss trends mirror our LLC code miss observations: Web, Cache1, and Cache2 incur substantial ITLB misses, while the miss rates are negligible for the remaining microservices. The drastically higher miss rate in Web illustrates the impact of its large JIT code cache.</p><p>DTLB miss rates are more variable across microservices. They typically follow the LLC MPKI trends shown in Fig. <ref type="figure" target="#fig_7">9</ref> with the exception of Feed1-despite a relatively high LLC MPKI of 9.3 it incurs a relatively low DTLB MPKI of 5.8. Feed1's main data structures are dense floating-point feature vectors and model weights, leading to good page locality despite a high LLC MPKI. However, the other microservices might benefit from software (like static or transparent huge pages) and hardware (e.g., <ref type="bibr" target="#b68">[69]</ref><ref type="bibr" target="#b69">[70]</ref><ref type="bibr" target="#b70">[71]</ref><ref type="bibr" target="#b71">[72]</ref><ref type="bibr" target="#b72">[73]</ref><ref type="bibr" target="#b73">[74]</ref><ref type="bibr" target="#b74">[75]</ref>) paging optimizations.</p><p>2.4.5 Memory bandwidth utilization. We inspect memory bandwidth utilization and its attendant effects on latency due to memory system queuing for each microservice in Fig. <ref type="figure" target="#fig_10">12</ref>. We first characterize the inherent bandwidth vs. latency trade-off of our two platforms-Skylake18 in the blue dots and Skylake20 in the yellow crosses-using a memory stress test <ref type="bibr" target="#b75">[76]</ref>. These curves show the characteristic horizontal asymptote at the unloaded memory latency and then exponential latency growth as memory system load approaches saturation. We then plot each microservice's measured average latency and bandwidth, using dots and crosses, respectively, to indicate the service platform.</p><p>Microservices like Web or Feed1 have high memory bandwidth utilization relative to the platform capability. Nevertheless, our microservices cannot push memory bandwidth utilization above a certain threshold-operating at higher bandwidth causes exponential memory latency increase, triggering service latency violations. Ads1 and Ads2 operate at higher latency than the characteristic curve predicts due to memory traffic burstiness. The curves also reveal why it is necessary to run Cache1 and Ads2 on the higher-peak-bandwidth Sky-lake20 platform to keep memory latency low. Nevertheless, several microservices under-utilize available bandwidth, and hence might benefit from optimizations that trade bandwidth to improve latency, such as hardware prefetching <ref type="bibr" target="#b76">[77]</ref>.</p><p>We summarize our findings in Table <ref type="table" target="#tab_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">"Soft" SKUs</head><p>Our microservices exhibit profound diversity in system-level and architectural traits. For example, we demonstrated diverse OS and I/O interaction, code/data cache miss ratios, memory bandwidth utilization, instruction mix ratios, and CPU stall behavior. One way to address such distinct bottlenecks is to specialize CPU architectures by building custom hardware server SKUs to suit each service's needs. However, such hardware SKU diversity is impractical, as it requires testing and qualifying each distinct SKU and careful capacity planning to provision each to match projected load. Given the uncertainties inherent in projecting customer demand, investing in diverse hardware SKUs is not effective at scale. Data center operators aim to maintain hardware resource fungibility to preserve procurement advantages that arise from economies of scale and limit the effort of qualifying myriad hardware platforms. To preserve fungibility, we seek strategies that enable a few server SKUs to provide performance and energy efficiency over diverse microservices. To this end, we propose exploiting coarse-grain (e.g., boot time) parameters to create "soft SKUs", tuning limited hardware SKUs to better support their assigned microservice. However, manually identifying microservice-specific soft-SKUs is impractical since the design space is large, code evolves quickly, synthetic load tests do not necessarily capture production behavior, and the effects of tuning a single knob are often small (a few percent performance change). Hence, we build an automated design tool-?SKU-that searches the configuration design space to optimize for each microservice.</p><p>4 ?SKU: System Design ?SKU is a design tool for quick discovery of performant and efficient "soft" SKUs. ?SKU automatically varies configurable server parameters, or "knobs," by searching within a predefined design space via A/B testing. A/B testing is the process of comparing two identical systems that differ only in a single variable. ?SKU conducts A/B tests by comparing the performance of two identical servers (i.e., same hardware platform, same fleet, and facing the same load) that differ only in their knob configuration. ?SKU collects copious fine-grain performance measurements while conducting automated A/B tests on production systems serving live traffic to search for statistically significant performance changes. We aim to ensure that ?SKU has a simple design so that it can be applied across microservices and hardware SKU generations while avoiding operational complexity. Key design challenges include: (1) identifying performance-efficient soft-SKU configurations in a large design space, (2) dealing with frequent code evolution, (3) capturing behavior in production systems facing diurnal or transient load fluctuations, and (4) differentiating actual performance variations from noise through appropriate statistical tests. We discuss how ?SKU's design meets these challenges.</p><p>We develop a ?SKU prototype that explores a soft-SKU design space comprising seven configurable server knobs. ?SKU accepts a few input parameters and then invokes its components-A/B test configurator, A/B tester, and soft SKU generator, as shown in Fig. <ref type="figure" target="#fig_11">13</ref>. We describe each component below.</p><p>Input file. The user provides an input file with the following three input parameters.</p><p>(1) Target Microservice. Several aspects of ?SKU's behavior must be tuned for the specific target microservice. ?SKU reboots the server while performing certain A/B tests (e.g., core count scaling). Some microservices may not tolerate reboots on live traffic and hence ?SKU disables these knobs in such cases. Furthermore, ?SKU disables knobs that do not apply to a microservice. For example, Statically-allocated Huge Pages (SHPs) are inapplicable to Ads1, since it does not use the APIs to allocate them. Our current ?SKU prototype estimates performance by measuring the Millions of Instructions per Second (MIPS) rate via EMON <ref type="bibr" target="#b38">[39]</ref>, which we have confirmed is proportional to several key microservices' throughput (e.g., Web and Ads1). However, we anticipate the performance metric that ?SKU measures to determine whether a particular soft SKU has improved performance to be microservice specific. In particular, MIPS may be insufficient to measure Cache's throughput, since Cache's code is introspective of performance. (It executes exception handlers when faced with knob configurations that engender QoS violations, which make instructions-per-query vary with performance.) ?SKU can be extended to perform A/B tests using microservice-specific performance metrics.</p><p>(2) Processor platform. The available settings in several ?SKU design space dimensions, such as specific core and uncore frequencies, core counts, and hardware prefetcher options, are hardware platform specific. "Wider" hardware branch predictors, sophisticated prediction algorithms Low data LLC capacity utilization ( ?2.4.1-3, ?2.4.5)</p><p>Trade-off LLC capacity for additional cores Low memory bandwidth util. ( ?2. <ref type="bibr">4.5)</ref> Optimizations that trade bandwidth for latency (e.g., prefetching)</p><p>(3) Sweep configuration. ?SKU's A/B tester measures the performance implications of sweeping server knobs either (1) independently, where individual knobs are scaled one-by-one and their effects are presumed to be additive when creating a soft SKU, or (2) exhaustively, where the design space sweep explores the cross product of knob settings. Note that some microservices receive code updates so frequently (O(hours)) that an exhaustive ?SKU sweep cannot be completed between code pushes. In practice, the gains from ?SKU's knobs are not strictly additive. Nevertheless, the knobs do not typically co-vary strongly, so we have had success in tuning knobs independently, as the exhaustive approach requires an impractically large number of A/B tests.</p><p>A/B test configurator. The A/B test configurator sets up the automatic A/B test environment by specifying the sweep configuration and knobs to be studied.</p><p>A/B tester. The A/B tester is responsible for independently or exhaustively varying configurable hardware and OS knobs to measure ensuing performance changes. Our ?SKU prototype varies seven knobs (suggested by our earlier characterization), but can be extended easily to support more. It varies (1) core frequency, (2) uncore frequency, (3) core count, (4) CDP in the LLC ways, (5) prefetchers, <ref type="bibr" target="#b5">(6)</ref> Transparent Huge Pages (THP), and (7) SHPs.</p><p>The A/B tester sweeps the design space specified by the A/B test configurator. For each point in the space, the tester suitably sets knobs and then launches a hardware performance counter-based profiling tool <ref type="bibr" target="#b38">[39]</ref> to collect performance observations. For each knob configuration, the A/B tester first discards observations during a warm-up phase that typically lasts for a few minutes to avoid cold start bias <ref type="bibr" target="#b77">[78]</ref>. Next, the A/B tester records performance counter samples via EMON <ref type="bibr" target="#b38">[39]</ref> with sufficient spacing to ensure independence. Finally, when the desired 95% statistical confidence is achieved, the A/B tester outputs mean estimates, which it records in a design space map. It then proceeds to the next knob configuration. The A/B tester typically achieves 95% confidence estimates with tens of thousands of performance counter samples (minutes to hours of measurement). If 95% confidence is not reached after collecting ? 30, 000 observations, ?SKU concludes there is no statistically significant performance difference and proceeds to the next knob configuration. The final design space map helps identify (with a 95% confidence) the most performant knob configurations.</p><p>Soft SKU generator. The A/B tester's design space map is fed to the soft SKU generator, which selects the most performant knob configurations. It then applies this configuration to live servers running the microservice. Once the selected soft SKU is deployed, ?SKU performs further A/B tests by comparing the QPS achieved (via ODS) by soft-SKU servers against hand-tuned production servers for prolonged durations (including across code updates and under diurnal load) to validate that the soft SKU offers a stable advantage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Methodology</head><p>We discuss the methodology we use to evaluate ?SKU.</p><p>Microservices. We focus our prototype ?SKU evaluation on the Web service on two generations of hardware platforms and on the Ads1 microservice on a single platform. These two microservices differ drastically in our characterization results while both being amenable to the use of MIPS rate as a performance metric. Moreover, the surrounding infrastructure for these services is sufficiently robust to tolerate failures and disruptions we might cause with the ?SKU prototype, allowing us to experiment on production traffic.</p><p>Hardware platforms. To evaluate ?SKU, we run Web on two hardware platforms-Broadwell16 and Skylake18, and Ads1 on Skylake18 (see Table <ref type="table" target="#tab_0">1</ref>). We evaluate Web on both Skylake18 and Broadwell16 to analyze the configurable server knobs' sensitivity to the underlying hardware platform. Henceforth, we refer to Web running on Skylake18 as Web (Skylake) and Broadwell16 as Web (Broadwell).</p><p>Experimental setup. We compare ?SKU's A/B test knob scaling studies against default production server knob configurations. Some default knob configurations arise from arduous manual tuning, and therefore differ from stock server configurations. We next describe how ?SKU implements A/B test scaling studies for each configurable knob.</p><p>(1) Core frequency. Our servers enable Intel's Turbo Boost technology <ref type="bibr" target="#b78">[79]</ref>. ?SKU scales core frequency from 1.6 GHz to 2.2 GHz (default) by overriding core frequency-controlling Model-Specific Registers (MSRs).</p><p>(2) Uncore frequency. ?SKU varies uncore (LLC, memory controller, etc.) frequency from 1.4 GHz to 1.8 GHz (default) by overriding uncore frequency-controlling MSRs <ref type="bibr" target="#b79">[80]</ref>.</p><p>(3) Core count. ?SKU scales core count from 2 physical cores to the platform-specific maximum (default), by directing the boot loader to incorporate the isolcpus flag <ref type="bibr" target="#b80">[81]</ref> specifying cores on which the OS may not schedule. ?SKU then reboots the server to operate with the new core count.</p><p>(4) LLC Code Data Prioritization. ?SKU uses Intel RDT <ref type="bibr" target="#b33">[34]</ref> to prioritize code vs. data in the LLC ways. Our servers' OS kernels have extensions that support Intel RDT via the Resctrl interface <ref type="bibr" target="#b81">[82]</ref>. ?SKU leverages these kernel extensions to vary CDP from one dedicated LLC way for data and the rest for code, to one dedicated way for code and the rest for data. Default production servers share LLC ways between code and data without CDP prioritization.</p><p>(5) Prefetcher. Our servers support four prefetchers <ref type="bibr" target="#b82">[83]</ref>: (a) L2 hardware prefetcher that fetches lines into the L2 cache, (b) L2 adjacent cache line prefetcher that fetches a cache line in the same 128-byte-aligned region as a requested line, (c) DCU prefetcher that fetches the next cache line into L1-D cache, and (d) DCU IP prefetcher that uses sequential load history to determine whether to prefetch additional lines. ?SKU considers five configurations: (a) all prefetchers off, (b) all prefetchers on (default on Web (Skylake) and Ads1), (c) only DCU prefetcher and DCU IP prefetcher on, (d) only DCU prefetcher on, and (e) only L2 hardware prefetcher and DCU prefetcher on (default on Web (Broadwell)). ?SKU adjusts prefetcher settings via MSRs.</p><p>(6) Transparent Huge Pages (THP): THP is a Linux kernel mechanism that automatically backs virtual memory allocations with huge pages (2MB or 1GB) when contiguous physical memory is available and defragments memory in the background to coalesce free space <ref type="bibr" target="#b83">[84]</ref>. ?SKU considers three THP configurations (a) madvise-THP is enabled only for memory regions that explicitly request huge pages (default), (b) always ON-THP is enabled for all pages, and (c) always OFF-THP is not used even if requested. ?SKU configures THP by writing to kernel configuration files.</p><p>(7) Statically-allocated Huge Pages (SHP): SHPs are huge pages (2MB or 1GB) reserved explicitly by the kernel at boot time and must be explicitly requested by an application. Once reserved, SHP memory can not be repurposed. ?SKU varies SHP counts from 0 to 600 in 100-step increments by modifying kernel parameters <ref type="bibr" target="#b84">[85]</ref>. ?SKU can be extended to conduct a binary search to identify optimal SHP counts.</p><p>Performance metric. ?SKU estimates performance in terms of throughput by measuring MIPS rate via EMON <ref type="bibr" target="#b38">[39]</ref>. We have verified that MIPS is proportional to Web and Ads1's throughput (QPS). We do not measure QPS directly as QPS reported by ODS is not sufficiently fine-grained. We aim to eventually have ?SKU replace tedious manual knob tuning for each microservice. Hence, we evaluate ?SKU-generated soft SKUs against (a) stock off-the-shelf and (b) hand-tuned production server configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>We first present ?SKU's A/B test results for all seven configurable server knobs. We then compare the throughput of "soft" server SKUs that ?SKU discovers against (a) handtuned production and (b) stock server configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Knob Characterization</head><p>We present ?SKU's A/B test results for each knob and compare it against the current production configuration, indicated by thick red bar/point outlines or red axis lines in our graphs. For each graph, we report mean throughput and 95% confidence intervals under peak-load production traffic. For the first three knobs, we find that ?SKU matches expert manual tuning decisions. However, for the next four knobs, ?SKU identifies configurations that outperform production settings.</p><p>(1) Core frequency. We illustrate ?SKU's core frequency scaling analysis in Fig. <ref type="figure" target="#fig_12">14 (a)</ref>. ?SKU varies core frequency  from 1.6 GHz to 2.2 GHz. We report relative throughput (MIPS) gains over cores operating at 1.6 GHz. Our production systems have a fixed CPU power budget that is shared between the core and uncore (e.g., LLC, memory and QPI controller, etc.) CPU components. The current production configuration enables Turbo Boost <ref type="bibr" target="#b78">[79]</ref> and runs Web (Skylake and Broadwell) at 2.2 GHz and Ads1 at 2.0 GHz (as indicated by the thick red bar outlines in Fig. <ref type="figure" target="#fig_12">14 (a)</ref>). Ads1 must operate at slightly lower frequency because its use of AVX operations consumes part of the CPU power budget.</p><p>?SKU aims to (1) identify whether there is a minimum core frequency knee below which throughput degrades rapidly and (2) diagnose if core frequency trends suggest that the microservice may be uncore bound. Web's and Ads1's throughputs increase precipitously from 1.6 GHz to 1.9 GHz, beyond which ?SKU reports continued but diminishing throughput gains. These microservices are all sensitive to core frequency, hence, operating at the maximum and enabling Turbo Boost are sensible tuning decisions. ?SKU configures soft SKUs that operate at 2.2 GHz core frequency for Web (Skylake and Broadwell) and 2.0 GHz for Ads1, matching experts' tuning.</p><p>(2) Uncore frequency. ?SKU varies the frequency of uncore CPU power domain (including LLC, QPI controller, and memory controller), from 1.4 GHz to 1.8 GHz. We report results normalized to 1.4 GHz uncore frequency (Fig. <ref type="figure" target="#fig_12">14 (b)</ref>). Our default production configuration runs both microservices at 1.8 GHz uncore frequency. Uncore frequency indicates the degree to which applications are sensitive to access latency when memory and core execution bandwidth are held constant. Both of these microservices are sensitive to memory latency, though the sensitivity is greater in Ads1. As with core frequency, ?SKU selects soft SKUs that operate at the maximum 1.8 GHz for both microservices, again matching the default production configuration.</p><p>(3) Core count. We present ?SKU's core count scaling results in Fig. <ref type="figure" target="#fig_13">15</ref>, where we report throughput gain relative to execution on only two physical cores. The grey line indicates ideal linear scaling. ?SKU scales Web (Skylake) to its maximum core count (18 cores) and Web (Broadwell) to its maximum <ref type="bibr" target="#b15">(16)</ref>. We exclude Ads1 from Fig. <ref type="figure" target="#fig_13">15</ref> since its load balancing design precludes ?SKU from meeting QoS  constraints with fewer cores. ?SKU observes that Web's performance scales almost linearly up to ?8 physical cores. As core count increases further, interference in the LLC causes the scaling curve to bend down. As with frequency, the best soft SKU selected by ?SKU operates with all available cores.</p><p>(4) Code Data Prioritization (CDP) in LLC ways. In our earlier characterization (Fig. <ref type="figure" target="#fig_7">9</ref>), we noted that Web exhibits a surprising number of off-chip code misses. Hence, ?SKU considers prioritizing code vs. data in the LLC ways. We report throughput gains over the production baseline (where CDP is not used and code and data share LLC ways) for Web (Skylake) and Ads1 in Fig. <ref type="figure" target="#fig_14">16</ref>(a) and Web (Broadwell) in Fig. <ref type="figure" target="#fig_14">16(b</ref>). Skylake18 and Broadwell16 have 11 and 12 LLC ways, respectively. We label each bar with {LLC ways dedicated to data, LLC ways dedicated to code}.</p><p>Here we find that Web (Skylake) achieves up to 4.5% mean throughput gain with 6 LLC ways dedicated to data and 5 LLC ways dedicated to code, a configuration that degrades LLC data misses by 0.60 MPKI but improves code misses by 0.30 MPKI. Although this configuration increases net LLC misses by almost 0.30 MPKI, it still results in a performance win because the latency of code misses is not hidden and they incur a greater penalty. Similarly, Ads1 achieves 2.5% mean throughput improvement with 9 LLC ways dedicated to data and 2 LLC ways dedicated to code, sacrificing 0.20 LLC data MPKI to improve LLC code MPKI by 0.06. ?SKU observes no throughput improvement in Web (Broadwell) since it saturates memory bandwidth under all CDP configurations. Hence, ?SKU can not trade-off increasing the net LLC MPKI to reduce LLC code misses. ?SKU selects soft server SKUs for Web (Skylake) and Ads1 such that they dedicate {6, 5} and {9, 2} LLC ways for data and code, respectively, improving over the present-day hand-tuned production configuration. ?SKU does not enable CDP in Web's (Broadwell) soft SKU.</p><p>(5) Prefetcher. We report ?SKU's results for prefetcher tuning in Fig. <ref type="figure" target="#fig_15">17</ref>. Our production systems enable (1) all prefetchers on Web (Skylake) and Ads1 and (2) only the L2 hardware prefetcher and DCU prefetcher on Web (Broadwell). On Web (Broadwell), ?SKU reveals a ? 3% mean throughput win over the production configuration when all prefetchers  are turned off. Web (Broadwell) is heavily memory bandwidth bound when prefetchers are turned on, unlike Web (Skylake) and Ads1. Turning off prefetchers reduces memory bandwidth pressure, enabling overall throughput gains. In contrast, Web (Skylake) and Ads1 are not memory bandwidth bound, and hence do not benefit from turning off prefetchers.</p><p>(6) Transparent Huge Pages (THPs). In our earlier characterization (see Fig. <ref type="figure" target="#fig_9">11</ref>), we found that Web suffers from significant ITLB and DTLB misses. Hence, ?SKU explores huge page settings to reduce TLB miss rates. The default THP setting on our production servers is madvise, where THP is enabled only for memory regions that explicitly request it. In Fig. <ref type="figure" target="#fig_16">18</ref>(a), ?SKU considers (1) always enabling huge pages (always ON) and (2) disabling huge pages even when requested (never ON), and compares with the default (baseline for the graph) madvise configuration.</p><p>?SKU identifies a mean 1.87% throughput gain on Web (Skylake) when THP is always ON, as it significantly reduces TLB misses compared to madvise. However, the always ON setting does not enhance Ads1 and Web (Broadwell)'s throughput as their TLB miss rates do not improve. Throughput achieved with the never ON configuration is comparable with madvise, as few allocations use the madvise hint.</p><p>(7) Statically-allocated Huge Pages (SHPs). We report ?SKU's SHP sweep results in Fig. <ref type="figure" target="#fig_16">18(b)</ref>. ?SKU excludes Ads1 from this study as it makes no use of SHPs. Our production systems reserve 200 SHPs for Web (Skylake) and 488 SHPs for Web (Broadwell). ?SKU shows that reserving 300 SHPs on Web (Skylake) and 400 SHPs on Web (Broadwell) can outperform our production systems by 1.4% and 1.0% respectively, due to modest TLB miss reductions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Soft SKU Performance</head><p>?SKU creates microservice-specific soft SKUs by independently analyzing each knob and then composing their best configurations. In Fig. <ref type="figure" target="#fig_17">19</ref>, we show the final throughput gains achieved by ?SKU's soft SKUs as compared to (1) hand-tuned production configurations and (2) stock server configurations (i.e., after a fresh server re-install). The stock configuration comprises (1) 2.2 GHz and 2.0 GHz core frequency for Web and Ads1 respectively, (2) 1.8 GHz uncore frequency, (3) all cores active, (4) no CDP in LLC, (5) all prefetchers turned on, (6) always ON for THP, and (7) no SHPs. We listed the hand-tuned configurations in Sec. 6.1.</p><p>Since these services operate on hundreds of thousands of machines, achieving even single-digit percent speedups with ?SKU can yield immense aggregate data center efficiency benefits by reducing a service's provisioning requirement. ?SKU's soft SKUs outperform stock configurations by 6.2% on Web (Skylake), 7.2% on Web (Broadwell), and 2.5% on Ads1 due to benefits enabled by CDP, prefetchers, THP, and SHP. Interestingly, ?SKU also outperforms the hand-tuned production configurations by 4.5% on Web (Skylake), 3.0% on Web (Broadwell), and 2.5% on Ads1. We confirmed that the MIPS improvement reported by ?SKU's soft SKUs yields a corresponding QPS improvement over a prolonged period (spanning several code pushes) by monitoring fleet-wide QPS via ODS. The statistically significant throughput gains are a substantial win in data centers' efficiency.</p><p>?SKU's prototype takes 5-10 hours to explore its knob design space and arrive at the final soft-SKU configurations. Even for knob settings where ?SKU identifies the same result as manual tuning by experts, the savings in engineering effort by relying on an automated system is significant. A key advantage of ?SKU is that it can be applied to microservices that do not have dedicated performance tuning engineers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>We discuss open questions and ?SKU prototype limitations.</p><p>Future hardware knobs. Our architectural characterization revealed significant diversity in architectural bottlenecks across microservices. We discussed opportunities for microservice-specific hardware modifications and motivated how soft SKUs can be designed using existing hardware-and OS-based configurable knobs. However, in light of a soft-SKU strategy, we anticipate that hardware vendors might introduce additional tunable knobs. ?SKU does not currently adjust knobs to address microservice differences in instruction mix, branch prediction, context switch penalty, and other opportunities revealed in our characterization.</p><p>QoS and perf/watt constraints. Our microservices face stringent latency, throughput, and power constraints in the form of Service-Level Objectives (SLO). ?SKU's prototype performs A/B testing in a coarse-grained design space and tunes configurable hardware and OS knobs to improve throughput. However, ?SKU does not consider energy or power constraints. QoS constraints are only addressed insofar as we discard parts of the ?SKU tuning space that lead to violations.</p><p>?SKU can be extended to consider a cluster's SLOs' full range. For example, Cache executes exception handlers when latency targets are violated, which makes MIPS an inappropriate metric to quantify Cache performance. With support for other performance metrics, ?SKU can perform A/B tests that discount exception-handling code when measuring throughput. With support to also measure system power/energy, ?SKU can be extended to perform energy-or power-efficiency optimization rather than optimizing only for performance. We leave such support to future work.</p><p>Exhaustive design-space sweep. We notice that throughput improvements achieved by individual knobs are not always additive when ?SKU composes them to generate a soft SKU. This observation implies that knob configurations may have subtle dependencies on which we might capitalize. An exhaustive characterization that determines a Pareto-optimal soft SKU might identify global performance maxima that are better than those found by our independent search. However, performing an exhaustive search is prohibitive; better search heuristics (e.g., hill climbing <ref type="bibr" target="#b85">[86]</ref>) may be required.</p><p>?SKU and co-location. Our production microservices run on dedicated hardware without co-runners. Co-location can raise interesting challenges for future work-scheduler systems that map service affinities can be designed in a ?SKU-aware manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>Architectural proposals for cloud services. Several works propose architectures suited to a particular, important cloud service. Ayers et al. <ref type="bibr" target="#b22">[23]</ref> characterize Google web search's memory hierarchy and propose an L4 eDRAM cache to improve heap accesses. Earlier work <ref type="bibr" target="#b86">[87]</ref> also discusses microarchitecture for Google search. Some works <ref type="bibr" target="#b87">[88]</ref><ref type="bibr" target="#b88">[89]</ref><ref type="bibr" target="#b89">[90]</ref> characterize low-power cores for search engines like Nutch and Bing. Trancoso et al. <ref type="bibr" target="#b90">[91]</ref> analyze the AltaVista search engine's memory behavior and find it similar to decision support workloads; Barroso et al. <ref type="bibr" target="#b91">[92]</ref> show that L2 caches encompass such workloads' working set, leaving memory bandwidth under-utilized. Microsoft's Catapult accelerates search ranking via FPGAs <ref type="bibr" target="#b92">[93]</ref>. DCBench studies latencysensitive cloud data analytics <ref type="bibr" target="#b93">[94]</ref>. Studying a single service class can restrict the generality of conclusions, as modern data centers typically execute diverse services with varied behaviors. In contrast, we characterize diverse production microservices running in the data centers of one of the largest social medial providers. We show that modern microservices exhibit substantial system-level and architectural differences, which calls for microservice-specific optimization.</p><p>Other works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b94">95]</ref> propose architectural optimizations for diverse applications. Kanev et al. <ref type="bibr" target="#b0">[1]</ref> profile different Google services and propose architectural optimizations. Kozyrakis et al. <ref type="bibr" target="#b94">[95]</ref> examine Microsoft's email, search, and analytics applications, focusing on balanced server design. However, these works do not customize SKUs for particular services.</p><p>Academic efforts develop and characterize benchmark suites for cloud services. Most notably, CloudSuite <ref type="bibr" target="#b20">[21]</ref> comprises both latency-sensitive and throughput-oriented scale-out cloud workloads. Yasin et al. <ref type="bibr" target="#b62">[63]</ref> perform a microarchitectural characterization of several CloudSuite workloads. However, our findings on production services differ from those of academic cloud benchmark suite studies <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b95">96,</ref><ref type="bibr" target="#b96">97]</ref>. For example, unlike these benchmark suites, our microservices have large L2 and LLC instruction working sets, high stall times, large front-end pipeline stalls, and lower IPC. While these suites are vital for experimentation, it is important to compare their characteristics against large-scale production microservices serving live user traffic.</p><p>Hardware tuning. Many works tune individual server knobs, such as selective voltage boosting <ref type="bibr" target="#b97">[98]</ref><ref type="bibr" target="#b98">[99]</ref><ref type="bibr" target="#b99">[100]</ref>, exploiting multicore heterogeneity <ref type="bibr" target="#b100">[101]</ref><ref type="bibr" target="#b101">[102]</ref><ref type="bibr" target="#b102">[103]</ref>, trading memory latency/bandwidth <ref type="bibr" target="#b103">[104]</ref><ref type="bibr" target="#b104">[105]</ref><ref type="bibr" target="#b105">[106]</ref><ref type="bibr" target="#b106">[107]</ref>, or reducing front-end stalls <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b95">96,</ref><ref type="bibr" target="#b107">108]</ref>. In contrast, we propose (1) performance-efficient soft SKUs rather than hardware changes, (2) target diverse mi-croservices, and (3) tune myriad knobs to create customized microservice-specific soft SKUs. Other works reduce coscheduled job interference <ref type="bibr" target="#b108">[109]</ref><ref type="bibr" target="#b109">[110]</ref><ref type="bibr" target="#b110">[111]</ref><ref type="bibr" target="#b111">[112]</ref><ref type="bibr" target="#b112">[113]</ref><ref type="bibr" target="#b113">[114]</ref> or schedule them in a machine characteristics-aware manner <ref type="bibr" target="#b114">[115]</ref><ref type="bibr" target="#b115">[116]</ref><ref type="bibr" target="#b116">[117]</ref><ref type="bibr" target="#b117">[118]</ref>. Such studies can benefit from architectural insights provided here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>Modern data centers face server architecture design challenges as they must efficiently support diverse microservices. We presented a detailed system-level and architectural characterization of key microservices used by a leading social media provider. We highlighted surprising and diverse bottlenecks and proposed future server architecture optimization opportunities, since each microservice might benefit from a custom server SKU. However, to avoid per-service SKU deployment challenges, we instead proposed the "soft" SKU concept, wherein we tune coarse-grain configuration knobs on a few hardware SKUs. We developed ?SKU to automatically tune server knobs to create microservice-specific soft SKUs that outperform stock servers by up to 7.2%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Acknowledgement</head><p>We acknowledge Carlos Torres, Pallab Bhattacharya, Xiaodong Wang, Joy Chaoyue Xiong, Oded Horovitz, Denis Sheahan, Ning Sun, Mark Santaniello, Amlan Nayak, Chao Li, and Yudong Guang who provided valuable insights on Facebook workload characteristics and analysis. We acknowledge Murray Stokeley, Kim Hazelwood, Bharath Muthiah, Bill Jia, Christina Delimitrou, Carole-Jean Wu, Vaibhav Gogte, Amrit Gopal, PR Sriraman, Brendan West, Amirhossein Mirhosseini, and the anonymous reviewers for their insightful suggestions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) A single request's latency breakdown for each ?service: few ?services block for a long time, (b) Web's request latency breakdown: thread over-subscription causes scheduling delays.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Max. achievable CPU utilization in user-and kernel-mode across ?services: utilization can be low to avoid QoS violations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Fraction of a second spent context switching (range): Cache1 &amp; Cache2 can benefit from context switch optimizations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Instruction type breakdown across ?services: instruction mix ratios vary substantially across ?services.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Per-core IPC across our ?services &amp; prior work (IPC measured on other platforms): our ?services have a high IPC diversity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Top-down bottleneck breakdown: several of our microservices face high front-end stalls.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: L1 &amp; L2 code &amp; data MPKI: our microservices typically have higher L1 MPKI than comparison applications.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: LLC code &amp; data MPKI: LLC data MPKI is high across microservices and Web incurs a high code LLC MPKI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: LLC code and data MPKI vs. LLC size: some microservices may benefit from trading LLC capacity for more cores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: ITLB &amp; DTLB (load &amp; store) MPKI breakdown: some microservices can benefit from huge page support.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Memory bandwidth vs. latency: microservices underutilize memory bandwidth to avoid latency penalties.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: ?SKU: system design</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Perf. trend with (a) core frequency scaling, (b) uncore frequency scaling: the max. frequency offers the best performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Perf. trend with core count scaling: Web is core-bound.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Perf. trend with CDP scaling: (a) Web (Skylake) &amp; Ads1 benefit due to lower code MPKI (b) Web (Broadwell) has no gains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Perf. trends with varied prefetcher config.: turning off prefetchers can improve bandwidth utilization in Web (Broadwell).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: Perf. trends with varied (a) THP: Web (Skylake) benefits from THP ON, (b) SHP: there is a sweet spot in optimal SHP count.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: Perf. gain with ?SKU over stock and hand-tuned servers: ?SKU outperforms even hand-tuned production servers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Skylake18, Skylake20, Broadwell16's key attributes.</figDesc><table><row><cell></cell><cell>Skylake18</cell><cell>Skylake20</cell><cell>Broadwell16</cell></row><row><cell>Microarchitecture</cell><cell>Intel Skylake</cell><cell>Intel Skylake</cell><cell>Intel Broadwell</cell></row><row><cell>Number of sockets</cell><cell>1</cell><cell>2</cell><cell>1</cell></row><row><cell>Cores/socket</cell><cell>18</cell><cell>20</cell><cell>16</cell></row><row><cell>SMT</cell><cell>2</cell><cell>2</cell><cell>2</cell></row><row><cell>Cache block size</cell><cell>64 B</cell><cell>64 B</cell><cell>64 B</cell></row><row><cell>L1-I$ (per core)</cell><cell>32 KiB</cell><cell>32 KiB</cell><cell>32 KiB</cell></row><row><cell>L1-D$ (per core)</cell><cell>32 KiB</cell><cell>32 KiB</cell><cell>32 KiB</cell></row><row><cell>Private L2$ (per core)</cell><cell>1 MiB</cell><cell>1 MiB</cell><cell>256 KiB</cell></row><row><cell>Shared LLC (per socket)</cell><cell>24.75 MiB</cell><cell>27 MiB</cell><cell>24 MiB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Avg. request throughput, request latency, &amp; path length across microservices: we observe great diversity across services.</figDesc><table><row><cell>?service</cell><cell>Throughput (QPS)</cell><cell>Req. latency</cell><cell>Insn./query</cell></row><row><cell>Web</cell><cell>O (100)</cell><cell>O (ms)</cell><cell>O (10 6 )</cell></row><row><cell>Feed1</cell><cell>O (1000)</cell><cell>O (ms)</cell><cell>O (10 9 )</cell></row><row><cell>Feed2</cell><cell>O (10)</cell><cell>O (s)</cell><cell>O (10 9 )</cell></row><row><cell>Ads1</cell><cell>O (10)</cell><cell>O (ms)</cell><cell>O (10 9 )</cell></row><row><cell>Ads2</cell><cell>O (100)</cell><cell>O (ms)</cell><cell>O (10 9 )</cell></row><row><cell>Cache1</cell><cell>O (100K)</cell><cell>O (?s)</cell><cell>O (10 3 )</cell></row><row><cell>Cache2</cell><cell>O (100K)</cell><cell>O (?s)</cell><cell>O (10 3 )</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Summary of findings and suggestions for future optimizations.</figDesc><table><row><cell>Finding</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Profiling a warehouse-scale computer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kanev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Darago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The biggest thing amazon got right: The platform</title>
		<ptr target="https://gigaom.com/2011/10/12/419-the-biggest-thing-amazon-got-right-the-platform/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Adopting microservices at netflix: Lessons for architectural design</title>
		<ptr target="https://www.nginx.com/blog/microservices-at-netflix-architectural-best-practices/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Scaling Gilt: from Monolithic Ruby Application to Distributed Scala Micro-Services Architecture</title>
		<ptr target="https://www.infoq.com/presentations/scale-gilt" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Evaluating the monolithic and the microservice architecture pattern to deploy web applications in the cloud</title>
		<author>
			<persName><forename type="first">M</forename><surname>Villamizar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Garc?s</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Verano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Salamanca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Casallas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computing Colombian Conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">What is microservices architecture?</title>
		<ptr target="https://smartbear.com/learn/api-design/what-are-microservices/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tradeoffs between power management and tail latency in warehouse-scale applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kanev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Workload Characterization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Nadareishvili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mclarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Amundsen</surname></persName>
		</author>
		<title level="m">Microservice Architecture: Aligning Principles, Practices, and Culture</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">?Suite: A Benchmark Suite for Microservices</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sriraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Workload Characterization</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unfair Data Centers for Fun and Profit</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sriraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wild and Crazy Ideas (ASPLOS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">?Tune: Auto-Tuned Threading for OLDI Microservices</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sriraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX conference on Operating Systems Design and Implementation</title>
		<meeting>the 12th USENIX conference on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed Caching with Memcached</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fitzpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linux J</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Mcrouter</title>
		<ptr target="https://github.com/facebook/mcrouter" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Treadmill: Attributing the Source of Tail Latency Through Precise Load Testing and Statistical Inference</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meisner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Practical Lessons from Predicting Clicks on Ads at Facebook</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Atallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Q N</forename><surname>Candela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Data Mining for Online Advertising</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">TAO: Facebook&apos;s Distributed Data Store for the Social Graph</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bronson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Amsden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cabrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chakka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ferris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Giardullo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Annual Technical Conference</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Dynamically providing a news feed about a user of a social network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zuckerberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sanghvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bosworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sittig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Geminder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Corson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">HHVM JIT: A Profile-guided, Region-based Compiler for PHP and Hack</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ottoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Programming Language Design and Implementation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Spec cpu2006 benchmark descriptions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Henning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>SIGARCH Comp. Arch. News</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Workload Characterization of the SPEC CPU2017 Benchmark Suite</title>
		<author>
			<persName><forename type="first">A</forename><surname>Limaye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Adegbija</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Performance Analysis of Systems and Software</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Clearing the Clouds: A Study of Emerging Scale-out Workloads on Modern Hardware</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kocberber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alisafaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Architectural Implications of Cloud Microservices</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Delimitrou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Architecture Letters</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Memory Hierarchy for Web Search</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Hack and HHVM: programming productivity without breaking things</title>
		<author>
			<persName><forename type="first">O</forename><surname>Yamauchi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The hiphop virtual machine</title>
		<author>
			<persName><forename type="first">K</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Maher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ottoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paroski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Simmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Yamauchi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Acm Sigplan Notices</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Understanding user beliefs about algorithmic curation in the facebook news feed</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM conference on human factors in computing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Exposure to ideologically diverse news and opinion on Facebook</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bakshy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Messing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Adamic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Applied Machine Learning at Facebook: A Datacenter Infrastructure Perspective</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Diril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dzhulgakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fawzy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kalro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on High Performance Computer Architecture</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Tao: how facebook serves the social graph</title>
		<author>
			<persName><forename type="first">V</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Amsden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bronson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cabrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chakka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ferris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Giardullo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Management of Data</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Carlson</surname></persName>
		</author>
		<title level="m">Redis in Action</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Inside 6th-generation intel core: new microarchitecture code-named skylake</title>
		<author>
			<persName><forename type="first">J</forename><surname>Doweck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-F</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>-Y. Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mandelblat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rahatekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rappoport</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rotem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yoaz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>IEEE Micro</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Unlock system performance in dynamic environments</title>
		<ptr target="https://www.intel.com/content/www/us/en/architecture-and-technology/resource-director-technology.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Improving Real-Time Performance by Utilizing Cache Allocation Technology</title>
		<author>
			<persName><forename type="first">C</forename><surname>Intel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-04">April, 2015</date>
		</imprint>
		<respStmt>
			<orgName>Intel Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Code and Data Prioritization -Introduction and Usage Models in the Intel Xeon Processor E5 v4 Family</title>
		<ptr target="https://software.intel.com/en-us/articles/introduction-to-code-and-data-prioritization-with-usage-models" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Apache Hadoop goes realtime at Facebook</title>
		<author>
			<persName><forename type="first">D</forename><surname>Borthakur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Muthukkaruppan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Spiegelberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Management of data</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Gorilla: A fast, scalable, in-memory time series database</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pelkonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Teller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Veeraraghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Storage infrastructure behind Facebook messages: Using HBase at scale</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Aiyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bautin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Damania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Khemani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Muthukkaruppan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Spiegelberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vaidya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Eng. Bull</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Google-wide profiling: A continuous profiling infrastructure for data centers</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hundt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE micro</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Emon user&apos;s guide</title>
		<ptr target="https://software.intel.com/en-us/download/emon-user-guide" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Design tradeoffs for SSD performance</title>
		<author>
			<persName><forename type="first">N</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wobber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Manasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Panigrahy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Annual Technical Conference</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Intel and Micron Produce Breakthrough Memory Technology</title>
		<ptr target="https://newsroom.intel.com/news-releases/intel-and-micron-produce-breakthrough-memory-technology/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Persistency for synchronization-free regions</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gogte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Diestelhorst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Programming Language Design and Implementation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Language-level Persistency</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kolli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gogte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Diestelhorst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Performance analysis and evaluation of infiniband fdr and 40gige roce on hpc and cloud computing systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vienne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wasi-Ur-Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Subramoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 20th Annual Symposium on High-Performance Interconnects</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Taming the Killer Microsecond</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Palit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Honarmand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attack of the Killer Microseconds</title>
		<author>
			<persName><forename type="first">L</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Enhancing server efficiency in the face of killer microseconds</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mirhosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sriraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on High Performance Computer Architecture</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hiding the Microsecond-Scale Latency of Storage-Class Memories with Duplexity</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mirhosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sriraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Non-Volative Memories Workshop</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">LASER: Light, Accurate Sharing dEtection and Repair</title>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sriraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fugate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pokam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Newburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Devietti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on High Performance Computer Architecture</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Performance-Efficient Notification Paradigms for Disaggregated OLDI Microservices</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sriraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>in Workshop on Resource Disaggregation</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deconstructing the Tail at Scale Effect Across Network Protocols</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sriraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gunbay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Annual Workshop on Duplicating, Deconstructing, and Debunking</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The context-switch overhead inflicted by hardware interrupts (and the enigma of do-nothing loops)</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tsafrir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Experimental computer science</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Quantifying the cost of context switch</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Experimental computer science</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">High performance network virtualization with SR-IOV</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Parallel and Distributed Computing</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">mTCP: A Highly Scalable User-level TCP Stack for Multicore Systems</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Y</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jamshed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ihm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Conference on Networked Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">IX: A Protected Dataplane Operating System for High Throughput and Low Latency</title>
		<author>
			<persName><forename type="first">A</forename><surname>Belay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Prekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klimovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bugnion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Conference on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Dune: Safe User-level Access to Privileged CPU Features</title>
		<author>
			<persName><forename type="first">A</forename><surname>Belay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bittau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mashtizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Terei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mazi?res</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">User Space Network Drivers</title>
		<author>
			<persName><forename type="first">P</forename><surname>Emmerich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pudelko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Applied Networking Research Workshop</title>
		<meeting>the Applied Networking Research Workshop</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">An FPGA-based in-line accelerator for memcached</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lavasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Angepat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Architecture Letters</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Fine-grained consistency mechanism for optimistic concurrency control using lock groups</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Learmont</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Dynamic thread pool tuning techniques</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Cuomo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Daughtrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Hogstrom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">BPF in LLVM and kernel</title>
		<author>
			<persName><forename type="first">A</forename><surname>Starovoitov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linux Plumbers Conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Deep-dive analysis of the data analytics workload in cloudsuite</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ben-Asher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Workload Characterization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">AutoFDO: Automatic feedback-directed optimization for warehouse-scale applications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moseley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Code Generation &amp; Optimization</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">ThinLTO: scalable and incremental LTO</title>
		<author>
			<persName><forename type="first">T</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM International Symposium on Code Generation and Optimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Reactive NUCA: Near-optimal Block Placement and Replication in Distributed Caches</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hardavellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Improving QoS and Utilisation in modern multi-core servers with Dynamic Cache Partitioning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Papadakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nikas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Karakostas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Koziris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joined Workshops COSH 2017 and VisorHPC</title>
		<meeting>the Joined Workshops COSH 2017 and VisorHPC</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Scale-out Processors</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lotfi-Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kocberber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Picorel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Idgunji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ozer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Scalable Distributed Shared Last-Level TLBs Using Low-Latency Interconnects</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bharadwaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Blasting Through the Front-End Bottleneck with Shotgun</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nagarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Translation-Triggered Prefetching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Efficient Address Translation for Architectures with Multiple Page Sizes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Increasing TLB reach by exploiting clustering in page translations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Eckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on High Performance Computer Architecture</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Colt: Coalesced large-reach TLBs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vaidyanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Redundant Memory Mappings for Fast Access to Large Memories</title>
		<author>
			<persName><forename type="first">V</forename><surname>Karakostas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ayar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cristal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nemirovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>?nsal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Intel Memory Latency Checker v3.6</title>
		<ptr target="https://software.intel.com/en-us/articles/intelr-memory-latency-checker" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">A primer on hardware prefetching</title>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Computer Architecture</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">BigHouse: A Simulation Infrastructure for Data Center Systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Meisner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Performance Analysis of Systems &amp; Software</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Intel architecture, code name Skylake deep dive: A new architecture to manage power performance and energy efficiency</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rotem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intel Developer Forum</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">An energy efficiency feature survey of the intel haswell processor</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hackenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sch?ne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ilsche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schuchart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Geyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Parallel and Distributed Processing Symposium Workshop</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Stepping towards noiseless linux environment</title>
		<author>
			<persName><forename type="first">H</forename><surname>Akkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Liebrock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International workshop on runtime and operating systems for supercomputers</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Intel resource director technology (rdt) in linux</title>
		<ptr target="https://01.org/intel-rdt-linux" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Disclosure of H/W prefetcher control on some Intel processors</title>
		<ptr target="https://software.intel.com/en-us/articles/disclosure-of-hw-prefetcher-control-on-some-intel-processors" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Transparent hugepage support</title>
		<author>
			<persName><forename type="first">A</forename><surname>Arcangeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KVM forum</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">X-xen: huge page support in xen</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Gadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kabra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Darak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linux Symposium</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Hill-climbing search</title>
		<author>
			<persName><forename type="first">B</forename><surname>Selman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Gomes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Encyclopedia of Cognitive Science</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Web search for a planet: The google cluster architecture</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Holzle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>IEEE Micro</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Understanding and designing new server architectures for emerging warehouse-computing environments</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mudge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Web search using mobile cores: quantifying and mitigating the price of efficiency</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Janapa</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chilimbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Fawn: A fast array of wimpy nodes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Operating Systems Principles</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">The memory performance of DSS commercial workloads in shared-memory multiprocessors</title>
		<author>
			<persName><forename type="first">P</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Larriba-Pey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Torrellas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium High-Performance Computer Architecture</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Memory system characterization of commercial workloads</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gharachorloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bugnion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">A reconfigurable fabric for accelerating large-scale datacenter services</title>
		<author>
			<persName><forename type="first">P</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Adrian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Constantinides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecuture</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Characterizing data analysis workloads in data centers</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Workload Characterization</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Server engineering insights for large-scale online services</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE micro</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Microarchitectural Implications of Event-driven Server-side Web Applications</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Richins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">MeNa: A memory navigator for modern hardware in a scale-out environment</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Makrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Homayoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Workload Characterization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Adrenaline: Pinpointing and Reining in Tail Queries with Quick Voltage Boosting</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Laurenzano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meisner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dreslinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on High Performance Computer Architecture</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Rubik: Fast analytical power management for latency-critical systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kasture</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Bartolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Energy Proportionality and Workload Consolidation for Latency-critical Applications</title>
		<author>
			<persName><forename type="first">G</forename><surname>Prekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Primorac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Belay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bugnion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Cloud Computing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Exploiting Heterogeneity for Tail Latency and Energy Efficiency</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Elnikety</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bianchini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Rinnegan: Efficient Resource Use in Heterogeneous Architectures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Panneerselvam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Swift</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Parallel Architectures and Compilation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Amdahl&apos;s law for tail latency</title>
		<author>
			<persName><forename type="first">C</forename><surname>Delimitrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Understanding Latency Variation in Modern DRAM Chips: Experimental Characterization, Analysis, and Optimization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pekhimenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Measurement and Modeling of Computer Science</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Rethinking Design Metrics for Datacenter DRAM</title>
		<author>
			<persName><forename type="first">M</forename><surname>Awasthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Memory Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">An effective dram cache architecture for scale-out servers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>tech. rep.</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Reducing DRAM Latency via Charge-Level-Aware Look-Ahead Partial Restoration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tavakkol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Orosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sadrosadati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Confluence: Unified Instruction Supply for Scale-out Servers</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Tales of the Tail: Hardware, OS, and Application-level Sources of Tail Latency</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R K</forename><surname>Ports</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Gribble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Cloud Computing</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Measuring interference between live datacenter applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kambadur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Bubble-up: Increasing utilization in modern warehouse scale computers via sensible co-locations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Skadron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Soffa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Cpi 2: CPU performance isolation for shared compute clusters</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hagmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jnagal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gokhale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wilkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Bobtail: Avoiding Long Tails in the Cloud</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Musgrave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NSDI</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">The Impact of Memory Subsystem Resource Sharing on Datacenter Applications</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vachharajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Soffa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Symposium on Computer Architecture</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Whare-map: heterogeneity in homogeneous warehouse-scale computers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Paragon: QoS-aware Scheduling for Heterogeneous Datacenters</title>
		<author>
			<persName><forename type="first">C</forename><surname>Delimitrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Elfen Scheduling: Fine-Grain Principled Borrowing from Latency-Critical Workloads Using Simultaneous Multithreading</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Blackburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Annual Technical Conference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Esp: A machine learning approach to predicting application interference</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Autonomic Computing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
