<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Open-Vocabulary Multi-Label Classification via Multi-modal Knowledge Transfer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-07-05">5 Jul 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sunan</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tsinghua Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Taian</forename><surname>Guo</surname></persName>
							<email>taianguo@tencent.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">YouTu Lab</orgName>
								<address>
									<settlement>Tencent</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Dai</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">College of Computer Science and Software Engineering</orgName>
								<orgName type="institution">Shenzhen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruizhi</forename><surname>Qiao</surname></persName>
							<email>ruizhiqiao@tencent.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">YouTu Lab</orgName>
								<address>
									<settlement>Tencent</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Ren</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">YouTu Lab</orgName>
								<address>
									<settlement>Tencent</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tsinghua Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Open-Vocabulary Multi-Label Classification via Multi-modal Knowledge Transfer</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-07-05">5 Jul 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2207.01887v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Real-world recognition system often encounters a plenty of unseen labels in practice. To identify such unseen labels, multi-label zero-shot learning (ML-ZSL) focuses on transferring knowledge by a pre-trained textual label embedding (e.g., GloVe). However, such methods only exploit singlemodal knowledge from a language model, while ignoring the rich semantic information inherent in image-text pairs. Instead, recently developed open-vocabulary (OV) based methods succeed in exploiting such information of image-text pairs in object detection, and achieve impressive performance. Inspired by the success of OV-based methods, we propose a novel open-vocabulary framework, named multimodal knowledge transfer (MKT), for multi-label classification. Specifically, our method exploits multi-modal knowledge of image-text pairs based on a vision and language pretraining (VLP) model. To facilitate transferring the imagetext matching ability of VLP model, knowledge distillation is used to guarantee the consistency of image and label embeddings, along with prompt tuning to further update the label embeddings. To further recognize multiple objects, a simple but effective two-stream module is developed to capture both local and global features. Extensive experimental results show that our method significantly outperforms state-of-theart methods on public benchmark datasets. Code will be available at https://github.com/seanhe97/MKT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Multi-label recognition, which aims to recognize all the relevant labels in an image, is a fundamental task in computer vision applications, like scene understanding, surveillance systems and self-driving cars. In real-world applications, multi-label recognition systems should learn tens of thousands of labels, locate them in images, and even deal with many unseen labels. To date, classic multi-label classification methods trained and tested with seen labels are far from satisfying the requirements for real applications, where there exist a plenty of unseen labels.</p><p>To identify the unseen labels in an image, many multilabel zero-shot learning (ML-ZSL) methods <ref type="bibr" target="#b12">(Huynh and Elhamifar 2020;</ref><ref type="bibr" target="#b30">Gupta et al. 2021;</ref><ref type="bibr" target="#b0">Ben-Cohen et al. 2021;</ref><ref type="bibr" target="#b30">Narayan et al. 2021</ref>) have been recently developed by transferring knowledge between seen and unseen labels. However, most existing methods <ref type="bibr" target="#b46">(Zhang, Gong, and Shah 2016;</ref><ref type="bibr"></ref> language pre-training (VLP) models. Such OV-based methods trained on billions of image-text pairs contain powerful image-text matching ability, and have achieved remarkable performance in computer vision tasks like object detection. However, how to extend such OV-based methods to multilabel classification, including unseen text labels, is less explored.</p><p>Motivated by the above observations, we propose a novel open-vocabulary framework, named multi-modal knowledge transfer (MKT), for multi-label classification. Unlike the previous ML-ZSL methods that exploit only languagebased information, our MKT utilizes multi-modal knowledge from image-text pairs from a vision and language pretraining (VLP) model. As shown in Figure <ref type="figure">1</ref>(c), our MKT mainly consists of an image encoder to extract image features, and a VLP image/text encoder to extract image/label embeddings. Specifically, to facilitate transferring the image-text matching ability of VLP models, knowledge distillation and prompt tuning are introduced to guarantee the consistency of image and label embeddings. In practice, knowledge distillation makes image embeddings align better with its relevant label embeddings, while prompt tuning adapts the label embeddings to better support classification task. Besides, to further improve the ability of feature expressions, we propose a simple but effective two-stream feature extraction module to capture both local and global features to extract more discriminative features. In this way, our MKT framework can capture the rich semantic information inherent in image-text pairs of VLP models.</p><p>The main contributions can be summarized as follows:</p><p>1.</p><p>We propose an open-vocabulary based multi-modal knowledge transfer (MKT) framework for multi-label classification, which jointly exploits semantic multimodal information in image-text pairs based VLP models. To the best of our knowledge, this is the first work to explore open-vocabulary multi-label classification task.</p><p>2. Our MKT framework mainly consists of an image encoder to extract image features, and a VLP image/text encoder to extract image/label embeddings. To guarantee the consistency of image and label embeddings, knowledge distillation strategy is incorporated into our MKT framework, along with prompt tuning to update the label embeddings iteratively. Besides, to further improve the ability of feature expressions of our method, we propose a two-stream feature extraction module by jointly capturing local and global features.</p><p>3. Extensive results show that our MKT method significantly outperforms the previous ML-ZSL methods and establishes a new state of the art for open-vocabulary multi-label classification on two large-scale benchmark datasets, namely NUS-WIDE and Open Images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Works</head><p>Multi-Label Zero-Shot Learning</p><p>The goal of standard multi-label classification task is to predict a set of labels in an image. A vanilla approach is to train a binary classifier for each label present in the training dataset without considering the dependence among the labels (Tsoumakas and Katakis 2007; <ref type="bibr" target="#b36">Read et al. 2011)</ref>. To capture the label correlation, structure learning <ref type="bibr" target="#b9">(Gong et al. 2014;</ref><ref type="bibr" target="#b41">Wang et al. 2016;</ref><ref type="bibr" target="#b49">Zhu et al. 2017;</ref><ref type="bibr" target="#b42">Wang et al. 2017</ref>) and graph methods <ref type="bibr" target="#b21">(Li et al. 2016;</ref><ref type="bibr" target="#b19">Lee et al. 2018;</ref><ref type="bibr" target="#b3">Chen et al. 2019</ref>) are introduced in this task. Recently, vision transformer based methods have received much attention due to the powerful ability of capturing the global dependency <ref type="bibr" target="#b18">(Lanchantin et al. 2021;</ref><ref type="bibr">Liu et al. 2021;</ref><ref type="bibr" target="#b4">Cheng et al. 2021)</ref>. Although these methods have achieved promising results in multi-label classification, these methods cannot handle unseen labels, thus limiting their real applications.</p><p>To identify the unseen labels, zero-shot learning (ZSL) usually utilizes semantic information like attributes or word embeddings <ref type="bibr" target="#b29">(Mikolov et al. 2013;</ref><ref type="bibr" target="#b43">Xian, Schiele, and Akata 2017)</ref>. In particular, Lampert et al. <ref type="bibr" target="#b16">(Lampert, Nickisch, and Harmeling 2009)</ref> proposed two attribute-based paradigms with direct attribute prediction (DAP) and indirect attribute prediction (IAP). The former aims to learn multiple intermediate attribute classifiers <ref type="bibr" target="#b17">(Lampert, Nickisch, and Harmeling 2014)</ref>, while the latter uses seen class proportions for prediction <ref type="bibr" target="#b47">(Zhang and Saligrama 2015)</ref>. While they can generalize to single unseen labels well, they cannot handle multiple unseen labels in an image.</p><p>As an extension of ZSL, multi-label zero-shot learning (ML-ZSL) is developed to identify multiple seen and unseen labels in an image. The keys to this task are the alignment of image with its relevant label embeddings and the relation between seen and unseen label embeddings. To this end, Fast0Tag <ref type="bibr" target="#b46">(Zhang, Gong, and Shah 2016)</ref> and ZS-SDL <ref type="bibr" target="#b0">(Ben-Cohen et al. 2021)</ref> aim to find principal directions of an image along which the relevant labels rank higher while LESA <ref type="bibr" target="#b12">(Huynh and Elhamifar 2020)</ref> and BiAM <ref type="bibr" target="#b30">(Narayan et al. 2021)</ref> introduce attention module to capture both local and global features for better recognition of multiple objects. On the other hand, GAN-MLZSL <ref type="bibr" target="#b30">(Gupta et al. 2021</ref>) introduces generative adversarial networks (GANs) to tackle the problem of multi-label feature synthesis from corresponding multi-label class embedding.</p><p>However, most existing ML-ZSL works <ref type="bibr" target="#b46">(Zhang, Gong, and Shah 2016;</ref><ref type="bibr" target="#b12">Huynh and Elhamifar 2020;</ref><ref type="bibr" target="#b30">Narayan et al. 2021;</ref><ref type="bibr" target="#b30">Gupta et al. 2021</ref>) exploit only single-modal knowledge via a language model(e.g., GloVe <ref type="bibr" target="#b32">(Pennington, Socher, and Manning 2014)</ref>) as label embedding. Due to lack of visual information during the training process, these languagebased models cannot capture visual consistency among labels, thus limiting the generalization ability between seen and unseen labels. Moreover, most methods utilize wordlevel label embeddings and cannot generate label embedding of arbitrary text directly, thus hindering the performance on text labels. By contrast, we attempt to explore multi-modal knowledge from VLP models to leverage the consistency of image and label embeddings and can handle multiple word and text unseen labels.        an alternative way to predict arbitrary labels. Large-scale pre-trained models first become prevalent in natural language processing (NLP), such as BERT <ref type="bibr" target="#b6">(Devlin et al. 2018</ref>), GPT2 <ref type="bibr" target="#b34">(Radford et al. 2019)</ref> and RoBERTa <ref type="bibr" target="#b26">(Liu et al. 2019)</ref>. Based on large-scale language corpus <ref type="bibr" target="#b35">(Raffel et al. 2020)</ref> and multiple task-agnostic pre-training objectives <ref type="bibr" target="#b6">(Devlin et al. 2018</ref>), these pre-trained models achieve promising results in downstream tasks. Recently, Vision and Language Pre-training (VLP) models <ref type="bibr" target="#b27">(Lu et al. 2019;</ref><ref type="bibr" target="#b2">Chen et al. 2020;</ref><ref type="bibr">Li et al. 2020;</ref><ref type="bibr">Li et al. 2020;</ref><ref type="bibr" target="#b33">Radford et al. 2021;</ref><ref type="bibr" target="#b15">Kim, Son, and Kim 2021;</ref><ref type="bibr" target="#b14">Jia et al. 2021</ref>) have received much attention in multi-modal tasks. For example, with billions of imagetext pairs as training samples, CLIP <ref type="bibr" target="#b33">(Radford et al. 2021)</ref> and ALIGN <ref type="bibr" target="#b14">(Jia et al. 2021)</ref> have impressive performance in image-text matching task. By transferring this matching ability to the classification task, we can achieve arbitrary text label prediction. Specifically, for any concept, we can generate its label embedding through the text encoder of VLP model and calculate its similarity to image embedding for classification. Due to the large scale training corpus, we can evacuate label embedding of an unbounded vocabulary and achieve open-vocabulary classification. Some works have explored the open-vocabulary classification in object detection <ref type="bibr" target="#b45">(Zareian et al. 2021;</ref><ref type="bibr" target="#b10">Gu et al. 2021;</ref><ref type="bibr" target="#b8">Du et al. 2022;</ref><ref type="bibr" target="#b28">Ma et al. 2022;</ref><ref type="bibr" target="#b44">Zang et al. 2022</ref>) and image segmentation <ref type="bibr" target="#b13">(Huynh et al. 2021;</ref><ref type="bibr" target="#b9">Ghiasi et al. 2021)</ref>. They usually replace the classification head with label embeddings and achieve impressive performance in arbitrary text concept recognition. Moreover, for boosting the classi-fication ability, knowledge distillation <ref type="bibr" target="#b11">(Hinton et al. 2015)</ref> and prompt tuning <ref type="bibr" target="#b23">(Li and Liang 2021;</ref><ref type="bibr">Liu et al. 2021</ref>) are introduced to facilitate transferring the image-text matching ability <ref type="bibr" target="#b10">(Gu et al. 2021;</ref><ref type="bibr" target="#b28">Ma et al. 2022;</ref><ref type="bibr" target="#b48">Zhou et al. 2021;</ref><ref type="bibr" target="#b8">Du et al. 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open-Vocabulary Classification</head><p>However, most existing open-vocabulary works focus on single label classification task. Multi-label classification is more practical and challenging because models need to recognize multiple objects and cannot be trained with contrastive loss directly. In this work, we first explore the open-vocabulary multi-label classification task and propose a novel multi-modal knowledge transfer (MKT) framework by jointly exploiting multi-modal knowledge of the imagetext pairs based on vision and language pre-training models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-modal Knowledge Transfer Problem Setting</head><p>Similar to the ML-ZSL problem, suppose we have two disjoint label sets Y S and Y U , where Y S denotes seen labels present in the training set and Y U denotes unseen labels without training images. Let (x 1 , y 1 ) , . . . , (x N , y N ) be N training sample, where x i denotes the i-th training samples and y i ? Y S denotes the labels present in the image. In the standard zero-shot learning (ZSL) task, the goal is to learn a classifier f ZSL : X ? Y U to identify the relevant unseen labels for a given image. Note that in a more challenging and realistic setup of generalized zero-shot learning (GZSL) task, the classifier needs to identify both seen and unseen labels present in the test image, i.e., f GZSL :</p><formula xml:id="formula_0">X ? Y U ? Y S .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Overall Framework</head><p>As illustrated in Figure <ref type="figure" target="#fig_0">2</ref>, we show the overall architecture of our multi-modal knowledge transfer (MKT) method, which mainly consists of a vision transformer and a vision and language pre-training (VLP) model. Specifically, We utilize the vision transformer <ref type="bibr" target="#b7">(Dosovitskiy et al. 2021)</ref> as our backbone network to extract semantic features from input images. Due to the powerful ability in learning visual representations of CLIP <ref type="bibr" target="#b33">(Radford et al. 2021)</ref>, we choose CLIP <ref type="bibr" target="#b33">(Radford et al. 2021)</ref> as our VLP model to extract semantic multi-modal knowledge from both VLP image and text encoders. Concretely, label embedding is first generated based on VLP text encoder, followed by further updates through prompt tuning. Moreover, knowledge distillation is introduced to facilitate the alignment between image embeddings and its relevant labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vision Transformer with Two-Stream Module</head><p>Let's denote an input image as x ? R C?H?W , where H ?W is the size of the image and C is the number of channels. Following <ref type="bibr" target="#b7">(Dosovitskiy et al. 2021)</ref>, we reshape it into a sequence of flattened 2D patches x patch ? R N ?(P 2 ?C) , where P denotes the size of each patch and the total number of patches is N = HW/P 2 . Followed by a trainable linear projection, x patch is mapped into xpatch ? R N ?D , where D is input embedding dimension. Then the processing of the k-th block in vision transformer can be formulated as</p><formula xml:id="formula_1">x 0 = [E cls , xpatch ] + E pos , y k = x k-1 + MSA (NORM (x k-1 )) , x k = y k + MLP (NORM (y k )) ,<label>(1)</label></formula><p>where E cls ? R 1?D is the class token embedding and E pos ? R (1+N )?D is the position embedding. [?, ?] means concatenation. MLP (?) and NORM (?) denote the multilayer perceptron and norm layer respectively. MSA (?) presents the multi-head self-attention module which can be formulated as</p><formula xml:id="formula_2">MSA (x) = [head 1 , . . . , head H ] W O , head i = softmax Q i K T i ? d h V i , Q i = xW Q i , K i = xW K i , V i = xW V i ,<label>(2)</label></formula><p>where d h is the dimension of the heads and H is the total number of the heads.</p><formula xml:id="formula_3">W Q i , W K i , W V</formula><p>i denote the query, key and value weight matrix respectively while W O is the output matrix.</p><p>Let's denote x L ? R (1+N )?D the output of the vision transformer backbone, which is written as</p><formula xml:id="formula_4">x L [o cls , o patch ] ,<label>(3)</label></formula><p>where o cls ? R 1?D and o patch ? R N ?D are corresponding to the class and patch token input respectively. We argue that o cls represents the global feature while o patch incorporates the local features. To identify multiple labels in an image, we propose a simple two-stream module consisting of local head ? L (?) and global head ? G (?), mapping local and global features into embedding space respectively,</p><formula xml:id="formula_5">e cls = ? G (o cls ) , e patch = ? L (o patch ) ,<label>(4)</label></formula><p>where e patch = [e 1 , e 2 , . . . , e N ] ? R N ?De and e cls ? R 1?De are local and global feature embeddings respectively. D e is the dimension of embedding space. Then, final prediction score can be formulated as</p><formula xml:id="formula_6">s i = z i , e cls + TopK ([ z i , e 1 , z i , e 2 , . . . , z i , e N ]) ,<label>(5)</label></formula><p>where z i ? R 1?De is label embedding and TopK (?) is the top-k mean pooling. ?, ? denotes inner product.</p><p>The ranking loss L rank on prediction scores are used to train the network:</p><formula xml:id="formula_7">L rank i p?yi,n / ?yi max (1 + s n i -s p i , 0) ,<label>(6)</label></formula><p>where y i ? Y S is the ground truth labels present in an image i. s n i and s p i denote the scores of negative and positive labels respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge Distillation for Alignment</head><p>As the critical point to generalize to unseen labels, the alignment of an image embedding with its associated seen label embeddings plays a critical role in open-vocabulary multilabel classification. Note that we use CLIP <ref type="bibr" target="#b33">(Radford et al. 2021</ref>) model as our VLP model, consisting of an image encoder and a text encoder. Considering that the pre-training task of CLIP is to match the paired image and text, the image embedding generated by CLIP image encoder should be similar to its relevant label embeddings generated by CLIP text encoder. Thus, we introduce knowledge distillation to facilitate the alignment between the embeddings of an image and its relevant labels.</p><p>Let's denote the teacher model (i.e., CLIP image encoder) as ? CLIP I (?), then the process of distillation can be formulated as</p><formula xml:id="formula_8">L dist ? CLIP I (x) -o cls 1 = o dist -o cls 1 ,<label>(7)</label></formula><p>where x is an image input, o cls is the global features generated by the student model (i.e., our vision backbone), and o dist denotes the output of CLIP image encoder. The reason for distillation on the global features instead of the local is twofold. First, both o cls and the output of CLIP image encoder are corresponding to the CLS token. Moreover, the local features o patch corresponding to different input patches are expected to be discriminative instead of identical, facilitating the recognition of multiple objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompt Tuning for Label Embedding</head><p>Following <ref type="bibr" target="#b33">(Radford et al. 2021)</ref>, we first design a manual prompt template as "There is a {label} in the scene". We fill up the blank in this template with label name and treat the whole sentence as the input of CLIP text encoder. The output of CLIP text encoder is utilized as the label embedding. Due to the different training objectives, we argue that  <ref type="figure" target="#fig_0">2</ref>, are fixed. We show that compared with the hand-crafted prompt, continuous search in embedding space based on CLIP text encoder facilitates the learning of optimal context embedding for our task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Functions</head><p>We divide the training process of our method into two stages.</p><p>In the first stage, label embedding is first generated by the pre-trained CLIP text encoder, and the vision encoder is trained with the objectives of ranking loss and distillation loss,</p><formula xml:id="formula_9">L stage1 = L rank + ?L dist ,<label>(8)</label></formula><p>where ? is the weight factor of knowledge distillation. Then, in the second stage, all parameters except for context embedding are fixed, and training is performed with the objective of ranking loss,</p><formula xml:id="formula_10">L stage2 = L rank . (<label>9</label></formula><formula xml:id="formula_11">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In this section, to demonstrate the effectiveness of MKT, we conduct detailed experiments on NUS-WIDE <ref type="bibr" target="#b5">(Chua et al. July 8-10, 2009)</ref> and Open Images <ref type="bibr" target="#b31">(Papadopoulos et al. 2016)</ref> datasets. We first describe the experiment settings and then show the experiment results. Extensive experiments show that our approach surpasses previous state-of-the-art methods on both benchmarks datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Setup</head><p>Datasets: In the NUS-WIDE dataset, there are 81 human verified labels, in addition to 925 labels based on Flickr user tags. Similar to (Huynh and Elhamifar 2020), we treat 925 labels as seen labels and the other 81 labels as unseen labels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>State-of-the-art Comparison</head><p>In this experiment, we compare our model with traditional ML-ZSL methods and propose a strong OV-ML baseline CLIP-FT which is a pre-trained VLP model CLIP <ref type="bibr" target="#b33">(Radford et al. 2021</ref>) fine-tuned on base categories with ranking loss and has already surpassed most existing ML-ZSL method on mAP. The experimental results on zero-shot learning(ZSL) and generalized zero-shot learning(GZSL) tasks are shown in Table <ref type="table" target="#tab_7">1</ref>. The mAP and F1 scores at top-K (K ? {3, 5} for NUS-WIDE and K ? {10, 20} for Open Images) are reported.</p><p>On NUS-WIDE, the recently proposed BiAM <ref type="bibr" target="#b30">(Narayan et al. 2021)</ref>, which utilizes a bi-level attention module to enrich the features, acquires the best results in ZSL task with mAP score of 25.9%. MKT surpasses BiAM with an absolute gain of 11.7% mAP and improves the F1 score by absolute gains of 1.4% and 1.3% at K=3 and K=5 respectively. In GZSL task, the approach of ZS-SDL (Ben-Cohen et al. 2021) achieves the best scores with 12.1% mAP. MKT improves the mAP by an absolute gain of 6.5% and reaches state of the art in terms of F1 score with 22.0% at K=3 and 25.4% at K=5. Compared with CLIP-FT, MKT shows significant improvement on both ZSL and GZSL task.</p><p>On Open Images, following BiAM <ref type="bibr" target="#b30">(Narayan et al. 2021</ref>), we also calculate mAP weighted on different sample numbers(denoted as WmAP). ZS-SDL <ref type="bibr" target="#b0">(Ben-Cohen et al. 2021)</ref> reaches the state of the art before in terms of F1 score in both ZSL and GZSL tasks. MKT achieves consistent improvement over it with absolute gains of 9.0% (2.7%) and 3.1% (2.5%) at K=10 and K=20 on ZSL (GZSL) task. In comparison with previous best results on mAP (WmAP) metric, MKT outperforms BiAM <ref type="bibr" target="#b30">(Narayan et al. 2021</ref>) by 2.5% (16.3%) on ZSL and have a comparable performance on GZSL task. MKT also surpasses CLIP-FT on both tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies</head><p>Effects of knowledge distillation and prompt tuning: To study the impacts of knowledge distillation and prompt tuning, we conduct experiments with different training schemes and illustrate the results in Table <ref type="table" target="#tab_8">2</ref>. We take the first row as the baseline for the following comparisons, which is trained without knowledge distillation and prompt tuning. It shows that the introduction of knowledge distillation improves the performance on both ZSL and GZSL tasks. We conjecture that knowledge distillation not only facilitates the image embedding to align better with VLP model based label embedding but also suppresses the overfitting of the model to seen labels. Moreover, we observe that prompt tuning can further improve performance. It can be attributed to the reason that the prompt-tuned context embedding tends to pay more attention to the visual information that benefits image classification. Compared with the baseline in the first row, MKT shows significant improvement with combination of knowledge distillation and prompt tuning.</p><p>Comparison of label embedding: Because prediction results are based on the similarity between image and label embeddings, label embedding has a significant impact on model performance.   sual consistency among similar labels because of the lack of visual information during the training process, thus limiting the generalization ability to unseen labels. To validate our assumption, we conduct a label retrieval experiment. We select 62 common labels in NUS-WIDE and divide them into 14 major categories based on their visual and semantic similarity. (Please refer to the Supplementary Material for more details.) Both language models (i.e., GloVe and Bert) and VLP models (i.e., CLIP and its prompt-tuned version) are utilized to generate label embeddings. All embeddings are normalized, and cosine similarity is used to retrieve the most similar embeddings. Figure <ref type="figure">3</ref> illustrates the retrieval results with the overall Top-3 accuracy and examples of retrieved labels. Notice that compared with language model, VLP model can capture both semantic and visual consistency between labels. For instance, "girls" contains similar visual information with its retrieved labels "man", "kid" and "person". We argue that label embedding with both visual and semantic consistency facilitates the generalization to unseen labels.</p><p>Effect of the two-stream module: To demonstrate the effectiveness of our proposed two-stream module, we conduct ablation studies of both local and global heads. Table <ref type="table" target="#tab_11">4</ref> shows the results in terms of mAP and F1 score on NUS-WIDE. Notice that the global head only model performs well on mAP while the local head only model achieves better F1 score in ZSL task. We speculate that this is due to the fact that the global representation is more general while the local  Varying the hyper-parameters: Here, we explore the effect of knowledge distillation and variation of k value in local head. Because knowledge distillation aims to transfer zero-shot classification ability, we are more concerned about its performance on unseen labels. Figure <ref type="figure" target="#fig_2">5a</ref> illustrates the results of ZSL task with respect to distillation weight ?.</p><p>Notice that when ? is smaller than 1, the performance of our approach improves because knowledge distillation facilitates the alignment of image and label embeddings. However, there is a drop in performance while ? is larger than 2. We argue that too large ? may impair the learning of classification objective L rank . The two-stream module is designed to improve the recognition of multiple labels, so we focus more on its ability to classify all labels. Figure <ref type="figure" target="#fig_2">5b</ref> illustrates the results of GZSL when altering k value in local head. As k increases, F1 score reaches the highest when k=18. We argue that when k is too small, the local head output is sensitive to noise. On the other hand, if k is too large, the output will be less discriminative. For example, if k is set as the total patch number, top-k pooling will equal to global average pooling. In contrast to F1 score, mAP tends to increase while k value increases. When k is small, the local head output tends to be discriminative but sensitive to noise, resulting in a lower mAP value. As k increases, the local head output becomes moderate and more resistant to noise, leading to a higher mAP value.</p><p>Varying the backbone: For a fair comparison with existing ML-ZSL works such as LESA <ref type="bibr" target="#b12">(Huynh and Elhamifar 2020)</ref> and BiAM <ref type="bibr" target="#b30">(Narayan et al. 2021)</ref>, we explore MKT performance with the same backbone (e.g., VGG19 (Simonyan and Zisserman 2014) and DINO ResNet-50 <ref type="bibr" target="#b1">(Caron et al. 2021</ref>)) to avoid impact of superior backbones. For these CNN backbones, we treat feature of the last convo-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Standard Multi-label Classification</head><p>To validate the multi-label recognition ability of MKT, we also conduct a standard multi-label classification experiment. In this setting, all labels are available during the training stage. We also introduce a strong baseline ViT (Dosovitskiy et al. 2021) with binary cross entropy loss which is usually used in standard multi-label classification tasks. Table <ref type="table" target="#tab_14">6</ref> shows the results on NUS-WIDE <ref type="bibr" target="#b5">(Chua et al. July 8-10, 2009)</ref> with 81 human annotated labels. Our method MKT outperforms existing ML-ZSL methods by a large margin and has a comparable performance with a standard multilabel baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Assessment</head><p>To further demonstrate the effectiveness of our approach, we visualize both predictions and attention maps on several samples. Figure <ref type="figure">6</ref> presents predictions of CLIP <ref type="bibr" target="#b33">(Radford et al. 2021)</ref>, BiAM <ref type="bibr" target="#b30">(Narayan et al. 2021</ref>) and our approach on ZSL and GZSL tasks respectively. Compared with CLIP, our approach produces more diverse predictions because the two-stream module captures discriminative features. Com-  pared to BiAM, our model with VLP based label embedding can identify semantic and visual similarity among labels. For example, in the last sample of Figure <ref type="figure">6</ref>, label "plane", "airplane" and "aircraft" are synonymous and should have similar scores. Figure <ref type="figure">7</ref> illustrates the comparison of attention maps between BiAM and ours. The results show that our method can capture relevant regions more precisely. For instance, in the first column, BiAM pays attention to large irrelevant areas while our method exactly focuses on the boat region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In To explore the comparison between language model based and VLP model based label embeddings, we conduct both numerical and visual experiments on part of common labels in NUS-WIDE. As presented in Table <ref type="table" target="#tab_17">7</ref>, we first select 62 typical labels from both 925 seen labels and 81 unseen labels, then divide them into 14 major categories based on their semantic and visual similarity. For instance, label "kid", "girls", "mother", "man", "tourist" and "person" are all human beings and have similar visual appearance. Due to the visual similarity among these labels, their image embeddings generated by the same vision encoder tend to be similar. To facilitate the classification based on the similarity between image and label embeddings, their label embeddings are expected to be similar as well. The results of label retrieval experiment in Figure <ref type="figure">3</ref> of the main paper demonstrate the advantage of VLP model based label embedding, namely, the ability to capture both semantic and visual consistency among labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>t-SNE Visualization</head><p>To further illustrate the difference between language model based and VLP model based label embeddings, we utilize t-SNE to visualize the label embeddings generated by GloVe <ref type="bibr" target="#b32">(Pennington, Socher, and</ref><ref type="bibr">Manning 2014), Bert (Devlin et al. 2018</ref>) and prompt-tuned VLP model. In Figure <ref type="figure">8</ref>, we use different colors to distinguish different major categories and use different shapes to discriminate seen labels from unseen labels. As discussed above, the human labels like "man", "kid" and "mother" have visual similarity, but language model is unable to recognize such relationship among them, causing these labels to be mapped far apart in the embedding space. In contrast, VLP model can capture both semantic and visual consistency among these similar labels, leading to a better distribution in embedding space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics Mean Average Precision</head><p>Following <ref type="bibr">(Veit et al. 2017b)</ref>, to compute mAP score, we calculate average precision for each class c as where Precision(n, c) is the precision for class c when retrieving n highest-ranked predicted scores and rel(n, c) is an indicator function that is 1 iff the image at rank n contains label c and 0 otherwise. N c denotes the number of positives for class c. Then mAP can be computed as</p><formula xml:id="formula_12">AP c = N n=1 Precision(n, c) ? rel(n, c) N c ,<label>(10)</label></formula><formula xml:id="formula_13">mAP = 1 d d c=1 AP c ,<label>(11)</label></formula><p>where d is the number of labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F1 score</head><p>Following (Gong et al. 2014), we assign n highest-ranked predictions to each image and compare them with the ground truth labels. The mean-per-label precision and mean-perlabel recall are defined as</p><formula xml:id="formula_14">P c N t c c N p c , R c N t c c N c ,<label>(12)</label></formula><p>where N t c is the number of true positive for label c and N p c is the number of positive predictions for label c. Then, the F1 score can be computed as</p><formula xml:id="formula_15">F 1 = 2P R P + R .<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion of Metrics</head><p>As discussed in Section 4.1 of the main paper, the mAP reflects the ranking accuracy of each label across all images and the F1 score reflects the label ranking accuracy of each image. We argue that compared with F1 score, mAP is more sensitive to wrong predictions with high scores. For instance, in Table <ref type="table" target="#tab_18">8</ref>, the difference of predicted values between label "White" and "Black" is the value of negative label. With other values identical, the higher wrongly predicted value leads to a lower mAP score. In contrast, in Table <ref type="table" target="#tab_18">8</ref>, the difference of predicted values between instance "A" and "C" is also the value of negative label. Nevertheless, with other values identical, the higher wrongly predicted value does not change the Top-3 prediction accuracy and F1 score. We argue that the reason is twofold. On the one hand, when calculating F1 score, the Top-3 predictions are treated equally and their relative order is ignored. However, when computing mAP, the ranking of every predicted value for a label matters. On the other hand, as revealed in Figure <ref type="figure" target="#fig_1">4</ref> of the main paper, the distribution of higher values is more dispersed, and minor perturbations may not change top-K predicted labels. However, the same perturbations on smaller value predictions for negatives may change their rankings and impair the mAP score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of Complexity</head><p>We compare the complexity of our model and BiAM in Table 9, which shows that our method works better with fewer parameters. Notice that in their paper <ref type="bibr" target="#b30">(Narayan et al. 2021)</ref>, they didn't take the complexity of backbone into considera- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Qualitative Results</head><p>To further demonstrate the superiority of our model, we present an additional comparison of predicted results on NUS-WIDE among CLIP <ref type="bibr" target="#b33">(Radford et al. 2021)</ref>, BiAM <ref type="bibr" target="#b30">(Narayan et al. 2021</ref>) and our approach. Figure <ref type="figure">9</ref> shows the Top-5 and Top-10 predictions in ZSL and GZSL task respectively. Compared with other methods, our model can recognize visual and semantic consistency and capture local and global features, leading to more precise and diverse predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Prompt Tuning</head><p>Hand-crafted prompts are by no means guaranteed to be optimal for our task, so we further utilize prompt tuning to adapt the context embedding. We argue that through prompt tuning, label embedding tends to pay more attention to visual information. For instance, as illustrated in Figure <ref type="figure">3b</ref> of the main paper, the Top-3 retrieval results of query "girls" on CLIP embedding are "man", "kid" and "school". Label "school" and "girls" have semantic relevance, but few visual similarity. In contrast, the Top-3 retrieval results of  query "girls" on Prompt embedding are "man", "kid" and "person". As mentioned above, these labels all have a similar visual appearance, and their label embeddings should be similar too. Figure <ref type="figure">10</ref> shows the Top-5 predictions in the first and second stage of our method on ZSL task. Notice that the images containing label "whales" usually consist of a blue background and a huge foreground object. The firststage model tends to confuse whales with a bird or plane in the sky. We speculate that prompt tuning can also make the embedding of visually similar labels more distinguishable.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The overall framework of our multi-modal knowledge transfer (MKT) model for open-vocabulary multi-label classification. Our MKT mainly consists of a vision and language pre-training (VLP) model and a vision transformer model. The VLP model aims to extract multi-modal knowledge of input image-text pairs, while vision transformer is used to extract semantic features of input images. Moreover, knowledge distillation is used to guarantee the consistency of image and its relevant label embeddings, along with prompt tuning to further update the label embeddings. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Distribution of global and local predictions of baseline model. Due to the fact that most of prediction scores locate in [0, 0.2], we show the distribution between 0.2 and 0.6 for better visualization. Notice that the local head tends to predict higher scores than the global head.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Impact of hyper-parameters. The results of ZSL task with respect to distillation weight ? and GZSL task with respect to k for top-k in local head are presented.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>global pooling as the local feature and the feature of the last fully connected layer as the global feature. A similar two-stream module is employed on these features. Considering the CNN backbone, we select CLIP with ResNet-101 backbone as the VLP model. Table5shows that our method MKT surpass LESA (Huynh and Elhamifar 2020) and BiAM<ref type="bibr" target="#b30">(Narayan et al. 2021</ref>) with the same backbone on both NUS-WIDE<ref type="bibr" target="#b5">(Chua et al. July 8-10, 2009)</ref> and Open Images<ref type="bibr" target="#b31">(Papadopoulos et al. 2016</ref>).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: Comparison of predictions on test samples from NUS-WIDE. The top row shows the Top-5 prediction in ZSL task, and the bottom is the Top-10 prediction in GZSL task. True positive predictions are shown in green and the red font denotes apparently incorrect predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>Figure 9: Comparison of predictions among our model, BiAM, and CLIP. The prediction results of test samples from NUS-WIDE are presented. The top row shows the Top-5 prediction in ZSL task, while the bottom is the Top-10 prediction in GZSL task. True positive predictions are shown in green and the red font denotes apparently incorrect predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 1 :</head><label>1</label><figDesc>State-of-the-art comparison for ZSL and GZSL tasks on the NUS-WIDE and Open Images datasets. The results are reported in terms of mAP, as well as precision (P), recall (R), and F1 score at K?{3, 5} for NUS-WIDE and K?{10, 20} for Open Images. '*' means that the results are reproduced based on official pre-trained models. Bold indicates the best score.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">NUS-WIDE ( #seen / #unseen = 925/81)</cell><cell></cell><cell cols="6">Open-Images ( #seen / #unseen = 7186/400)</cell></row><row><cell>Method</cell><cell cols="2">Setting Task</cell><cell>P</cell><cell>K = 3 R</cell><cell>F1</cell><cell>P</cell><cell>K = 5 R</cell><cell>F1</cell><cell>mAP</cell><cell>P</cell><cell>K = 10 R</cell><cell>F1</cell><cell>P</cell><cell>K = 20 R</cell><cell>F1</cell><cell>mAP WmAP</cell></row><row><cell>LESA (M=10)</cell><cell></cell><cell cols="7">ZSL GZSL 23.6 10.4 14.4 19.8 14.6 16.8 25.7 41.1 31.6 19.7 52.5 28.7</cell><cell>19.4 5.6</cell><cell cols="6">0.7 25.6 1.4 16.2 18.9 17.4 10.2 23.9 14.3 0.5 37.4 1.0</cell><cell>41.7 45.4</cell><cell>--</cell></row><row><cell>GAN-MLZSL</cell><cell>ZS</cell><cell cols="7">ZSL GZSL 30.9 13.6 18.9 26.0 19.1 22.0 26.6 42.8 32.8 20.1 53.6 29.3</cell><cell>25.7 8.9</cell><cell cols="6">1.3 42.4 2.5 33.6 38.9 36.1 22.8 52.8 31.9 1.1 52.1 2.2</cell><cell>43.0 49.7</cell><cell>--</cell></row><row><cell>ZS-SDL</cell><cell></cell><cell cols="7">ZSL GZSL 27.7 13.9 18.5 23.0 19.3 21.0 24.2 41.3 30.5 18.8 53.4 27.8</cell><cell>25.9 12.1</cell><cell cols="6">6.1 47.0 10.7 4.4 68.1 8.3 35.3 40.8 37.8 23.6 54.5 32.9</cell><cell>62.9 75.3</cell><cell>--</cell></row><row><cell>BiAM  *</cell><cell></cell><cell cols="7">ZSL GZSL 25.2 11.1 15.4 21.6 15.9 18.2 26.6 42.5 32.7 20.5 54.6 29.8</cell><cell>25.9 9.4</cell><cell cols="6">3.9 30.7 7.0 13.8 15.9 14.8 9.7 22.3 14.8 2.7 41.9 5.5</cell><cell>65.6 81.7</cell><cell>72.9 85.0</cell></row><row><cell>CLIP-FT</cell><cell></cell><cell cols="7">ZSL GZSL 33.2 14.6 20.3 27.4 20.2 23.2 19.1 30.5 23.5 14.9 39.7 21.7</cell><cell>30.5 16.8</cell><cell cols="6">10.8 84.0 19.1 5.9 92.1 11.1 37.5 43.3 40.2 25.4 58.7 35.4</cell><cell>66.2 77.5</cell><cell>88.2 85.9</cell></row><row><cell>MKT</cell><cell>OV</cell><cell cols="7">ZSL GZSL 35.9 15.8 22.0 29.9 22.0 25.4 27.7 44.3 34.1 21.4 57.0 31.1</cell><cell>37.6 18.3</cell><cell cols="6">11.1 86.8 19.7 6.1 94.7 11.4 37.8 43.6 40.5 25.4 58.5 35.4</cell><cell>68.1 81.4</cell><cell>89.2 89.8</cell></row></table><note><p><p><p>the label embeddings generated by pre-trained CLIP text encoder are not optimal for multi-label classification. Thus, we propose to further fine-tune the label embedding. However, it is very hard to fine-tune the entire text encoder due to the model collapse problem caused by insufficient training samples. Motivated by CoOp</p><ref type="bibr" target="#b48">(Zhou et al. 2021)</ref></p>, we introduce prompt tuning for the adaptation of label embedding. During the tuning process, all parameters except for context embedding of prompt template illustrated as the dotted box named prompt template in Figure</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 2 :</head><label>2</label><figDesc>Following official train/test split, we utilize 161,789 images for training and 107,859 images for testing. The Open Images (v4) dataset is more challenging because it consists of 9 million training images and 125,456 testing images. Similar to<ref type="bibr" target="#b12">(Huynh and Elhamifar 2020)</ref>, we treat 7,186 labels that have more than 100 images in training set as seen and the most frequent 400 test set labels that are not in training data as unseen. Metrics: Following (Huynh and Elhamifar 2020), we use mean Average Precision (mAP)(Veit et al. 2017a)  and F1 score at top-K predictions<ref type="bibr" target="#b9">(Gong et al. 2014)</ref> to evaluate our method. The mAP reflects the ranking accuracy of each label across all images and the F1 score reflects the label ranking accuracy of each image. Implementation Details: Unless otherwise stated, we will use the settings described below for all experiments. We use the ImageNet<ref type="bibr" target="#b37">(Russakovsky et al. 2015)</ref> pre-trained ViT-B/16 1<ref type="bibr" target="#b7">(Dosovitskiy et al. 2021)</ref> as our vision backbone. As for two-stream module, local head consists of two linear layers, and global head is a linear projection layer. To generate label embedding and conduct knowledge distillation on vision encoder, we select the pre-trained CLIP 2 with ViT-B/16 image encoder as our VLP model. Patch projection of ViT-B/16 yields 14 ? 14 = 196 patches for an image with a resolution of 224 ? 224. The k for top-k pooling is set to 18, and the weight of knowledge distillation ? is set to 1. In the first stage, we use AdamW optimizer with base learning rate of 0.001 and weight decay of 0.005. We adjust base learning rate of the AdamW optimizer to 0.00003 during the second stage for fine-tuning the context embedding. On NUS-Impact of knowledge distillation and prompt tuning. Results are reported in terms of mAP and F1 score on NUS-WIDE. Bold indicates the best score. WIDE, we train the model for 20 epochs with the mini-batch of 128 and 10 epochs with the mini-batch of 16 in the first and second stage respectively. Considering the large scale of Open Images, the model is trained for 4 epochs and 2 epochs in each stage with the same batch size as above.</figDesc><table><row><cell>Distill Prompt Task</cell><cell cols="3">mAP F1 (K = 3) F1 (K = 5)</cell></row><row><cell>ZSL</cell><cell>32.4</cell><cell>29.4</cell><cell>26.5</cell></row><row><cell>GZSL</cell><cell>16.8</cell><cell>21.0</cell><cell>24.0</cell></row><row><cell>ZSL</cell><cell>37.3</cell><cell>32.5</cell><cell>29.5</cell></row><row><cell>GZSL</cell><cell>18.2</cell><cell>21.7</cell><cell>24.9</cell></row><row><cell>ZSL</cell><cell>32.5</cell><cell>29.5</cell><cell>26.4</cell></row><row><cell>GZSL</cell><cell>16.8</cell><cell>21.1</cell><cell>24.1</cell></row><row><cell>ZSL</cell><cell>37.6</cell><cell>34.1</cell><cell>31.1</cell></row><row><cell>GZSL</cell><cell>18.3</cell><cell>22.0</cell><cell>25.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 3 :</head><label>3</label><figDesc>Impact of label embedding. For a fair comparison, we only change label embedding and train both models without knowledge distillation or prompt tuning. Results are reported in terms of mAP and F1 score on NUS-WIDE. Bold indicates the best score.</figDesc><table><row><cell cols="2">Embedding Task</cell><cell cols="4">mAP F1 (K = 3) F1 (K = 5)</cell></row><row><cell>GloVe</cell><cell>ZSL GZSL</cell><cell>27.1 16.1</cell><cell></cell><cell>22.8 20.6</cell><cell>21.4 23.4</cell></row><row><cell>CLIP</cell><cell>ZSL GZSL</cell><cell>32.4 16.8</cell><cell></cell><cell>29.4 21.0</cell><cell>26.5 24.0</cell></row><row><cell></cell><cell cols="4">Query Label Embedding Label 1</cell><cell>Label 2</cell><cell>Label 3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>GloVe</cell><cell>Desert</cell><cell>Tourist</cell><cell>Plane</cell></row><row><cell></cell><cell></cell><cell>Girls</cell><cell>Bert CLIP</cell><cell>Kid Man</cell><cell>Cat Kid</cell><cell>Dog School</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Prompt</cell><cell>Man</cell><cell>Kid</cell><cell>Person</cell></row><row><cell></cell><cell></cell><cell></cell><cell>GloVe</cell><cell>Plane</cell><cell>Dawn</cell><cell>Train</cell></row><row><cell></cell><cell></cell><cell>Airport</cell><cell>Bert CLIP</cell><cell cols="2">Hospital Locomotive Airplane Plane</cell><cell>Hotel Aircraft</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Prompt</cell><cell>Airplane</cell><cell>Aircraft</cell><cell>Plane</cell></row><row><cell cols="2">(a) Top-3 Accuracy</cell><cell cols="4">(b) Top-3 Retrieved Results</cell></row><row><cell cols="6">Figure 3: Results of label retrieval. Overall Top-3 accuracy</cell></row><row><cell cols="6">and examples of retrieved labels are reported. Retrieved la-</cell></row><row><cell cols="6">bels belonging to the same major category with the query</cell></row><row><cell cols="4">label are considered to be correct.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>shows the results of baseline</cell></row></table><note><p><p><p>the model based on GloVe embedding, the VLP embedding based model achieves superior performance on both ZSL and GZSL task. We speculate that language models like GloVe or Bert</p><ref type="bibr" target="#b6">(Devlin et al. 2018</ref></p>) cannot capture vi-</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 4 :</head><label>4</label><figDesc>Effectiveness of the two-steam module. Results are reported in terms of mAP and F1 score on NUS-WIDE. Bold indicates the best score, and underline indicates the second best.</figDesc><table><row><cell cols="4">Local Global Task mAP F1 (K = 3) F1 (K = 5)</cell></row><row><cell>ZSL</cell><cell>29.1</cell><cell>29.9</cell><cell>27.2</cell></row><row><cell cols="2">GZSL 15.7</cell><cell>20.8</cell><cell>23.8</cell></row><row><cell>ZSL</cell><cell>30.3</cell><cell>23.3</cell><cell>21.4</cell></row><row><cell cols="2">GZSL 15.5</cell><cell>19.4</cell><cell>22.1</cell></row><row><cell>ZSL</cell><cell>32.4</cell><cell>29.4</cell><cell>26.5</cell></row><row><cell cols="2">GZSL 16.8</cell><cell>21.0</cell><cell>24.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>representation is more discriminative. As illustrated in, the local head tends to predict higher scores than the global head. While the more discriminative feature allows relevant labels to stand out, it also makes the model more sensitive to noise, leading to wrong predictions. On the other hand, compared to F1 score, mAP is more susceptible to the wrong predictions with high scores. (Please refer to the Supplementary Material for further discussions.) Therefore, the local head only model acquires better F1 score and inferior mAP. With the combination of local and global heads, the two-stream module can acquire more discriminative prediction scores with resistance to noise, leading to a favorable overall performance.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 5 :</head><label>5</label><figDesc>Robustness to Backbone Variation. We modified MKT with VGG-19 and DINO ResNet-50 backbones. Results are reported in terms of mAP on both benchmarks. Bold indicates the best score.</figDesc><table><row><cell cols="2">Backbone Method</cell><cell cols="4">NUS-WIDE ZSL GZSL ZSL GZSL Open Images</cell></row><row><cell>VGG-19</cell><cell>BiAM MKT</cell><cell>26.3 28.5</cell><cell>9.3 9.9</cell><cell>73.6 87.0</cell><cell>84.5 87.1</cell></row><row><cell>ResNet-50</cell><cell>BiAM MKT</cell><cell>27.4 31.5</cell><cell>10.2 14.9</cell><cell>74.0 87.7</cell><cell>84.8 85.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 6 :</head><label>6</label><figDesc>Comparison for the standard multi-label classification on NUS-WIDE. Results are reported in terms of mAP, as well as precision (P), recall (R), and F1 score. Bold indicates the best score.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>this work, we propose an open-vocabulary based multimodal knowledge transfer (MKT) framework for multi-label classification, which jointly exploits semantic multi-modal information in image-text pairs based VLP models. To facilitate transferring the image-text matching ability of VLP model to classification, knowledge distillation and prompt tuning are introduced. And a two-stream module is proposed to capture both local and global features, leading to significant performance gains in multi-label task. Extensive results demonstrate that our model surpasses previous ML-ZSL methods and establishes a new state of the art for open-vocabulary multi-label classification on NUS-WIDE and Open Images datasets. We are the first work in openvocabulary multi-label classification and hope to encourage future works to explore multi-modal knowledge application in classification.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 7 :</head><label>7</label><figDesc>Label list. We select 62 labels from both seen and unseen label sets, and divide them into 14 major categories based on their semantic and visual similarity.</figDesc><table><row><cell>Major Category</cell><cell>Labels</cell></row><row><cell>Human</cell><cell>kid, girls, mother, man, tourist, person</cell></row><row><cell>Animal</cell><cell>horse, bear, elk, dog, cow, zebra, fox, tiger, horses, cat</cell></row><row><cell>Plant</cell><cell>plant, flowers, tree, grass, garden</cell></row><row><cell>Place</cell><cell>school, hotel, hospital, chapel, restaurant</cell></row><row><cell>Terrestrial</cell><cell>mountains, desert, glacier, clouds, mountain</cell></row><row><cell>Water</cell><cell>river, ocean, water, waterfall, lake</cell></row><row><cell>Time</cell><cell>sunrise, dawn, darkness, sunset, nighttime</cell></row><row><cell>Vehicle</cell><cell>truck, vehicle, cars</cell></row><row><cell>Building</cell><cell>buildings, cityscape, tower, temple</cell></row><row><cell>Rail</cell><cell>locomotive, railroad, train</cell></row><row><cell>Flight</cell><cell>aircraft, airplane, airport, plane</cell></row><row><cell>Ship</cell><cell>ship, boats</cell></row><row><cell>Material</cell><cell>wood, stone, metal</cell></row><row><cell>Furniture</cell><cell>carpet, chairs</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 8 :</head><label>8</label><figDesc>Figure8: t-SNE visualization of label embeddings based on different models. Same color denotes the labels (e.g., kid, woman) that belong to same major category like Human. For Bert and VLP model, label embeddings are generated with the same manual template "There is a {label} in the scene". Compared with GloVe and Bert, VLP model can capture semantic and visual consistency among similar labels. (Best viewed in color.) Wrong Prediction Impact on AP. Last row shows the average precision (AP) of different labels. Underline denotes the ground truth label. Gray cell denotes the difference of predicted values between label "White" and "Black", and between instance "A" and "C".</figDesc><table><row><cell cols="3">(a) GloVe Based Embedding</cell><cell></cell><cell></cell><cell cols="3">(b) Bert Based Embedding</cell><cell></cell><cell cols="2">(c) VLP Model Based Embedding</cell></row><row><cell>Marks</cell><cell cols="2">? Unseen Label ? Seen Label</cell><cell cols="2">Major Categories</cell><cell>Human Furniture</cell><cell>Animal Material</cell><cell>Plant Ship</cell><cell>Place Flight</cell><cell>Terrestrial Rail</cell><cell>Water Building</cell><cell>Time Vehicle</cell></row><row><cell>Sample</cell><cell>Dog</cell><cell cols="4">Prediction of Label Cat White Black</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>A</cell><cell>0.8</cell><cell>0.4</cell><cell>0.6</cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>B</cell><cell>0.3</cell><cell>0.6</cell><cell>0.5</cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>C</cell><cell>0.5</cell><cell>0.8</cell><cell>0.4</cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>D</cell><cell>0.6</cell><cell>0.1</cell><cell>0.2</cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>AP</cell><cell cols="5">0.75 0.75 0.806 0.639</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 9 :</head><label>9</label><figDesc>Comparison of Complexity. For a fair comparison, both models are run on the Tesla V100.</figDesc><table><row><cell cols="4">Method mAP Inference FLOPs Params</cell></row><row><cell>BiAM</cell><cell>25.9</cell><cell>8.3 ms</cell><cell>20.19 G 143.4 M</cell></row><row><cell>MKT</cell><cell>37.6</cell><cell>13.8 ms</cell><cell>16.99 G 86.8 M</cell></row><row><cell>tion.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>In this Supplementary Material, we present additional details and analyses to further understand our open-vocabulary multi-label classification method MKT. This material is organized as follows:</p><p>? Details of label selection and t-SNE visualization of different label embeddings. ? Definition and discussion of evaluation metrics.</p><p>? Comparison of complexity.</p><p>? Additional qualitative prediction results.</p><p>? Analysis on the effect of prompt tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of Label Embedding</head><p>Label Selection</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic Diversity Learning for Zero-Shot Multi-Label Classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ben-Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="640" to="650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Emerging Properties in Self-Supervised Vision Transformers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9650" to="9660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">UNITER: Learning UNiversal Image-TExt Representations</title>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-Label Image Recognition With Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Z.-M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.06195</idno>
		<title level="m">MlTr: Multi-label Classification with Transformer</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">NUS-WIDE: A Real-World Web Image Database from National University of Singapore</title>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM Conf. on Image and Video Retrieval (CIVR&apos;09)</title>
		<meeting>of ACM Conf. on Image and Video Retrieval (CIVR&apos;09)<address><addrLine>Santorini, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">July 8-10, 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<title level="m">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In ICLR 2021: The Ninth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.14940</idno>
		<title level="m">Learning to Prompt for Open-Vocabulary Object Detection with Vision-Language Model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep Convolutional Ranking for Multilabel Image Annotation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2014 : International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014">2021. 2014. 2014</date>
		</imprint>
	</monogr>
	<note>Open-Vocabulary Image Segmentation</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Openvocabulary Object Detection via Vision and Language Knowledge Distillation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van De Weijer ; Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11606</idno>
		<idno>arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Shared Multi-Attention Framework for Multi-Label Zero-Shot Learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<title level="m">Open-Vocabulary Instance Segmentation via Robust Cross-Modal Pseudo-Labeling</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="4904" to="4916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2021: 38th International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5583" to="5594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="951" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attribute-Based Classification for Zero-Shot Visual Object Categorization</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="453" to="465" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">General Multi-label Image Classification with Transformers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lanchantin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="16478" to="16488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-Label Zero-Shot Learning With Structured Knowledge Graphs</title>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unicoder-VL: A Universal Encoder for Vision and Language by Cross-Modal Pre-Training</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11336" to="11344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Conditional Graphical Lasso for Multi-Label Image Classification</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00190</idno>
		<title level="m">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.10834</idno>
		<title level="m">Query2Label: A Simple Transformer Way to Multi-Label Classification</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10385</idno>
		<title level="m">GPT Understands, Too</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Curran Associates Inc</publisher>
			<pubPlace>Red Hook, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<title level="m">Open-Vocabulary One-Stage Detection with Hierarchical Visual-Language Knowledge Distillation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Discriminative Region-Based Multi-Label Zero-Shot Learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8731" to="8740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">We Don&apos;t Need No Bounding-Boxes: Training Object Class Detectors Using Only Human Verification</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning Transferable Visual Models From Natural Language Supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2021: 38th International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Classifier chains for multi-label classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="333" to="359" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Katakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Data Warehousing and Mining (IJDWM)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2007">2014. 2007</date>
		</imprint>
	</monogr>
	<note>Multi-label classification: An overview</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning From Noisy Large-Scale Datasets With Minimal Supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning From Noisy Large-Scale Datasets With Minimal Supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">CNN-RNN: A Unified Framework for Multi-Label Image Classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multi-Label Image Recognition by Recurrently Discovering Attentional Regions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Zero-Shot Learning -the Good, the Bad and the Ugly</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<title level="m">Open-Vocabulary DETR with Conditional Matching</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Open-Vocabulary Object Detection Using Captions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14393" to="14402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fast Zero-Shot Image Tagging</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5985" to="5994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Zero-Shot Learning via Semantic Similarity Embedding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01134</idno>
		<title level="m">Learning to Prompt for Vision-Language Models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning Spatial Regularization With Image-Level Supervisions for Multi-Label Image Classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
