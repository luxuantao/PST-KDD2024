<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attention-Based Two-Stream Convolutional Networks for Face Spoofing Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haonan</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Zhejiang Provincial Key Laboratory for Network Multimedia Technologies</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guosheng</forename><surname>Hu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Queens University Belfast</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Lei</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yaowu</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Zhejiang Provincial Key Laboratory for Network Multimedia Technologies</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Neil</forename><forename type="middle">M</forename><surname>Robertson</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Queens University Belfast</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Transactions on Information Forensics and Security</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Attention-Based Two-Stream Convolutional Networks for Face Spoofing Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">270D52BA3615BCB16CA49024804C7195</idno>
					<idno type="DOI">10.1109/TIFS.2019.2922241</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIFS.2019.2922241, IEEE Transactions on Information Forensics and Security 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Face spoofing</term>
					<term>multi-scale retinex</term>
					<term>deep learning</term>
					<term>attention model</term>
					<term>feature fusion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Since the human face preserves the richest information for recognizing individuals, face recognition has been widely investigated and achieved great success in various applications in the past decades. However, face spoofing attacks (e.g. face video replay attack) remain a threat to modern face recognition systems.Though many effective methods have been proposed for anti-spoofing, we find that the performance of many existing methods is degraded by illuminations. It motivates us to develop illumination-invariant methods for anti-spoofing. In this paper, we propose a two stream convolutional neural network (TSCNN) which works on two complementary space: RGB space (original imaging space) and multi-scale retinex (MSR) space (illumination-invariant space). Specifically, RGB space contains the detailed facial textures yet is sensitive to illumination; MSR is invariant to illumination yet contains less detailed facial information. In addition, MSR images can effectively capture the high-frequency information, which is discriminative for face spoofing detection. Images from two spaces are fed to the TSCNN to learn the discriminative features for anti-spoofing. To effectively fuse the features from two sources (RGB and MSR), we propose an attention-based fusion method, which can effectively capture the complementarity of two features. We evaluate the proposed framework on various databases, i.e. CASIA-FASD, REPLAY-ATTACK and OULU, and achieve very competitive performance. To further verify the generalization capacity of the proposed strategies, we conduct cross-database experiments, and the results show the great effectiveness of our method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>C OMPARED with traditional authentication approaches including password, verification code and secret question, biometrics authentication is more user-friendly. Since the human face preserves rich information for recognizing individuals, face becomes the most popular biometric cue with the excellent performance of identity recognition. Currently, person identification can easily use the face images captured from a distance without physical contact with the camera on the mobile devices, e.g. mobile phone.</p><p>As the application of face recognition system becomes more and more popular with the widespread of the Mobile phone, their weaknesses of security become increasingly conspicuous. For example, owing to the popularity of social network, it is quite easy to access a person's face image on the Internet to *Equal Contribution Corresponding author (cyw@mail.bme.zju.edu.cn) attack a face recognition system. Hence, a deep attention for face spoofing detection has been drawn and it has motivated great quantity of studies in the past few years.</p><p>In general, there are mainly four types of face spoofing attacks: photo attack, masking attack, video replay attack and 3D attack. Due to the high cost of the masking attack and 3D attack, therefore, the photo attack and video reply attack are the two most common attacks. Photo and video replay attacks can be launched with still face images and videos of the user in front of the camera, which are actually recaptured from the real ones. Obviously, the recaptured image is of lower quality compared with the real one in the same capture conditions. The lower quality of attacks can result from: lack of high frequency information <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b4">[5]</ref>, image banding or moire effects <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, video noise signatures, etc. Clearly, these image quality degradation factors can work as the useful cues to distinguish the real faces and the fake ones.</p><p>Face spoofing detection, which is also called face liveness detection, has been designed to counter different types of spoofing attacks. Face spoofing detection usually works as a preprocessing step of the face recognition systems to judge whether the face image is acquired from a real person or a printed photo (replay video). Therefore, face spoofing detection is actually a binary classification problem.</p><p>To counter the face spoofing attacks, there are mainly four solutions available in the research literature: (1) microtexture based methods, (2) image quality based methods, (3) motion based methods, and (4) reflectance based methods. For <ref type="bibr" target="#b0">(1)</ref>, local micro-texture features are demonstrated as a useful cue when attacked by photo and video. Researchers start the texture-based methods by feeding hand-crafted features extracted from facial texture to classifiers <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b11">[12]</ref>. With the development of deep learning, CNN <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b14">[15]</ref> is utilized to learn discriminative features for face spoofing detection. For <ref type="bibr" target="#b1">(2)</ref>, the low imaging quality of the fake images offers the useful clues <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b6">[7]</ref>, e.g. the loss to high frequency information, these clues have successfully been used for spoofing detection. For (3), motion-based methods mainly contain: physiological reaction based <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref> and physical movement based <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Motion-based methods may become less effective when conducted by video replay which can present the facial motions. For <ref type="bibr" target="#b3">(4)</ref>, reflectance of the face image is another widely used cue for liveness detection because the lighting reflectance from real face (3D) and attacking (mostly 2D, such as photo and replay attacks) face is very different <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>.</p><p>In this work, we propose a novel deep learning based microtexture based (MTB) method. The existing MTB methods usually process and analyze the input images in original RGB color space. However, the RGB images are sensitive to illuminations. The RGB based MTB methods can potentially reduce their performance in the presence of illuminations. This motivates us to develop a illumination-robust MTB method. Therefore, we proposed a two-stream convolutional neural network (TSCNN) which is trained on two complementary space: RGB space (original space) and multi-scale retinex (MSR) <ref type="bibr" target="#b22">[23]</ref> space (illumination-invariant space).</p><p>First, both RGB and MSR images contain discriminative information: RGB images can be used to train end-to-end discriminative CNNs for spoofing detection; MSR can capture high frequency information, and this information is verified particularly effective for spoofing detection. Second, RGB and MSR images are complementary: RGB space contains the detailed facial information yet is sensitive to illumination; MSR is invariant to illumination yet contains less detailed facial information. In the framework of TSCNN, the RGB and MSR images are fed to two CNNs (two branches of TSCNN) separately and generate two features which are discriminative for anti-spoofing. To effectively fuse these two features, we propose a learning-based fusion method inspired by attention mechanism <ref type="bibr" target="#b23">[24]</ref> detailed in Section III-C. Apart from the commonly used fusion methods, e.g. feature averaging fusion, our attention-based fusion can adaptively weight features to achieve promising performance of fused features. Fig. <ref type="figure" target="#fig_0">1</ref> shows the complementarity of RGB and MSR and the importance of the feature fusion. Our contributions can be summarized as:</p><p>• We propose a two-stream CNN (TSCNN) which accepts two complementary information (RGB and MSR images) as input. To our knowledge, we are the first to investigate the fusion of these two discriminative clues (RGB and MSR) for face anti-spoofing.</p><p>• To adaptively and effectively fuse two features generated by TSCNN, we proposed an attention-based fusion method. The proposed fusion method can make the TSCNN generalize well to images under various lighting conditions.</p><p>• We conduct extensive evaluations on three popular antispoofing databases: CASIA-FASD, REPLAY-ATTACK and OULU. The results show the effectiveness of the proposed strategies. In addition, we run cross-database experiments with very competitive results, showing the great generalization capacity of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS A. Face Spoofing Detection</head><p>In these years, various methods have been proposed for face spoofing detection. In this section, we briefly review the existing anti-spoofing methods.</p><p>Texture Based Methods Texture based methods focus on exploring different texture-based features for face spoofing detection. The features can be simply classified as: handcrafted features and deep learning based features.</p><p>We first introduce hand-crafted feature based method. Based on the idea that specific frequency bands preserve most texture information of real faces, the work in <ref type="bibr" target="#b2">[3]</ref> employed various difference-of-Gaussian filters to select a favorable frequency band for detection. Texture features used in face detection and face recognition tasks can be migrate to face spoofing detection and perform quite well.</p><p>Apart from hand-crafted features, deep learning, in particular, CNN based features achieved great success in recent years. In this category, the CNN learns the discriminative features for liveness detection. The large amount of training data guides the CNN to learn an effective feature. <ref type="bibr" target="#b24">[25]</ref> extracts the local texture features and depth features from the face images and fuses them for face spoofing detection. Furthermore, a LSTM-CNN architecture <ref type="bibr" target="#b25">[26]</ref> was proposed to fuse the predictions of the multiple frames of a video, which was proved to be effective for video face spoofing detection.</p><p>Image Quality Based Methods Methods in this category are motivated by the fact that the photo and replay video are likely to have an image quality degradation in the recapture process. In <ref type="bibr" target="#b0">[1]</ref>, the method exploits to analyze the attack photos in 2D Fourier spectra, showing interesting results. However, the performance might drop for higher-quality image data. Moreover, in <ref type="bibr" target="#b4">[5]</ref>, an image quality based method was proposed by applying chromatic moment feature, specular reflection feature, blurriness feature and color diversity feature.</p><p>Motion Based Methods This type of methods aim to select the physiological reaction motions such as eye blinking, lips movements and the head motions to distinguish the real face from the fake one. In <ref type="bibr" target="#b19">[20]</ref>, different movements in the facial parts were extracted as features for this task. Though physiological sign based methods have shown satisfactory performance to counter printed photo attacks with the user cooperation, they may become less effective for video replay attack. However, <ref type="bibr" target="#b26">[27]</ref> advances a method for facial antispoofing by applying dynamic mode decomposition (DMD), which can conveniently represent the temporal information of the replay video as a single image with the same dimensions as frames in the video. This method based on the motion information is proved less time consuming and is more accurate.</p><p>Reflectance Based Methods The reflectance differences between the real and fake faces, in particular for the print attack and replay attack, can offer important information for face spoofing detection. The reflectance cue from a single image is used to detect the face spoofing <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b21">[22]</ref>. <ref type="bibr" target="#b27">[28]</ref> utilizes the different multi-spectral reflectance distributions to distinguish real and fake faces based on Lambertian model.</p><p>Multi-Feature Fusion Based Methods The fusion of multiple features show improved accuracy compared to individual feature. <ref type="bibr" target="#b28">[29]</ref> proposed a feature fusion with video motion feature and texture feature to distinguish the authenticity of the face. The author obtains the moving image from the face video and the LBP feature from the last frame, fuses them and uses the linear discriminant analysis (LDA) for classification. <ref type="bibr" target="#b8">[9]</ref> extracts the texture features from three multi-scale filtering methods, then the resulting features are concatenated to form the fused feature for classification.</p><p>Other Methods Apart from the aforementioned methods, additional hardwares can also be employed for face spoofing detection. Unlike face images directly captured by camera, 3D depth information <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b31">[32]</ref> and multi-spectrum and infrared (IR) image. <ref type="bibr" target="#b29">[30]</ref> proposed a method for face liveness detection based on 3D projective invariants. In <ref type="bibr" target="#b30">[31]</ref>, the authors proposed to recover sparse 3D shapes for face images to counter the different kinds of photo attacks.</p><p>Summary The methods we introduced can usually achieve promising performance of anti-spoofing on intra-database scenario, however, it is still challenging to achieve strong performance for inter-database scenario. The degraded generalization capacity results from many cross-database factors: different capture devices, different imaging environments, different illuminations, different facial poses, etc. In this work, we propose an anti-spoofing method which is illumination-robust, generalizing well to environments with strong illumination environments and without, achieves promising cross-database performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-Scale Retinex</head><p>Many related researches have been conducted to simulate the human vision system using different luminance algorithms. Land's Retinex theory <ref type="bibr" target="#b32">[33]</ref> proposed the a lightness model named as Retinex theory to measure the lightness reflexion in an image. After that, the Retinex algorithm has been successfully applied to image enhancement <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>. <ref type="bibr" target="#b35">[36]</ref> introduced a model called Single Scale Retinex (SSR), which applied the Gaussian filter to normalize illumination of source image. The work <ref type="bibr" target="#b36">[37]</ref> focused on the filter of the SSR and employed an improved SSR with the guided filter and achieved promising image enhancement performance. The performance of SSR algorithm is highly dependent on the parameter of Gaussian filter. To overcome this limitation, a multi-scale Retinex (MSR) model <ref type="bibr" target="#b22">[23]</ref>, which weights the outputs of several SSRs, is proposed. <ref type="bibr" target="#b37">[38]</ref> proposed a novel MSR based on an adaptive weights to aggregate the SSRs and applied in image contrast enhancement. In our work, we applied MSR because: (1) MSR can separate an image to illumination component and reflectance component, and the illuminationremoved reflectance component is used for liveness detection;</p><p>(2) the MSR algorithm can be regarded as a optimized high pass filter, thus it can effectively preserve the high frequency components which is discriminative between the real and fake faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Feature Fusion</head><p>Existing fusion methods consist of two part: early fusion (feature-level fusion) and late fusion (Score-level fusion). Feature aggregation or subspace learning is actually the early fusion. Aggregation approaches are usually performed by simply element averaging or concatenation <ref type="bibr" target="#b38">[39]</ref>. Subspace learning methods aim to project the concatenated feature to a subspace with the best use of the complementarity of the features. Late fusion is to fuse the predicted scores after computation based on different classifier by averaging <ref type="bibr" target="#b39">[40]</ref> or stacking another classifier result <ref type="bibr" target="#b40">[41]</ref>. For the deep learning task, researchers usually use simple fusion methods for fusing deep features features, such as score fusion, feature averaging, etc. In our work, we proposed an attention based fusion method, aiming to exploit the best use of the features to fuse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Visual Attention Model</head><p>Visual attention is a powerful mechanism that enables perception to focus on important part which offers more information. To combine spatial and temporal information <ref type="bibr" target="#b41">[42]</ref> employed an end-to-end deep neural network. In <ref type="bibr" target="#b42">[43]</ref>, the authors proposed a novel visual attention model to integrate different spatial features including color, orientation and luminance orientation features, which can reflect the region of interests of the human visual system. Different mechanisms of attention have been employed to deal with the computer vision tasks, including action recognition <ref type="bibr" target="#b43">[44]</ref>, emotion recognition <ref type="bibr" target="#b44">[45]</ref>, image classification <ref type="bibr" target="#b45">[46]</ref>. On the whole, the attention model is usually used for aggregating features extracted by different images. Inspired by the great success of attention models, we apply attention model to fuse our features derived from RGB images and MSR images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>Spoofing detection is actually a binary (real vs. fake face) classification problem. In deep learning era, a natural solution of this task is to feed the input RGB images to a carefully designed CNN with classification loss (softmax and cross entropy loss) for end-to-end training. This CNN-based framework has been widely investigated by <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b46">[47]</ref>- <ref type="bibr" target="#b49">[50]</ref>.</p><p>Despite the strong nonlinear feature learning capacity of deep learning, the performance of anti-spoofing degrades when the input images are captured by different devices, under different lighting, etc. In this work, we aim to train a CNN which generalizes better to various environments, mainly various lightings.</p><p>The RGB images are sensitive to illumination variations yet cover very detailed facial texture information. Motivated by extensive research of (single-scale and multi-scale) Retinex image, we find the Retinex (we use Multi-Scale Retinex -MSR in this work) image is invariant to illumination yet loses minor facial texture. Thus, in this work, we propose a In this section, firstly, we introduce the theory of the Retinex to explain the reason why MSR image is discriminative for anti-spoofing. After that, the complementarity of the RGB and MSR features is analyzed and the proposed TSCNN is detailed. Last, we introduce our attention-based feature fusion method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Retinex Theory</head><p>Assumption Retinex theory was first raised by <ref type="bibr">Land and McCann in 1971 [33]</ref>. According to the literal meaning of the word 'Retinex', it is a portmanteau constituted by 'retina' and 'cortex', imitating how the human visual system works. The Retinex theory is based on the assumption that the color of the object is determined by the reflection ability of light of different wavelengths. The color of the object is not affected by the non-uniformity illumination. The theory separates the source image S(x, y) into two parts: the reflectance R(x, y) and the illumination L(x, y). In particular, R(x, y) and L(x, y) contain different components of frequency. R(x, y) focuses on high frequency components, while L(x, y) tends to low frequency components. We formulate Retinex by Eq. (1):</p><formula xml:id="formula_0">S(x, y) = R(x, y) • L(x, y)<label>(1)</label></formula><p>where x and y are image pixel coordinates. Motivation L(x, y) and R(x, y) represent the illumination and reflectance (facial skin texture in our task) components respectively. L(x, y) is determined by the light source, while R(x, y) is determined by the property of the surface of captured objects, i.e face in our application. Illumination is clearly not relevant to most classification tasks including face spoofing detection, thus the separation of illumination and reflectance (texture) is important because the separated reflectance only can be used for illumination-invariant classification. Since Retinex theory aims to conduct this separation, Retinex is used in this work for illumination-invariant face spoofing detection.</p><p>Computation For the convenience of calculation, Eq. ( <ref type="formula" target="#formula_0">1</ref>) is usually transformed into the logarithmic domain:</p><formula xml:id="formula_1">log[S(x, y)] = log[R(x, y)] + log[L(x, y)]<label>(2)</label></formula><p>where log[S(x, y)], log[R(x, y)], and log[L(x, y)] are represented by s(x, y), r(x, y), and l(x, y) for convenience. Since s(x, y) is logarithmic form of the original image, we can calculate the Retinex output r(x, y) by appraising l(x, y). Thus, the performance of the Retinex is determined by the estimation of l(x, y). Selecting the apposite method to estimate l(x, y) is a considerable step for illumination normalization.</p><p>Summarizing the previous work of the Retinex, the illumination image can be generated from the source image using the center/surround Retinex. Single-scale Retinex (SSR) <ref type="bibr" target="#b35">[36]</ref> is a center/surround based Retinex and is formulated as Eq.</p><p>(3):</p><formula xml:id="formula_2">r(x, y) = s(x, y) -log[S(x, y) * F (x, y)]<label>(3)</label></formula><p>where F (x, y) denotes the surround function, and Symbol '*' is the convolution operation. There are several forms of the surround function which depends on the effect of the SSR.</p><p>The work <ref type="bibr" target="#b35">[36]</ref> shows that a Gaussian filter works well for the illumination normalization.</p><formula xml:id="formula_3">G(x, y) = Ke -(x 2 +y 2 )/c (4)</formula><p>where c is the scale parameter of Gaussian surround function.</p><p>The value of c is empirically determined. K is selected to satisfy:</p><formula xml:id="formula_4">F (x, y)dxdy = 1<label>(5)</label></formula><p>Let G(x, y) represent F (x, y), then Eq. ( <ref type="formula" target="#formula_2">3</ref>) can be rewritten as:</p><formula xml:id="formula_5">r(x, y) = s(x, y) -log[S(x, y) * G(x, y)]<label>(6)</label></formula><p>The large illumination discontinuities produce halo effects which are often visible. This limitation expands SSR to a more balanced method, multi-scale retinex (MSR) <ref type="bibr" target="#b22">[23]</ref>, by superposing several outputs of SSRs with small, middle, and large scale parameters at certain weights, shown in Fig. <ref type="figure" target="#fig_1">2 (B)</ref>. Specifically, this is expressed by,</p><formula xml:id="formula_6">r M SR (x, y) = k i=1 w i log[S(x, y)] -log[S(x, y) * G i (x, y)]<label>(7)</label></formula><p>Summary Retinex (MSR in our work) is used for face spoofing detection with two reasons. <ref type="bibr" target="#b0">(1)</ref> The MSR can separate illumination and reflectance. In this work, we use the reflectance images (MSR image) to train a CNN for illumination-invariant face spoofing detection. (2) Since the fake face image is regraded as the recaptured image in many cases, which may lose some high frequency information compared to genuine ones. Thus, high frequency information can work as a discriminative clue for anti-spoofing. MSR algorithm can be viewed as an optimized high pass filter to capture the high frequency information for spoofing detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Two Stream Convolutional Neural Network (TSCNN)</head><p>In this section, we introduce our framework for anti-spoof: TSCNN. Specifically, the original RGB images are converted to MSR images in an off-line way. The two image sources (RGB and MSR) are separately fed to two CNN for end-toend training with cross-entropy binary classification loss. The learned two features (derived from RGB and MSR images) are then learned to fuse using attention mechanism. In the remaining parts of this section, we will detail each component of our framework.</p><p>Complementarity of RGB and MSR Images RGB color space is commonly used for capturing and displaying color images. The advantage of the use of RGB images is clear: RGB images can naturally capture detailed facial texture which is discriminative for spoofing detection. However, the disadvantage of RGB image is that it is very sensitive to illumination variation. The intrinsic reason is that RGB space has high correlation between the three color channels, making it rather difficult to separate the luminance and chrominance information. Because the luminance conditions of face images in real world are different and the separation of luminance (illumination) and chrominance (skin color) is rather difficult, the features learned from RGB space tend to be affected by illumination.</p><p>The MSR algorithm can achieve illumination invariant face image by removing the illumination effects as introduced in SectionIII-A. Thus, the MSR face image preserves the microtexture information of facial skin without the illumination effects. Apart from the illumination-invariant merit of MSR images, MSR images can generate discriminative information for spoofing detection. Specifically, MSR algorithm removes the low frequency components (illumination) from the original image and leaves the high frequency ones (texture details). However, the high frequency information is discriminative for spoof detection because: the real faces have rich facial texture details, while the fake faces, in particular recaptured faces, lose some of such details.</p><p>As analyzed above, RGB and MSR images are complementary because: RGB images contain detailed facial texture yet are sensitive to illuminations; while MSR images contain less detailed texture yet are illumination invariant. In addition, MSR images can keep high frequency information, which is also discriminative for spoofing detection.</p><p>Two-stream Architecture Our method is motivated by the fact that both RGB and MSR features are discriminative for face spoofing detection. It is natural to train CNNs using these two sources of information. In this work, therefore, we proposed a two-stream convolutional neural network (TSCNN) as shown in Fig. <ref type="figure" target="#fig_1">2 (A</ref>). The TSCNN consists of two identical sub-networks with different inputs (RGB and MSR images) and extract the learned features derived from RGB and MSR images following the last convolution layer of the two subnetworks. Given one input image/frame, we use MTCNN <ref type="bibr" target="#b50">[51]</ref> for face and landmark detection. Then the detected faces are aligned using affine transformation. The RGB stream operates on single RGB frames extracted from a video sequence. For the MSR stream, the single RGB frames (processed to gray scale first) are converted to MSR images as shown in Fig. <ref type="figure" target="#fig_1">2-(B</ref>). Then MSR images are fed to the MSR subnetwork for training. Each stream is based on the same network, in this work, we use two successful networks (MobileNet <ref type="bibr" target="#b51">[52]</ref> and ResNet-18 <ref type="bibr" target="#b52">[53]</ref>). To effectively fuse the features from two streams, we propose an attention based fusion block, shown in Fig. <ref type="figure" target="#fig_1">2-(C</ref>), which will be detailed in Section III-C.</p><p>To formulate the TSCNN framework (M ), we introduce a quadruplet M = (E RGB , E M SR , F, C). Here E RGB and E M SR are features extractors for RGB and MSR streams respectively. F is a fusion function and C is the classifier. The feature extractor is a mapping E : I → f that takes an input image (either RGB or MSR) I and outputs a feature f of D-dimension.</p><p>Both the extracted feature f RGB and f M SR must have the same dimension of D to be compatible for early (feature) fusion. In particular, f RGB and f M SR can be obtained via different extractors (CNNs), while the feature dimension should be assured the same.</p><p>The fusion function F aggregates f RGB and f M SR into a fused feature v via F :</p><formula xml:id="formula_7">v = F (f RGB , f M SR )<label>(8)</label></formula><p>The fused feature is then fed into a classifier C. Thus, the TSCNN can be formulated as an optimization problem:</p><formula xml:id="formula_8">min w 1 N N i=1 l[C(F (f RGB , f M SR )), y]<label>(9)</label></formula><p>where l(:, :) is a loss function, N is the number of samples, y is the one-hot encoding label vector. Backbone Deep Networks CNNs have been successfully applied to face anti-spoofing <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b46">[47]</ref>- <ref type="bibr" target="#b48">[49]</ref>. Most existing works trained their CNN models from scratch using the existing face anti-spoofing databases, which are quite small and captured in unitary environments. Since CNNs are data hungry model, small training data might lead to overfitting. To overcome overfitting and improve the performance of many computer vision tasks, model finetuning/pretraining from big image classification database, usually ImageNet <ref type="bibr" target="#b53">[54]</ref>, is an effective way. In this work, we used two backbone networks pretrained on ImageNet, i.e MobileNet <ref type="bibr" target="#b51">[52]</ref> (lighter, less accurate) and ResNet-18 <ref type="bibr" target="#b52">[53]</ref> (heavier, more accurate) for spoofing detection.</p><p>To adapt the MobileNet and ResNet-18 models to our face anti-spoofing problem, we finetuned the pretrained models using the face spoofing database. The 2-class cross-entropy loss, i.e. Eq <ref type="bibr" target="#b9">(10)</ref>, is used for binary classification (real vs fake faces). The output of bottleneck layers of MobileNet (1024D) and ResNet-18 (512D) models work as the features for antispoofing.</p><formula xml:id="formula_9">C = - 1 N N i [y i lnŷ i + (1 -y i )ln(1 -ŷi )] (<label>10</label></formula><formula xml:id="formula_10">)</formula><p>where i is the index of training sample, N is the number of training samples, ŷi is the predict value of the i th sample, y i is the label of the i th sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Attention based Feature fusion</head><p>Feature fusion is important for performance improvement in many computer vision tasks. Improper fusion methods can make the fused feature works worse than individual features. In deep learning era, fusion methods including score averaging, feature concatenation, feature averaging, feature max pooling and feature min pooling are normally used. In our anti-spoofing task, we find these fusion methods cannot explore deeply the interplay of features from different sources, therefore, we propose an attention-based fusion method as shown in Fig. <ref type="figure" target="#fig_1">2-(C</ref>).</p><p>The proposed attention-based fusion methods is actually a general framework which can be used for many deep learning based fusion scenarios, certainly including the fusion of RGB and MSR features. Given a set of features {f i , i = 1, ..., N }, we try to learn a set of weights corresponding to the features {w i , i = 1, ..., N } to generate the aggregated feature v:</p><formula xml:id="formula_11">v = N i=1 w i f i (11)</formula><p>Clearly, the key part of our attention method is to learn the weights {w i } of Eq. <ref type="bibr" target="#b10">(11)</ref>. Note that our method becomes feature average fusion if w i = 1/N , showing the generalization capacity of our method. In our task of spoofing detection, N = 2, and the features to be fused are f RGB and f M SR .</p><p>Apart from learning w i directly, we learn a kernel q which has the same dimensionality of f i . q is used to filter the feature vectors via dot product:</p><formula xml:id="formula_12">d i = q T f i<label>(12)</label></formula><p>The filter generates a vector which represent the significance of the corresponding feature, named d i . To convert the significances to weights w i subject to i w i = 1, we passed d i to a softmax operator and achieve all positive weights w i :</p><formula xml:id="formula_13">w i = e di j e dj<label>(13)</label></formula><p>Obviously, the aggregation result r is unrelated with the quantity of input feature f i . The only parameters to learn is the filter kernel q, which is easy to be trained via standard backpropagation and stochastic gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this Section, we conduct extensive experiments and evaluate our method. We first have a brief introduction of three benchmark databases in Section IV-A. After that, we present the experimental settings of our method in section B so that the other researchers can reproduce our results. The following sections (SectionIV-C to G) present the results on the three databases. In particular, the results on CASIA-FASD are shown with the seven test scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Benchmark Database</head><p>In this subsection, to assess the effectiveness of our proposed anti-spoofing technique, an experimental evaluation on the CASIA Face Anti-Spoofing Database <ref type="bibr" target="#b54">[55]</ref>, the REPLAY-ATTACK database <ref type="bibr" target="#b55">[56]</ref> and the OULU database <ref type="bibr" target="#b56">[57]</ref> is provided. These three datasets consist of real client accesses and different types of attacks, which are captured in different imaging qualities with different cameras. In the following paragraphs, we will have a brief introduction of the databases.</p><p>1) The CASIA Face Anti-Spoofing Database (CASIA FASD): The CASIA Face Anti-Spoofing Database is divided into the training set consisted of 20 subjects and the test set containing 30 individuals(see, Fig. <ref type="figure" target="#fig_2">3</ref>). The fake faces were made by capturing the genuine faces. Three different cameras are used in this database to collect the videos with various imaging qualities: low, normal, and high. In addition, the individuals were asked to blink and not to keep still in the videos to collect abundant frames for detection. Three types of face attacks were designed as follows: 1) Warped Photo Attack: A high resolution (1920 × 1080) image, which is recorded by a Sony NEX-5 camera, was used to print a photo. The attacker simulates the facial motion by warps the photo in a warped photo attack. 2) Cut Photo Attack: The high resolution printed photos are then used for the cut photo attacks. In this scenario, an attacker hides behinds the photo   and exhibits eye-blinking through the holes of the eye region, which was cut off before attack. In addition, the attacker put a intact photo behind the cut photo, putting the eye region overlapping from the holes and moving the intact photo up and down slightly to simulate the blinking of the eyes. 3) Video Attack: In this attack, the high resolution videos are displayed on an iPad and captured by a camera.</p><p>2) REPLAY-ATTACK Database: The REPLAY-ATTACK Database consists of video recordings of real accesses and attack attempts to 50 clients (see, Fig. <ref type="figure" target="#fig_3">4</ref>). There are 1200 videos taken by the webcam on a MacBook with the resolution 320 × 240 under two illumination conditions: 1) controlled condition with a uniform background and light supplied by a fluorescent lamp, 2) adverse condition with non-uniform background and the day-light. For performance evaluation, the data set is divided into three subsets of training (360 videos), development (360 videos), and testing (480 videos). To generate the fake faces, a high resolution videos were taken for each person using a Canon PowerShot camera and an iPhone 3GS camera, under the same illumination conditions. Three types of attacks were designed: (1) Print Attacks: High resolution pictures were printed on A4 paper and recaptured by cameras; (2) Mobile Attacks: High resolution pictures and videos were displayed on the screen of an iPhone 3GS and recaptured by cameras; (3) High Definition Attacks: the pictures and the videos were displayed on the screen of an iPad with resolution of 1024 × 168.</p><p>3) OULU-NPU Database: OULU-NPU face presentation attack database consists of 4950 real access and attack videos that were recorded using front facing cameras of six different mobile phones (see, Fig. <ref type="figure" target="#fig_4">5</ref>). The real videos and attack materials were collected in three sessions with different illumination condition. The attack types considered in the OULU-NPU database are print and video-replay. These attacks were created using two printers (Printer 1 and 2) and two display devices (Display 1 and 2). The videos of the real accesses and attacks, corresponding to the 55 subjects, are divided into three subjectdisjoint subsets for training, development and testing with 20, 15 and 20 users, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Settings</head><p>In our experiments, we followed the protocols associated with each of the three databases which allows a fair comparison with other methods proposed in the state of art. For CASIA FASD, the model parameters are trained and tuned using the training set and the results are reported in terms of Equal Error Rate (EER) on the test set. Since the REPLAY-ATTACK database provides a validation set, the results are given in terms of EER on the validation set and the Half Total Error Rate (HTER) on the test set following the official test protocol. EER is achieved at the point where the false rejection rate (FRR) is equal to false acceptance rate (FAR). To compute HTER, we first compute EER and the corresponding threshold on the validation set. Then HTER can be calculated via the threshold on the test set.</p><p>Following <ref type="bibr" target="#b57">[58]</ref>, we evaluate our method on OULU-NPU database with two metrics: Attack Presentation Classification Error Rate (APCER) (Eq. ( <ref type="formula" target="#formula_14">14</ref>)) and Bona Fide Presentation Classification Error Rate (BPCER) (Eq. ( <ref type="formula" target="#formula_15">15</ref>)).</p><formula xml:id="formula_14">AP CER P AI = 1 N P AI N P AI i=1 (1 -R esi )<label>(14</label></formula><p>)</p><formula xml:id="formula_15">BP CER = N BF i=1 R esi N BF<label>(15)</label></formula><p>where, N P AI is the number of the attack presentations for the certain presentation attack instruments (PAI), N BF is the total number of the bona fide presentations. If the prediction of ith presentation is attack, R esi gets the value "1", while the prediction is bona fide, the value of R esi is "0". These two metrics correspond to the False Acceptance Rate (FAR) and False Rejection Rate (FRR) commonly used in the PAD related literature <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>. In addition, we apply the average of the APCER and the BPCER, called Average Classification Error Rate (ACER), to measure the overall performances.</p><p>For the operational systems, the metrics we used (EER, HTER, APCER and BPCER) cannot quantify verification performance. Following the Face Recognition Vendor Test (FRVT) and the common metrics of face recognition, the Receiver Operating Characteristic (ROC) is used to measure the performance of liveness detection. To clearly visualize the TPR@FAR=0.1 and TPR@FAR=0.01 in the figures, the logarithmic coordinates are used for the X-axis of the ROC curves.</p><p>To be consistent with many previous works, the preprocessing steps are needed, consisted of frame sampling and face alignment. Since these three databases consist of videos, we extract the frames from each video. After that, the MTCNN <ref type="bibr" target="#b50">[51]</ref> is used for face detection and landmark detection. Then the detected faces are aligned to size of 128 × 128. For every aligned face, we conduct data augmentation including horizontal flipping, random rotation (0-20 degree), and random crop (114 × 114).</p><p>For each database, we used the training set to fine-tune the MobileNet and ResNet-18 model with cross-entropy loss and the testing set and validation set are used to evaluate the performance.</p><p>For the learning parameter setting, we set the momentum as 0.9 and the learning rate as 0.0001 for training the network. It is observed that the network training converges after 50 epochs with the batch size 128 during the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results of CASIA-FASD</head><p>The CASIA-FASD is split into the training set comprised of 20 subjects and the test set containing 30 individuals. For each of the seven attacking scenarios, the data should then be selected from the corresponding training and test sets for model training and evaluation.</p><p>Different color spaces might lead to different performance of anti-spoofing <ref type="bibr" target="#b47">[48]</ref>, though RGB color is the most widely used. To explore the effect of color space, we conduct experiments and compare the performance of three color spaces: RGB, HSV and YCbCr. All the training settings of 3 color space keep the same. Specifically, the original input images/frames in database are converted to MSR images. Then the images of different color spaces are fed to our TSCNN respectively. The spoofing detection results (EER, the lower the better) based on MobileNet and ResNet-18 are reported in Table <ref type="table" target="#tab_0">I</ref>. The ROC curves are shown in Fig. <ref type="figure" target="#fig_5">6-(a)</ref> and the attention Fusion results in terms of TPR@FAR=0.1 and TPR@FAR=0.01 are presented in Table <ref type="table" target="#tab_6">VII</ref>.</p><p>Results: (1) From results on seven scenarios, RGB and YCbCr generally outperform HSV color space using both ResNet-18 and MobileNet. And the results of RGB and YCbCr are quite similar.</p><p>(2) We can see that RGB, HSV and YCbCr features all work better than MSR features for both MobileNet (4.931%, 5.134% and 5.091% vs. 9.531%) and ResNet-18 (3.437%, 4.831% and 3.635 vs. 7.883%).</p><p>(3) The fusion of MSR and RGB features works better than MSR and HSV, MSR and YCbCr for both MobileNet (4.175% VS 5.061% and 4.339%) and ResNet-18 (3.145% VS 4.661% and 4.761%). ( <ref type="formula">4</ref>) The fusion of MSR and RGB features works better than individual one for MobileNet (fusion: 4.175% vs RGB: 4.931% and MSR: 9.513%). The same conclusion can be drawn for ResNet-18 fusion. As for the reason why RGB is better than HSV and YCbCr, we believe that the MSR plays a role of reducing the impact of illuminations, while the RGB tries to preserve the detailed facial textures. However, HSV and YCbCr are based on the separation of the luminance and the chrominance, which are not effective for the fusion with MSR. It verifies the complementarity of RGB and MSR images.</p><p>(4) From the Table <ref type="table" target="#tab_6">VII</ref>, not surprisingly, the overall results of CASIA-FASD with ResNet (99.71% and 85.33%) are better than that with MobileNet (98.95% and 82.51%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results of REPLAY-ATTACK and OULU-NPU</head><p>REPLAY-ATTACK and OULU-NPU are divided into three subsets: training, test and development. The training set is used to train a classifier or feature extractor while the development set is typically employed to adjust parameters of the classifier. The test set is used for result evaluation. In this experiment, we follow the experimental settings of CASIA-FASD and use MobileNet and ResNet-18 for evaluation.</p><p>From Table <ref type="table" target="#tab_1">II</ref> and Fig. <ref type="figure" target="#fig_5">6</ref>-b, we can see the fusion of MSR and RGB works better than individual ones in terms of EER (fusion: 0.131% vs RGB: 0.384% and MSR: 7.365%) and HTER (fusion: 0.254% vs RGB: 1.561% and MSR: 8.584%) on REPLAY-ATTACK database using MobileNet. The same conclusion can be found for ResNet-18. From Table VII, the overall results of REPLAY-ATTACK using MobileNet (99.42% and 99.13%) are better than that with ResNet-18 (99.21% and 98.59%). In addition, we further fuse the fused MobileNet features (RGB+MSR) and fused ResNet-18 features (RGB+MSR). Because feature dimensionality of original MobileNet (1024D) and ResNet-18 (512D) is different, we change the bottleneck layer of the MobileNet to be of 512D to conduct our attention-based fusion. From Table <ref type="table" target="#tab_1">II</ref>, we can see this further fusion works better than ResNet fusion, but slightly worse than the MobileNet fusion.</p><p>To further verify the effectiveness of the fusion of RGB and MSR on illumination variations, we conduct the experiment on REPLAY-ATTACK database which contains two illumination conditions: 1) controlled condition with a uniform background and light supplied by a fluorescent lamp, 2) adverse condition with non-uniform background and the day-light. To discuss  For the OULU-NPU database, we follow <ref type="bibr" target="#b57">[58]</ref> to use four metrics: we present EER in development set and APCER, BPCER and ACER in test set.</p><p>Table <ref type="table" target="#tab_2">IV</ref> and Table <ref type="table" target="#tab_6">VII</ref> shows the results of RGB, MSR   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Attention based fusion results</head><p>As mentioned above, RGB feature is mainly focusing on micro-texture of facial skin on the all frequencies together, while the MSR feature is focusing on the high frequencies which reduces the influence of illumination. Table <ref type="table" target="#tab_0">I</ref>  First, we some qualitative results via visualization. Compared with average feature fusion which weights different features equally, attention fusion has the flexibility to adaptively weight the features in an asymmetry way. Therefore, our attention-based fusion has the potential to obtain the better weights leading to better performance. Fig. <ref type="figure" target="#fig_7">8-(A)</ref> shows this asymmetry weighting mechanism of our attention-based fusion method. The samples in Fig. <ref type="figure" target="#fig_7">8-(A</ref>) are selected from REPLAY-ATTACK database which covers two imaging lightness conditions: adverse illumination (uneven, complicated lightings), controlled illumination (even, neutral lightings). From the samples in Fig. <ref type="figure" target="#fig_7">8</ref>, we can see the weights for MSR and RGB are adaptively asymmetry. Under adverse (uneven, complicated lightings) illumination, the weights of MSR images are higher than those of RGB ones because MSR images are more illumination-invariant than RGB ones. Under controlled illumination, unsurprisingly, the RGB images gain higher weights. Fig. <ref type="figure" target="#fig_7">8</ref> (B) shows some samples under different illuminations with three scores (RGB, MSR, the fusion of them). We can see some samples failed with individual RGB or MSR scores, but the fusion results lead to correct recognition, showing the effectiveness of the fusion of RGB and MSR, in particular, under various illuminations.</p><p>Second, we show some qualitative results. Specifically, we compare the proposed attention-based fusion methods with some popular feature fusion methods including score averaging, feature concatenation, feature averaging, feature max pooling, feature min pooling and the proposed attention method. The fusion results are presented separately for different databases.</p><p>Table <ref type="table" target="#tab_4">V</ref> shows the results of CASIA-FASD with the seven scenarios. In addition, Fig. <ref type="figure" target="#fig_6">7-(b)</ref> shows the ROC curves of the popular feature fusion methods using MobileNet. The proposed attention based fusion method achieves the lowest EER across all other scenarios ('Overall') 4.175% (MobileNet) and 3.145% (ResNet-18), showing that the superiority of the our fusion methods against others. For MoblieNet and ResNet-18, the 2nd and 3rd best performed fusion methods are {'Feature Min' and 'Score Average'} and {'Score Average' and 'Concatenated Features'}, respectively.</p><p>Table <ref type="table" target="#tab_2">IV</ref>-E shows the fusion results on REPLAY-ATTACK and OULU-NPU. We can see that our attention-based fusion works consistently better than all other fusion methods on both REPLAY-ATTACK (EER and HTER) and OULU-NPU (EER). The promising performance results from the fact that attention-based fusion can adaptively weight the RGB and MSR features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Comparisons with State-of-the-art</head><p>Table <ref type="table" target="#tab_7">VIII</ref> presents the comparisons of our approach with the state-of-the-art methods for face spoofing detection. In general, the proposed algorithm outperforms many competitors, demonstrating the effectiveness of our method by fusing RGB feature and MSR feature with attention model.</p><p>For REPLAY-ATTACK database, the proposed method achieves the best (MobileNet+Attention) and 2nd best (ResNet-18+Attention) performance in terms of EER, showing the effectiveness of the fusion of two clues (RGB and MSR). In terms of HTER, our method (MobileNet+Attention) achieves the 2nd best performance, slightly lower than Bottleneck feature fusion + NN <ref type="bibr" target="#b49">[50]</ref>. However, our method greatly outperforms <ref type="bibr" target="#b49">[50]</ref> in terms of EER.</p><p>For CASIA-FASD database, it can be seen in Table VIII that we also achieve the best (ResNet-18 + Attention) and 2nd best (MobileNet + Attention) performance in terms of EER.</p><p>For OULU-NPU database, as shown in Table <ref type="table" target="#tab_9">IX</ref>, we can achieve 2nd best performance for most results under the four protocols, while the method of <ref type="bibr" target="#b62">[63]</ref> works best, which uses the additional information of 3D depth shape and rPPG (The rPPG signal provides temporal information about face liveness,  is related to the intensity changes of facial skin over time).</p><p>To summarize, our method can achieve very strong performance across all the three benchmark databases, showing the merits of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Cross-Database Comparisons</head><p>The spoofing faces of different databases are captured using different devices under different environments (e.g. lightings). Therefore, it is interesting to evaluate our strategy in a crossdatabase protocol to verify its generalization capacity.We conducted a cross-database evaluation between CASIA-FASD and REPLAY-ATTACK. To be more specific, cross-database is to train and tune the classifier on one database and test on another database. The generalization ability of the system in this case is manifested by the HTER obtained on the validation and test sets. The countermeasure was trained and tuned with CASIA-FASD or REPLAY-ATTACK each time, and then tested on the other databases. The results are reported in Table IV-F compared with the state-of-the-art techniques in this crossdatabase manner.</p><p>Due to the domain shift (different imaging environments) between databases, the performaence of all the anti-spoofing methods drops. Compared with the state-of-the-art methods,    <ref type="bibr" target="#b59">[60]</ref> 50.2% 47.9% LBP <ref type="bibr" target="#b55">[56]</ref> 55.9% 57.6% LBP-TOP <ref type="bibr" target="#b60">[61]</ref> 49.7% 60.6% Motion-Mag <ref type="bibr" target="#b63">[64]</ref> 50.1% 47.0% Spectral cubes <ref type="bibr" target="#b21">[22]</ref> 34.4% 45.5% CNN <ref type="bibr" target="#b13">[14]</ref> 48.5% 39.6% Color-LBP <ref type="bibr" target="#b9">[10]</ref> 47.0% 39.6% Colour Texture <ref type="bibr" target="#b7">[8]</ref> 30.3% 37.7% Depth + rPPG <ref type="bibr" target="#b62">[63]</ref> 27.6% 28.4% Deep-Learning <ref type="bibr" target="#b12">[13]</ref> 48.2% 45.4% KSA <ref type="bibr" target="#b64">[65]</ref> 33.1% 32.1% Frame difference <ref type="bibr" target="#b65">[66]</ref> 50.25%    our method (MobileNet + Attention) achieves the 2nd best performance (30.0% and 33.4%), slightly worse than the best one <ref type="bibr" target="#b62">[63]</ref> (27.6% and 28.4%). However, <ref type="bibr" target="#b62">[63]</ref> uses more auxiliary information (3D face shape, rPPG signals) than our method.</p><p>To explore the reasons of performance drop in the crossdatabase evaluation, we consider the standard distribution distance metric, maximum mean discrepancy (MMD) <ref type="bibr" target="#b66">[67]</ref> to measure the distance domain shift between the source feature and target feature distributions.</p><formula xml:id="formula_16">M M D(F T , F V ) = 1 |F T | ft∈F T φ(f t ) - 1 |F V | fv∈F V φ(f v )<label>(16)</label></formula><p>As shown in the equation above, we define a representation φ(), which operates on train data features, f t ∈ F T and validate data features, f v ∈ F V . The larger the value of MMD, the bigger the domain shift.</p><p>From the result of Table XI XII XIII, we can see that: (1) When we train and test on the same database, the MMD is smaller than that train and test on different databases for both MobileNet and ResNet-18.</p><p>(2) Since the CASIA-FASD has seven scenarios, when we train on the CASIA-FASD database and test on the REPLAY-ATTACK database, the MMD is bigger than that we train on the REPLAY-ATTACK and test on the CASIA-FASD database.</p><p>(3) The fusion of RGB and MSR features reduced the MMD of the cross-database compared with individual one for both MobileNet and ResNet-18.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we proposed an attention-based two stream convolutional networks for face spoofing detection to distinguish real and fake faces. The proposed approach applies the complementary features (RGB and MSR) extracted via CNN models (MobileNet and ResNet-18) and then employs the attention based fusion method to fuse these two features. The adaptively weighted features contain more discriminative information under various lighting conditions.</p><p>We evaluated our approaches of face spoofing on three challenging databases, i.e. CASIA-FASD, REPLAY-ATTACK and OULU-NPU, which indicated the competitive performance in both intra-database and inter-database. The experiments of fusion methods show that the attention model can achieve promising results on feature fusion. The cross-database evaluations show the effectiveness of the fusion of RGB and MSR information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Motivation of the fusion of RGB (Col 1) and MSR (Col 3) images. The individual feature scores of RGB (Col 2) and MSR (Col 4) and fused scores (Col 5) are shown. The fused scores are improved compared with individual scores.</figDesc><graphic coords="2,48.96,56.07,251.99,112.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (A) is the overall pipeline; In (B), every single block represents one SSR module. The outputs of all SSR modules are weighted with scale parameters to form MSR; (C) illustrates the work flow of attention-based fusion.</figDesc><graphic coords="4,54.00,56.07,503.99,263.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Sample from the CASIA FASD. From top to bottom: low, normal and high quality images. From the left to the right: real faces and warped photo, cut photo and video replay attacks.</figDesc><graphic coords="7,66.49,56.07,216.00,165.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Samples from the REPLAY-ATTACK database. The first row presents images taken from the controlled scenario, while the second row corresponds to the images from the adverse scenario. From the left to the right: real faces and high definition, mobile and print attacks.</figDesc><graphic coords="7,66.49,274.62,216.00,64.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Samples from the OULU-NPU database. From top to bottom is the three sessions with different acquisition conditions. From the left to the right: real faces, print attack 1, print attack 2, video attack 1 and video attack 2.</figDesc><graphic coords="7,66.49,401.86,216.00,226.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. ROC curves on REPLAY-ATTACK and CASIA-FASD databases. (a) ROC curves on CASIA-FASD with ResNet under different color spaces and MSR. (b) ROC curves on REPLAY-ATTACK with MobileNet with LBP and CNNs.</figDesc><graphic coords="9,90.00,262.38,432.00,337.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. ROC curves on on REPLAY-ATTACK and CASIA-FASD databases. (a) ROC curves on REPLAY-ATTACK with MobileNet under different illuminations. (b) ROC curves on CASIA-FASD with ResNet under different fusion methods.</figDesc><graphic coords="10,90.00,56.07,432.00,163.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Results on REPLAY-ATTACK database. (A) Attention fusion weights (numbers in the boxes) showing the importance of RGB and MSR. Samples cover 2 imaging lightness conditions: adverse illumination (Row 1 and 2) and controlled illumination (Row 3 and 4). (B) Three prediction scores: RGB, MSR and the fusion of them (numbers in the boxes). The red and green boxes indicate the wrong and correct predictions respectively.</figDesc><graphic coords="12,54.00,56.06,504.01,325.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I EER</head><label>I</label><figDesc>(%) OF THREE COLOR SPACES AND MSR FEATURES ON CASIA-FASD DATABASE IN SEVEN SCENARIOS</figDesc><table><row><cell></cell><cell>Attack Scenarios</cell><cell>Low</cell><cell>Normal</cell><cell>High</cell><cell>Warped</cell><cell>Cut</cell><cell>Video</cell><cell>Overall</cell></row><row><cell></cell><cell>RGB</cell><cell>15.301</cell><cell>8.996</cell><cell>6.412</cell><cell>8.551</cell><cell>6.011</cell><cell>5.661</cell><cell>7.802</cell></row><row><cell>LBP</cell><cell>MSR</cell><cell>10.690</cell><cell>10.302</cell><cell>5.331</cell><cell>7.609</cell><cell>8.091</cell><cell>8.701</cell><cell>9.003</cell></row><row><cell></cell><cell>RGB+MSR Fusion</cell><cell>8.996</cell><cell>9.330</cell><cell>5.981</cell><cell>7.604</cell><cell>6.771</cell><cell>4.390</cell><cell>7.408</cell></row><row><cell></cell><cell>RGB</cell><cell>10.610</cell><cell>4.606</cell><cell>5.260</cell><cell>5.934</cell><cell>3.978</cell><cell>3.846</cell><cell>4.931</cell></row><row><cell></cell><cell>HSV</cell><cell>8.714</cell><cell>5.884</cell><cell>6.995</cell><cell>3.723</cell><cell>4.709</cell><cell>4.682</cell><cell>5.143</cell></row><row><cell></cell><cell>YCbCr</cell><cell>8.441</cell><cell>4.993</cell><cell>4.519</cell><cell>6.410</cell><cell>5.792</cell><cell>3.904</cell><cell>5.091</cell></row><row><cell>MobileNet</cell><cell>MSR</cell><cell>7.056</cell><cell>8.129</cell><cell>5.818</cell><cell>9.828</cell><cell>5.126</cell><cell>9.833</cell><cell>9.531</cell></row><row><cell></cell><cell>RGB+MSR Fusion</cell><cell>6.745</cell><cell>4.068</cell><cell>3.258</cell><cell>5.258</cell><cell>2.453</cell><cell>2.647</cell><cell>4.175</cell></row><row><cell></cell><cell>HSV+MSR Fusion</cell><cell>7.633</cell><cell>4.982</cell><cell>5.601</cell><cell>4.679</cell><cell>4.510</cell><cell>4.511</cell><cell>5.061</cell></row><row><cell></cell><cell>YCbCr+MSR Fusion</cell><cell>7.003</cell><cell>5.120</cell><cell>3.227</cell><cell>4.031</cell><cell>6.001</cell><cell>3.799</cell><cell>4.339</cell></row><row><cell></cell><cell>RGB</cell><cell>4.021</cell><cell>5.851</cell><cell>1.703</cell><cell>5.019</cell><cell>1.941</cell><cell>2.679</cell><cell>3.437</cell></row><row><cell></cell><cell>HSV</cell><cell>6.341</cell><cell>2.291</cell><cell>5.815</cell><cell>3.459</cell><cell>2.992</cell><cell>4.578</cell><cell>4.831</cell></row><row><cell></cell><cell>YCbCr</cell><cell>7.441</cell><cell>2.185</cell><cell>1.713</cell><cell>4.249</cell><cell>3.329</cell><cell>3.716</cell><cell>3.635</cell></row><row><cell>ResNet</cell><cell>MSR</cell><cell>6.793</cell><cell>6.270</cell><cell>10.098</cell><cell>7.665</cell><cell>5.087</cell><cell>9.531</cell><cell>7.883</cell></row><row><cell></cell><cell>RGB+MSR Fusion</cell><cell>3.545</cell><cell>2.170</cell><cell>2.785</cell><cell>4.419</cell><cell>2.572</cell><cell>4.931</cell><cell>3.145</cell></row><row><cell></cell><cell>HSV+MSR Fusion</cell><cell>5.319</cell><cell>2.907</cell><cell>4.886</cell><cell>3.299</cell><cell>2.555</cell><cell>4.931</cell><cell>4.661</cell></row><row><cell></cell><cell>YCbCr+MSR Fusion</cell><cell>6.178</cell><cell>3.099</cell><cell>4.690</cell><cell>4.003</cell><cell>3.133</cell><cell>3.999</cell><cell>4.761</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II EER</head><label>II</label><figDesc>(%) AND HTER (%) OF RGB AND MSR FEATURES ON REPLAY-ATTACK DATABASEThe consistent improvement of feature fusion shows the effectiveness of the use of two information sources: RGB and MSR. As shown in TableIIand Table IV, the popular networks (MobileNet and ResNet-18) achieve competitive performances on REPLAY-ATTACK and OULU-NPU database .</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell cols="3">REPLAY-ATTACK EER HTER</cell></row><row><cell>LBP RGB</cell><cell></cell><cell></cell><cell>3.990</cell><cell cols="2">4.788</cell></row><row><cell>LBP MSR</cell><cell></cell><cell></cell><cell>4.701</cell><cell cols="2">5.060</cell></row><row><cell>MobileNet RGB</cell><cell></cell><cell></cell><cell>0.384</cell><cell cols="2">1.561</cell></row><row><cell>ResNet RGB</cell><cell></cell><cell></cell><cell>0.628</cell><cell cols="2">2.038</cell></row><row><cell>MobileNet MSR</cell><cell></cell><cell></cell><cell>7.365</cell><cell cols="2">8.584</cell></row><row><cell>ResNet MSR</cell><cell></cell><cell></cell><cell>8.350</cell><cell cols="2">9.576</cell></row><row><cell cols="2">LBP Attention Fusion</cell><cell></cell><cell>3.491</cell><cell cols="2">4.903</cell></row><row><cell cols="2">MobileNet Attention Fusion</cell><cell></cell><cell>0.131</cell><cell cols="2">0.254</cell></row><row><cell cols="2">ResNet Attention Fusion</cell><cell></cell><cell>0.210</cell><cell cols="2">0.389</cell></row><row><cell cols="3">ResNet + MobileNet Attention Fusion</cell><cell>0.177</cell><cell cols="2">0.293</cell></row><row><cell cols="2">TABLE III</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">EER (%) AND HTER (%) OF RGB AND MSR FEATURES ON ADVERSE</cell></row><row><cell cols="6">ILLUMINATION AND CONTROLLED ILLUMINATION IN REPLAY-ATTACK</cell></row><row><cell cols="2">DATABASE</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Adverse</cell><cell></cell><cell cols="2">Controlled</cell></row><row><cell>Method</cell><cell cols="3">illumination</cell><cell cols="2">illumination</cell></row><row><cell></cell><cell>EER</cell><cell cols="2">HTER</cell><cell>EER</cell><cell>HTER</cell></row><row><cell>MobileNet RGB</cell><cell>0.451</cell><cell cols="2">1.971</cell><cell>0.140</cell><cell>1.107</cell></row><row><cell>ResNet RGB</cell><cell>0.705</cell><cell cols="2">2.444</cell><cell>0.411</cell><cell>1.677</cell></row><row><cell>MobileNet MSR</cell><cell>7.660</cell><cell cols="2">8.621</cell><cell>6.138</cell><cell>7.218</cell></row><row><cell>ResNet MSR</cell><cell>8.720</cell><cell cols="2">9.031</cell><cell>7.993</cell><cell>8.930</cell></row><row><cell>MobileNet Attention Fusion</cell><cell>0.165</cell><cell cols="2">1.299</cell><cell>0.093</cell><cell>0.097</cell></row><row><cell>ResNet Attention Fusion</cell><cell>0.285</cell><cell cols="2">1.433</cell><cell>0.169</cell><cell>1.310</cell></row><row><cell cols="6">and fusion feature based on MobileNet and ResNet-18. In</cell></row><row><cell cols="6">terms of ACER and EER, we can see the fusion of RGB and</cell></row><row><cell cols="6">MSR performs better than individual ones. For most results in</cell></row><row><cell cols="6">four protocols, the fusion of features significantly outperforms</cell></row><row><cell>individual features.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV EER</head><label>IV</label><figDesc>(%), APCER (%) , BPCER (%) AND ACER (%) OF RGB AND MSR FEATURES ON OULU-NPU DATABASE</figDesc><table><row><cell>Prot.</cell><cell>Methods</cell><cell cols="4">Dev EER(%) APCER(%) BPCER(%) ACER(%) Test</cell></row><row><cell></cell><cell>MobileNet RGB</cell><cell>6.1</cell><cell>9.6</cell><cell>6.2</cell><cell>7.9</cell></row><row><cell></cell><cell>ResNet RGB</cell><cell>2.3</cell><cell>3.5</cell><cell>8.7</cell><cell>6.1</cell></row><row><cell>1</cell><cell>MobileNet MSR ResNet MSR</cell><cell>10.5 5.7</cell><cell>10.6 7.5</cell><cell>9.4 9.3</cell><cell>10.0 8.4</cell></row><row><cell></cell><cell>MobileNet Attention Fusion</cell><cell>5.2</cell><cell>3.9</cell><cell>9.5</cell><cell>6.7</cell></row><row><cell></cell><cell>ResNet Attention Fusion</cell><cell>2.1</cell><cell>5.1</cell><cell>6.7</cell><cell>5.9</cell></row><row><cell></cell><cell>MobileNet RGB</cell><cell>5.7</cell><cell>6.5</cell><cell>10.7</cell><cell>8.6</cell></row><row><cell></cell><cell>ResNet RGB</cell><cell>2.7</cell><cell>3.7</cell><cell>8.1</cell><cell>5.9</cell></row><row><cell>2</cell><cell>MobileNet MSR ResNet MSR</cell><cell>9.6 4.3</cell><cell>8.9 3.8</cell><cell>9.9 11.6</cell><cell>9.4 7.8</cell></row><row><cell></cell><cell>MobileNet Attention Fusion</cell><cell>5.1</cell><cell>3.6</cell><cell>9.0</cell><cell>6.3</cell></row><row><cell></cell><cell>ResNet Attention Fusion</cell><cell>2.0</cell><cell>7.6</cell><cell>2.2</cell><cell>4.9</cell></row><row><cell></cell><cell cols="3">MobileNet RGB 5.3±0.5 3.5±1.8</cell><cell>9.3±2.6</cell><cell>6.4±3.7</cell></row><row><cell></cell><cell>ResNet RGB</cell><cell cols="2">2.7±0.8 9.3±0.8</cell><cell>5.7±1.2</cell><cell>7.2±2.6</cell></row><row><cell>3</cell><cell cols="5">MobileNet MSR 10.8±1.2 6.9±2.5 12.3±0.9 9.7±1.9 ResNet MSR 4.6±0.8 8.3±1.9 9.4±1.8 8.7±2.1</cell></row><row><cell></cell><cell>MobileNet Attention Fusion</cell><cell cols="2">5.1±0.3 8.7±4.5</cell><cell>5.3±2.3</cell><cell>6.3±2.2</cell></row><row><cell></cell><cell>ResNet Attention Fusion</cell><cell cols="2">1.9±0.4 3.9±2.8</cell><cell>7.3±1.1</cell><cell>5.6±1.6</cell></row><row><cell></cell><cell cols="5">MobileNet RGB 6.3±0.4 12.3±7.5 9.7±2.6 10.3±3.1</cell></row><row><cell></cell><cell>ResNet RGB</cell><cell cols="4">2.6±0.5 17.9±9.1 10.1±5.5 14.9±6.4</cell></row><row><cell>4</cell><cell cols="5">MobileNet MSR 11.8±1.8 24.7±10.5 21.3±12.8 22.0±11.6 ResNet MSR 6.6±0.7 19.6±9.1 16.2±8.8 17.1±8.1</cell></row><row><cell></cell><cell>MobileNet Attention Fusion</cell><cell cols="4">6.1±0.7 10.9±4.6 12.7±5.1 11.3±3.9</cell></row><row><cell></cell><cell>ResNet Attention Fusion</cell><cell cols="3">2.3±0.3 11.3±3.9 9.7±4.8</cell><cell>9.8±4.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>, TableIIand Table IV have verified the effectiveness of the fusion of these two features (RGB and MSR). In this section, we further explore this effectiveness.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V EER</head><label>V</label><figDesc>(%) OF DIFFERENT FUSION METHODS ON CASIA-FASD DATABASES IN SEVEN SCENARIOS</figDesc><table><row><cell></cell><cell>Attack Scenarios</cell><cell>Low</cell><cell>Normal</cell><cell>High</cell><cell>Warped</cell><cell>Cut</cell><cell>Video</cell><cell>Overall</cell></row><row><cell></cell><cell>Concatenated Features</cell><cell>7.808</cell><cell>3.473</cell><cell>5.957</cell><cell>5.364</cell><cell>3.267</cell><cell>4.479</cell><cell>5.191</cell></row><row><cell></cell><cell>Score Average</cell><cell>10.611</cell><cell>4.612</cell><cell>5.312</cell><cell>5.934</cell><cell>3.971</cell><cell>3.877</cell><cell>4.953</cell></row><row><cell>MobileNet</cell><cell>Feature Average Feature Max</cell><cell>8.086 8.048</cell><cell>3.311 3.410</cell><cell>5.819 6.017</cell><cell>5.253 5.347</cell><cell>3.333 3.321</cell><cell>4.278 4.529</cell><cell>5.108 5.201</cell></row><row><cell></cell><cell>Feature Min</cell><cell>7.820</cell><cell>3.458</cell><cell>5.380</cell><cell>5.149</cell><cell>3.267</cell><cell>4.064</cell><cell>4.887</cell></row><row><cell></cell><cell>Attention Fusion</cell><cell>6.745</cell><cell>4.068</cell><cell>3.258</cell><cell>5.258</cell><cell>2.453</cell><cell>2.647</cell><cell>4.175</cell></row><row><cell></cell><cell>Concatenated Features</cell><cell>5.568</cell><cell>3.099</cell><cell>4.302</cell><cell>4.092</cell><cell>2.516</cell><cell>3.143</cell><cell>3.380</cell></row><row><cell></cell><cell>Score Average</cell><cell>5.902</cell><cell>2.969</cell><cell>3.830</cell><cell>4.202</cell><cell>2.658</cell><cell>3.224</cell><cell>3.332</cell></row><row><cell>ResNet</cell><cell>Feature Average Feature Max</cell><cell>6.242 5.846</cell><cell>3.291 4.039</cell><cell>4.689 4.536</cell><cell>3.935 4.331</cell><cell>2.929 3.091</cell><cell>3.956 4.198</cell><cell>3.895 4.189</cell></row><row><cell></cell><cell>Feature Min</cell><cell>7.244</cell><cell>2.825</cell><cell>4.941</cell><cell>4.280</cell><cell>3.030</cell><cell>3.984</cell><cell>4.157</cell></row><row><cell></cell><cell>Attention Fusion</cell><cell>3.545</cell><cell>2.170</cell><cell>2.785</cell><cell>4.419</cell><cell>2.572</cell><cell>4.931</cell><cell>3.145</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII TPR</head><label>VII</label><figDesc>@FAR=0.1 AND TPR@FAR=0.01 OF THE ATTENTION FUSION RESULTS ON CASIA-FASD, REPLAY-ATTACK AND OULU-NPU DATABASES</figDesc><table><row><cell>Database</cell><cell>Methods</cell><cell>Protocol</cell><cell>TPR@FAR=0.1</cell><cell>TPR@FAR=0.01</cell></row><row><cell>CASIA-FASD</cell><cell>ResNet Attention Fusion MobileNet Attention Fusion</cell><cell>overall overall</cell><cell>99.71% 98.95%</cell><cell>85.33% 82.51%</cell></row><row><cell>REPLAY-ATTACK</cell><cell>ResNet Attention Fusion MobileNet Attention Fusion</cell><cell>overall overall</cell><cell>99.21% 99.42%</cell><cell>98.59% 99.13%</cell></row><row><cell></cell><cell></cell><cell>Prot.1</cell><cell>94.15%</cell><cell>83.44%</cell></row><row><cell></cell><cell>ResNet Attention Fusion</cell><cell>Prot.2 Prot.3</cell><cell>95.11% 93.59%±0.5%</cell><cell>86.78% 84.39%±0.4%</cell></row><row><cell>OULU-NPU</cell><cell></cell><cell>Prot.4 Prot.1</cell><cell>93.09%±0.4% 98.94%</cell><cell>83.69%±0.5% 96.74%</cell></row><row><cell></cell><cell>MobileNet Attention Fusion</cell><cell>Prot.2 Prot.3</cell><cell>99.10% 98.41%±0.6%</cell><cell>96.86% 96.04%±0.5%</cell></row><row><cell></cell><cell></cell><cell>Prot.4</cell><cell>97.83%±0.4%</cell><cell>95.22%±0.6%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VIII COMPARISON</head><label>VIII</label><figDesc>BETWEEN THE PROPOSED COUNTERMEASURE AND STATE-OF-THE-ART METHODS ON REPLAY-ATTACK AND CASIA-FASD DATABASES IN TERMS OF EER(%) AND HTER(%)</figDesc><table><row><cell>Methods</cell><cell cols="2">REPLAY-ATTACK EER HTER</cell><cell>CASIA-FASD EER</cell></row><row><cell>Motion [60]</cell><cell>11.6</cell><cell>11.7</cell><cell>26.6</cell></row><row><cell>LBP [56]</cell><cell>13.9</cell><cell></cell><cell>18.2</cell></row><row><cell>LBP-TOP [61]</cell><cell>7.90</cell><cell>7.60</cell><cell>10.00</cell></row><row><cell>CDD [62]</cell><cell>-</cell><cell>-</cell><cell>11.8</cell></row><row><cell>DOG [3]</cell><cell>-</cell><cell>-</cell><cell>17.0</cell></row><row><cell>DMD [27]</cell><cell>5.3</cell><cell>3.8</cell><cell>21.8</cell></row><row><cell>IQA [4]</cell><cell>-</cell><cell>15.2</cell><cell>32.4</cell></row><row><cell>CNN [14]</cell><cell>6.10</cell><cell>2.10</cell><cell>7.40</cell></row><row><cell>IDA [5]</cell><cell>-</cell><cell>7.4</cell><cell>-</cell></row><row><cell>Motion + LBP [29]</cell><cell>4.50</cell><cell>5.11</cell><cell>-</cell></row><row><cell>Color-LBP [10]</cell><cell>0.40</cell><cell>2.90</cell><cell>6.20</cell></row><row><cell>Bottleneck feature fusion + NN [50]</cell><cell>0.83</cell><cell>0.00</cell><cell>5.83</cell></row><row><cell>Ours (MobileNet + Attention)</cell><cell>0.131</cell><cell>0.254</cell><cell>4.175</cell></row><row><cell>Ours (ResNet-18 + Attention)</cell><cell>0.210</cell><cell>0.389</cell><cell>3.145</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE X</head><label>X</label><figDesc>INTER-DATABASE TEST RESULTS IN TERMS OF HTER (%) ON THE CASIA-FASD AND REPLAY-ATTACK DATABASE</figDesc><table><row><cell>Method</cell><cell>Train CASIA</cell><cell>Test REPLAY</cell><cell>Train REPLAY</cell><cell>Test CASIA</cell></row><row><cell></cell><cell>FASD</cell><cell>ATTACK</cell><cell>ATTACK</cell><cell>FASD</cell></row><row><cell>Motion</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE XI INTER</head><label>XI</label><figDesc>-DATABASE TEST RESULTS FOR RGB FEATURES IN TERMS OF MAXIMUM MEAN DISCREPANCY ON THE CASIA-FASD AND REPLAY-ATTACK DATABASE</figDesc><table><row><cell>Model</cell><cell>Train</cell><cell>Val</cell><cell>MMD</cell></row><row><cell></cell><cell>CASIA-FASD</cell><cell>CASIA-FASD</cell><cell>0.7653</cell></row><row><cell>Resnet18 RGB</cell><cell>CASIA-FASD REPLAY-ATTACK</cell><cell>REPLAY-ATTACK REPLAY-ATTACK</cell><cell>1.4561 0.6871</cell></row><row><cell></cell><cell>REPLAY-ATTACK</cell><cell>CASIA-FASD</cell><cell>1.3484</cell></row><row><cell></cell><cell>CASIA-FASD</cell><cell>CASIA-FASD</cell><cell>0.8654</cell></row><row><cell>Mobilenet RGB</cell><cell>CASIA-FASD REPLAY-ATTACK</cell><cell>REPLAY-ATTACK REPLAY-ATTACK</cell><cell>1.3276 0.7469</cell></row><row><cell></cell><cell>REPLAY-ATTACK</cell><cell>CASIA-FASD</cell><cell>1.2765</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE XII INTER</head><label>XII</label><figDesc>-DATABASE TEST RESULTS FOR MSR FEATURES IN TERMS OF MAXIMUM MEAN DISCREPANCY ON THE CASIA-FASD AND REPLAY-ATTACK DATABASE</figDesc><table><row><cell>Model</cell><cell>Train</cell><cell>Val</cell><cell>MMD</cell></row><row><cell></cell><cell>CASIA-FASD</cell><cell>CASIA-FASD</cell><cell>0.9831</cell></row><row><cell>Resnet18 MSR</cell><cell>CASIA-FASD REPLAY-ATTACK</cell><cell>REPLAY-ATTACK REPLAY-ATTACK</cell><cell>1.8746 0.6541</cell></row><row><cell></cell><cell>REPLAY-ATTACK</cell><cell>CASIA-FASD</cell><cell>1.0133</cell></row><row><cell></cell><cell>CASIA-FASD</cell><cell>CASIA-FASD</cell><cell>0.8655</cell></row><row><cell>Mobilenet MSR</cell><cell>CASIA-FASD REPLAY-ATTACK</cell><cell>REPLAY-ATTACK REPLAY-ATTACK</cell><cell>1.7749 0.8811</cell></row><row><cell></cell><cell>REPLAY-ATTACK</cell><cell>CASIA-FASD</cell><cell>1.1661</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE XIII INTER</head><label>XIII</label><figDesc>-DATABASE TEST RESULTS FOR RGB AND MSR FUSION FEATURES IN TERMS OF MAXIMUM MEAN DISCREPANCY ON THE CASIA-FASD AND REPLAY-ATTACK DATABASE</figDesc><table><row><cell>Model</cell><cell>Train</cell><cell>Val</cell><cell>MMD</cell></row><row><cell></cell><cell>CASIA-FASD</cell><cell>CASIA-FASD</cell><cell>0.6215</cell></row><row><cell>Resnet18</cell><cell>CASIA-FASD</cell><cell>REPLAY-ATTACK</cell><cell>1.2511</cell></row><row><cell>RGB + MSR Fusion</cell><cell>REPLAY-ATTACK</cell><cell>REPLAY-ATTACK</cell><cell>0.7003</cell></row><row><cell></cell><cell>REPLAY-ATTACK</cell><cell>CASIA-FASD</cell><cell>1.1295</cell></row><row><cell></cell><cell>CASIA-FASD</cell><cell>CASIA-FASD</cell><cell>0.6619</cell></row><row><cell>Mobilenet</cell><cell>CASIA-FASD</cell><cell>REPLAY-ATTACK</cell><cell>1.3518</cell></row><row><cell>RGB + MSR Fusion</cell><cell>REPLAY-ATTACK</cell><cell>REPLAY-ATTACK</cell><cell>0.7139</cell></row><row><cell></cell><cell>REPLAY-ATTACK</cell><cell>CASIA-FASD</cell><cell>1.0551</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>The authors would like to thank the journal reviewers for their valuable suggestions. This work was supported in part by the National Natural Science Foundation of China (61876072, 61876178, 61872367, 61572501) and the Fundamental Research Funds for the Central Universities.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Live face detection based on the analysis of fourier spectra</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc Spie</title>
		<imprint>
			<biblScope unit="volume">5404</biblScope>
			<biblScope unit="page" from="296" to="303" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Face liveness detection from a single image with sparse low bilinear discriminative model</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2010 -11th European Conference on Computer Vision</title>
		<meeting><address><addrLine>Heraklion, Crete, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">September 5-11, 2010. 2010</date>
			<biblScope unit="page" from="504" to="517" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VI</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A face antispoofing database with diverse attacks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Iapr International Conference on Biometrics</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="26" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Face anti-spoofing based on general image quality assessment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Galbally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">22nd International Conference on Pattern Recognition, ICPR 2014</title>
		<meeting><address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">August 24-28, 2014, 2014</date>
			<biblScope unit="page" from="1173" to="1178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Face spoof detection with image distortion analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="746" to="761" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Face liveness detection by exploring multiple scenic clues</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th International Conference on Control Automation Robotics &amp; Vision, ICARCV 2012</title>
		<meeting><address><addrLine>Guangzhou, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">December 5-7, 2012, 2012</date>
			<biblScope unit="page" from="188" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Live face video vs. spoof face video: Use of moiré patterns to detect replay video attacks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Biometrics, ICB 2015</title>
		<meeting><address><addrLine>Phuket, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-22">19-22 May, 2015, 2015</date>
			<biblScope unit="page" from="98" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Face spoofing detection using colour texture analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Boulkenafet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Komulainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1818" to="1830" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scale space texture analysis for face anti-spoofing</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Boulkenafet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Komulainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Biometrics</title>
		<meeting><address><addrLine>Halmstad, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-13">2016. June 13-16, 2016, 2016</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Face anti-spoofing based on color texture analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Boulkenafet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Komulainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Image Processing</title>
		<meeting><address><addrLine>Quebec City, QC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-27">2015. September 27-30, 2015, 2015</date>
			<biblScope unit="page" from="2636" to="2640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mäenpää</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Face spoofing detection from single images using micro-texture analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Määttä</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Joint Conference on Biometrics, IJCB 2011</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">October 11-13, 2011, 2011</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep representations for iris, face, and fingerprint spoofing detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Menotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chiachia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Da Silva Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pedrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Falcão</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="864" to="879" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learn convolutional neural network for face anti-spoofing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1408.5601</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">December 3-6, 2012. 2012</date>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Eyeblink-based anti-spoofing in face recognition from a generic webcamera</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th International Conference on Computer Vision, ICCV 2007, Rio de Janeiro</title>
		<meeting><address><addrLine>Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">October 14-20, 2007, 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Monocular camera-based face liveness detection by combining eyeblink and scene context</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Telecommunication Systems</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="215" to="225" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Blinking-based live face detection using conditional random fields</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Biometrics, International Conference, ICB 2007</title>
		<meeting><address><addrLine>Seoul, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">August 27-29, 2007, Proceedings, 2007</date>
			<biblScope unit="page" from="252" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Motion-based countermeasures to photo attacks in face recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Anjos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Chakka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Iet Biometrics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="147" to="158" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Real-time face detection and motion analysis with application in &quot;liveness&quot; assessment</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kollreider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fronthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Faraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bigun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics Security</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="548" to="558" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Masked fake face detection using radiance measurements</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Opt Soc Am A Opt Image Sci Vis</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="760" to="766" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Face spoofing detection through visual codebooks of spectral temporal cubes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Da Silva Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pedrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4726" to="4740" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A multiscale retinex for bridging the gap between color images and the human observation of scenes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Jobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Woodell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="965" to="976" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural aggregation network for video face recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21">2017. July 21-26, 2017, 2017</date>
			<biblScope unit="page" from="5216" to="5225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Face anti-spoofing using patch and depth-based cnns</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Atoum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Joint Conference on Biometrics</title>
		<meeting><address><addrLine>Denver, CO, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-01">2017. October 1-4, 2017, 2017</date>
			<biblScope unit="page" from="319" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning temporal features using LSTM-CNN architecture for face anti-spoofing</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd IAPR Asian Conference on Pattern Recognition, ACPR 2015</title>
		<meeting><address><addrLine>Kuala Lumpur, Malaysia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">November 3-6, 2015, 2015</date>
			<biblScope unit="page" from="141" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Detection of face spoofing using visual dynamics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tirunagari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Poh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Windridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Iorliam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Suki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on information forensics and security</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="762" to="777" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Face liveness detection by learning multispectral reflectance distributions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/FG.2011.5771438</idno>
		<ptr target="https://doi.org/10.1109/FG.2011.5771438" />
	</analytic>
	<monogr>
		<title level="m">Ninth IEEE International Conference on Automatic Face and Gesture Recognition (FG 2011)</title>
		<meeting><address><addrLine>Santa Barbara, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-03-25">21-25 March 2011, 2011</date>
			<biblScope unit="page" from="436" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Complementary countermeasures for detecting scenic face spoofing attacks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Komulainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anjos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Biometrics, ICB 2013</title>
		<meeting><address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06-07">4-7 June, 2013. 2013</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Moving face spoofing detection via 3d projective invariants</title>
		<author>
			<persName><forename type="first">M</forename><surname>De Marsico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nappi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Riccio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dugelay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th IAPR International Conference on Biometrics, ICB 2012</title>
		<meeting><address><addrLine>New Delhi, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-04-01">March 29 -April 1, 2012, 2012</date>
			<biblScope unit="page" from="73" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Face liveness detection using 3d structure recovered from a single camera</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Biometrics, ICB 2013</title>
		<meeting><address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06">June, 2013. 2013</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Robust face antispoofing with depth information</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="332" to="337" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Lightness and retinex theory</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Land</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Mccann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Opt Soc Am</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Color image enhancement based on single-scale retinex with a jnd-based nonlinear filter</title>
		<author>
			<persName><forename type="first">D</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Circuits and Systems (ISCAS 2007)</title>
		<meeting><address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-05">May 2007. 2007</date>
			<biblScope unit="page" from="3948" to="3951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A LDCT image contrast enhancement algorithm based on single-scale retinex theory</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 International Conferences on Computational Intelligence for Modelling, Control and Automation (CIMCA 2008), Intelligent Agents, Web Technologies and Internet Commerce (IAWTIC 2008), Innovation in Software Engineering (ISE 2008)</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-12-12">10-12 December 2008. 2008</date>
			<biblScope unit="page" from="181" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Properties and performance of a center/surround retinex</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Jobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Woodell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="451" to="462" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Intensity variation normalization for finger vein recognition using guided filter based singe scale retinex</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="17" to="089" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adaptive multiscale retinex for image contrast enhancement</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth International Conference on Signal-Image Technology &amp; Internet-Based Systems, SITIS 2013</title>
		<meeting><address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">December 2-5, 2013. 2013</date>
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Combining multiple sources of knowledge in deep cnns for action recognition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Winter Conference on Applications of Computer Vision, WACV 2016</title>
		<meeting><address><addrLine>Lake Placid, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">March 7-10, 2016, 2016</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08">2014. December 8-13 2014. 2014</date>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Stacked generalization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="241" to="259" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">See the forest for the trees: Joint spatial and temporal recurrent neural networks for videobased person re-identification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21">2017. July 21-26, 2017, 2017</date>
			<biblScope unit="page" from="6776" to="6785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attentional pooling for action recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-09">2017, 4-9 December 2017. 2017</date>
			<biblScope unit="page" from="33" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">An attention model for group-level emotion recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<idno>abs/1807.03380</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21">2017. July 21-26, 2017, 2017</date>
			<biblScope unit="page" from="6450" to="6458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fingerprint liveness detection using convolutional neural networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>De Alencar Lotufo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Machado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1206" to="1213" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Face anti-spoofing via deep local binary patterns</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-17">2017. September 17-20, 2017, 2017</date>
			<biblScope unit="page" from="101" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning generalized deep feature representation for face anti-spoofing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2639" to="2652" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Integration of image quality and motion cues for face anti-spoofing: A neural network approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Po</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="451" to="460" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multi-task cascaded convolutional networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno>abs/1604.02878</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1704.04861</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Miami, Florida, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06">2009. June 2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A face antispoofing database with diverse attacks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th IAPR International Conference on Biometrics, ICB 2012</title>
		<meeting><address><addrLine>New Delhi, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-04-01">March 29 -April 1, 2012, 2012</date>
			<biblScope unit="page" from="26" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">On the effectiveness of local binary patterns in face anti-spoofing</title>
		<author>
			<persName><forename type="first">I</forename><surname>Chingovska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anjos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Biometrics Special Interest Group</title>
		<meeting>the International Conference of Biometrics Special Interest Group<address><addrLine>Darmstadt, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-09-06">2012. September 6-7, 2012, 2012</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">OULU-NPU: A mobile face presentation attack database with real-world variations</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Boulkenafet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Komulainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th IEEE International Conference on Automatic Face &amp; Gesture Recognition</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-05-30">2017. May 30 -June 3, 2017, 2017</date>
			<biblScope unit="page" from="612" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A competition on generalized software-based face presentation attack detection in mobile scenarios</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Boulkenafet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Komulainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Benlamoudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Samai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Bekhouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ouafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dornaika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Taleb-Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhilare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kanhangad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Costa-Pazo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vázquez-Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Perez-Cabo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Moreira-Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>González-Jiménez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohammadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Volkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Abe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Andaló</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Padilha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bertocco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wainer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Da Silva Torres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Angeloni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Folego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Godoy</surname></persName>
		</author>
		<author>
			<persName><surname>Hadid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="page" from="688" to="696" />
			<date type="published" when="2017-10-01">2017. October 1-4, 2017, 2017</date>
			<pubPlace>Denver, CO, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Presentation attack detection methods for face recognition systems: A comprehensive survey</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ramachandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Busch</surname></persName>
		</author>
		<idno type="DOI">10.1145/3038924</idno>
		<ptr target="http://doi.acm.org/10.1145/3038924" />
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2017-03">Mar. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Counter-measures to photo attacks in face recognition: A public database and a baseline</title>
		<author>
			<persName><forename type="first">A</forename><surname>Anjos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Joint Conference on Biometrics, IJCB 2011</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">October 11-13, 2011, 2011</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Face liveness detection using dynamic texture</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Komulainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anjos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M D</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP J. Image and Video Processing</title>
		<imprint>
			<biblScope unit="volume">2014</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Face liveness detection with component dependent descriptor</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Biometrics, ICB 2013</title>
		<meeting><address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06-07">4-7 June, 2013. 2013</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning deep models for face antispoofing: Binary or auxiliary supervision</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1803.11097</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Computationally efficient face spoofing detection with motion magnification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bharadwaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">I</forename><surname>Dhamecha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vatsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2013</title>
		<meeting><address><addrLine>Portland, OR, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">June 23-28, 2013. 2013</date>
			<biblScope unit="page" from="105" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for face anti-spoofing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1794" to="1809" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Face antispoofing based on frame difference and multilevel representation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Benlamoudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Aiadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ouafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Samai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oussalah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">43007</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Integrating structured biological data by kernel maximum mean discrepancy</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hans-Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bernhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="49" to="e57" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
