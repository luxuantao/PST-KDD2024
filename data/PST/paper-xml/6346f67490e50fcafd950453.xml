<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reconsidering OS Memory Optimizations in the Presence of Disaggregated Memory</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shai</forename><surname>Bergman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technion -Israel Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Priyank</forename><surname>Faldu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Boris</forename><surname>Grot</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Llu√≠s</forename><surname>Vilanova</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Imperial College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mark</forename><surname>Silberstein</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technion -Israel Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Disaggregated</forename><surname>Memory</surname></persName>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">ISMM &apos;22</orgName>
								<address>
									<addrLine>June 14</addrLine>
									<postCode>2022</postCode>
									<settlement>San Diego</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">ISMM &apos;22</orgName>
								<address>
									<addrLine>June 14</addrLine>
									<postCode>2022</postCode>
									<settlement>San Diego</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Reconsidering OS Memory Optimizations in the Presence of Disaggregated Memory</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3520263.3534650</idno>
					<note type="submission">200 0 400 600 800 1000 1200 1400 1600</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Disaggregated Memory, Operating Systems</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Tiered memory systems introduce an additional memory level with higher-than-local-DRAM access latency and require sophisticated memory management mechanisms to achieve cost-efficiency and high performance. Recent works focus on byte-addressable tiered memory architectures which offer better performance than pure swap-based systems. We observe that adding disaggregation to a byte-addressable tiered memory architecture requires important design changes that deviate from the common techniques that target lowerlatency non-volatile memory systems. Our comprehensive analysis of real workloads shows that the high access latency to disaggregated memory undermines the utility of well-established memory management optimizations. Based on these insights, we develop HotBox -a disaggregated memory management subsystem for Linux that strives to maximize the local memory hit rate with low memory management overhead. HotBox introduces only minor changes to the Linux kernel while outperforming state-of-the-art systems on memory-intensive benchmarks by up to 2.25√ó.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS Concepts:</head><p>‚Ä¢ Software and its engineering ‚Üí Memory management; ‚Ä¢ Hardware ‚Üí Emerging architectures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Tiered, or heterogeneous, memory architectures have emerged as a promising approach for delivering higher memory capacity at a lower cost. Such architectures extend the memory hierarchy with an additional memory tier that has a lower per-byte cost but also a higher access latency.</p><p>Two primary technologies are used in tiered memory systems, shown in Figure <ref type="figure" target="#fig_0">1</ref>, which can be categorized according to their access latency. The first, local byte-addressable nonvolatile memory (NVM), such as Intel Optane DC DIMMs, is a new type of memory hardware that is 2√ó-4√ó slower than DRAM <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b48">49]</ref>, but cheaper by about the same factor.</p><p>The second, remote disaggregated memory, is a system architecture that delivers high memory capacity at a rackor cluster-level over the network. Figure <ref type="figure" target="#fig_1">2</ref> shows the canonical disaggregated memory architecture where applications run on compute blades that host a small amount of local main memory, while the bulk of their datasets reside in a high-capacity remote memory blade accessed through a lowlatency interconnect <ref type="bibr" target="#b32">[33]</ref>. Disaggregation brings cost savings by reducing memory fragmentation across datacenter nodes, which share a large memory pool.</p><p>There are two mechanisms to transparently access disaggregated memory: using it as a swap device with accesses at page granularity, or via CPU load/store accesses at cache-line granularity. A hybrid approach combines cache-line accesses with page migration between remote and local memory, using the latter as a cache. Swapping can be employed in commodity systems today and has been thoroughly studied in prior work <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b41">42]</ref>. However, a hybrid approach has gained increased interest <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b47">48]</ref> with the expected emergence of lower latency networks offering end-to-end latencies as low as 750 nsec for optimistic mid-term estimates [2, <ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b42">43]</ref> (accounting for network delays in the cache-coherent fabric, e.g., CXL <ref type="bibr" target="#b2">[3]</ref> and Gen-Z [2], and local access in memory blades), to 1,500 nsec with today's interconnects <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>In this paper, we seek to optimize the performance of disaggregated memory systems with a hybrid access mechanism. The key question shared by all tiered memory systems is how to reduce memory costs by shifting memory usage into the slower and cheaper tier with minimal impact to the application performance. Most existing mechanisms for managing byte-addressable tiered memory target the lowerlatency end of the spectrum of 200-600 nsec <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b47">48]</ref>. In this work, we ask whether the performance optimizations suggested in prior work are also effective for byte-addressable disaggregated memory management, under their respective latency spectrum of 750-1500 nsec. Our analysis ( ¬ß 3) shows that this is not the case. Our conclusions are as follows:</p><p>1. A swap-only approach under higher memory latencies is inefficient. Several prior works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b41">42]</ref> regard the disaggregated memory blade as a swap device, whereby any access to data residing in the blade results in the page being copied to local DRAM. This approach leads to large-granularity accesses resulting in thrashing of local memory and poor performance on workloads with poor access locality. Our analysis shows that, for disaggregated memory latencies, such a swap-only approach can be inferior to a hybrid memory access mechanism that combines both cache-granular and page-granular accesses, even when using swap with an optimal offline page replacement policy.</p><p>2. Huge pages are detrimental. While prior approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b47">48]</ref> have advocated for using huge pages in tiered memory systems to reduce TLB pressure and shorten page walk latencies, we find that in a higher-latency regime, huge pages often have a detrimental effect on system performance. Our analysis shows the negative impact of a huge-page-induced phenomenon we call hotness fragmentation. Hotness fragmentation of a page implies that its constituting base pages are accessed at a different frequency (have different hotness) <ref type="foot" target="#foot_0">1</ref> . When migrated into local memory, hotness-fragmented huge pages decrease the effective local memory capacity and, as a result, increase the incidence of slow accesses to disaggregated memory.</p><p>3. Batch-migration of pages is inefficient. Prior studies <ref type="bibr" target="#b47">[48]</ref> argued for migrating batches of pages to amortize system overheads. However, we find that batching decreases both the accuracy and timeliness of migration decisions, while at the same time does not seem to reduce the migration costs.</p><p>To summarize, swapping, huge pages, and batching are well-established solutions to improve performance in tiered memory systems, but do so at the expense of reducing local memory hit rate with their coarse-granular migration policy and mechanism. This is a very profitable trade-off in existing tiered memory systems at the extremes of the latency differences between the fast and slow memories (e.g., local DRAM and NVM in the left half of Figure <ref type="figure" target="#fig_0">1</ref>), but we demonstrate that this trade-off requires a different system design point for disaggregated memory because of its particular access latency regime.</p><p>Based on these insights, we build HotBox, a novel memory management subsystem for the Linux kernel targeting byte-addressable disaggregated memory that maximizes local memory hit rate through the use of finer-grain management mechanisms. While maximizing hit rate is a common goal of all caching architectures, the primary challenge we cope with in disaggregated memory systems are their non-trivial migration and access monitoring overheads.</p><p>To this end, HotBox makes the following design choices: (1) use a hybrid access mechanism, (2) eliminate the negative effects of hotness fragmentation by using only base pages in local and disaggregated memory, and (3) do not use batching and instead migrate pages on demand, one page at a time.</p><p>HotBox takes several steps to reduce the costs of disaggregated memory management. It reduces the overheads of estimating local memory page hotness by using a dynamic page access sampling mechanism whose frequency increases with local memory pressure. This allows HotBox to accurately discriminate between pages of high hotness, leading to better eviction decisions for the local memory, exactly when it is particularly important for caching performance <ref type="bibr" target="#b39">[40]</ref>, yet keeps the sampling overheads low in the common case. To reduce sampling overheads for remote memory, HotBox takes advantage of the hierarchical translation structures, dismissing large memory regions where not a single page has been accessed (i.e., a sub-tree of the page table). Finally, Hot-Box monitors the utility of migrations, pausing migrations when deemed unnecessary (i.e., to prevent local memory thrashing).</p><p>We implement HotBox in Linux and evaluate it using Intel's Persistent Hybrid Memory Emulation Platform (PMEP) <ref type="bibr" target="#b18">[19]</ref>. We compare HotBox with state-of-the-art mechanisms, including swapping-based systems such as InfiniSwap <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref>, as well as the recent Nimble <ref type="bibr" target="#b47">[48]</ref> system for tiered memory, which relies on huge pages and batch migration to amortize page management overheads. We run memoryintensive applications including database (VoltDB), key-value store (memcached), and graph analytics (Ligra and Graph500). HotBox outperforms all state-of-the-art approaches, corroborating the conclusions of our earlier analysis. Compared to swapping-based systems and Nimble, HotBox achieves speedups of up to 4.5√ó and 2.25√ó respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Disaggregated memory systems expose an additional memory tier into the traditional system architecture. The memory hierarchy of such a system consists of a fast but limitedcapacity local DRAM, and a slower disaggregated memory blade. Figure <ref type="figure" target="#fig_2">3</ref> illustrates a hybrid disaggregated memory system in action. To attain maximum performance, frequently accessed (hot) pages are placed in the local DRAM, while less frequently accessed (cold) pages are placed in the disaggregated memory blade and accessed at cache line granularity over an interconnect fabric [2, <ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b15">16]</ref>. A page's hotness can change over time, and thus it is migrated between the tiers at runtime. Recent works on tiered memory systems <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b47">48</ref>] combine direct cache-line accesses to tiered memory and page migrations to optimize performance -that is, a hybrid system. They rely on mechanisms that sample page access frequency to determine hot and cold pages and migrate them accordingly. Cold pages are directly accessed from the tiered memory without migrating them first into local DRAM. These works, however, target low-latency tiered memory systems and are ill-suited for disaggregated memory systems as we show next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Analysis of existing approaches</head><p>We identify three aspects of tiered memory systems that are essential for performance: (1) the granularity of memory accesses to remote memory (i.e., swapping vs cache line accesses vs a hybrid), (2) the granularity of memory management (i.e., page size), and (3) the granularity at which pages are selected and migrated between local and remote memory. In this section, we revisit the conclusions presented in the recent literature concerning these three issues and show that the reported findings are not applicable to the higher latency of disaggregated memory systems. Methodology. Unless stated otherwise, we use several memoryintensive workloads, each of which has a working set size of about 10 GB, and analyze them on an evaluation platform with configurable latency for the disaggregated memory tier (see ¬ß 6 for details).</p><p>For brevity, we use the terms "local memory" and "remote memory" to refer to the local DRAM and disaggregated memories shown in Figure <ref type="figure" target="#fig_2">3</ref>, respectively. In a tiered memory system without disaggregation, we use the term "remote memory" to refer to the slower memory tier (e.g., NVM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Granularity of memory accesses: cache line vs. swap</head><p>Prior works on transparent support for tiered memory architectures define the system as following one of three models:</p><p>Swap: Remote memory is used as a swap device <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>Cache-line access: Data in remote memory is accessed directly using regular cache-line accesses <ref type="bibr" target="#b36">[37]</ref>.</p><p>Hybrid: A combination of the prior two models, as shown in Figure <ref type="figure" target="#fig_2">3</ref>, that allows both direct access to remote memory at cache line granularity and page migration to local memory for performance optimizations <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>Even though these approaches have been investigated independently in prior works, it remains unclear whether one model is superior to the others under disaggregated memory latencies. In this subsection, we confirm that the hybrid model is indeed essential for achieving high performance under these latencies: neither the swap nor cache-line access model in isolation is superior to the other one across all workloads.</p><p>We first consider the swap and cache line access models and show two applications where each model results in strictly poorer performance than the other. To do so, we compare regular cache line accesses against the optimal swap model using Belady's offline page replacement algorithm <ref type="bibr" target="#b11">[12]</ref>. Analysis. We compare the models' average memory access time (AMAT) using a simulator. The simulator processes an application trace and computes AMAT for a specific cache size, for local memory and remote memory configurations.</p><p>We collect traces, using PIN <ref type="bibr" target="#b33">[34]</ref>, of two graph processing applications: BFS (large working set, mostly random accesses) and Page Rank (PR) (small working set) implemented in Ligra <ref type="bibr" target="#b43">[44]</ref>. We choose the simulation parameters (Table <ref type="table" target="#tab_0">1</ref>) to match the scaled-down characteristics of a real system that we use for evaluation.</p><p>Figure <ref type="figure" target="#fig_3">4</ref> shows the AMAT slowdown of the two models over a configuration with only local memory (our ideal target), using various remote memory latencies (a lower slowdown implies better performance); note that we simulated both the optimal offline swap policy (based on Belady) as well as an online swap policy (similar to the one in Linux). The key result is that none of the access models outperforms the others in all the scenarios, depending on both the application and the system configuration.</p><p>In BFS, cache line access performs best across all the evaluated latencies and outperforms even the optimal swap policy. In contrast, cache line access in PR loses its benefits at higher latencies, becoming slower even than the sub-optimal online swap policy.</p><p>Takeaway: Neither the cache line access nor the swap model are superior across applications when we look at the latency regime of disaggregated memory. Therefore, a hybrid design is essential for dynamically determining the correct policy (swap or cache line access) for each accessed memory page in a disaggregated system.</p><p>The most recent state-of-the-art work that utilizes the hybrid model transparently and without dedicated hardware is Nimble <ref type="bibr" target="#b47">[48]</ref> 2 , hence it serves as a baseline in our analysis of current hybrid systems. Other works on disaggregated memory systems that operate in the access latency spectrum of 750-1500 nsec consider only the swap access model <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b41">42]</ref>, which we collectively refer to as "swap" in the rest of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Granularity of memory management: base pages vs. huge pages</head><p>The use of huge pages (2 MB) is known to lower address translation overheads by extending the TLB reach and reducing the page walk time <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>. Intuitively, huge pages are particularly appealing for systems where an additional memory tier expands the available memory size dramatically (i.e., as more memory can be used by applications). However, as we show in this section, these benefits diminish when the remote access latency increases, and are 2 Nimble was developed for tiered memory systems at a single node where fast DRAM is augmented with slower secondary memory. However, it is equally applicable to tiered memory systems comprised of local and remote DRAM (i.e., disaggregated memory) as stated in the paper. eventually outweighed by the negative effects of using large page sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Slow remote memory renders huge pages ineffective.</head><p>We run five benchmarks that were shown in prior studies to benefit from using huge pages in local memory <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>. To understand whether these benefits hold in a disaggregated memory system, we pin the working set entirely in remote memory and configure the system to use huge pages. Page tables are placed in local memory for the best performance <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29]</ref>. We compare huge and base page performance under varying remote memory latencies. This setup is aimed to highlight the performance impact of huge pages where remote memory accesses dominate execution time.</p><p>Figure <ref type="figure">5a</ref> shows diminishing speedups for huge pages as the remote memory latency increases. Huge pages yield up to a 30% speedup when used in a system with local memory only, mainly due to reducing memory access latency by about 70 nsec with faster page walks (recall that a page walk on x86 entails multiple references to the memory hierarchy, whose combined latency may exceed that of fetching the data). However, these savings become negligible when the access latency to remote memory is up to an order of magnitude larger than local memory latency. Then, the latency to fetch the data from remote memory dominates end-to-end latency, resulting in a best-case speedup of 3% when huge pages are used (i.e., page walks are served by local memory, and followed by a data access to remote memory).</p><p>Takeaway: The use of huge pages in remote memory results in small or no performance benefits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Hotness fragmentation makes huge pages harmful.</head><p>The use of huge pages is often associated with internal huge page fragmentation, also known as memory bloat <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b37">38]</ref>. This phenomenon leads to an increase in memory usage and results in poor memory utilization; i.e., some of the base pages that constitute a huge page are never utilized.</p><p>When huge pages are used in a tiered memory system, we observe poor memory utilization that cannot be explained by memory bloat alone. Rather, we find that the spatial locality of accesses to huge pages tends to be low. When a huge page spanning the equivalent of 512 base pages is migrated into local memory, its constituent base pages are all accessed but are not equally hot, resulting in sub-optimal use of local memory space. We call this phenomenon hotness fragmentation (ùêªùêπ ).</p><p>The performance implications of ùêªùêπ are particularly acute when the local memory size is a fraction of the working set size and can result in application slowdowns in a disaggregated memory system. We next characterize the effects of ùêªùêπ using a representative workload and then provide a more formal analysis. Intuition: ùêªùêπ in memcached. We compare the performance of the memcached-ETC workload <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b30">31]</ref> using base and huge (a) Benchmarks with all data placed in remote memory. The speedup of huge pages tapers off under higher latency.</p><p>(b) memcached with oracle static page placement. Huge pages degrade the performance for higher latency and smaller local memory.</p><p>(c) ùêªùêπ micro-benchmark with a TLB hit rate of 100% for huge and 0% for base pages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 5. Effect of using huge pages in disaggregated memory</head><p>pages. We illustrate the impact of ùêªùêπ while using a static policy that places the top hottest pages in local memory, as identified by offline profiling. This policy yields the best-case performance estimate, as it works better than the known best online policy <ref type="bibr" target="#b47">[48]</ref> for this workload.</p><p>As we reduce the fraction of the working set in local memory, the positive effect of huge pages should level off; i.e., we already saw in Figure <ref type="figure">5a</ref> that moving the working set to remote memory results in the same performance for base and huge pages (speedup=1). However, due to ùêªùêπ , huge pages in local memory result in lower performance than base pages. Figure <ref type="figure">5b</ref> shows that the slowdowns are pronounced for remote memory latencies of 750 nsec and above, the ranges of interest for disaggregated memory systems. Analysis. We define the measure of hotness fragmentation, ùêªùêπ , as ùêªùêπ = 1 ‚àí ùëÅ ‚Ñéùë¢ùëîùëí ùëÅ ùëèùëéùë†ùëí , where ùëÅ ‚Ñéùë¢ùëîùëí and ùëÅ ùëèùëéùë†ùëí are the number of accessed base pages in local memory, under hugeand base-page executions, respectively. Low values of ùêªùêπ imply that the utility of local memory for huge pages is high. However, high ùêªùêπ values correspond to an access pattern with poor spatial locality within huge pages, a pattern accommodated better by using base pages.</p><p>To illustrate the performance impact of ùêªùêπ , we run a micro-benchmark with a hot working set that fits entirely in local memory when base pages are used. We use a bimodal, uniformly random pattern where the skew of hot pages represents 90% of all accesses. To vary ùêªùêπ , we keep the number of hot base pages constant during the experiment but vary the distribution of the accesses across huge pages. Our benchmark is designed to favor huge pages: it ensures all memory accesses result in an LLC miss, has a 100% TLB hit rate when using huge pages, and has a 0% TLB hit rate when using base pages.</p><p>Figure <ref type="figure">5c</ref> shows that with disaggregated memory (remote memory latencies exceeding 750 nsec), ùêªùêπ makes huge pages inferior to base pages. With ùêªùêπ as low as 0.2 (i.e., 80% of the base pages constituting a huge page are hot), the 2√ó speedup of huge pages disappears at remote access latency of 750 nsec. At 1000 nsec remote access latency, the execution with huge pages is 20% slower than with base pages. ùêªùêπ in applications. We now measure ùêªùêπ in four real applications: memcached-ETC and Graph500 (where huge pages are advantageous), and Ligra-BFS and Ligra-PR, which are not sensitive to page size.</p><p>We consider two memory placement policies: offline static (top hot pages pinned in local memory) and online dynamic (which migrates pages using Nimble <ref type="bibr" target="#b47">[48]</ref>). To compute ùêªùêπ , we periodically (every 1 sec) sample and count all the basepage access bits and compute the average over the execution.</p><p>The center and bottom rows of Figure <ref type="figure" target="#fig_4">6</ref> show the ùêªùêπ statistics for each application, for different sizes of local memory. The top row shows the fraction of base pages accessed in local memory for each policy and page size.</p><p>We make several observations. First, as expected, smaller local memory consistently results in higher values of ùêªùêπ , for example when 20% of the working set is in local memory, memcached-ETC experiences 0.2 and 0.5 ùêªùêπ with static and dynamic policies, respectively. In other words, local memory utilization is effectively halved with huge pages under a dynamic policy as compared to the utilization with base pages. These results help explain the performance of memcached-ETC in Figure <ref type="figure">5b</ref>. Second, the dynamic migration policy in the state-of-the-art system <ref type="bibr" target="#b47">[48]</ref> causes significant ùêªùêπ . Finally, the memory bloat caused by huge pages (blue area in the graph) is relatively small as compared to the ùêªùêπ . The results of this experiment indicate that ùêªùêπ indeed occurs in real applications, and is correlated to their performance in disaggregated memory systems.</p><p>Thermostat <ref type="bibr" target="#b6">[7]</ref> considered a related question of the correlation between the huge page's hotness and the number of accesses to base pages in a huge page. They conclude that the spatial frequency of accesses within a huge page is poorly correlated with its true access rate. Their data supports our observation that ùêªùêπ is a real problem in huge-memory management, but they draw a different conclusion than us that serves their objective: motivating the use of huge pages with low-latency remote memory, where the adverse effects of ùêªùêπ is less pronounced.</p><p>Takeaway: The use of huge pages in disaggregated memory systems decreases the effective local memory capacity due to ùêªùêπ . Thus, using them for managing disaggregated memory is detrimental for performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Granularity of page migration:</head><p>on-demand vs. batch migrations Disaggregated memory systems require swift identification and migration of hot pages; when a remote page has been classified as hot, promptly migrating it into local memory reduces the number of high-latency remote memory accesses. Prior work argued that batch migration, i.e., grouping multiple pages to perform their migration together, is essential for achieving high migration performance <ref type="bibr" target="#b47">[48]</ref>. This is in contrast to on-demand page migration such as used in swapbased tiered memory systems <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref>, which migrate a page in response to a page fault, one page at a time. However, batch migration implies that pages are migrated ahead of time, in anticipation that they will be consequently accessed after the pages have become resident in local memory. In other words, pages are prefetched from remote memory according to the same pages' prior access statistics.</p><p>Due to this behavior, batch migration might result in imprecise migration decisions (evicting hotter local memory pages), whereas on-demand page-by-page migration is more conservative and costly. This presents a non-trivial trade-off between migration performance and migration accuracy, and prior work argued for optimizing the former at the expense of the latter.</p><p>We analyze the costs and benefits of the migration granularity trade-off: Do batch migrations amortize costs? Prior work <ref type="bibr" target="#b47">[48]</ref> observed that migrating a single page via the built-in Linux Figure <ref type="figure">7</ref>. Execution time breakdown of migrating a batch with a single 4 KB page to another NUMA node, using the existing move_pages system call. system call move_pages is slow, and propose using batching to amortize such costs. We show that batching is not a prerequisite for high migration performance.</p><p>We start by profiling move_pages. As can be seen in Figure <ref type="figure">7</ref>, 86% (26 ùúásec) of the time is indeed spent on amortizable operations -"batch syscall" and "CPU LRU drain"performed once per system call regardless of the number of pages that are migrated. But a closer examination reveals that such operations are unnecessary, as explained next.</p><p>Before a page can be migrated, it must be removed from the LRU list of its NUMA node, and Linux keeps a small per-core software cache of recent LRU list entries to reduce contention accessing the global LRU lists. The system call move_pages is designed to "drain" (i.e., flush) all per-core LRU list caches to their corresponding global LRU list before migrating any page. This operation is expensive and is conducted via interprocessor interrupts (IPIs) to all cores in the system.</p><p>However, we argue that draining is not essential, as pages in LRU caches can temporarily be considered non-migratable. The LRU caches are only 16 entries, constituting only a small fraction of the total memory and leaving many other potential candidates for migration; even a large multi-core system would only have a few hundreds of such pages. Furthermore, LRU caches are constantly updated, such that the desired page may soon leave the LRU cache and its migration would  <ref type="bibr" target="#b47">[48]</ref>). Local memory size is 20% of the working set. Batching causes aggressive migrations with low good-put. be re-enabled. In fact, the autoNUMA subsystem in Linux already implements this optimization.</p><p>We conclude that per-page migration time can be reduced to 4 ùúásec (from 30 ùúásec) that is not amortizable: the remaining operations must be performed for every page in a batch.</p><p>Takeaway: Batching is not required for high-performance page migration. Do batch migrations bring the correct pages? The effectiveness, or good-put, of the batch migration is critical in a system with limited local memory. We define good-put as the proportion of pages that have been migrated to local memory and accessed at least once before being evicted.</p><p>We measure the good-put of four memory-intensive benchmarks (see ¬ß 6 for details) on the Nimble system with batch migrations of base pages. Figure <ref type="figure" target="#fig_5">8</ref> shows that Nimble's goodput is low in three out of the four benchmarks, i.e., 27% in VoltDB-TPCC and about 46% in Ligra-BFS.</p><p>We also observe that the average migration rate of Nimble is considerably higher than on-demand page migration.</p><p>Takeaway: Batch migration leads to a lower good-put and higher migration bandwidth than on-demand migration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">HotBox</head><p>Based on the insights of the analysis in ¬ß 3, we design and implement HotBox, a novel memory management subsystem for disaggregated memory systems with four design goals:</p><p>1. Use of base pages to manage memory: to avoid ùêªùêπ , 2. On-demand hot-page migration under the hybrid access model: for improved migration utility, 3. Adaptive memory scanning: for improved accuracy of eviction decisions under high local memory pressure while retaining low overheads in a common case, 4. Adaptive throttling of migration mechanisms according to the utility of past migrations: to avoid local memory thrashing. The first two goals are derived from our analysis ( ¬ß 3), which argues for finer-granularity page management (single base page), whereas the last two aim to reduce the inherently higher overheads of fine-granularity page management. We explain the rationale for items 3 and 4 here and provide technical details about the complete system in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Achieving high accuracy of migration decisions</head><p>with low overheads Similar to prior tiered memory proposals <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b47">48]</ref>, HotBox uses the access bits of pages in local and remote memory to estimate page hotness. However, we observe that the rate at which the access bits are scanned (i.e., scanning rate) is a crucial parameter for correctly identifying migration candidates both on the remote side (to migrate in) and on the local side (to evict out). Intuitively, a higher scanning rate enables more accurate differentiation between frequently accessed pages. That is, the higher the rate, the more frequently a page should be accessed in order to be considered hot. Lower scanning rates naturally result in lower accuracy of the hotness statistics as many pages may end up being marked hot despite a potentially large difference in their access frequency.</p><p>While the observations above hold true both for local and remote memory, we find that local memory is particularly sensitive to the scanning rate since it serves as a cache for the remote tier and the majority of the pages are accessed relatively frequently. As a result, the ability to discriminate between hottest and less hot pages is crucial in order to make well-informed eviction decisions <ref type="bibr" target="#b39">[40]</ref>. Problematically, a high scanning rate is a costly proposition in terms of CPU cycles, especially when utilizing base pages, as more scanning operations are required to traverse a range of memory when compared to huge pages. While potentially acceptable if used for short intervals of time for local memory, frequent scanning incurs unacceptably-high overheads given large capacities of disaggregated memory.</p><p>What is needed is a mechanism that can dynamically navigate the cost-accuracy curve based on workload behavior in regard to memory pressure. To that end, HotBox employs two independent scanners for local and remote memory that interact via a closed feedback loop: (1) remote memory is scanned at a fixed rate that yields low overheads, and (2) local memory is scanned at an adaptive rate tied to the local memory pressure. Only when a remote page is classified as hot, it is migrated to local memory, and will subsequently increase the local memory pressure, resulting in a higher scanning rate. Thus HotBox ensures that the most frequently accessed pages in local memory are correctly identified as hot and are not evicted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Avoiding useless migrations</head><p>As mentioned above, scanning of access bits must necessarily occur more frequently (thus incurring a higher overhead) to better discern page hotness and improve the hit rate in local memory. However, when the hot working set size is too large, migrations stop being useful (since local and remote working sets have similar hotness) and may cause thrashing of local memory, hence hurting performance. To demonstrate this effect, we measure the throughput of memcached serving a random (non-skewed) set of requests <ref type="bibr" target="#b30">[31]</ref> using the Nimble page migration mechanism. We then run the same experiment, but disable the migrations after a warmup and measure the throughput again. We observe a 2.1√ó higher throughput with migrations disabled. This improvement is expected as the access pattern has no spatial locality, effectively causing local memory thrashing and useless page migration.</p><p>To counteract such cases, we devise a simple mechanism to evaluate the migration utility and pause migrations when the utility is low. Specifically, we monitor the application's accesses to local and remote memory, which allows the momentary local memory hit rate to be estimated. If this value remains unchanged over several measurement intervals, it indicates that migrations are not helpful in reducing accesses to remote memory and migration is paused. If the value deviates from the average by a few percentage points, migration is resumed. The intuition is that a change in either direction implies a change in the application access pattern. Stopping migrations averts the overheads of local and remote memory scanning and page migrations in cases where the system is in a steady-state and would otherwise continue to cycle pages fruitlessly between the local and remote memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Implementation</head><p>Favoring on-demand page migration reduces the implementation complexity significantly, allowing us to repurpose the existing autoNUMA and swap subsystems in Linux. HotBoxintroduces only minor modifications into the kernel (less than 2K LOC) and yet meets our design goals.</p><p>At a high level, we use (1) the swap subsystem to track and evict cold local pages, modified to use remote memory as the destination for evictions and (2) the autoNUMA subsystem to implement on-demand page migrations, modified to select only hot pages in remote memory and trigger evictions (via the kswapd daemon) if there is local memory pressure.</p><p>We now briefly survey the existing mechanisms in Linux that we reuse in HotBox, and then, we highlight the modifications necessary to adjust these mechanisms to our needs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Existing Linux Subsystems used in HotBox</head><p>Page and task balancing with autoNUMA. The autoN-UMA subsystem co-locates processes and the pages they use to minimize memory access latencies <ref type="bibr" target="#b17">[18]</ref> in a NUMA system. autoNUMA periodically unmaps a subset of pages in a process, accesses to which triggers a page fault, which the kernel uses to keep track of the current NUMA node of the faulting page and process. With this information, autoN-UMA reschedules tasks to the NUMA node where most of its accessed pages reside, while at the same time it migrates repeatedly accessed pages into the process's NUMA node. Page reclamation with kswapd and direct reclaim. Linux uses a page reclamation algorithm that swaps cold pages out from memory into a swap device. When the system detects memory pressure during the allocation of new pages, it wakes the kswapd daemon to free some additional pages asynchronously. If no pages are free, the kernel uses the direct reclaim path to free a small batch of pages synchronously.</p><p>When a new page cannot be allocated, direct reclaim is invoked. If a page allocation request can be fulfilled, but the resulting number of free pages in the system is lower than a configurable threshold (i.e., when there is memory pressure), the low watermark, the kswapd daemon is woken. The daemon attempts to relieve memory pressure by freeing multiple pages until a sufficient number are available, the configurable high watermark. Both subsystems gradually increase the aggressiveness of their heuristics when a sufficient number of pages cannot be freed. For example, direct reclaim will free dirty pages only when it is most aggressive since they require I/O. Instead, kswapd frees a skewed amount of file-backed and anonymous pages that change with the level of its aggressiveness.</p><p>To estimate page utility, the page reclamation algorithm classifies pages into two separate LRU lists (active and inactive). It has two sets of lists, for file-backed and anonymous pages, respectively. It also keeps track of two per-page bits: accessed (in the page table), and referenced (in the kernel's per-page metadata). The algorithm iteratively attempts to reclaim pages, becoming more aggressive at each iteration, until it reaches its target. At each iteration, it moves the bottom of the active LRU lists into the top of the inactive LRU lists, until their sizes conform to a preconfigured ratio. Then, it starts scanning the bottom of the inactive LRU lists until it reaches its target and, for each page, checks the accessed and referenced bits to determine whether the page is to be reclaimed or recirculated in the LRU lists <ref type="bibr" target="#b13">[14]</ref>.</p><p>All pages eventually end up at the tail of the inactive LRU list, giving the system an opportunity to scan them. The reclamation algorithm thus constantly recirculates the hottest pages and swaps the remaining pages out. In practice, the estimated utility of a page changes according to memory pressure. The greater the memory pressure, the faster the system cycles through the pages it scans, effectively increasing the average per-page scanning frequency. Page allocation. When allocating a new page, the kernel first attempts to use the NUMA node in which it is executing.</p><p>If this fails, it attempts to use other NUMA nodes until it finds a free page (or none is left in the system).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Extensions to existing Linux subsystems</head><p>We implement HotBox in Linux kernel version 4.9.99, tying the independent production-hardened heuristics present in Linux with new mechanisms that synergistically work to meet our goals.</p><p>1) We expose local and remote memories as separate NUMA nodes in the kernel.</p><p>2) We implement a new remote memory scanning algorithm that performs on-demand migrations of hot pages to increase migration utility. When enabled by the migration utility monitor, the algorithm, shown in Figure <ref type="figure" target="#fig_6">9</ref>, scans blocks of remote pages periodically to check their access bits (with a configurable batch size and period for scanning, effectively capping the migration bandwidth).</p><p>To further reduce the scanning overheads of remote memory, we perform a hierarchical scan of the page table radix tree and sample the accessed bits of the higher-level page tables (PMD): if the accessed bit is 0, we skip scanning all the entries in the underlying page table (PTE), as no page inside the page table has been accessed <ref type="bibr" target="#b24">[25]</ref>.</p><p>Remote memory pages' hotness is tracked with a per-page state that is updated by the periodic scans ( 1 cold, 2 warm, and 3 hot), and hot pages are migrated into local memory ondemand (after being unmapped and subsequently accessed; if no local pages are free, the page is re-mapped).</p><p>3) We modify kswapd to evict local cold pages into remote memory. When triggered, the modified kswapd refills and scans the inactive LRU list as usual (Steps a and b , respectively, in Figure <ref type="figure" target="#fig_6">9</ref>). If a cold page is found, it is evicted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4)</head><p>We modify page allocation to ensure kernel pages and all page tables are placed in local memory (for performance <ref type="bibr" target="#b5">[6]</ref>), while other pages have a "best effort" placement, as in Linux.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5)</head><p>We modify page allocations to not trigger direct reclaim when attempting to migrate a remote page, since an aggressive direct reclaim can stall applications for long periods of time and it is frequently better to keep the page remotely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Monitoring migration utility</head><p>We monitor the migration utility by maintaining a windowed average of the ratio between local and remote memory accesses of each task in the system (using the UOPS_LLC_-MISS_RETIRED.{REMOTE,LOCAL}_DRAM PMU counters <ref type="bibr" target="#b24">[25]</ref> via the in-kernel perf_event API) and consider the system to be in a steady state when the ratio has remained stable across three time windows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>We aim to answer two key questions: is HotBox effective in running applications in disaggregated memory with realistic access latency, and how does it compare with the state-ofthe-art tiered-memory OS management systems?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Methodology</head><p>As system support for cache-line remote-memory accesses is not commercially available as of yet, we obtained the results in this paper using Intel's Persistent Hybrid Memory Emulation Platform (PMEP <ref type="bibr" target="#b18">[19]</ref>, Table <ref type="table" target="#tab_1">2</ref>), which was also used for studying tiered memory in prior research <ref type="bibr" target="#b19">[20]</ref>. PMEP runs Linux, exposes a remote memory tier as a separate NUMA node, and uses special CPU microcode to emulate slower memory access latencies by injecting additional stalls, thereby allowing end-to-end full-system performance measurements. We verified that PMEP injects the requested latencies using Intel's MLC tool <ref type="bibr" target="#b45">[46]</ref>. All applications run on the first NUMA node. We disable HyperThreading and all unused CPUs. Local memory is placed on the first NUMA node. To limit its size, we use a simple balloon process that allocates and locks pages in memory. Certain workloads require a client application over the network, in which case we use an identical PMEP machine without HotBox connected via 10 Gbps Ethernet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">System and benchmark configurations</head><p>We evaluate HotBox against four alternative approaches: All-local: Data pages are located in local memory; no migration. This serves as an upper bound on performance. All-remote: Data pages are located in remote memory; no migration. This serves as a lower bound on performance. Swap: Remote memory is handled as a swap device; pages are paged-in and out via the Linux LRU mechanism. This is used to illustrate the performance of systems that do not perform direct access to remote memory (not hybrid) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref>. This is an optimized implementation that eliminates a block device layer similarly to frontswap <ref type="bibr" target="#b0">[1]</ref>. It is 1.4√ó faster than Linux swap when mounted on BRD <ref type="bibr" target="#b4">[5]</ref>. Nimble <ref type="bibr" target="#b47">[48]</ref>: Nimble migration management is evaluated using the same number of CPU cores as HotBox. In this configuration, their concurrent page migration mechanism degrades performance, and hence, we disable it. We note that Nimble's technique that enforces the local memory limit does not count kernel allocations (e.g., page tables) and filemapped pages (e.g., code) as part of local memory consumption, giving it a slight advantage over HotBox. We evaluate Nimble with both base and huge pages and the other approaches with base pages. In addition, we configure Figure <ref type="figure" target="#fig_0">10</ref>. End-to-end speedup over all-remote execution for different workloads and local memory sizes (WSS -working set size). Higher is better. Remote memory latency is 750 nsec. "Swap" did not finish for Graph500.</p><p>the maximum migration rate to be 2 GB/s in both Nimble and HotBox. This value provides an empirically good performance across all configurations for Nimble.</p><p>We run five benchmarks with a working set of 10 GB each: Graph500: A known graph processing benchmark <ref type="bibr" target="#b34">[35]</ref> with two kernels: graph construction and BFS. It was used in a prior study <ref type="bibr" target="#b47">[48]</ref>. The access pattern is random. Ligra-BFS: BFS on a popular Ligra graph processing framework <ref type="bibr" target="#b43">[44]</ref>. We use the included rMatGraph utility to create a graph with 40 M vertices and 400 M edges with the parameters from Graph500 <ref type="bibr" target="#b34">[35]</ref> (A=0.57, B=C=0. <ref type="bibr" target="#b18">19</ref>). We exclude graph loading time. The access pattern is random. Ligra-PR: Page Rank (PageRankDelta) on Ligra (same input as in BFS). The access pattern is initially random, but the algorithm reduces the effective working set over iterations. VoltDB-TPCC: TPC-C benchmark (we use 256 warehouses and 4 sites) <ref type="bibr" target="#b16">[17]</ref> running on VoltDB <ref type="bibr" target="#b44">[45]</ref>, an in-memory, rowbased relational database. We measure the average transaction latency over 5 min. The access pattern is random, but accesses are in blocks of 8 KB (the record size); there are also many random accesses to a small number of pages with data structures used by the Java Virtual Machine. memcached-ETC: An in-memory KV store <ref type="bibr" target="#b20">[21]</ref> measured using the Mutilate client <ref type="bibr" target="#b30">[31]</ref> and Facebook's ETC benchmark <ref type="bibr" target="#b10">[11]</ref>. The throughput is measured after loading 30 M records and a 1 min warmup. We average the throughput over 5 min. The access pattern is random with a skew; 90% of requests account for 10% of the keys. Despite this skewness, the distribution of the popular keys in memory is random.</p><p>All the benchmarks use four threads pinned to different CPU cores, except for memcached, which runs in a single thread (we do not have enough machines to saturate more).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">End-to-end evaluation</head><p>We start by evaluating the end-to-end performance of Hot-Box, compared to the state-of-the-art in tiered memory systems. We also evaluate HotBox without the adaptive throttling mechanism. In this study, we model a remote memory access latency of 750 nsec, which is optimistic for near-to mid-term time frame; other latencies are explored below. We evaluate the workloads with different local memory sizes that correspond to a fraction of the working set size (WSS) of the application; e.g., a point labeled "20% WSS" indicates a local memory size that can fit 20% of the application's working set. We thus focus on relative trends to decouple our evaluation from the constraints imposed by fixed choices of dataset and memory sizes.</p><p>Figure <ref type="figure" target="#fig_0">10</ref> shows that HotBox performs best on all studied configurations. The benefits of HotBox are particularly high for Ligra-PR, memcached-ETC and VoltDB-TPCC, with a relative speedup of up to 2.25√ó over the best performing state-of-the-art configuration. These workloads have a skewed memory access pattern where HotBox can provide more effective identification and migration of hot pages ( ¬ß 6.5 further analyzes the effectiveness of the migration decisions).</p><p>Graph500 exemplifies our hypothesis that ùêªùêπ and huge pages together lead to bad performance. This benchmark has large huge page speedups at low memory latencies (above 1.3√ó in Figure <ref type="figure">5a</ref>), but also has ùêªùêπ of 0.13 to 0.2 for the 20-40% WSS figures (see Figure <ref type="figure" target="#fig_4">6</ref>). As a result, the huge page optimizations in Nimble-huge are inefficient at higher remote memory latencies, and the same system with base pages, Nimble-base, performs 1.2√ó-1.6√ó better for 20%-40% WSS (as explained in ¬ß 3).</p><p>Ligra-BFS and memcached-ETC show that HotBox's adaptive mechanisms for avoiding page thrashing are also critical for performance. HotBox without adaptive throttling exhibits noticeably lower performance compared to HotBox in these workloads, demonstrating the adaptive throttling mechanism's benefits. We also note that Nimble-huge yields higher performance than Nimble-base in these workloads as a result of lower page management overheads. These are workloads where we know the hottest pages do not fit in local memory and, therefore, all policies but HotBox continuously thrash the local memory with migrations.</p><p>Interestingly, VoltDB-TPCC at 40% WSS shows that Swap outperforms Nimble by 2.3√ó and is almost as good as HotBox. This benchmark has sequential accesses to 8 KB blocks, and we believe that the read-ahead prefetcher in the Linux swap cache is identifying this pattern and reducing on-demand page migrations. This shows prefetching policies could have a secondary but important role, as HotBox still outperforms all policies without using prefetching.</p><p>In summary, HotBox outperforms all other systems by challenging traditional tiered memory management designs. This does not mean that previous memory management optimization efforts such as prefetching are unnecessary, but rather, as we will see next, they become secondary to local memory hit rate in the latency ranges of disaggregated memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Latency sensitivity and system overheads</head><p>Here we study the balance between system overheads and the accuracy of migration decisions on both Nimble and Hot-Box. To this end, Figure <ref type="figure" target="#fig_7">11</ref> shows absolute performance numbers and a breakdown of the system overheads for VoltDB-TPCC under different memory latencies. Nimble is invoked with base pages to isolate the impact of batching (Figure <ref type="figure" target="#fig_0">10</ref> above already showed that huge pages rarely provide a substantial benefit, sometimes having severe negative impacts).</p><p>The breakdown shows the runtime of the user application ("App"), the time spent scanning local and remote memory access bits ("Scan local" and "Scan remote," respectively), and the time spent migrating pages ("Migration"). We chose VoltDB-TPCC as it exhibits a random but skewed access pattern where the hottest pages fit local memory at 40% WSS.</p><p>First, recall that Nimble performs aggressive migration with relatively low good-put for this workload ( ¬ß 3, Figure <ref type="figure" target="#fig_5">8</ref>). The implications of this are evident here: migration overheads are much higher than in HotBox, and the fraction of the application time is larger and increases with the remote memory latency, indicating a significant amount of remote memory accesses. This is not the case for HotBox, which has better migration decisions (lower "App" time, in some cases close to optimal performance). The lower "App" time also indicates that with an additional dedicated core to offload both mechanisms' overheads ("Scan" and "Migration"), HotBox will still exceed Nimble's performance. Furthermore, HotBox is more resource efficient: under 40% WSS, HotBox's performance without an additional core (including the "Scan" and "Migration" overheads) still outperforms Nimble's "App" time alone, assuming an additional core is able to hide Nimble's overheads.</p><p>HotBox increases the memory scanning rate as a function of memory pressure, which is reflected in the "Scan local" and "Migration" times. These times are higher for 20% than for 40% WSS. Thus, this design enables gradual and more accurate migration decisions, resulting in smaller "Migration" and "App" overheads due to fewer and more accurate migrations.</p><p>To conclude, the overheads of HotBox are lower while, at the same time, its adaptive local scanning rate design provides better performance even at high remote memory latencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Effects of local memory size on migration bandwidth</head><p>Local memory size is decisive in application performance.</p><p>Figure <ref type="figure" target="#fig_8">12</ref> shows the execution time and effective migration bandwidth consumed by the system under different local memory sizes. Again, Nimble is executed with base pages. HotBox effectively executes with aggressive local memory size reductions (e.g., 10% WSS). Starting at 40% WSS, HotBox can effectively identify and migrate-in all hot pages. At 80% WSS, the migration bandwidth is close to zero, similar to that of Swap, and both are on par with the all-local execution. This implies that the hot working set of Ligra-PR represents approximately 80% of its full data set.</p><p>Notably, the migration bandwidth of Swap and HotBox decreases with a larger local memory, but Nimble's total bandwidth increases almost 5√ó as compared to its values with a small local memory. This is due to Nimble's pageexchange batching mechanism: Nimble's migration batch size is dependent on the size of the local memory; the batch size is the maximum between the number of cold pages in local memory and the hot pages in remote memory. As the local memory size increases, more victim pages are gathered from local memory, enabling larger batches with low goodput ratios.</p><p>In conclusion, as compared to all the other approaches, Hot-Box is more effective at a reduced local memory size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related work</head><p>Tiered memory systems have attracted considerable research attention, with many innovative ideas being proposed across both hardware and software stacks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b47">48]</ref>. Here, we review the most relevant research work in the context of OS support for memory disaggregation. Swap-only access models. Gao et al. analyzed the relationship between network and application performance <ref type="bibr" target="#b21">[22]</ref>, using a swap device implementation that injects additional latencies. Infiniswap <ref type="bibr" target="#b23">[24]</ref> applies a similar mechanism to utilize excess memory in remote servers. LegoOS <ref type="bibr" target="#b41">[42]</ref> proposes a disaggregation design, where local memory acts as a coarsegrain cache of remote memory with the help of specialized hardware support. These and other approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> differ from HotBox in that they do not support direct cache-line access to remote memory, and hence, their migration mechanisms and policies differ from ours. Further, their authors did not analyze the impact of huge pages, which is one of our core contributions.</p><p>Leap <ref type="bibr" target="#b8">[9]</ref> presents an efficient page prefetching from remote memory in the swap model. HotBox can benefit from such a prefetching policy: prefetching policies for the hybrid model can gain at least the same performance benefits as policies in the swap model (since the hybrid model includes the swap model). Prefetching for the hybrid model presents additional challenges, such as evaluating remote memory access patterns under direct cache-line accesses with low overhead, that we leave for future work. Hybrid access models. The approach most relevant to ours is the Nimble page management system <ref type="bibr" target="#b47">[48]</ref>, which uses huge pages and batch migrations to accelerate page migration in a tiered memory system with 200 nsec remote memory latency. In contrast, our analysis highlights the negative side effects of these optimizations, in particular for higherlatency disaggregated memory, leading to different design decisions in HotBox, which result in better performance.</p><p>Similarly, Thermostat <ref type="bibr" target="#b6">[7]</ref> targets tiered memory with the NVM latency range, focusing on the differentiation and placement of hot and cold huge pages. We demonstrate that this can hamper performance in a disaggregated memory setting.</p><p>HeteroOS <ref type="bibr" target="#b27">[28]</ref> targets virtualized systems and provides a guest-hypervisor cooperation mechanism for page placement in tiered memory systems. Our use of performance counters to reduce the migration overheads is similar to theirs. Our huge-page analysis is complementary to Het-eroOS, as it provides the necessary first step toward investigating the performance tradeoffs of utilizing huge pages in virtualized environments. Non-transparent access models. Dulloor et al. <ref type="bibr" target="#b19">[20]</ref> introduced a toolset to guide application data placement on a tiered memory system using profile data. HotBox employs dynamic memory management with online decisions, and hence, it is not directly comparable. Other works <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b46">47]</ref> rely on application data and require code changes to support disaggregated memory. HotBox runs at the kernel level and is applicable to all applications transparently. Hardware-based disaggregated architectures. A number of proposals considered new hardware support for disaggregated memory <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b41">42]</ref>. In particular, Scale-out NUMA <ref type="bibr" target="#b36">[37]</ref>, proposes a new hardware architecture for a non-coherent distributed compute and memory system with direct remote access capabilities, laying the foundations for future byte-addressable disaggregated systems. Huge pages. Prior studies indicate that the use of huge pages may hinder performance in a NUMA system due to false sharing of pages between NUMA nodes <ref type="bibr" target="#b22">[23]</ref> and may occupy additional memory resources due to memory bloat <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>. Our research is complementary, as we identify a new phenomenon, hotness fragmentation, that affects performance specifically in disaggregated memory systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>Memory disaggregation offers promising operational savings in a data center to meet the ever-increasing application memory requirements. Existing OS approaches for managing tiered memory <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b47">48]</ref> target memory latencies lower than those offered by current and near-future disaggregated memory latencies <ref type="bibr">[2-4, 13, 16]</ref>.</p><p>In this paper we show that the existing memory management approaches are ill-suited in the latency regimes of disaggregated memory, and introduce HotBox, a novel memory management subsystem for the Linux kernel targeting disaggregated memory. Based on a detailed analysis of state-of-the-art work and conventional memory management techniques, HotBox follows four design choices: to use only base pages for migration, to increase eviction accuracy by sampling page hotness according to memory pressure, to migrate remote memory pages only upon access to increase migration utility, and to throttle migration and page hotness sampling according to the utility of past migrations. These design choices enable HotBox to outperform state-of-theart systems by up to 2.5√ó on memory-intensive workloads. HotBox carefully reuses and extends existing Linux kernel subsystems, minimizing the impact on the existing kernel code (less than 2K LOC) and simplifying its integration in future releases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Latency ranges of tiered memory technologies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Traditional vs disaggregated memory architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Disaggregated memory system model. CPU can perform direct cache line access to disaggregated memory. Hot and cold pages are migrated between the memory tiers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. AMAT slowdown over all-local execution for different access models. No single model suits all configurations. Size of local memory: 20% of the working set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Local memory hit rate (top) and ùêªùêπ (center, bottom) vs. fraction of the working set in local memory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Page migration rate in the on-demand migration (aka swap) and batched migrations (aka Nimble with base pages<ref type="bibr" target="#b47">[48]</ref>). Local memory size is 20% of the working set. Batching causes aggressive migrations with low good-put.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Overview of local and remote page scanning and migration in HotBox.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Runtime breakdown of VoltDB-TPCC. Lower is better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Runtime and migration bandwidth of Ligra-PR. Lower is better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Disaggregated memory simulator parameters.</figDesc><table><row><cell>LLC Cache</cell><cell>6 MB, 8-way, 64 B cache block, latency: 30 cycles</cell></row><row><cell>Local Mem.</cell><cell>1 GB, 4 KB page size, latency: 210 cycles</cell></row><row><cell cols="2">Remote Mem. Unlimited size, 4 KB page size</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>PMEP platform.</figDesc><table><row><cell cols="2">Processor 2√ó Intel E5-4620 v2 w/ PMEP microcode patch</cell></row><row><cell>Memory</cell><cell>2√ó 64 GB DDR3 1333 Hz</cell></row><row><cell>Network</cell><cell>Intel 10-Gigabit SFI</cell></row><row><cell>OS</cell><cell>Ubuntu 16.04, Linux version 4.9.99</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Unlike the known problem of memory bloat, which stems from the internal fragmentation of huge pages due to non-contiguous allocations, hotness fragmentation is caused by mixed spatial access locality within a huge page.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank our anonymous reviewers for their insightful comments. This work was partially supported by the Technion Hiroshi Fujiwara Cyber Security Research Center, the Israel National Cyber Directorate, and the Israel Innovation Authority. We gratefully acknowledge support from Israel Science Foundation (Grant 1027/18).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://lwn.net/Articles/386103/" />
		<title level="m">Frontswap</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Gen-Z Core Specification</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Compute Express Link Specification</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Connectx-6 single/dual-port adapter supporting 200Gb/s with VPI</title>
		<ptr target="https://www.mellanox.com/page/products_dyn?product_family=265&amp;mtag=connectx_6_vpi_card" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Linux Block Ram Disk</title>
		<ptr target="https://www.kernel.org/doc/html/latest/admin-guide/blockdev/ramdisk.html" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mitosis: Transparently Self-Replicating Page-Tables for Large-Memory Machines</title>
		<author>
			<persName><forename type="first">Reto</forename><surname>Achermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Panwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Roscoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayneel</forename><surname>Gandhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting>the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="283" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Thermostat: Applicationtransparent page management for two-tiered main memory</title>
		<author>
			<persName><forename type="first">Neha</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="631" to="644" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Remote memory in the age of fast networks</title>
		<author>
			<persName><forename type="first">Marcos</forename><forename type="middle">K</forename><surname>Aguilera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Calciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Deguillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayneel</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratap</forename><surname>Subrahmanyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lalith</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiran</forename><surname>Tati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajesh</forename><surname>Venkatasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Symposium on Cloud Computing</title>
				<meeting>the 2017 Symposium on Cloud Computing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="121" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Effectively Prefetching Remote Memory with Leap</title>
		<author>
			<persName><forename type="first">Al</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mosharaf</forename><surname>Maruf</surname></persName>
		</author>
		<author>
			<persName><surname>Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 USENIX Annual Technical Conference (USENIX ATC 20</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="843" to="857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Can far memory improve job throughput</title>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Amaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Branner-Augmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcos</forename><forename type="middle">K</forename><surname>Aguilera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurojit</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvia</forename><surname>Ratnasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Shenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth European Conference on Computer Systems</title>
				<meeting>the Fifteenth European Conference on Computer Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Workload analysis of a large-scale key-value store</title>
		<author>
			<persName><forename type="first">Berk</forename><surname>Atikoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuehai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eitan</forename><surname>Frachtenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Paleczny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGMETRICS Performance Evaluation Review</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="53" to="64" />
			<date type="published" when="2012">2012</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A study of replacement algorithms for a virtualstorage computer</title>
		<author>
			<persName><forename type="first">A</forename><surname>Laszlo</surname></persName>
		</author>
		<author>
			<persName><surname>Belady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Systems journal</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="78" to="101" />
			<date type="published" when="1966">1966. 1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Intel¬Æ Omni-path architecture: Enabling scalable, high performance fabrics</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Mark S Birrittella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ram</forename><surname>Debbage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Huggahalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kunz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Lovett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><forename type="middle">D</forename><surname>Rimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Underwood</surname></persName>
		</author>
		<author>
			<persName><surname>Zak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE 23rd Annual Symposium on High-Performance Interconnects</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Understanding the Linux Kernel: from I/O ports to process management</title>
		<author>
			<persName><forename type="first">P</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Bovet</surname></persName>
		</author>
		<author>
			<persName><surname>Cesati</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>O&apos;Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rethinking software runtimes for disaggregated memory</title>
		<author>
			<persName><forename type="first">Irina</forename><surname>Calciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Talha Imran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanidhya</forename><surname>Puddu</surname></persName>
		</author>
		<author>
			<persName><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Maruf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aasheesh</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><surname>Kolli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting>the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="79" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">CCIX Base Specification Revision 1.1. Version 1.0</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ippokratis Pandis, and Radu Stoica</title>
		<author>
			<persName><forename type="first">Shimin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manos</forename><surname>Athanassoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPC-E vs. TPC-C: characterizing the new TPC-E benchmark via an I/O comparison study</title>
				<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="5" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">AutoNUMA: the other approach to NUMA scheduling</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Corbet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
	<note type="report_type">LWN. net</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">System software for persistent memory</title>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Subramanya R Dulloor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anil</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Keshavamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dheeraj</forename><surname>Lantz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajesh</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName><surname>Jackson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth European Conference on Computer Systems</title>
				<meeting>the Ninth European Conference on Computer Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Data tiering in heterogeneous memory systems</title>
		<author>
			<persName><forename type="first">Amitabha</forename><surname>Subramanya R Dulloor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheguang</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narayanan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadathur</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajesh</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><surname>Schwan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh European Conference on Computer Systems</title>
				<meeting>the Eleventh European Conference on Computer Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distributed caching with memcached</title>
		<author>
			<persName><forename type="first">Brad</forename><surname>Fitzpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linux journal</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2004">2004. 2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Network Requirements for Resource Disaggregation</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sagar</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><surname>Karandikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangjin</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachit</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvia</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Ratnasamy</surname></persName>
		</author>
		<author>
			<persName><surname>Shenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="249" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Large pages may be harmful on NUMA systems</title>
		<author>
			<persName><forename type="first">Fabien</forename><surname>Gaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Lepers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremie</forename><surname>Decouchant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Fuston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Fedorova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivien</forename><surname>Qu√©ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient Memory Disaggregation with Infiniswap</title>
		<author>
			<persName><forename type="first">Juncheng</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngmoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mosharaf</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><surname>Kang G Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="649" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Intel¬Æ 64 and ia-32 architectures software developer&apos;s manual</title>
		<author>
			<persName><forename type="first">Part</forename><surname>Guide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">System programming Guide, Part</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Heterovisor: Exploiting resource heterogeneity to enhance the elasticity of cloud platforms</title>
		<author>
			<persName><forename type="first">Vishal</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Schwan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="79" to="92" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Basic performance measurements of the intel optane DC persistent memory module</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Izraelevitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juno</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirsaman</forename><surname>Memaripour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><forename type="middle">Joon</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><surname>Subramanya R Dulloor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.05714</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Heteroos: Os design for heterogeneous memory management in datacenter</title>
		<author>
			<persName><forename type="first">Sudarsun</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ada</forename><surname>Gavrilovska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishal</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Schwan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture</title>
				<meeting>the 44th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="521" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Radiant: efficient page table management for tiered memory systems</title>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravinda</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sreenivas</forename><surname>Smruti R Sarangi</surname></persName>
		</author>
		<author>
			<persName><surname>Subramoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM SIGPLAN International Symposium on Memory Management</title>
				<meeting>the 2021 ACM SIGPLAN International Symposium on Memory Management</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="66" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Coordinated and efficient huge page management with ingens</title>
		<author>
			<persName><forename type="first">Youngjin</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangchen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Rossbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmett</forename><surname>Witchel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="705" to="721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Mutilate: high-performance memcached load generator</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Leverich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Utility-based hybrid memory management</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saugata</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongmoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Cluster Computing (CLUSTER)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="152" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Disaggregated memory for expansion and sharing in blade servers</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jichuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Mudge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">K</forename><surname>Parthasarathy Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="267" to="278" />
			<date type="published" when="2009">2009</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pin: building customized program analysis tools with dynamic instrumentation</title>
		<author>
			<persName><forename type="first">Chi-Keung</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Muth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harish</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artur</forename><surname>Klauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Lowney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><forename type="middle">Janapa</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Hazelwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acm sigplan notices</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Introducing the graph 500</title>
		<author>
			<persName><forename type="first">Kyle</forename><forename type="middle">B</forename><surname>Richard C Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">W</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">A</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><surname>Ang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cray Users Group (CUG)</title>
				<imprint>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="page" from="45" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Welcome to zombieland: Practical and energy-efficient memory disaggregation in a datacenter</title>
		<author>
			<persName><forename type="first">Vlad</forename><surname>Nitu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Teabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth EuroSys Conference</title>
				<meeting>the Thirteenth EuroSys Conference</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
	<note>Alain Tchana, Canturk Isci, and Daniel Hagimont</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scale-out NUMA</title>
		<author>
			<persName><forename type="first">Stanko</forename><surname>Novakovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Daglis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Bugnion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Grot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hawkeye: Efficient fine-grained os support for huge pages</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Panwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sorav</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><surname>Gopinath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting>the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="347" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Making huge pages actually useful</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Panwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravinda</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><surname>Gopinath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting>the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="679" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Data cache management using frequency-based replacement</title>
		<author>
			<persName><forename type="first">T</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murthy</forename><forename type="middle">V</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><surname>Devarakonda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems</title>
				<meeting>the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="134" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">AIFM: High-Performance, Application-Integrated Far Memory</title>
		<author>
			<persName><forename type="first">Zhenyuan</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malte</forename><surname>Schwarzkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcos</forename><forename type="middle">K</forename><surname>Aguilera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Belay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th USENIX Symposium on Operating Systems Design and Implementation</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="315" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">LegoOS: A Disseminated, Distributed OS for Hardware Resource Disaggregation</title>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiying</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th USENIX Symposium on Operating Systems Design and Implementation</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="69" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Shoal: A network architecture for disaggregated racks</title>
		<author>
			<persName><forename type="first">Vishal</forename><surname>Shrivastav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asaf</forename><surname>Valadarsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hitesh</forename><surname>Ballani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ki</forename><forename type="middle">Suh</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachit</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hakim</forename><surname>Weatherspoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th USENIX Symposium on Networked Systems Design and Implementation</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="255" to="270" />
		</imprint>
	</monogr>
	<note>NSDI 19</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Ligra: a lightweight graph processing framework for shared memory</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Shun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><forename type="middle">E</forename><surname>Blelloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Sigplan Notices</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="135" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The VoltDB Main Memory DBMS</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Stonebraker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Weisberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Eng. Bull</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="21" to="27" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Karthik</forename><surname>V Viswanathan</surname></persName>
		</author>
		<author>
			<persName><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><surname>Willhalm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intel memory latency checker. Intel Corporation</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Semeru: A Memory-Disaggregated Managed Runtime</title>
		<author>
			<persName><forename type="first">Chenxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyuan</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Bond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Netravali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miryung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoqing</forename><forename type="middle">Harry</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th USENIX Symposium on Operating Systems Design and Implementation</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="261" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Nimble Page Management for Tiered Memory Systems</title>
		<author>
			<persName><forename type="first">Zi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lustig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Nellans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting>the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="331" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">An empirical guide to the behavior and use of scalable persistent memory</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juno</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morteza</forename><surname>Hoseinzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Izraelevitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Swanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th USENIX Conference on File and Storage Technologies</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="169" to="182" />
		</imprint>
	</monogr>
	<note>FAST 20</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
