<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploiting Locality in Graph Analytics through Hardware-Accelerated Traversal Scheduling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Anurag</forename><surname>Mukkara</surname></persName>
							<email>anuragm@csail.mit.edu</email>
						</author>
						<author>
							<persName><forename type="first">Nathan</forename><surname>Beckmann</surname></persName>
							<email>beckmann@cs.cmu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Maleen</forename><surname>Abeydeera</surname></persName>
							<email>maleen@csail.mit.edu</email>
						</author>
						<author>
							<persName><forename type="first">Xiaosong</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Sanchez</surname></persName>
							<email>sanchez@csail.mit.edu</email>
						</author>
						<author>
							<persName><forename type="first">Mit</forename><surname>Csail</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Cmu</forename><surname>Scs</surname></persName>
						</author>
						<author>
							<persName><forename type="first">‡</forename><surname>Qcri</surname></persName>
						</author>
						<title level="a" type="main">Exploiting Locality in Graph Analytics through Hardware-Accelerated Traversal Scheduling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EB69ECAD15D03479B91919910B855318</idno>
					<idno type="DOI">10.1109/MICRO.2018.00010</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>graph analytics</term>
					<term>multicore</term>
					<term>caches</term>
					<term>locality</term>
					<term>scheduling</term>
					<term>prefetching</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph processing is increasingly bottlenecked by main memory accesses. On-chip caches are of little help because the irregular structure of graphs causes seemingly random memory references. However, most real-world graphs offer significant potential locality-it is just hard to predict ahead of time. In practice, graphs have well-connected regions where relatively few vertices share edges with many common neighbors. If these vertices were processed together, graph processing would enjoy significant data reuse. Hence, a graph's traversal schedule largely determines its locality.</p><p>This paper explores online traversal scheduling strategies that exploit the community structure of real-world graphs to improve locality. Software graph processing frameworks use simple, locality-oblivious scheduling because, on general-purpose cores, the benefits of locality-aware scheduling are outweighed by its overheads. Software frameworks rely on offline preprocessing to improve locality. Unfortunately, preprocessing is so expensive that its costs often negate any benefits from improved locality. Recent graph processing accelerators have inherited this design. Our insight is that this misses an opportunity: Hardware acceleration allows for more sophisticated, online locality-aware scheduling than can be realized in software, letting systems significantly improve locality without any preprocessing.</p><p>To exploit this insight, we present bounded depth-first scheduling (BDFS), a simple online locality-aware scheduling strategy. BDFS restricts each core to explore one small, connected region of the graph at a time, improving locality on graphs with good community structure. We then present HATS, a hardwareaccelerated traversal scheduler that adds just 0.4% area and 0.2% power over general-purpose cores.</p><p>We evaluate BDFS and HATS on several algorithms using large real-world graphs. On a simulated 16-core system, BDFS reduces main memory accesses by up to 2.4× and by 30% on average. However, BDFS is too expensive in software and degrades performance by 21% on average. HATS eliminates these overheads, allowing BDFS to improve performance by 83% on average (up to 3.1×) over a locality-oblivious software implementation and by 31% on average (up to 2.1×) over specialized prefetchers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Graph analytics is an increasingly important workload domain. While graph algorithms are diverse, most have a common characteristic: they are dominated by expensive main memory accesses. Three factors conspire to make graph algorithms memory-bound. First, these algorithms have low compute-to-communication ratio, as they execute very few instructions (usually few 10s) for each vertex or edge they process. Second, they suffer from poor temporal locality, as the irregular structure of graphs results in seemingly random accesses that are hard to predict ahead of time. Third, they suffer from poor spatial locality, as they perform many sparse accesses to small (e.g., <ref type="bibr" target="#b3">4</ref>-or 8-byte) objects.</p><p>The conventional wisdom has been that graph algorithms have essentially random accesses <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b30">31]</ref>. This misconception partially stems from limited evaluations that use synthetic, randomly generated graphs. However, a more detailed analysis reveals that many real-world graphs have abundant structure. Specifically, they have strong community structure corresponding to communities that exist in some meaningful sense in the real world <ref type="bibr" target="#b28">[29]</ref>. Many real-world graphs are also scalefree, i.e., they have skewed degree distributions where a small subset of vertices are much more popular, and hence accessed more frequently, than others <ref type="bibr" target="#b5">[6]</ref>. Graph algorithms thus offer significant potential locality <ref type="bibr" target="#b6">[7]</ref>, though it is irregular and difficult to predict.</p><p>This locality can be exploited by controlling the traversal schedule, i.e., the order in which vertices and edges of the graph are processed. Current software graph processing frameworks cannot exploit this insight at runtime because online traversal scheduling is simply too expensive. When only a few instructions are executed per edge, even trivial traversal scheduling adds prohibitive overheads. Instead, software frameworks process vertices in the order they are laid out in memory <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b51">52]</ref>, a strategy we call vertex-ordered scheduling. Vertex-ordered scheduling is sensible on systems with generalpurpose cores, but it forgoes significant locality. To recover this locality, current frameworks can use offline preprocessing that changes the graph layout to improve the locality of subsequent vertex-ordered traversals <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b63">64]</ref>. Unfortunately, preprocessing is itself very expensive, and thus only makes sense on graphs that change infrequently, ruling out many important applications <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>The key idea of this paper is that hardware acceleration enables more sophisticated, online traversal scheduling, allowing systems to improve locality without expensive preprocessing. We propose HATS, which introduces a simple, specialized scheduling unit near each core that runs ahead and chooses which edges to traverse. Prior graph accelerators for FPGAs <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref> and ASICs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44]</ref> include specialized scheduling logic, but they all implement the simple, locality-oblivious vertex-ordered scheduling. HATS is the first design that exploits hardware acceleration to improve traversal scheduling itself. Specifically, we propose bounded depth-first scheduling (BDFS). In BDFS, each core explores the graph in a depthfirst fashion up to a given maximum depth. This restricts each core to explore a small, well-connected region of the graph at a time, improving temporal locality on graphs with good community structure. Prior work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13]</ref> has observed that DFS is a good technique to exploit locality and has exploited it in offline graph preprocessing <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b60">61]</ref>. BDFS is the first to exploit DFS for online locality-aware scheduling. HATS implements BDFS with simple hardware similar to previous indirect prefetchers <ref type="bibr" target="#b57">[58]</ref>. Unlike these prefetchers, which only hide latency, BDFS changes the traversal to improve locality and thus reduces both latency and bandwidth.</p><p>Fig. <ref type="figure">1</ref> illustrates the benefits of BDFS for the PageRank Delta algorithm <ref type="bibr" target="#b34">[35]</ref> on the uk-2002 web graph <ref type="bibr" target="#b15">[16]</ref>. BDFS reduces memory accesses by 1.8× over the vertex-ordered schedule (VO). Prior prefetchers and graph accelerators do not reduce memory accesses, since they use the same vertexordered schedule as software frameworks.</p><p>Fig. <ref type="figure" target="#fig_0">2</ref> shows the execution time of PageRank Delta on uk-2002. In software, BDFS does not improve performance because its overheads outweigh its locality benefits. But hardware acceleration reverses this situation. HATS improves VO's performance by 1.8× (VO-HATS) due to accurate prefetching that hides memory latency. But prefetching saturates memory bandwidth, so improving performance further requires reducing memory accesses. BDFS achieves this and BDFS-HATS outperforms VO-HATS by 1.5× and VO by 2.7× .</p><p>We have prototyped BDFS-HATS in RTL and evaluated it using detailed microarchitectural simulation. We consider two system configurations: one where HATS engines are implemented in hardware and another where they use on-chip reconfigurable logic (similar to the Xilinx Zynq SoC, but with high-performance cores). BDFS-HATS is easy to implement and requires just 0.14 mm 2 and 72 mW at 65 nm (or 3.2 K FPGA LUTs). This translates to 0.4% area and 0.2% power overhead over a general-purpose core. We evaluate HATS on five important graph algorithms, processing real-world graphs whose working sets are much larger than the on-chip cache capacity. On a 16-core system, BDFS-HATS reduces main memory accesses by up to 2.4× and by 30% on average, and improves performance by up to 3.1× and by 83% on average. HATS thus gives a practical way to improve the locality of graph processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND AND MOTIVATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Current graph processing frameworks</head><p>Software graph processing frameworks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b62">63]</ref> provide a simple interface that lets application programmers specify algorithm-specific logic to perform operations on graph vertices and edges. The runtime is then responsible for scheduling and performing these operations. The runtime tracks which vertices are active in each iteration and performs algorithm-specific operations on them until there are no more active vertices or a termination condition (e.g., number of iterations) is reached. We assume a Bulk Synchronous Parallel (BSP) <ref type="bibr" target="#b53">[54]</ref> model, where updates to algorithm-specific data are made visible only at the end of each iteration.</p><p>Many graph algorithms are unordered and the runtime has complete freedom on how to schedule the processing of active edges in each iteration. Such scheduling does not affect the correctness of the algorithm, but has a large impact on locality. Before analyzing the locality tradeoffs of scheduling, we first describe these frameworks in more detail. Graph format: Most graph processing frameworks use the compressed sparse row (CSR) format, or its variations, for its simplicity and space efficiency <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b62">63]</ref>. As Fig. <ref type="figure" target="#fig_1">3</ref> shows, CSR uses two arrays, offset and neighbor, to store the graph structure. For each vertex id, the offset array stores where its neighbors begin in the neighbor array. Hence, each vertex v has edges to each neighbor</p><formula xml:id="formula_0">[i] for i = offset[v] → offset[v + 1]</formula><p>. The neighbor array stores the vertex id of each neighbor, so it has as many entries as edges in the graph. For weighted graphs, the neighbor array also stores the weight of each edge. Algorithm-specific data is stored in a separate vertex data array. For example, in PageRank, vertex data stores the score of each vertex.</p><p>Graph algorithms can perform pullor push-based traversals. In pull-based traversals, the CSR format encodes the incoming edges of each vertex, as Fig. <ref type="figure" target="#fig_1">3</ref> shows, and each processed vertex (the destination vertex) pulls updates from its in-neighbors (sources). In push-based traversals, the CSR format encodes the outgoing edges of each vertex, and each processed vertex (source) pushes updates to its out-neighbors (destinations). Vertex-ordered scheduling: State-of-the-art graph processing frameworks <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b62">63]</ref> follow a vertex-ordered schedule (VO), a simple technique that achieves spatial locality in accesses to edges but suffers from poor temporal and spatial locality on accesses to neighbor vertices.</p><p>The vertex-ordered schedule simply processes the active vertices in order of vertex id, and processes all the edges of each vertex consecutively, as specified by the graph layout. Processing an edge usually involves accessing the vertexdata of both the current and neighbor vertices. Listing 1 shows pseudocode for a single iteration of PageRank following the vertex-ordered schedule. We show the pull version, where destination vertices pull updates from their in-neighbors. When the in-memory graph layout (i.e., offset and neighbor arrays) does not correlate with the graph's community structure, the vertex-ordered schedule suffers from poor temporal locality. Consider the example graph shown in Fig. <ref type="figure" target="#fig_2">4</ref>. The graph has two wellconnected regions that are weakly connected to each other. To maximize temporal locality, all the vertices and edges in one region should be processed before moving to the next. But if the vertices of the two regions are interleaved in the graph layout as in Fig. <ref type="figure" target="#fig_2">4</ref>, the vertex-ordered schedule alternates between regions, yielding poor temporal locality. Preprocessing improves locality but is expensive: Prior work has proposed several graph preprocessing techniques to improve locality <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b63">64]</ref>. These techniques change the order in which vertices are stored so that closely connected communities are stored near each other in memory. This improves the temporal and spatial locality of the vertex-ordered schedule: it improves temporal locality by placing a vertex's neighbors close together, so accesses from those neighbors to the vertex happen nearby in time; and it improves spatial locality by placing related vertices in the same cache line.</p><p>For example, for the graph in Fig. <ref type="figure" target="#fig_2">4</ref>, preprocessing would analyze the graph structure, identify the two regions and modify the layout to place the vertices in the first region before those in the second. When the vertex-ordered schedule is used with the modified layout, it closely follows the community structure, fully processing the first region before moving to the second.</p><p>Although preprocessing improves locality, it is very expensive. Rewriting the graph requires several passes over the full edge list. As a result, preprocessing often takes longer than the graph algorithm itself, making it impractical for many important use cases <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b38">39]</ref>. Preprocessing costs can be amortized if the same graph is reused many times, but in many applications the graph changes over time or is produced by another algorithm, and is used once or at most a few times <ref type="bibr" target="#b33">[34]</ref>.</p><p>Fig. <ref type="figure" target="#fig_4">5</ref> illustrates this for one iteration of the PageRank algorithm on the uk-2002 graph <ref type="bibr" target="#b15">[16]</ref>. We compare (1) The vertex-ordered (VO) schedule (2) Slicing <ref type="bibr" target="#b21">[22]</ref>, a relatively cheap preprocessing technique that ignores graph structure, and (3) GOrder <ref type="bibr" target="#b54">[55]</ref>, an expensive preprocessing technique that heavily exploits graph structure. Because GOrder takes too long to simulate, we measure its preprocessing overhead on an Intel Xeon E5-2658 v3 (Haswell) processor running at 2.2 GHz with a 30 MB LLC.  Although both Slicing and GOrder reduce memory accesses significantly and improve PageRank's runtime over the vertexordered schedule, they incur significant preprocessing time. When including preprocessing time, these techniques are beneficial only when the algorithm converges in more than 10 and 5440 iterations, respectively.</p><p>Several preprocessing techniques exploit the locality benefits of depth-first search (DFS) and breadth-first search (BFS) traversals <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b60">61]</ref>. Children-DFS <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b54">55]</ref> partitions the graph by using a variant of DFS that seeks to group the neighbors of each vertex. PathGraph <ref type="bibr" target="#b58">[59]</ref> partitions the graph by performing local breadth-first traversals while limiting the partition sizes and relabels the vertices in each partition in DFS order. FBSGraph <ref type="bibr" target="#b60">[61]</ref> is a distributed framework that uses pathcentric partitioning and scheduling to improve convergence rate of asynchronous graph algorithms. It leverages the necessary graph partitioning pass and only improves temporal locality of vertex data since it does not change the graph layout.</p><p>Unlike these techniques, BDFS improves temporal locality without preprocessing. However, because BDFS does not change the graph's layout, it does not improve spatial locality. Online heuristics to improve locality: Motivated by the high costs of preprocessing, prior work has explored alternative, cheaper runtime techniques to improve locality. Milk <ref type="bibr" target="#b25">[26]</ref> and Propagation Blocking <ref type="bibr" target="#b7">[8]</ref> translate irregular indirect memory references into batches of efficient sequential DRAM accesses. These techniques are a spatial locality optimization, and only benefit algorithms with small vertex objects. By contrast, BDFS exploits the graph's community structure to improve temporal locality and benefits algorithms with large or small vertex objects. While these techniques are effective with unstructured (i.e., random) graphs, they forgo significant temporal locality for graphs with good community structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Prior hardware techniques to accelerate graph processing</head><p>Indirect prefetching: Conventional stream or strided prefetchers do not capture the indirect memory access patterns of graph algorithms. IMP <ref type="bibr" target="#b57">[58]</ref> is a hardware prefetcher that dynamically identifies and prefetches indirect memory access patterns, without requiring any application-specific information. Similarly, Ainsworth and Jones <ref type="bibr" target="#b2">[3]</ref> propose a specialized prefetcher that uses information about an application's data structures to prefetch indirect memory accesses.</p><p>These prefetchers all assume a vertex-ordered schedule and improve performance by hiding memory access latency. They easily saturate memory bandwidth and become bandwidthbound. By contrast, BDFS changes the traversal schedule to reduce bandwidth demand, allowing it to outperform perfect prefetching of a vertex-ordered schedule. HATS fetches graph data ahead of the core, but this is more similar to how graph accelerators non-speculatively fetch data than to indirect prefetchers like IMP, which predict the access pattern outside the core to issue speculative prefetches. Graph accelerators: Recent work has proposed specialized graph-processing accelerators for both FPGAs <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref> and ASICs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b59">60]</ref>. While these accelerators introduce specialized scheduling logic, they implement the same vertexordered schedule used by software algorithms and likewise rely on expensive preprocessing to improve locality <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b59">60]</ref>. The premise of our paper is that this misses an opportunity: specialization enables online locality-aware schedules that achieve most of the benefits of preprocessing without its overheads.</p><p>Beyond scheduling, these accelerators use both compute and memory system specialization to achieve large performance and energy efficiency gains. Our paper complements this prior work by using locality-aware scheduling to make better use of limited on-chip cache capacity. Although we describe our techniques in the context of a general-purpose system, we expect it to be more beneficial on accelerators that are even more bottlenecked on memory accesses. Decoupled access-execute: HATS takes inspiration from decoupled access-execute (DAE) architectures <ref type="bibr" target="#b48">[49]</ref>, where an access core performs all memory operations and an execute core performs all compute operations. Access and execute cores communicate through queues, allowing the access core to run ahead.</p><p>In some aspects, HATS is similar to an access core: it is decoupled from the main core through a queue, and runs ahead of it, exposing abundant memory-level parallelism. However, HATS is specialized to graph traversals, making it much cheaper and faster than a programmable access core. And unlike DAE, the main core still performs memory accesses instead of communicating them to HATS. This lets HATS focus on handling the traversal of the graph. Also, unlike in conventional DAE, communication between HATS and the main core is one-sided, letting HATS run far ahead of the core and avoiding the performance bottlenecks of DAE, where two-way communication often caused loss of decoupling <ref type="bibr" target="#b52">[53]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. IMPROVING LOCALITY WITH BDFS</head><p>Our goal is to achieve most of the benefits of preprocessing while avoiding its overheads. We improve temporal locality by scheduling edges at runtime to match the graph's community structure, without modifying the graph layout. This section describes our basic technique, BDFS, and the next describes our hardware implementation of this technique, HATS.</p><p>BDFS traverses the graph by performing a series of bounded depth-first searches, each of which visits a region of connected vertices. Bounded depth-first search is used in several contexts, such as iterative deepening <ref type="bibr" target="#b26">[27]</ref> and search and optimization techniques <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b50">51]</ref>. Moreover, as described in Sec. II-A, several preprocessing algorithms leverage DFS to improve locality <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b60">61]</ref>. However, to the best of our knowledge, we are the first to use BDFS for online locality-aware scheduling of graph traversals.</p><p>We first describe a sequential implementation of BDFS, analyze its locality, and then discuss its parallel implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. BDFS algorithm</head><p>Listing 2 shows the pseudocode for PageRank using a recursive implementation of BDFS. BDFS uses an active bitvector to track the vertices that are not yet processed. Listing 2 shows a version of PageRank where all the vertices are active in each iteration <ref type="bibr" target="#b44">[45]</ref>, so the bitvector is initialized to all ones. 1 BDFS starts processing at the first vertex (id 0). Thereafter, it chooses the next vertex to process from the neighbors of the current vertex, ignoring inactive vertices. This exploration proceeds in a depth-first fashion, always staying within maxDepth levels away from the root vertex. Once the exploration from a root vertex is finished, BDFS scans the active bitvector to find the next unvisited vertex. This repeats until all vertices are visited. 1 There are more efficient versions that do not process all vertices on each iteration. We later evaluate PageRank Delta, which performs this optimization. Listing 2: PageRank implementation using BDFS. yield() returns a value to the caller, but resumes at that point when the callee is next invoked. Fig. <ref type="figure" target="#fig_5">6</ref> shows the order in which BDFS processes vertices in the example graph using maxDepth of 10. (This is a pull-based traversal so BDFS traverses incoming edges, e.g., from vertex 0 to 4.) Unlike the vertex-ordered schedule, BDFS tends to process close-knit regions together. BDFS improves temporal locality of accesses to vertex data for two reasons. First, neighbors of a given vertex are more likely to share neighbors, so processing them together naturally exploits the community structure of realworld graphs. Second, since processing an edge involves accessing the vertex data of both the currently processed vertex and its neighbor, processing one of the neighbor vertices next results in at least one access to already cached data. Scheduling overheads: The main scheduling structures in BDFS are a LIFO stack and the active bitvector. BDFS requires only a small stack, which causes near-zero main memory accesses. Although the bitvector gets irregular accesses, it is much smaller than vertex data. For example, in PageRank the bitvector is 128× smaller than vertex data, which stores 16 B per vertex.</p><p>The real overheads of BDFS are not extra memory accesses, but the scheduling logic in Listing 2 to find the next vertex. Although BDFS has linear-time complexity (O(#Edges + #Vertices)), since most graph algorithms execute few instructions per edge, it is relatively expensive in software: BDFS not only executes 2-3× more instructions than VO, but these extra instructions have data-dependent branches that limit instruction-level parallelism. These overheads outweigh the locality improvements of BDFS, motivating HATS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Analysis of access patterns</head><p>Fig. <ref type="figure" target="#fig_6">7</ref> compares the memory access patterns of the vertexordered (VO) schedule (Listing 1) and BDFS (Listing 2). We show accesses to the neighbor and vertex data arrays.</p><p>VO processes vertices and their edges sequentially, which results in good spatial locality on the offset and neighbor arrays, and in vertex data accesses for the currently processed vertex. While the neighbor array is often the largest data structure in the graph, each cache line has many elements (typically 16), which amortizes their fetches well. However, VO suffers from poor temporal and spatial locality on vertex data accesses for neighbor vertices. These accesses dominate misses. Fig. <ref type="figure" target="#fig_7">8</ref> illustrates this by showing the breakdown of main memory accesses to different data structures in PageRank: 86% of main memory accesses are to neighbor vertex data.</p><p>Fig. <ref type="figure" target="#fig_6">7</ref> shows that BDFS improves temporal locality in neighbor vertex data accesses by processing communities together. However, it reduces spatial locality in offset and neighbor array accesses. In BDFS, the first access to a vertex's slice of the neighbor array often misses. Fortunately, accesses to the remaining neighbors enjoy the same spatial locality as VO. Fig. <ref type="figure" target="#fig_7">8</ref> shows this is a good tradeoff: neighbor vertex data misses are almost 5× lower, and while offset and neighbor misses increase, BDFS reduces memory accesses by 2.2×.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. BDFS does not require tuning the maximum depth</head><p>Alternative search strategies: An alternative to BDFS is bounded breadth-first scheduling (BBFS). BDFS outperforms BBFS and is also cheaper to implement. DFS has better locality than BFS <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b12">13]</ref>, and DFS works well with a small stack while BFS requires a large FIFO queue (up to the entire graph).</p><p>Fig. <ref type="figure" target="#fig_10">9</ref> illustrates this for PageRank. It shows the memory accesses of BDFS and BBFS as the fringe (BDFS stack or BBFS queue) grows. Memory accesses are normalized to the vertex-ordered schedule. BDFS outperforms BBFS at all fringe sizes and achieves near-peak performance with a 10-element fringe, whereas BBFS needs about a 100-element fringe to be effective. All the graphs we evaluate show similar trends. BDFS is insensitive to stack depth: Fig. <ref type="figure" target="#fig_10">9</ref> shows that BDFS's performance is flat after a stack depth of 5-10. Smaller fringes cause more misses because they traverse smaller communities   that use only a fraction of the cache. For example, with an average degree of 4 neighbors/vertex, a depth of 4 traverses only about 4 4 = 256 vertices, whereas a depth of 10 traverses about 4 10 = 1 M vertices. However, the converse is not true: deeper stacks do not add misses even if they result in huge traversals with many more vertices than the cache can hold. This stems from DFS's divideand-conquer nature <ref type="bibr" target="#b17">[18]</ref>. Suppose that, for a given graph, a stack of depth D yields the largest communities that fit on cache. Suppose we instead use depth D + 1. If the root node has N neighbors, this is equivalent to performing N depth-D traversals in sequence, each of which fits in cache. By induction, BDFS does not overwhelm the cache with a deeper stack.</p><p>This observation yields two nice properties. First, BDFS needs no tuning. Rather than analyzing the graph and figuring out the right stack depth, we simply use a fixed depth in hardware that is large enough to yield large traversals even with small degrees. Second, BDFS should yield good locality at different cache levels, regardless of their size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Parallel BDFS</head><p>We parallelize BDFS by evenly dividing the active bitvector across threads. Each thread then begins independent BDFS traversals through its chunk of the vertices, as in Listing 2. The only change is that operations on the active bitvector are done atomically (e.g., test-and-clear) to avoid repeating work. Finally, we use work-stealing <ref type="bibr" target="#b10">[11]</ref> to avoid load imbalance: when a thread finishes its chunk it tries to steal half of another thread's remaining vertices.</p><p>We tried a number of more sophisticated parallelization strategies that seek to keep all threads exploring within the same community of the graph. We found that, on most graphs, these added synchronization overheads without providing much benefit over our simple work-stealing approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. HATS: HARDWARE-ACCELERATED TRAVERSAL SCHEDULING</head><p>BDFS effectively reduces cache misses, but when implemented in software, its overheads negate the benefits of its higher locality. To address this problem, we present hardwareaccelerated traversal scheduling (HATS). HATS is a simple, specialized engine that performs traversal scheduling. HATS enables sophisticated scheduling strategies like BDFS. System architecture: Fig. <ref type="figure" target="#fig_11">10</ref> shows the system architecture we use in this paper. Each core is augmented with a HATS engine, which it configures to perform the traversal (e.g., passing in the addresses of CSR structures). Each HATS engine runs ahead of its core and communicates edges to the core through a FIFO buffer. Our design effectively offloads the traversal scheduling portion of the graph algorithm to the HATS engines, and uses cores exclusively for edge and vertex processing. For example, in Listing 2, the core executes the per-edge operations inside the PageRank() function, while the HATS engine executes everything else.</p><p>We propose and evaluate HATS on general-purpose processors, where HATS is implemented as either fixed-function hardware or using on-chip reconfigurable logic. We focus on general-purpose processors for two reasons. First, traversal scheduling is needed by all graph algorithms, so it is natural to specialize this common part and leave algorithm-specific edge and vertex processing to programmable cores. Second, specialized traversal schedulers impose negligible system-wide overheads, similar in cost to prior indirect prefetchers. And unlike prefetchers, HATS reduces both memory latency and bandwidth (Sec. II-B). HATS thus adds a small dose of specialization to get a large performance boost for an important application domain, without sacrificing the programmability and low entry cost of general-purpose processors.</p><p>That said, HATS can be applied to other system architectures, e.g., by replacing the general-purpose cores with an algorithmspecific accelerator. Generality: HATS supports both push-and pull-based traversals, and all-active and non-all-active algorithms. This lets HATS accelerate the vast majority of graph processing algorithms-the full spectrum of what state-of-the-art frameworks like Ligra support.</p><p>HATS assumes a CSR graph format, which is by far the most commonly used one <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b62">63]</ref>. With small additions, HATS could support other CSR variants (e.g., DCSR <ref type="bibr" target="#b11">[12]</ref>). Moreover, the reconfigurable logic implementation of HATS would allow supporting other graph formats with no overheads.</p><p>In the remainder of this section, we explain the HATS interface and operation in detail, which is common to all HATS variants. We then describe HATS implementations of VO and BDFS traversals, and compare the area and power overheads of the ASIC and FPGA implementations of HATS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. HATS interface and operation</head><p>HATS only accelerates traversal scheduling and leaves all other responsibilities to cores, including initialization, edge and vertex processing, and load balancing. Operation: Regardless of the traversal scheduling strategy implemented by HATS (VO or BDFS), each traversal (e.g., one iteration of PageRank) proceeds in the following steps: 1. Initialization: Software first initializes all the required data structures, including all graph data and, if needed, the active bitvector, which specifies the set of vertices to visit. The need for this bitvector depends on the graph algorithm and traversal schedule: VO-HATS (Sec. IV-B) uses an active bitvector only for algorithms where not all vertices are active each iteration (e.g., BFS), while BDFS-HATS (Sec. IV-C) always uses an active bitvector to avoid processing vertices multiple times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">HATS configuration:</head><p>Each thread then configures its own HATS unit by conveying the following information: 1) The base addresses and sizes of graph data structures: offset, neighbor, and vertex data arrays, and active bitvector.</p><p>2) The type of traversal (push or pull).</p><p>3) The chunk of vertices that the HATS is responsible for (start and end vertex ids). This configuration data is written using memory-mapped registers (e.g., like a DMA engine is configured). After the core writes this configuration, HATS starts the traversal. 3. Processing: During traversals, HATS reads the offset and neighbor arrays, as well as the active bitvector, if used. For BDFS, HATS also performs updates to the active bitvector. Finally, it prefetches vertex data.</p><p>As HATS finds unvisited active vertices, it fills its FIFO buffer with edges (source and destination vertex ids) for the core to process. The core uses a fetch edge instruction to fetch edges from the buffer (this is the only new instruction). fetch edge returns the source and destination ids in registers. Software takes two extra instructions to translate these ids to vertex data addresses. If HATS has finished traversing its assigned chunk of vertices, fetch edge returns (-1, -1). If the FIFO is empty, fetch edge stalls the core. If it fills up, HATS's traversal stalls. HATS is transparent to applications: We expose the above functionality to the software graph processing framework through a simple low-level API consisting of two methods: hats configure(...) performs the configuration step and hats fetch edge() translates to a fetch edge instruction. Graph algorithms need not deal with this API: we code all our algorithms to a highly optimized, Ligra-like graph processing framework (Sec. V-A). Only the framework needs to be modified to use HATS-application code is unchanged. Parallelism and load-balancing: Parallel operation is similar to the software BDFS implementation (Sec. III-D): vertices are divided into as many chunks as threads, and each HATS engine is responsible for scanning a separate chunk. We perform loadbalancing using work-stealing: if a HATS engine finishes its chunk early, its thread interrupts a randomly chosen thread, which donates half of the remaining chunk in its HATS engine. We use the same termination algorithm as Cilk <ref type="bibr" target="#b18">[19]</ref>. Handling preemption: Because HATS does work on behalf of the thread, some of its state is architecturally visible and must be considered on preemption events. If the OS deschedules a thread, it quiesces the HATS engine and saves this architectural state (which includes the remainder of the chunk and base addresses of all data structures). When the thread is rescheduled, its core's HATS is configured using this state. Note that this is needed only when the thread is descheduled, not when taking exceptions or system calls (similar to how FPU state is not saved when entering into the kernel). It is thus a rare event.</p><p>Virtual memory: Finally, HATS operates on virtual addresses. Like prior indirect prefetchers, HATS leverages the core's address-translation machinery <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b57">58]</ref>. Unlike indirect prefetchers, HATS does not monitor the core's cache accesses. Since we place HATS at the core's L2, we use the L2 TLB.</p><p>HATS may cause a page fault, which is handled as in prior indirect prefetchers: the core is interrupted and the OS page fault handler invoked. The HATS engine stalls until the page fault handler completes. Once the page fault handler finishes, core and HATS engine resume normal execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. VO-HATS implementation</head><p>We now describe the design of the HATS engine when using the vertex-ordered schedule. We describe its operation assuming a push-based traversal. Sec. IV-D discusses the changes needed for pull-based traversals. Our VO-HATS design uses a simple pipelined implementation, illustrated in Fig. <ref type="figure" target="#fig_12">11</ref>. Each pipeline stage corresponds to a particular step in the fetching of graph data: 1) Scan holds the current and last vertex ids of the HATS chunk. For an all-active algorithm, Scan simply outputs the current vertex id each cycle, and increments it. If the algorithm is not all-active, Scan loads the active bitvector line by line and outputs the ids of active vertices. 2) Fetch offsets takes a vertex id as input and outputs its start and end offsets, which it loads from the offsets array. 3) Fetch neighbors takes the start and end offsets of a single vertex as input and outputs its neighbor ids, which it loads from the neighbors array. The vertex and its neighbor ids are then queued in the FIFO buffer. 4) Prefetch issues prefetches for the vertex and its neighbors' vertex data.</p><p>To allow enough memory-level parallelism, these stages are decoupled using small FIFOs. In practice, we find that allowing the Scan and Fetch neighbors stages to each request up to two cache lines in parallel suffices to keep the FIFO full.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. BDFS-HATS implementation</head><p>Our implementation of BDFS-HATS shares many common elements with VO-HATS, but has additional logic to perform data-dependent traversals. Fig. <ref type="figure" target="#fig_13">12</ref> shows its design. We first explain its basic operation, and then the optimizations required to achieve good performance. Basic operation: The main component of the BDFS scheduler is a fixed-depth stack. Each stack level stores the following information about a single vertex: its vertex id, current and end offsets, and a cache line worth of neighbor ids. These fields are populated as the vertex is processed. The stack is provisioned for the maximum depth of BDFS exploration (10 levels in our implementation). As discussed in Sec. III-C, it is not necessary to tune BDFS's depth-we always use the maximum depth.</p><p>Unlike VO, BDFS always uses an active bitvector, even for all-active algorithms. HATS reads this bitvector to restrict its exploration to active vertices, and updates it as it traverses the graph, clearing the vertices it decides to explore.</p><p>The traversal begins with an empty stack. As in VO, the Scan stage traverses the active bitvector and produces the next active vertex id. This vertex is immediately marked as inactive. Then the vertex serves as the root of a bounded-depth first exploration: the vertex id is stored in the first level of the stack. Its offsets are fetched, and, once known, they are used to fetch the first cache line of neighbor vertex ids. These neighbors are checked in the active bitvector, and those that are active are marked inactive and stored, along with the vertex's offsets, in its stack entry.</p><p>The traversal continues in a depth-first fashion: the first neighbor of the topmost element is used to populate the next level of the stack as above, and so on until the stack is filled.</p><p>As the depth-first traversal proceeds, newly fetched neighbor ids are used to produce edges, which are queued to the FIFO buffer. Once the stack fills up, the neighbor ids of the last level are fetched and used to produce edges, but are not traversed. When all the neighbors at a given level have been traversed, the level is cleared and the exploration continues at the next neighbor of the previous level. When all of the root's neighbors have been explored, the current region has been fully explored, and the Scan stage provides the next root vertex.</p><p>BDFS-HATS uses a finite state machine (FSM) to implement this procedure, shown in Fig. <ref type="figure" target="#fig_13">12</ref>. On each cycle, the FSM decides on the next piece of data to fetch based on the current state of the stack. Exploiting intra-traversal parallelism: Unlike VO, BDFS traversals experience more data dependences and thus more serialization. Additional optimizations are needed to exploit the parallelism within a single BDFS traversal in order to obtain enough memory-level parallelism and saturate each core.</p><p>First, we move the active bitvector check-and-clear operations off the critical path and perform them in parallel. These checks constitute a substantial fraction of the work in BDFS. Instead of checking whether a vertex should be visited eagerly, we add all vertices to the neighbor list, issue pending bitvector checks if there is nothing else to do, and mark them as active or inactive as the responses arrive. Second, all levels in the stack expand the first two active neighbors in parallel, instead of only expanding the first one. Each stack level has an additional entry, and when the topmost element is populated, its first and second active neighbors are used to populate the next level. This way, when the level's current vertex is completely explored, the data for the next vertex is already available (and as soon as we switch to it, the data for the following vertex starts being fetched). This greatly reduces the critical path at the cost of some additional storage.</p><p>With these optimizations, we find that BDFS-HATS achieves enough throughput to avoid stalling the core.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Extending HATS for pull-based traversals</head><p>We have so far described push-based traversal variants of HATS. The above designs can be easily extended to perform pull-based traversals. The key difference is when the active bitvector checks happen. In a push-based traversal, the active bitvector is checked to filter vertices before exploring their neighbors. For example, VO-HATS does this in the Scan stage. By contrast, a pull-based traversal fetches the neighbors of all the vertices in the graph, then uses the bitvector to filter inactive neighbors. Thus, adapting our VO-HATS design simply requires performing the bitvector checks after the Fetch neighbors stage instead of in the Scan stage. The BDFS design requires similar changes. Our HATS prototypes support both push-and pullbased traversals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Hardware costs ASIC implementation:</head><p>We have written Verilog RTL for both VO-HATS and BDFS-HATS engines and synthesized them using a commercial 65 nm process. Both designs meet a 1.1 GHz target frequency. Table <ref type="table" target="#tab_2">I</ref> shows their area and power consumption under typical operating conditions. Area and power are negligible when compared to those of a core in the Intel Core 2 E6750, also manufactured in 65 nm <ref type="bibr" target="#b16">[17]</ref>. Table <ref type="table" target="#tab_2">I</ref> shows that BDFS-HATS takes about 0.4% of core area and 0.2% of core TDP. VO-HATS is even cheaper.</p><p>Prior indirect prefetchers do not allow for a direct area and power comparison, but we can use their internal storage requirements as a proxy. VO-HATS requires 2.5 Kbits of storage for its internal FIFO buffers and BDFS-HATS requires 6.4 Kbits for 10 stack levels. In addition, both designs use a 1 Kbit output FIFO buffer. In comparison, IMP <ref type="bibr" target="#b57">[58]</ref> requires 5.5 Kbits of storage, so our HATS designs have about the same cost. FPGA implementation: We also synthesized the HATS designs on an FPGA platform. VO-HATS and BDFS-HATS require 1725 and 3203 LUTs respectively, as shown in Table <ref type="table" target="#tab_2">I</ref>. This is less than 2% of the total LUT count on a modest Xilinx Zynq-7045 SoC <ref type="bibr" target="#b55">[56]</ref> (state-of-the-art FPGAs have 10× more LUTs <ref type="bibr" target="#b56">[57]</ref>). Both designs meet a 220MHz target frequency, 5× slower than the ASIC implementation.</p><p>To ensure that the HATS engine can match the core's throughput at this frequency, we need more parallelism within the engine. However, we do not need to replicate the entire HATS pipeline to achieve this. We find that active bitvector checks in the Filter neighbors stage become the bottleneck when operating at a lower frequency. Thus, we only replicate the bitvector check logic and perform multiple bitvector operations in parallel (4 in our case). Sec. V-C shows that with this support, even a 220MHz design can keep the core busy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EVALUATION A. Methodology</head><p>We now present our evaluation methodology, including the simulated system, graph algorithms, and datasets we use. Simulation infrastructure: We perform microarchitectural, execution-driven simulation using zsim <ref type="bibr" target="#b46">[47]</ref>. We have implemented detailed cycle-driven models of our proposed VO and BDFS HATS designs.</p><p>We simulate a 16-core system with parameters given in Table <ref type="table" target="#tab_3">II</ref>. The system uses out-of-order cores modeled after and validated against Intel Haswell cores. Each core has private L1 and L2 caches, and all cores share a banked 32 MB last-level cache. The system has four memory controllers, like Haswell-EP systems <ref type="bibr" target="#b22">[23]</ref>. We use McPAT <ref type="bibr" target="#b29">[30]</ref> to derive the energy numbers of chip components at 22 nm, and Micron DDR3L datasheets <ref type="bibr" target="#b36">[37]</ref> to compute main memory energy. Algorithms: We use five graph algorithms from the widely used Ligra <ref type="bibr" target="#b47">[48]</ref> framework, as shown in Table <ref type="table" target="#tab_4">III</ref>. These include both all-active and non-all-active algorithms. All algorithms use objects that are much smaller than a cache line <ref type="bibr">(64 B)</ref>.</p><p>PageRank computes the relative importance of vertices in a graph, and was originally used to rank webpages <ref type="bibr" target="#b44">[45]</ref>. PageRank Delta is a variant of PageRank in which vertices are active in an iteration only if they have accumulated enough change in their PageRank score <ref type="bibr" target="#b34">[35]</ref>. Connected Components divides a graph's vertices into disjoint subsets (or components) such that there is no path between vertices belonging to different subsets <ref type="bibr" target="#b12">[13]</ref>. Radii Estimation estimates the radius of each vertex No by performing multiple parallel BFS's from a small sample of vertices <ref type="bibr" target="#b31">[32]</ref>. Maximal Independent Set finds a maximal subset of vertices such that no vertices in the subset are neighbors <ref type="bibr" target="#b9">[10]</ref>. We obtain the source code for these algorithms from Ligra <ref type="bibr" target="#b47">[48]</ref>. We adapt the scheduling code to use the HATS programming model, without modifying the per-algorithm code. We also incorporate several optimizations in the scheduling code like careful loop unrolling that yield significant speedups: our implementations outperform Ligra by up to 2.5×.</p><p>Our approach lets us start with an optimized software baseline, which is important since it affects the qualitative tradeoffs. In particular, we find that well-optimized implementations are more memory-bound and saturate bandwidth more effectively. Datasets: We use several large real-world web and social graphs detailed in Table <ref type="table" target="#tab_5">IV</ref>. These graphs are diverse (harmonic diameter: 5-38; average degree: 9-38; clustering coefficient: 0.06-0.55). With the objects sizes listed in Table <ref type="table" target="#tab_4">III</ref>, the vertex data footprint is much larger than the last-level cache. We represent graphs in memory in compressed sparse row (CSR) format.</p><p>Graph algorithms are generally executed for several iterations until a convergence condition is reached. To avoid long simulation times, we use iteration sampling: we perform detailed simulation only for every 4th iteration and fast-forward through the other iterations (after skipping initialization). This yields accurate results since the execution characteristics of all algorithms change slowly over consecutive iterations. Even with iteration sampling, we perform detailed simulation for over 100 billion instructions for the largest graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. ASIC HATS Evaluation</head><p>Main memory accesses: Fig. <ref type="figure" target="#fig_1">13</ref> shows the main memory accesses of VO and BDFS for single-threaded PageRank. Each bar shows the breakdown of accesses to different data structures as in Fig. <ref type="figure" target="#fig_5">6</ref>. This includes misses due to demand accesses and prefetches. BDFS's benefits stem from reducing neighbor vertex data misses, as explained in Sec. III-B. Fig. <ref type="figure" target="#fig_1">13</ref> shows that these benefits hold across most graphs: BDFS reduces main memory accesses significantly, by up to 2.6× and by 60% on average. Other indicates accesses to BDFS's data structures, mainly the active bitvector. These are negligible except when the bitvector does not fit in cache (for web).</p><p>BDFS reduces misses on all graphs except twi, due to twi's weak community structure. On twi, BDFS does not improve temporal locality of vertex data accesses, and adds offset and neighbor misses. Preprocessing techniques <ref type="bibr" target="#b3">[4]</ref> also show lower benefits for twi. Excluding twi, BDFS reduces memory accesses by 2× on average for PageRank. twi's weak community structure is an outlier. For example, twi has a clustering coefficient of 0.06, whereas most realworld graphs are above 0.2 <ref type="bibr" target="#b28">[29]</ref>. Therefore, we expect BDFS to be beneficial in the common case. In Sec. V-D we present Adaptive-HATS, which can detect when graphs have weak community structure and switch to a VO schedule.</p><p>Fig. <ref type="figure" target="#fig_2">14</ref> shows the main memory accesses of BDFS at 16 threads for all five algorithms. BDFS reduces memory accesses significantly for all algorithms: by 44%, 29%, 18%, 19%, and 46% on average for PR, PRD, CC, RE, and MIS respectively. Some non-all-active algorithms like PRD, CC, and RE get slightly lower reductions. In these algorithms, only a subset of vertices are active in some iterations and as a result the active vertexdata is much more likely to fit in cache.</p><p>There is a slight increase in BDFS's memory accesses from 1 to 16 threads (compare Fig. <ref type="figure" target="#fig_1">13</ref> and PageRank in Fig. <ref type="figure" target="#fig_2">14</ref>). In the single-thread experiments the whole 32 MB LLC is available to a single traversal, whereas in the 16-thread experiments the LLC is shared among 16 concurrent traversals, causing interference. The increase is small except for the sk graph, which is quite sensitive to per-thread cache capacity. Performance: Fig. <ref type="figure" target="#fig_4">15</ref> shows the average slowdown of software BDFS over VO. In software, BDFS is slower than VO for all algorithms. This happens because, despite its large reductions in memory accesses, BDFS adds bookkeeping overheads when implemented in software. Since graph algorithms execute only a few instructions per edge, these overheads are relatively large.</p><p>Fig. <ref type="figure" target="#fig_5">16</ref> shows the speedup of three schemes over software VO: the IMP indirect memory prefetcher <ref type="bibr" target="#b57">[58]</ref>, and hardware-accelerated VO (VO-HATS) and BDFS (BDFS-HATS). To ensure IMP issues accurate prefetches, we configure it with explicit information about the graph structures as in <ref type="bibr" target="#b2">[3]</ref>.</p><p>IMP improves performance for the four non-all-active algorithms that are memory-latency bound (PRD, CC, RE, and MIS). When IMP does not saturate bandwidth (PRD, CC, and RE), VO-HATS achieves further gains by offloading traversal scheduling work to HATS. When IMP already saturates bandwidth (PR, MIS), VO-HATS does not improve performance further. Overall, VO-HATS improves performance over VO by up to 2.3× and by 85%, 58%, 61%, and 41% on average for PRD, CC, RE, and MIS respectively.</p><p>However, neither IMP nor VO-HATS reduce memory traffic, so their performance gains are limited by memory bandwidth. This is most noticeable for PR (Fig. <ref type="figure" target="#fig_5">16a</ref>): software VO already saturates memory bandwidth, so VO-HATS and IMP barely improve performance. By contrast, BDFS's reduced memory accesses translate to improved performance for BDFS-HATS on PR, with up to 2.2× speedup over VO on the arb graph. On average, BDFS-HATS improves the performance of PR by 46% over VO and by 43% over VO-HATS.</p><p>BDFS-HATS achieves similar but slightly lower gains over VO-HATS for the non-all-active algorithms. BDFS-HATS improves average performance over VO-HATS by 20%, 13%, 17%, and 35% for PRD, CC, RE, and MIS respectively and over VO by 2.2×, 78%, 88%, and 91%. Energy: Fig. <ref type="figure" target="#fig_15">17</ref> shows the energy breakdown for various schemes. For software-only VO, most of the energy comes from core and main memory, and the fraction of energy from cores depends on how compute-bound the algorithm is. For highly memory-bound applications like PageRank, main memory contributes 46% of total energy.  HATS reduces core energy because it offloads the traversal scheduling to specialized hardware, reducing instruction counts on general-purpose cores. In particular, non-all-active algorithms spend a significant fraction of instructions in activeness checking even with simple VO. HATS completely eliminates these instructions and, on average, reduces core energy by 35%, 36%, 25%, and 28% for PRD, CC, RE, and MIS respectively.</p><formula xml:id="formula_1">V I V H B H PR V I V H B H PRD V I V H B H CC V I V H B H RE V I V</formula><p>BDFS's reduction in main memory accesses causes proportional reductions in main memory energy. Overall, BDFS-HATS reduces energy by 19%, 33%, 28%, 22%, and 30% on average over VO for the five algorithms. The overall energy reductions would be higher on graph processing accelerators, which reduce core energy by over 10× <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b43">44]</ref>, making memory energy the main bottleneck. IMP barely reduces energy since it neither reduces instruction count nor memory accesses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. HATS on an on-chip reconfigurable fabric</head><p>Results so far assume an ASIC implementation of HATS. Fig. <ref type="figure" target="#fig_7">18</ref> shows results for our VO-HATS and BDFS-HATS reconfigurable logic implementations. We model an on-chip FPGA fabric that can issue accesses to the L2 cache, similar to the Xilinx Zynq SoC <ref type="bibr" target="#b55">[56]</ref> but using high-performance cores. Unlike Zynq, where the FPGA fabric is shared by all cores, we assume a per-core FPGA fabric near the private L2s. We later explore the effect of placing HATS at different points in the memory hierarchy, which accounts for less-integrated FPGA fabrics like HARP <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b45">46]</ref>.</p><p>The key difference with the ASIC implementation is the slower clock frequency, 220 MHz. Fig. <ref type="figure" target="#fig_7">18</ref> shows that when HATS has enough parallelism through replication of some parts of the pipeline, as explained in Sec. IV-E, even this slow clock is sufficient to keep the core busy. There is only a small drop in performance (1%) for both HATS versions. Without these changes (i.e., using the ASIC design on the FPGA), VO-HATS and BDFS-HATS are 15% and 34% slower on average. We also modeled a variant where HATS and the core communicate through a FIFO buffer in shared memory. This avoids the need for a dedicated FIFO channel between the HATS engine and the core, which some reconfigurable fabrics may not offer. It also avoids changing the ISA (no fetch edge instruction). Although buffer management operations increase core instructions by up to 10% (on PR), since the workloads are memory-bandwidth bound, there is negligible impact on performance, as Fig. <ref type="figure" target="#fig_10">19</ref> shows: VO-HATS is insensitive and BDFS-HATS shows at most 5% performance loss (on MIS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Adaptive-HATS</head><p>We now explore an adaptive version of HATS that switches between VO and BDFS dynamically. Adaptive-HATS is beneficial for two reasons. First, when the graph does not have good community structure, BDFS increases memory accesses over VO and lowers performance. This can be observed for the twi graph across all algorithms in Fig. <ref type="figure" target="#fig_5">16</ref>. Second, even for graphs with good community structure, the later phases of BDFS exploration usually process low-locality work. Using the simpler VO schedule in such phases improves performance due to lower scheduling overheads.  Adaptive-HATS requires small extensions to BDFS-HATS: switching between VO and BDFS only requires changing the maximum depth of exploration (1 for VO and 10 for BDFS). Every 50 M cycles, all HATS units switch to the alternative mode of exploration for 5 M cycles, and use the bestperforming mode for the next 45 M cycles. Fig. <ref type="figure" target="#fig_19">20a</ref> compares the performance of VO-HATS, BDFS-HATS and Adaptive-HATS on PRD for each graph. web and twi benefit the most from Adaptive-HATS on PRD; we observe similar benefits for other algorithms. Fig. <ref type="figure" target="#fig_19">20b</ref> shows gmean performance across graphs. Adaptive-HATS outperforms BDFS-HATS by 4%, 6%, 10%, 7%, and 4% for PR, PRD, CC, RE, and MIS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. BDFS-HATS versus other locality optimizations</head><p>We now compare BDFS-HATS with online and offline techniques to improve locality.</p><p>Propagation Blocking <ref type="bibr" target="#b7">[8]</ref> (PB) is an online heuristic to improve the spatial locality of all-active algorithms like PageRank. PB first accesses the graph sequentially to gather the updates to neighbors. It partitions these updates into bins, with each bin holding updates for a cache-fitting slice of vertices.  Updates are stored in main memory. PB then reads the updates from memory bin by bin, and finally applies them. Both phases generate sequential accesses to main memory.</p><p>We used all optimizations from the original implementation, which we obtained from PB's authors <ref type="bibr" target="#b7">[8]</ref>. We modified our simulator to model non-temporal stores, which are crucial to reduce PB's memory traffic. Moreover, we compare to Deterministic PB, which generates the per-update neighbor vertex ids only once and reuses them across iterations. We found that a bin size of 1 MB works best for our system. Fig. <ref type="figure" target="#fig_21">21a</ref> compares the memory accesses of BDFS-HATS and PB, normalized to VO. On average, PB achieves slightly lower memory accesses than BDFS-HATS, and works well even for unstructured graphs like twi. However, PB is a software technique that adds non-trivial compute to achieve these memory access reductions. Hence, as shown in Fig. <ref type="figure" target="#fig_21">21b</ref>, the performance gains of PB are limited: while PB achieves up to 43% speedup for sk, it hurts performance for twi. On average, PB is 17% faster then VO, whereas BDFS-HATS is 46% faster. Moreover, PB has several limitations. PB can be extended for non-all-active algorithms <ref type="bibr" target="#b25">[26]</ref>, but the per-update neighbor vertex ids cannot be reused across iterations. And PB works only on algorithms where updates are commutative. Fig. <ref type="figure" target="#fig_23">22a</ref> compares the memory accesses of BDFS-HATS with GOrder <ref type="bibr" target="#b54">[55]</ref> preprocessing, a very expensive heuristic (see Fig. <ref type="figure" target="#fig_4">5b</ref>) that heavily exploits graph structure. GOrder achieves much lower memory accesses than BDFS-HATS and these memory traffic reductions translate to improved performance as shown in Fig. <ref type="figure" target="#fig_23">22b</ref>. GOrder-HATS, which combines GOrder with VO-HATS, further improves performance significantly for non-all-active algorithms (PRD, CC, RE, MIS).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Sensitivity studies</head><p>Impact of prefetching: HATS engines accurately prefetch vertex data into the L2 to accelerate edge processing by the cores. Fig. <ref type="figure" target="#fig_24">23</ref> shows that this prefetching is effective by comparing VO and BDFS HATS variants with and without vertex data prefetching. Prefetching accounts for about a third of the speedup achieved by BDFS-HATS over VO. (Note that these prefetches are irregular, so a conventional prefetcher would not be able to perform them.) We find that HATS prefetches are timely. First, the limited size of the HATS buffer (64 entries) constrains how far ahead the HATS runs. Thus, prefetched data takes a small fraction of the L2 (up to 4 KB), avoiding too-early prefetches. Second, the HATS buffer is large enough to avoid late prefetches. We observe that a small fraction (5-10%) of prefetches are late (i.e., partially overlapped with the demand access). Even then, these late prefetches cover 90% of access latency on average. HATS location: Fig. <ref type="figure" target="#fig_25">24</ref> shows how the location of HATS changes the benefits of BDFS-HATS over VO. Performance changes only slightly when moving HATS from the L2 to the L1. However, placing HATS near the LLC (e.g., on a shared FPGA fabric) causes a noticeable drop in performance for nonall-active algorithms. With this configuration, HATS can only prefetch vertex data into the LLC. Thus, cores experience few tens of cycles of latency when accessing vertex data. Memory bandwidth: Fig. <ref type="figure" target="#fig_4">25</ref> shows the speedup of VO-HATS and BDFS-HATS over VO as the number of memory controllers grows from two to six (i.e., as peak main memory bandwidth grows from about 26 to 77 GB/s). While both VO-HATS and BDFS-HATS are more beneficial when the system has higher bandwidth, the improvement of BDFS-HATS over VO-HATS increases when the system has lower bandwidth. At two memory controllers, BDFS-HATS outperforms VO-HATS by 43%, 25%, 18%, 22%, and 43%. At six memory controllers, the speedups reduce to 37%, 10%, 3%, 8%, and 20%. General-purpose core type: Fig. <ref type="figure" target="#fig_5">26</ref> shows the speedup of BDFS-HATS different core types. All speedups are normalized to VO with Haswell-like cores. While compute-bound nonall-active algorithms (PRD, CC, RE) are more sensitive, BDFS-HATS retains a large fraction of its benefits with lean OOO (Silvermont-like) cores since the system is memory bandwidthbound. Moreover, HATS with energy-efficient in-order cores is faster than software VO with power-hungry OOO cores.</p><p>Cache size: Fig. <ref type="figure" target="#fig_27">27</ref> shows the performance of VO-HATS and BDFS-HATS at various cache sizes.  LLC replacement policy: Finally, Fig. <ref type="figure" target="#fig_7">28</ref> shows how the LLC replacement policy affects the benefits of BDFS. We evaluate BDFS-HATS with LRU and DRRIP <ref type="bibr" target="#b24">[25]</ref>, a highperformance replacement policy. BDFS-HATS achieves slightly higher gains when using DRRIP. This happens because DRRIP is scan-and thrash-resistant, so it reduces the cache pollution caused by access patterns with no temporal locality. This leaves more cache capacity for data with temporal locality, which BDFS exploits. These results show that BDFS-HATS and highperformance replacement policies are complementary. BDFS-HATS would also benefit from specializing the replacement policy for different graph data structures <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ADDITIONAL RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Memory system specialization for graph processing</head><p>Recent graph processing accelerators have proposed various ways to tune the memory hierarchy to the needs of graph algorithms, e.g., by using separate scratchpads to hold vertex and edge data and matching their word sizes to object sizes <ref type="bibr" target="#b43">[44]</ref>, or by adding a large on-chip eDRAM scratchpad that can hold larger graphs than is possible with SRAM <ref type="bibr" target="#b21">[22]</ref>.</p><p>Prior work has also proposed near-data processing (NDP)</p><p>designs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b59">60]</ref> that execute most graph-processing operations in logic close to main memory, reducing the cost of memory accesses. NDP's high memory bandwidth makes it attractive for processing large unstructured graphs without any locality, but as we have seen, graphs often have community structure and can use caches effectively. Moreover, NDP systems are still under development, so it is important to optimize systems that use conventional off-chip main memory. HATS complements this prior work by using locality-aware scheduling to make better use of limited on-chip storage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Preprocessing and locality optimizations</head><p>As discussed in Sec. II, preprocessing algorithms such as RCM <ref type="bibr" target="#b13">[14]</ref>, GOrder <ref type="bibr" target="#b54">[55]</ref>, and Rabbit Order <ref type="bibr" target="#b3">[4]</ref> improve the locality of VO, but they are very expensive. These techniques reorder graph vertices to assign close-by ids to related vertices. They turn the graph's adjacency matrix into a narrow band matrix, with most nonzeros clustered around its diagonal. However, this reordering is orders of magnitude more expensive than the runtime of a single traversal <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b38">39]</ref>. Similarly, edgecentric scheduling with Hilbert Order <ref type="bibr" target="#b35">[36]</ref> outperforms VO by balancing the locality of source and destination vertices but requires an expensive sort of all edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>Graph processing algorithms are increasingly bottlenecked by main memory accesses. We have shown how runtime scheduling strategies that exploit the community structure of real-world graphs can significantly improve locality. We propose bounded depth-first scheduling (BDFS), a simple yet highly effective scheduling technique to improve locality for graphs with good community structure, and HATS, a hardware-accelerated traversal scheduler. BDFS-HATS requires inexpensive hardware and reduces memory accesses significantly. On a simulated 16-core system, BDFS reduces main memory accesses by up to 2.4× and BDFS-HATS improves performance by up to 3.1× over a locality-oblivious software implementation and by up to 2.1× over specialized prefetchers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 1: BDFS reduces memory accesses by 1.8× for PageRank Delta on uk-2002.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Compressed sparse row (CSR) format. The offset array stores, for each vertex, the starting location of its neighbors' vertex ids in the neighbor array. The vertex data array stores algorithm-specific data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig.4: The vertex-ordered schedule ignores graph structure and alternates between the two communities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Memory accesses and execution time for one PageRank iteration on uk-2002 with various preprocessing schemes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: BDFS improves temporal locality by processing neighbors together and vertices within a community close in time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Memory access patterns with the vertex-ordered schedule (top) and BDFS (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Breakdown of memory accesses to different data structures for PageRank on the uk-2002 graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Larger sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Smaller sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Memory accesses of PageRank on the uk-2002 graph with BDFS and bounded BFS (BBFS) at different fringe sizes. BDFS reduces memory accesses with much less storage than BBFS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 :</head><label>10</label><figDesc>Fig.10: System architecture. Each core has a HATS engine that traverses the graph and sends edges to process to the core.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 11 :</head><label>11</label><figDesc>Fig. 11: Microarchitecture of VO-HATS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 12 :</head><label>12</label><figDesc>Fig. 12: Microarchitecture of BDFS-HATS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 13 :Fig. 14 :Fig. 15 :Fig. 16 :</head><label>13141516</label><figDesc>Fig. 13: Breakdown of main memory accesses by data structure for VO and BDFS on singlethreaded PageRank.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 17</head><label>17</label><figDesc>Fig. 17: Energy breakdown normalized to VO. (V = VO, I = IMP, VH = VO-HATS, BH = BDFS-HATS.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>:</head><label></label><figDesc>Fig. 17: Energy breakdown normalized to VO. (V = VO, I = IMP, VH = VO-HATS, BH = BDFS-HATS.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 18 :Fig. 19 :</head><label>1819</label><figDesc>Fig.18: HATS performance on an FPGA fabric.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 20 :</head><label>20</label><figDesc>Fig. 20: Adaptive-HATS outperforms BDFS-HATS by avoiding BDFS-HATS's performance pathologies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Fig. 21 :</head><label>21</label><figDesc>Fig. 21: Propagation Blocking reduces memory traffic significantly but shows limited performance gains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Fig. 22 :</head><label>22</label><figDesc>Fig. 22: BDFS-HATS versus GOrder preprocessing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Fig. 23 :</head><label>23</label><figDesc>Fig. 23: Impact of prefetching on performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Fig. 24 :</head><label>24</label><figDesc>Fig. 24: Sensitivity of BDFS-HATS to on-chip location.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>6 Fig. 25 :Fig. 26 :</head><label>62526</label><figDesc>Fig. 25: Speedup of VO-HATS (shaded) and BDFS-HATS over VO with 2-6 memory ctlrs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Fig. 27 :</head><label>27</label><figDesc>Fig. 27: Sensitivity to LLC size. All speedups are relative to software VO at 32 MB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>Area and power of VO-HATS and BDFS-HATS implementations: ASIC (65nm) and FPGA (Zynq-7045).</figDesc><table><row><cell>HATS</cell><cell cols="2">ASIC Area</cell><cell cols="2">ASIC Power</cell><cell cols="2">FPGA Area</cell></row><row><cell>Design</cell><cell>(mm 2 )</cell><cell>%core</cell><cell cols="2">(mW ) %TDP</cell><cell>(LUT s)</cell><cell>%FPGA</cell></row><row><cell>VO</cell><cell>0.07</cell><cell>0.19%</cell><cell>37</cell><cell>0.11%</cell><cell>1725</cell><cell>0.79%</cell></row><row><cell>BDFS</cell><cell>0.14</cell><cell>0.38%</cell><cell>72</cell><cell>0.22%</cell><cell>3203</cell><cell>1.47%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Configuration of the simulated system.</figDesc><table><row><cell cols="2">Cores 16 cores, x86-64 ISA, 2.2 GHz, Haswell-like OOO [47]</cell></row><row><cell>L1 caches</cell><cell>32 KB, per-core, 8-way set-associative, split D/I, 3-cycle latency</cell></row><row><cell>L2 cache</cell><cell>128 KB, private per-core, 8-way set-associative, 6-cycle latency</cell></row><row><cell>L3 cache</cell><cell>32 MB, shared, 16-way hashed set-associative, inclusive, 24-cycle bank latency, LRU replacement</cell></row><row><cell>Global</cell><cell>4×4 mesh, 128-bit flits and links, X-Y routing, 1-cycle</cell></row><row><cell>NoC</cell><cell>pipelined routers, 1-cycle links</cell></row><row><cell cols="2">Coherence MESI, 64 B lines, in-cache directory, no silent drops</cell></row><row><cell>Memory</cell><cell>4 controllers, FR-FCFS, DDR4 1600 (12.8 GB/s per controller)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>Graph algorithms.</figDesc><table><row><cell cols="2">Vertex All-</cell></row><row><cell cols="2">Algorithm Size Active?</cell></row><row><cell>PageRank (PR) 16B</cell><cell>Yes</cell></row><row><cell>PageRank Delta (PRD) 16B</cell><cell>No</cell></row><row><cell>Conn. Components (CC) 8B</cell><cell>No</cell></row><row><cell>Radii Estimation (RE) 24B</cell><cell>No</cell></row><row><cell>Max. Indep. Set (MIS) 8B</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV :</head><label>IV</label><figDesc>Graph datasets.</figDesc><table><row><cell cols="2">Graph Vertices Edges</cell><cell>Description</cell></row><row><cell>(M)</cell><cell>(M)</cell><cell></cell></row><row><cell>uk 19</cell><cell>298</cell><cell>uk-2002 [16]</cell></row><row><cell>arb 22</cell><cell>640</cell><cell>arabic-2005 [16]</cell></row><row><cell>twi 41</cell><cell cols="2">1468 Twitter followers [28]</cell></row><row><cell>sk 51</cell><cell>1949</cell><cell>sk-2005 [16]</cell></row><row><cell cols="3">web 118 1020 webbase-2001 [16]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>All speedups are relative to software VO at a fixed cache size of 32 MB, making speedups across cache sizes comparable. For PR and MIS, BDFS-HATS with just a 16 MB cache outperforms both VO-HATS and VO with a 32 MB cache. For PRD and RE, BDFS-HATS at 16 MB outperforms VO with a 32 MB cache. It closely matches VO-HATS with a 32 MB cache.</figDesc><table><row><cell></cell><cell cols="2">16MB</cell><cell cols="2">32MB (Default)</cell><cell>64MB</cell></row><row><cell>Speedup over VO (%)</cell><cell>0 50 100 150</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>VH BH</cell><cell>VH BH</cell><cell>VH BH</cell><cell>VH BH</cell><cell>VH BH</cell></row><row><cell></cell><cell>PR</cell><cell>PRD</cell><cell>CC</cell><cell>RE</cell><cell>MIS</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Nosayba El-Sayed, Joel Emer, Yee Ling Gan, Mark Jeffrey, Hyun Ryong Lee, Suvinay Subramanian, Po-An Tsai, Victor Ying, Guowei Zhang, and the anonymous reviewers for their feedback. We thank Scott Beamer for sharing his implementation of Propagation Blocking. This work was supported in part by NSF grant CAREER-1452994, DARPA SDH under contract HR0011-18-3-0007, and a grant from the Qatar Computing Research Institute. Nathan Beckmann was supported by a Google Faculty Research Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A work-efficient algorithm for parallel unordered depth-first search</title>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">A</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Charguéraud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rainey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SC15</title>
		<meeting>SC15</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A scalable processingin-memory accelerator for parallel graph processing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISCA-42</title>
		<meeting>ISCA-42</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Graph prefetching using data structure knowledge</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ainsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICS&apos;16</title>
		<meeting>ICS&apos;16</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rabbit order: Just-in-time parallel reordering for fast graph analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Arai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shiokawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yamamuro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Onizuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Iwamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IPDPS</title>
		<meeting>IPDPS</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Clustering a DAG for CAD Databases</title>
		<author>
			<persName><forename type="first">J</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Garza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TSE</title>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Emergence of scaling in random networks</title>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Barabási</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Albert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="issue">5439</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Locality exists in graph processing: Workload characterization on an Ivy Bridge server</title>
		<author>
			<persName><forename type="first">S</forename><surname>Beamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IISWC</title>
		<meeting>IISWC</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reducing Pagerank communication via Propagation Blocking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Beamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IPDPS</title>
		<meeting>IPDPS</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discrepancy-bounded depth first search</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Perron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CPAIOR-2</title>
		<meeting>CPAIOR-2</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Greedy sequential maximal independent set and matching are parallel on average</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Blelloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Fineman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPAA</title>
		<meeting>SPAA</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scheduling multithreaded computations by work stealing</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Blumofe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JACM</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On the representation and multiplication of hypersparse matrices</title>
		<author>
			<persName><forename type="first">A</forename><surname>Buluc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IPDPS</title>
		<meeting>IPDPS</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Cormen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Rivest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
		<title level="m">Introduction to algorithms</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note>rd ed.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reducing the bandwidth of sparse symmetric matrices</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cuthill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mckee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM</title>
		<meeting>ACM</meeting>
		<imprint>
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">FPGP: Graph processing framework on FPGA-a case study of breadth-first search</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. FPGA&apos;16</title>
		<meeting>FPGA&apos;16</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The University of Florida sparse matrix collection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOMS</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inside Intel Core microarchitecture</title>
		<author>
			<persName><forename type="first">J</forename><surname>Doweck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Hot Chips Symposium</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cacheoblivious algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Frigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Prokop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramachandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. FOCS-40</title>
		<meeting>FOCS-40</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The implementation of the Cilk-5 multithreaded language</title>
		<author>
			<persName><forename type="first">M</forename><surname>Frigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Randall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. PLDI</title>
		<meeting>PLDI</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Practical near-data processing for in-memory analytics frameworks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. PACT-24</title>
		<meeting>PACT-24</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">GraphX: Graph processing in a distributed dataflow framework</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Crankshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. OSDI-11</title>
		<meeting>OSDI-11</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Graphicionado: A high-performance and energy-efficient accelerator for graph analytics</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICRO-49</title>
		<meeting>MICRO-49</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Haswell: The fourth-generation intel core processor</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hammarlund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hallnor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Intel-Altera Heterogeneous Architecture Research Platform Program</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<ptr target="https://cpufpga.files.wordpress.com/2016/04/harpisca" />
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">High performance cache replacement using re-reference interval prediction (RRIP)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Steely</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISCA-37</title>
		<meeting>ISCA-37</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Optimizing indirect memory references with milk</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kiriansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. PACT-25</title>
		<meeting>PACT-25</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Depth-first iterative-deepening: An optimal admissible tree search</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Korf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAI</title>
		<imprint>
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">What is Twitter, a social network or a news media</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WWW-19</title>
		<meeting>WWW-19</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Statistical properties of community structure in large social and information networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WWW-17</title>
		<meeting>WWW-17</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">McPAT: An integrated power, area, and timing modeling framework for multicore and manycore architectures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Strong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICRO-42</title>
		<meeting>MICRO-42</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Challenges in parallel graph processing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lumsdaine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hendrickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Berry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PPL</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">01</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast computation of empirically tight bounds for the diameter of massive graphs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Magnien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Latapy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Habib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JEA</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Everything you always wanted to know about multicore graph processing but were afraid to ask</title>
		<author>
			<persName><forename type="first">J</forename><surname>Malicevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J E</forename><surname>Lepers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zwaenepoel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX ATC</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph stream algorithms: A survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mcgregor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMOD Record</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A uniform approach to accelerated PageRank computation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WWW-14</title>
		<meeting>WWW-14</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<title level="m">Scalability! But at what COST?&quot; in Proc. HotOS-15</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">1.35V DDR3L power calculator (4Gb x16 chips)</title>
		<author>
			<persName><surname>Micron</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Whirlpool: Improving dynamic cache management with static data classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mukkara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASPLOS-XXI</title>
		<meeting>ASPLOS-XXI</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Cache-Guided Scheduling: Exploiting caches to maximize locality in graph processing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mukkara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>in AGP&apos;17</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graph-PIM: Enabling instruction-level PIM offloading in graph computing frameworks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Nai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HPCA-23</title>
		<meeting>HPCA-23</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A lightweight infrastructure for graph analytics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lenharth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pingali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SOSP-24</title>
		<meeting>SOSP-24</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">GraphGen: An FPGA framework for vertex-centric graph computation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Nurvitadhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weisz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hurkat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. FCCM-22</title>
		<meeting>FCCM-22</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">GraphOps: A dataflow library for graph analytics acceleration</title>
		<author>
			<persName><forename type="first">T</forename><surname>Oguntebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Olukotun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. FPGA&apos;16</title>
		<meeting>FPGA&apos;16</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Energy efficient architecture for graph analytics accelerators</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Ozdal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yesil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ayupov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Greth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ozturk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISCA-43</title>
		<meeting>ISCA-43</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The PageRank citation ranking: Bringing order to the web</title>
		<author>
			<persName><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stanford InfoLab, Tech. Rep</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Accelerating datacenter workloads</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<ptr target="http://www.fpl2016.org/slides/Gupta--AcceleratingDatacenterWorkloads.pdf" />
	</analytic>
	<monogr>
		<title level="m">FPL&apos;16 Keynote</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">ZSim: Fast and accurate microarchitectural simulation of thousand-core systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISCA-40</title>
		<meeting>ISCA-40</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Ligra: A lightweight graph processing framework for shared memory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Blelloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. PPoPP</title>
		<meeting>PPoPP</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Decoupled access/execute computer architectures</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISCA-9</title>
		<meeting>ISCA-9</meeting>
		<imprint>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">GraphR: Accelerating graph processing using ReRAM</title>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HPCA-24</title>
		<meeting>HPCA-24</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">An analysis of consecutively bounded depthfirst search with applications in automated deduction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tyson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">GraphMat: High performance graph analytics made productive</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M A</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Dulloor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Vadlamudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dubey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Performance of the decoupled ACRI-1 architecture: The perfect club</title>
		<author>
			<persName><forename type="first">N</forename><surname>Topham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mcdougall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HPCN</title>
		<meeting>HPCN</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A bridging model for parallel computation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. ACM</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Speedup graph processing by graph ordering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGMOD&apos;16</title>
		<meeting>SIGMOD&apos;16</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><surname>Xilinx</surname></persName>
		</author>
		<ptr target="https://www.xilinx.com/support/documentation/boardsandkits/zc706/ug954-zc706-eval-board-xc7z045-ap-soc.pdf" />
		<title level="m">ZC706 evaluation board for the Zynq-7000 XC7Z045 all programmable SoC user guide</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Ultrascale+ FPGAs product tables and product selection guide</title>
		<author>
			<persName><surname>Xilinx</surname></persName>
		</author>
		<ptr target="https://www.xilinx.com/support/documentation/selection-guides/ultrascale-plus-fpga-product-selection-guide.pdf" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">IMP: Indirect Memory Prefetcher</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Devadas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICRO-48</title>
		<meeting>MICRO-48</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">PathGraph: A path centric graph processing system</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPDS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">GraphP: Reducing communication for PIM-based graph processing with efficient data partition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HPCA-24</title>
		<meeting>HPCA-24</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">FBSGraph: Accelerating asynchronous graph processing via forward and backward sweeping</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Making caches work for graph analytics</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kiriansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mendis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE BigData</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Gemini: A computation-centric distributed graph processing system</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. OSDI-12</title>
		<meeting>OSDI-12</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">GridGraph: Large-scale graph processing on a single machine using 2-level hierarchical partitioning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX ATC</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
