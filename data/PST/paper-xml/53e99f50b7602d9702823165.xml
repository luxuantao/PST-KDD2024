<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Feature Selection for Microarray Data Based on Multicriterion Fusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2010-10-15">15 Oct. 2010</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Feng</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Division of Control and Instrumentation</orgName>
								<orgName type="department" key="dep2">School of Electrical and Electronic Engineering</orgName>
								<orgName type="department" key="dep3">College of Engineering</orgName>
								<orgName type="department" key="dep4">Biomedical Electronics Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<addrLine>50 Nanyang Avenue</addrLine>
									<postCode>S1-B4b-06, 639798</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">K</forename><forename type="middle">Z</forename><surname>Mao</surname></persName>
							<email>ekzmao@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Division of Control and Instrumentation</orgName>
								<orgName type="department" key="dep2">School of Electrical and Electronic Engineering</orgName>
								<orgName type="department" key="dep3">College of Engineering</orgName>
								<orgName type="department" key="dep4">Biomedical Electronics Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<addrLine>50 Nanyang Avenue</addrLine>
									<postCode>S1-B4b-06, 639798</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Feature Selection for Microarray Data Based on Multicriterion Fusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2010-10-15">15 Oct. 2010</date>
						</imprint>
					</monogr>
					<idno type="MD5">91379BED91BCDD14690BD0AAC4AC0189</idno>
					<idno type="DOI">10.1109/TCBB.2010.103</idno>
					<note type="submission">received 5 Nov. 2009; revised 6 Apr. 2010; accepted 13 July 2010;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Feature selection</term>
					<term>multicriterion fusion</term>
					<term>recursive feature elimination</term>
					<term>robustness</term>
					<term>classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Feature selection often aims to select a compact feature subset to build a pattern classifier with reduced complexity, so as to achieve improved classification performance. From the perspective of pattern analysis, producing stable or robust solution is also a desired property of a feature selection algorithm. However, the issue of robustness is often overlooked in feature selection. In this study, we analyze the robustness issue existing in feature selection for high-dimensional and small-sized gene-expression data, and propose to improve robustness of feature selection algorithm by using multiple feature selection evaluation criteria. Based on this idea, a multicriterion fusion-based recursive feature elimination (MCF-RFE) algorithm is developed with the goal of improving both classification performance and stability of feature selection results. Experimental studies on five gene-expression data sets show that the MCF-RFE algorithm outperforms the commonly used benchmark feature selection algorithm SVM-RFE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ç</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>F EATURE selection plays an increasingly important role in machine learning and data mining with emerging of high-dimensional data such as microarray gene-expression data. Feature selection for gene-expression data, also known as gene selection, mainly serves two purposes. First, feature selection is to identify certain disease-related genes. Second, feature selection is to find a compact set of discriminative genes to build a pattern classifier with reduced complexity and improved generalization capabilities. Depending on the purpose of gene selection, two types of feature selection algorithms including ranking-based feature selection and set-based feature selection are employed in microarray gene-expression data analysis <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b32">[33]</ref>. In ranking-based feature selection, features are evaluated on an individual basis, without considering inter-relationship between features in general, while setbased feature selection evaluates features based on their role in a feature set by taking into account dependency between features.</p><p>Gene selection might seem to be a straightforward application of feature selection techniques to gene-expression data, but the problem is not so simple. Due to high dimensionality (as high as a few thousands) and small sample size (as small as a few dozens) of gene-expression data, gene selection encounters problems that are not commonly seen in feature selection for conventional data having relatively low dimensionality and large sample size. One issue encountered is the validity of cross validation. Cross validation is a widely used error estimation method, but the study in <ref type="bibr" target="#b4">[5]</ref> revealed that cross validation-based error estimation could exhibit large variance when applied to small sized data. Another problem that was often overlooked is the ties problem associated with classification error-based feature evaluation <ref type="bibr" target="#b44">[45]</ref>. Under small sample size, classification error has too few possible values to distinguish different features, and this in turn results in selection uncertainty. Peaking phenomenon also becomes a non-neglectable problem under high dimensionality and small sample size <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b20">[21]</ref>.</p><p>Robustness or stability of feature selection for highdimensional and small-sized data also received attentions in recent years. Robustness or stability of a feature selection algorithm refers to its sensitivity to varying conditions such as perturbations of training data. If a feature selection algorithm lacks robustness, it might produce unrepeatable results even only a few samples are added to or deleted from the training data set. Even without perturbation of training data different feature selection algorithms usually produce different selection results. The inconsistent gene selection results thus produced could cause confusions to biological researchers and result in loss of their enthusiasm and confidence in applying machine learning techniques to solve biological problems.</p><p>In the literature, just a few work explores the robustness issue of feature selection. Kalousis et al. <ref type="bibr" target="#b22">[23]</ref> examined three categories of existing stability measures in high-dimensional space and constructed stability profiles for some well-known feature selection algorithms; Somol and Novovicovae <ref type="bibr" target="#b37">[38]</ref> proposed a new consistency measure with reduced computational complexity; Krizek et al. <ref type="bibr" target="#b26">[27]</ref> developed an entropybased measure for stability assessment; Gulgezen et al. <ref type="bibr" target="#b18">[19]</ref> studied the stability and classification accuracy of Minimum Redundancy Maximum Relevance-based feature selection. Besides the above work that analyzes robustness of existing feature selection algorithms, some new algorithms aiming to improve robustness of feature selection have also been developed. For example, Saeys et al. <ref type="bibr" target="#b33">[34]</ref> proposed an instance perturbation-based ensemble scheme for single feature selection; Yu et al. <ref type="bibr" target="#b43">[44]</ref> proposed a general feature selection framework based on dense feature groups and developed Dense Relevant Attribute Group Selector (DRAGS); Loscalzo et al. <ref type="bibr" target="#b29">[30]</ref> proposed a consensus groupbased framework and developed Consensus Group Stable Feature Selection algorithm (CGS). Both DRAGS and CGS could achieve good stability without sacrificing classification performance.</p><p>In this study, we propose to improve robustness of feature selection through fusion of multiple criteria for feature evaluation. Based on this idea, a multicriterion fusion-based recursive feature elimination (MCF-RFE) algorithm is developed. As revealed in <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b33">[34]</ref>, the various feature selection algorithms are sensitive even to a minor variation of training samples. We believe this might be due to inaccurate estimation of statistical parameters such as sample mean and standard deviation employed in the feature evaluation criterion. By using multiple criteria, the merit evaluation of features tends to be less sensitive to the inaccurate estimation of the statistical parameters, and hence, the robustness of the feature selection algorithm is improved. In addition, the proposed new algorithm alleviates the disagreement between different feature selection algorithms by getting their consensus, and hence, improves the credibility of the selected features. Experimental studies on five gene-expression data sets show that the new MCF-RFE algorithm produces feature subsets with good stability and classification accuracy.</p><p>The rest of this paper is organized as follows: In Section 2, the robustness issue is illustrated by a case study and a stability measure is introduced. Section 3 explains the basic idea of multicriterion fusion as well as basic fusion methods. In Section 4, the detailed new feature selection algorithm MCF-RFE is described. In Section 5, experimental results and discussions are presented. The last, Section 6, concludes this study. To obtain reliable evaluation, the above experiment was repeated 300 times for each l value and the average number of common features in S 0 and S l was calculated. Fig. <ref type="figure" target="#fig_0">1</ref> shows the results of Fisher's ratio-based feature selection on CNS gene-expression data, which has 60 samples and 7,129 genes (details of the data are given in Section 5). From Fig. <ref type="figure" target="#fig_0">1</ref>, it is observed that when only 1 sample is removed, the number of common genes among the top 10 genes is reduced from 10 to 8, on average, and the number is further reduced to 5, when 5 samples are removed. This result tells us that the commonly used Fisher's ratio is actually sensitive to training data perturbation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">ISSUE OF ROBUSTNESS IN GENE SELECTION</head><p>To further investigate the robustness of Fisher's ratiobased feature selection, experimental study on simulated data was also conducted. Two artificial data sets containing 7,129 features with 60 and 1,000 samples were generated, respectively, using the Matlab code in Guyon <ref type="bibr" target="#b15">[16]</ref>. As shown in Fig. <ref type="figure" target="#fig_0">1</ref> (the curve "FisherRatio: SimulationData60"), the feature selection results of Fisher's ratio on the simulated data set with 60 samples are not stable. But for the data set with 1,000 training samples (see the curve "FisherRatio: SimulationData1000" in Fig. <ref type="figure" target="#fig_0">1</ref>), the selection results are less sensitive to removal of training data. This result explains why robustness is not an issue in feature selection for data with large sample size. But for small sized data like geneexpression data, the robustness of feature selection is indeed an issue that should be considered seriously because such a minor variation/perturbation of training data is very likely to occur, in practice, such as addition of a few new cases or samples.</p><p>Fig. <ref type="figure" target="#fig_0">1</ref> shows the results of Fisher's ratio as an example, but our study found that almost all commonly used feature selection algorithms are prone to perturbation of smallsized training data. It is thus, necessary to develop robust feature selection algorithms to alleviate the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Robustness Measures</head><p>The robustness (stability) of a feature selection algorithm can be evaluated based on its ability to select repeated features, given different batches of data under the same distribution. Since the true distribution of real data is usually unknown and only a number of samples are available during the learning process, the different batches of data can be generated through resampling.</p><p>Assume S i and S 0 denote feature subsets selected using the ith batch of resampled data and the full data, respectively. The similarity between the two feature subsets can be measured using Jaccard Index (or Tanimoto Index) <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b33">[34]</ref> </p><formula xml:id="formula_0">J i ðkÞ ¼ jS i T S 0 j jS i S S 0 j ;<label>ð1Þ</label></formula><p>where k is the cardinality of S i and S 0 , jS i \ S 0 j is the number of common features between S i and S 0 and jS i [ S 0 j is the total number of features without reduplication in S i and S 0 . The above similarity measure takes into account only the common features between two feature subsets. But there exist a great number of features that are highly correlated in gene-expression data. In order to give a more general and precise measure of the similarity between two feature subsets, we employ the following similarity index JCð2 ½0; 1Þ that takes into account the correlations (Pearson correlation in this study) between the different features of two feature subsets:</p><formula xml:id="formula_1">JC i ðkÞ ¼ jS i T S 0 j þ SC i k ;<label>ð2Þ</label></formula><p>where SC i is the sum of absolute correlation values between the dissimilar features from S i and S 0 . The idea behind the index is illustrated in Fig. <ref type="figure">2</ref>, where S 0 i and S 0 0 are two feature subsets after removing the common features between S i and S 0 , and each node represents a feature and each edge denotes the correlation between the corresponding features. The final sum of correlations between feature subsets S 0 i and S 0 0 is</p><formula xml:id="formula_2">SC i ¼ jCorrðf i1 ; f 01 Þj þ jCorrðf i3 ; f 02 Þj þ jCorrðf i2 ; f 03 Þj ¼ jÀ0:9j þ j0:8j þ j0:7j ¼ 2:4;</formula><p>where jCorrðf ij ; f 0j Þj is the absolute value of correlation between features f ij and f 0j . The above similarity index resembles the one proposed in <ref type="bibr" target="#b43">[44]</ref>, where SC i is calculated in an optimal way. For computational simplicity, SC i is computed using the greedy search algorithm in this study.</p><p>Assume totally m batches of data are generated by resampling and m feature subsets are selected. The robustness or stability measure of the feature selection algorithm is defined as</p><formula xml:id="formula_3">JCðkÞ ¼ P m i¼1 JC i ðkÞ m :<label>ð3Þ</label></formula><p>For the experiment study in Section 2.1, the robustness indices calculated by (3) are shown in Fig. <ref type="figure">3</ref>, where the number of removed samples is fixed to 10. These results are in line with those in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>The above robustness index provides a sensible evaluation of the stability of a feature selection algorithm. But it should be noted that a robust feature selection may not necessarily guarantee good classification performance because the measure is independent of a classification model. In practice, both stability and classification performance should be considered when evaluating a feature selection algorithm because a stable but classificationineffective selection result does not make any sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ROBUST FEATURE SELECTION BASED ON FUSION</head><p>OF MULTIPLE CRITERIA</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Motivation of Using Multicriterion Fusion</head><p>In pattern classification, it is well acknowledged that combining or integrating multiple classifiers, especially uncorrelated weak ones could greatly improve the classification performance <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b39">[40]</ref>. Motivated by the success of multiple classifier combination, in this study, we propose to improve the robustness of feature selection through integrating multiple feature selection criteria (algorithms). The reasons for integrating multiple criteria are manifold. First, different feature selection algorithms produce different feature subsets. Table <ref type="table" target="#tab_1">1</ref> shows the top 10 genes (denoted by their serial numbers in the table) selected by five different feature selection criteria, including Fisher's ratio, Relief, ADC (asymmetric dependency coefficient), AW-SVM (absolute weight of SVM), and SVM-RFE, the details of which can be found in the following Section 3.2 and Section 4. Obviously, the selection results are mostly different. Among the different selection results, which result is superior and should be adopted? Actually, there is no agreed ways to decide. Integrating different "opinions" from multiple feature selection criteria to yield a consensus seems to be a reasonable solution. Second, a model built upon weak assumptions usually performs more robust than a model built upon stringent assumptions. The existing feature selection criteria are generally built upon certain assumption(s) of data distribution. But the distribution of the learning data is usually rather complicated (e.g., a mixture of many different distributions) and unknown. Even the distribution is precisely available, it may violate the assumptions at certain extent. A criterion that aggregates multiple feature selection criteria can help to weaken the assumptions, and consequently, improve the robustness. Third, feature subsets produced by different feature selection criteria may exhibit complementary effects because of the nonindependence among features, and thus, a fusion of these feature subsets may produce a better representation in feature space to describe the data. Fourth, each feature selection criterion usually has its own specific but restrained ability to search in the feature space, and thus, may be stuck at a local optimum, while fusion of multiple criteria utilizes and aggregates the search abilities of each of the criteria to obtain a wider "vision" that may help to get closer to a global optimal solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Basis Criteria</head><p>Many feature evaluation criteria have been proposed in the literature. However, it is unnecessary or impractical to use all of them in the criterion fusion. In this study, the criteria selected for fusion are named as basis criteria.</p><p>It was found that if two feature selection criteria produce similar results, fusion of such two criterion does not help. This finding provides us a guideline for basis criteria selection: basis criteria should exhibit diversity. The deeper reason for diversity is that the diverse results produced by the diverse multiple basis criteria complement each other. Another reason for using diverse basis criteria is to prevent the fusion result from being dominated by criteria that produce similar results.</p><p>For computational simplicity and performance diversity analyzed above, the basis criteria used in this study are Fisher's ratio, Relief, ADC (asymmetric dependency coefficient), and AW-SVM (absolute weight of SVM). Fisher's ratio is a univariate filter method evaluating each feature individually, while Relief is a multivariate filter method taking into account dependencies between features. ADC is a information theory-based filter method and AW-SVM is an embedded method that ranks features based on their corresponding coefficients in the SVM classifier. Details of the four basis criteria are presented below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Fisher's Ratio</head><p>Fisher's ratio is an individual feature evaluation criterion that measures the discriminative power of a feature j by the ratio of interclass difference to intraclass spread</p><formula xml:id="formula_4">FRðjÞ ¼ ðb j1 À b j2 Þ 2 2 j1 þ 2 j2 ;<label>ð4Þ</label></formula><p>where b jc is the sample mean of feature j within class c and 2 jc is the variance of feature j within class c, for c ¼ 1; 2. The larger the F R value, the more discriminative the feature is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Relief</head><p>Relief is a weight-based feature ranking method inspired by the instance-based learning <ref type="bibr" target="#b31">[32]</ref>. It evaluates the "Relevance" of features through multiple iterations. At each iteration, a sample x is first randomly selected from the data set and its nearest-Hit x H (nearest neighbor from the same class) and nearest-Miss x M (nearest neighbor from other class) are identified, and the relevance of all features are then updated using the difference between the nearest-Hit and nearest-Miss as follows:</p><formula xml:id="formula_5">W ðjÞ ¼ W ðjÞ À diffðj; x; x H Þ n þ diffðj; x; x M Þ n ;<label>ð5Þ</label></formula><p>where W ðjÞ is the relevance of feature j to the targets and it is initialized to zero; diffðj; x; x 0 Þ denotes the difference of feature j between samples x and x 0 . For continuous features, diffðj; x; x 0 Þ is the actual difference normalized to interval ½0; where x j and x 0 j are the values of feature j in x and x 0 , x j max and x j min are the maximal and minimal values of feature j in all samples.</p><formula xml:id="formula_6">1 diffðj; x; x 0 Þ ¼ jx j À x 0 j j x j max À x j min ;<label>ð6Þ</label></formula><p>The updating process in <ref type="bibr" target="#b4">(5)</ref> repeats n times for a training data set with sample size n to get the final relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">ADC</head><p>The Asymmetric Dependency Coefficient (ADC) is a feature ranking method that measures the dependency of class label Y on a feature j (corresponding variable is denoted by X j ) using information gain <ref type="bibr" target="#b35">[36]</ref> ADCðY ; jÞ ¼</p><formula xml:id="formula_7">MIðY ; X j Þ HðY Þ ;<label>ð7Þ</label></formula><p>where HðY Þ is the entropy of Y and MIðY ; X j Þ is the mutual information between label Y and feature j defined as</p><formula xml:id="formula_8">HðY Þ ¼ À X y pðY ¼ yÞ log pðY ¼ yÞ;<label>ð8Þ</label></formula><formula xml:id="formula_9">HðX j Þ ¼ À X x pðX j ¼ xÞ log pðX j ¼ xÞ;<label>ð9Þ</label></formula><formula xml:id="formula_10">MIðY ; X j Þ ¼ HðY Þ þ HðX j Þ À HðY ; X j Þ:<label>ð10Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">AW-SVM</head><p>Support vector machine (SVM) is a popular classification algorithm suitable for high-dimensional data because of its insensibility to data dimensionality <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b12">[13]</ref>. There are several variants of the basic SVM. In this work, the linear binary SVM with soft margin is employed because of its good performance for gene-expression data. The linear SVM classifier is, in fact, a hyper plane defined by <ref type="bibr" target="#b41">[42]</ref>:</p><formula xml:id="formula_11">X p j¼1 w j x j þ b 0 ¼ 0;<label>ð11Þ</label></formula><p>where p is the total number of features and w j is the weight of feature j. The weight w j indicates the importance of feature j, and hence, the absolute weight of SVM (AW-SVM) can be used to evaluate and rank the features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Basic Fusion Methods</head><p>In this study, we used two fusion methods including score-based multicriterion fusion and ranking-based multicriterion fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Score-Based Multicriterion Fusion</head><p>In score-based multicriterion fusion, each basis criterion first produces a score vector containing scores of all features, a score combination algorithm is then employed to aggregate the multiple score vectors into one consensus score vector, and a feature ranking procedure is finally performed to rank the features based on their consensus scores. The score-based multicriterion fusion procedure is illustrated in Fig. <ref type="figure" target="#fig_2">4</ref>.</p><p>In score aggregating, it is essential to ensure that the scores produced by different basis criteria are comparable. Thus, score normalization should be done before score combination is performed. In this study, the scores produced by each basis criterion are normalized to the range of ½0; 1. Assume u i is the score vector produced by basis criterion i, the score normalization is performed as follows:</p><formula xml:id="formula_12">u 0 i ¼ u i À u i min u i max À u i min ;<label>ð12Þ</label></formula><p>where u i min and u i max are the minimum and maximum values in vector u i . For all the basis criteria, it is assumed that the larger the score, the better the feature. A simple yet effective score combination method is to take the average of the normalized scores</p><formula xml:id="formula_13">u ¼ 1 m X m i¼1 u 0 i ;<label>ð13Þ</label></formula><p>where m is the number of basis criteria used in fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Ranking-Based Multicriterion Fusion</head><p>Ranking-based multicriterion fusion is to integrate multiple feature selection criteria according to the feature rankings.</p><p>In the fusion process, a basis criterion first produces a feature ranking, where each feature has a position (or order) value; and then, a ranking combination algorithm is applied onto all feature rankings to generate the final consensus ranking. The ranking-based multicriterion fusion procedure is illustrated in Fig. <ref type="figure">5</ref>.</p><p>Compared to the score-based fusion in Fig. <ref type="figure" target="#fig_2">4</ref>, the ranking-based fusion requires a basis criterion to produce a feature ranking rather than a feature score vector.</p><p>Ranking combination (usually termed as rank aggregation) is a common problem in many fields and it has been extensively studied <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b28">[29]</ref>. Among the existing aggregation methods, Borda count, which is originally a voting method based on rankings is simple yet effective rank aggregation method <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b42">[43]</ref>. Suppose, there are m voters (i.e., basis criteria in this study) and a fix set of p candidates (i.e., features). In Borda count, each voter i first gives points to all candidates to generate a point vector v i as follows: the top ranked candidate is given p points, the second ranked candidate is given p À 1 points, and so on. The final points of candidates are the sum of points from the m voters</p><formula xml:id="formula_14">v ¼ X m i¼1 v i<label>ð14Þ</label></formula><p>and the aggregated ranking is obtained by descendingly sorting the final points in v.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MULTICRITERION FUSION BASED RECURSIVE FEATURE ELIMINATION (MCF-RFE) ALGORITHM</head><p>Section 3 presents a robust feature evaluation and ranking method based on multicriterion fusion. If the purpose of feature selection is to improve classification, the above feature ranking method may not necessary be a good choice. It is well acknowledged that a collection of the best features does not necessarily produce the best feature subset. In order to obtain a feature subset to produce good classification results, the multicriterion fusion-based feature evaluating method must be combined with a search strategy.</p><p>Recursive feature elimination (RFE) is a frequently used search strategy in feature selection, see, for example, <ref type="bibr" target="#b16">[17]</ref>. The RFE search procedure can be briefly summarize as follows:</p><p>RFE Procedure:</p><p>1. Given the full feature set F 0 , set i ¼ 0.</p><p>2. Evaluate the merit of each feature in the feature set F i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>Remove the least important feature(s) from F i to obtain feature set F iþ1 . 4. Set i ¼ i þ 1 and goes to Step 2 until a stopping criterion is satisfied. The RFE algorithm generates a group of nested feature subsets, i.e., F 0 ' F 1 ' F 2 . . . . The original RFE eliminates one feature at each iteration and could be computational intensive if it is applied to high-dimensional data such as gene-expression data. For computational efficiency, a variant of the RFE is to eliminate a portion of the features at each iteration (e.g., 50 percent). Based on the RFE strategy and SVM, Guyon et al. <ref type="bibr" target="#b16">[17]</ref> proposed a feature subset selection algorithm SVM-RFE (Support Vector Machine Recursive Feature Elimination), where the merit of a feature is evaluated in terms of its corresponding coefficient in the SVM classifier. SVM-RFE produces feature subsets leading to good classification performance and is often used as a benchmark algorithm <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b38">[39]</ref>. Despite its popularity, SVM-RFE lacks robustness. Motivated by strengths of multicriterion fusion, we formulate a robust feature selection algorithm by combining the multicriterion fusionbased feature evaluation and the RFE search strategy. We name the new algorithm as MCF-RFE (Multicriterion Fusion-based Recursive Feature Elimination) whose procedure is described in Fig. <ref type="figure" target="#fig_3">6</ref>.</p><p>In multiple-criteria fusion, both score-based and rankingbased fusion methods are used: a score-based fusion method is first used to generate a feature ranking, which is then added to the m feature rankings produced by individual basis criteria. After that, the m þ 1 feature rankings are aggregated by a combination to generate the final feature ranking. The fusion procedure is illustrated in Fig. <ref type="figure" target="#fig_4">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data Sets</head><p>In the experimental study, extensive experiments were conducted on the following five gene-expression data sets:</p><p>Colon Data [2]: This type of data were first described by <ref type="bibr" target="#b1">Alon et al., in 1999</ref>. The data set contains expression levels of 2,000 genes for 62 samples including 22 normal samples, and 40 colon cancer samples. The task is to distinguish between normal and tumor samples. The original data can be downloaded from http://microarray.princeton.edu/oncology/.</p><p>Leukemia Data <ref type="bibr" target="#b14">[15]</ref>: Introduced by <ref type="bibr" target="#b14">Golub et al., in 1999</ref>, this data set contains expression levels of 7,129 genes for 47 ALL (Acute lymphoblastic leukemia) leukemia patients and 25 AML (Acute myelogenous leukemia) leukemia patients. The original data can be downloaded at: http://wwwgenome. wi.mit.edu/cgi-bin/cancer/datasets.cgi.</p><p>Prostate Data <ref type="bibr" target="#b36">[37]</ref>: This data set contains expression level of 12,600 genes for 136 samples including 77 prostate tumors, and 59 normal samples. The data set was first described by Singh et al., and the original data are available at: http:// www-genome.wi.mit.edu/cgi-bin/cancer/datasets.cgi.</p><p>CNS Data <ref type="bibr" target="#b30">[31]</ref>: The goal of this study is the molecular investigation of treatment effectiveness for embryonal CNS  (Central Nervous System) tumors. The task is to distinguish between failed and succeed treatment outcomes. There are 60 patients with 7,129 genes in this data set, where 21 patients are survivors and 39 patients are failures. The original data are available at: http://www-genome.wi.mit.edu/mpr/ CNS/.</p><p>DLBCL Data <ref type="bibr" target="#b34">[35]</ref>: This set of data contains 58 DLBCL (diffuse large b-cell lymphoma) samples and 19 FL (Follicular Lymphoma) samples with 7,129 genes. The data are available at: http://www-genome.wi.mit.edu/cgi-bin/ cancer/datasets.cgi.</p><p>The five data sets are briefly summarized in Table <ref type="table" target="#tab_2">2</ref>, where SDR denotes the sample-to-dimension ratio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Setup</head><p>In the experiment, linear SVM was employed as the pattern classifier. The linear SVM was implemented using the LIBSVM <ref type="bibr" target="#b5">[6]</ref> toolbox of version 2.88 and parameter C was empirically set to 0.01 for the first two data sets and 0.1 for the last three data sets. Due to small sample size of the data sets, classification error was estimated using .632 bootstrap <ref type="bibr" target="#b7">[8]</ref> with 300 repeats. Considering the class imbalance in the gene-expression data sets, AUC (area under the ROC curve) <ref type="bibr" target="#b3">[4]</ref> was also employed in the evaluation and comparative study of the feature selection algorithms.</p><p>Besides assessment in classification performance, feature selections algorithms were also evaluated in terms of their capacity to deal with training data perturbation based on the robustness measure <ref type="bibr" target="#b2">(3)</ref>.</p><p>For a pattern classification system without a feature selection component, the variance of classification error estimation reflects the sensitivity of a classifier to variations in training and testing samples. For a pattern classification system that includes a feature selection component, although the variance of classification error estimation is the combined effects of both the feature selection algorithm and the pattern classifier, variance still can be used as an indication of the robustness of the feature selection algorithm. This is because a feature selection algorithm sensitive to training data variation usually produces feature subsets leading to large variance in the classification error estimation. For this reason, the standard deviation of the classification error estimation for each feature selection algorithm was investigated in the experiment.</p><p>In addition to linear SVM, KNN (K ¼ 3) was also used in the experiment. It was found that KNN classification results could obviously support the conclusions made from linear SVM classification results. Due to page limitation, we only present the classification results by linear SVM in this paper. For classification results by KNN, readers can refer to the supplementary files on the website: http:// www3.ntu.edu.sg/home2006/yang0159/RobustFS.htm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparative Study of MCF-RFE with Basis</head><p>Criteria and SVM-RFE We first compare MCF-RFE with the four ranking-based basis criteria: Fisher's ratio (FR), ADC, Relief, and AW-SVM. From the estimated classification error on the five data sets (Figs. 8a, 9a, 10a, 11a, and 12a), we can observe that MCF-RFE outperforms the basis criteria on all five data sets. For example, on DLBCL data (Fig. <ref type="figure" target="#fig_8">12a</ref>), MCF-RFE produces the least classification error of 2.6 percent with only 120 features, while the best basis criterion for DLBCL data, Fisher's ratio, produces an error of 3.8 percent with the same number of features, and achieves its least classification error of 2.8 percent with 290 features. The AUC results (see Figs. 8c, 9c, 10c, 11c, and 12c) are in line with those of the classification error except on CNS data (Fig. <ref type="figure" target="#fig_0">11c</ref>) where MCF-RFE is slightly interior to AW-SVM when the number of selected features is less than 130. The comparison of the standard deviation of error estimation (Figs. 8b, 9b, 10b, 11b, and 12b) also proves the effectiveness of MCF-RFE. Take again the results on DLBCL data as an example (see Fig. <ref type="figure" target="#fig_8">12b</ref>), the standard deviation is 0.024 for MCF-RFE and 0.031 for Fisher's ratio when 120 features are selected. As for feature stability, we find that MCF-RFE does not perform the best but it produces a compromised result of the four basis criteria and the difference between MCF-RFE, and the stablest basis criterion is not much.</p><p>In evaluating a feature selection algorithm, both classification performance and feature stability should be considered. But classification performance should be the first consideration and feature stability should be the secondary because a stable but classification-ineffective selection result does not make any sense. Based on the above considerations, we think the MCF-RFE outperforms the basis criteria because it produces feature subsets with better classification performance and reasonably good stability.</p><p>When it comes to the performance comparison between MCF-RFE and SVM-RFE, we observe that MCF-RFE achieves substantial improvements over SVM-RFE in the feature stability performance on all five data sets (refer to Figs. <ref type="figure" target="#fig_7">8d,  9d, 10d, 11d,</ref> and<ref type="figure" target="#fig_8">12d</ref>). MCF-RFE also produces better classification performance than SVM-RFE except the AUC on CNS data. Here, we still take the results on DLBCL data (see Fig. <ref type="figure" target="#fig_8">12</ref>) as an example. With a subset of 120 features, the respective values of estimated classification error, standard deviation of error estimation, AUC and feature stability of MCF-RFE versus SVM-RFE are 2.6 percent versus 3.6 percent, 0.024 versus 0.030, 0.996 versus 0.992 and 80.7 percent versus 67.2 percent. The good results of MCF-RFE again prove the strengths of multiple-criteria fusion.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparative Study of MCF-RFE with</head><p>Bagging-Based Ensemble Technique (BBET)</p><p>This part conducts a comparative study between MCF-RFE and the bagging-based ensemble feature selection technique (BBET) proposed by Saeys et al. <ref type="bibr" target="#b33">[34]</ref>. BBET is based on instance perturbation and can be applied to any of rankingbased feature selection algorithms. In this study, the procedure of BBET is as follows: first, a number of bags (i.e., subsamples) are generated from the training samples using resampling with replacement technique and a feature ranking algorithm is performed on each of the bags to produce separate feature rankings; then all the feature rankings are combined to form a final feature ranking using an aggregation method. In the experiment, BBET was applied to each of the four basis criteria with 40 bags (which is the same as in <ref type="bibr" target="#b33">[34]</ref>) and Borda count was used as the linear aggregation method. Due to page limitation, only the experimental results on Colon, Prostate, and CNS data sets are presented in Figs. 13, 14, and 15, and the results on Leukemia and DLBCL data sets can be found on the same website as that for KNN classification results. We first compare the feature stability performances because BBET is originally designed to improve robustness of feature selection algorithms. From the stability results in Figs. <ref type="figure" target="#fig_9">13d,</ref><ref type="figure" target="#fig_2">14d</ref>, and 15d, and on website, it can be observed that MCF-RFE performs better than or comparable to the best basis criterion with BBET. For the classification performance including estimated classification error, standard deviation of error estimation and AUC, MCF-RFE performs better with exceptions of AUC and standard deviation on CNS data.</p><p>After comparing carefully, the results of the four basis criteria with BBET with those without BBET, we find that BBET may not always be beneficial, which is different from the conclusion in <ref type="bibr" target="#b33">[34]</ref> that BBET generally provides more robust results. For example, in most of the cases, AW-SVM benefits from BBET while Fisher's ratio does not. We think one possible reason is the small sample size of geneexpression data. In addition, due to the resampling with replacement used in .632 bootstrap in our experiments, only about 63:2% Â 63:2% ¼ 39:9% of the original samples were retained for construction of each feature selector in the ensemble, while in <ref type="bibr" target="#b33">[34]</ref> 90% Â 63:2% ¼ 56:9% of the original samples were used to construct each feature selector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we have analyzed and discussed multicriterion fusion for feature selection on high-dimensional and small-sized data. Motivated by the strengths of fusion of multiple criteria and the recursive feature elimination (RFE) search strategy, we have proposed a feature subset selection algorithm-MCF-RFE. Extensive experiment study of MCF-RFE with Fisher's ratio, Relief, ADC (asymmetric dependency coefficient), AW-SVM (absolute weight of SVM) and the commonly used benchmark algorithm SVM-RFE based on three performance indices including classification error, standard deviation of error estimation and feature stability has been conducted, and the results show that MCF-RFE outperforms in classification performance with reasonably good stability. An comparative study     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Average number of common genes versus number of samples removed when a subset of 10 genes is selected.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. Computation of sum of correlation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Score-based multicriterion fusion. Fig. 5. Ranking-based multicriterion fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The procedure of MCF-RFE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Score and ranking-based multicriterion fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figs. 8 , 9 ,</head><label>89</label><figDesc>Figs. 8, 9, 10, 11, and 12 show the classification results including estimated classification error, standard deviation of classification error estimation and AUC values, as well as the feature stability of six feature selection methods including Fisher's ratio (FR), ADC, Relief, AW-SVM, SVM-RFE, and MCF-RFE, on the five data sets.We first compare MCF-RFE with the four ranking-based basis criteria: Fisher's ratio (FR), ADC, Relief, and AW-SVM. From the estimated classification error on the five data sets (Figs. 8a, 9a, 10a, 11a, and 12a), we can observe that MCF-RFE outperforms the basis criteria on all five data sets. For example, on DLBCL data (Fig.12a), MCF-RFE produces the least classification error of 2.6 percent with only 120 features, while the best basis criterion for DLBCL data, Fisher's ratio, produces an error of 3.8 percent with the same number of features, and achieves its least classification error of 2.8 percent with 290 features. The AUC results (see Figs. 8c, 9c, 10c, 11c, and 12c) are in line with those of the classification error except on CNS data (Fig.11c) where MCF-RFE is slightly interior to AW-SVM when the number of selected features is less than 130. The comparison of the standard deviation of error estimation (Figs. 8b, 9b, 10b, 11b, and 12b) also proves the effectiveness of MCF-RFE. Take again the results on DLBCL data as an example (see Fig.12b), the standard deviation is 0.024 for MCF-RFE and 0.031 for Fisher's ratio when 120 features are selected. As for feature stability, we find that MCF-RFE does not perform the best but it produces a compromised result of the four basis criteria and the difference between MCF-RFE, and the stablest basis criterion is not much.In evaluating a feature selection algorithm, both classification performance and feature stability should be considered. But classification performance should be the first consideration and feature stability should be the secondary because a stable but classification-ineffective selection result does not make any sense. Based on the above considerations, we think the MCF-RFE outperforms the basis criteria because it produces feature subsets with better classification performance and reasonably good stability.When it comes to the performance comparison between MCF-RFE and SVM-RFE, we observe that MCF-RFE achieves substantial improvements over SVM-RFE in the feature stability performance on all five data sets (refer to Figs. 8d, 9d, 10d, 11d, and 12d). MCF-RFE also produces better classification performance than SVM-RFE except the AUC on CNS data. Here, we still take the results on DLBCL data (see Fig.12) as an example. With a subset of 120 features, the respective values of estimated classification error, standard deviation of error estimation, AUC and feature stability of MCF-RFE versus SVM-RFE are 2.6 percent versus 3.6 percent, 0.024 versus 0.030, 0.996 versus 0.992 and 80.7 percent versus 67.2 percent. The good results of MCF-RFE again prove the strengths of multiple-criteria fusion.</figDesc><graphic coords="7,40.59,81.78,222.29,75.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .Fig. 9 .Fig. 11 .</head><label>8911</label><figDesc>Fig. 8. Performance comparisons on colon data. (a) Classification error. (b) Standard deviation of error estimation. (c) AUC. (d) Feature stability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Performance comparisons on prostate data. (a) Classification error. (b) Standard deviation of error estimation. (c) AUC. (d) Feature stability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Performance comparisons on DLBCL data. (a) Classification error. (b) Standard deviation of error estimation. (c) AUC. (d) Feature stability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. MCF-RFE and bagging-based ensemble comparisons on colon data. (a) Classification error. (b) Standard deviation of error estimation. (c) AUC. (d) Feature stability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. MCF-RFE and bagging-based ensemble comparisons on CNS data. (a) Classification error. (b) Standard deviation of error estimation. (c) AUC. (d) Feature stability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc>Top 10 Genes Selected by Five Different Feature Selection Criteria on CNS Data Set with all 60 Samples as Training Data</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 Data</head><label>2</label><figDesc>Sets Characteristics</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank the reviewers for their detailed and valuable comments and suggestions which have helped to improve the manuscript in many ways.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Review of Feature Selection Techniques via Gene Expression Profiles</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">K</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Norwawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Deris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Othman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Symp. Information Technology (ITSim &apos;08)</title>
		<meeting>Int&apos;l Symp. Information Technology (ITSim &apos;08)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Broad Patterns of Gene Expression Revealed by Clustering Analysis of Tumor and Normal Colon Tissues Probed by Oligonucleotide Arrays</title>
		<author>
			<persName><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Barkai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Notterman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gishdagger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ybarradagger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mackdagger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Nat&apos;l Academy of Sciences USA</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="6745" to="6750" />
			<date type="published" when="1999-06">June 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Blocking Strategy to Improve Gene Selection for Classification of Gene Expression Data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bontempi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Computational Biology and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="293" to="300" />
			<date type="published" when="2007-04">Apr. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Use of the Area under the ROC Curve in the Evaluation of Machine Learning Algorithms</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Bradley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1145" to="1159" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Is Cross-Validation Valid for Small-Sample Microarray Classification?</title>
		<author>
			<persName><forename type="first">U</forename><surname>Braga-Neto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Dougherty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="374" to="380" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">LIBSVM : A Library for Support Vector Machines</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/~cjlin/libsvm" />
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Peaking Phenomenon in the Presence of Feature-Selection</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Dougherty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recoginition Letters</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1667" to="1674" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Chernick</surname></persName>
		</author>
		<title level="m">Bootstrap Methods: A Guide for Practitioners and Researchers</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Support-Vector Networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1995-09">Sept. 1995</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="273" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Machine Learning Research: Four Current Directions</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Magazine</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="97" to="136" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ensemble Methods in Machine Learning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. First Int&apos;l Workshop Multiple Classifier Systems (MCS &apos;00)</title>
		<meeting>First Int&apos;l Workshop Multiple Classifier Systems (MCS &apos;00)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">1857</biblScope>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rank Aggregation Revisited</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sivakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Int&apos;l World Wide Web Conf</title>
		<imprint>
			<date type="published" when="2001-05">May 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Support Vector Machine Classification and Validation of Cancer Tissue Samples Using Microarray Expression Data</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Furey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duffy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Bednarski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="906" to="914" />
			<date type="published" when="2000-10">Oct. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semisupervised Learning for Molecular Profiling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Serafini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Merler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jurman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Computational Biology and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="110" to="118" />
			<date type="published" when="2005-10">Oct. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Molecular Classification of Cancer: Class Discovery and Class Prediction by Gene Expression Monitoring</title>
		<author>
			<persName><forename type="first">T</forename><surname>Golub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="issue">5439</biblScope>
			<biblScope unit="page" from="531" to="537" />
			<date type="published" when="1999-10">Oct. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<ptr target="http://www.clopinet.com/isabelle/Projects/NIPS2001/" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gene Selection for Cancer Classification Using Support Vector Machines</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Barnhill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="389" to="422" />
			<date type="published" when="2002-01">Jan. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An Introduction to Variable and Feature Selection</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elisseeff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1157" to="1182" />
			<date type="published" when="2003-03">Mar. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stable and Accurate Feature Selection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gulgezen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cataltepe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Machine Learning and Knowledge Discovery in Databases: Part I (ECML PKDD &apos;09)</title>
		<meeting>European Conf. Machine Learning and Knowledge Discovery in Databases: Part I (ECML PKDD &apos;09)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">5781</biblScope>
			<biblScope unit="page" from="455" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Comparing Rank and Score Combination Methods for Data Fusion in Information Retrieval</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Isak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="449" to="480" />
			<date type="published" when="2005-01">Jan. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Optimal Number of Features as a Function of Sample Size for Various Classification Rules</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lowey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Dougherty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1509" to="1515" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Effective Gene Selection Method with Small Sample Sets Using Gradient-Based and Point Injection Techniques</title>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W S</forename><surname>Chow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Computational Biology and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="467" to="475" />
			<date type="published" when="2007-09">July-Sept. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stability of Feature Selection Algorithms: A Study on High-Dimensional Spaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kalousis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Prados</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hilario</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="95" to="116" />
			<date type="published" when="2007-05">May 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dynamic Classifier Integration Method</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Workshop Multiple Classifier Systems (MCS &apos;05)</title>
		<meeting>Int&apos;l Workshop Multiple Classifier Systems (MCS &apos;05)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="97" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An Unsupervised Learning Algorithm for Rank Aggregation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Small</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Machine Learning (ECML &apos;07)</title>
		<meeting>European Conf. Machine Learning (ECML &apos;07)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="616" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Wrapper for Feature Subset Selection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="273" to="324" />
			<date type="published" when="1997-12">Dec. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improving Stability of Feature Selection Methods</title>
		<author>
			<persName><forename type="first">P</forename><surname>Krizek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hlavac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th Int&apos;l Conf. Computer Analysis of Images and Patterns (CAIP&apos;07)</title>
		<meeting>12th Int&apos;l Conf. Computer Analysis of Images and Patterns (CAIP&apos;07)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4673</biblScope>
			<biblScope unit="page" from="929" to="936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Multiple-Filter-Multiple-Wrapper Approach to Gene Selection and Microarray Data Classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Computational Biology and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="108" to="117" />
			<date type="published" when="2010-03">Jan.-Mar. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Supervised Rank Aggregation</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. World Wide Web</title>
		<meeting>Int&apos;l Conf. World Wide Web</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="481" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Consensus Group Stable Feature Selection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Loscalzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Knowledge Discovery and Data Mining (KDD &apos;09)</title>
		<meeting>Int&apos;l Conf. Knowledge Discovery and Data Mining (KDD &apos;09)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Prediction of Central Nervous System Embryonal Tumor Outcome Based on Gene Expression</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Pomeroy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">415</biblScope>
			<biblScope unit="page" from="265" to="271" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Theoretical and Empirical Analysis of ReliefF and RReliefF</title>
		<author>
			<persName><forename type="first">M</forename><surname>Robnik-Sikonja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kononenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="23" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A Review of Feature Selection Techniques in Bioinformatics</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Saeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Inza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Larranaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="2507" to="2517" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Robust Feature Selection Using Ensemble Feature Selection Techniques</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Saeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Abeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Van De Peer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Machine Learning and Knowledge Discovery</title>
		<meeting>European Conf. Machine Learning and Knowledge Discovery</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="313" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Diffuse Large B-Cell Lymphoma Outcome Prediction by Gene-Expression Profiling and Supervised Machine Learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Shipp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Medicine</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="68" to="74" />
			<date type="published" when="2002-01">Jan. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Information Theoretic Subset Selection for Neural Network Models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Shridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Seagrave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Chemical Eng</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="613" to="626" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Gene Expression Correlates of Clinical Prostate Cancer Behavior</title>
		<author>
			<persName><forename type="first">D</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Cell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="209" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Evaluating the Stability of Feature Selectors that Optimize Feature Subset Cardinality</title>
		<author>
			<persName><forename type="first">P</forename><surname>Somol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Novovicova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Workshop Structural, Syntactic, and Statistical Pattern Recognition</title>
		<meeting>Int&apos;l Workshop Structural, Syntactic, and Statistical Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="956" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Development of Two-Stage SVM-RFE Gene Selection Strategy for Microarray Expression Data Analysis</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Computational Biology and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="365" to="381" />
			<date type="published" when="2007-09">July-Sept. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Theoretical Foundations of Linear and Order Statistics Combiners for Neural Pattern Classifiers</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<idno>95-02-98</idno>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>The Computer and Vision Research Center, Univ. of Texas</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Variants of the Borda Count Method for Combining Ranked Classifier Hypotheses</title>
		<author>
			<persName><forename type="first">M</forename><surname>Van Erp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schomaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Workshop Frontiers in Handwriting Recognition</title>
		<meeting>Int&apos;l Workshop Frontiers in Handwriting Recognition</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="443" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">Statistical Learning Theory</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<ptr target="http://en.wikipedia.org/wiki/Borda_count" />
		<title level="m">Wikipedia</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Stable Feature Selection via Dense Feature Groups</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Loscalzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Knowledge Discovery and Data Mining (KDD &apos;08)</title>
		<meeting>Int&apos;l Conf. Knowledge Discovery and Data Mining (KDD &apos;08)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="803" to="811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">He is currently working on robustness of gene selection for microarray gene-expression data. His research interests include dimension reduction, feature selection of very high-dimensional and small sample size data, and classification of gene-expression data. K.Z. Mao received the BEng</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Z</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">He joined the School of Electrical and Electronic Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="2507" to="2515" />
			<date type="published" when="1989">2006. 1989, 1992, and 1998. 1998 to September 1998. September 1998 to May 2001. June 2001</date>
			<pubPlace>Singapore; MEng</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Nanyang Technological University (NTU) ; PhD degrees from Jinan University, Northeastern University, and Sheffield University ; Nanyang Technological University as</orgName>
		</respStmt>
	</monogr>
	<note>Bioinformatics. where, currently, he is an associate professor. His research interests include machine learning, computational intelligence, biomedical image analysis. and bioinformatics</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">For more information on this or any other computing topic, please visit our Digital Library at www</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
