<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Semi-Supervised Subspace Clustering via Non-Negative Low-Rank Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Xiaozhao</forename><surname>Fang</surname></persName>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Xuelong</forename><surname>Li</surname></persName>
							<email>xuelong_li@opt.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Zhihui</forename><surname>Lai</surname></persName>
							<email>lai_zhi_hui@163.com</email>
						</author>
						<author>
							<persName><forename type="first">Wai</forename><forename type="middle">Keung</forename><surname>Wong</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Bio-Computing Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Graduate School</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Bio-Computing Research Center</orgName>
								<orgName type="department" key="dep2">Shenzhen Graduate School</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">Key Laboratory of Network Oriented Intelligent Computation</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Center for OPTical IMagery Analysis and Learning</orgName>
								<orgName type="department" key="dep2">Xi&apos;an Institute of Optics and Precision Mechanics</orgName>
								<orgName type="laboratory">State Key Laboratory of Transient Optics and Photonics</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>710119</postCode>
									<settlement>Xi&apos;an, Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Institute of Textiles &amp; Clothing</orgName>
								<orgName type="institution">Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Hong Kong Polytechnic University Shenzhen Research Institute</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Semi-Supervised Subspace Clustering via Non-Negative Low-Rank Representation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">588CFBA2683C8A86BD59B05729E81C8D</idno>
					<idno type="DOI">10.1109/TCYB.2015.2454521</idno>
					<note type="submission">received February 2, 2015; revised April 11, 2015; accepted July 3, 2015.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Affinity matrix</term>
					<term>low-rank representation (LRR)</term>
					<term>subspace clustering</term>
					<term>supervision information</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Low-rank representation (LRR) has been successfully applied in exploring the subspace structures of data. However, in previous LRR-based semi-supervised subspace clustering methods, the label information is not used to guide the affinity matrix construction so that the affinity matrix cannot deliver strong discriminant information. Moreover, these methods cannot guarantee an overall optimum since the affinity matrix construction and subspace clustering are often independent steps. In this paper, we propose a robust semi-supervised subspace clustering method based on non-negative LRR (NNLRR) to address these problems. By combining the LRR framework and the Gaussian fields and harmonic functions method in a single optimization problem, the supervision information is explicitly incorporated to guide the affinity matrix construction and the affinity matrix construction and subspace clustering are accomplished in one step to guarantee the overall optimum. The affinity matrix is obtained by seeking a non-negative low-rank matrix that represents each sample as a linear combination of others. We also explicitly impose the sparse constraint on the affinity matrix such that the affinity matrix obtained by NNLRR is nonnegative low-rank and sparse. We introduce an efficient linearized alternating direction method with adaptive penalty to solve the corresponding optimization problem. Extensive experimental results demonstrate that NNLRR is effective in semi-supervised subspace clustering and robust to different types of noise than other state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S UBSPACE analysis is an important technology in sig- nal processing and pattern recognition. An underlying assumption of subspace analysis is that the data often contain some types of structure <ref type="bibr" target="#b0">[1]</ref>. Subspace has been successfully applied to different visual data such as face <ref type="bibr" target="#b1">[2]</ref> and motion <ref type="bibr" target="#b2">[3]</ref> for data visual analysis and clustering <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. Subspace methods can be roughly divided into three categories. The first one is unsupervised subspace learning and typical examples include principal component analysis (PCA) <ref type="bibr" target="#b5">[6]</ref>, clustering and projected clustering with adaptive neighbors (CAN) <ref type="bibr" target="#b6">[7]</ref>, locally line embedding <ref type="bibr" target="#b7">[8]</ref>, and locality preserving projections <ref type="bibr" target="#b8">[9]</ref>. The second category is supervised subspace learning, in which the label information is used to capture discriminant feature representation. The most popular methods of this category are the well-known linear discriminant analysis (LDA) <ref type="bibr" target="#b9">[10]</ref>, 2-D LDA <ref type="bibr" target="#b10">[11]</ref>, marginal fisher analysis <ref type="bibr" target="#b11">(12)</ref>, and neighborhood minmax projections <ref type="bibr" target="#b12">[13]</ref>. The third category is semi-supervised subspace learning <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref>, which utilizes relatively limited labeled data and sufficient unlabeled data to obtain the subspace.</p><p>Subspace clustering is an important clustering problem which attracts much attention in recent years. Generalized PCA (GPCA) is a typical subspace method for clustering data, which transforms the subspace clustering into the problem of how to fit the data with polynomials <ref type="bibr" target="#b16">[17]</ref>. Sparse subspace clustering (SSC) method has been proposed to cluster datapoints that lie in a union of low-dimensional subspaces <ref type="bibr" target="#b17">[18]</ref>. SSC can be used as spectral clustering method, which first learns an affinity matrix from the training datapoints and then obtains the final clustering results based on the constructed affinity matrix by using the corresponding clustering method such as normalized cuts <ref type="bibr" target="#b18">[19]</ref> and Gaussian fields and harmonic functions (GFHF) <ref type="bibr" target="#b14">[15]</ref>. Random sample consensus clusters the datapoints by modeling mixed data as a set of independent datapoints drawn from a mixture of probabilistic distributions <ref type="bibr" target="#b19">[20]</ref>. Unfortunately, some existing subspace clustering methods (i.e., GPCA and matrix recovery <ref type="bibr" target="#b20">[21]</ref>) assume that the data strictly drawn from a single subspace. However, in some real-world applications, the data cannot be characterized by a single subspace. Thus, it is reasonable to consider that the data are approximately drawn from a mixture of 2168-2267 c 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.</p><p>See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p><p>several low-dimensional subspaces <ref type="bibr" target="#b0">[1]</ref>. Recovering such subspace structure naturally imposes a challenging to the subspace clustering. With this view, given a set of datapoints, which may be corrupted by errors and approximately drawn from the subspaces, a good subspace clustering should try to correct the possible errors and at the same time to cluster data into their respective subspaces with each clustering corresponding to a subspace <ref type="bibr" target="#b0">[1]</ref>.</p><p>During the past two decades, a number of robust subspace clustering methods, which are mainly based on low-rank representation (LRR) <ref type="bibr" target="#b0">[1]</ref> and sparse representation theories have been proposed. The well-known robust PCA (RPCA) <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> can efficiently seize the lowdimensional subspace structure by seeking a low-rank component and an error component to approximate the original data. Latent LRR (LatLRR) <ref type="bibr" target="#b22">[23]</ref> and its robust version (RobustLatLRR) <ref type="bibr" target="#b23">[24]</ref> seamlessly integrate subspace clustering and feature exaction into a unified framework. LatLRR can learn two low-rank matrices one of which is used for robust subspace clustering and the other is able to robustly extract salient features from the observation data. Latent SSC <ref type="bibr" target="#b24">[25]</ref> integrates dimensionality reduction and subspace clustering into a framework. The use of the projection can reduce the influence of noise to some extent. Non-negative low-rank and sparse (NNLRS) <ref type="bibr" target="#b25">[26]</ref> graph for semi-supervised learning learns the weights of edges in graph by seeking a NNLRS matrix that represents each datapoint as a linear combination of others. The obtained graph structure can capture both global mixture of subspaces structure and locally linear structure of the data. Despite their great success based on LRR and sparse representation theories, these methods have two obvious disadvantages.</p><p>1) In most of these methods, robust subspace clustering can be performed by first learning an affinity matrix from the given data and then clustering the datapoints to respective subspaces by using the corresponding clustering methods. It is evident that these two steps are independent and thus an overall optimum cannot be guaranteed.</p><p>2) The labeled training samples are always insufficient due to the expensive labeling cost. In the LRR-based subspace clustering, the affinity matrix plays a significant role in exploiting the subspace structure. Thus, it is necessary to use the limited label information to guide the affinity matrix construction so that it can deliver strong discriminant information. However, in the conventional LRR-based subspace clustering methods, the label information is not used to guide the affinity matrix construction. Inspired by the above insights, we propose a robust semi-supervised subspace clustering via non-negative LRR (NNLRR) method. By combining the LRR framework and the GFHF method in a single optimization problem, the supervision information is explicitly incorporated to guide the affinity matrix construction and the affinity matrix construction and subspace clustering are accomplished in one step to guarantee the overall optimum. More specifically, the affinity matrix is obtained by seeking a nonnegative low-rank matrix that represents each data sample as a linear combination of others. We also explicitly impose the sparse and non-negative constraints on the affinity matrix such that it is sparse and the elements in the affinity matrix can be directly used to cluster data. Benefiting from the breakthroughs in high-dimensional optimization <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b28">[29]</ref>, the optimization problem can be solved by convex relaxation. The convex optimization associated with the NNLRR model can be efficiently solved by the linearized alternating direction method with adaptive penalty (LADMAP) <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, which can efficiently use less auxiliary matrix and matrix inversion <ref type="bibr" target="#b25">[26]</ref> and thus it can effectively reduce the computation cost. We conduct extensive experiments of semi-supervised subspace clustering and the validity of NNLRR is demonstrated by the experiment results. In summary, the contributions of this paper includes the following.</p><p>1) The label information is explicitly incorporated to guide the affinity matrix construction so that the affinity matrix can effectively exploit the subspace structure of data.</p><p>Thus the data can be accurately clustered to respective subspaces. 2) Unlike previous semi-supervised subspace clustering methods which separately treat the affinity matrix construction and clustering algorithm, NNLRR integrates these two tasks into one single optimization framework to guarantee the overall optimum. The remaining of this paper is organized as follows. Section II briefly reviews some methods that are closely related to our method. Section III introduces the basis idea of NNLRR and some related discussions. Extensive experiments are conducted in Section IV. Finally, we conclude this paper in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>Since the method proposed in this paper is based on LRR <ref type="bibr" target="#b0">[1]</ref> and GFHF <ref type="bibr" target="#b14">[15]</ref>, we briefly review them to help reading this paper. Before delving in, we list some notations in the following. Sample set matrix is denoted as X = [x 1 , . . . , x u , x u+1 , . . . , x n ] ∈ m×n , where x i | u i=1 and x j | n j=u+1 are the labeled and unlabeled samples, respectively. The labels of labeled samples are denoted as y i ∈ {1, 2, . . . , c}, where c is the total number of classes. The label indicator binary matrix Y ∈ n×c is defined as follows: for each training sample x i (i = 1, 2, . . . , n), y i ∈ c is its label vector. If x i is from the kth (k = 1, 2, . . . , c) class, then only the kth entry of y i is one and all the other entries are zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Gaussian Fields and Harmonic Functions</head><p>GFHF is a well-known semi-supervised learning method, in which the predicted label matrix F ∈ n×c is estimated on the graph with respect to the label fitness and manifold smoothness <ref type="bibr" target="#b29">[30]</ref>. Let us denote F i and Y i as the ith rows of F and Y, respectively. GFHF minimizes the following objective function:</p><formula xml:id="formula_0">min F 1 2 n i,j=1 F i -F j 2 S ij + λ ∞ u i=1 F i -Y i 2 (1)</formula><p>where λ ∞ is a very large number such that u i=1 F i -Y i 2 = 0 is approximately satisfied and F is the predicted labels for all the samples. S n×n is the graph weight matrix which represents the similarity of a pair of training samples. As shown in <ref type="bibr" target="#b29">[30]</ref>, (1) can be reformulated as min</p><formula xml:id="formula_1">F 1 2 Tr F T LF + Tr (F -Y) T U(F -Y) (2)</formula><p>where L ∈ n×n is the graph Laplacian matrix and calculated as L = D -S, where D ii = j S ij is a diagonal matrix. U ∈ n×n is also a diagonal matrix with the first u and the rest nu diagonal elements as λ∞ and 0, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Low-Rank Representation</head><p>We assume that the observed data matrix X ∈ m×n is approximately drawn from a union of c low-dimensional subspaces { i } c i=1 contaminated by error E. The objective function of LRR can be formulated as min</p><formula xml:id="formula_2">Z,E rank(Z) + γ E 0 , s.t. X = AZ + E (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>where γ is a parameter and A is the dictionary that spans the union of subspace c i=1 i . Minimizer Z is the lowest rankness representation of Z with respect to the dictionary A. E is the matrix that characterizes the error in the original X.</p><p>• 0 is the sparsity measure and is defined as the number of nonzero entries. Obviously, direct optimization of (3) is NP-hard <ref type="bibr" target="#b30">[31]</ref>. Thus the optimization problem of ( <ref type="formula" target="#formula_2">3</ref>) is relaxed into the following optimization problem: min</p><formula xml:id="formula_4">Z,E Z * + γ E 1 , s.t. X = AZ + E (4)</formula><p>where Z * is the nuclear norm (i.e., the sum of the singular values) of Z which can approximate the rank of Z. E 1 is a good relaxation of E 0 . When A = I, LRR degenerates to RPCA, which is suitable for recovering a matrix drawn from a single subspace. Generally, dictionary A is replaced by the original matrix X. Thus, (4) can be written as min</p><formula xml:id="formula_5">Z,E Z * + γ E 1 , s.t. X = XZ + E. (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>When ( <ref type="formula" target="#formula_5">5</ref>) is applied to subspace clustering, the obtained Z is used to define an affinity matrix (|Z| + |Z T |), then the clustering results are obtained by applying the corresponding clustering method to the defined affinity matrix. In robust semi-supervised subspace clustering, the affinity matrix is constructed by <ref type="bibr" target="#b4">(5)</ref>, and then the clustering algorithm such as GFHF is directly applied to the constructed affinity matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. NNLRR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Motivations of NNLRR</head><p>In this paper, we focus on robust semi-supervised subspace clustering. How to reasonably exploit the limited label information and to ensure the algorithmic overall optimum are two important issues in machine learning and computer vision fields. The label information is very effective to improve the discriminant ability of the affinity matrix <ref type="bibr" target="#b29">[30]</ref>. However, as shown in Section II-B, in LRR, it is evident that the limited label information is not exploited to guide the affinity matrix construction. Moreover, in LRR, the affinity matrix is first constructed and then the clustering algorithm is applied to the constructed affinity matrix. Such independent two steps cannot guarantee the overall optimum.</p><p>GFHF uses a concise way to incorporate the label information into semi-supervised leaning, which provides a feasible solution to address these two problems. Specifically, we integrate LRR and GFHF into a unified framework so that the label information can be introduced to guide the affinity matrix construction and the affinity matrix construction and subspace clustering are simultaneously performed in one step in order to guarantee the overall optimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model of NNLRR</head><p>The problem at hand is to design a compact model that naturally unifies LRR and GFHF. Our idea is to simultaneously perform the affinity matrix construction and semi-supervised subspace clustering. Therefore, the model of NNLRR is given as follows:</p><p>min</p><formula xml:id="formula_7">F,Z,E n i=1 n j=1 F i -F j 2 Z ij + Tr (F -Y) T U(F -Y) + γ Z * + β E 2,1 s.t. X = AZ + E, Z ≥ 0, Z 0 ≤ T (6)</formula><p>where</p><formula xml:id="formula_8">E 2,1 = n j=1 m i=1 (E j i ) 2 is the 2,1 -norm of E [32]</formula><p>, E j i is the (i, j)th entry of E. γ and β are the parameters to balance the importance of the corresponding terms. The other variables have the same definitions as ( <ref type="formula">2</ref>) and ( <ref type="formula">4</ref>). The nonnegative sparse constraint (Z ≥ 0 and Z 0 ≤ T) is to ensure that the obtained low-rank and sparse matrix can be directly used as the affinity matrix. E 2,1 encourages the columns in E to be zero, which assumes the error is "sample-specific," i.e., some samples are corrupted and the others are clean <ref type="bibr" target="#b0">[1]</ref>. There of course are many choices to match the error term, such as E 2 F for the small Gaussian noise and E 1 for the random corruption. In this paper, we focus on the 2,1 -norm. The first term evaluates the label fitness. The role of the second term is the same as that in <ref type="bibr" target="#b1">(2)</ref>. The third term ensures that Z can capture the global mixture of subspaces via lowrank constraint. The fourth term tries to fit the error in the original data. After we obtain the minimizer (Z * , E * ), AZ * (or X -E * ) can be used to obtain a low-rank recovery of matrix X. The obtained F can be directly used to perform semi-supervised subspace clustering by using the following way. If h = arg max κ F κ i (i = u + 1, . . . , n; κ = 1, . . . , c), then the ith sample is assigned to the hth class, where F κ i denotes the (i, κ)th entry of F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Solution to NNLRR</head><p>The difficulty to solve the NNLRR problem ( <ref type="formula">6</ref>) is that there are three terms closely related to Z. A feasible way is to make the objective function ( <ref type="formula">6</ref>) separable. To this end, an auxiliary matrix W is introduced into (6). We first convert (6) to the </p><formula xml:id="formula_9">Initialization: Z 0 = W 0 = O; E 0 = O; F 0 = O; Y 1,0 = O; Y 2,0 = O; μ 0 = 0.1, μ max = 10 7 , ρ 0 = 1.01, γ = 1, 1 = 10 -7 , 2 = 10 -6 , θ = A 2 F , k = 0; while not converged do 1.</formula><p>Fix the others and update Z by solving (9) 2. Fix the others and update F by solving <ref type="bibr" target="#b9">(10)</ref>.</p><p>3. Fix the others and update E by solving <ref type="bibr" target="#b10">(11)</ref>. 4. Fix the others and update W by solving <ref type="bibr" target="#b11">(12)</ref>. 5. Update the multipliers as follows:</p><formula xml:id="formula_10">Y k+1 1 ← Y k 1 + μ k (X -AZ k -E k ) Y k+1 2 ← Y k 2 + μ k (Z k -W k ) 6.</formula><p>Update the parameter μ follows:</p><formula xml:id="formula_11">μ k+1 = min(μ max , ρμ k ), where ρ = ρ 0 if μ k / X F ≤ 2 1 otherwise 7. Check the convergence conditions X -AZ k -E k F / X F ≤ 1 or μ k / X F ≤ 2 where = max √ θ Z k -Z k+1 F , W k -W k+1 F , E k -E k+1 F , F k -F k+1 F 8. Update k: k ← k + 1. end while Output: F, Z, E. following equivalent problem: min F,Z,E,W n i=1 n j=1 F i -F j 2 W i,j + Tr (F -Y) T U(F -Y) + γ Z * + β E 2,1 s.t. X = AZ + E, Z = W, W ≥ 0, W 0 ≤ T. (<label>7</label></formula><formula xml:id="formula_12">)</formula><p>This problem can be solved by the LADMAP which can use less auxiliary matrix and matrix inversion (Z is not replaced by another auxiliary matrix), hence computation cost can be reduced. Equation ( <ref type="formula" target="#formula_11">7</ref>) can be converted into the following augmented Lagrangian function:</p><formula xml:id="formula_13">(Z, W, F, E, Y 1 , Y 2 , μ) = n i=1 n j=1 F i -F j 2 W i,j + Tr (F -Y) T U(F -Y) + γ Z * + β E 2,1 + Y 1 , X -AZ -E + Y 2 , Z -W + μ 2 X -AZ -E 2 F + Z -W 2 F = n i=1 n j=1 F i -F j 2 W i,j + Tr (F -Y) T U(F -Y) + γ Z * + β E 2,1 + ψ(Z, W, E, Y 1 , Y 2 , μ) - 1 2μ Y 1 2 F + Y 2 2 F s.t. W ≥ 0 ( 8 )</formula><p>where</p><formula xml:id="formula_14">ψ(Z, W, E, Y 1 , Y 2 , μ) = μ/2( X-AZ-E+(Y 1 /μ) 2 F + Z -W + (Y 2 /μ) 2 F ) and A, B = Tr(A T B)</formula><p>. Y 1 and Y 2 are Lagrange multipliers and μ ≥ 0 is a penalty parameter. The LADMAP updates the variables Z, W, E, and F alternately, by minimizing with other variables fixed and then updates Y 1 and Y 2 . With some algebra, the updating scheme can be designed as follows:</p><formula xml:id="formula_15">Z k+1 = arg min Z γ Z * + Z ϕ Z k , W k , E k , Y k 1 , Y k 2 , μ k , Z -Z k + μ k θ 2 Z -Z k 2 F = arg min Z γ Z * + μ k θ 2 Z -Z k + -X T X -AZ k -E k + Y k 1 μ k + Z k -W k + Y k 2 μ k θ 2 F (<label>9</label></formula><formula xml:id="formula_16">)</formula><formula xml:id="formula_17">F k+1 = arg min F n i=1 n j=1 F i -F j 2 W k ij + Tr (F -Y) T U(F -Y) = arg min F Tr F T L k F + Tr (F -Y) T U(F -Y) (<label>10</label></formula><formula xml:id="formula_18">)</formula><formula xml:id="formula_19">E k+1 = arg min E β E 2,1 + μ k 2 X -AZ k+1 + Y k 1 μ k -E 2 F (<label>11</label></formula><formula xml:id="formula_20">)</formula><formula xml:id="formula_21">W k+1 = arg min W≥0, W 0 ≤T Tr( (R W)) + μ k 2 W -Z k+1 + Y k 2 μ k 2 F (<label>12</label></formula><formula xml:id="formula_22">)</formula><p>where Z ϕ is the partial differential of ϕ with respect to Z, θ = A 2 F in <ref type="bibr" target="#b8">(9)</ref>. L ∈ n×n in <ref type="bibr" target="#b9">(10)</ref> is the graph Laplacian matrix and calculated as</p><formula xml:id="formula_23">L k = D k -W k , where D k ii = j W k ij is the diagonal matrix. In (12), R ij = 1 2 F k+1 i -F k+1 j 2 ,</formula><p>is the Hadamard product operator of matrices and is a matrix with all elements are ones. The complete algorithm is outlined in Algorithm 1. Note that steps 1-3 of Algorithm 1 are convex problems and both have closed form solutions. Step 1 is solved via the singular value thresholding <ref type="bibr" target="#b32">[33]</ref>, it can be computed in the closed form</p><formula xml:id="formula_24">Z k+1 = j γ θμ k ⎛ ⎜ ⎜ ⎜ ⎜ ⎝ Z k - -X T X -AZ k -E k + Y k 1 μ k + Z k -W k + Y k 2 μ k θ ⎞ ⎟ ⎟ ⎟ ⎟ ⎠ (<label>13</label></formula><formula xml:id="formula_25">)</formula><p>where j is the thresholding operator with respect to the singular value γ /(θμ k ). It can be found that Z is solved by a proximal method.</p><p>For the problem in step 2, it is straightforward to set the derivative of <ref type="bibr" target="#b9">(10)</ref> with respect to F to zero, namely</p><formula xml:id="formula_26">∂ Tr F T L k F + Tr (F -Y) T U(F -Y) ∂F = 0 (14)</formula><p>then, we have</p><formula xml:id="formula_27">F k+1 = L k + U -1 UY. (<label>15</label></formula><formula xml:id="formula_28">)</formula><p>Step 3 is solved by via the following Lemma 1.</p><p>Lemma 1 <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b33">[34]</ref>: Let Q be a given matrix. If the optimal solution to min</p><formula xml:id="formula_29">P α P 2,1 + 1 2 Q -P 2 F (<label>16</label></formula><formula xml:id="formula_30">)</formula><p>is P * , then the ith column of P * is</p><formula xml:id="formula_31">P * i = P i 2 -α P i 2 Q i if Q i 2 &gt; 0 0 otherwise (<label>17</label></formula><formula xml:id="formula_32">)</formula><p>where P i and Q i are the ith columns of matrices P and Q, respectively.</p><p>To ensure the solution of determining W is sparse, we impose constraint of W i 0 ≤ T on W. We decompose problem <ref type="bibr" target="#b11">(12)</ref> into n independent subproblems each of which can be formulated as a weighted nonnegative sparse coding problem, namely min</p><formula xml:id="formula_33">W i W i R i + μ k 2 W i -Z k+1 + Y k 2 μ k i 2 2 s.t. W i ≥ 0, W i 0 ≤ T (<label>18</label></formula><formula xml:id="formula_34">)</formula><p>where W i and R i are the ith (i = 1, 2, . . . , n) columns of matrices W and R, respectively. And (18) has a closed form solution <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Discussion</head><p>In essence, the goal of our method is to estimate a function F on a NNLRS graph. The integration of the affinity matrix learning and semi-supervised clustering can guarantee that the estimated function F and the learned affinity matrix are perfectly matched, i.e., the algorithmic optimum can be guaranteed. In addition, the label information of labeled samples can enable the learned affinity matrix to have strong discriminant ability. Generally, our method is based on two basic assumptions: 1) local consistency and 2) manifold assumptions. The former implies that nearby samples are likely to have the same label, whereas the latter says samples lying in the same manifold tend to have the same label. In our method, we use a NNLRS graph (affinity matrix) to approximate the underlying manifold and simultaneously propagate labels to unlabeled samples along the learned graph.</p><p>A vector F i ∈ F (i = 1, 2, . . . , n) corresponds to a classification function. ∀F i ∈ F assigns c real values to sample x i and the maximal value of c real values denotes the class of sample x i belongs. To find the optimal vector F i to accurately classify sample x i , the objective function of GFHF <ref type="bibr" target="#b29">[30]</ref> is used as the cost function. The first term of ( <ref type="formula">6</ref>) is the smoothness cost, which means that a good classification function not changes too much between nearly samples. In other words, samples that are close nearby tend to have the nearly same labels. By using the constraint of X = AZ + E, the nearby samples are selected to reconstruct the original samples. The second term of ( <ref type="formula">6</ref>) means a good classification function should not change too much from the labels of the labeled samples. Note that this term is only used on the labeled samples. The goal of the third term of ( <ref type="formula">6</ref>) is to enforce Z to have block-wise structure, which explicitly represents the neighborhood to neighborhood reconstruction. That is to say that the obtained affinity matrix Z can better characterize the similarity of samples and thus the label information can be accurately propagated by the learned graph. The goal of the fourth term of ( <ref type="formula">6</ref>) is to filter out the noisy information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Difference From NNLRS [26]</head><p>To our best knowledge, NNLRS is the most similar one to ours. NNLRS is the originally designed for the semisupervised clustering problem by using the affinity matrix, namely, NNLRS solves the following problem: min</p><formula xml:id="formula_35">Z,E Z * + β Z 1 + λ E 2,1 s.t. X = AZ + E, Z ≥ 0. (<label>19</label></formula><formula xml:id="formula_36">)</formula><p>Once obtaining the optimal Z * , the column vectors of Z * are normalized by z * i = z * i / z i * 2 and the elements in each column vector are pruned by a predefined threshold</p><formula xml:id="formula_37">z * ij = z * ij , if z * ij ≥ 0, otherwise. (<label>20</label></formula><formula xml:id="formula_38">)</formula><p>Although we use the affinity matrix to perform the semisupervised clustering, our NNLRR is quite different from NNLRS in three aspects.</p><p>1) In NNLRS, the NNLRS graph (the affinity matrix) is firstly learned and then the clustering algorithms are performed on the learned graph. In contrast, by integrating GFHF and LRR into a unified framework, the affinity matrix learning and semi-supervised clustering are simultaneously accomplished in a step. Such integration can guarantee the overall optimum. 2) In NNLRS, the limited label information is not exploited to guide the affinity matrix construction, while in our NNLRR, the label information is used to endow the affinity matrix with strong discriminant ability. 3) In NNLRS, to obtain the optimal graph, the learned affinity matrix needs to prune, i.e., some elements of the affinity matrix should be set to 0 by a given threshold value . However, in practice, how to estimate the optimal threshold value is dataset dependent. In NNLRR, by integrating GFHF and LRR, the learned affinity matrix is optimal without any extra operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Connection Between NNLRR and Other Methods</head><p>1) Connection Between NNLRR and GHFH <ref type="bibr" target="#b14">[15]</ref>: If we set U = 0, γ = 0, and β → ∞, then the objective function of IEEE TRANSACTIONS ON CYBERNETICS NNLRR in <ref type="bibr" target="#b5">(6)</ref> reduces the following problem: min</p><formula xml:id="formula_39">F,Z n i=1 n j=1 F i -F j 2 Z ij + Tr (F -Y) T U(F -Y) s.t. X = AZ, Z ≥ 0, Z 0 ≤ T (21)</formula><p>which can be written as min</p><formula xml:id="formula_40">F,Z n i=1 n j=1 F i -F j 2 Z ij + Tr (F -Y) T U(F -Y) + ν X -AZ 2 F , s.t. Z ≥ 0, Z 0 ≤ T (<label>22</label></formula><formula xml:id="formula_41">)</formula><p>where ν is the parameter to balance the different terms. If we further set ν = 0 and discard the constraints, then <ref type="bibr" target="#b21">(22)</ref> becomes the objective function of GFHF.</p><p>2) Connection Between NNLRR and CAN <ref type="bibr" target="#b6">[7]</ref>: If we set U = 0, γ = 0, and β → ∞, then (6) reduces min</p><formula xml:id="formula_42">F,Z n i=1 n j=1 F i -F j 2 Z ij + ν X -AZ 2 F s.t. Z ≥ 0, Z 0 ≤ T (<label>23</label></formula><formula xml:id="formula_43">)</formula><p>which can be somewhat seen as the formulation of CAN in the purpose of assigning the adaptive neighbors for each sample without using the constraints Z ≥ 0 and Z 0 ≤ T. Please note that in CAN the affine constraint and rank constraint are, respectively, imposed on Z and L (graph Laplacian matrix) to ensure that the connected components in the resulted affinity matrix are exactly equal to the cluster number <ref type="bibr" target="#b6">[7]</ref> and the orthogonal constraint F T F = I is used to avoid a trivial solution.</p><p>3) Semi-Supervised CAN: If we set U = 0, γ = 0, and β → ∞. then (6) reduces <ref type="bibr" target="#b21">(22)</ref> which is a formulation of semi-supervised CAN by integrating GHFH and sample reconstruction into a unified framework. Such integration guarantees that for each sample the neighbor assignment is an adaptive process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Convergence and Complexity Analyses</head><p>The convergence of the exact augmented lagrange multipliers algorithm has been proven in the condition that the objective function is smooth <ref type="bibr" target="#b36">[37]</ref>. It is very difficult to prove the convergence of the more complicated LADMAP. In RPCA <ref type="bibr" target="#b21">[22]</ref>, the convergence of inexact augmented lagrange multipliers has been proven in which two variable matrices were iterated alternately. The proposed NNLRR in this paper involves four iterating variable matrices (Z, W, E, F) and the objective of the optimization problem ( <ref type="formula">6</ref>) is not smooth. Thus it is not easy to prove the convergence in theory. According to the theoretical results in <ref type="bibr" target="#b0">[1]</ref>, three conditions are sufficient (but may not necessary) for Algorithm 1 that has a good convergence properties which are as follows.</p><p>1) The parameter μ in step 6 is needed to be upper bounded.</p><p>2) The so-called dictionary A (A = X in our method) is of full column rank.</p><p>3) In each iteration step, the residual produced by η =</p><formula xml:id="formula_44">(Z k , F k , W k ) -(Z, F, W) 2</formula><p>F is monotonically decreasing, where Z k and W k , respectively, denote the solution produced at the kth iteration and (Z, F, W) = arg min whose value is more than that of (Z k , F k , W k ). It has been shown in <ref type="bibr" target="#b0">[1]</ref> that the above conditions can be approximately satisfied. Condition 1 is easy to be guaranteed by step 6 in the proposed method. The dictionary A can be substituted by the orthogonal basis of this dictionary in practice and thus condition 2 is easy to obey. As discussed in <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b37">[38]</ref>, the convexity of the Lagrangian function could guarantee condition 3 satisfied to some extent, although it is not easy to strictly prove the monotonically decreasing condition. Therefore, Algorithm 1 can be expected to have good convergence properties.</p><p>Generally speaking, the major computation burden of NNLRR lies in step 1 since it involves the singular value decomposition (SVD). Specifically, in step 1, the SVD is operated on an n × n matrix, which is time consuming if the number of samples (i.e., n) is very large. As it is referred in <ref type="bibr" target="#b0">[1]</ref>, by substituting A with the orthogonal basis of the dictionary, the computation complexity of step 1 is O(nr 2 A ), where r A is the rank of the dictionary A. The computation complexity of step 2 is about O(n 3 ). The computation complexity of step 3 is about O(n 2 r A ). The computation complexity of step 4 is trivial owing its simple closed solution. Thus, the computation complexity of NNLRR is O(τ (nr 2  A + n 3 + n 2 r A )) in general, where τ is the number of iterations. The iteration number τ depends on the choice of ρ; τ is small while ρ is large, and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In the experiments, the proposed NNLRR method will be tested using four datasets: 1) Yale <ref type="bibr" target="#b8">[9]</ref>; 2) AR <ref type="bibr" target="#b1">[2]</ref>; 3) Extended YaleB <ref type="bibr" target="#b22">[23]</ref>; and 4) COIL20 <ref type="bibr" target="#b29">[30]</ref>. The compared methods include LRR <ref type="bibr" target="#b0">[1]</ref>, SSC <ref type="bibr" target="#b17">[18]</ref>, LatLRR <ref type="bibr" target="#b22">[23]</ref>, RobustLatLRR <ref type="bibr" target="#b23">[24]</ref>, Local subspace Analysis (LSA) <ref type="bibr" target="#b38">[39]</ref>, and NNLRS <ref type="bibr" target="#b25">[26]</ref>. Since the affinity matrix in NNLRR is nonnegative, we also compare the performance of NNLRR and the non-negative sparse graph (SPG) <ref type="bibr" target="#b39">[40]</ref> in terms of semi-supervised clustering performance. For the sake of fair comparison, apart from NNLRR, all the other methods firstly construct an affinity matrix by respective corresponding technique and then GFHF is performed on the constructed affinity matrix to obtain the semisupervised clustering results. In addition, we modify all of the compared methods to the same noise norm, i.e., the 2,1 -norm for fair comparison. For each dataset, we randomly select different samples from per subject as labeled samples and used the remaining as unlabeled samples and all experiments are run five times (unless otherwise stated) and then the mean classification result and standard deviation (%) are reported. The parameters of all these methods are carefully adjusted in order to obtain the best clustering results. From the experiment, we can find the performance of NNLRR is robust to parameter γ (see Fig. <ref type="figure" target="#fig_3">3</ref>). Thus, we set γ = 1 in all the experiments which can guarantee a satisfactory result. Better results may be achieved with tuning it. Parameter β of NNLRR controls the   tradeoff between the error term and other terms. The selection of parameter β is usually based on the prior of the error level of data. In our experiments, we used the grid-search strategy to conduct parameter selection for each algorithm that we implemented. The MATLAB code of NNLRR is publicly available at http://www.yongxu.org/lunwen.html.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiment on the Yale Face Dataset</head><p>The Yale face dataset (http://www.cvc.yale.edu/projects/ yalefaces/yalefaces.html) contains 165 images of 15 individuals and each person provides 11 different images with various facial expressions and lighting conditions. In our experiments, each image was manually cropped and resized to 32 × 32 pixels. Fig. <ref type="figure" target="#fig_0">1</ref> shows the sample images of a person from the Yale face dataset.</p><p>In this experiment, 2-6 images per person are randomly selected as labeled samples and the remaining are regarded as unlabeled samples. Parameter β of NNLRR is set to 34. The clustering results are shown in Table <ref type="table" target="#tab_0">I</ref>, in which NNLRR outperforms other state-of-the-art algorithms. For example, when we select 5 and 6 images per person as labeled samples, the classification accuracy of NNLRR is 78.44% and 82.33% which are, respectively, higher NNLRS (the second best algorithm) by 1.56% and 0.80%, respectively. We randomly select some images which contain different error: glasses, beard, and expression, to demonstrate the performance of corrupted images recovery by NNLRR. The recovery results are shown in Fig. <ref type="figure" target="#fig_2">2</ref>, in which these images with corruption are approximately recovered (indicated with the red boxes). Fig. <ref type="figure" target="#fig_3">3</ref> shows the clustering results (%) versus the variations of  parameter β and γ , respectively, in which the first six images per person were labeled and the remaining were unlabeled. Please note that the experiment (γ ) is just only to verify the algorithmic robustness to γ . Fig. <ref type="figure" target="#fig_4">4</ref> shows the convergence process of algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiment on the AR Face Dataset</head><p>AR face dataset contains over 4000 images corresponding to 126 persons. These images were captured under different facial expressions, illuminations, and occlusions (sun glasses and scarf). The pictures were taken under strictly controlled conditions. In this experiment, we take all the images of the first 30 persons from a subset which provides 3120 gray images from 120 subjects with each subject providing 26 images. Thus, there are 718 images in total are selected in this experiment. Each image is cropped and resized to 32 × 32 pixels. Fig. <ref type="figure" target="#fig_5">5</ref> shows some images of one person from the AR dataset. In this experiment, 2, 5, 8, 11, and 14 images per person are randomly selected as labeled samples and the remaining images are regarded as unlabeled samples. Parameter β of NNLRR was set to 93. IEEE TRANSACTIONS ON CYBERNETICS  Table <ref type="table" target="#tab_1">II</ref> shows the clustering results on the AR dataset. It can be seen that the clustering performance of NNLRR is almost better than all the compared algorithms. Especially, when the size of labeled samples is small, the superiority of NNLRR is very obvious. As the labeled samples increase, LRR and NNLRR have almost similar performance.</p><p>In order to elaborate NNLRRs validity on different noises, following <ref type="bibr" target="#b40">[41]</ref>, we test NNLRR and other algorithms on two subsets of the AR face dataset. The first subset excludes the images wearing glasses or scarf and thus the errors in this subset are mainly shadows and expression (see Fig. <ref type="figure" target="#fig_6">6</ref>). The second subset includes only the first 13 face images and thus the time-caused error is excluded (see Fig. <ref type="figure" target="#fig_7">7</ref>). For the first subset, 1-3 images per subject are randomly selected as labeled samples and the remaining images are regarded as unlabeled samples. For the second subset, the first seven images (no glasses and scarf occlusions) per person are used as labeled samples and the remaining images (glasses or scarf occlusions) are used as unlabeled samples. Table <ref type="table" target="#tab_1">III</ref> shows the clustering results on the first subset, in which NNLRR outperforms all the other methods (parameter β of NNLRR is set to 120). Table <ref type="table" target="#tab_3">IV</ref> shows the clustering results on the second subset (parameter β of NNLRR is set to 160). From this table, we can see that NNLRR outperforms other algorithms when dealing occlusion. Although RobustLatLRR and NNLRS have strong power in handing occlusion, the label information is not used to guide the affinity matrix construction. Thus the improvement of performances of them are not obvious.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiment on the COIL20 Dataset</head><p>The COIL20 dataset (http://www.cs.columbia.edu/CAVE/ software/softlib/coil-20.php) contains 1440 images of  20 objects and each object provides 72 images. The images of each subject were taken at pose intervals of 5 0 . The original images were normalized to 128 × 128 pixels. In this experiment, each image was converted to a gray-scale image of 32 × 32 pixels for computational efficiency in the experiments. Fig. <ref type="figure" target="#fig_8">8</ref> shows some images from the COIL20 dataset. In this experiment, 2, 4, 6, 8, and 10 images per subject are randomly selected as labeled samples and the remaining images are used as unlabeled samples. Table <ref type="table" target="#tab_3">V</ref> shows the clustering results on the COIL20 dataset (parameter β of NNLRR is set to 0.2). Again, NNLRR performs better than the other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experiment on the Extended Yale B Dataset</head><p>The Extent Yale B dataset (http://www.cad.zju.edu.cn/home/ dengcai/Data/FaceData.html) consists of 2432 human face images of 38 subjects. Each subject contains about 64 images taken under different illuminations. Half of the images are corrupted by shadows or reflection. Each image is cropped and resized to 32 × 32 pixels. Fig. <ref type="figure" target="#fig_9">9</ref> shows some images of one person from the Extended Yale B dataset. As with the AR dataset, we use the first 18 persons and 1134 images in total in the Extended Yale B to test different methods.</p><p>In this experiment, 4, 7, 10, 13, and 16 images per subject are randomly selected as labeled samples and the remaining images are regarded as unlabeled samples. It is obvious that NNLRR outperforms other methods as elaborated in Table VI (parameter β of NNLRR was set to 145). The classification accuracy of NNLRR is higher than other methods on the most of cases.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Experiment on Contiguous Occlusions and Random Pixel Corruptions</head><p>In this section, we selected the first 15 persons from the Extended Yale B face dataset in order to test the robustness of NNLRR to different corruptions. We simulate various levels of contiguous occlusions and random pixel corruptions as follows.</p><p>1) Contiguous Occlusions: The block occlusions are randomly added to different locations in the labeled and unlabeled images with block size of 5 × 5, 10 × 10, 15 × 15, and 20 × 20 (see Fig. <ref type="figure" target="#fig_10">10</ref>). 2) Random Pixel Corruptions: We randomly choose pixels from labeled and unlabeled samples and corrupt them by salt and pepper noises. The rates of corrupted pixels are 5%, 10%, 15%, and 20% (see Fig. <ref type="figure" target="#fig_10">10</ref>). Thirty images per person are randomly selected as labeled samples and the remaining images are used as unlabeled samples. Since SPG is somewhat not suitable to address the corruptions and thus we do not compare it in this experiment. Tables VII and VIII, respectively, show the clustering results of different algorithms on the contiguous occlusions and random pixel corruptions. Obviously, the clustering performance of NNLRR is better than other methods on these two cases, which shows the robustness of NNLRR for the contiguous occlusions and random pixel corruptions. Recovering a clear face image from the images contaminated by different level errors is not an easy job <ref type="bibr" target="#b40">[41]</ref>. From Fig. <ref type="figure" target="#fig_11">11</ref>, we can see NNLRR can remove block and salt and pepper noises well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Discussion</head><p>Based on the experimental results shown in the above sections, the following observations can be concluded.</p><p>1) In most previous low-rank-based semi-supervised subspace clustering methods in which the label information is not used so that the constructed affinity matrix cannot deliver enough discriminant information and thus, the performance of them cannot be obviously improved. However, in NNLRR, the label information (i.e., label matrix Y) is used to guide the affinity matrix construction. It can be found from the experiments that NNLRR performs better than the state-of-the-art semi-supervised  subspace clustering methods on the mean classification accuracy by making effective use of the label information in the process of the affinity matrix construction. In most cases, the improvement of classification accuracy is significant such as on the Yale and AR face datasets. 2) NNLRR is more robust than the other compared methods, especially on the dataset with multiple noises. For example, the images in the AR dataset involve different errors such as occlusions (glasses and scarf), illuminations, and expressions, it is not easy job to cluster them. However, the affinity matrix construction and subspace clustering are integrated into one step in NNLRR so that NNLRR can find an overall optimum. In other words, NNLRR can find an optimal balance between the error fitting (i.e., E) and subspace clustering. Thus, NNLRR outperforms the similar methods such as LRR, LatLRR, RobustLatLRR, and NNLRS in most cases. 3) Although SPG imposes nonnegative sparse constraint on the affinity matrix, such constraint only captures locally linear structure of the data but the global mixture of subspaces structure may be lost <ref type="bibr" target="#b25">[26]</ref>. NNLRR can capture the global mixture of subspaces structure via the explicit low-rank constraint. As shown in all the experiments, NNLRR has obvious advantages in terms of clustering performance than SPG. Similarly, the global mixture of subspaces structure is not captured in SSC. Although NNLRS imposes non-negative sparse and low-rank constraints on the affinity matrix, the label information is not used to guide the affinity matrix construction. Thus, the improvement of clustering accuracy is not obvious. 4) NNLRR can properly deal with both contiguous occlusions and random pixel corruptions (see Fig. <ref type="figure" target="#fig_11">11</ref>). 5) From the deduction of algorithm in Section III-C, we also find the main limitation of NNLRR is that the computation cost is still high since it involves to the SVD. We would like to speed up our algorithm in the future. In addition, a issue in NNLRR is how to estimate the parameter β, especially when the data are contaminated by different level errors such as outlier, noise, and corruption, the selection of β is quite challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we propose a novel semi-supervised subspace clustering method called NNLRR, in which the label information is used to guide the affinity construction. Moreover, NNLRR integrates the affinity matrix construction and subspace clustering into one step to guarantee an overall optimum. An associated efficient iteratively LADMAP is introduced to solve the optimization problem, which uses less auxiliary variables and matrix inversion. We conduct adequate experiments to verify that NNLRR is superior to the state-of-the-art methods. In the future, we will explore the applications of this idea on other methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1 :</head><label>1</label><figDesc>Solving NNLRR by LADMAP Input: Data set matrix X; Label indicator matrix Y; Matrix U; Parameters γ and β;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Images of one person from the Yale face dataset.</figDesc><graphic coords="7,49.99,234.27,249.36,90.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Some samples of using NNLRR to correct the errors in the Yale face dataset. Left: the original data matrix X. Middle: the corrected data XZ. Right: the error E.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Clustering results (%) versus parameter. (a) β. (b) γ .</figDesc><graphic coords="7,345.50,306.47,184.08,142.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Convergence process for NNLRR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Some face images of one person from the AR dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Some face images of one person from the first subset of the AR dataset.</figDesc><graphic coords="8,101.49,237.48,146.16,72.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Some face images of one person from the second subset of the AR dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Some images from the COIL20 dataset.</figDesc><graphic coords="9,101.49,320.14,145.68,117.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Some images of one person from the Extended Yale B dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Some examples of original and corrupted images under varying level of contiguous occlusions and different percentages of salt and pepper noises.</figDesc><graphic coords="9,313.00,320.27,249.60,93.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Two examples of using NNLRR to recovery the corrupted Extended Yale B face images. Left: the contaminated matrix X. Middle: the corrected data XZ. Right: the error E.</figDesc><graphic coords="10,119.49,226.48,109.68,66.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I CLUSTERING</head><label>I</label><figDesc>RESULTS (%) ON THE YALE DATASET. NOTE THAT # TR DENOTES THE NUMBER OF LABELED SAMPLES OF ONE SUBJECT</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II CLUSTERING</head><label>II</label><figDesc>RESULTS (%) ON THE AR DATASET. NOTE THAT # TR DENOTES THE NUMBER OF LABELED SAMPLES PER SUBJECT TABLE III CLUSTERING RESULTS ON THE FIRST SUBSET. NOTE THAT # TR DENOTES THE NUMBER OF LABELED SAMPLES OF SUBJECT</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V CLUSTERING</head><label>V</label><figDesc>RESULTS (%) ON THE COIL20 DATASET. NOTE THAT # TR DENOTES THE NUMBER OF LABELED SAMPLES PER SUBJECT TABLE VI CLUSTERING RESULTS (%) ON THE EXTENDED YALE B DATASET. NOTE THAT # TR DENOTES THE NUMBER OF LABELED SAMPLES PER SUBJECT</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE VII CLUSTERING</head><label>VII</label><figDesc>RESULTS (%) ON THE EXTENDED YALE B DATASET WITH CONTIGUOUS OCCLUSIONS TABLE VIII CLUSTERING RESULTS (%) ON THE EXTENDED YALE B DATASET WITH RANDOM PIXEL CORRUPTIONS</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>&amp; Clothing, Hong Kong Polytechnic University, Hong Kong, and Hong Kong Polytechnic University Shenzhen Research Institute, Shenzhen, China. His current research interests include artificial intelligence, pattern recognition, and optimization of manufacturing scheduling, planning, and control. He has published over 50 scientific articles in refereed journals, including the IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, the Pattern Recognition, the International Journal of Production Economics, the European Journal of Operational Research, the International Journal of Production Research, the Computers in Industry, and the IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science Foundation of China under Grant 61370163, Grant 61233011, and Grant 61332011, in part by the Shenzhen Municipal Science and Technology Innovation Council under Grant JCYJ20130329151843309, Grant JCYJ20130329151843309, Grant JCYJ20140417172417174, and Grant CXZZ20140904154910774, in part by the China Post-Doctoral Science Foundation under Project 2014M560264, and in part by the Shaanxi Key Innovation Team of Science and Technology under Grant 2012KCT-04. This paper was recommended by Associate Editor Y. Jin.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>He is currently with the Institute of Textiles</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust recovery of subspace structure by low-rank representation</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="171" to="184" />
			<date type="published" when="2013-01">Jan. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Data uncertainty in face recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1950" to="1961" />
			<date type="published" when="2014-10">Oct. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large-margin multi-view information bottleneck</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1559" to="1572" />
			<date type="published" when="2014-08">Aug. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Manifold regularized multitask learning for semi-supervised multilabel image classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="523" to="536" />
			<date type="published" when="2013-02">Feb. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ReLISH: Reliable label inference via smoothness hypothesis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI<address><addrLine>Quebec, QC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1840" to="1846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Topographic NMF for data representation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1762" to="1771" />
			<date type="published" when="2014-10">Oct. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Clustering and projected clustering with adaptive neighbors</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. KDD</title>
		<meeting>KDD<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="977" to="986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Locality and similarity preserving embedding for feature selection</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Z</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="304" to="315" />
			<date type="published" when="2014-03">Mar. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Orthogonal locality minimizing globality maximizing projections for feature extraction</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Zhang</surname></persName>
		</author>
		<idno>ID 017202</idno>
	</analytic>
	<monogr>
		<title level="j">Opt. Eng</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2009">2009</date>
			<pubPlace>Art</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Incremental linear discriminant analysis using sufficient spanning sets and its application</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="216" to="232" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Algebraic feature extraction for image recognition based on an optimal discriminant criterion</title>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="903" to="911" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graph embedding and extensions: A general framework for dimensionality reduction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="40" to="51" />
			<date type="published" when="2007-01">Jan. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neighborhood minmax projections</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI</title>
		<meeting>IJCAI<address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="993" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised learning on Riemannian manifolds</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyógi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="209" to="239" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using Gaussian fields and harmonic functions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multiview vector-valued manifold regularization for multilabel image classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="709" to="722" />
			<date type="published" when="2013-05">May 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generalized principal component analysis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vidál</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1945" to="1959" />
			<date type="published" when="2005-12">Dec. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sparse subspace clustering</title>
		<author>
			<persName><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vidál</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR<address><addrLine>Miami, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2790" to="2797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000-08">Aug. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Principal component pursuit with reduced linear measurements</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symp. Inf. Theory (ISIT)</title>
		<meeting>Int. Symp. Inf. Theory (ISIT)<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1281" to="1285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust principal component analysis: Exact recovery of corrupted low-rank matrices by convex optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">G</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS<address><addrLine>Whistler, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Latent low-rank representation for subspace segmentation and feature extraction</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1615" to="1622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust latent lowrank representation for subspace clustering</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="369" to="373" />
			<date type="published" when="2014-12">Dec. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Latent space sparse subspace clustering</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="225" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Non-negative low-rank and sparse graph for semi-supervised learning</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR<address><addrLine>Providence, RI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2328" to="2335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Linearized alternating direction method with adaptive penalty for low-rank representation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS<address><addrLine>Granada, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="612" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Alternating direction algorithms for L1-problems in compressive sensing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="250" to="278" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Divide-and-conquer anchoring for near-separable nonnegative matrix factorization and completion in high dimensions</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDM</title>
		<meeting>ICDM<address><addrLine>Dallas, TX, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="917" to="926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Flexible manifold embedding: A framework for semi-supervised and unsupervised dimension reduction</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-H</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1921" to="1932" />
			<date type="published" when="2010-07">Jul. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Optimal mean robust principal component analysis</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1062" to="1070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Joint embedding learning and sparse regression: A framework for unsupervised feature selection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="793" to="804" />
			<date type="published" when="2014-06">Jun. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A singular value thresholding algorithm for matrix completion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Candés</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1956" to="1982" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A fast algorithm for edgepreserving variational multichannel image restoration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Imag. Sci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="569" to="592" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sparse representation classifier steered discriminative projection with application to face recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1023" to="1035" />
			<date type="published" when="2013-07">Jul. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Kernel sparse representation-based classifier</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1684" to="1695" />
			<date type="published" when="2012-04">Apr. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Nonquadratic penalty functions: Convex programming&quot; in Constrained Optimization and Lagrange Multiplier Methods</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bertsekas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Athena Scientific</publisher>
			<biblScope unit="page" from="326" to="340" />
			<pubPlace>Belmont, MA, USA</pubPlace>
		</imprint>
	</monogr>
	<note>ch. 5, sec. 5.3</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On the Douglas-Rachford splitting method and the proximal point algorithm for maximal monotone operators</title>
		<author>
			<persName><forename type="first">J</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="293" to="318" />
			<date type="published" when="1992-04">Apr. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A general framework for motion segmentation: Independent, articulated, rigid, non-rigid, degenerate and nondegenerate</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV<address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="94" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Non-negative sparse coding for discriminative semi-supervised learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">W</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR<address><addrLine>Providence, RI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2849" to="2856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Robust subspace segmentation via low-rank representation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1432" to="1445" />
			<date type="published" when="2013-08">Aug. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
