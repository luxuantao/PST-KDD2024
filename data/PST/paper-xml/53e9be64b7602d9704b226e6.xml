<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Limiting Privacy Breaches in Privacy Preserving Data Mining</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alexandre</forename><surname>Evfimievski</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Johannes</forename><surname>Gehrke</surname></persName>
							<email>johannes@cs.cornell.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ramakrishnan</forename><surname>Srikant</surname></persName>
							<email>srikant@almaden.ibm.com</email>
							<affiliation key="aff2">
								<orgName type="institution">IBM Almaden Research Center</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Limiting Privacy Breaches in Privacy Preserving Data Mining</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0988F704F0E39D0DF8C650765581F1D3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There has been increasing interest in the problem of building accurate data mining models over aggregate data, while protecting privacy at the level of individual records. One approach for this problem is to randomize the values in individual records, and only disclose the randomized values. The model is then built over the randomized data, after first compensating for the randomization (at the aggregate level). This approach is potentially vulnerable to privacy breaches: based on the distribution of the data, one may be able to learn with high confidence that some of the randomized records satisfy a specified property, even though privacy is preserved on average.</p><p>In this paper, we present a new formulation of privacy breaches, together with a methodology, "amplification", for limiting them. Unlike earlier approaches, amplification makes it is possible to guarantee limits on privacy breaches without any knowledge of the distribution of the original data. We instantiate this methodology for the problem of mining association rules, and modify the algorithm from <ref type="bibr" target="#b10">[9]</ref> to limit privacy breaches without knowledge of the data distribution. Next, we address the problem that the amount of randomization required to avoid privacy breaches (when mining association rules) results in very long transactions. By using pseudorandom generators and carefully choosing seeds such that the desired items from the original transaction are present in the randomized transaction, we can send just the seed instead of the transaction, resulting in a dramatic drop in communication and storage cost. Finally, we define new information measures that take privacy breaches into account when quantifying the amount of privacy preserved by randomization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The explosive progress in networking, storage, and processor technologies is resulting in an unprecedented amount of digitization of information. In concert with this dramatic and escalating increase in digital data, concerns about privacy of personal infor-This work was supported in part by NSF Grants IIS-0084762, IIS-0121175, IIS-0133481, and CCR-0205452, the Cornell Information Assurance Institute, and by gifts from Microsoft and Intel. mation have emerged globally <ref type="bibr" target="#b6">[6,</ref><ref type="bibr">8,</ref><ref type="bibr" target="#b16">15]</ref>. The concerns over massive collection of data are naturally extending to analytic tools applied to data. Data mining, with its promise to efficiently discover valuable, non-obvious information from large databases, is particularly vulnerable to misuse <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b16">15,</ref><ref type="bibr" target="#b21">20]</ref>.</p><p>The concept of privacy-preserving data mining has been recently been proposed in response to the above concerns <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b14">13]</ref>. There have been two broad approaches. The randomization approach focuses on individual privacy, and reveals randomized information about each record in exchange for not having to reveal the original records to anyone <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b11">10,</ref><ref type="bibr" target="#b19">18]</ref>. In the secure multi-party computation approach, the goal is to build a data mining model across multiple databases without revealing the individual records in each database to the other databases <ref type="bibr" target="#b14">[13,</ref><ref type="bibr" target="#b13">12,</ref><ref type="bibr" target="#b22">21]</ref>. In this paper, we focus on privacy breaches in the context of the randomization approach. We now describe prior work in this area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Randomization Approach</head><p>The problem of building classification models over randomized data was addressed in <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b1">1]</ref>. Each client has a numerical attribute ¡ £¢ , e.g. age, and the server wants to learn the distribution of these attributes in order to build a classification model. The clients randomize their attributes ¡ ¤¢ by adding ran- dom distortion values ¥ ¦¢ drawn independently from a known distri- bution such as a uniform distribution over a segment or a Gaussian distribution. The server collects the values of ¡ ¢ § ¥ ¢ and recon- structs the distribution of the ¡ ¢ 's using a version of the Expectation Maximization (EM) algorithm that provably <ref type="bibr" target="#b1">[1]</ref> converges to the maximum likelihood estimate of the desired original distribution.</p><p>In <ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b10">9]</ref>, the goal is to discover association rules over randomized data. Each client has a set of items (called a transaction), e.g. product preferences, and here the server wants to determine all itemsets whose support (frequency of being a subset of a transaction) is equal to or above a certain threshold. To preserve privacy, the transactions are randomized by discarding some items and inserting new items, and then are transmitted to the server. Statistical estimation of original supports and variances given randomized supports allows the server to adapt Apriori algorithm <ref type="bibr" target="#b2">[2]</ref> to mining itemsets frequent in the non-randomized transactions by looking at only randomized ones.</p><p>Privacy It is not enough to simply concentrate on randomization and recovery of the model. We must also ensure that the randomization is sufficient for preserving privacy, as we randomized in the first place to achieve privacy. For example, suppose we randomize age ¡ ¢ by adding a random number ¥ ¢ drawn uniformly from a segment © ¦ . Assuming that the server receives age 120 from a user, privacy is somewhat compromised, as the server can conclude that the real age of the user cannot be less than 70 (otherwise ¡ £¢ § ¥ ¦¢ "! § $# &amp;% ¦' ( ). Thus the server has learned a potentially valuable piece of information about the client -information that is correct with % ) 0 01 probability. Analogously, sup- pose we randomize a small set of items (a transaction) by replacing each item by a random item with probability 80%. If the transaction contains a subset 2 of 3 items that has a support of % 31 , it has 4 5 ' 06 87 9# @ A5 0 B C# @ A5 B D1 chance to retain the same set of three items after the randomization. Thus whenever the server sees 2 in the randomized transaction, it learns with high probability of the presence of 2 in the original transaction as well. Indeed, there are % 31 FE D 5 0B G# H A5 0B D1 randomized transactions that have 2 both before and after randomization, while the probability that 2 occurs in 10 randomly inserted items (out of, say, 10,000 possible items) is less than % 3 I QP 31 <ref type="bibr" target="#b10">[9]</ref>.</p><p>We are aware of two approaches for quantifying how privacypreserving a randomization method is. One approach relies on information theory <ref type="bibr" target="#b1">[1]</ref>, the other approach is based on the notion of privacy breaches <ref type="bibr" target="#b10">[9]</ref>. The former approach measures the average amount of information disclosed in a randomized attribute by computing the mutual information between the original and the randomized distribution. The latter approach is a worst-case notion, it gives a criterion that should be satisfied by any privacy-preserving algorithm. Intuitively, a privacy breach occurs if a property of the original data record gets revealed if we see a certain value of the randomized record. In our previous example, the randomized age of 120 is an example of a privacy breach as it reveals that the actual age is at least 70. As another example, a privacy breach occurs if a subset within a randomized transaction makes it likely that some item occurs in the original transaction.</p><p>As we show in this paper, these two approaches are different: Privacy breaches can occur even though mutual information is small, and therefore propose other information-theoretical measures, called "worst-case information," that do bound privacy breaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Paper Outline</head><p>We introduce some basic notation in Section 2, followed by an overview of the contributions of the paper. We define privacy breaches in Section 3, and show how the amplification methodology can limit privacy breaches in Section 4. In Section 5, we use pseudorandom generators to dramatically reduce communication and storage cost of randomized transactions. We present new information measures that take privacy breaches into account in Section 6. We conclude with a summary and directions for future work in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">OVERVIEW</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Basic Notions</head><p>The Model Suppose there are R clients S T 3 U5 )5 )5 ) S WV connected to one server; each client S X¢ has some private information ¡ £¢ . The server needs to learn certain aggregate (statistical) properties of the clients' data. The clients are comfortable with this, but they are reluctant to disclose their personal information ¡ ¢ . To ensure privacy, each client S ¢ sends to the server a modified version Y ¢ of ¡ ¢ . The server collects the modified information from all clients and uses it to recover the statistical properties it needs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assumptions</head><p>We assume that each client's piece ¡ ¢ of private information belongs to the same fixed finite set ba . Furthermore, we assume that each ¡ ¢ is chosen independently at random accord- ing to the same fixed probability distribution. This distribution, denoted c ba , is not private, the clients allow the server to learn it.</p><p>The assumption of independence implies that, once c ¤a is known, the private information ¡ d of all clients S ed besides client S X¢ tells nothing new about S X¢ 's own private information ¡ £¢ . Randomization Before sending it to the server, each client S ¢ hides its personal data ¡ £¢ by applying a randomization operator f 4 ¡ Q6 . The output of f 4 ¡ ¢ 6 is random, whose distribution depends on ¡ ¢ and on nothing else. Only one instance Y ¢ of f 4 ¡ ¢ 6 is sent to the server by client S ¢ . The set of all possible outputs of f 4 ¡ Q6 is denoted by ¤g and is assumed to be finite. For all ¡ ih 9`a and Y ph q¤g , the probability that f 4 ¡ Q6 outputs Y is denoted by c r¨¡ ts uY wv # P ¨f 4 ¡ Q6 x# yY 5</p><p>By receiving Y ¢ from S ¢ , the server learns something about ¡ ¢ . Note that, by independence assumption above, all Y d for F # disclose nothing about ¡ £¢ and can be ignored in privacy analysis; they certainly help the server to learn distribution c a , but for pri- vacy analysis we assume that the server knows c a . The problem is to measure how much can be disclosed by Y D¢ about ¡ £¢ , and to find randomization operators that keep the disclosure limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Contributions</head><p>Refined Definition of Privacy Breaches A privacy breach is a situation when, for some client S X¢ , the disclosure of its randomized private information Y D¢ to the server reveals that a certain property of S ¢ 's private information holds with high probability. Privacy brea- ches were defined in <ref type="bibr" target="#b10">[9]</ref>; here we refine that definition by explicitly setting the limit to prior probability of a property. Prior probability is the likelihood of the property in the absence of any knowledge about S ¢ 's private information; posterior probability is the likeli- hood of the property given the randomized value Y ¢ . Without a bound on prior probability, there always are properties whose posterior probability is very high even if no information is disclosed, e. g. the property 4 ¡ £6 "¡ # ¡ ", In Section 3, we give the new definition (Definition 1) of privacy breaches and then further classify them into upward and downward privacy breaches. We give an example for both kinds of breaches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Amplification</head><p>(see Definition 3 for details). In Statement 1 we prove that if a randomization operator satisfies this condition for some randomized value Y , then the disclosure of Y to the server has a limited effect at breaching privacy, depending on the value of .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Itemset Randomization</head><p>In Section 4.2 we apply amplification (Statement 1) to randomizing itemsets (in the framework of mining association rules). We give a heuristic, based on the solution of an optimization problem, that allows us to choose randomization parameters so that e the randomization operator satisfies condition (1); e the supports of the original itemsets can be recovered from randomized transactions. We illustrate the practical utility of our method through some tradeoff charts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compression of Randomized Transactions</head><p>Both in the earlier approaches and in the amplification approach for itemset randomization, the randomized transactions may be very long and memory-consuming. Each randomized transaction often contains many thousands of items (order of magnitude more than original transactions); this is needed in order to hide the true items, for preserving privacy. Fortunately, there is a way to "compress" randomized transactions without compromising privacy or support recovery. The idea is to use a pseudorandom generator for computing which items belong to each randomized transaction. The seed for the pseudorandom f generator, one seed per transaction, is chosen so that the randomized transaction contains or does not contain certain pre-selected items from the original transaction. This seed is sufficient to compute the whole randomized transaction or any portion of it, so it serves as a "compressed" randomized transaction.</p><p>Section 5 explains how one can construct a suitable pseudorandom generator using error-correcting codes. The method can reduce the size of randomized transactions by several orders of magnitude, without any effect on either privacy or support recovery. The use of the pseudorandom generator results in dropping the full probabilistic independence of "false" items inserted into the randomized transaction, but instead having only g -wise independence for a sufficiently large integer g . Privacy preserving capability of the new randomization operator can be evaluated using amplification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Worst-Case Information</head><p>In Section 6, we elaborate upon the work in <ref type="bibr" target="#b1">[1]</ref> on measures of privacy. We show that the value of the classical mutual information does not ensure safety from privacy breaches, and introduce new information-theoretic privacy measures whose values provably bound privacy breaches. It turns out that two different subclasses of privacy breaches called "upward" and "downward" privacy breaches (Definition 2) are bounded by different measures, though measures are defined in a very similar way. Worst-case information is obtained from mutual information by writing it in terms of the Kullback-Leibler distance and replacing the expectation with the maximum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PRIVACY BREACHES</head><p>Let S ¢ be any client, let ¡ ¢ be its private information. For the server, prior to randomization, each possible value ¡ of S ¢ 's private information has probability c ¤a 4 ¡ Q6 (see Section 2.1). Let us define a random variable h such that P ¨h i# j¡ b wv # kc ¤a 4 ¡ Q6 5</p><p>Random variable h is the best description of the server's prior knowledge about ¡ ¢ . Now, suppose that the client randomizes ¡ ¢ by computing Y 0¢ # f 4 ¡ ¤¢ 6 , then sends Y D¢ to the server. From the server's point of view, the randomized value Y 0¢ is an instance of a random variable l such that P ¨l m# yY wv # n o 0p Dq ¦r P ¨h i# j¡ b sE c ¨¡ ts uY t5</p><p>Random variables h and l are dependent; their joint distribution is given by:</p><formula xml:id="formula_1">P ¨h i# j¡ u Wl v# jY G# kc ¤a 4 ¡ Q6 uE c ¨¡ ts @Y 5</formula><p>Given Y ¢ , the server can better evaluate the probabilities of possi- ble values for S ¢ 's private information. It uses Bayes formula and computes posterior probabilities:</p><formula xml:id="formula_2">P ¨h i# j¡ Gw l v# jY ¢ wv # P ¨h i# j¡ b bE xc ¨¡ ts @Y D¢ y P ¨l z# jY ¢ 5</formula><p>We can also find the posterior probability of any property 4 ¡ Q6 , where mv ba {s }| true false~:</p><formula xml:id="formula_3">P ¨ 4 h G6 Ww l m# yY ¢ $# n x o ¦ o 0p q r P ¨h i# j¡ Gw l m# jY ¢ 5</formula><p>Informally, a privacy breach is a situation when, for some property 4 ¡ Q6 , the disclosure of Y ¢ to the server significantly increases the probability of this property. If it is important to the client that property 4 ¡ ¤¢ 6 of its private information is not disclosed, then a Given:</p><p>h # y h h q| ' A ¦5 )5 )5 U B 0 0 Dñ f T reveals a lot of information about h when f T 4 h G6 happens to equal zero: the server learns with high probability that h originally was zero. Without knowing that f T 4 h G6 # } , the server considers h # { to be just 1%-likely; but when f T 4 h q6 # y is revealed, h ¡# becomes about 70%-likely. This does not hap- pen when f 4 h G6 # H is revealed, the probability of h d# H be- comes only ¢ s5 B D1 . However, a different kind of personal informa- tion breaks through: the server knows with 100% certainty that h</p><p>does not lie between 200 and 800. The prior probability of this property is about 40%. Only f 7 seems to be a good privacy pre- serving randomization.</p><p>As Example 1 shows, some randomization operators may not be safe because, if they are used, learning a randomized value sometimes significantly affects posterior probabilities for certain properties of the original private value. To fix this, we either have to make sure that all involved properties are harmless when disclosed to the server, or that no property significantly changes its posterior probability. In this paper we take the latter approach. According to Definition 1, for f T 4 ¡ £6 we have a 1%-to-70% privacy breach with respect to property T 4 ¡ £6 , and for f 4 ¡ £6 we have a 40%-to-100% privacy breach with respect to property 4 ¡ Q6 .</p><p>What changes in probability should we classify as "significant"? In Example 1 there are two kinds of changes:</p><p>1. Some property T 4 ¡ Q6 has very low prior probability (i.e., is unlikely), but becomes likely once we learn that</p><formula xml:id="formula_4">f 4 h G6 # Y .</formula><p>In Example 1, the property h £# has a probability jump from 1% to above 70% when f T 4 h q6 # y is revealed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Some property</head><p>4 ¡ Q6 has a probability far from 100% (i.e., is uncertain), but becomes almost 100%-probable (i.e., almost certain) once we learn that f 4 h G6 ¤# FY . Another way of looking at it is by taking a negation: property ¥ e 4 h G6 is likely, but becomes very unlikely once</p><formula xml:id="formula_5">f 4 h G6 # "Y is re- vealed.</formula><p>In Example 1, the property "' ( 0 h B 0 0 " has a downward probability jump from almost 60% to 0% when</p><formula xml:id="formula_6">f 4 h q6 ¦# H is revealed, making it certain that either h § C' 0 or h ¡ CB 0 .</formula><p>This observation suggests that there are two important subclasses of privacy breaches. Let us now give the formal definitions for both of these subclasses. Let T and s be two probabilities, such that T corresponds to our intuitive notion of "very unlikely" (e.g., T # y A5 % ) whereas s corresponds to "likely" (e.g., s ¨# y A5 ); let T 4 ¡ £6 and ¦ 4 ¡ Q6 be two properties.</p><p>Definition 2. We say that there is a straight or upward £T -to- privacy breach with respect to T if for some Y th q¤g</p><formula xml:id="formula_7">P ¨ T 4 h G6 T P ¨ T 4 h G6 Ww f 4 h q6 # jY © s ¦5</formula><p>We say that there is an inverse or downward -to-£T privacy breach with respect to if for some Y ph q¤g</p><formula xml:id="formula_8">P ¨ r 4 h G6 s ¦ P ¨ r 4 h G6 Ww f 4 h q6 # jY T 5</formula><p>Using property ª # {¥ e , we could write this as</p><formula xml:id="formula_9">P ¨ ª 4 h q6 % W© w s ¦ P ¨ ª 4 h G6 Ww f 4 h q6 # jY © «% ¨© $ T 5</formula><p>We also say that the breach is caused by the value Y ph q`g from the inequalities; we assume that P ¨f 4 h G6 x# yY C .</p><p>So, the probability of % © i s corresponds to the intuitive notion of "uncertain," and the probability of % © T means "almost cer- tain." Our task in the next section is to define sufficient conditions for randomization operators that guarantee no breaches of either kind for any property (for given T and s ), regardless of the prior distribution c ¤a . Then we shall look at the problem of constructing the operators that satisfy these conditions and still allow aggregate data mining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">AMPLIFICATION</head><p>If we attempt to use Definition 1 directly to check whether a given randomization operator f causes privacy breaches, we im- mediately encounter two difficulties:</p><p>1. There are ' A¬ q 3r ¬ possible properties, far too many to check them all; 2. We cannot use Definition 1 if we do not know the prior distribution c ¤a of h . In practice, however, a randomization operator must be chosen before c ¤a is learned.</p><p>It turns out that there exists a sufficient test that has neither of these shortcomings, and there are practically useful randomization operators that satisfy this test. The test is based on comparing the operator's transitional probabilities c ¦¨¡ s }Y for the same Y h $`g but different ¡ ih ®ba . Intuitively, if all of the ¡ -values are reasonably likely to be randomized into a given Y , then revealing " f 4 ¡ Q6 # yY " does not tell too much about ¡ . We call this approach amplification because it limits how much some c ¨¡ s ¯Y 's can be amplified with respect to others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">General Approach</head><p>Let us define our privacy preserving restriction on randomization operators, and then prove a statement on bounding privacy breaches:</p><formula xml:id="formula_10">Definition 3. A randomization operator f 4 ¡ Q6 is at most -amplifying for Y th ¤g if ¡ T ¡ Q h qba v c ¨¡ uT s @Y c ¨¡ s @Y ±°( 2)</formula><p>here To prove the statement for downward -to-£T breaches, we first represent them as upward ª T -to-ª breaches with ª T # % © G and ª # % W© $ T , and then note that condition (3) stays satisfied:</p><formula xml:id="formula_11">²%</formula><formula xml:id="formula_12">ª ª T E % W© w ª T % W© w ª # % W© w T % W© w s E s T 5</formula><p>We sometimes call inequality (2) amplification condition for a given Y 9h ¤g . We need to enforce this condition for all Y wh j¤g </p><formula xml:id="formula_13">c ¨¡ ts Y q# Ç T È T 8É T § T T É É T DÊ ½ Ë Y ph 9¨¡ © % 3 0 ¡ § % ) 0 ¦ T È § T T É É T 0Ê Ì Í sÎ UÏ Ð ¨½ ¿Ñ Î 5</formula><p>Their fractional difference is % § % 3 A% ¦ ' % « §AE . Using State- ment 1, we can claim that there are no £T -to-upward breaches from £T # Ã% 3 0! Ã% U¢ 1 to # Ã% ¦ 0' # m 01 , nor the correspond- ing downward breaches. And we do not even need to know c ¤a to claim this.</p><p>Background Knowledge. Amplification condition (2) limits privacy breaches in the presence of certain kinds of background information about the clients. Suppose that client S ¢ has private infor- mation ¡ ¢ , and the server knows the value of some function Ò 4 ¡ ¢ 6 , or more generally, an instance of some random variable Ó that de- pends on ¡ ¢ . From the server's point of view, the probability dis- tribution of the possible values for ¡ ¢ (i. e. of random variable h ), prior and posterior, becomes conditional:</p><p>e Prior: P ¨h i# j¡ b ts P ¨h i# j¡ Gw Ó j# yÔ e Posterior: P ¨h i# j¡ Gw f 4 h q6 # jY ts</p><formula xml:id="formula_14">s P ¨h i# j¡ qw f 4 h G6 x# yY £ ¤Ó j# yÔ</formula><p>If the background information is independent from the randomization operator, all transitional probabilities c r¨¡ Hs ÕY remain the same, so amplification condition remains unaffected and Statement 1 still applies. However, Definition 1 of £T -to-privacy breach in the presence of background knowledge is modified: the breach now occurs when P ¨ 4 h G6 Ww DÓ C# {Ô ( £T and P ¨ 4 h G6 Ww l v# jY £ bÓ j# yÔ " 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Itemset Randomization</head><p>Now we are going to show how to construct randomization operators that satisfy amplification condition (2) for a given and still allow for aggregate data mining by the server. This will be done for one important special case, previously discussed in <ref type="bibr" target="#b10">[9,</ref><ref type="bibr" target="#b18">17]</ref>: randomization of itemsets in association rule mining. Let us start with defining the problem.</p><p>Let Ö be a set of items, for example products in an on-line store. Suppose there are R clients, each having a transaction × ¢ , where × ¢ is a subset of Ö . The items in × ¢ may represent purchases or preferences of client . We assume that all transactions have the same size Ø and that each transaction is an independent instance of some distribution that is not hidden. In real life, transactions have different sizes, but the server can group together transactions according to their nonrandomized size if the size is not hidden.</p><p>The server wants to learn itemsets 2 ÃÙ Ö that occur frequently within transactions. That is, it needs all itemsets whose support sup</p><formula xml:id="formula_15">4 2 ¤6 jv # ÛÚ | 3 Xw ¦ e# % x5 )5 )5 R ¸ b2 Ü × ¢ R</formula><p>is at least some minimal support Ý 3Þ eß à . However, the clients are not willing to disclose their personal transactions, so they use randomization. Here we are going to consider the class of randomizations called "select-a-size," defined in <ref type="bibr" target="#b10">[9]</ref>. The definition is as follows: 1. The operator selects an integer at random from the set | ( A 3% )5 )5 35 ) 8Ø G~so that P ¨ is chosen Q# 9c ±¨ ; 2. It selects items from × , uniformly at random (without re- placement). These items, and no other items of × , are placed into × ª ; 3. It considers each item ã y h × in turn and tosses a coin with probability of "heads" and % ¦© C of "tails". All those items for which the coin faces "heads" are added to × 8ª .</p><p>Let us constrain the select-a-size operator with our amplification condition, to ensure the desired limitation on privacy breaches. We shall use the non-strict form <ref type="bibr" target="#b2">(2)</ref> </p><p>Then condition (2) becomes</p><formula xml:id="formula_17">DT 3 v ï ¨ 0T £ ï ¨ " 5<label>(5)</label></formula><p>While satisfying this condition, we want to transmit as much aggre ð gate information as possible. Randomized transactions are used by the server in order to determine frequent itemsets. So, we would like to ensure that frequent itemsets in randomized transactions have supports as different as possible from infrequent itemsets, with respect to the standard deviation of the supports. Among the parameters of select-a-size, determines the amount of new items added, and | c ±¨ ~á d Uâ É determines the amount of original items deleted. Given , a reasonable heuristic is to set the c ±¨ 's so that, on average, as many original items as possible make it to the randomized transaction. Thus, we are maximizing the follow- </p><formula xml:id="formula_18">ing expectation: ñ 4 c Q6 Cv # E ò » ôó õ ò w × ä p× ª w # á n d â É E 8c</formula><formula xml:id="formula_19">(÷ ¤h $| ( )% 0 35 U5 )5 ) Ø m© i% ¦~, we have E ï ¨ ( £# E ï % U Q# m5 U5 )5 # E ï ¨ ÷ Q# # ï ¨ ÷ § % U Q# 5 )5 U5 # ï ¨Ø 5 (6)</formula><p>The proof of this statement is in Appendix A.1.</p><p>If, instead of trying to have more items of × in × xª , we are trying to have more -itemsets of × in × ª , then we are maximizing</p><formula xml:id="formula_20">E ò » ôó õ ò Ú | ¦2 Ü × Ww 2 Ü × ª (w 2 w # y ¤ṽ# á n d â É è ê E 8c u¨</formula><p>which is also subject to Statement 2 since function Ò 4 6 # È d ø Ê is increasing. So, the solution again has the form (6), possibly with a different (÷ .</p><p>Our heuristic thus reduces the problem of selecting parameters and | c ±¨ ~á d Uâ É to the problem of selecting and (÷ , where ÷ is discrete. How to set these two parameters depends on the expected properties of the data, such as how many items are in the itemsets we are mining and what supports these itemsets and their subsets are likely to have. We can use methods from <ref type="bibr" target="#b10">[9]</ref> to evaluate the variance in the support estimators, with extra caution when inverting the transition matrix for partial supports since it may be singular for some and ÷ .</p><p>We computed how much is recoverable after a select-a-size randomization whose parameters are restricted by the amplification condition. The graphs presented here are similar to those in <ref type="bibr" target="#b10">[9]</ref>. Again, we use the notion of the lowest discoverable support (LDS), which is the lowest possible support that, when recovered after randomization, has a statistically significant separation from zero. By "statistically significant" we mean a separation from zero by four standard deviations. We have computed LDS, in percent, for 1item, 2-item, and 3-itemsets while varying three numbers:</p><p>1. The privacy breach level £T (in percent), which we define as the least prior probability for an allowed £T -to-privacy breach with # { 01 ;  Parameters ÷ and are chosen to minimize the maximum of 1- item, 2-item, and 3-item LDS.</p><p>Figure <ref type="figure">1</ref> shows how LDS depends on the privacy requirement.</p><p>We require that there are no breaches with the prior below T and posterior at 50%, where T # k% 31 {5 )5 )5 U% 3 D1 . As we see, we can recover supports of about 0.5% when the worst breaches (to 50%) allowed are from 5% to 50%.</p><p>The graph on Fig. <ref type="figure" target="#fig_3">2</ref> has its T fixed at 5%, but varies transac- tion size from 3 to 10. Of course, the longer the transaction, the harder it is to recover supports, since there is more private data to be randomized. Finally, the graph on Fig. <ref type="figure" target="#fig_4">3</ref> shows how the number of transactions affects the recovery (in other graphs the default is 5 million transactions). LDS is roughly inversely proportional to the square root of the number of transactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">COMPRESSING RANDOMIZED TRANSACTIONS</head><p>When applying select-a-size randomization operator (Definition 4) to transactions, we generate randomized transactions with lots of false items. In fact, the size of each randomized transaction is comparable to the overall number of considered items, which may be in many thousands. Sending these randomized transactions may take significant network resources, and such a database will require a lot of memory. Fortunately, there is a way to compress randomized transactions without causing privacy breaches. The idea is u# % 5 U5 )5 å wv</p><formula xml:id="formula_21">P ¨û 4 6 x# % ®w h ý ú AÎ )Î ) $# þ ;</formula><p>2. For all integers % T ² 5 U5 )5 ² ÿ å and when h ý ú Î )Î 3 , the random variables û 4 T 6 , û 4 8 x 36 , . . . , û 4 ÿ (6 are statistically independent.</p><p>Here ú Î )Î 3 is a finite set, å and g are positive integers, t % , and the sign "h ý " means "is chosen uniformly at random from."</p><p>Let å be the overall number of possible items. Instead of rep- resenting a randomized transaction by the list of items it contains, we are going to represent it by a seed h ú Î )Î 3</p><p>. Then, for every item number , we can check whether or not this item belongs to the randomized transaction by computing û 4 6 : item belongs to the transaction if and only if û 4 8 x6 W# Ã% . In other words, there is a mapping from seeds to transactions:</p><formula xml:id="formula_22">4 6 # | item w û 4 6 # % (5<label>(7)</label></formula><p>The set ú Î )Î 3 in many cases can be the set of Boolean strings | ¦ A 3% ¦~ø , where ¡ å .</p><p>Suppose we want to randomize transaction × that has Ø items.</p><p>We shall define a randomization operator (called pseudorandom select-a-size) that uses a pseudorandom generator. The operator is similar to select-a-size operator from Definition 4 and has the same parameters: ® ² &amp;% and | c u¨ y¦á d Uâ É , the latter being a proba- bility distribution over | ( 3% 0 35 )5 U5 ) Ø G~. Given a transaction × and a 4 tú Î )Î 3 å e g A s6 -pseudorandom generator with g yØ , the operator generates the seed # f ª 4 × 6 in three steps:</p><p>1. The operator selects an integer at random from the set | ( 3% 0 35 )5 U5 ) Ø G~so that P ¨ is chosen Q# 9c ±¨ ; 2. It selects items from × , uniformly at random (without replacement). Without loss of generality, assume that items × )% t × U¨' ¦ 35 )5 )5 ) x× )¨ are selected. Any seed that satisfies this condition must have equal chances to be selected. This seed is returned as (the seed for) the randomized transaction.</p><p>Pseudorandom select-a-size operator will always find some seed at Step 3 because, if we take h ý ú Î )Î 3 any ¢ ²Ü 2 we have (see (7) for the definition of ): P ¨f 4 × 6 ä t2 y# £¢ i# P ¨ 4 f ª 4 × 6 6 Qä t2 y# £¢ 5 (8) Proof. Let us pay attention only to the items within set 2 ¥¤ × . There are at most g such items. By the definition of pseudorandom gener- ator (Definition 5), as long as seed is chosen uniformly at random, the values of û 4 6 for h y2 ¦¤ q× are independent and equal 1 with probability . The first two steps of both randomization opera- tors are the same: we select which subset × É Ü × is going to belong to the randomized transaction. During the last step, e In the "usual" operator, each item from 2 ¨ § ±× , independently, has probability to get into f 4 × 6 ; e In the pseudorandom operator, we select a seed h ý ú Î )Î 3</p><formula xml:id="formula_23">such that 4 f ª 4 × 6 6 ä $× # &amp;× É . Since h ý ú AÎ )Î )</formula><p>, the distribution of items from 2 © § ¤× is not affected by the choice of × 8É ; each item, independently, has probability to get into 4 f ª 4 × 6 6 .</p><p>So, both operators induce identical distributions on items within 2 § × , and in particular, satisfy (8).</p><p>It follows from Statement 3 that all the mathematical apparatus for support and variance estimation from <ref type="bibr" target="#b10">[9]</ref> is applicable to pseudorandom select-a-size operators as well, as long as we are working with itemsets of size at most g ¤© iØ . Indeed, pseudorandom oper- ator f 4 × 6 is a per-transaction operator (it randomizes each transac- tion independently and its distribution is defined by × ). Generally speaking, it is not item-invariant; however, for an itemset 2 of size at most g © $Ø we have</p><formula xml:id="formula_24">P )w 4 f ª 4 × 6 6 ©ä 2 ¦w 0# £ ª # n ¬ ¬ â » P ¨ 4 f ª 4 × 6 6 ©ä t2 y# £¢ # # "n ¬ ¬ â » P ¨f 4 × 6 Dä ¤2 j# ¢ # P 3w f 4 × 6 Dä ¤2 w # £ ª # ²c r¨ ©s ! ª</formula><p>where c ¦¨ Qs " ª is defined in <ref type="bibr" target="#b10">[9]</ref> as c ¨ ©s # ª G# P 3w f 4 × 6 ä t2 w # © ª w ¨w × ä 2 ¦w 0# £ 3 and is shown to depend only on , ª , Ø , w 2 ¦w , and on the parameters of select-a-size randomization. Therefore, we can "bypass" iteminvariance. Now let us find out when pseudorandom select-a-size operator protects from privacy breaches. Here we can no longer restrict ourselves to a few items only, since all items at once are involved in a privacy breach. Instead, we can use the amplification condition (2) and Statement 1 in the same way as we used them for the "usual" select-a-size operator in Section 4.2. The following statement shows that the amplification condition in pseudorandom case translates into exactly the same condition <ref type="bibr" target="#b5">(5)</ref>  3¢ u# zw × É ¢ w for r# % 0 ' . Then</p><formula xml:id="formula_25">P ¨f ª 4 × 8¢ 6 x# G# # P ¨f ª 4 × ¢ 6 x# w $ 4 f ª 4 × ¢ 6 6 Qä t× ¢ # C× É ¢ sE P ¨× É ¢ chosen at Step 2 # P ¨ ý # w ý h ý ú AÎ )Î 3 % 4 ý 6 ©ä p× ¢ # C× É ¢ E Ac ±¨ ¢ è Ø ¢ ê I T # P ¨ ý # w ý h ý ú AÎ UÎ 3 P ¨ 4 ý 6 ©ä t× ¢ # C× É ¢ w ý h ý ú Î )Î 3 E Ac ±¨ ¢ è Ø ¢ ê I T # w ú AÎ )Î 3 w I T d '&amp; 4 % ¨© $ s6 á I d '&amp; E Ac ±¨ 3¢ è Ø ¢ ê I T</formula><p>For the "usual" operator f 4 × 6 , this probability is (for × 8ª # ( 4 6 , w × xª 8w # yØ ª ):</p><formula xml:id="formula_26">P ¨f 4 × ¢ 6 x# C× ª $# ¶c ±¨ ¢ è Ø 3¢ ê I T E U á » I d '&amp; 4 % W© w s6 8ae I £á I ¤á » yç d '&amp; # á » 4 % W© w s6 ae I £á » d '&amp; 4 % W© $ s6 á I d '&amp; E Ac ±¨ ¢ è Ø ¢ ê I T</formula><p>If we divide two probabilities like this, the constant multiplier will cancel out:</p><formula xml:id="formula_27">P ¨f ª 4 × T 6 x# P ¨f ª 4 × 6 x# # P ¨f 4 × T 6 x# ) 4 6 P ¨f 4 × 6 x# ) 4 6 # ï ¨ T ï ¨</formula><p>where ï ¨ ¢ were defined in (4).</p><p>As a consequence of Statement 4, all methodology described in Section 4.2 for select-a-size randomization operators can be applied for pseudorandom select-a-size operators.</p><p>It remains to construct an example of a 4 tú AÎ )Î 3 å e g s6pseudorandom generator. For Ã# % ¦ 0' , these generators, also known as orthogonal arrays <ref type="bibr" target="#b12">[11]</ref>, can be constructed using linear error-correcting codes <ref type="bibr" target="#b15">[14,</ref><ref type="bibr" target="#b17">16]</ref>. A binary linear error-correcting code of size å and distance 0 is the kernel | ¦¡ h 21 ae w 3 {¡ i# 54 6 õf a ( ü å )-matrix 3 (called the parity check matrix) over the field 1 of residues modulo 2 such that any nonzero å -dimensional vector ¡ from the kernel has at least 0 nonzero coordinates.</p><p>The following statement is well-known: Statement 5. In a parity check ( ü å )-matrix for an errorcorrecting code of distance 0 any collection of 0 p© % columns is linearly independent over 1 . If a vector is chosen uniformly at random from 1 ø , then in 3 ©7 any collection of g ¸# 80 © y% bits is distributed as g independent random bits, each with probability % 3 0' of being zero.</p><p>The proof of this statement is in Appendix A.2.</p><p>Let å be the number of all possible items, let Ø be the orig- inal transaction size (considered fixed), and let be the default probability of an item in the select-a-size operator, represented in the form y# uã s 0' @9 where ã and ï are integers. Suppose the server is interested in supports of itemsets of size up to Ý , but no more; then we need a 4 tú Î )Î 3 å e g A x s6 -pseudorandom generator with g # jØ § Ý . Consider an error-correcting code with size ï å and dis- tance 0 $# ï g § % ; let 3 be its parity check matrix with rows, and let ú AÎ UÎ 3 # «| ( 3% (~ø .</p><p>Given h i| ( 3% (~ø and Xh i| D% 0 35 )5 )5 U å ±~, our pseudorandom gen- erator computes a bit as follows:</p><p>1. Compute vector ¡ t# ©3 7 over 1 ;</p><p>2. Take the following subvector of size ï bits:</p><p>¡ ¢ # BA ¡ u¨ï 4 b© $% ¦6 ¤¡ ±¨ï 4 b© w% 36 § % U u5 )5 U5 ) b¡ u¨ï s© i% U DC This pseudorandom generator satisfies Definition 5. Indeed, by Statement 5, if h ý | ¦ A 3% (~ø then any combination of ï g bits of ¡ t# 3 ©7 is independently distributed, each bit being 1 with prob- ability % ¦ ' . As a consequence, any combination of g disjoint ï -bit subvectors is independently distributed, and each ï -bit subvector is "showing" a binary representation of a number below ã with prob- ability ã s 0' E9 X# C .</p><p>How well can we compress randomized transactions using errorcorrecting codes? Consider, for example, the Bose-Chaudhuri-Hocquenghem (BCH) codes <ref type="bibr" target="#b15">[14,</ref><ref type="bibr" target="#b17">16]</ref>; there, for any positive integers ¥ and ' ý I T © y% , we have a parity check matrix of size ¥ @ ü 4 ' ý © y% 36 with distance ' @ § % . If we are dealing with trans- actions of size Ø £# @% ) and are interested in itemsets of size up to Ý $# § and if # }% ¦ ' making ï # }% , for example, then we need distance ï 4 Ø § Ý ¦6 § % q# @% 3AE , which makes ¤# &amp;B . If there are % 3 0 0 items overall, we need ¥ ¦# Ã% ¦! , and hence the size of the compressed transaction is ¥ F # % HG 0AE bits, much less than the ordinary way which needs % 3 A 0 bits. For w# &amp;% ¦ % 3AE we have ï # ¢ and ï 4 Ø § Ý ¦6 § % # AE A% making u# IG 0 , ¥ r# ²% 3 , and com- pressed transaction becomes 0! ( bits, while ordinary way needs at least P 4 % ¦ % 3AE D6 £E )% 3 FQ 2G @G A ! 0' ( bits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">WORST-CASE INFORMATION</head><p>Amplification approach from Section 4 is designed to be independent on the prior distribution, to depend only on the randomization operator itself. There can be other ways to restrict disclosure, other privacy measures that depend both on the prior distribution of private data and on the operator. In this section we consider a class of privacy measures inspired by Shannon's information theory <ref type="bibr" target="#b20">[19]</ref>, adjusted so that they bound privacy breaches.</p><p>In the paper <ref type="bibr" target="#b1">[1]</ref> the authors introduce a measure of privacy which is a function of mutual information between two distributions, the original data distribution and the randomized data distribution. Suppose that h is a random variable such that each data record is its independent instance. Let l z# f 4 h G6 The second randomization f can output , % , or "empty record" d . Whatever its input ¡ is, it outputs d with probability 99.99%, otherwise it outputs ¡ with probability 0.0099% and % r© j¡ with probability 0.0001%:</p><formula xml:id="formula_28">P ¨l # d w h i# j¡ b ¤# y A5 0 0 A P ¨l © W# j¡ Gw h i# j¡ b ¤# y A5 0 0 0 # y E 0% 3 I e P ¨l # m% ¨© $¡ Gw h i# j¡ b ¤# y A5 0 0 0 % # % ¨E 0% ) I e 5 Intuitively,</formula><p>f is a very poor randomizer since if we see, say, l # % , then we know with very high probability that h &amp;# % : P ¨h i# % w l # m% U q# P ¨h &amp;# y w (l # { ¦ $# # 0 ¤E 0% ) AI e ¨E 3 5 0 ¤E 0% ) I e E 3 5 § % WE D% 3 I e E ¦ A5 # £ A5 0 For l T , this probability is only 5 AE , which is much more rea- Now we can compute and compare mutual informations. For l ±T , both of S YT 4 c a ¬ g é â W V c a 6 for Y # m )% are the same, so the average is R 4 h °l ±T 6 x y A5 0' 0 0 °For l © , the Kullback-Leibler distances are very different, and since P ¨l © W# d Q# y A5 0 0 , the average is R 4 h °l 6 x y A5 0 0 E 3 § A5 0 A% E ) A5 A% ) D' % ¡ R 4 h °l ±T 6 5</p><p>Thus, counter to intuition, mutual information says that f is more privacy-preserving than f T .</p><p>Of course, mutual information fails to detect privacy breaches in Example 2 because they are very infrequent: they occur only in 0.01% randomizations. But once a breach occurs, it is detectable, and noone wants to be the unfortunate client who has the breach. Mutual information averages all Kullback-Leibler distances; however, by looking at these distances without taking the average, some breaches become visible. Indeed, in Example 2, distances S UT 4 c a ¬ g é â W V ±c a 6 for f T are both small ( 5 0' D ), whereas for f some distances are big, e.g. S UT 4 c a ¬ g ¦ë â ±T V ±c ¤a 6 &amp; A5 % 3 D' % This indicates that revealing "l © m# Â% " may lead to a privacy breach. The measure that shows the worst possible Kullback-Leibler distance rather than averages them will do better at measuring privacy. We come to the following definition: Definition 6. Let h and l be discrete random variables. We define worst-case information as follows: R Hi 4 h °l ¦6 yv # p¹ (º W S YT 4 c a ¬ g uâ W V c ¤a 6 5   . We define inverse worst-case information with respect to Ò as follows: s ö i 4 h °l 6 Cv # p¹ (º W S YT ö 4 c a V ±c a ¬ g ±â W 6 5</p><p>Even though Kullback-Leibler distance is called "distance," it is not symmetrical, so usually s ö i 4 h °l ¦6 ® # tR ö i 4 h °l 6 . The main difference between the two is that the value of R ö i whereas the value of s ö i 4 h °l 6 depends on the behavior of proper- ties likely a priori. Indeed, in R ö i 4 h °l ¦6 , the average is taken with respect to distribution c a ¬ g ±â W , while in s ö i 4 h °l ¦6 the average is with respect to c ba . The inverse worst-case information is related to downward breaches in the same way as the straight worst-case information to upward breaches. Let us formulate it in the following statement.</p><p>Statement 7. Suppose that revealing f 4 h G6 # Y for some Y causes a downward -to-£T privacy breach with respect to prop- erty 4 h G6 . Then E 3Ò p s £T q § 4 % W© w 6 eE (Ò p % © $ s % © $ £T q s ö i 4 h °f 4 h q6 6 5</p><p>The proof of this statement is in Appendix A.4. Table <ref type="table" target="#tab_12">2</ref> gives average-case and worst-case information measures (with Ò w# a Fc</p><p>) for the three randomization operators from Example 1 (see Section 3). The table shows that f T is more sensitive to upward privacy breaches, f is more sensitive to downward pri- vacy breaches, and f 7 has little sensitivity to both of them. The same trend was shown in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION</head><p>We presented a new defintion of privacy breaches, and developed a general approach, called amplification, that provably limits breaches. Amplification can be used to limit privacy breaches with respect to any single-record property. More importantly, unlike earlier approaches, this approach does not require knowledge of the data distribution to provide privacy guarantees. We instantiated this approach for the problem of mining association rules, and derived the amplification condition for the select-a-size randomization operator.</p><p>Next, we gave a method for compressing long randomized transactions by using pseudorandom generators, and showed that this could reduce their sizes by orders of magnitude. Finally, we defined several new information-theoretical privacy measures that provably bound privacy breaches.</p><p>We conclude with some interesting directions for future research.</p><p>e How do we extend amplification to continuous distributions? e What is the relationship between the specific randomization operators, and the tradeoff between privacy and accuracy?</p><p>In particular, how do we identify the randomization operator and parameters that will provide the highest accuracy in the mining model for a given level of privacy breaches?</p><p>e Are there ways to combine the randomization and the secure multi-party computation approaches that work better than either approach alone?</p><p>Extend ¡ to an å -dimensional vector by setting coordinates ¡ , 5 )5 )5 , ¡ ae to zeros. We get a nonzero vector from the matrix's kernel, which has less than 0 nonzero coordinates -a contradic- tion.</p><p>Consider any 0 © y% coordinates in 3 7 ; w.l.o.g., assume they are the first 0 W© G% coordinates. Since the first 0 ¨© G% rows of 3 7 are linearly independent, they form a 4 0 e© p% ¦6 ü -submatrix 3 {ª of rank 0 © j% . For any 4 0 © j% 36 -dimensional vector , equation 3 ª # has the same number ' ø I ç T of solutions. When every vector is equally likely, every vector ®# (3 ª is therefore also equally likely, that is, its coordinates behave independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Proof of Statement 6</head><p>We first prove two simple lemmas and a corollary. p % × q p % × T § 4 % © x6 % × q Ò p % × T § 4 % ¨© e6 % × q 5 Substitution of the definition of gives × Ò p % × T q § 4 % W© £6 % × Ò p % × q p % × § 4 % W© £6 % × q Ò p % × § 4 % W© £6 % × q which is equivalent to (9).</p><p>Lemma 2. Let h , l , and Ó be discrete random variables such that Ó is independent from l given h , and let × ¤Ò 4 × 6 be convex on × X . Then, for all possible Y , S UT  It is clear that n % , and therefore g T % ¨© 4 m © $ 6 % 0 £T x© m T g £T % °so, g T and g ¦ can serve as probabilities. Let us employ them, then. Define a Boolean random variable Ó that depends on h as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S YT</head><p>1. If 4 h G6 , then Ó says "true" with probability g T ;</p><p>2. If ¥ e 4 h q6 , then Ó says "true" with probability g .</p><p>Of course, Ó is independent from l given h , so Corollary 1 is applicable:</p><p>S UT The statement is proven.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Proof of Statement 7</head><p>Let us start with another corollary of Lemma 2. where P ¨Ó mw ¦l v# jY # £T ) P ¨Ó W £# j 5</p><p>The statement is proven.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Section 4.1 develops a new approach that allows to ensure limitations on privacy breaches for a randomization operator, without any knowledge about the prior distribution c a and applicable to any property of client's private information. Our privacy preserving restriction involves only the operator's transition probabilities c r¨¡ ts uY : ¡ T ¡ Q h ba v c r¨¡ T Xs uY c r¨¡ £ s uY d</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Definition 4 .</head><label>4</label><figDesc>The select-a-size randomization operator has parameters % and | c ±¨ y(á d â É , the latter being a probability dis- tribution over | ¦ A 3% 0 35 U5 )5 ) Ø q~. Given a transaction × , the operator generates another transaction × ª # f 4 × 6 in three steps:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>d 4 %</head><label>4</label><figDesc>, because it will allow us to solve an optimization problem. Denote × ª # f 4 × 6 , Ø ª # Hw × ª w , # "w × ±ä × ª w , and å q# vw Ö w . Then the transitional probabilities of the select-a-size can be written asc ¨× xs @× ª t# c ±¨ È á d QÊ E á » I d 4 % ¨© G s6 8ae I ¤á I £á » yç d 5If there are two transactions × UT and × with 0T 9# µw × T ä ®× xª xw and # zw × ä × 8ª 8w , we have W© w s6 á I d the "default" probability of selecting items from × , and "balance" the c ±¨ 's by dividing them by the "default" probabilities: ï ¨ 9v # c u¨ £ c £ì )í î ¨</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Lowest discoverable support versus transaction size. 5 million transactions, breach level is T # { 01 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Lowest discoverable support versus number of transactions. Transaction size is 5, breach level is T # { 01 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>3 .</head><label>3</label><figDesc>It selects a random seed h ú Î )Î 3 such that û 4 8× U¿% ô6 # 5 )5 )5 # û 4 8× U¨ y6 # m% and û 4 8× U¨ § % ô6 # 5 )5 )5 # û 4 8× U¨Ø ô6 x# { 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>5 ©</head><label>5</label><figDesc>sonable. What does mutual information indicate, however? Let us compute S YT 4 c a ¬ g f&amp; â W V ±c ¤a 6 for e# % 0 ' and Y # y 3% 0 d : Y # y A )% v a @c P ¨h i# jY ¸w (l ±T # jY (AE FG A a @c P ¨h i# % W© wY ¸w (l T # yYP ¨h i# m% W© wY # a @c 5 ¢ A5 G 0' % 3 S UT 4 c a ¬ g é â W V ±c ¤a 6 y A5 AE ¤E ¦ A5 ' (AE FG 0 © i A5 ¢ E 3 A5 G 0' % 3 ¦ y A5 D' ( 0 0 Y # y A )% v a @c P ¨h i# jY ¸w (l # jY ¢ gG A S UT 4 c a ¬ g ¦ë Uâ W V ±c ¤a 6 y A5 0 E 3 5 0B D © i A5 % E 3 5 AE ¢ FG 0 ¦ { 5 A% 3 0' % °Y # d A¡ t# y A 3% v aFc P ¨h i# j¡ Gw ¦l © # d a ¬ g ¦ë Uâ h V ±c ¤a 6 # y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>V ±c Q )6 jv # E o ó `é Ò È c T 4 ¡ Q6 c Q 4 ¡ Q6 Ê 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>V ±c a ¬ g ±â W6 5    Proof. Let us prove the first and then the second inequality using the definition of S YT ö . We shall use Jensen's inequality E 4 £6 4 E £6 with respect to function 4 × 6 e# {Ò 4 % ¦ 3× 6 , which is convex on × X by Lemma 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>ö 4 cPCorollary 1 .Proof.</head><label>41</label><figDesc>a ¬ g uâ W V ±c ¤a ¤6 # E o ó a ¬ g uâ W Ò p P ¨h i# j¡ Gw l m# yYP ¨h # y¡ b q # E ó ¬ g ±â W E o ó a ¬ â g eâ W Ò p % P ¨h i# j¡ b P ¨h i# y¡ Gw ¦l v# jY q E ó ¬ g ±â W Ò d % e f d E o ó a ¬ â g eâ W P ¨h i# y¡ s P ¨h i# y¡ Gw (l v# jY g h 2g h° Using the independence of Ó from l given h , we transform the internal i expectation to the desired fraction: ¨h i# y¡ sP ¨h &amp;# j¡ Gw ¦l v# jY # # E o ó a P ¨h i# j¡ Gw Ó j# yÔ s l v# jY P ¨h i# j¡ Gw l m# yY # E o ó a P ¨Ó C# {Ô w (h i# y¡ 8l v# jY P ¨Ó y# {Ô w (l v# jY # E o ó a P ¨Ó C# {Ô w (h i# y¡ s P ¨Ó C# {Ô pw ¦l v# jY # P ¨Ó j# yÔ P ¨Ó y# yÔ w l m# yY 5The first inequality is thus proven. The second inequality is very analogous: ¬ â P ¨h i# j¡ Gw (l m# jYP ¨h i# j¡ b q lq °E o ó a ¬ â P ¨h i# j¡ qw l m# yY P ¨h i# j¡ b # # E o ó a ¬ g eâ W P ¨h &amp;# j¡ w Ó C# yÔ P ¨h i# j¡ b # E o ó a ¬ geâ W P ¨Ó j# yÔ w yh # j¡ b P ¨Ó j# yÔ # E o ó a ¬ g eâ W P ¨Ó j# yÔ w yh # j¡ u l m# yY P ¨Ó C# {Ô ( # P ¨Ó y# yÔ w yl # yY P ¨Ó C# {Ô ( 5 Both inequalities are now proven. Under the conditions in Lemma 2, we have Follows immediately from the first inequality of Lemma 2: if for every number in one set there is at least as large number in the other set, then the maximal number of the first set the maximal number of the other. Proof of Statement 6. Now we have all the tools to prove the bound on upward privacy breaches. Proof. Let us denote l v# f 4 h G6 and m T # P ¨ 4 h G6 m ¸# P ¨ 4 h G6 Ww l m# yY 5 By Definition 2, we have m T T s m . Let us define , g T , and g ¦ as follows: g 0T t# § 4 % W© m 6 Ág # £T x© m T 3 z# s X© $ T m X© m T 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>:PPP</head><label></label><figDesc>It remains to check that this inequality is exactly what we are proving. Indeed, denote R ¦# £R ö i 4 h °l ¦6 and "open up" the definition of S UT ö ¨Ó mw ¦l v# jY bE (Ò p ¨Ó w l v# jYP ¨Ó W q § § P ¨¥ Ó w l v# jY bE 3Ò p ¨¥ xÓ mw l # {Y P ¨¥ xÓ W q R ¤5Now compute the prior and posterior probabilities of Ó :P ¨Ó W $# g 0T E m T § g E 4 % W© m T 6 # # m T 4 § 4 % ¨© m 6 6 § 4 % ¨© m T 6 4 £T © m T 6 # £ £T § m T 4 © $ £T 6 ¦© j m T 4 m © m T 6 # £ £T § m T 4 © $ £T 6 ¦© m T 4 © w £T 6 # £T °analogously, P ¨Ó mw ¦l v# jY G# g 0T E m § g E 4 % W© m 6 # # m 4 s § 4 % © m )6 6 § 4 % W© m )6 4 T © m T 6 # þ T § m 4 s © $ T 6 § 4 % W© m )6 4 m W© m T 6# þ T § m 4 s © $ T 6 § 4 % W© m )6 4 s © w T 6 # s (5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Corollary 2 .Proof.Proof of Statement 7 .</head><label>27</label><figDesc>Under the conditions in Lemma 2, we have Follows from the second inequality of Lemma 2 in the same way as Corollary 1 from the first. We are now ready to prove Statement 7.Proof. The proof is almost analogous to that of Statement 6. We only have to change places between prior and posterior distributions. Namely, we define m # P ¨ 4 h G6 m T t# P ¨ 4 h q6 ¨w (l v# jY g 0T p# § 4 % ¨© m 6 £g # £T x© m T 3 m# s © w T m X© m T 5 Again, by Definition 2 we have m T £T ¨ m ; we define Ó exactly like before, and in the end get P ¨Ó X £E ¦Ò p P ¨Ó X P ¨Ó w (l v# jY q § § P ¨¥ xÓ X ¤E (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : Prior and posterior (given</head><label>1</label><figDesc></figDesc><table><row><cell>othing f T 4 h G6 # y f 4 h G6 # y f 4 h G6 # y 7</cell><cell>1% 71.6% 4.8% 2.9%</cell><cell>40.5% 83.0% 100% 70.8% f 4 h G6 x# {</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>) probabilities for properties in Example 1</head><label></label><figDesc></figDesc><table><row><cell>significant increase in probability may be a violation of privacy.</cell></row><row><cell>Here is the formal definition of a privacy breach:</cell></row><row><cell>Definition 1. We say that there is a T -to-s privacy breach with respect to property 4 ¡ Q6 if for some Y th `g P ¨ 4 h G6 T and P ¨ 4 h G6 Ww (l v# jY " s (5</cell></row><row><cell>Here T s {% and P ¨l v# jY C .</cell></row><row><cell>Let us consider the following example on privacy breaches.</cell></row><row><cell>Example 1. Suppose that private information ¡ is a number be-tween 0 and 1000. This number is chosen as a random variable h</cell></row><row><cell>such that 0 is 1%-likely whereas any non-zero is only about 0.1%-</cell></row><row><cell>likely:</cell></row><row><cell>P ¨h &amp;# y ¦ $# 5 A%</cell></row><row><cell>P ¨h # { q# 5 0 0 0 $ # % 5 U5 )5 )% 3 0</cell></row><row><cell>Suppose we want to randomize such a number by replacing it with a new random number Y {# f 4 ¡ £6 that retains some information</cell></row><row><cell>3. Given ¡ , let uniformly random number otherwise. f 4 ¡ Q6 be f 4 ¡ Q6 with 50% probability, and a 7</cell></row></table><note><p><p>about the original number ¡ . Here are three possible ways to do it:</p>1. Given ¡ , let f T 4 ¡ £6 be ¡ with 20% probability, and some other number (chosen uniformly at random) with 80% probability. 2. Given ¡ , let f 4 ¡ Q6 be ¡ § i 4 % 3 A% ¦6 , where is chosen uniformly at random in | © % ) 0 A 35 U5 35 ) )% 3 ~. In Table 1 we compute prior and posterior probabilities of two properties of h : property T 4 h G6 "h # " and property r 4 h G6 "h h | (' 0 ¦5 )5 )5 ) B 0 D~." We can see that randomiza- tion operator</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>and ³ A¡ 9v Dc r¨¡ ws ´Y . Operator f 4 ¡ Q6 is at most -amplifying if it is at most -amplifying for all suitable Y h q`g . Consider any distribution c ¤a ; since it is nonzero on at least one ¡ h Gba , we have P ¨l z# jY P ¨h i# j¡ b bE xc ¦¨¡ ts uY H A5</figDesc><table><row><cell>We know that P the abo Å ve that P</cell><cell cols="2">¨ 4 h G6 rw bl Ä# vY X s v , and it follows from ¨ 4 h q6 . Therefore, we can divide the lower</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">inequality by the upper one: P ¨¥ x 4 h G6 ¨w ¦l v# yY P ¨ 4 h G6 Ww (l v# jY Let us remember that f 4 ¡ £6 is at most c r¨¡ Q ¨s uY c r¨¡ uT s uY -amplifying for Y : E P ¨¥ e 4 h G6 P ¨ 4 h G6 % W© P ¨ 4 h G6 Ww (l v# jY P ¨ 4 h q6 ¨w ¦l v# jY % E % ¨© P ¨ 4 h q6 P ¨ 4 h G6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">It remains to notice that % W© $ % W© P ¨ 4 h G6 Ww l v# jY P ¨ 4 h G6 Ww (l m# {Y and we arrive to contradiction with condition (3). °% © P ¨ 4 h G6 P ¨ 4 h G6</cell><cell>% ¨© G £T £T</cell><cell cols="4">Statement 1. Let a randomized value such that ³ ¡ v sc ¨¡ ®s µY ¨ ² , and let w f be a randomization operator, let Y 9h `g be £T i &amp;  ¶% be two probabilities from Definition 2. Suppose that f is at most -amplifying for Y . Revealing " to-T privacy breach with respect to any property if the following cause neither upward £T -to-privacy breach nor downward -f 4 h q6 # Y " will</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">condition is satisfied:</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>s</cell><cell>T</cell><cell>E</cell><cell>% W© $ T % W© $ s</cell><cell>5</cell><cell>(3)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Proof. Note that otherwise is infinite. Let us denote l ¡ "h ba we have c ¨¡ Fs •Y ¸ } because ´ f 4 h G6 as a random</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">variable. By way of contradiction, let us assume that for property</cell><cell>4 ¡ Q6</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">sense:</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">¡ T h v| 3¡ h qba mw 0 h v| 3¡ h q`a w D¥ e 4 ¡ Q6 and c ¦¨¡ ts @Y # p¹ ¦º e o 3» ¼ c r¨¡ ª s @Y (¡ 4 ¡ Q6 and c r¨¡ ts @Y # ½ ¿¾ À e o » c r¨¡ ª s @Y (Ĩ</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">n words, ¡ T is a private value that has property likely to get randomized into Y , and ¡ Q is another value that does 4 ¡ Q6 and is most not satisfy 4 ¡ Q6 and is least likely to get randomized into Y . By</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">the definition of conditional probability,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>P</cell><cell cols="3">¨ 4 h G6 ¨w (l m# yY # Án e o ¦</cell><cell>P ¨h i# y¡ Gw (l v# jY #</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">§c ¨¡ T Xs uY P ¨l m# yY</cell><cell># n x o ¦ E n e o ¦</cell><cell>P ¨h i# j¡ b sE 8c r¨¡ ts uY P ¨l v# jY P ¨h &amp;# j¡ b # zc ¦¨¡ uT s uY sE</cell><cell>P P ¨l v# yY ¨ 4 h G6</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">and, in the same way,</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">P ¨¥ e</cell><cell cols="2">4 h G6 w ¦l v# yY # Ân À e o ¦</cell><cell>P ¨h i# y¡ Gw (l v# jY #</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2"># Ân À x o ¦</cell></row></table><note><p><p>we have a £T -to-privacy breach.</p>4 ¡ £6 cannot be true for all ¡ ih 9ba because P ¨ 4 h q6 T v% by the definition of privacy breach. Analogously, 4 ¡ Q6 cannot be false for all ¡ h qba because P ¨ 4 h G6 ¤w l "# Y x y s r « . So, the following definitions make P ¨h &amp;# j¡ b sE c ¨¡ ts @Y P ¨l m# {Y c r¨¡ s @Y P ¨l v# jY E tn À e o ¦ P ¨h i# j¡ b # Ãc r¨¡ £ Ws @Y bE P ¨¥ x 4 h G6 P ¨l z# jY</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>±¨</figDesc><table><row><cell cols="8">Statement 2. For any nonconstant function Ò 4 6 increasing on r# y ¨5 U5 )5 Ø , the quantity ñ 0ö</cell></row><row><cell>4 c Q6 Cv #</cell><cell>á n d â É</cell><cell>Ò</cell><cell>4 6 eE 8c ±¨ $#</cell><cell>á n d â É</cell><cell>ï ¨ £E 3Ò</cell><cell>4 6 c ì )í î ¨</cell></row><row><cell cols="8">reaches maximum (conditioned by (5) and by | c ±¨ ¦á d Uâ É being a</cell></row><row><cell>probability</cell><cell cols="3">distribution)</cell><cell cols="2">when,</cell><cell>for</cell><cell>some</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Lowest discoverable support versus breach level T . 5 million transactions, transaction size is 5.</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>1-itemsets</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ù Lowest Discoverable Support in %</cell><cell>0.2 0.4 0.6 0.8</cell><cell>2-itemsets 3-itemsets</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Privacy Level Rho1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Figure 1: 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 ù Lowest Discoverable Support in %</cell><cell>1-itemsets 2-itemsets 3-itemsets</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Transaction Size</cell></row><row><cell cols="5">2. The transaction size;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="13">3. The number of transactions used for support recovery.</cell></row><row><cell cols="6">The amplification parameter</cell><cell></cell><cell cols="6">is computed according to for-</cell></row><row><cell cols="5">mula (3) of Statement 1:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>#</cell><cell>s</cell><cell>T</cell><cell>E</cell><cell>% ¨© w T % ¨© w s</cell><cell>#</cell><cell>A5</cell><cell>T</cell><cell>E</cell><cell>% ¨© G T A5</cell><cell>#</cell><cell>% T</cell><cell>© % 05</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>S UT 4 c T V ec Q )6 is Kullback-Leibler distance between the dis- tributions X c T 4 ¡ Q6 and c Q 4 ¡ Q6 of two random variables:S UT 4 c uT YV uc 6 v # E P ¨h i# j¡ Gw ¦l v# jY t5 It is assumed that the larger R 4 h °l 6 is, the less privacy is preserved. Unfortunately, there are situations where privacy is obviously not preserved, but mutual information does not show any sign of trouble. Here is an example:</figDesc><table><row><cell>o ó `é ba 4 ¡ u 8Y s6 v # P ¨h i# j¡ u xl v# jY t Fc c T 4 ¡ Q6 c 4 ¡ Q6 c a ¬ g uâ W 4 ¡ Q6 v # Example 2. Let our private data be just one bit: ba u# @| ¦ A 3% (~. c ¤a g Assume that both and % are equally likely: P ¨h &amp;# y ( ©# P ¨h i# % ¨# &amp;% ¦ ' . Now consider two randomizations, l T # f T 4 h G6 and l © ¦# f 4 h q6 . The first randomization, given ¡ h ba , outputs ¡ with probability 60% and outputs % © w¡ with probability 40%:</cell></row><row><cell>P ¨l T # y¡ Gw (h i# y¡ s £# y A5 AE</cell></row><row><cell>P ¨l ±T # % ¨© w¡ Gw (h i# y¡ s £# y A5 ¢</cell></row><row><cell>be another ran-f is randomization) such that each randomized data record is an instance of l . Then mutual information R 4 h °l 6 is dom variable ( R 4 h °l 6 v # ¨S UT 4 c a g 2V ±c a c £g X6 x# # E W ó g S UT 4 c a ¬ g ±â W V ±c a 6</cell></row></table><note><p>where</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Let h and l be discrete random variables, and let Ò 4 × 6 be a numerical function such that × sÒ 4 × 6 is convex on × .We define worst-case information with respect to Ò as follows:</figDesc><table><row><cell>Instead of the logarithm, we can use a different numerical func-tion Ò 4 × 6 as long as × bÒ 4 × 6 is a convex function on the interval × C :</cell></row><row><cell>Definition 7.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>The proof of this statement is in Appendix A.3. As claimed in Statement 6, worst-case information allows us to bound upward privacy breaches. But what to do with downward privacy breaches? It turns out that they are bounded by a measure similar to worst-case information, but in a way "inside-out," or inverse worst-case information. Here is the definition:</figDesc><table><row><cell>Now we are going to show that knowing worst-case information</cell></row><row><cell>gives a bound on upward privacy breaches.</cell></row><row><cell>Statement 6. Suppose that revealing causes an upward T -to-s privacy breach with respect to prop-f 4 h G6 # µY for some Y erty 4 h G6 . Then s E 3Ò p £T rq  § 4 % W© w s )6 ±E (Ò ö p % ¨© $ % ¨© $ £T rq R i 4 h °f 4 h G6 6 5</cell></row><row><cell>Definition 8. Let h and l be discrete random variables, and let Ò 4 × 6 be a numerical function such that × sÒ 4 × 6 is convex on × $</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 2 : The values of average-case and worst-case information measures in Example 1.</head><label>2</label><figDesc>4 h °l 6 depends on the behavior of properties likely after l has been revealed,</figDesc><table><row><cell>Measure R 4 h °f 4 h q6 6 1.27 2.32 0.55 f T f f 7 R ui 4 h °f 4 h G6 6 3.90 2.33 0.55 s i 4 h °f 4 h G6 6 1.72 v 0.49</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A. PROOFS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Proof of Statement 2</head><p>Proof. Let us show that, for any | c ±¨ y(á d â É that satisfies (5), we can construct a distribution of type <ref type="bibr" target="#b6">(6)</ref> for which the value of ñ 0ö 4 c Q6 is at least as large. The idea is to raise and lower some c ±¨ 's while keeping the other c ±¨ 's, p # v , in constant relation to each other, and so that ñ Dö 4 c £6 does not decrease in the process.</p><p>Given r# y A 3% )5 35 )5 ) Ø , suppose we increase c ±¨ by the factor of Y and decrease all other c u¨ 's, for # , by a factor of ¡ (thereby 4 w c £6 from decreasing. Note that raising and lowering does not affect relations w ï ¨ T ô w ï ¨ for T # # j . We modify the distribution in two steps. First, we lower c ±¨ ( and raise c ±¨Ø t . We have ï ¨Ø t # E ï ¨ ( : indeed, when we lowered c u¨ ¦ , ï ¨ ¦ became the smallest of all ï ¨ 's, so it will set the limit to raising c ±¨Ø . Then, we repeat the following process: choose c ±¨ that can be lowered or raised, and lower it or raise. Clearly, neither c u¨ ¦ nor c ±¨Ø will ever be chosen since they limit each other and since always Ò 4 D6 ñ 4 c Q6 ® }Ò 4 Ø 6 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Proof of Statement 5</head><p>Proof. Suppose, w.l.o.g., that columns 3 T , 3 9 , 5 )5 )5 , 3 I T are not linearly independent. Then there is a nonzero linear combination that equals a zero vector:</p><p>¡ uT 3 T § ¡ 3 § 5 )5 )5 § ¡ I T 3 I T # 4</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the design and quantification of privacy preserving data mining algorithms</title>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Symposium on Principles of Database Systems</title>
		<meeting>the 20th Symposium on Principles of Database Systems<address><addrLine>Santa Barbara, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-05">May 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast algorithms for mining association rules</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Very Large Databases</title>
		<meeting>the 20th International Conference on Very Large Databases<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-09">September 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Privacy preserving data mining</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD Conference on Management of Data</title>
		<meeting><address><addrLine>Dallas, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-05">May 2000</date>
			<biblScope unit="page" from="439" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Privacy preserving data mining</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGMOD Conference on Management of Data</title>
		<meeting>the 19th ACM SIGMOD Conference on Management of Data<address><addrLine>Dallas, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-05">May 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Security and privacy implications of data mining</title>
		<author>
			<persName><forename type="first">C</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery</title>
		<imprint>
			<date type="published" when="1996-05">May 1996</date>
			<biblScope unit="page" from="15" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">The Economist. The End of Privacy</title>
		<imprint>
			<date type="published" when="1999-05">May 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Data swapping: Balancing privacy against precision in mining for logic rules</title>
		<author>
			<persName><forename type="first">V</forename><surname>Estivill-Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Brankovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Warehousing and Knowledge Discovery DaWaK-99</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Mohania</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Tjoa</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="389" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><surname>Springer-Verlag</surname></persName>
		</author>
		<title level="m">Lecture Notes in Computer Science 1676</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Directive on Privacy Protection</title>
		<imprint>
			<date type="published" when="1998-10">October 1998</date>
			<publisher>European Union</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Privacy preserving mining of association rules</title>
		<author>
			<persName><forename type="first">A</forename><surname>Evfimievski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th ACM SIGKDD International Conference on Knowledge Discovery in Databases and Data Mining</title>
		<meeting>the 8th ACM SIGKDD International Conference on Knowledge Discovery in Databases and Data Mining<address><addrLine>Edmonton, Alberta, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">July 23-26 2002</date>
			<biblScope unit="page" from="217" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Privacy preserving mining of association rules</title>
		<author>
			<persName><forename type="first">A</forename><surname>Evfimievski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 8th ACM SIGKDD Int&apos;l Conference on Knowledge Discovery and Data Mining</title>
		<meeting>of the 8th ACM SIGKDD Int&apos;l Conference on Knowledge Discovery and Data Mining<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-07">July 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Orthogonal Arrays: Theory and Applications</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Hedayat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J A</forename><surname>Sloane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stufken</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999-08">August 1999</date>
			<publisher>Springer Verlag</publisher>
			<biblScope unit="page">440</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Privacy-preserving distributed mining of association rules on horizontally partitioned data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kantarcioglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clifton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery</title>
		<imprint>
			<date type="published" when="2002-06">June 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Privacy preserving data mining</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lindell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pinkas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CRYPTO</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="36" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The Theory of Error-Correcting Codes</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J C</forename><surname>Macwilliams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J A</forename><surname>Sloane</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978">1978</date>
			<biblScope unit="page">762</biblScope>
			<pubPlace>North-Holland, Amsterdam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Ontario. Data Mining: Staking a Claim on Your Privacy</title>
		<imprint>
			<date type="published" when="1998-01">January 1998</date>
			<publisher>Office of the Information and Privacy Commissioner</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Pretzel</surname></persName>
		</author>
		<title level="m">Error-Correcting Codes and Finite Fields</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page">398</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Maintaining data privacy in association rule mining</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Rizvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Haritsa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Very Large Data Bases</title>
		<meeting>the 28th International Conference on Very Large Data Bases<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-08">August 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Privacy-preserving association rule mining</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Rizvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Haritsa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 28th Int&apos;l Conference on Very Large Databases</title>
		<meeting>of the 28th Int&apos;l Conference on Very Large Databases</meeting>
		<imprint>
			<date type="published" when="2002-08">August 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Communication theory of secrecy systems</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell System Technical Journal</title>
		<imprint>
			<biblScope unit="page" from="28" to="32" />
			<date type="published" when="1949">1949</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Data mining and privacy: A conflict in making</title>
		<author>
			<persName><forename type="first">K</forename><surname>Thearling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DS*</title>
		<imprint>
			<date type="published" when="1998-03">March 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Privacy preserving association rule mining in vertically partitioned data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vaidya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Clifton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 8th ACM SIGKDD Int&apos;l Conference on Knowledge Discovery and Data Mining</title>
		<meeting>of the 8th ACM SIGKDD Int&apos;l Conference on Knowledge Discovery and Data Mining<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-07">July 2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
