<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
							<email>andriy@deepmind.com</email>
						</author>
						<author>
							<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
						</author>
						<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BB0CFF57FC9CE5687D4B36E9DCBFE94C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Continuous-valued word embeddings learned by neural language models have recently been shown to capture semantic and syntactic information about words very well, setting performance records on several word similarity tasks. The best results are obtained by learning high-dimensional embeddings from very large quantities of data, which makes scalability of the training method a critical factor. We propose a simple and scalable new approach to learning word embeddings based on training log-bilinear models with noise-contrastive estimation. Our approach is simpler, faster, and produces better results than the current state-of-theart method. We achieve results comparable to the best ones reported, which were obtained on a cluster, using four times less data and more than an order of magnitude less computing time. We also investigate several model types and find that the embeddings learned by the simpler models perform at least as well as those learned by the more complex ones.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Natural language processing and information retrieval systems can often benefit from incorporating accurate word similarity information. Learning word representations from large collections of unstructured text is an effective way of capturing such information. The classic approach to this task is to use the word space model, representing each word with a vector of co-occurrence counts with other words <ref type="bibr" target="#b15">[16]</ref>. Representations of this type suffer from data sparsity problems due to the extreme dimensionality of the word count vectors. To address this, Latent Semantic Analysis performs dimensionality reduction on such vectors, producing lower-dimensional real-valued word embeddings.</p><p>Better real-valued representations, however, are learned by neural language models which are trained to predict the next word in the sentence given the preceding words. Such representations have been used to achieve excellent performance on classic NLP tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b16">17]</ref>. Unfortunately, few neural language models scale well to large datasets and vocabularies due to use of hidden layers and the cost of computing normalized probabilities.</p><p>Recently, a scalable method for learning word embeddings using light-weight tree-structured neural language models was proposed in <ref type="bibr" target="#b9">[10]</ref>. Although tree-structured models can be trained quickly, they are considerably more complex than the traditional (flat) models and their performance is sensitive to the choice of the tree over words <ref type="bibr" target="#b12">[13]</ref>. Inspired by the excellent results of <ref type="bibr" target="#b9">[10]</ref>, we investigate a simpler approach based on noise-contrastive estimation (NCE) <ref type="bibr" target="#b5">[6]</ref>, which enables fast training without the complexity of working with tree-structured models. We compound the speedup obtained by using NCE to eliminate the normalization costs during training, by using very simple variants of the log-bilinear model <ref type="bibr" target="#b13">[14]</ref>, resulting in parameter update complexity linear in the word embedding dimensionality.</p><p>We evaluate our approach on two analogy-based word similarity tasks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b9">10]</ref> and show that despite the considerably shorter training times our models outperform the Skip-gram model from <ref type="bibr" target="#b9">[10]</ref> trained on the same 1.5B-word Wikipedia dataset. Furthermore, we can obtain performance comparable to that of the huge Skip-gram and CBOW models trained on a 125-CPU-core cluster after training for only four days on a single core using four times less training data. Finally, we explore several model architectures and discover that the simplest architectures learn embeddings that are at least as good as those learned by the more complex ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural probabilistic language models</head><p>Neural probabilistic language models (NPLMs) specify the distribution for the target word w, given a sequence of words h, called the context. In statistical language modelling, w is typically the next word in the sentence, while the context h is the sequence of words that precede w. Though some models such as recurrent neural language models <ref type="bibr" target="#b8">[9]</ref> can handle arbitrarily long contexts, in this paper, we will restrict our attention to fixed-length contexts. Since we are interested in learning word representations as opposed to assigning probabilities to sentences, we do not need to restrict our models to predicting the next word, and can, for example, predict w from the words surrounding it as was done in <ref type="bibr" target="#b3">[4]</ref>.</p><p>Given a context h, an NPLM defines the distribution for the word to be predicted using the scoring function s θ (w, h) that quantifies the compatibility between the context and the candidate target word. Here θ are model parameters, which include the word embeddings. The scores are converted to probabilities by exponentiating and normalizing:</p><formula xml:id="formula_0">P h θ (w) = exp(s θ (w, h)) w exp(s θ (w , h)) .<label>(1)</label></formula><p>Unfortunately both evaluating P h θ (w) and computing the corresponding likelihood gradient requires normalizing over the entire vocabulary, which means that maximum likelihood training of such models takes time linear in the vocabulary size, and thus is prohibitively expensive for all but the smallest vocabularies.</p><p>There are two main approaches to scaling up NPLMs to large vocabularies. The first one involves using a tree-structured vocabulary with words at the leaves, resulting in training time logarithmic in the vocabulary size <ref type="bibr" target="#b14">[15]</ref>. Unfortunately, this approach is considerably more involved than ML training and finding well-performing trees is non-trivial <ref type="bibr" target="#b12">[13]</ref>. The alternative is to keep the model but use a different training strategy. Using importance sampling to approximate the likelihood gradient was the first such method to be proposed <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, and though it could produce substantial speedups, it suffered from stability problems. Recently, a method for training unnormalized probabilistic models, called noise-contrastive estimation (NCE) <ref type="bibr" target="#b5">[6]</ref>, has been shown to be a stable and efficient way of training NPLMs <ref type="bibr" target="#b13">[14]</ref>. As it is also considerably simpler than the tree-based prediction approach, we use NCE for training models in this paper. We will describe NCE in detail in Section 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Scalable log-bilinear models</head><p>We are interested in highly scalable models that can be trained on billion-word datasets with vocabularies of hundreds of thousands of words within a few days on a single core, which rules out most traditional neural language models such as those from <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b3">[4]</ref>. We will use the log-bilinear language model (LBL) <ref type="bibr" target="#b11">[12]</ref> as our starting point, which unlike traditional NPLMs, does not have a hidden layer and works by performing linear prediction in the word feature vector space. In particular, we will use a more scalable version of LBL <ref type="bibr" target="#b13">[14]</ref> that uses vectors instead of matrices for its context weights to avoid the high cost of matrix-vector multiplication. This model, like all other models we will describe, has two sets of word representations: one for the target words (i.e. the words being predicted) and one for the context words. We denote the target and the context representations for word w with q w and r w respectively. Given a sequence of context words h = w 1 , .., w n , the model computes the predicted representation for the target word by taking a linear combination of the context word feature vectors:</p><formula xml:id="formula_1">q(h) = n i=1 c i r wi ,<label>(2)</label></formula><p>where c i is the weight vector for the context word in position i and denotes element-wise multiplication. The context can consist of words preceding, following, or surrounding the word being predicted. The scoring function then computes the similarity between the predicted feature vector and one for word w:</p><formula xml:id="formula_2">s θ (w, h) = q(h) q w + b w ,<label>(3)</label></formula><p>where b w is a bias that captures the context-independent frequency of word w. We will refer to this model as vLBL, for vector LBL.</p><p>vLBL can be made even simpler by eliminating the position-dependent weights and computing the predicted feature vector simply by averaging the context word feature vectors: q(h) = 1 n n i=1 r wi . The result is something like a local topic model, which ignores the order of context words, potentially forcing it to capture more semantic information, perhaps at the expense of syntax. The idea of simply averaging context word feature vectors was introduced in <ref type="bibr" target="#b7">[8]</ref>, where it was used to condition on large contexts such as entire documents. The resulting model can be seen as a non-hierarchical version of the CBOW model of <ref type="bibr" target="#b9">[10]</ref>.</p><p>As our primary concern is learning word representations as opposed to creating useful language models, we are free to move away from the paradigm of predicting the target word from its context and, for example, do the reverse. This approach is motivated by the distributional hypothesis, which states that words with similar meanings often occur in the same contexts <ref type="bibr" target="#b6">[7]</ref> and thus suggests looking for word representations that capture their context distributions. The inverse language modelling approach of learning to predict the context from the word is a natural way to do that. Some classic word-space models such as HAL and COALS <ref type="bibr" target="#b15">[16]</ref> follow this approach by representing the context distribution using a bag-of-words but they do not learn embeddings from this information.</p><p>Unfortunately, predicting an n-word context requires modelling the joint distribution of n words, which is considerably harder than modelling the distribution of a single word. We make the task tractable by assuming that the words in different context positions are conditionally independent given the current word w:</p><formula xml:id="formula_3">P w θ (h) = n i=1 P w i,θ (w i ).<label>(4)</label></formula><p>Though this assumption can be easily relaxed without giving up tractability by introducing some Markov structure into the context distribution, we leave investigating this direction as future work.</p><p>The context word distributions P w i,θ (w i ) are simply vLBL models that condition on the current word and are defined by the scoring function</p><formula xml:id="formula_4">s i,θ (w i , w) = (c i r w ) q wi + b wi .<label>(5)</label></formula><p>The resulting model can be seen as a Naive Bayes classifier parameterized in terms of word embeddings. As this model performs inverse language modelling, we will refer to it as ivLBL.</p><p>As with our traditional language model, we also consider the simpler version of this model without position-dependent weights, defined by the scoring function</p><formula xml:id="formula_5">s i,θ (w i , w) = r w q wi + b wi .<label>(6)</label></formula><p>The resulting model is the non-hierarchical counterpart of the Skip-gram model <ref type="bibr" target="#b9">[10]</ref>. Note that unlike the tree-based models, such as those in the above paper, which only learn conditional embeddings for words, in our models each word has both a conditional and a target embedding which can potentially capture complementary information. Tree-based models replace target embeddings with parameters vectors associated with the tree nodes, as opposed to individual words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Noise-contrastive estimation</head><p>We train our models using noise-contrastive estimation, a method for fitting unnormalized models <ref type="bibr" target="#b5">[6]</ref>, adapted to neural language modelling in <ref type="bibr" target="#b13">[14]</ref>. NCE is based on the reduction of density estimation to probabilistic binary classification. The basic idea is to train a logistic regression classifier to discriminate between samples from the data distribution and samples from some "noise" distribution, based on the ratio of probabilities of the sample under the model and the noise distribution. The main advantage of NCE is that it allows us to fit models that are not explicitly normalized making the training time effectively independent of the vocabulary size. Thus, we will be able to drop the normalizing factor from Eq. 1, and simply use exp(s θ (w, h)) in place of P h θ (w) during training. The perplexity of NPLMs trained using this approach has been shown to be on par with those trained with maximum likelihood learning, but at a fraction of the computational cost.</p><p>Suppose we would like to learn the distribution of words for some specific context h, denoted by P h (w). To do that, we create an auxiliary binary classification problem, treating the training data as positive examples and samples from a noise distribution P n (w) as negative examples. We are free to choose any noise distribution that is easy to sample from and compute probabilities under, and that does not assign zero probability to any word. We will use the (global) unigram distribution of the training data as the noise distribution, a choice that is known to work well for training language models. If we assume that noise samples are k times more frequent than data samples, the probability that the given sample came from the data is P h (D = 1|w) = P h d (w) P h d (w)+kPn(w) . Our estimate of this probability is obtained by using our model distribution in place P h d :</p><formula xml:id="formula_6">P h (D = 1|w, θ) = P h θ (w) P h θ (w) + kP n (w) = σ (∆s θ (w, h)) ,<label>(7)</label></formula><p>where σ(x) is the logistic function and ∆s θ (w, h) = s θ (w, h) -log(kP n (w)) is the difference in the scores of word w under the model and the (scaled) noise distribution. The scaling factor k in front of P n (w) accounts for the fact that noise samples are k times more frequent than data samples.</p><p>Note that in the above equation we used s θ (w, h) in place of log P h θ (w), ignoring the normalization term, because we are working with an unnormalized model. We can do this because the NCE objective encourages the model to be approximately normalized and recovers a perfectly normalized model if the model class contains the data distribution <ref type="bibr" target="#b5">[6]</ref>.</p><p>We fit the model by maximizing the log-posterior probability of the correct labels D averaged over the data and noise samples:</p><formula xml:id="formula_7">J h (θ) =E P h d log P h (D = 1|w, θ) + kE Pn log P h (D = 0|w, θ) =E P h d [log σ (∆s θ (w, h))] + kE Pn [log (1 -σ (∆s θ (w, h)))] ,<label>(8)</label></formula><p>In practice, the expectation over the noise distribution is approximated by sampling. Thus, we estimate the contribution of a word / context pair w, h to the gradient of Eq. 8 by generating k noise samples {x i } and computing</p><formula xml:id="formula_8">∂ ∂θ J h,w (θ) = (1 -σ (∆s θ (w, h))) ∂ ∂θ log P h θ (w) - k i=1 σ (∆s θ (x i , h)) ∂ ∂θ log P h θ (x i ) . (<label>9</label></formula><formula xml:id="formula_9">)</formula><p>Note that the gradient in Eq. 9 involves a sum over k noise samples instead of a sum over the entire vocabulary, making the NCE training time linear in the number of noise samples and independent of the vocabulary size. As we increase the number of noise samples k, this estimate approaches the likelihood gradient of the normalized model, allowing us to trade off computation cost against estimation accuracy <ref type="bibr" target="#b5">[6]</ref>.</p><p>NCE shares some similarities with a training method for non-probabilistic neural language models that involves optimizing a margin-based ranking objective <ref type="bibr" target="#b3">[4]</ref>. As that approach is non-probabilistic, it is outside the scope of this paper, though it would be interesting to see whether it can be used to learn competitive word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluating word embeddings</head><p>Using word embeddings learned by neural language models outside of the language modelling context is a relatively recent development. An early example of this is the multi-layer neural network of <ref type="bibr" target="#b3">[4]</ref> trained to perform several NLP tasks which represented words exclusively in terms of learned word embeddings. <ref type="bibr" target="#b17">[18]</ref> provided the first comparison of several word embeddings learned with different methods and showed that incorporating them into established NLP pipelines can boost their performance.</p><p>Recently the focus has shifted towards evaluating such representations more directly, instead of measuring their effect on the performance of larger systems. Microsoft Research (MSR) has released two challenge sets: a set of sentences each with a missing word to be filled in <ref type="bibr" target="#b19">[20]</ref> and a set of analogy questions <ref type="bibr" target="#b10">[11]</ref>, designed to evaluate semantic and syntactic content of word representations respectively. Another dataset, consisting of semantic and syntactic analogy questions has been released by Google <ref type="bibr" target="#b9">[10]</ref>.</p><p>In this paper we will concentrate on the two analogy-based challenge sets, which consist of questions of the form "a is to b is as c is to ", denoted as a : b → c : ? . The task is to identify the held-out fourth word, with only exact word matches deemed correct. Word embeddings learned by neural language models have been shown to perform very well on these datasets when using the following vector-similarity-based protocol for answering the questions. Suppose w is the representation vector for word w normalized to unit norm. Then, following <ref type="bibr" target="#b10">[11]</ref>, we answer a : b → c : ? , by finding the word d * with the representation closest to b -a + c according to cosine similarity:</p><formula xml:id="formula_10">d * = arg max x ( b -a + c) x b -a + c . (<label>10</label></formula><formula xml:id="formula_11">)</formula><p>We discovered that reproducing the results reported in <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b10">[11]</ref> for publicly available word embeddings required excluding b and c from the vocabulary when looking for d * using Eq. 10, though that was not clear from the papers. To see why this is necessary, we can rewrite Eq. 10 as</p><formula xml:id="formula_12">d * = arg max x b x -a x + c x<label>(11)</label></formula><p>and notice that setting x to b or c maximizes the first or third term respectively (since the vectors are normalized), resulting in a high similarity score. This equation suggests the following interpretation of d * : it is simply the word with the representation most similar to b and c and dissimilar to a, which makes it quite natural to exclude b and c themselves from consideration.</p><p>5 Experimental evaluation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We evaluated our word embeddings on two analogy-based word similarity tasks released recently by Google and Microsoft Research that we described in Section 4. We could not train on the data used for learning the embeddings in the original papers as it was not readily available. <ref type="bibr" target="#b9">[10]</ref> used the proprietary Google News corpus consisting of 6 billion words, while the 320-million-word training set used in <ref type="bibr" target="#b10">[11]</ref> is a compilation of several Linguistic Data Consortium corpora, some of which available only to their subscribers.</p><p>Instead, we decided to use two freely-available datasets: the April 2013 dump of English Wikipedia and the collection of about 500 Project Gutenberg texts that form the canonical training data for the MSR Sentence Completion Challenge <ref type="bibr" target="#b18">[19]</ref>. We preprocessed Wikipedia by stripping out the XML formatting, mapping all words to lowercase, and replacing all digits with 7, leaving us with 1.5 billion words. Keeping all words that occurred at least 10 times resulted in a vocabulary of about 872 thousand words. Such a large vocabulary was used to demonstrate the scalability of our method as well as to ensure that the models will have seen almost all the words they will be tested on. When preprocessing the 47M-word Gutenberg dataset, we kept all words that occurred 5 or more times, resulting in an 80-thousand-word vocabulary. Note that many words used for testing the representations are missing from this dataset, which greatly limits the accuracy achievable when using it. To make our results directly comparable to those in other papers, we report accuracy scores computed using Eq. 10, excluding the second and the third word in the question from consideration, as explained in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Details of training</head><p>All models were trained on a single core, using minibatches of size 100 and the initial learning rate of 3 × 10 -2 . No regularization was used. Initially we used a validation-set based learning rate adaptation scheme described in <ref type="bibr" target="#b13">[14]</ref>, which halves the learning rate whenever the validation set perplexity failed to improve after some time, but found that it led to poor representations despite achieving low perplexity scores, which was likely due to undertraining. The linear learning rate schedule described in <ref type="bibr" target="#b9">[10]</ref> produced better results. Unfortunately, using it requires knowing in advance how many passes through the data will be performed, which is not always possible or convenient. Perhaps more seriously, this approach might result in undertraining of representations for rare words because all representation share the same learning rate.</p><p>AdaGrad <ref type="bibr" target="#b4">[5]</ref> provides an automatic way of dealing with this issue. Though AdaGrad has already been used to train neural language models in a distributed setting <ref type="bibr" target="#b9">[10]</ref>, we found that it helped to learn better word representations even using a single CPU core. We reduced the potentially prohibitive memory requirements of AdaGrad, which requires storing a running sum of squared gradient values for each parameter, by using the same learning rate for all dimensions of a word embedding. Thus we store only one extra number per embedding vector, which is helpful when training models with hundreds of millions of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>Inspired by the excellent performance of tree-based models of <ref type="bibr" target="#b9">[10]</ref>, we started by comparing the best-performing model from that paper, the Skip-gram, to its non-hierarchical counterpart, ivLBL without position-dependent weights, proposed in Section 3, trained using NCE. As there is no publicly available Skip-gram implementation, we wrote our own. Our implementation is faithful to the description in the paper, with one exception. To speed up training, instead of predicting all context words around the current word, we predict only one context word, sampled at random using the non-uniform weighting scheme from the paper. Note that our models are also trained using the same context-word sampling approach. To make the comparison fair, we did not use AdaGrad our models in these experiments, using the linear learning rate schedule as in <ref type="bibr" target="#b9">[10]</ref> instead.</p><p>Table <ref type="table" target="#tab_0">1</ref> shows the results on the word similarity tasks for the two models trained on the Wikipedia dataset. We ran NCE training several times with different numbers of noise samples to investigate the effect of this parameter on the representation quality and training time. The models were trained for three epochs, which in our experience provided a reasonable compromise between training time and representation quality. <ref type="foot" target="#foot_0">1</ref> All NCE-trained models outperformed the Skip-gram. Accuracy steadily increased with the number of noise samples used, as did the training time. The best compromise between running time and performance seems to be achieved with 5 or 10 noise samples.</p><p>We then experimented with training models using AdaGrad and found that it significantly improved the quality of embeddings obtained when training with 10 or 25 noise samples, increasing the semantic score for the NCE25 model by over 10 percentage points. Encouraged by this, we trained two ivLBL models with position-independent weights and different embedding dimensionalities for several days using this approach. As some of the best results in <ref type="bibr" target="#b9">[10]</ref> were obtained with the CBOW model, we also trained its non-hierarchical counterpart from Section 3, vLBL with positionindependent weights, using 100/300/600-dimensional embeddings and NCE with 5 noise samples, for shorter training times. Note that due to the unavailability of the Google News dataset used in that paper, we trained on Wikipedia. The scores for ivLBL and vLBL models were obtained using the conditional word and target word representations respectively, while the scores marked with d × 2 were obtained by concatenating the two word representations, after normalizing them.</p><p>The results, reported in Table <ref type="table" target="#tab_1">2</ref>, show that our models substantially outperform their hierarchical counterparts when trained using comparable amounts of time and data. For example, the 300D ivLBL model trained for just over a day, achieves accuracy scores 3-9 percentage points better than the 300D Skip-gram trained on the same amount of data for almost twice as long. The same model trained for four days achieves accuracy scores that are only 2-4 percentage points lower than those of the 1000D Skip-gram trained on four times as much data using 75 times as many CPU cycles. By computing word similarity scores using the conditional and the target word representations concatenated together, we can bring the accuracy gap down to 2 percentage points at no additional computational cost. The accuracy achieved by vLBL models as compared to that of CBOW models follows a similar pattern. Once again our models achieve better accuracy scores faster and we can get within 3 percentage points of the result obtained on a cluster using much less data and far less computation.</p><p>To determine whether we were crippling our models by using position-independent weight, we evaluated all model architectures described in Section 3 on the Gutenberg corpus. The models were trained for 20 epochs using NCE5 and AdaGrad. We report the accuracy obtained with both conditional and target representation (left and right columns respectively) for each of the models in Ta- ble 3. Perhaps surprisingly, the results show that representations learned with position-independent weights, designated with (I), tend to perform better than the ones learned with position-dependent weights. The difference is small for traditional language models (vLBL), but is quite pronounced for the inverse language model (ivLBL). The best-performing representations were learned by traditional language model with the context surrounding the word and position-independent weights.</p><p>Sentence completion: We also applied our approach to the MSR Sentence Completion Challenge <ref type="bibr" target="#b18">[19]</ref>, where the task is to complete each of the 1,040 test sentences by picking the missing word from the list of five candidate words. Using the 47M-word Gutenberg dataset, preprocessed as in <ref type="bibr" target="#b13">[14]</ref>, as the training set, we trained several ivLBL models with NCE5 to predict 5 words preceding and 5 following the current word. To complete a sentence, we compute the probability of the 10 words around the missing word (using Eq. 4) for each of the candidate words and pick the one producing the highest value. The resulting accuracy scores, given in Table <ref type="table" target="#tab_3">4</ref> along with those of several baselines, show that ivLBL models perform very well. Even the model with the lowest embedding dimensionality of 100, achieves 51.0% correct, compared to 48.0% correct reported in <ref type="bibr" target="#b9">[10]</ref> for the Skip-gram model with 640D embeddings. The 55.5% correct achieved by the model with 600D embeddings is also better than the best single-model score on this dataset in the literature (54.7% in <ref type="bibr" target="#b13">[14]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>We have proposed a new highly scalable approach to learning word embeddings which involves training lightweight log-bilinear language models with noise-contrastive estimation. It is simpler than the tree-based language modelling approach of <ref type="bibr" target="#b9">[10]</ref> and produces better-performing embeddings faster. Embeddings learned using a simple single-core implementation of our method achieve accuracy scores comparable to the best reported ones, which were obtained on a large cluster using four times as much data and almost two orders of magnitude as many CPU cycles. The scores we report in this paper are also easy to compare to, because we trained our models only on publicly available data.</p><p>Several promising directions remain to be explored. <ref type="bibr" target="#b7">[8]</ref> have recently proposed a way of learning multiple representations for each word by clustering the contexts the word occurs in and allocating a different representation for each cluster, prior to training the model. As ivLBL predicts the context from the word, it naturally allows using multiple context representations per current word, resulting in a more principled approach to the problem based on mixture modeling. Sharing representations between the context and the target words is also worth investigating as it might result in betterestimated rare word representations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Accuracy in percent on word similarity tasks. The models had 100D word embeddings and were trained to predict 5 words on both sides of the current word on the 1.5B-word Wikipedia dataset. Skip-gram(*) is our implementation of the model from<ref type="bibr" target="#b9">[10]</ref>. ivLBL is the inverse language model without position-dependent weights. NCEk denotes NCE training using k noise samples.</figDesc><table><row><cell></cell><cell></cell><cell>GOOGLE</cell><cell></cell><cell>MSR</cell><cell>TIME</cell></row><row><cell>MODEL</cell><cell cols="3">SEMANTIC SYNTACTIC OVERALL</cell><cell></cell><cell>(HOURS)</cell></row><row><cell>SKIP-GRAM(*)</cell><cell>28.0</cell><cell>36.4</cell><cell>32.6</cell><cell>31.7</cell><cell>12.3</cell></row><row><cell>IVLBL+NCE1</cell><cell>28.4</cell><cell>42.1</cell><cell>35.9</cell><cell>34.9</cell><cell>3.1</cell></row><row><cell>IVLBL+NCE2</cell><cell>30.8</cell><cell>44.1</cell><cell>38.0</cell><cell>36.2</cell><cell>4.0</cell></row><row><cell>IVLBL+NCE3</cell><cell>34.2</cell><cell>43.6</cell><cell>39.4</cell><cell>36.3</cell><cell>5.1</cell></row><row><cell>IVLBL+NCE5</cell><cell>37.2</cell><cell>44.7</cell><cell>41.3</cell><cell>36.7</cell><cell>7.3</cell></row><row><cell>IVLBL+NCE10</cell><cell>38.9</cell><cell>45.0</cell><cell>42.2</cell><cell>36.0</cell><cell>12.2</cell></row><row><cell>IVLBL+NCE25</cell><cell>40.0</cell><cell>46.1</cell><cell>43.3</cell><cell>36.7</cell><cell>26.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Accuracy in percent on word similarity tasks for large models. The Skip-gram † and CBOW † results are from<ref type="bibr" target="#b9">[10]</ref>. ivLBL models predict 5 words before and after the current word. vLBL models predict the current word from the 5 preceding and 5 following words.</figDesc><table><row><cell></cell><cell cols="2">EMBED. TRAINING</cell><cell></cell><cell>GOOGLE</cell><cell></cell><cell>MSR</cell><cell>TIME</cell></row><row><cell>MODEL</cell><cell>DIM.</cell><cell>SET SIZE</cell><cell cols="3">SEM. SYN. OVERALL</cell><cell></cell><cell>(DAYS)</cell></row><row><cell>SKIP-GRAM †</cell><cell>300</cell><cell>1.6B</cell><cell>52.2</cell><cell>55.1</cell><cell>53.8</cell><cell></cell><cell>2.0</cell></row><row><cell>SKIP-GRAM †</cell><cell>300</cell><cell>785M</cell><cell>56.7</cell><cell>52.2</cell><cell>55.5</cell><cell></cell><cell>2.5</cell></row><row><cell>SKIP-GRAM †</cell><cell>1000</cell><cell>6B</cell><cell>66.1</cell><cell>65.1</cell><cell>65.6</cell><cell></cell><cell>2.5×125</cell></row><row><cell>IVLBL+NCE25</cell><cell>300</cell><cell>1.5B</cell><cell>61.2</cell><cell>58.4</cell><cell>59.7</cell><cell>48.8</cell><cell>1.2</cell></row><row><cell>IVLBL+NCE25</cell><cell>300</cell><cell>1.5B</cell><cell>63.6</cell><cell>61.8</cell><cell>62.6</cell><cell>52.4</cell><cell>4.1</cell></row><row><cell>IVLBL+NCE25</cell><cell>300×2</cell><cell>1.5B</cell><cell>65.2</cell><cell>63.0</cell><cell>64.0</cell><cell>54.2</cell><cell>4.1</cell></row><row><cell>IVLBL+NCE25</cell><cell>100</cell><cell>1.5B</cell><cell>52.6</cell><cell>48.5</cell><cell>50.3</cell><cell>39.2</cell><cell>1.2</cell></row><row><cell>IVLBL+NCE25</cell><cell>100</cell><cell>1.5B</cell><cell>55.9</cell><cell>50.1</cell><cell>53.2</cell><cell>42.3</cell><cell>2.9</cell></row><row><cell>IVLBL+NCE25</cell><cell>100×2</cell><cell>1.5B</cell><cell>59.3</cell><cell>54.2</cell><cell>56.5</cell><cell>44.6</cell><cell>2.9</cell></row><row><cell>CBOW †</cell><cell>300</cell><cell>1.6B</cell><cell>16.1</cell><cell>52.6</cell><cell>36.1</cell><cell></cell><cell>0.6</cell></row><row><cell>CBOW †</cell><cell>1000</cell><cell>6B</cell><cell>57.3</cell><cell>68.9</cell><cell>63.7</cell><cell></cell><cell>2×140</cell></row><row><cell>VLBL+NCE5</cell><cell>300</cell><cell>1.5B</cell><cell>40.3</cell><cell>55.4</cell><cell>48.5</cell><cell>48.7</cell><cell>0.3</cell></row><row><cell>VLBL+NCE5</cell><cell>100</cell><cell>1.5B</cell><cell>45.0</cell><cell>56.8</cell><cell>51.5</cell><cell>52.3</cell><cell>2.0</cell></row><row><cell>VLBL+NCE5</cell><cell>300</cell><cell>1.5B</cell><cell>54.2</cell><cell>64.8</cell><cell>60.0</cell><cell>58.1</cell><cell>2.0</cell></row><row><cell>VLBL+NCE5</cell><cell>600</cell><cell>1.5B</cell><cell>57.3</cell><cell>66.0</cell><cell>62.1</cell><cell>59.1</cell><cell>2.0</cell></row><row><cell>VLBL+NCE5</cell><cell>600×2</cell><cell>1.5B</cell><cell>60.5</cell><cell>67.1</cell><cell>64.1</cell><cell>60.8</cell><cell>3.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results for various models trained for 20 epochs on the 47M-word Gutenberg dataset using NCE5 with AdaGrad. (D) and (I) denote models with and without position-dependent weights respectively. For each task, the left (right) column give the accuracy obtained using the conditional (target) word embeddings. nL (nR) denotes n words on the left (right) of the current word.</figDesc><table><row><cell></cell><cell>CONTEXT</cell><cell></cell><cell></cell><cell>GOOGLE</cell><cell></cell><cell></cell><cell>MSR</cell><cell>TIME</cell></row><row><cell>MODEL</cell><cell>SIZE</cell><cell cols="2">SEMANTIC</cell><cell>SYNTACTIC</cell><cell cols="2">OVERALL</cell><cell>(HOURS)</cell></row><row><cell>VLBL(D)</cell><cell>5L + 5R</cell><cell>2.4</cell><cell cols="5">2.6 24.7 23.8 14.6 14.2 23.4 23.1</cell><cell>2.6</cell></row><row><cell>VLBL(D)</cell><cell>10L</cell><cell>1.9</cell><cell cols="3">2.8 22.1 14.8 12.9</cell><cell cols="2">9.3 20.9</cell><cell>9.0</cell><cell>2.6</cell></row><row><cell>VLBL(D)</cell><cell>10R</cell><cell>2.7</cell><cell cols="2">2.4 13.1 24.1</cell><cell cols="2">8.4 14.2</cell><cell>8.8 23.0</cell><cell>2.6</cell></row><row><cell>VLBL(I)</cell><cell>5L + 5R</cell><cell>3.0</cell><cell cols="5">2.9 27.5 29.6 16.4 17.5 22.9 24.2</cell><cell>2.3</cell></row><row><cell>VLBL(I)</cell><cell>10L</cell><cell>2.5</cell><cell cols="5">2.8 23.5 16.1 14.0 10.1 19.8 10.1</cell><cell>2.3</cell></row><row><cell>VLBL(I)</cell><cell>10R</cell><cell>2.3</cell><cell cols="2">2.6 16.2 24.6</cell><cell cols="3">9.9 14.6 10.0 20.3</cell><cell>2.1</cell></row><row><cell>IVLBL(D)</cell><cell>5L + 5R</cell><cell>2.8</cell><cell cols="2">2.3 15.1 13.0</cell><cell>9.5</cell><cell cols="2">8.1 14.5 14.0</cell><cell>1.2</cell></row><row><cell>IVLBL(I)</cell><cell>5L + 5R</cell><cell>2.8</cell><cell cols="5">2.6 26.8 26.8 15.9 15.8 21.4 21.0</cell><cell>1.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Accuracy on the MSR Sentence Completion Challenge dataset.</figDesc><table><row><cell>MODEL</cell><cell>CONTEXT</cell><cell cols="2">LATENT PERCENT</cell></row><row><cell></cell><cell>SIZE</cell><cell>DIM</cell><cell>CORRECT</cell></row><row><cell>LSA [19]</cell><cell>SENTENCE</cell><cell>300</cell><cell>49</cell></row><row><cell>SKIP-GRAM [10]</cell><cell>10L+10R</cell><cell>640</cell><cell>48.0</cell></row><row><cell>LBL [14]</cell><cell>10L</cell><cell>300</cell><cell>54.7</cell></row><row><cell>IVLBL</cell><cell>5L+5R</cell><cell>100</cell><cell>51.0</cell></row><row><cell>IVLBL</cell><cell>5L+5R</cell><cell>300</cell><cell>55.2</cell></row><row><cell>IVLBL</cell><cell>5L+5R</cell><cell>600</cell><cell>55.5</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We checked this by training the Skip-gram model for 10 epochs, which did not result in a substantial increase in accuracy.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Volodymyr Mnih for his helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rejean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Quick training of probabilistic neural nets by importance sampling</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Sébastien</forename><surname>Senécal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>In AISTATS&apos;03</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptive importance sampling to accelerate training of a neural probabilistic language model</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Sébastien</forename><surname>Senécal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="713" to="722" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning</title>
		<meeting>the 25th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">U</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="307" to="361" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Zellig</surname></persName>
		</author>
		<author>
			<persName><surname>Harris</surname></persName>
		</author>
		<title level="m">Distributional structure. Word</title>
		<imprint>
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Černockỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning</title>
		<meeting>the 24th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A fast and simple algorithm for training neural probabilistic language models</title>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning</title>
		<meeting>the 29th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1751" to="1758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical probabilistic neural network language model</title>
		<author>
			<persName><forename type="first">Frederic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIS-TATS&apos;05</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="246" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The Word-Space Model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in high-dimensional vector spaces</title>
		<author>
			<persName><forename type="first">Magnus</forename><surname>Sahlgren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<pubPlace>Stockholm</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semisupervised learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The Microsoft Research Sentence Completion Challenge</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
		<idno>MSR-TR-2011-129</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Microsoft Research</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A challenge set for advancing language modeling</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT</title>
		<meeting>the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="29" to="36" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
