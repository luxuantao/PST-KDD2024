<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SMIL: Multimodal Learning with Severely Missing Modality</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mengmeng</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Delaware</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Ren</surname></persName>
							<email>jren@snap.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Snap Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Long</forename><surname>Zhao</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
							<email>stulyakov@snap.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Snap Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cathy</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Delaware</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xi</forename><surname>Peng</surname></persName>
							<email>xipeng@udel.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Delaware</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SMIL: Multimodal Learning with Severely Missing Modality</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Full modality (paired)</term>
					<term>Test: Full modality (paired)</term>
					<term>(b) Train: Full modality (paired)</term>
					<term>Test: Missing modality</term>
					<term>(c) Train: Full modality (unpaired)</term>
					<term>Test: Full modality (paired)</term>
					<term>(d) Train: Missing modality</term>
					<term>Test: Missing modality</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A common assumption in multimodal learning is the completeness of training data, i.e. full modalities are available in all training examples. Although there exists research endeavor in developing novel methods to tackle the incompleteness of testing data, e.g., modalities are partially missing in testing examples, few of them can handle incomplete training modalities. The problem becomes even more challenging if considering the case of severely missing, e.g., 90% training examples may have incomplete modalities. For the first time in the literature, this paper formally studies multimodal learning with missing modality in terms of flexibility (missing modalities in training, testing, or both) and efficiency (most training data have incomplete modality). Technically, we propose a new method named SMIL that leverages Bayesian meta-learning in uniformly achieving both objectives. To validate our idea, we conduct a series of experiments on three popular benchmarks: MM-IMDb, CMU-MOSI, and avMNIST. The results prove the state-of-the-art performance of SMIL over existing methods and generative baselines including autoencoders and generative adversarial networks. Our code is available at https://github.com/ mengmenm/SMIL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multimodal learning attracts intensive research interest because of broad applications such as intelligent tutoring <ref type="bibr" target="#b29">(Petrovica, Anohina-Naumeca, and Ekenel 2017)</ref>, robotics <ref type="bibr" target="#b26">(Noda et al. 2014)</ref>, and healthcare <ref type="bibr" target="#b5">(Frantzidis et al. 2010)</ref>. Generally speaking, existing research efforts mainly focus on how to fuse multimodal data effectively <ref type="bibr" target="#b22">(Liu et al. 2018;</ref><ref type="bibr" target="#b47">Zadeh et al. 2017a</ref>) and how to learn a good representation for each modality <ref type="bibr" target="#b38">(Tian, Krishnan, and Isola 2020)</ref>.</p><p>A common assumption underlying multimodal learning is the completeness of modality as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. Existing methods <ref type="bibr" target="#b24">(Ngiam et al. 2011;</ref><ref type="bibr" target="#b48">Zadeh et al. 2017b;</ref><ref type="bibr" target="#b10">Hou et al. 2019)</ref> often assume full and paired modalities are available in both training and testing data. However, such an assumption may not always hold in real world due to privacy concerns or budget limitations. For example, in social network, we may not be able to access full-modality data since users would apply various privacy and security constraints. In autonomous driving, we may collect many imaginary data but not as so for 3D point cloud because LiDARs are much less affordable than cameras.</p><p>Although there exist a bunch of research efforts <ref type="bibr" target="#b41">(Tsai et al. 2019;</ref><ref type="bibr" target="#b30">Pham et al. 2019)</ref> in developing novel methods to tackle the incompleteness of testing data, few of them can handle incomplete training modalities. An interesting yet challenging research question then arises: Can we learn a multimodal model from an incomplete dataset while its performance should as close as possible to the one that learns from a full-modality dataset?</p><p>For the first time in the literature, we systematically study this problem by proposing multimodal learning with severely missing modality (SMIL). We consider an even more challenging setting that the missing ratio can be as much as 90%. More specifically, we design two objectives for SMIL: flexibility and efficiency. The former requires our model to uniformly tackle three different missing patterns in training, testing, or both. The latter enforces our model to effectively learn from incomplete modality as fast as possible.</p><p>To jointly achieve both objectives, we leverage Bayesian meta-learning framework in designing a new method. The key idea is to perturb the latent feature space so that embeddings of single modality can approximate ones of full modality. We highlight that our method is better than typical generative designs, such as Autoencoder (AE) <ref type="bibr" target="#b40">(Tran et al. 2017)</ref>, Variational Autoencoder (VAE) <ref type="bibr" target="#b13">(Kingma and Welling 2013)</ref>, or Generative Adversarial Network (GAN) <ref type="bibr" target="#b6">(Goodfellow et al. 2014)</ref>, since they often require a significant amount of full-modality data to learn from, which is usually not available in severely missing modality learning. To summarize, our contribution is three-fold:</p><p>• To the best of our knowledge, we are the first work to systematically study the problem of multimodal learning with severely missing modality.</p><p>• We propose a Bayesian meta-learning based solution to uniformly achieve the goals of flexibility (missing modalities in training, testing, or both) and efficiency (most training data have incomplete modality).</p><p>• Extensive experiments on MM-IMDb, CMU-MOSI, and avMNIST validate the state-of-the-art performance of SMIL over generative baselines including AE and GAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Multimodal learning. Multimodal learning utilizes complementary information contained in multimodal data to improve the performance of various computer vision tasks. One important direction in this area is multimodal fusion, which focuses on effective fusion of multimodal data. Early fusion is a common method which fuses different modalities by feature concatenation, and it has been widely adopted in previous studies <ref type="bibr" target="#b45">(Wang et al. 2017;</ref><ref type="bibr" target="#b31">Poria et al. 2016</ref> Recently, there have been a wide range of research interests in handling missing modalities for multimodal learning, such as testing-time modality missing et al. <ref type="bibr" target="#b41">(Tsai et al. 2019)</ref> and learning with data from unpaired modalities <ref type="bibr" target="#b33">(Shi et al. 2020)</ref>. In this paper, we address a more challenging and novel multimodal-learning setting where both training and testing data contain samples that have missing modalities. Generative approaches, such as auto-encoders <ref type="bibr" target="#b40">(Tran et al. 2017;</ref><ref type="bibr" target="#b19">Lee et al. 2019)</ref>, GANs <ref type="bibr" target="#b6">(Goodfellow et al. 2014)</ref>, and VAEs <ref type="bibr" target="#b13">(Kingma and Welling 2013)</ref>, offer a straightforward solution to handle this setting, but these methods are neithor flexible nor efficient as SMIL.</p><p>Meta-regularization. Meta-learning algorithms focus on designing models that are able to learn new knowledge and adapt to novel environments quickly with only a few training samples. Previous methods studied meta-learning from the perspective of metric learning <ref type="bibr" target="#b14">(Koch 2015;</ref><ref type="bibr" target="#b44">Vinyals et al. 2016;</ref><ref type="bibr" target="#b36">Sung et al. 2018;</ref><ref type="bibr" target="#b34">Snell, Swersky, and Zemel 2017)</ref> or probabilistic modeling <ref type="bibr" target="#b2">(Fe-Fei et al. 2003;</ref><ref type="bibr" target="#b15">Lawrence and Platt 2004)</ref>. Recent advances in optimization-based approaches have evoked more interests in meta-learning. MAML <ref type="bibr" target="#b3">(Finn, Abbeel, and Levine 2017</ref>) is a general optimization algorithm designed for few-shot learning and reinforcement learning. It is compatible with models that learn through gradient descent. <ref type="bibr" target="#b25">Nichol et al. (Nichol, Achiam, and Schulman 2018)</ref> further improved the computation efficiency of MAML. Other works adapted MAML for domain generalization <ref type="bibr" target="#b20">(Li et al. 2018;</ref><ref type="bibr" target="#b32">Qiao, Zhao, and Peng 2020)</ref> and knowledge distillation <ref type="bibr" target="#b49">(Zhao et al. 2020)</ref>. In this work, we extend MAML by learning two auxiliary networks for missing modality reconstruction and feature regularization.</p><p>Conventional handcrafted regularization techniques <ref type="bibr" target="#b10">(Hoerl and Kennard 1970;</ref><ref type="bibr" target="#b39">Tibshirani 1996)</ref> regularize model parameters to avoid overfitting and increase interpretability. Balaji et al. (Balaji, Sankaranarayanan, and Chellappa 2018) modeled the regularization function as an additional network learned through meta-learning to regularize model parameters. <ref type="bibr" target="#b21">Li et al. (Li et al. 2019)</ref> followed the same idea of (Balaji, Sankaranarayanan, and Chellappa 2018) but learned an additional network to regularize latent features. <ref type="bibr" target="#b18">Lee et al. (Lee et al. 2020b</ref>) proposed a more general algorithm for latent feature regularization. Other than perturbing features, we propose to learn the regularization function following <ref type="bibr" target="#b18">(Lee et al. 2020b</ref>) but regularize the feature to reduce discrepancy between the reconstructed and true modality.</p><p>Multimodal generative models. Generative models for multimodal learning fall into two categories: cross-modal generation and joint-model generation. Cross-modal generation methods, such as conditional VAE (CVAE) <ref type="bibr" target="#b35">(Sohn, Lee, and Yan 2015)</ref> and conditional multimodal autoencoder <ref type="bibr" target="#b27">(Pandey and Dukkipati 2017)</ref>, learn a conditional generative model over all modalities. On the other hand, joint-model generation approaches learn the joint distribution of multimodal data. Multimodal variational autoencoder (MVAE) <ref type="bibr" target="#b46">(Wu and Goodman 2018</ref>) models the joint posterior as a product-of-expert (PoE). Multimodal VAE (JM-VAE) <ref type="bibr" target="#b37">(Suzuki, Nakayama, and Matsuo 2016</ref>) learns a shared representation with a joint encoder. With only a few modifications to the original algorithms, we show that multimodal generative models serve as strong baselines for learning with severely missing modalities proposed in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>We are interested in multimodal learning with severely missing modality, e.g., 90% of the training samples contain incomplete modalities. In this paper, without loss of generality, we consider a multimodal dataset containing two modalities. Formally, we let D = {D f , D m } denote a multimodal dataset; D f = {x 1 i , x 2 i , y i } i is a modality-complete dataset, where x 1 i and x 2 i represent two different modalities of i-th sample and y i is the corresponding class label; D m = {x 1 j , y j } j is a modality-incomplete dataset, where one modality is missing. Our target is to leverage both modality-complete and modality-incomplete data for model training. We propose to address this problem from two perspectives: 1) Flexibility: how to handle missing modality at different stages, i.e., training, testing, or both, in a unified framework? 2) Efficiency: how to improve training efficiency when major data suffers from missing modality? Flexibility. We aim to achieve a unified model that can handle missing modality in training, testing, or both. Our idea is to employ a feature reconstruction network to achieve this goal. Instead of the conventional data reconstruction approaches <ref type="bibr" target="#b19">(Lee et al. 2019;</ref><ref type="bibr" target="#b40">Tran et al. 2017)</ref>, the feature reconstruction network will leverage the available modality to generate an approximation of the missing-modality feature in a highly efficient way. This will generate complete data in the latent feature space and facilitate the flexibility in two aspects. On the one hand, our model can excavate the full potential of hybrid data by using both modality-complete and -incomplete data for joint training. On the other hand, by turning on or off the feature reconstruction network, our model can tackle modality-incomplete or -complete inputs in a unified manner.</p><p>Efficiency. The severely missing-modality setting poses significant learning challenges to the feature reconstruction network. The model would be highly bias-prone due to the scarcity of modality-complete data, yielding degraded and low-quality feature generations. We propose a feature regularization approach to address this issue. The idea is to leverage a Bayesian neural network to assess the data uncertainty by performing feature perturbations. The uncertainty assessment is used as feature regularization to overcome model and data bias. Comparing with the previous deterministic regularization approaches <ref type="bibr">(Balaji, Sankaranarayanan, and Chellappa 2018;</ref><ref type="bibr" target="#b49">Zhao et al. 2020)</ref>, the proposed uncertaintyguided feature regularization will significantly improve the capacity of the multimodal model for robust generalization behaviors in tackling severely incomplete data.</p><p>A meta-learning framework. In the proposed algorithm, there are three networks to be optimized: the main network f θ parameterized by θ, the reconstruction network f φc parameterized by φ c , and the regularization network f φr parameterized by φ r . Instead of training the networks separately, we effectively integrate all networks in a modified Model-Agnostic Meta-Learning (MAML) <ref type="bibr" target="#b3">(Finn, Abbeel, and Levine 2017)</ref> framework. An overview of our learning framework is shown in Figure <ref type="figure" target="#fig_1">2</ref>. In the following sections, we will describe the implementation of each component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Missing Modality Reconstruction</head><p>We introduce the feature reconstruction network to approximate the missing modality. For a modality-incomplete sample, the missing modality is reconstructed conditioned on the available modality. Given the observed modality x 1 , in order to obtain the reconstruction x2 of the missing modality, we optimize the following objective for the reconstruction network:</p><formula xml:id="formula_0">φ * c = arg min φc E p(x 1 , x 2 ) (−log p(x 2 |x 1 ; φ c )).<label>(1)</label></formula><p>However, under severely missing modality, it is non-trivial to train a reconstruction network from limited modalitycomplete samples. Inspired by <ref type="bibr" target="#b14">(Kuo et al. 2019)</ref>, we approximate the missing modality using a weighted sum of modality priors learned from the modality-complete dataset.</p><p>In this case, the reconstruction network are trained to predict weights of the priors instead of directly generating the missing modality. We achieve this by learning a set of modality priors M which can be clustered among all modalitycomplete samples using K-means (MacQueen 1967) or PCA <ref type="bibr" target="#b28">(Pearson 1901)</ref>. Specifically, instead of generating ω = f φc (x 1 ) directly, we model ω as a multivariate Gaussian with fixed means and changeable variances as N (I, σ). The variances are predicted by σ = f φc (x 1 ). Given the weights ω, we can reconstruct the missing modality x2 by calculating the weighted sum of the modality priors. Then, the reconstructed missing modality can be achieved by: x2 = ω, M , where ω ∼ N (I, σ).</p><p>(2)</p><p>We note that modeling ω as multivariate random variables introduces randomness and uncertainty to the reconstruction process, which has been proved to be beneficial in learning sophisticated distributions <ref type="bibr" target="#b18">(Lee et al. 2020b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Uncertainty-Guided Feature Regularization</head><p>We propose to regularize the latent features by a feature regularization network. In each layer, the regularization network takes the features of the previous layer as input and applies regularization to the features of the current layer. Let r denote the generated regularization and h l be the latent feature of the l-th layer. Instead of generating a deterministic regularization r = f φr (h l−1 ), we assume that r follows a multivariate Gaussian distribution N (µ, σ), where the means and variances are calculated using (µ, σ) = f φr (h l−1 ). Then, we can compute the regularized feature by the following equation:</p><formula xml:id="formula_1">h l := h l • Softplus(r), where r ∼ N (µ, σ),<label>(3)</label></formula><p>where • is a predefined operation (either addition or multiplication) for feature regularization. In our experiments, we observe that directly applying regularization to latent features will prevent the feature regularization network from convergence. Hence, we adopt Softplus <ref type="bibr" target="#b1">(Dugas et al. 2000)</ref> activation to weaken the regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">A Bayesian Meta-Learning Framework</head><p>We leverage a Bayesian Meta-Learning framework to jointly optimizing all the networks. Specifically, we meta-train the main network f θ on D m with the help of reconstruction f φc network and regularization f φr network. Then, we meta-test the updated main network f θ * on D f . Finally, we metaupdate network parameters {θ, φ c , φ r } by gradient descent.</p><p>For simplicity, we let ψ = {φ c , φ r } denote the combination of the parameters of the reconstruction and regularization network. Our framework aims to optimize the following objective function:</p><formula xml:id="formula_2">min θ,ψ L(D f ; θ * , ψ), where θ * = θ − α∇ θ L(D m ; ψ).<label>(4)</label></formula><p>For the above function, L denotes the empirical loss such as cross entropy, and α is the inner-loop step size.</p><p>We use X and Y to represent all training samples and their corresponding labels, respectively. Let z = {ω, r} be the collection of the generated weights and regularization. Then, inspired by <ref type="bibr" target="#b4">(Finn, Xu, and Levine 2018;</ref><ref type="bibr">Gordon et al. 2019;</ref><ref type="bibr" target="#b17">Lee et al. 2020a)</ref>, we define the generative process as optimizing the likelihood in a meta-learning framework:</p><formula xml:id="formula_3">p(Y, z|X; θ) = p(z) N i=1 p(y i |x 1 i , x 2 i , z; θ) M j=1</formula><p>p(y j |x 1 j , z; θ).</p><p>(5) The goal of Bayesian Meta-Learning is to maximize the conditional likelihood: log p(Y|X; θ). However, solving it involves the true posterior p(z|X), which is intractable. Instead, we approximate the true posterior distribution by an amortized distribution q(z|X; ψ) <ref type="bibr" target="#b4">(Finn, Xu, and Levine 2018;</ref><ref type="bibr">Gordon et al. 2019;</ref><ref type="bibr" target="#b17">Lee et al. 2020a</ref>). The resulting form of approximated lower bound for our meta-learning framework can be defined as:</p><formula xml:id="formula_4">L θ,ψ = E q(z|X;θ,ψ) [log p(Y|X, z; θ)]− KL[q(z|X; ψ) p(z|X)]. (6)</formula><p>We maximize this lower bound by Monte-Carlo (MC) sampling. After combining all these together, we obtain the full Algorithm 1: Bayesian Meta-Learning Framework.</p><p>Input: Multimodal dataset D = {D f , D m }; # of iterations K; inner learning rate α; outer learning rate β.</p><formula xml:id="formula_5">1 while not converged do 2 Sample {x 1 j , y j } ∼ D m ; {x 1 i , x 2 i , y i } ∼ D f 3 θ 0 ← θ 4 Meta-train: 5 for k = 0 to K − 1 do 6 Sample zj ∼ p(z j |x 1 j ; ψ, θ k ) 7 θ k+1 ← θ k − α∇ θ k [−log p(y j |x 1 j , zj ; θ k )] 8 end 9 θ * ← θ K 10 Meta-update: 11 θ ← θ − β∇ θ [−log p(y i |x 1 i , x 2 i , zi ; θ * )] 12 ψ ← ψ − β∇ ψ [−log p(y i |x 1 i , x 2 i , zi ; θ * )] 13 end</formula><p>training objective of the proposed meta-learning framework for θ and ψ which is defined as:</p><formula xml:id="formula_6">min θ,ψ 1 L L l=1 −log p(y j |x 1 j , x 2 j , z l ; θ) + KL[q(z|X; ψ) p(z|X)]</formula><p>with z l ∼ q(z|X; ψ), (7) where L is the number of MC sampling. We show our detailed algorithm in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we analyze the results of the proposed algorithm for multimodal learning with severely missing modality on three datasets from two perspectives: efficiency under severely missing modality (Section 4.2) and flexibility to various modality missing pattern (Section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setting</head><p>Datasets. Totally three datasets are used in the experiment: • The Multimodal IMDb (MM-IMDb) <ref type="bibr" target="#b0">(Arevalo et al. 2017)</ref> contains two modalities: the image modality includes movie posters, and the text modality includes the corresponding plot of a movie poster. We conduct experiments on this dataset to predict a movie genre using image or text modality, which is a multi-label classification task as multiple genres could be assigned to a single movie. The dataset includes 25, 956 movies and 23 classes. We follow the training and validation splits provided in the previous work <ref type="bibr" target="#b43">(Vielzeuf et al. 2018)</ref>  <ref type="bibr" target="#b42">(Tzanetakis and Cook 2002)</ref> as the representation of audio modality. Each raw audio is processed by MFCCs to get a sample with a size of 20 × 20 × 1. The dataset contains 1, 500 samples for both image and audio modalities. We randomly select 70% data for training and use the rest for validation. Evaluation metrics. For MM-IMDb dataset, we follow previous works <ref type="bibr" target="#b0">(Arevalo et al. 2017;</ref><ref type="bibr" target="#b43">Vielzeuf et al. 2018</ref>) by adopting the F1 Samples and F1 Micro to evaluate multilabel classification. For CMU-MOSI, we follow <ref type="bibr" target="#b22">Liu et al. (2018)</ref> to compute the binary classification accuracy and F1 Score. For avMNIST dataset, we compute accuracy to measure the performance.</p><p>Baseline methods. We compare the proposed approach with the following baseline methods:</p><p>• Lower-Bound is a model trained using single modality of the data, i.e., 100% image, 100% text, etc. It serves as the lower bound for our method.</p><p>• Upper-Bound is a model trained leveraging all modalities of the data, i.e., 100% images and 100% text, etc. We regard it as the upper bound.</p><p>• AE (Autoencoder) <ref type="bibr" target="#b19">(Lee et al. 2019</ref>) is a deep model used for efficient data encoding. We can use AE to preprocess the original dataset to tackle the severely missing modality problem. We now describe the procedure for preprocessing. First, we sample a dataset containing only modality-complete samples from the original dataset. train AE to reconstruct the missing modality. Finally, we impute the missing modality of modality-incomplete data using the trained AE. After finishing the imputation, the dataset is now available for multimodal learning. • GAN (Generative adversarial network) is a deep generative model composed of a generator and a discriminator. We leverage GAN to tackle our problem following the same procedure as described in AE. • MVAE (Wu and Goodman 2018) is proposed for multimodal generative task. We adopt the widely used linear evaluation protocol to adapt MVAE for classification. Specifically, we first train MVAE using all the modalities. We then keep the learned MVAE frozen to train a randomly initialized linear classifier using the latent representation generated by the encoder of MVAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Efficiency with Severely Missing Modality</head><p>Conclusion: Our method demonstrates consistent efficiency, across different datasets, when training data contains a different ratios of modality missing.</p><p>Setting of missing modality. We evaluate the efficiency of our algorithm on two datasets: MM-IMDb and CMU-MOSI. In both datasets, modalities are incomplete for some samples. We define the text modality ratio as η = M N , where M is the number of samples with text modality and N is the size of overall samples. η indicates the severity of modality missing. The smaller of η, the severer the modality is missing. For both datasets, we assume image modality to be complete, and the text modality to be incomplete. We express all available data points in the form of 100% Image + η% Text for both datasets.</p><p>Implementation details. CMU-MOSI. We follow <ref type="bibr" target="#b22">Liu et al. (2018)</ref> to get features for the image and text modality. We use three fully-connected (FC) layers with dimension 16 to get the embedding of image modality. One layer LSTM <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber 1997)</ref> extracts the embedding for text modality. The concatenated feature of two modalities is then fed to FC layers for classification. For training process, we use Adam (Kingma and Ba 2014) optimizer with a batch size of 32 and train the networks for 5, 000 iterations with a learning rate of 10 −4 for both innerloop and outer-loop of meta-learning. MM-IMDB. For image and text modalities, we adopt the feature extraction mod-Figure <ref type="figure">3</ref>: F1 Samples score of each movie genre on the MMdataset for the lower-bound baseline (blue) and SMIL (red). The number of image samples for each movie genre is indicated by the green line. els from <ref type="bibr" target="#b0">Arevalo et al. (2017)</ref>. We feed the feature from each modality to a FC layer to align their output dimension. On top of it, we fuse the feature together and send it to FC layers to conduct multi-label classification. We apply Adam optimizer with a batch size of 128. We train the models for 10, 000 iteration with a learning rate of 10 −4 for inner-loop and 10 −3 fro outer-loop. Besides, we follow previous work <ref type="bibr" target="#b43">(Vielzeuf et al. 2018)</ref> to add a weight of 2.0 on the positive label to balance the precision and recall since the labels are unbalanced.</p><p>Different ratios of modality missing. The results on CMU-MOSI are shown in Table <ref type="table">1</ref>. As can be seen, our approach significantly outperforms all baselines among all ratios of modality missing, which showcases the efficiency of our approach in the missing modality problem. The results also show that the severer the missing modality is, the more efficient our approach is. More specifically, when η is 20%, our approach outperforms AE and GAN around 5.0%, while the improvements increase to 7.6% and 7.4%, respectively, when η decreases to 10%. Moreover, our improvements are also consistent on MM-IMDb, as shown in Table <ref type="table">2</ref>. The improvement increases as the modality ratio decreasing. From Table <ref type="table">2</ref>, we see that our approach performs better than all baseline method under different text ratio. Our method outperforms Lower-Bound and MVAE by a large margin, and quite close to Upper-Bound.</p><p>We further show the effect of multimodal learning for different classes of MM-IMDb when η = 20% in Figure <ref type="figure">3</ref>. We make two interesting observations. First, our method (shown as red bars) can largely improve the model performance even on the tailed genres, such as Sport and Film-Noir, while the model trained only using images (shown as blue bars) can hardly predict the classes with less training samples. Second, an interesting phenomenon in Figure <ref type="figure">3</ref> is that text modality will slightly decrease the performance of movie genres like Family and Animation. The possible reason is that there is a large overlap between genres of family and animations. As a result, text modality may enforce the model to learn the shared knowledge between these two genres, which reduces the discrepancy and decrease the accuracy.</p><p>Visualization of embedding space. We visualize the embedding space of three genres in MM-IMDb in Figure <ref type="figure" target="#fig_4">5</ref>. We observe two interesting findings. First, the visualization results of lower bound and upper bound show our approach can effectively disentangle the latent embedding of the three genres, while the model learned only from image modality cannot. Besides, Our method is efficient when modality is severely missing. Form Figure <ref type="figure" target="#fig_4">5</ref>, we see that our model trained using only 10% text modality is comparable to a model trained using 100% text modality.</p><p>Justification of symbol '-' used in Table <ref type="table">1</ref>, 2. We use the '-' symbol for two reasons. First, not applicable. Lower-Bound only requires image modality for training, so it is not applicable to report a Lower-Bound result trained using both image and text. Second, not necessary. For example, in table 1, MVAE trained without missing modality (100% image + 100% text) achieves acc=58.5%. In comparison, our model trained with severely missing modality (100% im-age+10% text) achieves acc=60.7%. So it is not necessary to train MVAE under severely missing modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Flexibility with Different Missing Patterns</head><p>Conclusion: Our method shows flexibility in handling various missing patterns: (1) full or missing modality at training; and (2) full or missing modality at test time.</p><p>Implementation details. Our network contains two modality-specific feature extractors and a few FC layers. We use LeNet-5 to extract features for image modality, and a modified LeNet-5 to extract audio features. Extracted features are then fused through concatenation and sent into FC layers to perform classification. For the training process, we use Adam optimizer with a batch size of 64 and train the networks for 15, 000 iterations with a learning rate of 10 −3 for both inner-and outer-loop of meta-learning.</p><p>Setting of missing pattern. For the avMNIST dataset, the missing modality problem only happens to audio modality. Table <ref type="table">3</ref>: Ablation study on the effect of modality reconstruction, feature regularization, and Bayesian inference on MM-IMDb under two text modality ratios (10% and 20%).</p><p>We are interested in two different missing patterns: (1) training with 100% Image + η% Audio and testing with Image Only;</p><p>(2) training with 100% Image + η% Audio and testing with Image + Audio. In this section, we show that our approach can flexibly handle these two missing patterns.</p><p>Missing pattern 1: testing with image only. Figure 4 (left) shows the mean and standard deviation of classification accuracy under different audio ratio. We have two interesting observations. First, We see that our approach can successfully handle testing with image modality only, but baseline methods such as AE and GAN fail in this scenario. We argue that the failure of baseline methods is mainly due to the bias of the reconstructed missing modality. In single modality testing, the method is required to generate the missing modality conditioned on the available modality. The baseline method does not consider the bias of the reconstructed missing modality. In contrast, our method can leverage learned meta-knowledge to to generate an unbiased missing modality. Besides, in situations where audio modality is missing severely (i.e., η = 5%), The classification accuracy of our method is 1.10% higher than the lower bound. The improvement demonstrates clear advantages of our model under severely missing modality.</p><p>Missing pattern 2: testing with image and audio. <ref type="bibr">Figure 4 (right)</ref> shows the result of our approach dealing with full modality testing. Two observations can be found. Our method outperforms the Lower-Bound, AE, and GAN by a large margin and closes to the performance of Upper-Bound. Moreover, it is worth noticing that missing pattern 2 is identical to the missing pattern used in Section 4.2. Therefore, we verify the efficiency of our method on three datasets. And it performs consistently through all the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>We conduct the ablation analysis on the MM-IMDb dataset to evaluate the effectiveness of the missing modality reconstruction, feature regularization, and Bayesian Inference. We show the results in Table <ref type="table">3</ref>.</p><p>Effectiveness of missing modality reconstruction. In Section 3.1, we use reconstruction network to generate weights for missing modality reconstruction. Here we denote the method that uses the reconstruction network to directly generate the feature of missing modality as SMIL w/o which has worse performance and proves the necessity of K-Means for reconstruction.</p><p>Effectiveness of feature regularization. In Section 3.2, we introduce feature regularization. Here we denote the method without feature regularization as SMIL w/o Regularization. The performance of SMIL w/o Regularization is inferior to SMIL (Full), which verifies conducting multimodal learning on D without regularization leads to a sub-optimal model. The superior performance of the regularized model is essential to the explicit objective of reducing discrepancy.</p><p>Effectiveness of Bayesian inference. In Section 3.3, we introduce the Bayesian Meta-Learning Framework. In this section, we compare it with two variants. SMIL w/ Fixed Gaussian: We fix the distribution of feature regularization to a Gaussian distribution, which is N (0, I); SMIL w/ Deterministic: The missing modality construction and feature regularization is deterministic so the sampling in Eqn. 7 is removed. These two variants are inferior to Bayesian inference.The results demonstrate the superiority of Bayesian Meta-Learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we address a challenging and novel problem in multimodal learning: multimodal learning with severely missing modality. We further propose a novel learning strategy based on the meta-learning framework. This framework tackles two important perspectives: missing modality reconstruction (flexibility) and feature regularization (efficiency). We apply the Bayesian meta-learning framework to infer the posterior of them and propose a variational inference framework to estimate the posterior.</p><p>In the experiments, we show that our model outperforms the generative method significantly on three multimodal datasets. Further analysis on the results shows that involving modality reconstruction and feature regularization can effectively handle the missing modality problem and flexible to various missing patterns. We believe that our work makes a meaningful step towards the real-world application of multimodal learning where partial modalities are missing or hard to collect.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Multimodal learning configurations. (a) Train and test with full and paired modality (Ngiam et al. 2011); (b) Testing with missing modality (Tsai et al. 2019); (c) Training with unpaired modality (Shi et al. 2020); (d) We study the most challenging configurations of severely missing modality in training, testing, or both.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: SMIL can uniformly learn from severely missing modality and test with either single or full modality. The reconstruction network φ c outputs a posterior distribution, from which we sample weight ω to reconstruct the missing modality using modality priors. The regularization network φ r also outputs a posterior distribution, from which we sample regularizer r to perturb latent features for smooth embedding. The collaboration (φ c and φ r ) guarantees flexible and efficient learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Classification accuracy (%) on avMNIST with two missing patterns. Left: training with 100% Image + η% Audio and testing with Image Only. Right: training with 100% Image η% Audio and testing with Image + Audio.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5</head><label>5</label><figDesc>Figure 5: t-SNE visualization for embeddings of the lowerbound baseline (a), SMIL (b), and upper-bound baseline (c) on the MM-IMDb dataset. Three movie genres, including Sport, Film-Noir, and Western are visualized.</figDesc><graphic url="image-76.png" coords="7,401.65,71.87,69.75,55.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Then, we assume one modality is missing and</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="2">F1 Samples ↑</cell><cell></cell><cell cols="2">F1 Micro ↑</cell></row><row><cell></cell><cell cols="6">10% 20% 100% 10% 20% 100%</cell></row><row><cell>Lower-Bound</cell><cell>-</cell><cell>-</cell><cell>47.6</cell><cell>-</cell><cell>-</cell><cell>48.2</cell></row><row><cell>Upper-Bound</cell><cell>-</cell><cell>-</cell><cell>61.7</cell><cell>-</cell><cell>-</cell><cell>62.0</cell></row><row><cell>MVAE</cell><cell>-</cell><cell>-</cell><cell>48.4</cell><cell>-</cell><cell>-</cell><cell>48.6</cell></row><row><cell>AE</cell><cell cols="2">44.5 50.9</cell><cell>-</cell><cell cols="2">44.8 50.7</cell><cell>-</cell></row><row><cell>GAN</cell><cell cols="2">45.0 51.1</cell><cell>-</cell><cell cols="2">44.6 51.0</cell><cell>-</cell></row><row><cell>SMIL</cell><cell cols="2">49.2 54.1</cell><cell>-</cell><cell cols="2">49.5 54.6</cell><cell>-</cell></row><row><cell cols="7">Table 2: Multi-label classification scores (F1 Samples and</cell></row><row><cell cols="7">F1 Micro) for different methods under three text modality</cell></row><row><cell cols="7">ratios (10%, 20%, and 100%) on the MM-IMDb dataset.</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgements</head><p>This work is partially supported by the Data Science Institute (DSI) at University of Delaware and Snap Research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Metareg: Towards domain generalization using meta-regularization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Arevalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Solorio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Montes-Y Gómez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International conference on learning representations 2017 workshop</title>
				<meeting><address><addrLine>Balaji, Y</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017. 2018</date>
			<biblScope unit="page" from="998" to="1008" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Incorporating second-order functional knowledge for better option pricing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dugas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bélisle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="472" to="478" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Bayesian approach to unsupervised oneshot learning of object categories</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fe-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Ninth IEEE International Conference on Computer Vision</title>
				<meeting>Ninth IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1134" to="1141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Model-agnostic metalearning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
				<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Probabilistic model-agnostic meta-learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9516" to="9527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the classification of emotional biosignals evoked while viewing affective pictures: an integrated data-mining-based approach for healthcare applications</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Frantzidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bratsas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Klados</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Konstantinidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Lithari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Vivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Papadelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kaldoudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Bamidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Technology in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="309" to="318" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Turner</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Meta-Learning Probabilistic Inference for Prediction</title>
		<ptr target="https://openreview.net/forum?id=HkxStoC5F7" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep Multimodal Multilinear Fusion with High-order Polynomial Pooling</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Hoerl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Kennard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="1970">1970. 2019</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="12113" to="12122" />
		</imprint>
	</monogr>
	<note>Ridge regression: Biased estimation for nonorthogonal problems</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Z</forename><surname>Jackson</surname></persName>
		</author>
		<ptr target="https://github.com/Jakobovski/decoupled-multimodal-learning." />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Free Spoken Digit Dataset</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Shapemask: Learning to segment novel objects by refining shape priors</title>
		<author>
			<persName><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning Deep Learning workshop</title>
				<imprint>
			<date type="published" when="2015">2015. 2019</date>
			<biblScope unit="page" from="9207" to="9216" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE International Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to learn with the informative vector machine</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-first international conference on Machine learning</title>
				<meeting>the twenty-first international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">65</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to Balance: Bayesian Meta-Learning for Imbalanced and Out-of-distribution Tasks</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rkeZIJBYvr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Meta Dropout: Learning to Perturb Latent Features for Generalization</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJgd81SYwr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Audio Feature Generation for Missing Modality Problem in Video Action Recognition</title>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3956" to="3960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to generalize: Meta-learning for domain generalization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Featurecritic networks for heterogeneous domain generalization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11448</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Efficient low-rank multimodal fusion with modality-specific factors</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">B</forename><surname>Lakshminarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00064</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Macqueen</surname></persName>
		</author>
		<ptr target="https://projecteuclid.org/euclid.bsmsp/1200512992" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability</title>
				<meeting>the Fifth Berkeley Symposium on Mathematical Statistics and Probability<address><addrLine>Berkeley, Calif</addrLine></address></meeting>
		<imprint>
			<publisher>University of California Press</publisher>
			<date type="published" when="1967">1967</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
				<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">On first-order metalearning algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multimodal integration learning of robot behavior using deep neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Noda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Arie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Suga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ogata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="721" to="736" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Variational methods for conditional multimodal deep learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dukkipati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Joint Conference on Neural Networks (IJCNN)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="308" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">LIII. On lines and planes of closest fit to systems of points in space</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="559" to="572" />
			<date type="published" when="1901">1901</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Emotion recognition in affective tutoring systems: Collection of ground-truth data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Petrovica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anohina-Naumeca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Ekenel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="437" to="444" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Found in translation: Learning robust joint representations by cyclic translations between modalities</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Manzini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Póczos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6892" to="6899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Convolutional MKL based multimodal emotion recognition and sentiment analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE 16th international conference on data mining (ICDM)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="439" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to learn single domain generalization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="12556" to="12565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Siddharth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01179</idno>
		<title level="m">Relating by Contrasting: A Data-efficient Framework for Multimodal Generative Models</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3483" to="3491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Joint multimodal learning with deep generative models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nakayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsuo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01891</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Contrastive Representation Distillation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SkgpBJrtvS" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Missing Modalities Imputation via Cascaded Residual Autoencoder</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning Factorized Multimodal Representations</title>
		<author>
			<persName><forename type="first">Y.-H</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Musical genre classification of audio signals</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tzanetakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on speech and audio processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="293" to="302" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Centralnet: a multilayer approach for multimodal fusion</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vielzeuf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lechervy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pateux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
				<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Select-additive learning: Improving generalization in multimodal sentiment analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Meghawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Multimedia and Expo (ICME)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="949" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multimodal generative models for scalable weakly-supervised learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5575" to="5585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Tensor Fusion Network for Multimodal Sentiment Analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
				<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2017">2017a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pincus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07250</idno>
		<idno>arXiv:1606.06259</idno>
		<imprint>
			<date type="published" when="2016">2017b. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Tensor fusion network for multimodal sentiment analysis</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Knowledge as Priors: Cross-Modal Knowledge Generalization for Datasets without Superior Knowledge</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6528" to="6537" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
