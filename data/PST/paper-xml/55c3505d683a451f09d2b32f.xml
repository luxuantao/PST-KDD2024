<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Shadow Detection and Removal from a Single Image</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
						</author>
						<author role="corresp">
							<persName><roleName>Member, IEEE, F. Sohel</roleName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
							<email>mohammed.bennamoun@uwa.edu.au</email>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">R</forename><surname>Togneri</surname></persName>
						</author>
						<author>
							<persName><forename type="first">F</forename><surname>Sohel</surname></persName>
						</author>
						<author>
							<persName><forename type="first">•</forename><forename type="middle">R</forename><surname>Togneri</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Software Engineering</orgName>
								<orgName type="institution">The University of Western Australia</orgName>
								<address>
									<addrLine>35 Stirling Highway</addrLine>
									<postCode>6009</postCode>
									<settlement>Crawley</settlement>
									<region>WA</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">The University of Western Australia</orgName>
								<address>
									<addrLine>35 Stirling Highway</addrLine>
									<postCode>6009</postCode>
									<settlement>Crawley</settlement>
									<region>WA</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automatic Shadow Detection and Removal from a Single Image</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3D68FA81FD3F4EF34DFE9A327E7198E7</idno>
					<idno type="DOI">10.1109/TPAMI.2015.2462355</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2015.2462355, IEEE Transactions on Pattern Analysis and Machine Intelligence received April 25, 2014; revised April 28, 2015. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2015.2462355, IEEE Transactions on Pattern Analysis and Machine Intelligence This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2015.2462355, IEEE Transactions on Pattern Analysis and Machine Intelligence</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Feature Learning</term>
					<term>Bayesian shadow removal</term>
					<term>Conditional Random Field</term>
					<term>ConvNets</term>
					<term>Shadow detection</term>
					<term>Shadow matting</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a framework to automatically detect and remove shadows in real world scenes from a single image. Previous works on shadow detection put a lot of effort in designing shadow variant and invariant hand-crafted features. In contrast, our framework automatically learns the most relevant features in a supervised manner using multiple convolutional deep neural networks (ConvNets). The features are learned at the super-pixel level and along the dominant boundaries in the image. The predicted posteriors based on the learned features are fed to a conditional random field model to generate smooth shadow masks. Using the detected shadow masks, we propose a Bayesian formulation to accurately extract shadow matte and subsequently remove shadows. The Bayesian formulation is based on a novel model which accurately models the shadow generation process in the umbra and penumbra regions. The model parameters are efficiently estimated using an iterative optimization procedure. Our proposed framework consistently performed better than the state-of-the-art on all major shadow databases collected under a variety of conditions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Shadows are a frequently occurring natural phenomenon, whose detection and manipulation are important in many computer vision (e.g., visual scene understanding) and computer graphics applications. As early as the time of Da Vinci, the properties of shadows were well studied <ref type="bibr" target="#b0">[1]</ref>. Recently, shadows have been used for tasks related to object shape <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, size, movement <ref type="bibr" target="#b3">[4]</ref>, number of light sources and illumination conditions <ref type="bibr" target="#b4">[5]</ref>. Shadows have a particular practical importance in augmented reality applications, where the illumination conditions in a scene can be used to seamlessly render virtual objects and their casted shadows. Contrary to the above mentioned assistive roles, shadows can also cause complications in many fundamental computer vision tasks. For instance, they can degrade the performance of object recognition, stereo, shape reconstruction, image segmentation and scene analysis. In digital photography, information about shadows and their removal can help to improve the visual quality of photographs. Shadows are also a serious concern for aerial imaging and object tracking in video sequences <ref type="bibr" target="#b5">[6]</ref>.</p><p>Despite the ambiguities generated by shadows, the Human Visual System (HVS) does not face any real difficulty in filtering out the degradations caused by shadows. We need to equip machines with such visual comprehension abilities. Inspired by the hierarchical architecture of the human visual cortex, many deep representation learning architectures have been proposed in the last decade. We draw our motivation from the recent successes of these deep learning methods in many computer vision tasks where learned features out-performed hand-crafted features <ref type="bibr" target="#b6">[7]</ref>. On that basis, we propose to use multiple convolutional neural networks (ConvNets) to learn useful feature representations for the task of shadow detection. ConvNets are biologically inspired deep network architectures based on Hubel and Wiesel's <ref type="bibr" target="#b7">[8]</ref> work on the cat's primary visual cortex. Once shadows are detected, an automatic shadow removal algorithm is proposed which encodes the detected information in the likelihood and prior terms of the proposed Bayesian formulation. Our formulation is based on a generalized shadow generation model which models both the umbra and penumbra regions. To the best of our knowledge, we are the first to use 'learned features' in the context of shadow detection, as opposed to the common carefully designed and hand-crafted features. Moreover, the proposed approach detects and removes shadows automatically without any human input (Fig. <ref type="figure" target="#fig_0">1</ref>).</p><p>Our proposed shadow detection approach combines local information at image patches with the local information across boundaries (Fig. <ref type="figure" target="#fig_0">1</ref>). Since the regions and the boundaries exhibit different types of features, we split the detection procedure into two respective portions. Separate ConvNets are consequently trained for patches extracted around the scene boundaries and the super-pixels. Predictions made by the ConvNets are local and we therefore need to exploit the higher level interactions between the neighboring pixels. For this purpose, we incorporate local beliefs in a Conditional Random Field (CRF) model which enforces the labeling consistency over the nodes of a grid graph defined on an image (Sec. 3). This removes isolated and spurious labeling outcomes and encourages neighboring pixels to adopt the same label.</p><p>Using the detected shadow mask, we identify the umbra (Latin meaning shadow), penumbra (Latin meaning almost-shadow) and shadow-less regions and propose a Bayesian formulation to automatically remove shadows. We introduce a generalized shadow generation model which separately defines the umbra and penumbra generation process. The resulting optimization problem has a relatively large number of unknown parameters, whose MAP estimates are efficiently computed by alternatively solving for the parameters (Eq. 26). The shadow removal process also extracts smooth shadow matte that can be used in applications such as shadow compositing and editing (Sec. 4).</p><p>A preliminary version of this research (which solely focuses on shadow detection) appeared in <ref type="bibr" target="#b8">[9]</ref>. In addition, the current study includes: <ref type="bibr" target="#b0">(1)</ref> a new approach to estimate shadow statistics, (2) automatic shadow removal and shadow matte extraction, (3) a substantial number of additional experiments, analysis and limitations, (4) possible applications in many computer vision and graphics tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK AND CONTRIBUTIONS</head><p>Shadow Detection: One of the most popular methods to detect shadows is to use a variety of shadow variant and invariant cues to capture the statistical and deterministic characteristics of shadows <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. The extracted features model the chromatic, textural <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> and illumination <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref> properties of shadows to determine the illumination conditions in the scene. Some works give more importance to features computed across image boundaries, such as intensity and color ratios across boundaries and the computation of texton features on both sides of the edges <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref>. Although these feature representations are useful, they are based on assumptions that may not hold true in all cases. As an example, chromatic cues assume that the texture of the image regions remains the same across shadow boundaries and only the illumination is different. This approach fails when the image regions under shadows are barely visible. Moreover, all of these methods involve a considerable effort in the design of hand-crafted features for shadow detection and feature selection (e.g., the use of ensemble learning methods to rank the best features <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>). Our data-driven framework is different and unique: we propose to use deep feature learning methods to 'learn the most relevant features' for shadow detection.</p><p>Owing to the challenging nature of the shadow detection problem, many simplistic assumptions are commonly adopted. Previous works made assumptions related to the illumination sources <ref type="bibr" target="#b4">[5]</ref>, the geometry of the objects casting shadows and the material properties of the surfaces on which shadows are cast. For example, Salvador et al. <ref type="bibr" target="#b13">[14]</ref> consider object cast shadows while Lalonde et al. <ref type="bibr" target="#b10">[11]</ref> only detect shadows that lie on the ground. Some methods use synthetically generated training data to detect shadows <ref type="bibr" target="#b16">[17]</ref>. Techniques targeted for video surveillance applications take advantage of multiple images <ref type="bibr" target="#b17">[18]</ref> or time-lapse sequences <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> to detect shadows. User assistance is also required by many proposed techniques to achieve their attained performances <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. In contrast, our shadow detection method makes absolutely 'no prior assumptions' about the scene, the shadow properties, the shape of objects, the image capturing conditions and the surrounding environments. Based on this premise, we tested our proposed framework on all of the publicly available databases for shadow detection from single images. These databases contain common real world scenes with artifacts such as noise, compression and color balancing effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shadow Removal and Matting:</head><p>Almost all approaches that are employed to either edit or remove shadows are based on models that are derived from the image formation process. A popular choice is to physically model the image into a decomposition of its intrinsic images along with some parameters that are responsible for the generation of shadows. As a result, the shadow removal process is reduced to the estimation of the model parameters. Finlayson et al. <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> addressed this problem by nullifying the shadow edges and reintegrating the image, which results in the estimation of the additive scaling factor. Since such global integration (which requires the solution of a 2D Poisson equation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref>) causes artifacts, the integration along a 1D Hamiltonian path <ref type="bibr" target="#b25">[26]</ref> is proposed for shadow removal. However, these and other gradient based methods (such as <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>) do not account for the shadow variations inside the umbra region. To address this shortcoming, Arbel and Hel-Or <ref type="bibr" target="#b28">[29]</ref> treat the illumination recovery problem as a 3D surface reconstruction and use a thin plate model to successfully remove shadows lying on curved surfaces. Alternatively, information theory based techniques are proposed in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b29">30]</ref> and a bilateral filtering based approach is recently proposed in <ref type="bibr" target="#b30">[31]</ref> to recover intrinsic (illumination and reflectance) images. However, these approaches either require user assistance, calibrated imaging sensors, careful parameter selection or considerable processing times. To overcome these shortcomings, some reasonably fast and accurate approaches have been proposed which aim to transfer the color statistics from the nonshadow regions to the shadow regions ('color transfer based approaches' e.g., <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>). Our proposed shadow removal algorithm also belongs to the category of color transfer based approaches. However, in contrast to previous related works, we propose a generalized image formation model which enables us to deal with non-uniform umbra regions as well as soft shadows. Color transfer is also made at multiple spatial levels, which helps in the reduction of noise and color artifacts. An added advantage of our approach is our ability to separate smooth shadow matte from the actual image.</p><p>Several assumptions are made in the shadow removal literature due to the ill-posed nature of recovering the model parameters for each pixel. The camera sensor parameters are needed in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b30">31]</ref>. Multiple narrowband sensor outputs for each scene are required in <ref type="bibr" target="#b30">[31]</ref>, while <ref type="bibr" target="#b1">[2]</ref> employs a sequence of images to recover the intrinsic components. Lambertian surface and Planckian lightening assumptions are made in <ref type="bibr" target="#b30">[31]</ref>. Though several approaches work just on a single image, they require considerable user interaction to identify either tri-maps <ref type="bibr" target="#b35">[36]</ref>, quad-maps <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>, gradients <ref type="bibr" target="#b36">[37]</ref> or exact shadow boundaries <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. Su and Chen <ref type="bibr" target="#b37">[38]</ref> tried to minimize the user effort by specifying the complete shadow boundary from the user provided strokes. In contrast, our framework does not require any form of user interaction and makes no assumption regarding the camera or scene properties (except that the object surfaces are assumed to be Lambertian).</p><p>The key contributions of our work are outlined below:</p><p>• We propose a new approach for robust shadow detection combining both regional and acrossboundary learned features in a probabilistic framework involving CRFs (Sec. 3). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED SHADOW DETECTION FRAME-WORK</head><p>Given a single color image, we aim to detect and localize shadows precisely at the pixel level (see block diagram in Fig. <ref type="figure" target="#fig_1">2</ref>). If y denotes the desired binary mask encoding class relationships, we can model the shadow detection problem as a conditional distribution:</p><formula xml:id="formula_0">P(y|x; w) = 1 Z(w) exp(-E(y, x; w))<label>(1)</label></formula><p>where, the parameter vector w includes the weights of the model, the manifest variables are represented by x where x i denotes the intensity of pixel i ∈ {p i } 1×N and Z(w) denotes the partition function. The energy function is composed of two potentials; the unary potential ψ i and the pairwise potential</p><formula xml:id="formula_1">ψ ij : E(y, x; w) = i∈V ψi(yi, x; wi) + (i,j)∈E ψij(yij, x; wij)<label>(2)</label></formula><p>In the following discussion, we will explain how we model these potentials in a CRF framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature Learning for Unary Predictions</head><p>The unary potential in Eq. 2 considers the shadow properties both at the regions and at the boundaries inside an image.</p><p>ψi(yi, x; wi)</p><formula xml:id="formula_2">= region φ r i (yi, x; w r i ) + boundary φ b i (yi, x; w b i )<label>(3)</label></formula><p>We define each of the boundary and regional potentials, φ r and φ b respectively, in terms of probability estimates from the two separate ConvNets,</p><formula xml:id="formula_3">φ r i (yi, x; w r i ) = -w r i log P cnn1 (yi|xr) φ b i (yi, x; w b i ) = -w b i log Pcnn2(yi|x b )<label>(4)</label></formula><p>This is logical because the features to be estimated at the boundaries are likely to be different from the ones estimated inside the shadowed regions. Therefore, we train two separate ConvNets, one for the regional potentials and the other for the boundary potentials. The ConvNet architecture used for feature learning consists of alternating convolution and sub-sampling layers (Fig. <ref type="figure" target="#fig_2">3</ref>). Each convolutional layer in a ConvNet consists of filter banks which are convolved with the input feature maps. The sub-sampling layers pool the incoming features to derive invariant representations. This layered structure enables ConvNets to learn multilevel hierarchies of features. The final layer of the network is fully connected and comes just before the output layer. This layer works as a traditional MLP with one hidden layer followed by a logistic regression output layer which provides a distribution over the classes. Overall, after the network has been trained, it takes an RGB patch as an input and processes it to give a posterior distribution over binary classes. ConvNets operate on equi-sized windows, so it is required to extract patches around desired points of interest. For the case of regional potentials, we extract super-pixels by clustering the homogeneous pixels 1 . Afterwards, a patch (I r ) is extracted by centering a τ s × τ s window at the centroid of each superpixel. Similarly for boundary potentials, we first apply a Bilateral filter and then extract boundaries using the gPb technique <ref type="bibr" target="#b39">[40]</ref>. We traverse each boundary with a stride λ b and extract a τ s × τ s patch at each step to incorporate local context 2 . Therefore, ConvNets operate on sets of boundary and super-pixel patches, x r = {I r (i, j)} 1×|F slic (x)| and</p><formula xml:id="formula_4">x b = {I b (i, j)} 1× |F gPb (x)| λ b</formula><p>respectively, where |.| is the cardinality operator. Note that we include synthetic data (generated by artificial linear transformations <ref type="bibr" target="#b40">[41]</ref>) during the training process. This data augmentation is important not only because it removes the skewed class distribution of the shadowed regions but it also results in an enhanced performance. Moreover, data augmentation helps to reduce the overfitting problem in ConvNets (e.g., in <ref type="bibr" target="#b41">[42]</ref>) which results in the learning of more robust feature representations.</p><p>During the training process, we use stochastic gradient descent to automatically learn feature representations in a supervised manner. The gradients are computed using back-propagation to minimize the cross entropy loss function <ref type="bibr" target="#b42">[43]</ref>. We set the training parameters (e.g., momentum and weight decay) using a cross validation process. The training samples are shuffled randomly before training since the network can learn faster from unexpected samples. The weights of the ConvNet were initialized with randomly drawn samples from a Gaussian distribution of zero mean and a variance that is 1. In our implementation we used SLIC <ref type="bibr" target="#b38">[39]</ref>, due to its efficiency. 2. the step size is λ b = τs/4 to get partially overlapping windows. The ConvNet trained on boundary patches learn to separate shadow and reflectance edges while the ConvNet trained on regions can differentiate between shadow and non-shadow patches. For the case of the regions, the posteriors predicted by ConvNet are assigned to each super pixel in an image. However, for the boundaries, we first localize the probable shadow location using the local contrast and then average the predicted probabilities over each contour generated by the Ultra-metric Contour Maps (UCM) <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Contrast Sensitive Pairwise Potential</head><p>The pairwise potential in Eq. 2 is defined as a combination of the class transition potential φ p1 and the spatial transition potential φ p2 :</p><formula xml:id="formula_5">ψij(yij, x; wij) = wijφp 1 (yi, yj)φp 2 (x).<label>(5)</label></formula><p>The class transition potential takes the form of an Ising prior:</p><formula xml:id="formula_6">φp 1 (yi, yj) = α1 y i =y j = 0 if yi = yj α otherwise (6)</formula><p>Fig. <ref type="figure">4</ref>: The Proposed Shadow Removal Framework: After the detection of the shadows in the image, we estimate the umbra, penumbra and object-shadow boundary. Given this information, a multi-level color transfer is applied to obtain a crude estimate of shadow-less image. This rough estimate is further improved using the proposed Bayesian formulation which estimates the optimal shadow-less image along with the shadow model parameters.</p><p>The spatial transition potential captures the differences in the adjacent pixel intensities:</p><formula xml:id="formula_7">φp 2 (x) = [exp(- xi -xj 2 βx xi -xj 2 )]<label>(7)</label></formula><p>where, • denotes the average contrast in an image. The parameters α and β x were derived using cross validation on each database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Shadow Contour Generation using CRF Model</head><p>We model the shadow contour generation in the form of a two-class scene parsing problem where each pixel is labeled either as a shadow or a non-shadow. This binary classification problem takes probability estimates from the supervised feature learning algorithm and incorporates them in a CRF model. The CRF model is defined on a grid structured graph topology, where graph nodes correspond to image pixels (Eq. 2). When making an inference, the most likely labeling is found using the Maximum a Posteriori (MAP) estimate (y * ) upon a set of random variables y ∈ L N . This estimation turns out to be an energy minimization problem since the partition function Z(w) does not depend on y:</p><formula xml:id="formula_8">y * = argmax y∈L N P(y|x; w) = argmin y∈L N E(y, x; w)<label>(8)</label></formula><p>The CRF model proved to be an elegant source to enforce label consistency and the local smoothness over the pixels. However, the size of the training space (labeled images) makes it intractable to compute the gradient of the likelihood. Therefore the parameters of the CRF cannot be found by simply maximizing the likelihood of the hand labeled shadows. Hence, we use the 'margin rescaled algorithm' to learn the parameters (w in Eq. 8) of our proposed CRF model (see <ref type="bibr">Fig 3 in [44]</ref> for details).</p><p>Because our proposed energies are sub-modular, we use graph-cuts for making efficient inferences <ref type="bibr" target="#b44">[45]</ref>. In the next section, we describe the details of our shadow removal and matting framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PROPOSED SHADOW REMOVAL AND MAT-TING FRAMEWORK</head><p>Based on the detected shadows in the image, we propose a novel automatic shadow removal approach. A block diagram of the proposed approach is presented in Fig. <ref type="figure">4</ref>. The first step is to identify the umbra, penumbra and the corresponding non-shadowed regions in an image. We also need to identify the boundary where the actual object and its shadow meet. This identification helps to avoid any errors during the estimation of shadow/nonshadow statistics (e.g., color distribution). In previous works (such as <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34]</ref>), this process has been carried out manually through human interaction. We, however, propose a simple procedure to automatically estimate the umbra, penumbra regions and the object-shadow boundary.</p><p>Heuristically, the object-shadow boundary is relatively darker compared to other shadow boundaries where differences in light intensity are significant. Therefore, given a shadow mask, we calculate the boundary normals at each point. We b / c max ≤ 0.5 are considered to correspond to the segments which separate the object and its cast shadow. This simple procedure performs reasonably well for most of our test examples (Fig. <ref type="figure" target="#fig_3">5</ref>). In the case where  </p><formula xml:id="formula_9">1: h S , h N ← Get histogram of color distribution in S, N 2: g S , g N ← Fit GMM on h S , h N using EM algorithm 3: for each j ∈ [0, J] do             </formula><p>Channel wise color transfer between corresponding Gaussians using Eqs. 9, 10. Get probability of a pixel/super-pixel to belong to a Gaussian component using Eq. 11. Calculate overall transfer for each color channel using Eq. 12. 4: Combine multiple transfers:</p><p>C * (x, y) = 1 J+1 j C j (x, y) 5: Calculate probability of a pixel to be shadow or non-shadow:</p><formula xml:id="formula_10">p S (x, y) = K k=1 ω k S |D k N (x,y)| |D k S (x,y)|+|D k N (x,y)|</formula><p>6: Modify color transfer using Eq. 13 7: Improve result from above step using Eq. 14 return ( Î(x, y))</p><p>the object shadow boundary is not visible, no boundary portion is classified as an object shadow boundary and the shadow-less statistics are taken from all around the shadow region. In most cases, this does not affect the removal performance as long as the object-shadow boundary is not very large compared to the total shadow boundary.</p><p>To estimate the umbra and penumbra regions, the boundary is estimated at each point of the shadow contour by fitting a curve and finding the corresponding normal direction. This procedure is adopted to extract accurate boundary estimates instead of local normals which can result in erroneous outputs at times. We propagate the boundaries along the estimated normal directions until the intensity change becomes insignificant (Fig. <ref type="figure" target="#fig_5">6</ref>). This results in an approximation of the penumbra region. We then exclude this region from the shadow mask and the remaining region is considered as the umbra region. The region immediately adjacent to the shadow region, with twice the width of the penumbra region is treated as the non-shadow region. Note that our approach is based on the assumption that the texture remains approximately the same across the shadow boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Rough Estimation of Shadow-less Image by Color-transfer</head><p>The rough shadow-less image estimation process is based on the one adopted by the color transfer techniques in <ref type="bibr" target="#b31">[32]</ref> and <ref type="bibr" target="#b33">[34]</ref>. As opposed to <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b33">34]</ref>, we perform a multilevel color transfer and our method does not require any user input. The color statistics of the shadowed as well as the non-shadowed regions are modeled using a Gaussian mixture model (GMM). For this purpose, a continuous probability distribution function is estimated from the histograms of both regions using the Expectation-Maximization (EM) algorithm. The EM algorithm is initialized using an unsupervised clustering algorithm (k-means in our implementation) and the EM iterations are carried out until convergence. We treat each of the R, G and B channels separately and fit mixture models to each of the respective histograms. It is considered that the estimated Gaussians, in the shadow and non-shadow regions, correspond to each other when arranged according to their means. Therefore, the color transfer is computed among the corresponding Gaussians using the following pair of equations:</p><formula xml:id="formula_11">D k S (x, y) = I(x, y) -µ k S σ k S (9) C k (x, y) = µ k N + σ k N D S (x, y)<label>(10)</label></formula><p>where D(•) measures the normalized deviation for each pixel, S and N denote the shadow and non-shadow regions respectively. The index k is in range <ref type="bibr">[1, K]</ref>, where K denotes the total number of Gaussians used to approximate the histogram of S. The probability that a pixel (with coordinates x, y) belongs to a certain Gaussian component can be represented in terms of its normalized deviation:</p><formula xml:id="formula_12">p k G (x, y) = |D k S (x, y)| K k=1 1 |D k S (x, y)| + -1<label>(11)</label></formula><p>The overall transfer is calculated by taking the weighted sum of transfers for all Gaussian components:</p><formula xml:id="formula_13">C j=0 (x, y) = K k=1 p k G (x, y)C k (x, y).<label>(12)</label></formula><p>The color transfer performed at each pixel location (i.e. at level j = 0) using Eq. 12 is local, and it thus, does not accurately restore the image contrast in the shadowed regions. Moreover, this local color transfer is prone to noise and discontinuities in illumination. We therefore resort to a hierarchical strategy which restores color at multiple levels and combines all transfers which results in a better estimation of the shadow-less image. A graph based segmentation procedure <ref type="bibr" target="#b45">[46]</ref> is used to group the pixels. This clustering is performed at J levels, which we set to 4 in the current work based on the performance on a small validation set, where we noted an over-smoothing and a low computational efficiency when J ≥ 5. Since, the segment size is kept quite small, it is highly unlikely that the differently colored pixels will be grouped together. At each level j ∈ [1, J], the mean of each cluster is used in the color transfer process (using Eqs. 9, 10) and the resulting estimate (Eq. 12) is distributed to all pixels in the cluster. This gives multiple color transfers C j (x, y) at J different resolutions plus the local color transfer i.e. C j=0 (x, y). At each level, a pixel or a super-pixel is treated as a discrete unit during the color transfer process. The resulting transfers are integrated to produce the final outcome:</p><formula xml:id="formula_14">C * (x, y) = 1 J+1 J j=0 C j (x, y).</formula><p>This process helps in reducing the noise. It also restores a better texture and improves the quality of the restored image. It should be noted that our hierarchical strategy helps in successfully retaining the self shading patterns in the recovered image compared to previous works (Sec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.3).</head><p>To avoid possible errors due to the small non-shadow regions that may be present in the selected shadow region S, we calculate the probability of a pixel to be shadowed using: p S (x, y) = However, the penumbra region pixels will not get accurate intensity values. To correct this anomaly, we define a relation which measures the probability (in a naive sense) of a pixel to belong to the penumbra region. Since the penumbra region occurs around the shadow boundary, we define it as: b S (x, y) = d(x, y)/d max . The penumbra region is recovered using the exemplar based inpainting approach of Criminisi et al. <ref type="bibr" target="#b46">[47]</ref>. The resulting improved approximation of the shadow-less image is,</p><formula xml:id="formula_15">Î(x, y) = (1 -b S (x, y))E(x, y) + b S (x, y)C (x, y) (14)</formula><p>where, E is the inpainted image.</p><p>In our approach, the crude estimate of a shadowless image (Eq. 14) is further improved using Bayesian estimation (Sec. <ref type="bibr">4.3)</ref>. But first we need to introduce the proposed shadow generation model used in our Bayesian formulation (Sec. 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Generalised Shadow Generation Model</head><p>Unlike previous works (such as <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>), which do not differentiate between the umbra and the penumbra regions during the shadow formation process, we propose a model which treats both types of shadow  regions separately. It is important to make such distinction because the umbra and penumbra regions exhibit distinct illumination characteristics and have a different influence from the direct and indirect light (Fig. <ref type="figure" target="#fig_5">6</ref>).</p><formula xml:id="formula_16">(i) (ii) (iii) (iv) (i) (ii) (iii) (iv)</formula><p>Let us suppose that we have a scene with illuminated and shadowed regions. A normal illuminated image can be represented in terms of two intrinsic images according to the image formation model of Barrow et al. <ref type="bibr" target="#b47">[48]</ref>:</p><formula xml:id="formula_17">I(x, y) = L(x, y)R(x, y)<label>(15)</label></formula><p>where L and R are the illumination and reflectance respectively and x, y denote the pixel coordinates. The illumination intrinsic image takes into account the illumination differences such as shadows and shading. We assume that a single source of light is casting the shadows. The ambient light is assumed to be uniformly distributed in the environment due to the indirect illumination caused by reflections. Therefore,</p><formula xml:id="formula_18">I(x, y) = (L d (x, y) + L i (x, y))R(x, y)<label>(16)</label></formula><p>A cast shadow is formed when the direct illumination is blocked by some obstructing object resulting in an occlusion. A cast shadow can be described as the combination of two regions created by two distinct phenomena, umbra (U) and penumbra (P). Umbra is surrounded by the penumbra region where the light intensity changes sharply from dark to illuminated. The occlusion which casts the shadow block all of the direct illumination and parts of the indirect illumination to create the umbra region. We can represent this as;</p><formula xml:id="formula_19">I u (x, y) = β (x, y)L i (x, y)R(x, y) ∀x, y ∈ U (17) ∵ L d (x, y) ≈ 0 ∀x, y ∈ U</formula><p>where, β (x, y) is the scaling factor for the U region. Using Eq. 16 and 17, we have;</p><formula xml:id="formula_20">I(x, y) = I u (x, y) β (x, y) + α(x, y)<label>(18)</label></formula><formula xml:id="formula_21">I u = I(x, y)β (x, y) -α(x, y)β (x, y)<label>(19)</label></formula><p>where, α(x, y) = L d (x, y)R(x, y).</p><p>For the case of the penumbra region, all direct light is not blocked, rather its intensity decreases from a fully lit region towards the umbra region. Since the major source of change is the direct light, we can neglect the variation caused by the indirect illumination in the penumbra region. Therefore,</p><formula xml:id="formula_22">I p (x, y) = (β (x, y)L d (x, y) + L i (x, y))R(x, y)<label>(20)</label></formula><p>∵ ∆L i (x, y) ≈ 0 ∀x, y ∈ P where, β (x, y) is the scaling factor for the P region. Using Eq. 16 and 20, we have:</p><formula xml:id="formula_23">I p (x, y) = I(x, y) -α(x, y)(1 -β (x, y)).<label>(21)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Bayesian Shadow Removal and Matting</head><p>Having formulated the shadow generation model, we can now describe the estimation procedure of the model parameters in probabilistic terms. We represent our problem in a well-defined Bayesian formulation and estimate the required parameters using maximum a posteriori estimate (MAP):</p><formula xml:id="formula_24">{α * , β * } = argmax α,β P(α, β | U, P, N)<label>(22)</label></formula><p>= argmax α,β P(U, P, N| α, β)P(α)P(β) P(U, P, N)</p><p>= argmax </p><p>Let I s (x, y) ∀x, y ∈ {U ∪ P} represent the complete shadow region. Then, the first term in Eq. 25 can be written as a function of I s since the parameters α and β do not affect the region N, therefore:</p><formula xml:id="formula_27">= argmax α,β P (I s | α, β) + P (α) + P (β)<label>(26)</label></formula><p>The first term in Eq. 26 can be modeled by the difference between the current pixel values and the estimated pixel values, as follows: where, η(x, y) = 1 -λ(x,y) λmax and π is an indicator function which switches on for the penumbra region pixels. λ(•) is the distance metric which quantifies the shortest distance between a valid shadow boundary (i.e., excluding the object-shadow boundary). The estimated shadowed image ( Îs ) can be decomposed as follows using Eqs. 19 and 21.</p><formula xml:id="formula_28">P (Is| α, β) = -</formula><formula xml:id="formula_29">Îs (x, y) = ( Î(x, y) -α(x, y))β (x, y) ∀{x, y} ∈ U ⊂ S Î(x, y) -α(x, y)(1 -β (x, y)) ∀{x, y} ∈ P ⊂ S</formula><p>It can be noted that P (I s | α, β) models the error caused by the estimated parameters and encourages the recovered pixel values ( Îs (x, y)) to lie close to (I s (x, y)) with variance σ 2 I following a Gaussian distribution. However, in the above formulation, there are nine unknowns for each pixel located inside the shadowed region. If we had a smaller scale problem (e.g., finding the precise shadow matte in the penumbra region by Chuang et al. <ref type="bibr" target="#b35">[36]</ref>), we could have directly solved for the unknowns. But in our case, the large number of variables makes the likelihood calculation rather difficult and time consuming, especially when the number of shadowed pixels is large. We therefore resort to optimize the crude shadow-less image ( Î(x, y)) calculated in Sec. 4.1, Eq. 14.</p><p>The prior P (β) can be modeled as a Gaussian probability distribution centered at the mean ( β) of the neighboring pixels. This helps in estimating a smoothly varying beta mask. So,</p><formula xml:id="formula_30">P (β) = - {x,y} |β(x, y) -β(x , y )| 2 2σ 2 β , (x , y ) ∈ N (x, y)<label>(28)</label></formula><p>The prior P (α) can also be modeled in a similar fashion. However, we require α to model the variations in the penumbra region as well. Therefore, an additional term (called the 'image consistency term') is introduced in the prior P (α) to smooth the estimated shadow-less image along the boundaries and to incorporate feedback from the previously estimated crude shadowless image. Therefore,</p><formula xml:id="formula_31">P (α) = - {x,y} |α(x, y) -ᾱ(x , y )| 2 2σ 2 α - 1 2σ 2 I {x,y}∈S (1 - λ(x, y) λ max )|I(x, y) -Î(x, y)| 2 , (x , y ) ∈ N (x, y)<label>(29)</label></formula><p>In the image consistency term (second term in Eq. 29), I(x, y) will take different values according to Eqs. 19 and 21:</p><formula xml:id="formula_32">I(x, y) = I u /β (x, y) + α(x, y) ∀{x, y} ∈ U I p (x, y) + α(x, y)(1 -β (x, y)) ∀{x, y} ∈ P</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Parameter Estimation</head><p>In spite of the crude shadow image estimation, it can be seen from Eq. 27 that the objective function is not linear or quadratic in term of the unknowns. To apply the gradient based energy optimization procedure, we simplify our problem by breaking it into two sub-optimization problems and apply an iterative joint optimization as follows: For the umbra region,</p><formula xml:id="formula_33">β (x, y) = γ 2 β β(x , y ) -γ 2 I [α(x, y)Is(x, y) -Î(x, y)Is(x, y)] γ 2 β -γ 2 I [2 Î(x, y)α(x, y) -α 2 (x, y) -Î2 (x, y)]<label>(30)</label></formula><p>For the penumbra:</p><formula xml:id="formula_34">β (x, y) = αγ 2 Is [∆(x, y) + α] + γ 2 β β + αγ 2 I η(x, y)[∆(x, y) + α] α 2 γ 2 Is + γ 2 β + α 2 γ 2 I η(x, y)<label>(31)</label></formula><p>where, γ = σ -1 . To optimize α, the parameter β is held constant and the first order partial derivative is taken with respect to α and is set to zero. We get the following set of equations: For the umbra region:</p><formula xml:id="formula_35">α(x, y) = γ 2 α ᾱ(x , y ) -γ 2 I [β (x, y)Is(x, y) -Î(x, y)β 2 (x, y)] γ 2 α + γ 2 I β 2 (x, y)<label>(32)</label></formula><p>For the penumbra:</p><formula xml:id="formula_36">α(x, y) = -γ 2 Is (1 -β )∆(x, y) + γ 2 α ᾱ -γ 2 I (1 -β )η(x, y)∆(x, y) γ 2 Is (1 -β ) 2 + γ 2 α + γ 2 I (1 -β ) 2 η(x, y)<label>(33)</label></formula><p>Algorithm 4.2: BAYESIANREMOVAL(U, P, N, Î) where, ∆(x, y) = I s (x, y)-Î(x, y). We iteratively perform this procedure on each pixel in the shadow region until convergence.</p><formula xml:id="formula_37">β ← 1, α ← 0, 0 ← 10 -3 while δ &gt; 0 do                          for each {x, y} ∈ S do                  if {x,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Boundary Enhancement in a Shadow-less Image</head><p>The resulting shadow-less image exhibits traces of shadow boundaries in some cases. To remove these artifacts, we divide the shadow boundary into a group of segments, where each segment contains nearly similar colored pixels. The boundary segments which belong to the object shadow boundary are excluded from further processing. For each non-object shadow boundary segment, we perform Poisson smoothing <ref type="bibr" target="#b48">[49]</ref> to conceal the shadow boundary artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS AND ANALYSIS</head><p>We evaluated our technique on three widely used and publicly available datasets. For the qualitative comparison of shadow removal, we also evaluate our technique on a set of commonly used images in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>UCF Shadow Dataset is a collection of 355 images together with their manually labeled ground truths. Zhu et al. have used a subset of 255/355 images for shadow detection <ref type="bibr" target="#b9">[10]</ref>. CMU Shadow Dataset consists of 135 consumer grade images with labels for only those shadow edges which lie on the ground plane <ref type="bibr" target="#b10">[11]</ref>. Since our algorithm is not restricted to ground shadows, we tested our approach on the more challenging criterion of full shadow detection which required the generation of new ground truths. UIUC Shadow Dataset contains 108 images each of which is paired with its corresponding shadow-free image to generate a ground truth shadow mask <ref type="bibr" target="#b12">[13]</ref>. Test/Train Split: For UCF and UIUC databases, we used the split mentioned in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13]</ref>. Since CMU database <ref type="bibr" target="#b10">[11]</ref> did not report the split, we therefore used even/odd images for training/testing (following the procedure in Jiang et al. <ref type="bibr" target="#b11">[12]</ref>).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation of Shadow Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Results</head><p>We assessed our approach both quantitatively and qualitatively on all the major datasets for single image shadow detection. We demonstrate the success of our shadow detection framework on different types of scenes including beaches, forests, street views, aerial images, road scenes and buildings. The databases also contain shadows under a variety of illumination conditions such as sunny, cloudy and dark environments. For quantitative evaluation, we report the performance of our framework when only the unary term (Eq. 3) was used for shadow detection. Further, we also report the perpixel accuracy achieved using the CRF model on all the datasets. This means that labels are predicted for every pixel in each test image and are compared with the ground-truth shadow masks. For the UCF and CMU datasets, the initial learning rate of η 0 = 0.1 was used, for the UIUC dataset we set η 0 = 0.01 based on the performance on a small validation set. After every 20 epochs the learning rate was decreased by a small factor β = 0.5 which resulted in a best performance. Table <ref type="table" target="#tab_3">1</ref> summarizes the overall results of our framework and shows a comparison with several state-ofthe-art methods in shadow detection. It must be noted that the accuracy of Jiang's method <ref type="bibr" target="#b11">[12]</ref> (on the CMU database) is given by the Equal Error Rate (EER). All other accuracies represent the highest detection rate achieved, which may not necessarily be an EER. Using the ConvNets and the CRF, we were able to get the best performance on the UCF, CMU and UIUC databases with a respective increase of 0.50%, 4.48% and 4.55% compared to the previous best results 3 . For the case of the UCF dataset, a gain of 0.5% accuracy may look modest. But it should be noted that the previous best methods of Zhu et al. <ref type="bibr" target="#b9">[10]</ref> and Guo et al. <ref type="bibr" target="#b12">[13]</ref> were only evaluated on a subset (255/355 images). In contrast, we report results on the complete dataset because the exact subset used in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13]</ref> is not known. Compared to Jiang et al. <ref type="bibr" target="#b11">[12]</ref>, which is evaluated on the complete dataset, we achieved a relative accuracy gain of 8.56%. On five sets of 255 randomly selected images from the UCF dataset, our method resulted in an accuracy of 91.4 ± 4.2% which is a relative gain of 1.3% over Guo et al. <ref type="bibr" target="#b12">[13]</ref>.</p><p>Table <ref type="table" target="#tab_5">2</ref> shows the comparison of class-wise accuracies. The true positives (correctly classified shadows) 3. Relative increase in performance is calculated by: 100×(our accuracyprevious best)/previous best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods/Datasets</head><p>Shadows Non-Shadows UCF Dataset -BDT-BCRF (Zhu et al. <ref type="bibr" target="#b9">[10]</ref>)</p><p>63.9% 93.4% -Unary-Pairwise (Guo et al. <ref type="bibr">[</ref>  are reported as the number of predicted shadow pixels which match with the ground-truth shadow mask. True negative (correctly classified non-shadows) are reported as the number of predicted non-shadow pixels which match with the ground-truth non-shadow mask. It is interesting to see that our framework has the highest shadow detection performance on the UCF, CMU and UIUC datasets. For the case of CMU dataset, our approach got a relatively lower non-shadow region detection accuracy of 90.9% compared to 96.4% of Lalonde et al. <ref type="bibr" target="#b10">[11]</ref>. This is due to the reason that <ref type="bibr" target="#b10">[11]</ref> only consider ground shadows and thus ignore many false negatives. In contrast, our method is evaluated on more challenging case of general shadow detection i.e. all types of shadows. The ROC curve comparisons are shown in Fig. <ref type="figure" target="#fig_14">10</ref>. The plotted ROC curves represent the performance of the unary detector since we cannot generate ROC curves from the outcome of the CRF model. Our approach achieves the highest AUC measures for all datasets (Fig. <ref type="figure" target="#fig_14">10</ref>).</p><p>Some representative qualitative results are shown in Fig. <ref type="figure" target="#fig_13">9</ref> and Fig. <ref type="figure" target="#fig_16">11</ref>. The proposed framework successfully detects shadows in dark environments (Fig. <ref type="figure" target="#fig_13">9:</ref> 1 st row, middle image) and distinguishes between dark nonshadow regions and shadow regions (Fig. <ref type="figure" target="#fig_13">9</ref>: 2 nd row, 2 nd and 5 th image from left). It performs equally well on satellite images (Fig. <ref type="figure" target="#fig_13">9</ref>: last column) and outdoor scenes with street views (Fig. <ref type="figure" target="#fig_13">9</ref>: 1 st row, 3 rd and 5 th images; 2 nd row, middle image), buildings (Fig. <ref type="figure" target="#fig_13">9</ref>: 1 st column) and shadows of animals and humans (Fig. <ref type="figure" target="#fig_13">9</ref>: 2 nd column).   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Discussion</head><p>The previously proposed methods (e.g., Zhu et al. <ref type="bibr" target="#b9">[10]</ref>, Lalonde et al. <ref type="bibr" target="#b10">[11]</ref>) that use a large number of handcrafted features, not only require a lot of effort in their design but also require long training times when ensemble learning methods are used for feature selection. As an example, Zhu et al.  We extensively evaluated our approach on all available databases and our proposed framework turned out to be fairly generic and robust to variations. It achieved the best results on all the single image shadow databases known to us. In contrast, previous techniques were only tested on a portion of database <ref type="bibr" target="#b10">[11]</ref>, one <ref type="bibr" target="#b9">[10]</ref> or at most two databases <ref type="bibr" target="#b12">[13]</ref>. Another interesting observation was that the proposed framework performed reasonably well when our ConvNets were trained on one dataset and tested on another dataset. Table <ref type="table" target="#tab_6">3</ref> summarizes the results of cross-dataset evaluation experiments. These performance levels show that the feature representations learned by the ConvNets across the different datasets were common to a large extent. This observation further supports our claim regarding the generalization ability of the proposed framework.</p><p>In our experiments, objects with dark albedo turned out to be a difficult case for shadow detection. Moreover, some ambiguities were caused by the complex self shading patterns created by tree leaves. There were some inconsistencies in the manually labeled ground-truths, in which a shadow mask was sometimes missing for an attached shadow. Narrow shadowy regions caused by structures like poles and pipes also proved to be a challenging case for shadow detection. Examples of the above mentioned failure cases are shown in Fig. <ref type="figure" target="#fig_16">11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation of Shadow Removal</head><p>For a quantitative evaluation of our shadow removal framework, we used all images from the UIUC Shadow dataset which come with their corresponding shadowfree ground truths <ref type="bibr" target="#b12">[13]</ref>. The qualitative results of our method are evaluated against the common evaluation images used in the literature for a fair comparison. To further illustrate the performance of our algorithm, we also included qualitative results on some example images from UIUC, UCF and CMU shadow datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Quantitative Evaluation</head><p>Table <ref type="table" target="#tab_8">4</ref> presents the per pixel root mean square error (RMSE) for the UIUC dataset, calculated in LAB color space <ref type="bibr" target="#b12">[13]</ref>. The first row gives the actual error between the same image, with and without shadow. The difference between the two versions of the same image is calculated for both the shadow and the lit regions. Note that the error is large for the shadowed region (as expected), but it is not zero for the lit regions for two reasons: the shadow masks are not perfect and there is a little difference in the light intensity due to the change in the ambient light for the lit regions when  the object casting shadow is present. We achieved an average RMSE error of 6.8 compared to 7.4 and 12.6 achieved by the methods of Guo et al. <ref type="bibr" target="#b12">[13]</ref> and Wu et al. <ref type="bibr" target="#b33">[34]</ref>, respectively. Following Guo et al. <ref type="bibr" target="#b12">[13]</ref>, we also include the removal performance when the ground truth (GT) shadow masks are used for removal. This gives a more precise estimate of the performance of the recovery algorithm. When we evaluated our method using GT masks, our method achieved an error of 6.1 compared to 6.4 and 9.7 reported by <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b33">[34]</ref> respectively. We also tested the removal results without the Bayesian optimization, which resulted in an RMSE error of 7.9. This is high compared to the results achieved after optimization. In summary, our method achieved a reduction in error of 8.1% (removal using the detected masks) and 4.6% (removal using ground truths) compared to the approach of Guo et al. in <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Qualitative Evaluation</head><p>For the qualitative evaluation, we show some example images and their corresponding recovered images along with the shadow masks in Fig. <ref type="figure" target="#fig_17">12</ref>. It can be seen that our method works well under different settings e.g., outdoor images (first five images from the left) and indoor images (first two images from the right). The complex texture in the shadow regions is preserved and the arbitrary shadow matte are precisely recovered. Note that while Fig. <ref type="figure" target="#fig_2">13</ref>: Comparison with Automatic/Semi-Automatic Methods: Recovered shadow-less images are compared with the state-ofthe-art shadow removal methods which are either automatic <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23]</ref> or require minimal user input <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b34">35]</ref>. We compare our work with: (from left to right) Finlayson et al. <ref type="bibr" target="#b22">[23]</ref>, Shor and Lischinski <ref type="bibr" target="#b20">[21]</ref>, Xiao et al. <ref type="bibr" target="#b34">[35]</ref> and Guo et al. <ref type="bibr" target="#b12">[13]</ref> respectively. The results achieved using our method (second column from right) are comparable or better than the previous best results (columns 1-5 from left). Additionally, our method works without any user input and provides shadow matte (last column) which can be used to generate composite images. (Best viewed in color and enlarged)</p><p>our method can remove hard and smooth shadows (e.g., 1 st , 5 th and 6 th image from left), it also works well for the soft and variable shadows (e.g., 2 nd , 3 rd and 4 th image from left). Overall, the results are visually pleasing and the extracted shadow matte are smooth and accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Comparisons</head><p>We provide a qualitative comparison with two distinct categories of shadow removal methods. First, we show comparisons (see Fig. <ref type="figure" target="#fig_2">13</ref>) with the state-of-theart shadow removal methods which are either fully automatic (e.g., <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23]</ref>) or require minimal user input (e.g., <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b34">35]</ref>). From left to right we show the original image along with the results from Finlayson et al. <ref type="bibr" target="#b22">[23]</ref>, Shor and Lischinski <ref type="bibr" target="#b20">[21]</ref>, Xiao et al. <ref type="bibr" target="#b34">[35]</ref>, Guo et al.</p><p>[13] and our technique. In comparison to the previous automatic and semi-automatic (requiring minimal user input) methods, our approach produces cleaner recovered images (second column from the right) along with an accurate shadow matte (right most column). Since, there are only very few automatic shadow removal methods in the literature, we also compare our approach with the most popular approaches but which require user input (see Fig. <ref type="figure" target="#fig_18">14</ref>). From left to right, we show our recovered images (bottom row) along with the results from Wu et al. <ref type="bibr" target="#b33">[34]</ref>, Liu and Gleicher <ref type="bibr" target="#b26">[27]</ref>, Arbel and Hel-Or <ref type="bibr" target="#b28">[29]</ref>, Vicente and Samaras <ref type="bibr" target="#b49">[50]</ref>, Fredembach and Finlayson <ref type="bibr" target="#b25">[26]</ref> and Kwatra et al. <ref type="bibr" target="#b29">[30]</ref>. For the 'puzzled child' image, it can be seen that the contrast of the recovered region is much better than the one recovered by Wu et al. <ref type="bibr" target="#b33">[34]</ref>. The shadow-less image has no trace of strong shadow boundaries and the recovery in the penumbra region is smooth due to introduction of α in the model and the exclusion of the spatial affinity term <ref type="bibr" target="#b33">[34]</ref> or boundary nullification <ref type="bibr" target="#b32">[33]</ref> during the rough shadowless image estimation process. Similar effects can be seen with the other images; e.g., in 3 rd image from the left, the result of Arbel and Hel-Or <ref type="bibr" target="#b28">[29]</ref> has a high contrast while our result is smooth and successfully retains texture. Similarly, for the case of the 4 th , 5 th and 6 th images from the left, our shadow removal result is visually pleasing and considerably better than the recent state-of-the-art methods. Note however that the recovery result of the 2 nd image from the left has an over-smoothing effect, probably because the color distributions of differently colored shadowed regions could not be separated during the Gaussian fitting process. Overall, the results are quite reasonable considering that the algorithm does not require any user assistance and it does not make any prior assumptions such as a Planckian light source or a narrow-band camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4">Failure Cases and Limitations</head><p>Our shadow removal technique does not perform well on curved surfaces and in the case of highly non-uniform shadows (e.g., Fig. <ref type="figure" target="#fig_19">15:</ref> 1 st and 3 rd image from left). Since, we apply a multi-level color transfer scheme, very fine texture details of image regions with similar appearance can be removed during this transfer process (e.g., Fig. <ref type="figure" target="#fig_19">15:</ref> 2 nd image from left). For the cases of shadows in dark environments, our method appears to increase the contrast of the recovered region. These limitations are due to the constraints imposed on the shadow generation model, where the higher order statistics are ignored during the shadow generation process (Eqs. 19 and 21).  <ref type="bibr" target="#b25">[26]</ref> and Kwatra et al. <ref type="bibr" target="#b29">[30]</ref> respectively. The results achieved by our method (last row) are comparable or better than the previous best results (second row). Additionally, our method works without any user input and provides shadow matte (third row) which can be used to generate composite images. (Best viewed in color and enlarged)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.5">Discussion</head><p>Our method does not require any user input and it automatically removes shadow after its detection. The proposed shadow removal approach makes comparatively fewer assumptions about the scene type, the type of light source or camera. The only assumptions are that of Lambertian surfaces and the correspondence between the shadow and the non-shadow region color distributions. The shadow removal method of <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> cannot separate the shadow from shading. With the inclusion of the image consistency term in P (I s |α, β), we are able to deal with the shading by introducing a penalty on the distribution of the shadow effect through the parameters β and α. The proposed shadow removal approach takes 82.2 ± 25 sec for each image on the UIUC database. The main overhead during the shadow removal process is the Bayesian refinement step (which is required mainly for shadow matting). It takes 73.6±20 sec out of 82.2±25 sec per image on the UIUC database. In comparison, the method by Guo et al. <ref type="bibr" target="#b12">[13]</ref> takes 104.7±18 sec for shadow removal. The main overhead in their removal process is also due to Levin et al.'s matting algorithm <ref type="bibr" target="#b50">[51]</ref> which takes around 91.4 ± 11 sec per image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Applications</head><p>Shadow detection, removal and matting have a number of applications. A direct application is the generation ages, we combine extracted shadows with the original images to create fake effects.</p><p>Image Editing: Fig. <ref type="figure" target="#fig_5">16</ref>(b) shows how a detected shadow can be edited to create fake effects. For example, shadow direction/length can be modified to give a fake impression of illumination source or time of day.</p><p>Image Parsing: Fig. <ref type="figure" target="#fig_5">16</ref>(c) shows how shadow removal can increase the accuracy of segmentation methods (e.g., <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref>). The segmentations are computed using the graph based technique of <ref type="bibr" target="#b45">[46]</ref> (we used a minimum region size of 600). It can be seen that shadows change the appearance of a class (e.g., ground in this case) and thus can introduce errors in the segmentation process.</p><p>Boundary Detection: We tested a recently proposed boundary detector <ref type="bibr" target="#b53">[54]</ref> on the original and recovered image (Fig. <ref type="figure" target="#fig_5">16(d)</ref>). The boundaries identified in the recovered image are more accurate. Since shadows do not constitute an object class, the recovered image can help in achieving more accurate object detection proposals and consequently a higher recognition performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We presented a data-driven approach to learn the most relevant features for the detection of shadows from a single image. We demonstrated that our framework performs the best on a number of databases regardless of the shape of objects casting shadows, the environment and the type of scene. We also proposed a shadow removal framework which extracts the shadow matte along with the recovered image. A Bayesian formulation constitutes the basis of our shadow removal procedure and thereby makes use of an improved shadow generation model. Our shadow detection results show that a combination of boundary and region ConvNets incorporated in the CRF model provides the best performance. For shadow removal, the multi-level color transfer followed by the Bayesian refinement performs well on unconstrained images. The proposed framework has a number of applications including image editing and enhancement tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: From left to right: Original image (a). Our framework first detects shadows (c) using the learned features along the boundaries (top image in (b)) and the regions (bottom image in (b)). It then extracts the shadow matte (e) and removes it to produce a shadow free image (d).</figDesc><graphic coords="2,48.96,53.14,514.08,112.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: The proposed shadow detection framework. (Best viewed in color)</figDesc><graphic coords="4,311.97,203.94,251.07,121.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: ConvNet Architecture used for Automatic Feature Learning to Detect Shadows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Detection of Object and Shadow Boundary: We use the gradient profile along the direction perpendicular to a boundary point (four sample profiles are plotted on the anti-diagonal of above figure) to separate the object-shadow boundary (shown in red in lower right image).</figDesc><graphic coords="5,311.97,186.30,251.06,158.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>cluster the boundary points according to the direction of their normals. This results in separate boundary segments which join to form the boundary contour around the shadow. Then, the boundary segments in the shadow contour with a minimum relative change in intensity are classified to represent the object-shadow boundary. If c b denotes the mean intensity change along the normal direction at a boundary segment b of the shadow contour c, all boundary segments s.t. c</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Detection of Umbra and Penumbra Regions: With the detected shadow map (2 nd image from left), we estimate the umbra and penumbra regions (rightmost image) by analyzing the gradient profile (4 th image from left) at the boundary points.</figDesc><graphic coords="6,48.96,53.14,514.08,73.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Algorithm 4 . 1 :</head><label>41</label><figDesc>ROUGHESTIMATION(S, N)    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>K</head><label></label><figDesc>k=1 ω k S p k S (x, y), where ω k S is the weight of Gaussians (learned by the EM algorithm) and p k S (x, y) = |D k N |/(|D k S | + |D k N |). The color transfer is modified as: C (x, y) = (1 -p S (x, y))I S (x, y) + p S (x, y)C * (x, y) (13)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Shadow-less Patch with Multi-level Color Transfer (Sec. 4.1, Eq. 14)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Multi-level Color Transfer: (from left to right) (i) Two example images (a and b), with selected shadow regions. (ii) The recovered shadow-less patch using the technique of Wu et al. [33]. To highlight the difference with the original patch, we also show the difference image in color. (iii) The result of the local color transfer and its difference with the original patch. (iv) The result of the multi-level color transfer. Note that the multi-level transfer removes noise and preserves the local texture.</figDesc><graphic coords="7,299.80,60.23,199.97,258.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Shadow Removal Steps: (from left to right) (i) An original image with shadow. (ii) An initial estimate of the shadow-less image using a multi-level color transfer strategy. (iii) Improved estimate along the boundaries using in-painting. (iv, v and vi) The Bayesian formulation is optimized to solve for α (iv) and β matte (vi) and the final shadow-less image (v).</figDesc><graphic coords="8,48.96,53.14,514.10,88.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>P</head><label></label><figDesc>(U, P, N| α, β) + P (α) + P (β) -P (U, P, N)<ref type="bibr" target="#b23">(24)</ref> where, P = log P(•) is the log likelihood and U, P and N represent the umbra, penumbra and non-shadow regions respectively. The last term in the above equation can be neglected during optimization because it is independent of the model parameters. Therefore: = argmax α,β P (U, P, N| α, β) + P (α) + P (β)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>2 I</head><label>2</label><figDesc>y)η(x, y)|I(x, y) -Î(x, y)| 2 2σ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Examples of our results; Images (1 st , 3 rd row) and shadow masks (2 nd , 4 th row); Shadows are in white.</figDesc><graphic coords="11,48.96,53.14,514.09,193.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: ROC curve comparisons of proposed framework with previous works.</figDesc><graphic coords="11,311.97,397.35,251.06,83.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc><ref type="bibr" target="#b9">[10]</ref> extracted different shadow variant and invariant features alongside an additional 40 classification results from the Boosted Decision Tree (BDT) for each pixel as their features. Their approach required a huge amount of memory (∼9GB for 125 training images of average size of approximately 480×320). Even after parallelization and training on multiple processors, they reported 10 hours of training with 125 images. Lalonde et al.<ref type="bibr" target="#b10">[11]</ref> used 48 dimensional feature vectors extracted at each pixel and fed these to a boosted decision tree in a similar manner as Zhu et al.<ref type="bibr" target="#b9">[10]</ref>. Jiang et al. included illumination features on top of the features that are used by Lalonde et al.<ref type="bibr" target="#b10">[11]</ref>. Although, enriching the feature set in this manner improves the performance, it not only takes much more effort to design such features but it also slows down the detection procedure. In contrast, our feature learning procedure is fully automatic and requires only ∼1GB memory and approximately one hour training for each of the UCF, CMU and UIUC databases. The proposed approach is also efficient at test time because the ConvNet feature extraction and unary potential computation take an average of 1.3±0.35 sec per image on the UCF, CMU and UIUC databases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 11 :</head><label>11</label><figDesc>Fig. 11: Examples of Ambiguous Cases: (From left to right) Our framework misclassified a dark non-shadow region, textureless black window glass, very thin shadow region and trees due to complex self shading patterns. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 12 :</head><label>12</label><figDesc>Fig. 12: Qualitative Evaluation: Shadow recovery on sample images from UIUC, UCF databases and other images used in literature. Given a original image with shadow mask (first row), our method is able to extract exact shadows (second row) and to automatically recover the shadow-less images (third row). (Best viewed in color)</figDesc><graphic coords="12,48.96,53.14,514.05,162.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 14 :</head><label>14</label><figDesc>Fig.14: Comparison with Methods Requiring User Interaction: Recovered shadow-less images are compared with the state-ofthe-art shadow removal methods (which require considerable amount of user input). We compare our work with: (from left to right in the second row) Wu et al.<ref type="bibr" target="#b33">[34]</ref>, Liu and Gleicher<ref type="bibr" target="#b26">[27]</ref>, Arbel and Hel-Or<ref type="bibr" target="#b28">[29]</ref>, Vicente and Samaras<ref type="bibr" target="#b49">[50]</ref>, Fredembach and Finlayson<ref type="bibr" target="#b25">[26]</ref> and Kwatra et al.<ref type="bibr" target="#b29">[30]</ref> respectively. The results achieved by our method (last row) are comparable or better than the previous best results (second row). Additionally, our method works without any user input and provides shadow matte (third row) which can be used to generate composite images. (Best viewed in color and enlarged)</figDesc><graphic coords="14,48.96,53.14,514.10,275.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 15 :</head><label>15</label><figDesc>Fig. 15: Examples of Failure Cases: Our technique does not perfectly remove shadows on curved surfaces, highly nonuniform shadows and shadows in dark environments. (Best viewed in color and enlarged)</figDesc><graphic coords="14,311.97,409.52,251.05,147.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="13,48.96,53.14,514.06,212.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>y} ∈ U</figDesc><table><row><cell>then</cell><cell>Approximate β  *  using Eq. 30 Approximate α  *  using Eq. 32</cell></row><row><cell cols="2">else if {x, y} ∈ P</cell></row><row><cell>then</cell><cell>Approximate β  *  using Eq. 31 Approximate α  *  using Eq. 33</cell></row><row><cell>δ ← α</cell><cell></cell></row></table><note><p>* -α + β * -β return (α, β)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 1 :</head><label>1</label><figDesc>Evaluation of the proposed shadow detection scheme; All performances are reported in terms of pixel-wise accuracies.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 2 :</head><label>2</label><figDesc>Class-wise accuracies of our proposed framework in comparison with the state-of-the-art techniques. Our approach gives the highest accuracy for the class 'shadows'.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Tested on</cell><cell>UCF</cell><cell cols="2">Trained on CMU UIUC</cell></row><row><cell>UCF</cell><cell>-</cell><cell cols="2">80.3% 80.5%</cell></row><row><cell>CMU</cell><cell>77.7%</cell><cell>-</cell><cell>76.8%</cell></row><row><cell>UIUC</cell><cell cols="2">82.8% 81.5%</cell><cell>-</cell></row></table><note><p>Results when ConvNets were trained and tested across different datasets.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 4 :</head><label>4</label><figDesc>Quantitative Evaluation: RMSE per pixel for the UIUC Subset of Images. (The smaller RMSE the better)</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>JOURNAL OF L A T E X CLASS FILES, VOL. 6, NO. 1, JULY 2015</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank the anonymous reviewers for their valuable comments and suggestions to improve the quality of the paper. This research was supported by the IPRS awarded by The University of Western Australia and the Australian Research Council (ARC) grants DP110102166, DP150100294 and DE120102960. The Flickr images are used under a Creative Commons license from Flickr users: moun-taintrekker2001, Ulrich J and Kanshian.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Da</forename><surname>Vinci</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Notebooks. Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Illumination normalization with time-dependent intrinsic images for video surveillance</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nishino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ikeuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sakauchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1336" to="1347" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attached shadow coding: estimating surface normals from shadows under unknown reflectance and lighting conditions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1693" to="1700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Illusory motion from shadows</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kersten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Knill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mamassian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">B</forename><surname>Ülthoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">379</biblScope>
			<biblScope unit="issue">6560</biblScope>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Illumination from shadows</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ikeuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="290" to="300" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detecting moving shadows: Algorithms and evaluation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Prati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mikic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="918" to="923" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep reconstruction models for image set classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="713" to="727" />
			<date type="published" when="2015-04">April 2015</date>
		</imprint>
	</monogr>
	<note>Pattern Analysis and Machine Intelligence</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Receptive fields, binocular interaction and functional architecture in the cat&apos;s visual cortex</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Wiesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Physiology</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">106</biblScope>
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic feature learning for robust shadow detection</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to recognize shadows in monochromatic natural images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Masood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Tappen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="223" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Detecting ground shadows in outdoor consumer photographs</title>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Lalonde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="322" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Shadow detection based on colour segmentation and estimated illumination</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Schofield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Wyatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Paired regions for shadow detection and removal</title>
		<author>
			<persName><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cast shadow segmentation using invariant color features</title>
		<author>
			<persName><forename type="first">E</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="238" to="259" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Estimating shadows with the bright channel cue</title>
		<author>
			<persName><forename type="first">R</forename><surname>Panagopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CRICV (with ECCV)</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Describing reflectances for color segmentation robust to shadows, highlights, and textures</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Baldrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="917" to="930" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Illumination estimation and cast shadow detection through a higher-order graphical model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Panagopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="673" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Detecting illumination in images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Finlayson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fredembach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Drew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to detect moving shadows in dynamic environments</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Papanikolopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2055" to="2063" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Detection and removal of chromatic moving shadows in surveillance scenarios</title>
		<author>
			<persName><forename type="first">I</forename><surname>Huerta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Holte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1499" to="1506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The shadow meets the mask: Pyramid-based shadow removal</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="577" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">User-assisted intrinsic images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bousseau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2009">2009</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the removal of shadows from images</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Finlayson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Hordley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Drew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="68" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Removing shadows from images</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Finlayson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Hordley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Drew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2353</biblScope>
			<biblScope unit="page" from="823" to="836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Entropy minimization for shadow removal</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Finlayson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="57" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hamiltonian path based shadow removal</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fredembach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Finlayson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="502" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Texture-consistent shadow removal</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="437" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Editing soft shadows in a digital photograph</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tumblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Choudhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="23" to="31" />
			<date type="published" when="2007">2007</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Shadow removal using intensity surfaces and texture anchor points</title>
		<author>
			<persName><forename type="first">E</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hel-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1202" to="1216" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Shadow removal for aerial imagery by information theoretic intrinsic image analysis</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCP</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2012">2012</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Shadow removal using bilateral filtering</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="4361" to="4368" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Color transfer between images</title>
		<author>
			<persName><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Adhikhmin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gooch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shirley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics and Applications, IEEE</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="34" to="41" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A bayesian approach for shadow extraction from a single image</title>
		<author>
			<persName><forename type="first">T.-P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="480" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Natural shadow matting</title>
		<author>
			<persName><forename type="first">T.-P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOG</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fast shadow removal using adaptive multi-scale illumination transfer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>She</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="207" to="218" />
			<date type="published" when="2013">2013</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A bayesian approach to digital matting</title>
		<author>
			<persName><forename type="first">Y.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">264</biblScope>
			<date type="published" when="2001">2001</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">User assisted separation of reflections from a single image using a sparsity prior</title>
		<author>
			<persName><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1647" to="1654" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A three-stage approach to shadow field estimation from partial boundary information</title>
		<author>
			<persName><forename type="first">Y.-F</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2749" to="2760" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Slic superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Smote: synthetic minority over-sampling technique</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of AI Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efficient backprop</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Mller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade, ser. LNCS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7700</biblScope>
			<biblScope unit="page" from="9" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning crfs using graph cuts</title>
		<author>
			<persName><forename type="first">M</forename><surname>Szummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="582" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graph cuts and efficient nd image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Funka-Lea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="109" to="131" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Efficient graph-based image segmentation</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="181" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Region filling and object removal by exemplar-based image inpainting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1200" to="1212" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Recovering intrinsic scene characteristics</title>
		<author>
			<persName><forename type="first">H</forename><surname>Barrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978">1978</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note>Computer vision systems</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Poisson image editing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gangnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="313" to="318" />
			<date type="published" when="2003">2003</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Single image shadow removal via neighbor based region relighting</title>
		<author>
			<persName><forename type="first">T</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CPCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>with ECCV</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A closed-form solution to natural image matting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="242" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Geometry driven semantic labeling of indoor scenes</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Togneri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="679" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Separating objects and clutter in indoor scenes</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Togneri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CPPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Structured forests for fast edge detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
