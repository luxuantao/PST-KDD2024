<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast and Robust Short Video Clip Search Using an Index Structure</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
							<email>jyuan@i2r.a-star.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Infocomm Research</orgName>
								<address>
									<addrLine>21 Heng Mui Keng Terrace</addrLine>
									<postCode>119613</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Dept. of ECE</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Infocomm Research</orgName>
								<address>
									<addrLine>21 Heng Mui Keng Terrace</addrLine>
									<postCode>119613</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<email>tian@i2r.a-star.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Infocomm Research</orgName>
								<address>
									<addrLine>21 Heng Mui Keng Terrace</addrLine>
									<postCode>119613</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Infocomm Research</orgName>
								<address>
									<addrLine>21 Heng Mui Keng Terrace</addrLine>
									<postCode>119613</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fast and Robust Short Video Clip Search Using an Index Structure</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">95DCAAC86ECF879750BA7F7D483BFB35</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Search process. I.4.7 [Image Processing and Computer Vision]: Feature Measurement-Feature representation Algorithms</term>
					<term>Design</term>
					<term>Experimentation Video Similarity Search</term>
					<term>Video Content Identification</term>
					<term>Fast Query</term>
					<term>Spatial-Temporal Feature</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present an index structure-based method to fast and robustly search short video clips in large video collections. First we temporally segment a given long video stream into overlapped matching windows, then map extracted features from the windows into points in a high dimensional feature space, and construct index structures for these feature points for querying process. Different from linear-scan similarity matching methods, querying process can be accelerated by spatial pruning brought by an index structure. A multi-resolution kd-tree (mrkd-tree) is employed to complete exact K-NN Query and range query with the aim of fast and precisely searching out all short video segments having the same contents as the query. In terms of feature representation, rather than selecting representative key frames, we develop a set of spatial-temporal features in order to globally capture the pattern of a short video clip (e.g. a commercial clip, a lead in/out clip) and combine it with the color range feature to form video signatures. Our experiments have shown the efficiency and effectiveness of the proposed method that the very first instance of a given 10-sec query clip can be identified from a 10.5hour video collection in tens of milliseconds. The proposed method has been also compared with the fast sequential search algorithm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Content based video search and retrieval has attracted high research interests. According to different user requirements, we may have different kinds of video search intentions including the detection of duplicate/near-duplicate entry of a given clip, the identification of recurrent instances of a given clip, and similar video content search in a sense of coarse or clear semantic meanings. Application comprises video copyright management <ref type="bibr">[6]</ref>  <ref type="bibr" target="#b13">[13]</ref>, video content identification <ref type="bibr" target="#b1">[1]</ref> [6] <ref type="bibr" target="#b9">[9]</ref> [12] <ref type="bibr" target="#b15">[15]</ref> and content-based video retrieval <ref type="bibr" target="#b2">[2]</ref> [3] <ref type="bibr" target="#b4">[4]</ref>  <ref type="bibr" target="#b14">[14]</ref>.</p><p>For content-based video retrieval and identification, most existing search methods are derived from the sequential correlation matching widely used in the signal processing domain. Those sequential matching methods usually focus much more on feature extraction and a similarity matching function in order to search desired video content. The search efficiency is ignored to some extent. Although there exist some techniques to improve the linear scanning speed, such as temporal pruning <ref type="bibr" target="#b1">[1]</ref> or coarser granularity searching <ref type="bibr" target="#b12">[12]</ref>, their searching time-complexity still remains at least linear to the size of database as it usually requires exhaustively seeking out the whole data. In this paper, we present a fast and robust short video clip searching scheme using an index structure. This is motivated by the flexible and efficient query capabilities brought by fruitful research in high-dimensional index structures <ref type="bibr" target="#b5">[5]</ref>. We will employ an mrkd-tree index structure <ref type="bibr" target="#b10">[10]</ref> and apply different query strategies to short video clip search in this paper. A comparison will be performed between an index-based work and the temporal pruning-based active searching <ref type="bibr" target="#b1">[1]</ref>.</p><p>From the database point of view, video search can be treated as a querying problem <ref type="bibr" target="#b5">[5]</ref> [7] <ref type="bibr" target="#b8">[8]</ref>. That is, the essential purpose of video search is to find out those contents that are the most similar to the query, namely the "neighbors" of a query point in the feature space. Different query strategies can be applied to fulfill different applications <ref type="bibr" target="#b5">[5]</ref>. For example, K-NN Query can retrieve nearby points by their similarity ranking and then it is suitable for content based retrieval task. In contrast, Range Query is able to find all individual points within a certain distance from a query and then it can be used for duplicate/near-duplicate detection or content identification tasks. Such query strategies have ever been successfully applied for fast content-based image retrieval. Nevertheless, different from an image database, a video database contains large amounts of image sequences exhibiting a temporal order and a high redundancy. Indexing each image frame is thus ineffective and practically impossible. In our approach, we chop the long video stream into a series of overlapped video segments of a fixed size (we call them as "3D volume elements"), and build an index structure for these volume elements instead of individual frames. Compared with traditional key-frames based representation, our approach can avoid the shot segmentation and key frames selection. This is useful for identifying video content containing ambiguous shot boundaries (such as dynamic commercials and TV program lead-in &amp; out clip <ref type="bibr" target="#b12">[12]</ref>) in a long video stream. In order to robustly represent video segments, we introduce a set of spatial-temporal features as well as the color range features to construct a video signature. The obtained signatures tend to depict the video segment globally rather than focusing on its sequential details as key-frames based representation.</p><p>In consideration of the successful story of mrkd-tree in terms of searching high dimensional space <ref type="bibr" target="#b10">[10]</ref>  <ref type="bibr" target="#b11">[11]</ref>, we employ it to construct the index structure of the chopped segments. As mrkd-tree supports different query strategies such as point query, range query and k-NN query <ref type="bibr" target="#b5">[5]</ref>, it is suitable to various applications such as video copy detection, video content identification and retrieval. There have been some index based video similarity searching schemes, such as VQ index <ref type="bibr" target="#b7">[7]</ref> and hash index <ref type="bibr" target="#b8">[8]</ref>. However, their purpose is for approximation K-NN search where missed detection is probabilistically unavoidable. Our mrkd tree index emphasizes the exact K-NN query that is able to search out all the closest points of the given query, namely the K most similar video segments without any missed detection and false alarm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">OVERVIEW</head><p>Figure <ref type="figure" target="#fig_0">1</ref> illustrates our video similarity search scheme based on mrkd-tree structure. An index structure of the video database can be built off-line before querying process. Compared with traditional video similarity search method, our approach has the following features: • In contrast to fast sequential search scheme applying temporal pruning <ref type="bibr" target="#b1">[1]</ref>, searching based on the index structure employs spatial pruning to accelerate the speed. The prior query results are always the most similar contents. A user can decide whether to do further searching according to the available results. Exhaustive seeking on the whole database is thus avoided.</p><p>• Instead of using representative images to characterize video content, we employ spatio-temporal feature and cumulative color histogram as a compact and robust signature to describe video content. Complex training phase (e.g. Vector Quantization employed in <ref type="bibr" target="#b1">[1]</ref>) is unnecessary for extracting these signatures. Good performance has been proved in the experiment.</p><p>• As the index of video database is built for those temporally overlapped video segments with a predefined length, this scheme is also applicable to search for a sub-shot, a shot, and a series of shots. And by using different query strategies (point, K-NN, and range query), our method has provided a more general framework to fulfill different user search requirements. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Scheme</head><formula xml:id="formula_0">) 1 ( )} , ( ) , ( : 1 , } { \ { ) , , ( 1 1 Q w D Q w D K i i w w DB w DB w w K Q DB KNNQuery P Pi PK P P PK P ′ &gt; ≤ ≤ ¬∃ ∧ ∈ ′ ¬∃ ∈ = L K</formula><p>Accordingly, range query is defined as:</p><formula xml:id="formula_1">} ) , ( { ) , ,<label>( ε ε ≤</label></formula><formula xml:id="formula_2">∈ = Q w D DB w Q DB RangeQuery p p (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Signature Extraction of Video Segments</head><p>As one of the common visual features, color histogram is extensively used in video retrieval and identification <ref type="bibr">[6]</ref>  <ref type="bibr" target="#b9">[9]</ref>. <ref type="bibr" target="#b9">[9]</ref> applies compressed domain color features to form compact signature for fast video search. In <ref type="bibr">[6]</ref>, each individual frame is represented by four 178-bin color histograms on the HSV color space. And spatial information is incorporated by partitioning the image into four quadrants. Despite certain levels of success in <ref type="bibr">[6]</ref> and <ref type="bibr" target="#b9">[9]</ref>, the drawback is also obvious, for example, color histogram is fragile to color distortion problems and it is inefficient to describe each individual key frame using a color histogram as in <ref type="bibr">[6]</ref>.</p><p>In consideration of the above features, Ferman et al. <ref type="bibr" target="#b4">[4]</ref> presents various histogram-based color descriptors to reliably capture the color properties of video segments. Although such descriptors are reported to be robust and invulnerable to outlier frames within the shot, only color range information is considered, while both spatial information within each individual frame and temporal information are ignored. However our experiments show that spatial and color range information are both important for identification task.</p><p>Another type of feature which is robust to color distortion is ordinal feature proposed in <ref type="bibr" target="#b16">[16]</ref>. Hampapur et al. <ref type="bibr" target="#b13">[13]</ref> compared performance of using ordinal feature, motion feature and color feature respectively for video sequence matching. It was concluded that ordinal signature has the best performance. Based on this conclusion, we believe better performance could be achieved when combining ordinal feature and color range feature appropriately, with the former providing spatial information and the latter providing range information. Experiments in Section 4 will reveal these facts. As a matter of fact, many works such as <ref type="bibr" target="#b2">[2]</ref> and <ref type="bibr" target="#b15">[15]</ref> also incorporate the combined feature in order to improve the performance of retrieval and identification tasks.</p><p>Generally, the selection of the color feature and spatial-temporal feature as signature for identification task is motivated by the following reasons <ref type="bibr" target="#b19">[19]</ref>:</p><p>(1) Compared with cost features such as edge, texture or refined color histogram, such as CCV (color coherent vector) <ref type="bibr" target="#b17">[17]</ref> which also contain spatial information, they are inexpensive to acquire and could be estimated from MPEG compressed domain directly.</p><p>(2) Different from compact signature proposed in <ref type="bibr" target="#b8">[8]</ref>, these signatures still retain perceptual meaning. They are more robust to video formats variations.</p><p>(3) Ordinal feature is immune to global changes in the quality of the video and contain spatial information <ref type="bibr" target="#b13">[13]</ref>  <ref type="bibr" target="#b16">[16]</ref>, therefore it is a good complimentary to color feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Color Signature</head><p>We use the cumulative color histograms of all the sub-sampled I frames within a video segment as the color signature. For computational simplicity, the cumulative color distribution is estimated using the DC coefficients extracted from I frame in MPEG compressed video stream. The normalized cumulative histogram is:</p><formula xml:id="formula_3">B j j H M H M k k b b i i ccd , , 1 ) ( 1 1 L = = ∑ - + = (3) where 1 1 , , , - + + = M k k k i b b b i H L</formula><p>denotes the color histogram of corresponding I frame within the video sequence. M is the number of I frames and B is the color bin number. In this paper, B is selected as 24, uniform quantization; for each channel c = Y, Cb and Cr, H c ccd is thus a 24-dimensional feature vector. And the total size of the color signature H ccd is 72-dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Spatial -Temporal Signature</head><p>The common drawbacks of color histogram descriptor lie in the lack of spatial information and its sensitivity to color variations <ref type="bibr" target="#b4">[4]</ref>. In this section, we propose a spatial-temporal signature as compensation to the color signature. This signature contains both temporal and spatial information and is robust to color shifting, while remaining as compact as the color signature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2. Spatial-Temporal Signature Description</head><p>As illustrated in Figure <ref type="figure">2</ref>, each I frame is represented by a reduced image, of size 2 × 2. For each Y, Cb or Cr channel, we calculate the average value of each of the 4 sub-images by DC coefficients extracted from compressed domain directly. Raw feature extraction is then followed by the ordinal measure process. Each possible combination of ordinal measure result will be treated as an individual pattern. Therefore each I frame will be distributed a pattern code. All the patterns along the temporal axis are then accumulated to form a histogram. After the above operations, video segment can be compactly represented by 3 normalized 24dimensional ordinal pattern distribution histograms, corresponding to Y, Cb, and Cr channels respectively.</p><p>For each channel c = Y, Cb, Cr, the video clip is represented as:</p><formula xml:id="formula_4">∑ = ≤ ≤ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ = i i i NoP l opd c h and h h h h h H 1 1 0 ) , ,<label>, , , ( 2 1 (4)</label></formula><p>Here NoP = 4! = 24 is the dimension of the histogram, namely the number of possible patterns mentioned above. As a result, the total dimension of the spatial-temporal signature H opd also becomes 72, the same size as the color signature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Distance Metric</head><p>For both color signature and spatial-temporal signature, the distance of each Y, Cb and Cr channel is defined as Euclidean distance. The overall distance measure is defined as the linear combination of the average of spatial-temporal signature distance and the minimum of color signature distance among the Y, Cb, and Cr channels. And the overall similarity measure is defined as the reciprocal of distance value. Choosing such definition could further alleviate the affects of color shifting and other video variations. This is obtained by experimental comparison. In this paper, w is set to 0.5. We have the similarity definition between query and sliding matching widows as follows:</p><formula xml:id="formula_5">∑ = = Cr Cb Y c opd SW opd Q opd c opd SW opd Q opd I H H D H H D , , ) , (<label>3</label></formula><formula xml:id="formula_6">1 ) , (<label>(5)</label></formula><p>)} , ( { ) , ( , , An mrkd-tree is introduced in <ref type="bibr" target="#b10">[10]</ref> as an extension of kd-tree structure (Figure <ref type="figure" target="#fig_1">3</ref>). The decoration is, at each node of kd-tree, it adds extra statistics about the node's data, such as their centroid, covariance and count. With such mrkd-tree structure, it is feasible to query the database with the same flexibility as a conventional linear search at greatly reduced computational cost. Since mrkdtree structure adds sufficient statistics about all the data points below each node in the tree, it also has extended applications to speed up other operations such as locally weighted regression and mixture model based clustering and density estimation algorithms <ref type="bibr" target="#b11">[11]</ref>. Here we use it as an effective index structure to complete exact K-NN query and range query.</p><formula xml:id="formula_7">opd SW opd Q ccd c Cr Cb Y c ccd SW ccd Q ccd I H H D Min H H D = = (<label>6</label></formula><formula xml:id="formula_8">) ccd I I SW Q D w D w H H D opd × - + × = ) 1 ( ) , (<label>(7)</label></formula><p>By using an mrkd-tree structure for querying, at each node during the search, the algorithm considers whether it 1) can ignore all the points below that node because they are irrelevant to the current query, 2) can estimate the effects of all the points below that node without visiting them individually, or 3) must search further down the tree. The result is that most queries need only visit a small number of nodes in the tree to find out the nearest neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>In our experiments, we apply both the fast sequential search and the index structure-based searching to complete fast video clip querying. An mrkd-tree was employed as the index structure, which can support different types of query such as K-NN query and Range query. These queries can be associated with most existing applications such as content-based retrieval and duplicate/near duplicate detection, etc. On the other hand, we implement the fast sequential scan method of "active search" proposed in <ref type="bibr" target="#b1">[1]</ref> to compare it with an index structure-based query. Different from an index structure-based query, the active search requires seeking out the whole dataset to achieve the final results, no matter what query purpose it is. A performance comparison will be done in our experiments.  half-hour segments are combined into one video sequence with the total length of 10.5 hours. An mrkd tree index will be built for this 10.5-hour sequence after it is chopped into segments of 10 seconds with an interval of around 0.4 second, the temporal distance of two neighbored I frames. Different from <ref type="bibr" target="#b3">[3]</ref> which chops a long sequence into non-overlapped segments of 0.5 second, our chopped segments have an overlapped section of 9.6 seconds between the neighbored ones.</p><p>The query clip set contains a news lead-out clip in the ABC broadcast news video (Figure <ref type="figure">4</ref>) and 78 commercials. The lead-out clip is at the length of 10 second and has 11 full instances in total and 2 incomplete instances in the 10.5-hours video dataset. The length of 78 commercials ranges from 10 to 60 seconds, and have in total 202 instances. Most of the instances have a slight color shifting compared with the original query. Our task is to fast and robustly identify and locate these instances inside the long video stream. We want to mention that the identification and retrieval of such repeated non-news sections inside a video stream helps to reveal the video structure. These sections include TV commercials, program lead-in/lead-out and other Video Structure Elements (VSE) which appear very often in many types of video to indicate starting or ending points of a particular video program, for instance, news programs or replay of sports video.</p><p>We first chop a video stream into a series of overlapping segments of fixed size. Both color signature and spatial-temporal signature are then extracted for each segment. Figure <ref type="figure">4</ref> and Figure <ref type="figure">5</ref> illustrate extracted signatures. The two examples are a news lead-out clip and a news lead-in clip in the ABC broadcast news video. Both of them are 10-sec long. As shown in Figure <ref type="figure">4</ref> and Figure <ref type="figure">5</ref>, different from the accumulative color histogram representing the color density distribution, the proposed ordinal pattern distribution histogram (spatial-temporal signature) provides a unique sparse distribution, and thus it is more distinguishable than color range feature (color signature). For example, the lead-out (Figure <ref type="figure">4</ref>) and the lead-in (Figure <ref type="figure">5</ref>) clips have similar color signature because of similar dominant colors in the content; however, their spatial-temporal signatures are still quite distinguishable. In conclusion, as the ordinal pattern distribution is more unique and insensitive to a global color shifting or other color variations, this feature is a good supplementary to the color range feature towards a robust and composite feature set.</p><p>All the simulations were performed on a standard P4 @ 2.53G Hz PC (512 M memory). The algorithm was implemented in C++.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Query based on mrkd-tree Index</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Exact K-NN Query Results</head><p>We only consider the news lead-out clip (10 seconds) for K-NN query. The distance metric is given in equation <ref type="bibr" target="#b7">(7)</ref>. The CPU cost of the pre-processing for querying is listed in Table <ref type="table" target="#tab_2">1</ref>. Note that for this 10.5 hour MPEG1 video dataset, only less than 0.5 hour is spent for feature extraction and building index.</p><p>In Table <ref type="table" target="#tab_3">2</ref>, CPU cost of K-NN query is given with different K. By using the NPT package <ref type="bibr" target="#b10">[10]</ref>, merely tens of milliseconds is needed to search out the very first best matching with the built index. While for the sequential searching, only when the whole database has been exhaustively searched out can it present the best matched result. To some extent, our proposed scheme provides a more flexible method for accessing the database and the user can determine whether further search is necessary according to available results. Take an example of video copy detection in large video set; current linear search method usually needs to exhaustively search out the whole database in order to claim no copy is detected. Nevertheless, with an index structure discussed above, we can apply K-NN search to fast locate the contents that are the most possible examples of the query according to the defined similarity. If the best matched result is not "similar" enough to be a copy, further searching is unnecessary. Computation is therefore greatly saved.   Figure <ref type="figure" target="#fig_3">6</ref> depicts the results of K-NN query. Since the original long video sequence is chopped into overlapped segments with the length of 10-seconds, each point in Figure <ref type="figure" target="#fig_3">6</ref> corresponds to a matching window with the width of 10 seconds. Noted not each of the 100 retrieval points represents an individual instance. It is easy to understand that when one instance is found, its neighbors may also exhibit high similarity values due to the large overlap (9.6 sec/10 sec). Therefore only an area with clustered points can be claimed to be an instance from Figure <ref type="figure" target="#fig_3">6</ref>. In other words, the number of detected instance should be counted by the peaks (12 peaks shown in Figure <ref type="figure" target="#fig_3">6</ref>) instead of the number of retrieved points K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Range Query Results</head><p>The 78 commercials are considered for Range Query to search out all the 202 instances in the 10.5-hour dataset. In order to be compatible with the built mrkd-tree, the first 10-sec section of each commercial is cut out as the query clip with the same length as the matching window. By utilizing mrkd-tree, the range searching cost of n points is O(nlogn) when ε is small. The performance of range searching is presented in Figure <ref type="figure" target="#fig_4">7</ref>, by changing range ε adaptively.</p><p>In the experiment, we found that false alarms and missed detections are mainly caused by the I frame shifted matching problem, when the sub-sampled I frames of a given clip and that of the matching window are not well aligned at the temporal axis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Query based on Fast Sequential Search</head><p>In order to evaluate the performance of an index-structure based searching and justify our proposed new feature, we also implement the active search algorithm proposed in <ref type="bibr" target="#b1">[1]</ref> for query acceleration. Different from the feature set proposed in <ref type="bibr" target="#b1">[1]</ref>, the complex training phase such as vector quantization is not required for our signature extraction. Nevertheless, our proposed signature is still compatible with the active search algorithm. The active search process is briefly introduced as follows:</p><p>Let the similarity metric array be { }</p><formula xml:id="formula_9">1 1 ; - + ≤ ≤ n m i S i</formula><p>corresponding to m+n-1 sliding windows, where n and m are the total I frame number of given clip and target stream respectively. Based on <ref type="bibr" target="#b1">[1]</ref> and <ref type="bibr" target="#b18">[18]</ref>, the search process can be accelerated by skipping unnecessary w i steps.</p><formula xml:id="formula_10">⎪ ⎩ ⎪ ⎨ ⎧ &lt; + - = otherwise S if S N floor w i i i 1 1 1 )) 1 ( 2 ( θ θ<label>(9)</label></formula><p>where S i is the similarity value defined in equation ( <ref type="formula">8</ref>); and its reciprocal is actually the distance value defined in equation <ref type="bibr" target="#b7">(7)</ref>. N is the number of I frames of the corresponding matching window. Andθ is the predefined skip threshold.</p><p>After search, potential start position of the match will be determined by local maximum above the threshold, which fulfills the following conditions:</p><formula xml:id="formula_11">} , max{ 1 1 σ k m T S and S S S k k k k + &gt; ≥ ≤ + -<label>(10)</label></formula><p>where T is the pre-defined preliminary threshold, m is the mean and σ is the deviation of the similarity curve; k is an empirically determined constant. Only when similarity value exceeds the maximum value of T and σ k m + , it can be treated as the detected instance. In our experiment, w in eq. ( <ref type="formula" target="#formula_8">7</ref>) is set to 0.5, and θ in eq. ( <ref type="formula" target="#formula_10">9</ref>) is set to 0.05. T in eq. ( <ref type="formula" target="#formula_11">10</ref>) is 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 8. System chart of applying active search algorithm [1] with the proposed video signature</head><p>Figure <ref type="figure">8</ref> gives the system chart of applying our proposed signature to active search algorithm. Figure <ref type="figure" target="#fig_5">9</ref> presents the performance comparison when applying different features. It is clear that an appropriate combination of the color signature and the spatialtemporal signature can improve the performance against using them separately. Although we cannot achieve the accuracy of 100% with the proposed signatures, a comparable performance is still achieved with that in <ref type="bibr" target="#b12">[12]</ref>. But their signature's size is 15 times larger than our proposed one. The main advantage of active search is the fast speed. In our experiments, in order to search through the whole 10.5 hour video, the active searching on average costs 0.011 second when using the signatures extracted off-line <ref type="bibr" target="#b19">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION AND FUTURE WORK</head><p>In this paper, we deal with the video searching in large video collection as a sub-pattern matching problem and employ an index structure to accelerate querying process. A multi resolution kdtree (mrkd tree) is used to construct an index for the temporally overlapped matching windows. Compared with the fast sequential scan searching, the mrkd-tree data structure provides an efficient mechanism for examining only those points closest to the query, thereby greatly reducing the computation towards the best matches. From our results, we found both an index based search (using NN query) and the active search are very fast to search out the very first instance of the query (i.e. searching out the 10.5 hour video in tens of milliseconds with the features extracted off-line). However, when more instances are needed, the active search is expected to be more efficient than the index based search in terms of CPU cost. Unlike the key-frames based shot matching, our method is based on elementary video segment of fixed-size, which could be a sub-shot, a shot or several shots. Since the research in high-dimensional index structure has been very active and fruitful over the past few years, many existing index structures (such as mrkd-tree, etc.) can support different types of queries. These various query strategies (such as K-NN query, range query) can reasonably produce different applications such as retrieval, duplicate/near-duplicate detection while avoiding going though the whole dataset. Therefore this can fulfill different user requirements in a more general framework. In this paper, we only give simple applications of using exact K-NN query and range query to identify video content in the sequence. How to select appropriate K or ε for querying and how to use different query strategies to fulfill other applications such as video clip copy detection and content based retrieval will be our future work. Moreover, it might be expected to make the index structure accessible to queries with various lengths. More experiments will be done including the detection of dynamic commercial, flying logo detection etc. from more extensive video collections.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. System Diagram</figDesc><graphic coords="2,54.00,445.68,246.90,103.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Example of an mrkd-Tree. The dots are the individual data points. The sizes and positions of the disks show the node counts and centroids. The ellipses and rectangles show the covariance and bounding boxes [20]</figDesc><graphic coords="3,327.54,476.16,219.54,181.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Figure 4. ABC News program lead-out clip (Left, 10 seconds) and its color and spatial-temporal signature representation (Right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Searching results of exact K-NN query (K =100). Dataset consists of 10.5 hours MPEG-1 video sequence and the query length is 10-sec. The searching cost is 1.536 sec.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Searching results of Range query (Precision = detects /( detects + false alarms))(Recall = detects / (detects + miss detects))</figDesc><graphic coords="6,110.10,499.86,400.86,194.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Performance Comparison using different features: proposed feature vs. 3 × 720-d ordinal feature in [12] (Upper); proposed feature vs. 3 × 24-d cumulative color feature and 3 × 24-d ordinal feature respectively (Bottom); the detection curves are generated by varying the parameter k in eq. (10) (Precision = detects /( detects + false alarms)) (Recall = detects / (detects + miss detects))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>3.1 Problem Formulation</head><label></label><figDesc></figDesc><table><row><cell>The problems of exact K-NN query and range query are formu-</cell></row><row><cell>lated as follows. Let DB represent the video database containing</cell></row><row><cell>temporally chopped video segments W P , namely a series of match-</cell></row><row><cell>ing windows, where P is the temporal position of a segment</cell></row><row><cell>(matching window). Let Q denote the query clip with the same length as W P . ) , ( ⋅ ⋅ D and ) , ( ⋅ ⋅ S are the function for dissimilarity</cell></row><row><cell>and similarity measure between a query point and a matching window point in the feature space. Definition of ) , ( ⋅ ⋅ D and ) , ( ⋅ ⋅ S</cell></row><row><cell>are given in Section 3.3. Now we have exact K-NN query:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>4.1 Dataset Description and Feature Extrac- tion</head><label></label><figDesc>The video database contains 22 streams of half-hour broadcast ABC news video obtained from TRECVID news dataset, encoded in MPEG1 at 1.5 Mb/sec and at frame rate of 29.97 fps. All of the video data are compressed with the GoP pattern IBBPBBPBBPBB, with I frame resolution of around 400ms. Among the 22 TV news videos, 12 of them are with image size of 352 × 240 and the other 10 with image size of 352 × 264. After feature extraction, all the</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 . CPU time cost for compressed domain feature extrac- tion and mrkd tree index built (10.5 hours MPEG-1 video)</head><label>1</label><figDesc></figDesc><table><row><cell>Process</cell><cell>CPU Time Cost (sec)</cell><cell>Size</cell></row><row><cell>Feature Extraction</cell><cell>1178.034</cell><cell>89,507 K Byte</cell></row><row><cell>mrkd Tree Built</cell><cell>470.610</cell><cell>18,301 Nodes of tree</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 . Exact K-NN query results using mrkd-tree</head><label>2</label><figDesc></figDesc><table><row><cell>K-NN search /</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(feature space</cell><cell>K=</cell><cell>K=</cell><cell>K=</cell><cell>K=</cell><cell>K=</cell><cell>K=</cell><cell>K=</cell></row><row><cell>of 93,700</cell><cell>1</cell><cell>10</cell><cell>50</cell><cell>100</cell><cell>200</cell><cell>1000</cell><cell>5000</cell></row><row><cell>points)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Time Consum-ing (sec)</cell><cell>0.078</cell><cell>0.204</cell><cell>0.969</cell><cell>1.563</cell><cell>2.641</cell><cell>5.078</cell><cell>20.365</cell></row><row><cell>Number of</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Contained</cell><cell>1</cell><cell>2</cell><cell>7</cell><cell>11+1*</cell><cell>11+2*</cell><cell>11+2*</cell><cell>11+2*</cell></row><row><cell>Instances</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">* indicate the uncompleted instances</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Quick Search Method for Audio and Video Signals Based on Histogram Pruning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kashino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="348" to="357" />
			<date type="published" when="2003-09">Sep. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Query by video clip</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Multimedia System</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="369" to="384" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Video retrieval using spatio-temporal descriptors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dementhon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM MM&apos;03</title>
		<meeting>of ACM MM&apos;03</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="508" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust color histogram descriptors for video segment retrieval and identification</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Ferman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2002-05">May 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Searching in high-dimensional spaces -index structures for improving the performance of multimedia databases</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bohm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Computing Survey (CSUR)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="322" to="373" />
			<date type="published" when="2001-09">September 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient video similarity measurement with video signature</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zakhor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Trans. on Circuits and System for Video Technology</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="59" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">VQ-index: an index structure for similarity searching in multimedia databases</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tuncel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM MM&apos;02</title>
		<meeting>of ACM MM&apos;02<address><addrLine>Juan Les Pins, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-12">December 2002</date>
			<biblScope unit="page" from="543" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Feature extraction and a database strategy for video fingerprinting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Oostveen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual 2002</title>
		<title level="s">LNCS 2314</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Novel Scheme for Fast and Efficient Video Sequence Matching Using Compact Signatures</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Naphade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE, Storage and Retrieval for Media Databases 2000</title>
		<meeting>SPIE, Storage and Retrieval for Media Databases 2000</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">3972</biblScope>
			<biblScope unit="page" from="564" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-resolution instanced based learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
		<ptr target="http://www.autonlab.org/astro/npt/index.html" />
	</analytic>
	<monogr>
		<title level="m">Proc. of Int. Joint Conf. on Artificial Intelligence&apos;95</title>
		<meeting>of Int. Joint Conf. on Artificial Intelligence&apos;95<address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="1233" to="1239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Very fast mixture-model-based clustering using multi-resolution kd-trees</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing System 10</title>
		<imprint>
			<date type="published" when="1999-04">April 1999</date>
			<biblScope unit="page" from="543" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast and Robust Search Method for Short Video Clips from Large Video Collection</title>
		<author>
			<persName><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ranganath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICPR&apos;04</title>
		<meeting>of ICPR&apos;04<address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-08">August 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Comparison of Sequence Matching Techniques for Video Copy Detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hampapur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bolle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE. Storage and Retrieval for Media Databases</title>
		<meeting><address><addrLine>San Jose, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-01">2002. Jan. 2002</date>
			<biblScope unit="volume">4676</biblScope>
			<biblScope unit="page" from="194" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A match and tiling approach to content-based video retrieval</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICME&apos;01</title>
		<meeting>of ICME&apos;01</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="301" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Video clip recognition using joint audiovisual processing model</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kulesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICPR&apos;02</title>
		<meeting>of ICPR&apos;02</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="500" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ordinal measures for image correspondence</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Trans. on PAMI</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="415" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Comparing images using color coherence vectors</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM Multimedia&apos;96</title>
		<meeting>of ACM Multimedia&apos;96</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="65" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Quick Search Method for Multimedia Signals Using Feature Compression Based on Piecewise Linear Maps</title>
		<author>
			<persName><forename type="first">Akisato</forename><surname>Kimura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP&apos;02</title>
		<meeting>of ICASSP&apos;02</meeting>
		<imprint>
			<date type="published" when="2002-05">May 2002</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="3656" to="3659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast Video Segment Identification from Large Video Collection</title>
		<author>
			<persName><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">To appear in 2004 Pacific-Rim Conference on Multimedia (PCM&apos;04)</title>
		<meeting><address><addrLine>Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-11">Nov. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Nonparametric Density Estimation: Toward Computational Tractability</title>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM International Conference on Data Mining&apos;03</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
