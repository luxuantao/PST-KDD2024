<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3731E25B811075A2409AD73B542B3CCB</idno>
					<idno type="DOI">10.1109/MSP.2015.2481563</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T</head><p>his article presents a powerful algorithmic framework for big data optimization, called the block successive upper-bound minimization (BSUM). The BSUM includes as special cases many well-known methods for analyzing massive data sets, such as the block coordinate descent (BCD) method, the convex-concave procedure (CCCP) method, the block coordinate proximal gradient (BCPG) method, the nonnegative matrix factorization (NMF) method, the expectation maximization (EM) method, etc. In this article, various features and properties of the BSUM are discussed from the viewpoint of design flexibility, computational efficiency, parallel/ distributed implementation, and the required communication overhead. Illustrative examples from networking, signal processing, and machine learning are presented to demonstrate the practical performance of the BSUM framework. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>InTroducTIon</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview Of OptimizatiOn fOr big data</head><p>With advances in sensor, communication, and storage technologies, data acquisition is now more ubiquitous than at any other time. This has made available big data sets in many areas of engineering, biological, social, and physical sciences. While the proper modeling and analysis of such data sets can yield valuable information for inference, estimation, tracking, learning, and decision making, their size and complexity present great challenges in algorithm design and implementation. Due to its central role in big data analytics, large-scale optimization has recently attracted significant attention not only from the optimization community, but also from the machine-learning, statistics, and signal processing communities. For example, emerging problems in image processing, social network analysis, and computational biology can easily exceed millions or billions of variables, and significant research is underway to enable fast accurate solutions to these problems <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref>. Also, problems related to the design and provision of large-scale smart infrastructures such as wireless communication networks require real-time efficient resource allocation decisions to ensure optimal network performance. Traditional general-purpose optimization tools are inadequate for these problems due to the complexity of the model, the heterogeneity of the data, and, most importantly, the sheer data size <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b8">[9]</ref>. Modern large-scale optimization algorithms, especially those that are capable of exploiting problem structures; dealing with distributed, time-varying, and incomplete data sets; and utilizing massively parallel computing and storage infrastructures, have become the workhorse in the big data era.</p><p>To be efficient for big data applications, optimization algorithms must have certain properties:</p><p>1) Each of their computational steps must be simple and easy to perform.</p><p>2) The intermediate results are easily stored.</p><p>3) They can be implemented in a distributed and/or parallel manner so as to exploit the modern multicore and cluster computing architectures. 4) A high-quality solution can be found using a small number of iterations. These requirements preclude the use of high-order information about the problem (i.e., the Hessian matrix of the objective), which is usually too expensive to obtain, even for modest-sized problems. the bCd methOd A very popular family of optimization algorithms that satisfies most of the aforementioned properties is the BCD method, sometimes known as the alternating minimization/maximization (AM) algorithm. The basic steps of the BCD are simple: 1) partition the entire optimization variables into small blocks and 2) optimize one block variable (or few blocks of variables) at each iteration while holding the remaining variables fixed. More specifically, consider the following block-structured optimization problem ( , , , ), , , , ..., ,</p><formula xml:id="formula_0">f x x x x i n 1 2 minimize s.t. X n i i x 1 2 f ! =<label>(1)</label></formula><p>where ( ) f : is a continuous function (possibly nonconvex, nonsmooth), each R Xi </p><formula xml:id="formula_1">X i i ! ! - -<label>(2)</label></formula><p>where we have defined : ( , , , , , );</p><formula xml:id="formula_2">x x x x x i r r i r i r n r 1 1 1 1 1 1 1 1 g g = - - - - - + - -</formula><p>for the rest of the variables , j i ! let . x x j r j r 1</p><p>= -See Figure <ref type="figure" target="#fig_5">1</ref> for a graphical illustration of the algorithm.</p><p>The BCD algorithm is intuitively appealing and very simple to implement, yet it is extremely powerful. It enjoys tremendous popularity in a wide range of applications from signal processing communications to machine learning. Representative examples include image deblurring <ref type="bibr" target="#b9">[10]</ref>, statistical learning <ref type="bibr" target="#b10">[11]</ref>, wireless communications <ref type="bibr" target="#b11">[12]</ref>, etc. In recent years, there has been a surge of renewed interests to extend and generalize the BCD type of algorithms due to its applications in modern big data optimization problems. Theoretically, the BCD algorithm and its variants have been generalized significantly to accommodate various coordinate update rules <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> and have been made suitable for implementation on modern parallel processing architecture <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b18">[19]</ref>. It can handle a wide range of nonsmooth, nonconvex cost functions <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b21">[22]</ref>. Practically, it has been used in emerging large-scale signal processing and machine-learning applications, such as compressive sensing/sparse signal recovery <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, matrix completion <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, and tensor decomposition <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, to name just a few. A recent survey of this algorithm can be found in <ref type="bibr" target="#b28">[29]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>the bSUm methOd</head><p>In this article, we introduce a unifying framework, the BSUM method, which generalizes the BCD type of algorithms <ref type="bibr" target="#b20">[21]</ref>. Simply put, the BSUM framework includes algorithms that successively optimize certain upper bounds or surrogate functions of the original objectives, possibly in a block-by-block manner. The BSUM framework significantly expands the application domain of the traditional BCD algorithms. For example, it covers many classical statistics and machine-learning algorithms such as the EM method <ref type="bibr" target="#b29">[30]</ref>, the CCCP <ref type="bibr" target="#b30">[31]</ref>, and the multiplicative NMF <ref type="bibr" target="#b31">[32]</ref>. It also includes as special cases many well-known signal processing algorithms such as the family of interference pricing algorithms <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> and the weighted minimum mean square error algorithms <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b34">[35]</ref> for interference management in large-scale wireless systems.</p><p>The generality and flexibility of the BSUM offers an ideal platform to explore optimization algorithms for big data. Through the lens of the BSUM, one obtains not only a thorough understanding of the variety of algorithms and applications that are being covered, but more importantly the principle for designing a good algorithm suitable for big data. To this end, this article will first provide a concise overview of a few key theoretical characterizations of the algorithms that fall under the BSUM framework, BCD included. Our emphasis will be given on providing intuitive understanding as to when and where the BSUM framework should (or should not) work, and how its performance can be characterized. The second part of this article offers a detailed account of many existing largescale optimization algorithms that fall under the BSUM framework, together with a few big data related applications that are of significant interests to the signal processing community. The last part of the article outlines some interesting extensions of the BSUM that further help expand its application domains. Throughout this article, special emphasis will be given on computational issues arising from big data optimization problems such as algorithm design for parallel and distributed computation, algorithm implementation on multicore computing platforms, and distributed storage of the data. In particular, we will discuss how these issues can be addressed in the BSUM framework by providing theoretical insights and examples from real practical problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BSuM and ITS TheoreTIcal ProPerTIeS the main idea</head><p>We start by providing a high-level description of the BSUM method and some of its theoretical properties. In practice, one of the main problems of directly applying BCD to solve (1) is that each of its subproblems (2) is often difficult to solve exactly, especially when ( ) f x is nonconvex. Moreover, even such exact minimization can be performed, the BCD may not necessarily converge. One of the key insights to be offered by the BSUM framework is that, for both practical and theoretical considerations, obtaining an approximate solution of ( <ref type="formula" target="#formula_1">2</ref>) is good enough to keep the algorithm going. To be more specific, let us introduce ( , ): </p><formula xml:id="formula_3">u x z R X i i i "</formula><formula xml:id="formula_4">I I i r x i i r r k r k r r 1 1 X i i 6 6 " ! ! = ! - - )<label>(3)</label></formula><p>[TaBle <ref type="table">1</ref>] a PSeudocode oF The BSuM algorIThM.</p><p>1 Find a Feasible point x X 0 ! and set r 0 The complete description of the BSUM is given in Table <ref type="table">1</ref>. See Figure <ref type="figure" target="#fig_10">2</ref> for a graphical comparison of the iterates generated by BSUM and BCD for a two-dimensional problem. It should be clear at this point that when {(</p><formula xml:id="formula_5">= 2 rePeaT 3 pick index set I r 4 let ( , ), argmin x u x x i r x i i r 1 X i i ! ! - i I r 6 ! 5 set , x x k I k r k r r 1 6 " = - 6 , r r 1 = + 7 unTIl some convergence criterion is met x 2 x 2 x 1 x 1 Contour of f (x 1 , x 2 ) Contour of f (x 1 , x 2 ) Contour of u 1 (x 1 , x) Contour of u 2 (x 2 , x)</formula><formula xml:id="formula_6">) } mod r n 1 I r = + and no approxima- tion is used [i.e., ( , ) ( , )] u x z f x z i i i i = -</formula><p>and at each iteration a single coordinate is selected), then the BSUM reduces to the classical cyclic BCD method. In Table <ref type="table" target="#tab_0">2</ref>, we also present several index selection rules that are covered by the BSUM framework. For simplicity of presentation, we will use the classical cyclic index selection rule, where {( ) }, mod r n 1 I r = + in the remainder of this article unless otherwise noted.</p><p>Next, we introduce the precise definition of the approximation function. The main idea is that, for each , i the approximation ( , ) u x x i i r should be an upper bound of the original objective function at the point of x r (hence, the BSUM name of the framework); see Figure <ref type="figure" target="#fig_1">3</ref> for an illustration of the upper-bound minimization process. Intuitively, picking an upper-bound approximation function is reasonable because optimizing it at least should guarantee some descent of the original objective ;</p><p>f see Figure <ref type="figure" target="#fig_1">3</ref>(c).</p><p>To be more precise, let us first define the directional derivative of a given function ( ):</p><formula xml:id="formula_7">f x R X " at a point x X ! in direction : d ( ; ) ( ) ( ) . liminf f x d f x d f x 0 _ m m + - . m l</formula><p>Using this definition, we make the following assumptions on the uis.</p><p>Assumption A</p><formula xml:id="formula_8">( , ) ( ), , u x x f x x i X i i 6 6 ! = (A1) ( , ) ( , ), , , u x z f x z x z i X X i i i i i i 6 6 6 $ ! ! - (A2) ( , ; ) | (; ),</formula><p>( , , , , ) ,</p><formula xml:id="formula_9">u x z d f z d d d z d i 0 0s.t. X i i i x z i i i i i i 6 f f 6 ! = = + = l l (A3) ( , ) (, ), . u x z xz i is continuous in i i i 6 (A4)</formula><p>Intuitively, Assumptions (A1) and (A2) imply that the approximation function is a global upper bound of ( ); f x while the assumption (A3) guarantees that the first-order behaviors of the </p><formula xml:id="formula_10">x i r x i r x i r x i x i x i f(x i , x -i ) r f(x i , x -i ) r f(x i , x -i ) r u i (x i , x ) r u i (x i , x ) r x i r +1 (a) (b) (c) f( f f x i x x , x -i ) i i r f( f f x i x x , x -i ) i i r u i (x i x x , x ) r f( f f x i x , x -i ) i i r u i (x i x x , x ) r [</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>'</head><p>â€¢ Gauss-southwell (G-so) rule: at each iteration , r I r contains a single index i * that satisFies:</p><formula xml:id="formula_11">| max i i x x q x x * i r i r j j r j r 1 1 ! $ - - - - t t $ .</formula><p>For some constant  objective function and the approximation function are the same at the point of approximation (cf. Figure <ref type="figure" target="#fig_1">3</ref>). In Table <ref type="table">3</ref>, we provide a few commonly used upper bounds that satisfy Assumption A; see also <ref type="bibr" target="#b20">[21]</ref> and <ref type="bibr" target="#b21">[22]</ref> for additional examples. More discussion will be given in subsequent sections on how these bounds are used in practice.</p><p>For a popular subclass of problem (1), Assumption A can be further simplified; see the following example <ref type="bibr">[</ref> Now that we have seen the main steps of the algorithm, we describe its theoretical properties. We address the following questions related to the convergence of the BSUM: When does the BSUM converge? How fast does it converge? When does the BSUM fail and why? The answers to these questions will be instrumental in understanding the framework as well as evaluating the performance of various related algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>when dOeS the bSUm COnverge?</head><p>To discuss the convergence property of the algorithm, we first investigate the optimality conditions for (1), which characterize the set of solutions that we would like our algorithm to reach. To this end, we introduce two related notions, one is called the stationary solution and the other is the coordinatewise minimum solution; see Table <ref type="table">4</ref>  </p><formula xml:id="formula_12">f z d d d d d d 0 0 R 1 2 2 1 2 6 $ ! ! = l t</formula><p>but this point is not a stationary solution as ( ; )</p><formula xml:id="formula_13">f z d 0 1 l t t for . ( , ) d 4 3 = - t</formula><p>This fact can also be observed in the contour plot of the function in Figure <ref type="figure">4</ref>.</p><p>This example confirms that the coordinatewise minimum can be much inferior to the stationary solution. Therefore in the subsequent discussion we will mainly focus on finding the stationary solutions rather than the coordinatewise minimum. An immediate question is: can one easily distinguish these two types of solutions, or for that matter, when does a coordinatewise minimum become a stationary solution? Let us define a regular point x X ! as a point that satisfies the following statement: if xis coordinatewise minimum, then it is a stationary solution. For a large and popular subclass of (1) expressed below in <ref type="bibr" target="#b4">(5)</ref>, where the nonsmooth function is separable across the blocks, all feasible points x X ! are regular</p><formula xml:id="formula_14">( ): ( , , ) ( ), , , , . min f x g x x h x x i n 1 s.t. X n i i n i i i 1 1 g g ! = + = = /<label>(5)</label></formula><p>[TaBle <ref type="table">3</ref>] coMMonly uSed uPPer BoundS SaTISFyIng aSSuMPTIon a. Here, the first part of the result deals with the possibility of an unbounded sequence, whereas the second part assumes that the sequence lies in a compact set, therefore it has a slightly stronger claim. Note that Theorem 1 can be easily extended to all the coordinate selection rules given in Table <ref type="table" target="#tab_0">2</ref>, and for the randomized version the convergence is with probability 1.</p><p>Special attention should be given to the conditions set forth by Theorem 1. First, it says that the upper-bound function needs to be picked carefully to satisfy both Assumption A and the requirement that at least n 1 -subproblems (3) have a unique solution. However, when the objective function ( ) f x is convex, the uniqueness requirement of the per-block solution can be dropped; see <ref type="bibr" target="#b35">[36]</ref>. Also, Theorem 1 requires the problem to be well-defined so that coordinatewise optimal solutions are equivalent to the stationary solution (which precludes objective function Ax 1 in Example 2). Next we will demonstrate, through a couple of concrete examples, that relaxing some of these conditions indeed leads to the divergence of the BSUM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>when dOeS bSUm fail?</head><p>Here we provide a few examples in which BSUM fails to converge to any stationary solutions. The examples in this section serve as reminders that in practice, to avoid those pitfalls, extreme care must be exercised when designing large-scale optimization algorithms.</p><p>Our first example comes from Example 2. It demonstrates the consequence of lacking the regularity condition.</p><p>Example 3: Consider the following unconstrained convex optimization problem</p><formula xml:id="formula_15">, min A x x i i i 1 2 1 = / where [ ] A 3 2 T 1 = and [ ] . A 4 1 T 2 =</formula><p>Clearly by defining</p><formula xml:id="formula_16">[ ; ], A 3 4 2 1 =</formula><p>the objective function can be rewritten as , Ax 1 which is not regular at point ( , ) ( , )</p><formula xml:id="formula_17">x x 4 3 1 2 = - (cf. Example 2). It follows that, by setting ( , ) u x z Ax i i i i i 1 2 1 = = /</formula><p>(no approximation is used) and letting ( , ) ( , ),</p><formula xml:id="formula_18">x x 4 3 1 0 2 0 = -</formula><p>the BSUM algorithm will not be able to move forward for either x1 or , x2 thus it will be stuck at the noninteresting point ( , ), 4 3 -far away from the only stationary solution ( , ). 0 0 The next example shows that BSUM fails to converge if the feasible set X is no longer a Cartesian product of feasible sets , , ,X X n 1 g a fact that we have taken for granted so far. Example 4: Consider the following simple quadratic problem:</p><formula xml:id="formula_19">, . . min x x x x 2 s.t 1 2 2 2 1 2 + + =</formula><p>The optimal objective value is two. Consider the BSUM algorithm with an arbitrary approximation function satisfying Assumption A, but initiated at the point . ( , ) ( , )</p><formula xml:id="formula_20">x x 0 2 1 0 2 0 =</formula><p>Then the BSUM method will be stuck at this noninteresting point without making any progress because it is not possible to change a single block without violating the coupling constraint.</p><p>Our next example shows that the BSUM could diverge if the approximation function ui violates Assumption A.</p><p>Example 5: Consider the simple quadratic problem</p><formula xml:id="formula_21">, . , . min x x x x x x 2 1 1 s.t 1 2 2 2 1 2 1 2 # # + + -</formula><p>The optimal objective value is , 0 with . ( , ) ( , )</p><formula xml:id="formula_22">x x 0 0 * * 1 2 =</formula><p>Consider using the BSUM algorithm with a linear approximation function, which violates Assumption (A2). More specifically, for a given feasible tuple ( , ) ( , ),</p><formula xml:id="formula_23">x x x x 1 2 1 2 = t t the x1's subproblem becomes , , . . min x x x x 1 1 s.t x 1 2 1 1 1 # # G H + - t t</formula><p>Clearly, the optimal solution is either x</p><formula xml:id="formula_24">1 1 =-or . x 1 1 =</formula><p>The same happens when solving the subproblem for .</p><p>x2 Therefore the algorithm will never reach the optimal solution ( , ) ( , ).</p><formula xml:id="formula_25">x x 0 0 * * 1 2 =</formula><p>Further, if the feasible sets of x1 and x2 are unbounded, then the BSUM can diverge.</p><p>Our last example is taken from Powell <ref type="bibr" target="#b36">[37]</ref>. It shows that without the uniqueness assumption of each subproblem (3), the BSUM algorithm is not necessarily convergent. </p><formula xml:id="formula_26">( ): ( ) ( ) min f x x x x x x x x x 1 1 1 2 2 3 3 1 1 2 1 2 =- - - + - + -- + + ( ) ( ) ( ) ( ) , x x x x 1 1 1 1 2 2 2 2 3 2 3 2 + - + -- + - + -- + + + +</formula><p>where the notation ( )</p><formula xml:id="formula_27">z 2 + means . ( { , }) max z 0 2</formula><p>In this case, fixing ( , ) x x 2 3 and optimizing over x1 yields the following solution</p><formula xml:id="formula_28">[ , ], 1 1 - otherwise x = ( | |) ( ) , , . x x x x x x 1 2 1 0 sign if 1 2 3 2 3 2 3! + + + + *<label>(6)</label></formula><p>A similar solution can be obtained for x2 and x3 as well. Suppose we set ( , ) ( ) <ref type="figure">( ,</ref><ref type="figure">,</ref><ref type="figure"></ref> ),</p><formula xml:id="formula_29">u x z f x i i = for all i (no approximation is used) and let ( , , ) ( / , , / ) x x x 1 12 1 1 1 4 1 0 2 0 3 0 e e e = -- + -- for some . 0 2 e</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Then it can be shown the applying the cyclic version of the BSUM algorithm, the iterates will be cycling around six points</head><formula xml:id="formula_30">1 1 1 - ( , , ), 1 1 1 -- ( , , ), 1 1 1 - ( , , ), 1 1 1 -- ( , , ), 1 1 1 - ( , ,</formula><p>), 1 1 1 --and none of these six points is a stationary solution of the original problem. The reason for such pathological behavior is that, in any one of the six limit points above, there are at least two subproblems that have multiple optimal solutions. For example, at ( , ,</p><p>),</p><formula xml:id="formula_31">1 1 1 - and fixing , x x 2 3 (resp. , ), x x 1 3</formula><p>the optimal solution for x1 (resp. )</p><p>x2 is any element in the interval [ , ];</p><p>1 1 -cf. <ref type="bibr" target="#b5">(6)</ref>. A natural question at this point is, can we make the BSUM work for these examples? The answer is affirmative, but how this can be done requires a case by case study. To handle the first two examples (i.e., Example 3 and 4), a generalized version of BSUM is needed, which will be discussed in the "Extensions" section. For the third example, one can simply pick a better upper bound to guarantee convergence. For example, if we pick the proximal upper bound (cf. Table <ref type="table">3</ref></p><formula xml:id="formula_32">): ( , ) ( ) , / u x z f x x z 2 i i i i 2 c</formula><p>= + -then each subproblem will have a unique solution, and the algorithm will not be trapped by the noninteresting solutions given in Example 6. Notice that the use of / x z</p><formula xml:id="formula_33">2 i i 2 c</formula><p>inhibits the move of xi from its current position .</p><p>zi Thus, the main message here is that when optimizing each block, it is beneficial, at least theoretically, to be less greedy so that a conservative step is taken towards reducing the objective. The extent of the "conservativeness" for the per block update is then naturally characterized by the chosen upper bounds. Quite interestingly, the difficulty with the nonunique subproblem solution can also be resolved by using randomization. Formally, we have the following corollary to Theorem 1 <ref type="bibr" target="#b37">[38]</ref>.</p><p>Corollary 1: Suppose the level set </p><formula xml:id="formula_34">{ ( ) ( )} x f x f x X 0 0 # = is compact. Then,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>hOw faSt dOeS the bSUm COnverge?</head><p>Now that we have examined the convergence of the BSUM, let us proceed next to characterize the convergence speed of the algorithm. There is no doubt that this is an important issue, especially so for big data optimization-the sheer size of the data and limited computational resource makes it difficult to optimize a problem to global optimality. Consequently, we are generally satisfied with high-quality suboptimal solutions and are mostly concerned with how quickly such solutions can be obtained.</p><p>Recently, extensive research efforts have been devoted to analyzing the convergence rate for various BSUM-type algorithms, most of which use randomized coordinate selection rules and/or quadratic upper-bound functions (cf. Table <ref type="table">3</ref>) to solve convex problems; for example; see <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, and <ref type="bibr" target="#b38">[39]</ref>- <ref type="bibr" target="#b42">[43]</ref>. Since it is not possible to go over all the details of these different variations of BSUM, here we present, at a high level, a fairly general result that covers a large family of upper-bound functions satisfying Assumption A, as well as all coordinate selection rules outlined in Table <ref type="table" target="#tab_0">2</ref>.</p><p>To this end, let us make the following additional assumptions.</p><formula xml:id="formula_35">Assumption B B1) ( ): ( ) ( ), h f x g x x i i n i 1 = + = /</formula><p>where ( ) g x is a smooth convex function with Lipschitz continuous gradient, i.e., there exists</p><formula xml:id="formula_36">a constant L such that ( ) ( ) , g x g y L x y d d # - - . , x y X 6 ! Further both g and hi s are convex functions. B2) The level set { ( ) ( ), } x f x f x x X 0 # ! is compact. B3) Each upper-bound function ( , ) u x z i i is strongly convex with respect to . xi An e-optimal solution x X ! e is defined as an { , ( ) ( ) }, x x x f x f x X * ! ! # e - e</formula><p>where ( ) f x * is the globally optimal objective value of problem <ref type="bibr" target="#b4">(5)</ref>. Suppose both Assumptions A and B are satisfied. Then it is shown in <ref type="bibr" target="#b37">[38]</ref> and <ref type="bibr" target="#b41">[42]</ref> that BSUM takes at most / c e iterations to find an e-optimal solution, for all coordinate rules specified in Table <ref type="table" target="#tab_0">2</ref>, where c 0 2 is a constant only related to the description of the problem. Such a type of convergence rate is known as sublinear convergence. Here, the main message is that under Assumptions A and B, the algorithm generally converges sublinearly in the order of / .</p><p>1 e Further, for different special forms of BSUM, the constant c in front of / 1 e can be significantly refined so that it is independent of problem dimension; see <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b7">[8]</ref>. It is also interesting to note here that when the objective f is either strongly convex or convex but with certain special structure, the BSUM achieves the linear rate of convergence. That is, BSUM takes at most ( ( / )) log c O e iterations to find an e-optimal solution, which is much faster than the sublinear rate; see, e.g., <ref type="bibr" target="#b43">[44]</ref> and <ref type="bibr" target="#b44">[45]</ref> for the related discussions.</p><p>Finally, we briefly mention that it is possible to relax certain conditions in Assumption B to obtain refined rates. For example, <ref type="bibr" target="#b41">[42]</ref> shows that dropping the per-block strong convexity assumption in (B3) still achieves an ( / )</p><p>1 O e sublinear convergence. In <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b45">[46]</ref> it is shown that, when removing the convexity Assumption (B1), it is also possible to characterize the convergence rate to stationary solutions. In <ref type="bibr" target="#b41">[42]</ref> it is shown that when there are two blocks of variables, the cyclic version of the BSUM can be accelerated to achieve an improved ( / )</p><p>1 O e complexity. In a few recent works <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, it is shown that when randomized block selection and the quadratic upper bound are used, it is possible to accelerate the BSUM with any n 2 2 blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>algorIThMS covered By The BSuM FraMework</head><p>Now that we have a good understanding of the theoretical properties of the family of BSUM algorithms, we demonstrate in this section the wide applicability of the BSUM framework by revealing its connection to a few well-known algorithms for highdimensional massive data analysis. For each of the examples presented below, we pay special attention to the design of the upper-bound functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>the bCd algOrithm</head><p>The first algorithm that the BSUM covers is obviously the classic BCD described in the "The BCD Method" section. Here the upperbound function is simply the original objective itself, i.e., ( , ): ( , ),</p><formula xml:id="formula_37">u x z f x z i i i i = - , , . x z i X X i i 6 6 ! !</formula><p>We would like to mention that all the convergence and rate of convergence analysis of BSUM carries over to the BCD method. Specifically, the result in the section "How Fast Does the BSUM Converge?" implies that the BCD method (with coordinate update rules specified in Table <ref type="table" target="#tab_0">2</ref>) converges in a sublinear manner whenever Assumptions (B1) and (B2) are satisfied. This result by itself is interesting, as there has been limited theoretical analysis for the general form of a BCD algorithm when applied to problems satisfying Assumptions (B1) and (B2), but not (B3).</p><p>the CCCp Consider the following unconstrained nonconvex problem, known as the difference of convex (DC) program:</p><p>( ):</p><formula xml:id="formula_38">min f x = ( ) ( ) g x g x 1 2</formula><p>where both g1 and g2 are convex functions. The well-known CCCP algorithm <ref type="bibr" target="#b30">[31]</ref>  </p><formula xml:id="formula_39">( ) u x x g x x x g x g x r r r r 1 2 2 d G H = -- -<label>)</label></formula><p>Clearly, the linear upper bound in Table <ref type="table">3</ref> is used here, therefore CCCP is a special case of the BSUM algorithm with a single block of variables. Furthermore, the updates can also be done in a block coordinate manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>the majOrizatiOn-minimizatiOn algOrithm</head><p>The majorization-minimization (MM) algorithm, which has been widely used in statistics <ref type="bibr" target="#b48">[49]</ref>, can be viewed as the single block version of BSUM. Consider the problem of ( ) min f x</p><p>x X ! where ( ) f x is a smooth function. The basic idea of the MM algorithm is to first construct a "majorization" function ( , ) u x z for the original objective ( ),</p><formula xml:id="formula_40">f z such that ( , ) ( ), , ,<label>( , ) ( ),</label></formula><formula xml:id="formula_41">. u x z f z x z u x x f x x X X 6 6 $ ! ! =<label>(7)</label></formula><p>Then the algorithm generates a sequence of iterates by successively minimizing . ( , ) u x x r This algorithm represents a slight generalization of the CCCP discussed previously, but nevertheless still falls in the framework of BSUM.</p><p>As another concrete example of the MM algorithm, let us consider the celebrated EM algorithm <ref type="bibr" target="#b29">[30]</ref>. Let w be a vector observation used for estimating the parameter .</p><p>x The maximum likelihood estimate of x is given as (where ( | ) p w x denotes the conditional probability of w given )</p><p>x</p><formula xml:id="formula_42">. ( | ) arg max ln x p w x x ML = t (8)</formula><p>Let the random vector z be some hidden/unobserved variable.</p><p>The EM algorithm generates a sequence { } x r by iteratively performing the following two steps 1) E-Step: Calculate ( , ):</p><formula xml:id="formula_43">{ ( , | )} ln g x x pw z x E | , r z w x r = and 2) M-Step: x r 1 = + . ( , ) arg max g x x x r</formula><p>To see why the EM algorithm is a special case of MM (hence a special case of BSUM) let us rewrite <ref type="bibr" target="#b7">(8)</ref> as:</p><p>( | ), min ln p w x x -the objective of which can be bounded by</p><formula xml:id="formula_44">( | ) ( | , ) ln ln p w x pw z x E | z x - = - ( | , ) ( | , ) ( | , ) ln p z w x p z w x p w z x E | z x r r =- = G ( | , ) ( | ) ( | , ) ln p z w x p z x p w z x E | , z w x r r =- = G ( | , ) ( | ) ( | , ) ln p z w x p z x p w z x E | , z w x r r # - = G ( , | ) ( | , ) ln ln p w z x p z w x E E | , | , z w x z w x r r r =- + : ( , ), u x x r =</formula><p>where the inequality is due to the fact that a convex function =therefore, both conditions in <ref type="bibr" target="#b6">(7)</ref> are satisfied. Similarly as in the previous case, one can extend the EM to a block coordinate version; see <ref type="bibr" target="#b20">[21]</ref> for a detailed discussion.</p><formula xml:id="formula_45">f must satisfy [ ( )] ( [ ]) f x f x E E $ (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>the prOximal pOint algOrithm (</head><p>The classical proximal point algorithm (PPA) (see, e.g., <ref type="bibr" target="#b49">[50,</ref><ref type="bibr">Sec. 3.4.3]</ref>) obtains a solution of the problem ( ) min f x</p><p>x X ! by solving the following equivalent problem</p><formula xml:id="formula_46">( ) , min f x x y 2 2 , x y X X c + - ! !<label>(9)</label></formula><p>where ( ) f : is a convex function, X is a closed convex set, and 0 2 c is a coefficient. Clearly ( <ref type="formula" target="#formula_46">9</ref>) is strongly convex in both x and y so long as ( ) f x is convex [but not jointly strongly convex in ( , ) .</p><p>] x y This problem can be solved by performing the following two steps alternatingly . ( ) , arg min</p><formula xml:id="formula_47">x fx c x y y x 2 1 r x r r r 1 1 2 X = + = - ! + - ' 1<label>(10)</label></formula><p>Equivalently, the PPA algorithm can be viewed as successively minimizing the single block version of the proximal upper bound ( ; ) u x x r given in Table <ref type="table">3</ref>. Note that for a problem with a single block of variables, if x X ! is coordinatewise minimum, then it must be a global minimum solution. Therefore, every feasible solution x X ! is regular, and the convergence of PPA is covered by BSUM.</p><p>Furthermore, the PPA can be generalized to solve the multiblock problem <ref type="bibr" target="#b0">(1)</ref>, where ( ) f : is convex in each of its block components, but not necessarily strictly convex. Directly applying BCD may fail to find a stationary solution for this problem, as the per-block subproblems can contain multiple solutions (cf. <ref type="bibr">Example 6)</ref>. Alternatively, we can consider an alternating PPA <ref type="bibr" target="#b50">[51]</ref>, which successively solves the following:</p><formula xml:id="formula_48">( , ) , . min f x x x x x 2 s.t. X x i i r i i r i i 2 i ! c + - -</formula><p>Clearly this algorithm is a special form of BSUM with strongly convex proximal upper bound (cf. Table <ref type="table">3</ref>). It follows that each subproblem has a unique optimal solution, and by Theorem 1 it must converge to a stationary solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>the fOrward-baCkward Splitting algOrithm</head><p>The forward-backward splitting (FBS) algorithm (also known as the proximal splitting algorithm; see, e.g., <ref type="bibr" target="#b51">[52]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>= + -</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>!</head><p>The FBS iteration is given below <ref type="bibr" target="#b51">[52]</ref>:</p><formula xml:id="formula_49">( ( )) , x x g x prox r h r r 1 backward step forward step d b = - b + 1 2 3 444 444 &gt;<label>(11)</label></formula><p>where .</p><formula xml:id="formula_50">( , / ] L 0 1 ! b Define ( , ): ( ) , ( ) ( ), u x x h x x x x x g x g x 2 1 r r r r r 2 d G H b = + - + - +<label>(12)</label></formula><p>which is the quadratic upper bound in Table <ref type="table">3</ref>, with : / I.</p><formula xml:id="formula_51">1 1 b U =</formula><p>It is easy to see that the iteration ( <ref type="formula" target="#formula_49">11</ref>) is equivalent to the following iteration ( , ), arg min</p><formula xml:id="formula_52">x u x x r x r 1 X = ! +</formula><p>therefore, it again falls under the BSUM framework. Similar to the previous example, we can generalize the FBS algorithm to solve multiple block problems of the form <ref type="bibr" target="#b4">(5)</ref>. The resulting algorithm, sometimes also known as the BCPG method, has received significant attention recently due to its efficiency for solving certain big data optimization problem such as the least absolute shrinkage and selection operator (LASSO) <ref type="bibr" target="#b10">[11]</ref>. For recent developments and applications for BCPG, see <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b38">[39]</ref>, and <ref type="bibr" target="#b52">[53]</ref>.</p><p>Here we make a special note that, by appealing to the general convergence rate result in the section "How Fast Does the BSUM Converge?" the BCPG method with any coordinate selection rules in Table <ref type="table" target="#tab_0">2</ref> gives a sublinear convergence rate, when it is used to solve (5) that satisfies Assumption B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>the nmf algOrithm</head><p>Consider the following NMF problem:</p><formula xml:id="formula_53">( , ): , . , , min f W H V WH W H 2 1 0 0 s.t F 2 , W H R R M K K M $ $ = - ! ! # #<label>(13)</label></formula><p>where V R M N ! # + is given. The problem has been extensively studied since Lee and Seung's seminal work <ref type="bibr" target="#b31">[32]</ref>, and it has wide applications in factor analysis, dictionary learning, speech analysis and so on <ref type="bibr" target="#b53">[54]</ref>. In <ref type="bibr" target="#b31">[32]</ref>, a simple and efficient multiplicative algorithm is proposed:</p><formula xml:id="formula_54">[ ] [ ] [( ) ] [( ) ] , , , , , , H H W W H W V j Ki N 1 1 , , , , r j i r j i r T r r j i r T j i 1 g g = = = + (14a) [ ] [ ] [ ( ) ] [ ( ) ] ,</formula><p>, , , , , .</p><formula xml:id="formula_55">W W W H H V H j Mi K 1 1 , , , , r j i r j i r r r T j i r T j i 1 1 1 1 g g = = = + + + + (14b)</formula><p>Here  <ref type="formula">14b</ref>) is also covered by BSUM <ref type="bibr" target="#b54">[55]</ref>.</p><p>Let Hi and Vi represent the i th column of H and , V respectively. Then, at a given iterate { , }, W H r r the subproblem for optimizing Hi is given by</p><formula xml:id="formula_56">F ( , { , }): min f H W H V W H V W H 2 1 2 1 H i r r i r i F j r j r j i 0 2 2 i = - + - ! $ . / (15) Define the upper-bound function ( ,{ , }) u H W H i i r r as ) ( , { , }) H f H W H d - ( , { , }): ( , { , }) ( u H W H f H W H H i i r r i r r r i i r T H i r r i = + ( ) ( , )( ), H H W H H H 2 1 i i r T i r i r i i r U + - - where ( , ) W H i r i r U is a diagonal matrix given by ( , ): [ ] [( ) ] , , [ ] [( ) ] . W H H W W H H W W H Diag i r i r i r r T r i r i r K r T r i r K 1 1 g U = e o Clearly, ( , ) , W H 0 i r i r ( U and it is easy to show that ( , ) ( ) , W H W W i r i r r T r ( U where ( ) W W r T</formula><p>r is the Hessian of the objective of (15) <ref type="bibr" target="#b31">[32]</ref>. This implies that ( , { , })</p><formula xml:id="formula_57">u H W H i i r r</formula><p>is the quadratic upper bound given in Table <ref type="table">3</ref> </p><formula xml:id="formula_58">of . ( , { , }) f H W H i r r</formula><p>Further, one can check that the subproblem that minimizes ( , { , })</p><formula xml:id="formula_59">u H W H i i r r</formula><p>has a unique solution, given by (14a). Similar analysis can be established for the W-block update rule as well. Therefore, we conclude that the iterates (14a) and (14b) are a special case of BSUM. Finally, we note that it is also possible to use different upper-bound functions to derive more efficient update rules for the NMF problem <ref type="bibr" target="#b12">(13)</ref>; see, e.g., <ref type="bibr" target="#b55">[56]</ref>, where both the concave upper bound and the Jensen's upper bound (cf. Table <ref type="table">3</ref>) are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>the iterative reweighted leaSt SqUareS methOd</head><p>The iterative reweighted least squares (IRLS) method is a popular algorithm used for solving big data problems such as sparse recovery <ref type="bibr" target="#b56">[57]</ref>. Consider</p><formula xml:id="formula_60">( ) | |( , . , ( min A x b h x x s.t R j j j m 1 2 x ! + + , = /<label>(16) where</label></formula><formula xml:id="formula_61">, R A j k m i ! # , b R j ki</formula><p>! and ( ) h x is some convex function not necessarily smooth. For a set of applications for this model, see <ref type="bibr" target="#b40">[41,</ref><ref type="bibr">Sec. 4]</ref>. Consider the following smooth approximation of (16):</p><formula xml:id="formula_62">, . x s.t R ! , ( ) ( ): ( ) min h x g x h x A x b x j j j m 1 2 2 h + = + + + , = / (<label>17</label></formula><formula xml:id="formula_63">)</formula><p>where h is some small constant and ( ) g x denotes the smooth part of the objective. The IRLS algorithm solves <ref type="bibr" target="#b16">(17)</ref> by performing the following iteration:</p><formula xml:id="formula_64">( ) . arg min x hx A x b A x b 2 1 r x j r j j j j 1 2 2 1 2 2 R m h h = + + + + + , ! + = ) 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>/</head><p>Define the following function for ( ):</p><formula xml:id="formula_65">g x ( , ) . u x x A x b A x b A x b 2 1 r j r j j j j j r j 2 2 1 2 2 2 2 h h h = + + + + + + + , = e o /<label>(18)</label></formula><p>It is clear that ( ) ( , ), g x u x x r r r = so Assumption (A1) is satisfied. To verify Assumption (A2), we apply the arithmetic-geometric inequality, and have </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>/</head><p>Then according to Example 1, Assumption (A3) is automatically true, therefore we have verified that ( , ) u x x r defined in ( <ref type="formula" target="#formula_65">18</ref>) is indeed an upper-bound function for the smooth function . ( ) g x It follows algorithm corresponds to a single-block BSUM algorithm. Notice that using the BSUM framework we can easily generalize the IRLS to the multiblock scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>aPPlIcaTIonS oF The BSuM FraMework</head><p>Here we briefly review a few applications of the BSUM framework in wireless communication, bioinformatics, signal processing, and machine learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>wireleSS COmmUniCatiOn and tranSCeiver deSign</head><p>Consider a multiple-input, multiple-output interference channel with K transmitter-receiver pairs. Let M (resp. )</p><p>N be the number of antennas at each transmitter (resp. receiver) and each transmitter , k , , , , respectively. A crucial task in modern wireless networks is to design the transmit and receive beamformers vk and uk to maximize a given utility of the system. Here, for simplicity of presentation, we consider the sum rate utility function as our objective. Therefore, our goal is to solve the following optimization problem: . ( ) , , , , ,</p><formula xml:id="formula_66">k K 1 2 f = is interested</formula><formula xml:id="formula_67">u v max R v P k K 1 2 s.t , u,v k k K k k 1 2 6 f # = = / (<label>19</label></formula><formula xml:id="formula_68">)</formula><p>where Pk is the total power budget of user k and ( , ), R u v k which is the communication rate of user , k is given by <ref type="formula" target="#formula_67">19</ref>) is nonconvex and known to be NP-hard <ref type="bibr" target="#b57">[58]</ref>.</p><formula xml:id="formula_69">( , ) | | . | | log R u 1 u v u H v u H v k k j k k H kj j k H kk k 2 2 2 2 v = + + ! f p / Problem (</formula><p>Using the well-known relation between the signal-to-interference-plus-noise ratio (SINR) and the mean square error (MSE) value, one can rewrite <ref type="bibr" target="#b18">(19)</ref> as <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b58">[59]</ref>:</p><p>.</p><formula xml:id="formula_70">P k K 1 2 s.t u v v k K k k k 1 2 v,u 6 f # = = ^h /<label>( , ) , , , , , min log e</label></formula><p>where ( , ) e u v k is the MSE value and is given by</p><formula xml:id="formula_72">( , ) | | | | . e 1 u v u H v uH v k k H kk k j k k H kj j 2 2 2 v = - + + ! /</formula><p>Since the ( ) log : function is concave, it is upper bounded by its first-order approximation (i.e., the linear upper bound in Table <ref type="table">3</ref>). Therefore, we can define the function  <ref type="formula">21</ref>) is a valid upper bound in the BSUM framework and at each iteration , r this choice of approximation function leads to a quadratic programming problem which has closed-form solutions. The resulting algorithm, dubbed WMMSE, converges to a stationary point of the problem and, in practice, it typically converges in a few iterations <ref type="bibr" target="#b11">[12]</ref> even for larger-size problems <ref type="bibr" target="#b59">[60]</ref>.</p><p>For more details of the algorithm and its extensions to various beamformer design scenarios and different utility functions, refer to <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b60">[61]</ref>, and <ref type="bibr" target="#b61">[62]</ref>. It is also worth noting that many other interesting transceiver design algorithms also fall into the BSUM framework; see <ref type="bibr" target="#b32">[33]</ref> and <ref type="bibr" target="#b62">[63]</ref>- <ref type="bibr" target="#b64">[65]</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>biOinfOrmatiCS and Signal prOCeSSing</head><p>Here we briefly outline two interesting big data applications of the BSUM framework in bioinformatics and signal processing. Given the observed reads, the likelihood of the abundance levels , , </p><formula xml:id="formula_73">AbuNdANCE</formula><p>As a special case of the EM algorithm, a popular approach for solving this optimization problem is to successively minimize a local tight upper bound of the objective function. In particular, the eXpress software <ref type="bibr" target="#b3">[4]</ref> solves the following optimization problem at the rth iteration of the algorithm: / / which makes the algorithm computationally efficient at each step. In practice, the above algorithm for abundance estimation converges in a few iterations. Moreover, this algorithm is perfectly suitable for distributed storage and multicore machines. In particular, since the number of reads N is much larger than the number of sequences , M one can store the reads , , R RN 1 f in np different processing units. Hence, at each iteration , r the processing unit , , , , </p><formula xml:id="formula_75">p p n 1 p f = can compute the local value , , , , N m M 1 1 , M m p r nm m m r nm m r n N 1 1 N p 6 f</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>/</head><p>For a very recent application of BSUM algorithm in gene RNA-seq abundance estimation, see <ref type="bibr" target="#b67">[68]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>tENSOR dECOMPOSItION</head><p>The CANDECOMP/PARAFAC (CP) decomposition has applications in different areas such as clustering <ref type="bibr" target="#b68">[69]</ref> and compression <ref type="bibr" target="#b69">[70]</ref>.</p><p>For ease of presentation, here we only consider third-order tensors. Given a third-order tensor , R X m m m</p><formula xml:id="formula_76">1 2 3 ! # # its rank R CP decomposition is given by , a b c X r r r r R 1 % % = = / where , a R r m1 ! , b R r m2 ! ; c R r m3</formula><p>! and the notation " c " stands for the outer product operator.</p><p>In general, finding the CP decomposition of a given tensor is NP-hard <ref type="bibr" target="#b70">[71]</ref>. A well-known algorithm for finding the CP decomposition is the alternating least squares (ALS) algorithm proposed in <ref type="bibr" target="#b71">[72]</ref> and <ref type="bibr" target="#b72">[73]</ref>. This algorithm is, in essence, the BCD algorithm on the following optimization problem:</p><formula xml:id="formula_77">. min a b c X r r R r r F 1 2 { , , } a b c r r r r R 1 % % - = = / (24)</formula><p>In the ALS algorithm, we consider three blocks of variables:{ } ,</p><formula xml:id="formula_78">ar r R 1 = { } , br r R 1 = and { } . cr r R 1 =</formula><p>At each iteration of the algorithm, two blocks are held fixed and only one block is updated by solving <ref type="bibr" target="#b23">(24)</ref>. The block selection rule is cyclic and, therefore, one needs the uniqueness of the minimizer assumption at each iteration for theoretical convergence guarantee. Clearly, this assumption does not hold for <ref type="bibr" target="#b23">(24)</ref>, therefore convergence is not always guaranteed. In addition, another well-known drawback of the ALS algorithm is the "swamp" effect where the objective remains almost constant for many iterations and then starts decreasing again. It has been observed in the literature that the employment of proximal upper bound (see Table <ref type="table">3</ref>) could help reduce the swamp effect <ref type="bibr" target="#b73">[74]</ref>. It is also suggested in <ref type="bibr" target="#b73">[74]</ref> that decreasing the proximal coefficient (c in Table <ref type="table">3</ref>) during the ALS algorithm could further improve the performance of the algorithm. Notice that these modifications in the algorithm makes the algorithm a special case of BSUM framework. Consequently, its theoretical convergence is also guaranteed by Theorem 1.</p><p>Figure <ref type="figure" target="#fig_17">5</ref> compares the performance of the naive ALS algorithm with the one using proximal upper bound. The figure shows that the proximal ALS algorithm has less swamp effect as compared to the naive ALS method. For more details of the algorithm, refer to <ref type="bibr" target="#b20">[21]</ref> and <ref type="bibr" target="#b73">[74]</ref>; and to <ref type="bibr" target="#b74">[75]</ref> for the application of BSUM and CP decomposition in gene expression and brain imaging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>maChine learning: SparSe diCtiOnary learning and SparSe linear diSCriminant analySiS dICtIONARy LEARNING fOR SPARSE REPRESENtAtION</head><p>In compressive sensing <ref type="bibr" target="#b75">[76]</ref>, <ref type="bibr" target="#b76">[77]</ref> problems, a given data signal is represented by sparse linear combination of the signals in a given set called a dictionary. In many applications even the dictionary is not known a priori, therefore, it should be learned from the data. the dictionary-learning problem can be written as <ref type="bibr" target="#b77">[78]</ref>, <ref type="bibr" target="#b78">[79]</ref> . <ref type="bibr">( , , )</ref> , ,</p><formula xml:id="formula_79">min d s.t Y A X A X A X , A X ! ! (<label>25</label></formula><formula xml:id="formula_80">)</formula><p>where the sets X and A are given based on the prior knowledge on the data. The function ( , , ) d $ $ $ measures the goodness-of-fit of the model. For example, a popular choice of the function ( , , ) d $ $ $ and the set A leads to the following optimization problem <ref type="bibr" target="#b78">[79]</ref>:</p><p>. , min</p><formula xml:id="formula_81">s.t a Y AX X , i F i 2 2 1 A X # b m - +</formula><p>where the first term in the objective keeps our estimated signals close to the training set and the second term forces the representation to be sparse. One popular approach in the dictionary-learning algorithm is to alternatingly update the dictionary A and the coefficients X <ref type="bibr" target="#b79">[80]</ref>. However, naively updating these two variables to its global optimum requires solving a sparse recovery problem at each iteration, which is costly for larger-sized problems. Motivated by the idea of inexact steps in the BSUM framework, one can iteratively replace the objective by a locally tight upper bound, which is easier to minimize at each iteration and, hence, leads to computationally cheaper steps in the algorithm. It is not hard to see that utilizing the quadratic upper bound in Table <ref type="table">3</ref> with diagonal matrices i U leads to closed-form updates at each step <ref type="bibr" target="#b77">[78]</ref>. Unlike many existing algorithms in the literature <ref type="bibr" target="#b78">[79]</ref>, <ref type="bibr" target="#b79">[80]</ref>, the resulting algorithm is guaranteed to converge theoretically to the set of stationary solutions as the result of Theorem 1.   <ref type="table">5</ref> show the performance of the resulting algorithm for dictionary learning in an image denoising problem. The denoising is performed on the Lena image corrupted by additive Gaussian noise with various variances .</p><p>2 v As can be seen from Table <ref type="table">5</ref>, the proposed algorithm results in larger PSNR values than the K-SVD method <ref type="bibr" target="#b79">[80]</ref> when the noise level is large. Moreover, the proposed algorithm contains less visual artifacts. Furthermore, each step of the proposed algorithm is in closed form and is computationally favorable, while each step of the K-SVD method requires an inner iterative method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SPARSE LINEAR dISCRIMINANt ANALySIS</head><p>The linear discriminant analysis (LDA), which is closely related to analysis of variance (ANOVA) and regression analysis, is widely used in machine learning and statistics for classification and dimensionality reduction purposes; see, e.g., <ref type="bibr" target="#b80">[81]</ref>. Let us, for the ease of presentation, focus only on the binary classification problem: Let</p><formula xml:id="formula_82">, x R i p ! , , , , i N 1 2 f =</formula><p>denote the zero-centered observations, where each observation xi belongs to one and only one of the two classes C0 and . C1 Given the binary classes, the standard within-class covariance can be calculated by</p><formula xml:id="formula_83">( )( ) , N 1 x x { , } w i k i k i k T 0 1 Ck n n R = - - ! ! t t t / / where /N 1 x k i i Ck n = ! t / is observations mean in class</formula><p>. Ck Similarly, the standard between-class covariance estimate is given by</p><formula xml:id="formula_84">, N N N 1 b T T 0 0 0 1 1 1 n n n n R = + t t t t t</formula><p>^h with N0 (resp. ) N1 being the cardinality of the set C0 (resp. ). C1 The goal of LDA is to find a lower-dimensional subspace so that the projection of the observations onto the selected subspace leads to well-separated classes. In other words, the task is to project data points into a subspace with large between-class variance relative to the within-class variance. For simplicity, consider projection onto one-dimensional subspace defined by the vector ; R p ! b see <ref type="bibr" target="#b81">[82]</ref> for details on projection to larger-dimensional subspaces. Then the inner product , x G H b is the projection of the observation x onto the selected subspace; and the within-class variance of the projected data points is given by ;</p><formula xml:id="formula_85">w T w v b b R = t t</formula><p>while the between class variance can be written as . Unfortunately, when the number of features is large relative to , N the matrix w R t is rank deficient and therefore ( <ref type="formula" target="#formula_79">25</ref>) is ill posed. To resolve this issue and to have a small generalization error, <ref type="bibr" target="#b81">[82]</ref> suggests to regularize the optimization problem with a convex penalty function ( ); P $ and solve .</p><p>( )</p><formula xml:id="formula_86">. max P 1 s.t T b T w # b b b b b R R - b t t<label>(26)</label></formula><p>Clearly, this optimization problem is nonconvex. As suggested in <ref type="bibr" target="#b81">[82]</ref>, one can linearize the first part of the objective in ( <ref type="formula" target="#formula_86">26</ref>) iteratively to obtain a tight upper bound of the objective. It is not hard to see that the algorithm used in <ref type="bibr" target="#b81">[82]</ref> is BSUM with the linear upper bound given in Table <ref type="table">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>exTenSIonS</head><p>In this section, we discuss extensions and generalizations of the BSUM framework in various settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>StOChaStiC OptimizatiOn</head><p>Consider the following stochastic optimization problem:</p><p>.  where X is a closed convex set and p is a random variable modeling the uncertainty in our optimization problem. A standard classical approach for solving <ref type="bibr" target="#b26">(27)</ref> is the sample average approximation (SAA) method; see <ref type="bibr" target="#b82">[83]</ref> and the references therein. At iteration r of the SAA method, given a new realization r p of the random variable , p the SAA method generates a new iterate x r by solving a problem with the sample average / ( , ) r g x 1</p><formula xml:id="formula_87">( ) ( , ) , min f x g x x s.t E X x _ ! p p 6 @<label>(27)</label></formula><formula xml:id="formula_88">r i i 1 p = /</formula><p>as its objective, where , , , r 1 2 f p p p are independent identically distributed realizations of the random variable . p A major drawback of the SAA method is that each of its iteration can be computationally very expensive. The computational inefficiency arises from either the nonconvexity of the objective, or not having closed-form solutions at each iteration.</p><p>Motivated by the BSUM framework, the authors of <ref type="bibr" target="#b19">[20]</ref> and <ref type="bibr" target="#b83">[84]</ref> suggest using an inexact version of the SAA method, in which a sequence of upper bounds of the objective are minimized. In particular, at each iteration , r the optimization variable is updated by .</p><formula xml:id="formula_89">, x s.t X ! ( , , ) arg min x r g x x 1 r x i r i i 1 1 ! p = t (â€¢, , ) g x i i 1 p - t</formula><p>is an upper bound of the function (â€¢, )</p><formula xml:id="formula_90">g i p around the point . x i 1 - The approximation function (â€¢, ) g x i i 1 p - t</formula><p>is assumed to be in the form of the BSUM approximation. The resulting algorithm, named stochastic successive upper-bound minimization (SSUM), is guaranteed to converge to the set of stationary solutions almost surely; see <ref type="bibr" target="#b19">[20]</ref> for more details. Further, it is shown to be capable of dealing with various practical problems in signal processing and machine learning. For example, as we will see shortly, the authors of <ref type="bibr" target="#b84">[85]</ref> apply the SSUM framework to cope with uncertainties in channel estimation for a wireless beamformer design problem. As another example, the online sparse dictionarylearning algorithm proposed in <ref type="bibr" target="#b85">[86]</ref> is a special case of SSUM.</p><p>The stochastic optimization framework is well suited for many modern big data applications, especially when the entire data set is not available initially and the data points are made available over time. These problems can be considered as the previously mentioned general stochastic optimization problem; see also <ref type="bibr" target="#b86">[87]</ref>. In addition, many statistical model fitting problems, such as the simple classical regression problem, can be cast as minimizing the following sum-cost function ( , ).</p><formula xml:id="formula_91">g x L 1 p, , =</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>/</head><p>Typically, the number of data points L is very large, making it difficult for batch processing. Therefore, it is desirable to implement algorithms working with only one (or a few) data point(s) at each step. In these scenarios, the stochastic optimization framework is useful since the sum-cost minimization problem can be viewed as a stochastic optimization problem ( , ) min g x E p 6 @ with p being uniformly drawn from the set . { , , }</p><formula xml:id="formula_92">L 1 f p p</formula><p>As an example of the SSUM method, consider the wireless transceiver design problem discussed in the "Wireless Communication and Transceiver Design" section, where the channel coefficients { } H , ij i j are not exactly known. In this scenario, we can consider the channel coefficients as random variables and solve the following stochastic optimization problem:</p><p>.</p><formula xml:id="formula_93">[ ( , )] , ,<label>, , , max R</label></formula><formula xml:id="formula_94">P k K 1 2 s.t u v v E , k K k k k 1 2 u v 6 f # = = /<label>(28)</label></formula><p>which is the stochastic counterpart of the optimization problem <ref type="bibr" target="#b18">(19)</ref>. Utilizing the upper bound <ref type="bibr" target="#b20">(21)</ref> in the SSUM algorithm leads to the stochastic WMMSE algorithm <ref type="bibr" target="#b84">[85]</ref>. Figure <ref type="figure">7</ref> illustrates the numerical performance of the SSUM methods as compared with SAA. At each iteration of the SAA procedure, one should solve a nonconvex optimization problem. Two different methods are considered: the gradient descent method with random initialization and the WMMSE algorithm, which is known to converge in few iterations for this problem. As Figure <ref type="figure">7</ref> illustrates, the running time of the SAA algorithm is much longer than that of the SSUM. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COUpling COnStraintS</head><p>So far in this article, we have assumed that the constraints in the optimization problem is separable and convex. In other words, the constraint set X in (1) is of the form X X X n 1 # f = with each Xi being convex. A natural extension of the BSUM framework is to modify it to deal with coupling and nonconvex constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LINEAR COuPLING</head><p>Consider the following convex problem with linear coupling constraints . ( , , ) , , , , ,</p><formula xml:id="formula_95">min f x x A x b x i n 1 2 s.t X ( , , ) x x n i i n i i i 1 1 n 1 f 6 f ! = = f = / (29) where , x R i mi ! , A R i k mi ! # b R k ! and ( ) f $ is a convex func- tion.</formula><p>As seen in Example 4, the direct extension of the BCD/ BSUM approach does not work for this type problem. A popular approach for solving the above optimization problem is the alternating direction method of multipliers (ADMM) <ref type="bibr" target="#b87">[88]</ref>, <ref type="bibr" target="#b88">[89]</ref>. This approach is based on finding a saddle point of the augmented Lagrangian function ( , , ;</p><formula xml:id="formula_96">L x x f x x A x b Ax b 2 n n i i n i i i n 1 1 1 2 f f m m t = + -+ - = = / / where R k ! m<label>) ( , , ) , ,</label></formula><p>is the Lagrange multiplier corresponding to the linear constraint; 0 2 t is the augmented Lagrangian coefficient; and ,â€¢ â€¢ G H denotes the inner product operator.</p><p>At each iteration of the ADMM method, either a primal block variable xi is updated according to ( , ; ), arg min</p><formula xml:id="formula_97">x L x x i r x i i r r 1 X i i ! m ! + -</formula><p>or the dual Lagrange multiplier m is updated according to the gradient ascent rule </p><p>, (BSUMM) <ref type="bibr" target="#b43">[44]</ref>, is guaranteed to converge to the global optimal of (29) under some regularity assumptions <ref type="bibr" target="#b43">[44]</ref>. For extensions to nonconvex problems, see the recent work <ref type="bibr" target="#b89">[90]</ref> and <ref type="bibr" target="#b90">[91]</ref>. There are a few other interesting techniques that deal with linearly coupling constraint. For example, <ref type="bibr" target="#b91">[92]</ref> and <ref type="bibr" target="#b92">[93]</ref> propose to randomly pick two blocks of variables to update at each iteration, and <ref type="bibr" target="#b93">[94]</ref> proposes new algorithms based on minimizing the augmented Lagrangian function. We refer the readers to these papers for more related works in this direction.</p><formula xml:id="formula_99">L x x u x x A x A x b Ax A x b 2 i i r r i i r r i i j j i j r i i j j i</formula><p>Let us illustrate an application of BSUMM to a multicommodity routing problem, which arises in the design of next-generation cloud-based communication networks <ref type="bibr" target="#b94">[95]</ref>. Consider a connected wireline network ( , ) N V L = that is controlled by K 1 + network controllers (NCs) as illustrated in Figure <ref type="figure">8</ref>. Let V denote the set of network nodes, which is partitioned into K subsets, i.e., , The central NC 0 controls the subnetwork N 0 , consisting of the master node and the links connecting different subnetworks, i.e., . L L i j ij 0 0 , = ! We consider two types of network constraints: 1) Link capacity constraints. Assume each link l L ! has a fixed capacity denoted as .</p><formula xml:id="formula_100">V V i K i 1 , = = , V V i j + Q = . i j 6 !</formula><p>Cl The total flow rate on link l is constrained by</p><formula xml:id="formula_101">, , C l 1 f L T l l 6 # ! (<label>30</label></formula><formula xml:id="formula_102">)</formula><p>where 1 is the all-one vector and [ , , ] . 2) Flow conservation constraints. For any node v V ! and data flow , m the total incoming flow should be equal to the total outgoing flow: , <ref type="figure">,</ref><ref type="figure">,</ref><ref type="figure">,</ref><ref type="figure"></ref> ,</p><formula xml:id="formula_103">f f f , , l l l M T 1 f _ N = (V, L) N 1 = (V 1 , L 1 ) N 4 = (V 4 , L 4 ) N 2 = (V 2 , L 2 ) N 3 = (V 3 , L 3 ) N 5 = (V 5 , L 5 ) [NC1] [NC4] [NC2]</formula><formula xml:id="formula_104">f r f r m M v 1 1 1 V , ( ) ( ) , (<label>)</label></formula><formula xml:id="formula_105">( ) l m l v v s m m l m l v v d m m In Out g 6 ! + = + = ! ! = = / /<label>(31)</label></formula><p>where</p><formula xml:id="formula_106">( ) { } v l d v In L l _ ! = and ( ) { } v l s v Out L l _ !</formula><p>= denote the set of links going into and coming out of a node v respectively; 1</p><formula xml:id="formula_107">1 v x = = if v x = otherwise . 1 0 v x = =</formula><p>To provide fairness to the users, we maximize the minimum rate of all data flows. The problem can be formulated as the following linear program (LP) </p><formula xml:id="formula_108">. , , , , max r r r m M 0 1 s.t f min m in m , f r g $ $ =<label>(32a)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>_</head><p>= Obviously, one can use off-the-shelf optimization packages such as Gurobi <ref type="bibr" target="#b95">[96]</ref> to solve the LP <ref type="bibr" target="#b31">(32)</ref>, this is only viable in a centralized setting where all the flows are managed by a single controller.</p><p>To enable distributed/parallel network management across the NCs, we need to allow each NC i to independently optimize the variables belonging to the subnetwork . N i However, this task is difficult because the optimization variables of (32) is coupled (indeed each flow rate fm appears in exactly two flow conservation constraints). To address this problem, we introduce a few sets of new variables to decouple the flow conservation constraints across different subnetworks (see <ref type="bibr" target="#b94">[95]</ref> for the detailed reformulation). The reformulated problem <ref type="bibr" target="#b31">(32)</ref> is given by max rmin</p><formula xml:id="formula_109">x .{ , } , { , } , , r x s.t x x x X X X min i i i i i 02 0 1 2 1 3 2 ! ! ! , , , , x x x x x x i K 1 i i i i i i 01 02 1 01 2 3 in in and i n N N N N i i 0 0 g = = = = 1 2 3 4 4 4 &gt;</formula><p>where , X0 , Xi1 and Xi2 are some feasible sets, and { , , , , , } r x x x x x</p><formula xml:id="formula_110">min i i i 02 01 1<label>2</label></formula><p>3 are the block variables. By applying the BSUMM, we can obtain a parallel/distributed algorithm. A few remarks about the implementation of this algorithm are:</p><p>1) The replication of link/flow variables for links across different subnetworks allows each subnetwork to be considered separately and independently. This feature makes the BSUMM subproblems solvable in parallel. The requirement of the replicated variables being the same as the original variables is enforced by the linear coupling constraints, and they can be satisfied asymptotically as the BSUMM algorithm converges.</p><p>2) The subproblems of the proposed BSUMM-based algorithm can be solved by each NC very efficiently. For example, the update of { , } r x min 02 can be performed by each NC in closed form; the update of { , }</p><p>x x i i 01 can be performed by running the well-known RELAX code <ref type="bibr" target="#b96">[97]</ref>.</p><p>3) A careful implementation of the BSUMM allows the NCs to act asynchronously, in the sense that they do not need to coordinate with each other for computation. Such asynchronous implementation has the potential of greatly improving the computational efficiency. We illustrate the BSUMM implementation over a mesh wireline network with 126 nodes, which is randomly partitioned into nine subnetworks with 306 directed links within these subnetworks and 100 directed links connecting the subnetworks. The capacities for the links within (resp. between) the subnetworks are uniformly distributed in <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b99">100]</ref> megabits/second (resp. <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b49">50]</ref> megabits/second). All simulation results are averaged over 200 randomly selected data flow pairs and link capacity.</p><p>To demonstrate the benefit of parallelization, we also utilize a high-performance computing cluster, and make each computing node to be an NC. We compare a few different approaches for solving <ref type="bibr" target="#b31">(32)</ref>:</p><p>1) use Gurobi <ref type="bibr" target="#b95">[96]</ref>, a centralized LP solver 2) apply the synchronous BSUMM algorithm, with K 10 = NCs; the computation is done by either a single or by ten distributed computing cores 3) apply the asynchronous BSUMM with K 10 = NCs; the computation is done in ten distributed computing cores. Note that the asynchrony in the network arises naturally from the per-node computational delay and network communication delay. In Table <ref type="table">6</ref>, we demonstrate the performance of various algorithms when . M 200 = Clearly, the asynchronous BSUMM with a small number of NCs outperforms all the rest of the algorithms.</p><p>The numerical results suggest that appropriate network decomposition and asynchronous implementation are both critical for the fast convergence of BSUMM. In practice, we observe that the network should be decomposed following a few guidelines:</p><p>â–  the computation burden across the subnetworks is well balanced â–  the subroutine within the network can achieve its maximum efficiency â–  the total number of replicated auxiliary variables is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NONCONvEx CONStRAINtS</head><p>The BSUM idea can be straightforwardly extended to a nonconvex constraint scenario for single block optimization problems.</p><p>To proceed, consider the optimization problem: </p><p>where the functions ( ) fi $ are not necessarily convex. Since dealing with nonconvex constraints is often not easy, one popular approach is to replace the functions ( ), , , , ,</p><formula xml:id="formula_112">f x i 1 2 i f , =</formula><p>with their locally tight upper bound ( , ) u x x i r iteratively. In other words, the update rule of the iterates is given by <ref type="bibr" target="#b97">[98]</ref> . ( , ) ( , ) , , , , .</p><formula xml:id="formula_113">arg min x ux x u x x i 0 12 s.t r x r i r 1 0 ! 6 f, # = +<label>(34)</label></formula><p>As illustrated in Figure <ref type="figure">9</ref>, the iterative approximation of the constraints is a restriction of the constraints and hence the iterates remain feasible during the algorithm. If, in addition, some constraint qualification conditions are satisfied, the resulting algorithm is to converge to the set of stationary solutions of <ref type="bibr" target="#b32">(33)</ref>; see <ref type="bibr" target="#b37">[38,</ref><ref type="bibr">Th. 1]</ref> for detailed conditions and analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>parallel verSiOn and extenSiOnS tO game theOry</head><p>With the recent advances in multicore and cluster computational platforms, it is desirable to design "parallel" algorithms for multiblock optimization problem where multiple cores update the block variables in parallel to optimize the objective function. A naive parallel extension of the BCD approach for solving (1) is to update all blocks (or a subset of them) in parallel by solving ( , ), , , . arg min</p><formula xml:id="formula_114">x fx x i n 1 i r x i i r 1 X i i ! 6 = ! + -</formula><p>Unfortunately, this naive extension of the BCD algorithm does not converge, in general, and might result in a zigzag/oscillating or divergent behavior. As an example, consider the problem . ( ) , .</p><formula xml:id="formula_115">min x x x x 1 1 s.t ( , ) x x 1 2 2 1 2 1 2 # # - -</formula><p>Clearly, this problem is convex with bounded feasible set and its optimal value is zero. However, the above naive parallel extension of the algorithm leads to the following iteration path:</p><formula xml:id="formula_116">( , ) ( , ) ( , ) ( , ) ( , ) ( , ) , x x x x x x 1 1 1 1 1 1 1 0 2 0 1 1 2 1 1 2 2 2 " " " g = - = - = -</formula><p>which is clearly not convergent. This is caused by aggressive steps used in the algorithm. To make the algorithm convergent, it is then necessary to employ controlled steps that are also small enough. Furthermore, in the case of nonconvex objective function ( ) f $ in (1), the approximation functions could be again used to obtain computationally efficient update rules. The resulting algorithm, dubbed parallel successive convex approximation (PSCA), is summarized in Table <ref type="table">7</ref>. To see the convergence analysis of this algorithm and other related algorithms such as the flexible parallel algorithm, refer to <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b98">[99]</ref>, <ref type="bibr" target="#b99">[100]</ref>, and the references therein.</p><p>Notice that PSCA can be viewed as a way of solving a multiagent optimization problem where multiple agents/users try to optimize a common objective by updating their own variable iteratively. Furthermore, it can be used in a game-theoretic setting where each player in the game utilizes the best response strategy by optimizing a locally tight upper bound of its own utility function. This algorithm is guaranteed to converge for some particular class of games under some regularity assumptions on the players' utility functions. The convergence analysis presented in <ref type="bibr" target="#b100">[101]</ref> is based on certain contraction approach as well as monotone convergence for potential games.</p><p>Figure <ref type="figure" target="#fig_5">10</ref> illustrates the behavior of the cyclic and randomized parallel PSCA method as compared with their serial counterparts (i.e., the "cyclic BCD" and the "randomized BCD") applied to the LASSO problem. The performance of the PSCA method is also illustrated for a different number of processors and various block selection rules. In Figure <ref type="figure" target="#fig_5">10</ref>(a) and (b), parallelization can result in more efficient algorithm; however, the convergence speed does not grow linearly with the number of processing cores. Moreover, increasing the number of processors beyond certain point results in slower convergence, which can be attributed to the increased communication overhead among the nodes.</p><p>Note that the parallel update rule is very useful in dealing with distributed data sets. Consider solving the LASSO problem with the following objective:</p><p>. Ax b x 2 1 m -+ Assume we have q processing cores each having their own memory. Let us partition the matrix A and vector x into q blocks: .</p><formula xml:id="formula_117">[ , , ] [ , , ] A A A x x x and q T q T T 1 1 f g = =</formula><p>If PSCA is implemented in a way that each core j is only responsible for updating [FIg9] an illustration of constraint convexification. <ref type="table">7</ref>] a PSeudocode oF The PSca algorIThM.  </p><formula xml:id="formula_118">x r f i (x ) â‰¤ 0 u i (x, x r ) â‰¤ 0 [TaBle</formula><formula xml:id="formula_119">i r x i i r 1 X i i = ! - t i I r 6 ! 5 set , x x k I k r k r r 1 6 " = - 6 set ( ) , x x x x i I i r i r r i r i r r 1 1 6 ! c = + - - - t<label>7</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>/</head><p>Notice that the value of b j l can be calculated by letting each node i compute the value of A x i i and broadcast it to other nodes. Under this architecture, each node does not need to know the complete matrix A and only local information is enough for a distributed implementation of the PSCA method.</p><p>praCtiCal COnSideratiOnS There are a number of factors that we need to consider when using the BSUM framework. The first consideration is about the choice of the upper bound. What is a good bound for a given application? The answer is generally problem dependent, as we have already seen in a few examples. The general guideline is that a good upper bound should be able to ensure algorithm convergence, best exploit the problem structure, and make the subproblems easily solvable (preferably in closed form). For example, a simple proximal upper bound is not likely to perform well for the transceiver design problem discussed in the section "Wireless Communication and Transceiver Design" as the resulting subproblems will not decompose over the variables.</p><p>The second consideration is about the choice of the block update rules. As we have seen in the section "How Fast Does the BSUM Converge?" different update rules lead to quite distinct convergence behavior. For convex problems, deterministic rules such as the cyclic rule promise the worst-case rates, while the randomized rule ensures convergence rate in either averaged or high-probability sense. Further, there is barely any theoretical rate analysis for nonconvex problems, regardless of the block selection rules. Therefore, the best strategy in practice is to perform an extensive numerical study and pick the best rule for the application at hand. For example, researchers have found that the MBI rule is effective for certain tensor decomposition problems <ref type="bibr" target="#b12">[13]</ref>; the cyclic rule can be superior to the randomized rule for certain LASSO problems, and certain G-So rules can outperform the randomized rule <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b13">[14]</ref>.</p><p>The third consideration is about the choice of the parallelization schemes. There has been extensive research on parallelizing various special cases and variations of the BSUM type algorithm; see <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b98">[99]</ref>, <ref type="bibr" target="#b102">[103]</ref>- <ref type="bibr" target="#b105">[106]</ref>, and the references therein. These algorithms differ in a number of implementation details and in their applicability. For example, most of the implementations use randomized block selection rules to pick the variable blocks, while <ref type="bibr" target="#b98">[99]</ref> and <ref type="bibr" target="#b104">[105]</ref> additionally use certain variations of the G-So rule. The majority of the schemes only work for convex problems, with the exception of <ref type="bibr" target="#b98">[99]</ref> and <ref type="bibr" target="#b102">[103]</ref>, which work for general nonsmooth and nonconvex problems in the form of <ref type="bibr" target="#b4">(5)</ref>. When assessing whether a given problem is suitable for parallelization, it is important to know that oftentimes the number of blocks that can be updated in parallel is data dependent. For example, when solving LASSO problems, it is shown in <ref type="bibr" target="#b104">[105]</ref> and <ref type="bibr" target="#b105">[106]</ref> that the degree of parallelization is dependent on the maximum eigenvalue of certain submatrices of the data matrix. Some recent results <ref type="bibr" target="#b106">[107]</ref> show that for a certain randomized coordinate descent method, such dependency could be mild. For solving general Cyclic BCD Randomized BCD Cyclic PSCA q = 32 Randomized PSCA q = 32 Cyclic PSCA q = 8 Randomized PSCA q = 8 Cyclic PSCA q = 4 Randomized PSCA q = 4 Randomized BCD Cyclic BCD Randomized PSCA q = 8 Cyclic PSCA q = 8</p><p>Randomized PSCA q = 16 Cyclic PSCA q = 16 Randomized PSCA q = 32 Cyclic PSCA q = 32 convex nonsmooth problems, <ref type="bibr" target="#b103">[104]</ref> shows that the step size should be carefully selected based on both the "separability" of the problem (or the sparsity of the data matrix) as well as the degree of parallelization. If the application at hand does not satisfy these conditions, the alternatives usually are: 1) exploit the problem structure and pick a good upper bound, so that the subproblems are decomposable, leading to parallel and step-size-free updates; see, for example, the NMF problem in the section "The NMF Algorithm" and the WMMSE algorithm in the section "Wireless Communication and Transceiver Design"; 2) to use the diminishing stepsizes for updating the blocks; see, for example, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b98">[99]</ref>, and <ref type="bibr" target="#b102">[103]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ISSueS and oPen reSearch ProBleMS</head><p>This article presents a comprehensive algorithmic framework, BSUM, for block-structured large-scale optimization. The main strength of the BSUM framework is its strong theoretical convergence guarantee and its flexibility. As demonstrated in this article, the BSUM framework covers a number of well-known but seemingly unrelated algorithms as well as their new extensions. Moreover, it is amenable to a number of different data models as well as to parallel implementation on modern multicore computing platforms.</p><p>To close, we briefly highlight a couple of issues and open research topics related to the BSUM framework.</p><p>â–  Communication delay and overhead in parallel implementations: As discussed in the section "Parallel Version and Extensions to Game Theory," the convergence speed of the parallel version of the BSUM framework does not increase linearly with the number of computational nodes. In fact, after a point, increasing the number of computational nodes can lead to a slower convergence speed. As mentioned previously, this is due to the delay caused by communication among the nodes. This observation gives rise to two important research questions: First, given the maximum allowable number of computation nodes and the communication overhead of the nodes, what is the optimum choice of the number of cores for solving a given optimization problem? Answering this question requires computation/communication tradeoff analysis of the proposed optimization approach. Second, can the BSUM framework be extended and implemented in a (semi)asynchronous manner? If this is possible, then the communication overhead can be reduced significantly since nodes are not required to wait for each other before updating the variables, making the algorithm lock-free. For recent efforts on this research direction see <ref type="bibr" target="#b16">[17]</ref>.</p><p>â–  Nonlinear coupling constraints: As we observe in the "Extensions" section, the BSUM framework can also be used in the presence of linear coupling or nonconvex decoupled constraints. How can the BSUM framework be generalized to problems with nonlinear coupling constraints? More precisely, can the BSUM framework with block-wise update rules be applied to the optimization problem of the following form? ( , , ) min f x xn 0 1</p><p>x f . ( , , ) , , , , .</p><formula xml:id="formula_120">f x x i n 0 1 2 s.t i n 1 f 6 f # =</formula><p>Example 4 shows that the naive extension of the BCD approach fails to find the optimal solution even in the convex setting. A popular approach to tackle the aforementioned problem is to place the constraints in the objective using Lagrange multipliers and update the multipliers iteratively. However, this approach typically leads to double-loop algorithms and requires subgradient steps in the dual space, which is known to be slow. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>acknowledgMenTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>[[</head><label></label><figDesc>With applications in machine learning and signal processing ] A Unified Algorithmic Framework for Block-Structured Optimization Involving Big Data Digital Object Identifier 10.1109/MSP.2015.2481563 Date of publication: 29 December 2015 Â©is to ck ph ot o.c om / Vic to ria Ba r, Blo ck ima ge lic en se d By gr ap hic sto ck</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>g 3 =</head><label>3</label><figDesc>When applying the classical BCD method to solve (1), at every iteration , r a single block of varia-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>[</head><label></label><figDesc>FIg1] The Bcd method for a two-dimensional problem. The dashed curves represent the contours of the objective function, the solid lines represent the progress of the algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>as an approximation function of ( , ) f x z i i for each coordinate i at a given feasible point . z X ! Let us define a set I r (possibly with | | ) 1 I r 2 as the block-variable indices to be picked at iteration . r Then at each iteration , r the BSUM method performs the following simple update .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>a comparison of BSuM and Bcd methods for solving a two-dimensional problem. each time a single coordinate is picked for update. The blue dashed curves represent the contours of the objective function ( , ), resp. red) solid lines represent the progress of the Bcd (resp. BSuM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 +</head><label>1</label><figDesc>The upper-bound minimization step of the BSuM method is shown. here we assume that coordinate i is updated at iteration . r It is clear from the figure that after solving the BSuM subproblem (3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>minimum but not a stationary solution [21]. (Figure reprinted with permission from the Society for Industrial and applied Mathematics.) Example 6: Consider the following unconstrained problem</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>under the randomized block selection rule, the iterates generated by the BSUM algorithm converge to the set of stationary points almost surely, i.e.,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>2 v</head><label>2</label><figDesc>in transmitting one data stream to its own receiver. Let x C k M ! be the transmitted signal of user ;k assuming linear channel model, the received signal of user k can be written as , When linear beamformers are employed at the transmitters and receivers, the transmitted signal and the estimated received data stream can be respectively written as , the transmit and receive beamformers. Here the transmitted data stream and the estimated data stream at the receiver are denoted by s C</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>s inequality, it is not hard to check that (23) is a valid upper bound of<ref type="bibr" target="#b21">(22)</ref> in the BSUM framework. Moreover, (23) has a closed-form solution obtained by ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>where</head><label></label><figDesc>Np is the set of reads stored at processor p with .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>a comparison of the alS and proximal alS algorithm<ref type="bibr" target="#b20">[21]</ref>. The proximal alS algorithm is the BSuM approach using the proximal upper bound; see Table3. The "alS with diminishing proximal" algorithm utilizes a decreasing proximal coefficient during the iterates of the algorithm. In the example . with permission from the Society for Industrial and applied Mathematics.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 6</head><label>6</label><figDesc>Figure 6 and Table5show the performance of the resulting algorithm for dictionary learning in an image denoising problem. The denoising is performed on the Lena image corrupted by addi-</figDesc><graphic coords="13,55.43,76.44,103.70,103.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>[TaBle 5 ]</head><label>5</label><figDesc>The IMage denoISIng reSulT coMParISon on The "lena IMage" For dIFFerenT noISe levelS. valueS are averaged over Ten MonTe carlo SIMulaTIonS [78].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>[</head><label></label><figDesc>FIg7] an iteration and running time comparison of SSuM versus Saa<ref type="bibr" target="#b19">[20]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>is the dual step size at iteration . r The update orders for the primal and dual variables could be either cyclic or randomized.Similar to the BSUM framework, one can replace the augmented Lagrangian function (â€¢, ; )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>r</head><label></label><figDesc>being a locally tight approximation of the function (â€¢, ) f x i r -around the point xi r satisfying Assumption A. The resulting algorithm, named the BSUM method of multipliers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>[</head><label></label><figDesc>FIg8] a wireline network consists of five subnetworks. each of them is controlled by an nc, and these ncs are coordinated globally by a central nc 0 [95].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>[</head><label></label><figDesc>FIg10] a comparison of the serial Bcd with PSca method for solving the laSSo problem: min Ax b A and vector b are generated according to<ref type="bibr" target="#b101">[102]</ref>; and q denotes the number of processors used in each experiment. The dimension of A is ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TaBle 2 ] The coMMonly uSed coordInaTe SelecTIon ruleS.</head><label>2</label><figDesc></figDesc><table><row><cell cols="3">at each iteration , r deFine a set oF auxiliary variables { } xi r t</cell><cell>i n =</cell><cell>1</cell><cell cols="2">as:</cell><cell></cell><cell></cell></row><row><cell>x t</cell><cell>i r</cell><cell>!</cell><cell cols="4">argmin x X i i !</cell><cell cols="3">, u x x i i ^h , r 1 -</cell><cell>i</cell><cell>=</cell><cell>1</cell><cell>, , . n g</cell></row><row><cell cols="10">then We have the FolloWing commonly used coordinate selection rules.</cell></row><row><cell cols="10">â€¢ CyCliC rule: the coordinates are chosen cyclically, i.e., in the order oF , , , , , , . n 1 2 1 2 g g</cell></row><row><cell cols="10">â€¢ essentially CyCliC (e-C) rule: there exists a given period T 1 $ during Which each block is updated at least once, i.e.,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>T</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">I r i +</cell><cell>=</cell><cell>{ , , }, . n r 1 g 6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>i</cell><cell>=</cell><cell>1</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TaBle 4] oPTIMalITy condITIonS.</head><label></label><figDesc></figDesc><table><row><cell cols="20">â€¢ proxiMal upper bound: given a constant</cell><cell cols="2">c</cell><cell cols="2">2</cell><cell>0</cell><cell>,</cell><cell>one can construct a bound by adding a quadratic penalization (i.e., the</cell></row><row><cell cols="5">proximal term)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>( , ): ( , ) u x z f x z i i i i = -</cell><cell>+</cell><cell>2 c</cell><cell>x z i -</cell><cell>i</cell><cell>2</cell><cell>.</cell></row><row><cell cols="24">â€¢ QuadratiC upper bound: suppose ( ) f x g x ( , , ) x n 1 g =</cell><cell>+</cell><cell>( , , ), h x x n 1 g</cell><cell>Where g is smooth With Hi as the hessian matrix For the i th block. then</cell></row><row><cell cols="21">one can construct the FolloWing bound</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">( , ): u x z i i</cell><cell cols="2">=</cell><cell>( , ) g z z i i -</cell><cell>+</cell><cell>( , ) h x z i i -</cell><cell>+</cell><cell>G</cell><cell>d</cell><cell>i</cell><cell>( , ), gz z x z i i i --</cell><cell>i</cell><cell>H</cell><cell>+</cell><cell>2 1</cell><cell>(</cell><cell>x z i -</cell><cell>i</cell><cell>)</cell><cell>T</cell><cell>U</cell><cell>( x z i i -</cell><cell>i</cell><cell>) ,</cell></row><row><cell cols="6">Where both i U and</cell><cell cols="2">i U -</cell><cell cols="2">H</cell><cell>i</cell><cell cols="13">are positive semideFinite matrices.</cell></row><row><cell cols="24">â€¢ linear upper bound: suppose f is diFFerentiable and concave, then one can construct</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>( , ): ( , ) u x z f z z i i i i = -</cell><cell>+</cell><cell>G</cell><cell>d</cell><cell>i</cell><cell>( , ), f z z x z i i i --</cell><cell>i</cell><cell>H</cell><cell>.</cell></row><row><cell cols="24">â€¢ Jensen's upper bound: suppose ( ): ( f x f a x T 1 1 g , , = w R i mi ! + denote a Weight vector With w i 1 = a x n T . 1</cell><cell>), then one can use jensen's inequality and construct n Where a R i mi ! is a coeFFicient vector, and f is convex With respect to each</cell><cell>a x T i</cell><cell>. i let</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>( , ) : u x z i i</cell><cell>=</cell><cell>j / m 1 i =</cell><cell>( ) w j f w j ( ) ( ) a j x j z j ( ) ( ) i i i i i -c ^h</cell><cell>+</cell><cell>, a z z i T i</cell><cell>i -</cell><cell>, m</cell></row><row><cell cols="2">Where</cell><cell cols="2">( ) w j i</cell><cell cols="20">represents the jth element in vector</cell><cell>wi</cell><cell>.</cell></row><row><cell cols="24">The main convergence result for the BSUM method is given</cell></row><row><cell cols="18">below, which is adapted from [21, Th. 2]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="24">Theorem 1: Suppose the cyclic coordinate selection rule is</cell></row><row><cell cols="4">chosen, i.e.,</cell><cell>I r =</cell><cell>{(</cell><cell>r</cell><cell cols="3">mod</cell><cell cols="5">) n 1 }. +</cell><cell cols="8">Let { } x r</cell><cell>r 1 3 = be a sequence</cell></row><row><cell cols="24">generated by the BSUM algorithm. Suppose Assumption A holds,</cell></row><row><cell cols="24">and that each x r is regular. Then the following is true:</cell></row><row><cell cols="20">a) Suppose that the function ( , ) u x y i i</cell><cell cols="4">is quasi-convex in xi</cell></row><row><cell>for</cell><cell>i</cell><cell>=</cell><cell cols="2">, , . n 1 f</cell><cell cols="19">Furthermore, assume that the subproblem</cell></row><row><cell cols="20">(3) has a unique solution for any point</cell><cell cols="2">x</cell><cell cols="2">r 1 ! -</cell><cell>X</cell><cell>.</cell><cell>Then every</cell></row><row><cell cols="24">limit point x * of { } x r is a stationary point of (1).</cell></row><row><cell cols="12">b) Suppose the level set</cell><cell cols="2">X 0</cell><cell>=</cell><cell cols="5">{ ( ) x f x</cell><cell></cell><cell cols="2">#</cell><cell>( )} f x 0</cell><cell>is compact.</cell></row><row><cell cols="24">Furthermore, assume that the subproblem (3) has a unique</cell></row><row><cell cols="10">solution for any point</cell><cell></cell><cell>x</cell><cell cols="3">r 1 ! -</cell><cell>X</cell><cell>,</cell><cell cols="7">r 1 $ for at least n 1 -</cell></row><row><cell cols="24">blocks. Then { } x r converge to the set of stationary points, i.e.,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">( , ) lim d x X * r r " 3</cell><cell cols="2">=</cell><cell>0</cell><cell>,</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">where ( , ) min d x X * _</cell><cell cols="4">x X * !</cell><cell>*</cell><cell cols="3">x x -</cell><cell>*</cell><cell cols="6">and X * is the set of sta-</cell></row><row><cell cols="5">tionary points.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[â€¢ stationary solutions: the point x * is a stationary solution oF</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(1) iF ( ; ) f x d * l</cell><cell>$</cell><cell>0</cell><cell>For all d such that</cell><cell>. x d X ! +</cell><cell>let X * denote</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>the set oF stationary solutions.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>â€¢ Coordinatewise MiniMuM solutions: x X ! t</cell><cell>is coordinateWise</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>minimum oF problem (1) With respect to the coordinates in</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>, R m m R 1</cell><cell>, , 2 f</cell><cell>R</cell><cell>, mn</cell><cell>iF</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>( f x d + t</cell><cell>) 0 k</cell><cell>$</cell><cell>( ), f x t</cell><cell>d 6</cell><cell>k</cell><cell>!</cell><cell>R</cell><cell>m</cell><cell>k</cell><cell>with</cell><cell>x d + t</cell><cell>k 0</cell><cell>!</cell><cell>, X</cell><cell>k 6</cell><cell>=</cell><cell>, , , , n 1 2 f</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Where</cell><cell>d</cell><cell>k 0</cell><cell>=</cell><cell>( , , , , ). d 0 0 k f f</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>EStIMAtION IN MOdERN HIGH-tHROuGHPut SEQuENCING tECHNOLOGIESAn essential step in the analysis of modern high throughput sequencing technologies of biological data is to estimate the abundance level of each transcript in the experiment. Mathematically, this problem can be stated as follows.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Consider M transcript</cell></row><row><cell cols="4">sequences , , s 1 g abundance levels s</cell><cell cols="2">{ , , , } A C G T , , M ! 1 g t M t such that L with the corresponding . 1 m m M 1 t = = / Let</cell></row><row><cell>R</cell><cell>, , 1 g</cell><cell>RN</cell><cell cols="3">be noisy sequencing reads originated from the tran-</cell></row><row><cell cols="5">script sequences, where each read , Rn</cell><cell>n</cell><cell>=</cell><cell>, , , N 1 g</cell><cell>is originated</cell></row><row><cell cols="6">from only one of the transcript sequences , , . s sM 1 g</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>M. Hong and M. Razaviyayn contributed equally to this work. M. Hong is supported by the National Science Foundation, grant CCF-1526078. Z.-Q. Luo is supported by the National Science Foundation, grant CCF-1526434. auThorS Mingyi Hong (mingyi@iastate.edu) received his B.E. degree in communications engineering from Zhejiang University, China, in 2005; his M.S. degree in electrical engineering from Stony Brook University, New York, in 2007; and his Ph.D. degree in systems engineering from the University of Virginia in 2011. He was previously a research assistant professor with the Department of Electrical and Computer Engineering, University of Minnesota, Twin Cities. He is currently an assistant professor with the Department of Industrial and Manufacturing Systems Engineering, Iowa State University, Ames. His research interests include signal processing, wireless communications, large-scale optimization and its applications in compressive sensing, complex networks, and high-dimensional data analysis. Meisam Razaviyayn (meisamr@stanford.edu) received his B.S. degree in electrical engineering from Isfahan University of Technology, Iran, in 2008; his M.S. degrees in mathematics and electrical engineering from the University of Minnesota in 2013; and his Ph.D. degree in electrical engineering from the University of Minnesota in 2014. He is currently a postdoctoral research fellow in the Electrical Engineering Department at Stanford University, California. His research interests include large-scale optimization, machine learning and statistics signal processing, and wireless communication and its computational issues. Zhi-Quan Luo (luozq@umn.edu) received his B.S. degree in applied mathematics in 1984 from Peking University, Beijing, China. He received his Ph.D. degree in operations research in 1989 from the Massachusetts Institute of Technology. From 1989 to 2003, he held a faculty position with the Department of Electrical and Computer Engineering, McMaster University, Hamilton, Canada, where he eventually became the department head and held a Canada research chair in information processing. Since 2003, he has been with the Department of Electrical and Computer Engineering at the University of Minnesota, Twin Cities as a full professor and holds an endowed ADC chair in digital technology. Since 2014, he has been with the Chinese University of Hong Kong (Shenzhen) as the vice president (academic). His research interests include optimization algorithms, signal processing, and digital communication. Jong-Shi Pang (jongship@usc.edu) received his B.S. degree in mathematics from National Taiwan University in 1973 and his</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Ph.D. degree in operations research from Stanford University, California, in 1977. He is the Epstein family chair professor in the Department of Industrial and Systems Engineering, University of Southern California. He was the head and Caterpillar professor in the Department of Industrial and Enterprise Systems Engineering at the University of Illinois at Urbana-Champaign. He was previously the Margaret A. Darrin Distinguished Professor in applied mathematics at Rensselaer Polytechnic Institute in Troy, New York. He has received several awards and honors. He was the member of the inaugural 2009 class of the SIAM fellows. He is an ISI highly cited author in the mathematics category. His research interests are in continuous optimization and equilibrium programming and their applications in engineering, economics, and finance. reFerenceS</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Signal processing of social networks: Interactive sensing and learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Namvar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hamdi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Signal Process</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Special Issue on Signal Processing for Big Data</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Mag</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2014-09">Sept. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<author>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optimization for Machine Learning</title>
		<meeting><address><addrLine>MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Streaming fragment assignment for real-time analysis of sequencing experiments</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pachter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="73" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficiency of coordinate descent methods on huge-scale optimization problems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="341" to="362" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convex optimization for big data: Scalable, randomized, and parallel algorithms for big data analytics</title>
		<author>
			<persName><forename type="first">V</forename><surname>Cevher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="32" to="43" />
			<date type="published" when="2014-09">Sept. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">First order methods for large-scale sparse optimization</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Aybat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>Columbia Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function</title>
		<author>
			<persName><forename type="first">P</forename><surname>RichtÃ¡rik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>TakÃ¡c</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ë‡</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Big Data over Networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Moura</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Cambridge Univ. Press</publisher>
			<pubPlace>Cambridge, U.K.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A regularization approach to joint blur identification and image restoration</title>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaveh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="416" to="428" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Regularization paths for generalized linear models via coordinate descent</title>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Stat. Softw</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An iteratively weighted MMSE approach to distributed sum-utility maximization for a MIMO interfering broadcast channel</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Razaviyayn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4331" to="4340" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Maximum block improvement and polynomial optimization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="107" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Coordinate descent converges faster with the Gauss-Southwell rule than random selection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nutini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Laradji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Friedlander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Koepke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 30th Int. Conf. Machine Learning (ICML)</title>
		<meeting>30th Int. Conf. Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1632" to="1641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Parallel coordinate descent methods for big data optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>RichtÃ¡rik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>TakÃ¡c</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ë‡</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint/>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Flexible parallel algorithms for big data optimization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Facchinei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sagratella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Scutari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2014 IEEE Int. Conf. Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>2014 IEEE Int. Conf. Acoustics, Speech and Signal essing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="7208" to="7212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An asynchronous parallel stochastic coordinate descent algorithm</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bittorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Machine Learning (ICML)</title>
		<meeting>Int. Conf. Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Parallel successive convex approximation for nonsmooth nonconvex optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Razaviyayn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing</title>
		<meeting>Neural Information essing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Parallel selective algorithms for nonconvex big data optimization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Facchinei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Scutari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sagratella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1874" to="1889" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A stochastic successive minimization method for nonsmooth nonconvex optimization with applications to transceiver design in wireless communication networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Razaviyayn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sanjabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A unified convergence analysis of block successive minimization methods for nonsmooth optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Razaviyayn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1126" to="1153" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Optimization with first-order surrogate functions</title>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Machine Learning (ICML)</title>
		<meeting>Int. Conf. Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="783" to="791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A fast iterative shrinkage-thresholding algorithm for linear inverse problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Imaging Sci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="183" to="202" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gradient projection for sparse reconstruction: Application to compressed sensing and other inverse problems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A T</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="586" to="597" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exact matrix completion via convex optimization</title>
		<author>
			<persName><forename type="first">E</forename><surname>CandÃ¨s</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="111" to="119" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Guaranteed matrix completion via nonconvex factorization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Algorithms for nonnegative matrix and tensor factorizations: A unified view based on block coordinate descent framework</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Glob Optim</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="285" to="319" />
			<date type="published" when="2014-02">Feb. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Tensor decompositions and applications</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Bader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="455" to="500" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Coordinate descent algorithms</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="34" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Series B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The concave-convex procedure</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rangarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="915" to="936" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Algorithms for non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems (NIPS)</title>
		<meeting>Neural Information essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="556" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Monotonic convergence of distributed interference pricing in wireless networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Honig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Symp. Information Theory</title>
		<meeting>IEEE Int. Conf. Symp. Information Theory</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1619" to="1623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Signal processing and optimal resource allocation for the interference channel</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Academic Press Library in Signal Processing</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Decomposition by successive convex approximation: A unifying approach for linear transceiver design in interfering heterogeneous networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Wireless Commun</title>
		<imprint/>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Convergence of a block coordinate descent method for nondifferentiable minimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Optim. Theory Appl</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="475" to="494" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On search directions for minimization algorithms</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J D</forename><surname>Powell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="193" to="201" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Successive convex approximation: Analysis and applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Razaviyayn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>Univ. of Minnesota</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On the complexity analysis of randomized block-coordinate descent methods</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="615" to="642" />
			<date type="published" when="2015-08">Aug. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On the convergence of block coordinate descent type methods</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tetruashvili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2037" to="2060" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On the convergence of alternating minimization with applications to iteratively reweighted least squares and decomposition schemes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="185" to="209" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Iteration complexity analysis of block coordinate descent methods</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Razaviyayn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">On the nonasymptotic convergence of cyclic coordinate descent method</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="576" to="601" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">A block successive upper bound minimization method of multipliers for linearly constrained convex optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Razaviyayn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On the linear convergence of approximate proximal splitting methods for non-smooth convex minimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kadkhodaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sanjabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Oper. Res. Soc. China</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="141" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A block coordinate descent method for regularized multiconvex optimization with applications to nonnegative tensor factorization and completion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Imaging Sci</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1758" to="1789" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">An accelerated proximal coordinate gradient method and its application to regularized empirical risk minimization</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint/>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Accelerated, parallel and proximal coordinate descent</title>
		<author>
			<persName><forename type="first">O</forename><surname>Fercoq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>RichtÃ¡rik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1927" to="2023" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A tutorial on MM algorithms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lange</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. Stat</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Parallel and Distributed Computation: Numerical Methods, 2nd ed</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Athena-Scientific</publisher>
			<pubPlace>Belmont, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">On the convergence of the block nonlinear Gauss-Seidel method under convex constraints</title>
		<author>
			<persName><forename type="first">L</forename><surname>Grippo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sciandrone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oper. Res. Lett</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="127" to="136" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Proximal splitting methods in signal processing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Combettes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Pesquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fixed-Point Algorithms for Inverse Problems in Science and Engineering</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="185" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Randomized block coordinate non-monotone gradient method for a class of nonlinear programming</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Online dictionary learning for sparse coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th Annu. Int. Conf. Machine Learning (ICML)</title>
		<meeting>26th Annu. Int. Conf. Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">On the convergence of Lee-Seung&apos;s multiplicative update for NMF</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>Tech. Note</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Algorithms for nonnegative matrix factorization with the beta-divergence</title>
		<author>
			<persName><forename type="first">C</forename><surname>FÃ©votte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Idier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Iteratively reweighted least squares minimization for sparse recovery</title>
		<author>
			<persName><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Devore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fornasier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Gunturk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Pure Appl. Math</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Dynamic spectrum management: Complexity and duality</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">- Q</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="73" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Cross-layer provision of future cellular networks: A WMMSE-based approach</title>
		<author>
			<persName><forename type="first">H</forename><surname>Baligh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Razaviyayn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sanjabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Mag</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="56" to="68" />
			<date type="published" when="2014-11">Nov. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Joint user grouping and transceiver design in a mimo interfering broadcast channel</title>
		<author>
			<persName><forename type="first">M</forename><surname>Razaviyayn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Baligh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Callard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="85" to="94" />
			<date type="published" when="2014-01">Jan. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Weighted sum-rate maximization using weighted MMSE for MIMO-BC beamforming design</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Cioffi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Wireless Commun</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4792" to="4799" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Joint base station clustering and beamformer design for partial coordinated transmission in heterogenous networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Baligh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Areas Commun</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="226" to="240" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Distributed interference pricing with MISO channels</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Honig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 46th Annual Allerton Conf. Communication, Control, and Computing</title>
		<meeting>46th Annual Allerton Conf. Communication, Control, and Computing</meeting>
		<imprint>
			<date type="published" when="2008-09">2008. Sept. 2008</date>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Optimal resource allocation for MIMO Ad Hoc Cognitive Radio Networks</title>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Giannakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3117" to="3131" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Linear precoding in cooperative MIMO cellular networks with limited coordination clusters</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T K</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Areas Commun</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1446" to="1454" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Ultrafast and memoryefficient alignment of short dna sequences to the human genome</title>
		<author>
			<persName><forename type="first">B</forename><surname>Langmead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Trapnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Salzberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Fast and accurate short read alignment with Burrows-Wheeler transform</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Durbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1754" to="1760" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Near-optimal RNA-Seq quantification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Melsted</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pachter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Non-negative tensor factorization with applications to statistics and computer vision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hazan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd Int. Conf. Machine Learning (ICML)</title>
		<meeting>22nd Int. Conf. Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="792" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Application of the three-way decomposition for matrix compression</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ibraghimov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numer. Linear Algebra Appl</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6-7</biblScope>
			<biblScope unit="page" from="551" to="565" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Tensor rank is NP-complete</title>
		<author>
			<persName><forename type="first">J</forename><surname>HÃ¥stad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Algorithms</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="644" to="654" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Analysis of individual differences in multidimensional scaling via an N-way generalization of &apos;Eckart-Young&apos; decomposition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="283" to="319" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Foundations of the parafac procedure: Models and conditions for an &quot;explanatory&quot; multimodal factor analysis</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">UCLA Working Papers in Phonetics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1" to="84" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Swamp reducing technique for tensor decomposition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Navasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">De</forename><surname>Lathauwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kindermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th European Signal Processing Conf. (EUSIP-CO)</title>
		<meeting>16th European Signal essing Conf. (EUSIP-CO)</meeting>
		<imprint>
			<date type="published" when="2008-08">Aug. 2008</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Rank regularization and Bayesian inference for tensor completion and extrapolation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bazerque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mateos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Giannakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">21-24</biblScope>
			<biblScope unit="page" from="5689" to="5703" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Compressed sensing</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1289" to="1306" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>CandÃ¨s</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Romberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="489" to="509" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Dictionary learning for sparse representation: Complexity and algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Razaviyayn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-W</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech and Signal Processing</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="5247" to="5251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Efficient sparse coding algorithms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>Advances in Neural Information essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="801" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4311" to="4322" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Mclachlan</surname></persName>
		</author>
		<title level="m">Discriminant Analysis and Statistical Pattern Recognition</title>
		<meeting><address><addrLine>Hoboken, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">544</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Penalized classification using Fisher&apos;s linear discriminant</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Series B (Stat. Methodol.)</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="753" to="772" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">A guide to sample average approximation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pasupathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Series in Operations Research and Management Science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Fu</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">216</biblScope>
			<biblScope unit="page" from="207" to="243" />
		</imprint>
	</monogr>
	<note>Handbook of Simulation Optimization</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Stochastic majorization-minimization algorithms for large-scale optimization</title>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2283" to="2291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">A stochastic weighted MMSE approach to sum rate maximization for a MIMO interference channel</title>
		<author>
			<persName><forename type="first">M</forename><surname>Razaviyayn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sanjabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 14th Workshop on Signal Processing Advances in Wireless Communications (SPAWC)</title>
		<meeting>IEEE 14th Workshop on Signal essing Advances in Wireless Communications (SPAWC)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="325" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Online learning for matrix factorization and sparse coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="19" to="60" />
			<date type="published" when="2010-01">Jan. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Stochastic approximation vis-a-vis online learning for big data analytics</title>
		<author>
			<persName><forename type="first">K</forename><surname>Slavakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mateos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Giannakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Mag</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="124" to="129" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Distributed optimization and statistical learning via the alternating direction method of multipliers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="122" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Numerical Methods for Nonlinear Variational Problems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Glowinski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Convergence analysis of alternating direction method of multipliers for a family of nonconvex problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Razaviyayn</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>submitted for publication</note>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Alternating directions method of multipliers for l1-penalized zero variance discriminant analysis and principal component analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ames</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>submitted for publication</note>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">A random coordinate descent method on large-scale optimization problems with linear constraints</title>
		<author>
			<persName><forename type="first">I</forename><surname>Necoara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nestrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Glineur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Univ. Politehnica Bucharest</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Tech. Rep</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Large-scale randomizedcoordinate descent methods with non-separable linear constraints</title>
		<author>
			<persName><forename type="first">S</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hefny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">An asynchronous distributed proximal method for composite convex optimization</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Aybat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Iyengar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 32th Int. Conf. Machine Learning (ICML)</title>
		<meeting>32th Int. Conf. Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2454" to="2462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Semi-asynchronous routing for large-scale hierarchical networks</title>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Farmanbar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2894" to="2898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Gurobi Optimizer Reference Manual</title>
		<ptr target="http://www.gurobi.com/documentation/6.0/refman/" />
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Gurobi Optimization, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Relaxation methods for network flow problems with convex arc costs</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hosein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Control Optim</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1219" to="1243" />
			<date type="published" when="1987-09">Sept. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">A general inner approximation algorithm for nonconvex mathematical programs</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oper. Res</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="681" to="683" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Flexible parallel algorithms for big data optimization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Facchinei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sagratella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Scutari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Decomposition by partial linearization: Parallel optimization of multi-agent systems</title>
		<author>
			<persName><forename type="first">G</forename><surname>Scutari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Facchinei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Palomar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="641" to="656" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">A unified distributed algorithm for non-cooperative games with non-convex and non-differentiable objectives</title>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Razaviyayn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Gradient methods for minimizing composite functions</title>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="125" to="161" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Parallel successive convex approximation for nonsmooth nonconvex optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Razaviyayn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>Advances in Neural Information essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">Distributed coordinate descent method for learning with big data</title>
		<author>
			<persName><forename type="first">P</forename><surname>RichtÃ¡rik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>TakÃ¡c</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ë‡</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Parallel and distributed sparse optimization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 2013 Asilomar Conf. Signals, Systems and Computers</title>
		<meeting>IEEE 2013 Asilomar Conf. Signals, Systems and Computers</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="659" to="646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Feature clustering for accelerating parallel coordinate descent</title>
		<author>
			<persName><forename type="first">C</forename><surname>Scherrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Halapanavar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Haglin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advanced in Neural Information Processing Systems (NIPS)</title>
		<meeting>Advanced in Neural Information essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="28" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Fast distributed coordinate descent for non-strongly convex losses</title>
		<author>
			<persName><forename type="first">O</forename><surname>Fercoq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>RichtÃ¡rik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>TakÃ¡c</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ë‡</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Workshop on Machine Learning for Signal Processing</title>
		<meeting>IEEE Int. Workshop on Machine Learning for Signal essing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
