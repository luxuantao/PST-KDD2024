<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Embedding Learning via Invariant and Spreading Instance Feature</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mang</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong Baptist University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Pong</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
							<email>pcyuen@comp.hkbu.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong Baptist University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Embedding Learning via Invariant and Spreading Instance Feature</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper studies the unsupervised embedding learning problem, which requires an effective similarity measurement between samples in low-dimensional embedding space. Motivated by the positive concentrated and negative separated properties observed from category-wise supervised learning, we propose to utilize the instance-wise supervision to approximate these properties, which aims at learning data augmentation invariant and instance spreadout features. To achieve this goal, we propose a novel instance based softmax embedding method, which directly optimizes the 'real' instance features on top of the softmax function. It achieves significantly faster learning speed and higher accuracy than all existing methods. The proposed method performs well for both seen and unseen testing categories with cosine similarity. It also achieves competitive performance even without pre-trained network over samples from fine-grained categories.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep embedding learning is a fundamental task in computer vision <ref type="bibr" target="#b13">[14]</ref>, which aims at learning a feature embedding that has the following properties: 1) positive concentrated, the embedding features of samples belonging to the same category are close to each other <ref type="bibr" target="#b31">[32]</ref>; 2) negative separated, the embedding features of samples belonging to different categories are separated as much as possible <ref type="bibr">[52]</ref>. Supervised embedding learning methods have been studied to achieve such objectives and demonstrate impressive capabilities in various vision tasks <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b51">53]</ref>. However, annotated data needed for supervised methods might be difficult to obtain. Collecting enough annotated data for different tasks requires costly human efforts and special domain expertise. To address this issue, this paper tackles the unsupervised embedding learning problem (a.k.a. unsupervised metric learning in <ref type="bibr" target="#b20">[21]</ref>), which aims at learning discriminative embedding features without human annotated labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN Input Images</head><p>Output Features Unsupervised embedding learning usually requires that the similarity between learned embedding features is consistent with the visual similarity or category relations of input images. In comparison, general unsupervised feature learning usually aims at learning a good "intermediate" feature representation from unlabelled data <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34]</ref>. The learned feature is then generalized to different tasks by using a small set of labelled training data from the target task to fine-tune models (e.g., linear classifier, object detector, etc.) for the target task <ref type="bibr" target="#b2">[3]</ref>. However, the learned feature representation may not preserve visual similarity and its performance drops dramatically for similarity based tasks, e.g. nearest neighbor search <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b49">50]</ref>.</p><p>The main challenge of unsupervised embedding learning is to discover visual similarity or weak category information from unlabelled samples. Iscen et al. <ref type="bibr" target="#b20">[21]</ref> proposed to mine hard positive and negative samples on manifolds. However, its performance heavily relies on the quality of the initialized feature representation for label mining, which limits the applicability for general tasks. In this paper, we propose to utilize the instance-wise supervision to approximate the positive concentrated and negative separated properties mentioned earlier. The learning process only relies on instance-wise relationship and does not rely on relations between pre-defined categories, so it can be well generalized to samples of arbitrary categories that have not been seen before (unseen testing categories) <ref type="bibr" target="#b11">[12]</ref>.</p><p>For positive concentration: it is usually infeasible to mine reliable positive information with randomly initialized network. Therefore, we apply a random data augmentation (e.g., transformation, scaling) to each image instance and use the augmented image as a positive sample. In other words, features of each image instance under different data augmentations should be invariant. For negative separation: since unlabelled data are usually highly imbalanced <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b48">49]</ref>, the number of negative samples for each image instance is much larger than that of positive samples. Therefore, a small batch of randomly selected instances can be approximately treated as negative samples for each instance. With such assumption, we try to separate each instance from all the other sampled instances within the batch, resulting in a spread-out property <ref type="bibr">[52]</ref>. It is clear that such assumption may not always hold, and each batch may contain a few false negatives. However, through our extensive experiments, we observe that the spread-out property effectively improves the discriminability. In summary, our main idea is to learn a discriminative instance feature, which preserves data augmentation invariant and spread-out properties for unsupervised embedding learning, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>To achieve these goals, we introduce a novel instance feature-based softmax embedding method. Existing softmax embedding is usually built on classifier weights <ref type="bibr" target="#b7">[8]</ref> or memorized features <ref type="bibr" target="#b45">[46]</ref>, which has limited efficiency and discriminability. We propose to explicitly optimize the feature embedding by directly using the inner products of instance features on top of softmax function, leading to significant performance and efficiency gains. The softmax function mines hard negative samples and takes full advantage of relationships among all sampled instances to improve the performance. The number of instance is significantly larger than the number of categories, so we introduce a Siamese network training strategy. We transform the multi-class classification problem to a binary classification problem and use maximum likelihood estimation for optimization.</p><p>The main contributions can be summarized as follows:</p><p>‚Ä¢ We propose a novel instance feature-based softmax embedding method to learn data augmentation invariant and instance spread-out features. It achieves significantly faster learning speed and higher accuracy than all the competing methods.</p><p>‚Ä¢ We show that both the data augmentation invariant and instance spread-out properties are important for instance-wise unsupervised embedding learning. They help capture apparent visual similarity between samples and generalizes well on unseen testing categories.</p><p>‚Ä¢ The proposed method achieves the state-of-the-art performances over other unsupervised learning methods on comprehensive image classification and embedding learning experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>General Unsupervised Feature Learning. Unsupervised feature learning has been widely studied in literature. Existing works can be roughly categorized into three categories <ref type="bibr" target="#b2">[3]</ref>: 1) generative models, this approach aims at learning a parameterized mapping between images and predefined noise signals, which constrains the distribution between raw data and noises <ref type="bibr" target="#b45">[46]</ref>. Bolztmann Machines (RBMs) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b39">40]</ref>, Auto-encoders <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">42]</ref> and generative adversarial network (GAN) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> are widely studied. 2) Estimating Between-image Labels, it usually estimates between-image labels using the clustering technique <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b25">26]</ref> or kNN-based methods <ref type="bibr" target="#b40">[41]</ref>, which provide label information. Then label information and feature learning process are iteratively updated. 3) Self-supervised Learning, this approach designs pretext tasks/signals to generate "pseudo-labels" and then formulate it as a prediction task to learn the feature representations. The pretext task could be the context information of local patches <ref type="bibr" target="#b5">[6]</ref>, the position of randomly rearranged patches <ref type="bibr" target="#b30">[31]</ref>, the missing pixels of an image <ref type="bibr" target="#b33">[34]</ref> or the color information from gray-scale images <ref type="bibr" target="#b50">[51]</ref>. Some attempts also use video information to provide weak supervision to learn feature representations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b43">44]</ref>.</p><p>As we discussed in Section 1, general unsupervised feature learning usually aims at learning a good "intermediate" feature representation that can be well generalized to other tasks. The intermediate feature representation may not preserve visual similar property. In comparison, unsupervised embedding learning requires additional visual similarity property of the learned features.</p><p>Deep Embedding Learning. Deep embedding learning usually learns an embedding function by minimizing the intra-class variation and maximizing the inter-class variation <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref>. Most of them are designed on top of pairwise <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30]</ref> or triplet relationships <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b28">29]</ref>. In particular, several sampling strategies are widely investigated to improve the performance, such as hard mining <ref type="bibr" target="#b15">[16]</ref>, semihard mining <ref type="bibr" target="#b34">[35]</ref>, smart mining <ref type="bibr" target="#b12">[13]</ref> and so on. In comparison, softmax embedding achieves competitive performance without sampling requirement <ref type="bibr" target="#b17">[18]</ref>. Supervised learning has achieved superior performance on various tasks, but they still rely on enough annotated data.</p><p>Unsupervised Embedding Learning. According to the evaluation protocol, it can be categorized into two cases, 1) the testing categories are the same with the training categories (seen testing categories), and 2) the testing categories are not overlapped with the training categories (unseen testing categories). The latter setting is more challenging. Without category-wise labels, Iscen et al. <ref type="bibr" target="#b20">[21]</ref> proposed to mine hard positive and negative samples on manifolds, and then train the feature embedding with triplet loss. However, it heavily relies on the initialized representation for label mining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>Our goal is to learn a feature embedding network</p><formula xml:id="formula_0">f Œ∏ (‚Ä¢) from a set of unlabelled images X = {x 1 , x 2 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , x n }. f Œ∏ (‚Ä¢) maps the input image x i into a low-dimensional em- bedding feature f Œ∏ (x i ) ‚àà R d ,</formula><p>where d is the feature dimension. For simplicity, the feature representation f Œ∏ (x i ) of an image instance is represented by f i , and we assume that all the features are ‚Ñì 2 normalized, i.e. f i 2 = 1. A good feature embedding should satisfy: 1) the embedding features of visual similar images are close to each other; 2) the embedding features of dissimilar image instances are separated.</p><p>Without category-wise labels, we utilize the instancewise supervision to approximate the positive concentrated and negative seperated properties. In particular, the embedding features of the same instance under different data augmentations should be invariant, while the features of different instances should be spread-out. In the rest of this section, we first review two existing instance-wise feature learning methods, and then propose a much more efficient and discriminative instance feature-based softmax embedding. Finally, we will give a detailed rationale analysis and introduce our training strategy with Siamese network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Instance-wise Softmax Embedding</head><p>Softmax Embedding with Classifier Weights. Exemplar CNN <ref type="bibr" target="#b7">[8]</ref> treats each image as a distinct class. Following the conventional classifier training, it defines a matrix</p><formula xml:id="formula_1">W = [w 1 , w 2 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , w n ] T ‚àà R n√ód ,</formula><p>where the j-th column w j is called the corresponding classifier weight for the j-th instance. Exemplar CNN ensures that image instance under different image transformations can be correctly classified into its original instance with the learned weight. Based on Softmax function, the probability of sample x j being recognized as the i-th instance can be represented as</p><formula xml:id="formula_2">P (i|x j ) = exp(w T i f j ) n k=1 exp(w T k f j ) .<label>(1)</label></formula><p>At each step, the network pulls sample feature f i towards its corresponding weight w i , and pushes it away from the classifier weights w k of other instances. However, classifier weights prevent explicitly comparison over features, which results in limited efficiency and discriminability. Softmax Embedding with Memory Bank. To improve the inferior efficiency, Wu et al. <ref type="bibr" target="#b45">[46]</ref> propose to set up a memory bank to store the instance features f i calculated in the previous step. The feature stored in the memory bank is denoted as v i , which serves as the classifier weight for the corresponding instance in the following step. Therefore, the probability of sample x j being recognized as the i-th instance can be written as</p><formula xml:id="formula_3">P (i|x j ) = exp(v T i f j /œÑ ) n k=1 exp(v T k f j /œÑ ) , (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>where œÑ is the temperature parameter controlling the concentration level of the sample distribution <ref type="bibr" target="#b16">[17]</ref>. v T i f j measures the cosine similarity between the feature f j and the i-th memorized feature v i . For instance x i at each step, the network pulls its feature f i towards its corresponding memorized vector v i , and pushes it away from the memorized vectors of other instances. Due to efficiency issue, the memorized feature v i corresponding to instance x i is only updated in the iteration which takes x i as input. In other words, the memorized feature v i is only updated once per epoch. However, the network itself is updated in each iteration. Comparing the real-time instance feature f i with the outdated memorized feature v i would cumber the training process. Thus, the memory bank scheme is still inefficient.</p><p>A straightforward idea to improve the efficiency is directly optimizing over feature itself, i.e. replacing the weight {w i } or memory {v i } with f i . However, it is implausible due to two reasons: 1) Considering the probability P (i|x i ) of recognizing x i to itself, since f T i f i =1, i.e. the feature and "pseudo classifier weight" (the feature itself) are always perfectly aligned, optimizing the network will not provide any positive concentrated property; 2) It's impractical to calculate the feature of all the samples (f k , k = 1, . . . , n) on-the-fly in order to calculate the denominator in Eq. ( <ref type="formula" target="#formula_3">2</ref>), especially for large-scale instance number dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Softmax Embedding on 'Real' Instance Feature</head><p>To address above issues, we propose a softmax embedding variant for unsupervised embedding learning, which directly optimizes the real instance feature rather than classifier weights <ref type="bibr" target="#b7">[8]</ref> or memory bank <ref type="bibr" target="#b45">[46]</ref>. To achieve the goal that features of the same instance under different data augmentations are invariant, while the features of different instances are spread-out, we propose to consider 1) both the original image and its augmented image, 2) a small batch of randomly selected samples instead of the full dataset.</p><p>For each iteration, we randomly sample m instances from the dataset. To simplify the notation, without loss of generality, the selected samples are denoted by</p><formula xml:id="formula_5">{x 1 , x 2 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , x m }.</formula><p>For each instance, a random data augmentation operation T (‚Ä¢) is applied to slightly modify the original image. The augmented sample T (x i ) is denoted by xi , and its embedding feature f Œ∏ (x i ) is denoted by fi . Instead of considering the instance feature learning as a multiclass classification problem, we solve it as binary classification problem via maximum likelihood estimation (MLE). In particular, for instance x i , the augmented sample xi should be classified into instance i, and other instances x j , j = i shouldn't be classified into instance i. The probability of xi being recognized as instance i is defined by On the other hand, the probability of x j being recognized as instance i is defined by</p><formula xml:id="formula_6">P (i|x i ) = exp(f T i fi /œÑ ) m k=1 exp(f T k fi /œÑ ) . (<label>3</label></formula><formula xml:id="formula_7">) CNN FC L2 Norm FC L2 Norm CNN Low-dim Low-dim Embedding Space f 1 f 2 f 3 f 1 f 2 f 3 ùê± 1 ùê± 2 ùê± 3 ùê± 1 ùê± 2 ùê± 3 Data Augmentation Share Weights</formula><formula xml:id="formula_8">P (i|x j ) = exp(f T i f j /œÑ ) m k=1 exp(f T k f j /œÑ ) , j = i<label>(4)</label></formula><p>Correspondingly, the probability of x j not being recognized as instance i is 1 ‚àí P (i|x j ).</p><p>Assuming different instances being recognized as instance i are independent, the joint probability of xi being recognized as instance i and x j , j = i not being classified into instance i is</p><formula xml:id="formula_9">P i = P (i|x i ) j =i (1 ‚àí P (i|x j ))<label>(5)</label></formula><p>The negative log likelihood is given by</p><formula xml:id="formula_10">J i = ‚àí log P (i|x i ) ‚àí j =i log(1 ‚àí P (i|x j ))<label>(6)</label></formula><p>We solve this problem by minimizing the sum of the negative log likelihood over all the instances within the batch, which is denoted by</p><formula xml:id="formula_11">J = ‚àí i log P (i|x i ) ‚àí i j =i log(1 ‚àí P (i|x j )). (7)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Rationale Analysis</head><p>This section gives a detailed rationale analysis about why minimizing Eq. ( <ref type="formula" target="#formula_10">6</ref>) could achieve the augmentation invariant and instance spread-out feature. Minimizing Eq. ( <ref type="formula" target="#formula_10">6</ref>) can be viewed as maximizing Eq. ( <ref type="formula" target="#formula_6">3</ref>) and minimizing Eq. ( <ref type="formula" target="#formula_8">4</ref>).</p><p>Considering Eq. ( <ref type="formula" target="#formula_6">3</ref>), it can be rewritten as</p><formula xml:id="formula_12">P (i|x i ) = exp(f T i fi /œÑ ) exp(f T i fi /œÑ ) + k =i exp(f T k fi /œÑ ) ,<label>(8)</label></formula><p>Maximizing Eq. ( <ref type="formula" target="#formula_6">3</ref>) requires maximizing exp(f T i fi /œÑ ) and minimizing exp(f T k fi /œÑ ), k = i. Since all the features are ‚Ñì 2 normalized, maximizing exp(f T i fi /œÑ ) requires increasing the inner product (cosine similarity) between f i and fi , resulting in a feature that is invariant to data augmentation. On the other hand, minimizing exp(f T k fi /œÑ ) ensures fi and other instances {f k } are separated. Considering all the instances within the batch, the instances are forced to be separated from each other, resulting in the spread-out property.</p><p>Similarly, Eq. ( <ref type="formula" target="#formula_8">4</ref>) can be rewritten as,</p><formula xml:id="formula_13">P (i|x j ) = exp(f T i f j /œÑ ) exp(f T j f j /œÑ ) + k =j exp(f T k f j /œÑ ) ,<label>(9)</label></formula><p>Note that the inner product f T j f j is 1 and the value of œÑ is generally small (say 0.1 in the experiment). Therefore, exp(f T j f j /œÑ ) generally determines the value of the whole denominator. Minimizing Eq. ( <ref type="formula" target="#formula_8">4</ref>) means that exp(f T i f j /œÑ ) should be minimized, which aims at separating f j from f i . Thus, it further enhances the spread-out property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training with Siamese Network</head><p>We proposed a Siamese network to implement the proposed algorithm as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. At each iteration, m randomly selected image instances are fed into in the first branch, and the corresponding augmented samples are fed into the second branch. Note that data augmentation is also be used in the first branch to enrich the training samples. For implementation, each sample has one randomly augmented positive sample and 2N ‚àí 2 negative samples to compute Eq. ( <ref type="formula">7</ref>), where N is the batch size. The proposed training strategy greatly reduces the computational cost. Meanwhile, this training strategy also takes full advantage of relationships among all instances sampled in a mini-batch <ref type="bibr" target="#b31">[32]</ref>. Theoretically, we could also use a multibranch network by considering multiple augmented images for each instance in the batch. Methods kNN RandomCNN 32.1 DeepCluster (10) <ref type="bibr" target="#b2">[3]</ref> 44.4 DeepCluster (1000) <ref type="bibr" target="#b2">[3]</ref> 67.6 Exemplar <ref type="bibr" target="#b7">[8]</ref> 74.5 NPSoftmax <ref type="bibr" target="#b45">[46]</ref> 80.8 NCE <ref type="bibr" target="#b45">[46]</ref> 80. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>We have conducted the experiments with two different settings to evaluate the proposed method 1 . The first setting is that the training and testing sets share the same categories (seen testing category). This protocol is widely adopted for general unsupervised feature learning. The second setting is that the training and testing sets do not share any common categories (unseen testing category). This setting is usually used for supervised embedding learning <ref type="bibr" target="#b31">[32]</ref>. Following <ref type="bibr" target="#b20">[21]</ref>, we don't use any semantic label in the training set. The latter setting is more challenging than the former setting and it could apparently demonstrate the quality of learned features on unseen categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiments on Seen Testing Categories</head><p>We follow the experimental settings in <ref type="bibr" target="#b45">[46]</ref> to conduct the experiments on CIFAR-10 <ref type="bibr" target="#b22">[23]</ref> and STL-10 [4] datasets, where training and testing set share the same categories. Specifically, ResNet18 network <ref type="bibr" target="#b14">[15]</ref> is adopted as the backbone and the output embedding feature dimension is set to 128. The initial learning rate is set to 0.03, and it is decayed by 0.1 and 0.01 at 120 and 160 epoch. The network is trained for 200 epochs. The temperature parameter œÑ is set to 0.1. The algorithm is implemented on PyTorch with SGD optimizer with momentum. The weight decay parameter is 5√ó10 ‚àí4 and momentum is 0.9. The training batch size is set to 128 for all competing methods on both datasets. Four kinds of data augmentation methods (RandomResizedCrop, RandomGrayscale, ColorJitter, RandomHorizontalFlip) in PyTorch with default parameters are adopted.</p><p>Following <ref type="bibr" target="#b45">[46]</ref>, we adopt weighted kNN classifier to evaluate the performance. Given a test sample, we retrieve its top-k (k = 200) nearest neighbors based on cosine similarity, then apply weighted voting to predict its label <ref type="bibr" target="#b45">[46]</ref>   plar CNN <ref type="bibr" target="#b7">[8]</ref>, NPSoftmax <ref type="bibr" target="#b45">[46]</ref>, NCE <ref type="bibr" target="#b45">[46]</ref> and Triplet loss with and without hard mining. Triplet (hard) is the online hard negative sample within each batch for training <ref type="bibr" target="#b15">[16]</ref>, and the margin parameter is set to 0.5. DeepCluster <ref type="bibr" target="#b2">[3]</ref> and NCE <ref type="bibr" target="#b45">[46]</ref> represent the state-of-the-art unsupervised feature learning methods. The results are shown in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Classification Accuracy. Table <ref type="table" target="#tab_0">1</ref> demonstrates that our proposed method achieves the best performance (83.6%) with kNN classifier. DeepCluster <ref type="bibr" target="#b2">[3]</ref> performs well in learning good "intermediate" features with large-scale unlabelled data, but the performance with kNN classification drops dramatically. Meanwhile, it is also quite sensitive to cluster numbers, which is unsuitable for different tasks. Compared to Exemplar CNN <ref type="bibr" target="#b7">[8]</ref> which uses the classifier weights for training, the proposed method outperforms it by 9.1%. Compared to NPSoftmax <ref type="bibr" target="#b45">[46]</ref> and NCE <ref type="bibr" target="#b45">[46]</ref>, which use memorized feature for optimizing, the proposed method outperform by 2.8% and 3.2% respectively. The performance improvement is clear due to the idea of directly performing optimization over feature itself. Compared to triplet loss, the proposed method also outperforms it by a clear margin. The superiority is due to the hard mining nature in Softmax function.</p><p>Efficiency. We plot the learning curves of the competing methods at different epochs in Fig. <ref type="figure" target="#fig_4">3</ref>. The proposed method takes only 2 epochs to get a kNN accuracy of 60% while <ref type="bibr" target="#b45">[46]</ref> takes 25 epochs and <ref type="bibr" target="#b7">[8]</ref> takes 45 epochs to reach the same accuracy. It is obvious that our learning speed is much faster than the competitors. The efficiency is guaranteed by directly optimization on instance features rather than classifier weights <ref type="bibr" target="#b7">[8]</ref> or memory bank <ref type="bibr" target="#b45">[46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">STL-10 Dataset</head><p>STL-10 dataset <ref type="bibr" target="#b3">[4]</ref> is an image recognition dataset with colored images of size 96 √ó 96, which is widely used in unsupervised learning. Specifically, this dataset is originally designed with three splits: 1) train, 5K labelled images in ten The classifier is used to predict the label of test samples. We implement NPSoftmax <ref type="bibr" target="#b45">[46]</ref>, NCE <ref type="bibr" target="#b45">[46]</ref> and DeepCluster <ref type="bibr" target="#b2">[3]</ref> (cluster number 100) under the same settings with their released code. By default, we only use 5K training images without using labels for training. The performances of some state-of-the-art unsupervised methods (k-MeansNet <ref type="bibr" target="#b4">[5]</ref>, HMP <ref type="bibr" target="#b1">[2]</ref>, Satck <ref type="bibr" target="#b52">[54]</ref> and Exemplar <ref type="bibr" target="#b7">[8]</ref>) are also reported. Those results are taken from <ref type="bibr" target="#b32">[33]</ref>.</p><p>As shown in Table <ref type="table" target="#tab_1">2</ref> , when only using 5K training images for learning, the proposed method achieves the best accuracy with both classifiers (kNN: 74.1%, Linear: 69.5%), which are much better than NCE <ref type="bibr" target="#b45">[46]</ref> and DeepCluster <ref type="bibr" target="#b2">[3]</ref> under the same evaluation protocol. Note that kNN measures the similarity directly with the learned features and Linear requires additional classifier learning with the labelled training data. When 105K images are used for training, the proposed method also achieves the best performance for both kNN classifier and linear classifier. In particular, the kNN accuracy is 74.1% for 5K training images, and it increases to 81.6% for full 105K training images. The classification accuracy with linear classifier also increases from 69.5% to 77.9%. This experiment verifies that the proposed method can benefit from more training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiments on Unseen Testing Categories</head><p>This section evaluates the discriminability of the learned feature embedding when the semantic categories of training samples and testing samples are not overlapped. We follow the experimental settings described in <ref type="bibr" target="#b31">[32]</ref> to conduct experiments on CUB200-2011(CUB200) <ref type="bibr" target="#b42">[43]</ref>, Stanford Online Product (Product) <ref type="bibr" target="#b31">[32]</ref> and Car196 <ref type="bibr" target="#b21">[22]</ref>   Implementation Details. We implement the proposed method on PyTorch. The pre-trained Inception-V1 <ref type="bibr" target="#b38">[39]</ref> on ImageNet is used as the backbone network following existing methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37]</ref>. A 128-dim fully connected layer with ‚Ñì 2 normalization is added after the pool5 layer as the feature embedding layer. All the input images are firstly resized to 256 √ó 256. For data augmentation, the images are randomly cropped at size 227√ó 227 with random horizontal flipping following <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b29">30]</ref>. Since the pre-trained network performs well on CUB200 dataset, we randomly select the augmented instance and its corresponding nearest instance as positive. In testing phase, a single center-cropped image is adopted for fine-grained recognition as in <ref type="bibr" target="#b29">[30]</ref>. We adopt the SGD optimizer with 0.9 momentum. The initial learning rate is set to 0.001 without decay. The temperature parameter œÑ is set to 0.1. The training batch size is set to 64.</p><p>Evaluation Metrics. Following existing works on supervised deep embedding learning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">32]</ref>, the retrieval performance and clustering quality of the testing set are evaluated. Cosine similarity is adopted for similarity mea-  as well as 5NN features from different categories (Negative). The distributions of the cosine similarity of different methods are shown in Fig. <ref type="figure" target="#fig_6">5</ref>. A more separable distribution indicates a better feature embedding. It shows that the proposed method performs best to separate positive and negative samples. We could also observe that our learned feature preserves the best spread-out property.</p><p>It is interesting to show how the learned instance-wise feature helps the category label prediction. We report the cosine similarity distribution based on other category definitions (attributes in <ref type="bibr" target="#b18">[19]</ref>) instead of semantic label in Fig. <ref type="figure" target="#fig_7">6</ref>. The distribution clearly shows that the proposed method also performs well to separate other attributes, which demonstrates the generalization ability of the learned feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose to address the unsupervised embedding learning problem by learning a data augmentation invariant and instance spread-out feature. In particular, we propose a novel instance feature based softmax embedding trained with Siamese network, which explicit- ly pulls the features of the same instance under different data augmentations close and pushes the features of different instances away. Comprehensive experiments show that directly optimizing over instance feature leads to significant performance and efficiency gains. We empirically show that the spread-out property is particularly important and it helps capture the visual similarity among samples.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of our basic idea. The features of the same instance under different data augmentations should be invariant, while features of different image instances should be separated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The framework of the proposed unsupervised learning method with Siamese network. The input images are projected into low-dimensional normalized embedding features with the CNN backbone. Image features of the same image instance with different data augmentations are invariant, while embedding features of different image instances are spread-out.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>4.1.1 CIFAR-10 Dataset CIFAR-10 datset [23] contains 50K training images and 10K testing images from the same ten classes. The image size are 32 √ó 32. Five methods are included for comparison: DeepCluster [3] with different cluster numbers, Exem-1 Code is available at https://github.com/mangye16/ Unsupervised_Embedding_Learning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Evaluation of the training efficiency on CIFAR-10 dataset. kNN accuracy (%) at each epoch is reported, demonstrating the learning speed of different methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: 4NN retrieval results of some example queries on CUB200-2011 dataset. The positive (negative) retrieved results are framed in green (red). The similarity is measured with cosine similarity.</figDesc><graphic url="image-13.png" coords="8,246.58,72.85,142.40,183.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The cosine similarity distributions on CIFAR-10 [23]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The cosine similarity distributions of randomly initialized network (left column) and our learned model (right column) with different attributes on CIFAR-10 [23].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>kNN accuracy (%) on CIFAR-10 dataset.</figDesc><table><row><cell></cell><cell>4</cell></row><row><cell>Triplet</cell><cell>57.5</cell></row><row><cell>Triplet (Hard)</cell><cell>78.4</cell></row><row><cell>Ours</cell><cell>83.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Classification accuracy (%) with linear classifier and kNN classifier on STL-10 dataset. * Results are taken from<ref type="bibr" target="#b32">[33]</ref>, the baseline network is different.</figDesc><table><row><cell>Methods</cell><cell>Training</cell><cell>Linear</cell><cell>kNN</cell></row><row><cell>RandomCNN</cell><cell>None</cell><cell>-</cell><cell>22.4</cell></row><row><cell>k-MeansNet  *  [5]</cell><cell>105K</cell><cell>60.1</cell><cell>-</cell></row><row><cell>HMP  *  [2]</cell><cell>105K</cell><cell>64.5</cell><cell>-</cell></row><row><cell>Satck  *  [54]</cell><cell>105K</cell><cell>74.3</cell><cell>-</cell></row><row><cell>Exemplar  *  [8]</cell><cell>105K</cell><cell>75.4</cell><cell>-</cell></row><row><cell>NPSoftmax [46]</cell><cell>5K</cell><cell>62.3</cell><cell>66.8</cell></row><row><cell>NCE [46]</cell><cell>5K</cell><cell>61.9</cell><cell>66.2</cell></row><row><cell>DeepCluster(100) [3]</cell><cell>5K</cell><cell>56.5</cell><cell>61.2</cell></row><row><cell>Ours</cell><cell>5K</cell><cell>69.5</cell><cell>74.1</cell></row><row><cell>Ours</cell><cell>105K</cell><cell>77.9</cell><cell>81.6</cell></row><row><cell cols="4">classes for training, 2) test, 8K images from the same ten</cell></row><row><cell cols="4">classes for testing, 3) unlabelled, 100K unlabelled images</cell></row><row><cell cols="4">which share similar distribution with labelled data for un-</cell></row><row><cell cols="4">supervised learning. We follow the same experimental set-</cell></row><row><cell cols="4">ting as CIFAR-10 dataset and report classification accuracy</cell></row><row><cell cols="4">(%) with both Linear Classifier (Linear) and kNN classier</cell></row><row><cell>(kNN) in</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Linear classifier means training a SVM classifier on the learned features and the labels of training samples.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>datasets. No semantic label is used for training. Caltech-UCSD Birds</figDesc><table><row><cell>Methods</cell><cell cols="5">R@1 R@2 R@4 R@8 NMI</cell></row><row><cell>Initial (FC)</cell><cell>39.2</cell><cell>52.1</cell><cell>66.1</cell><cell>78.2</cell><cell>51.4</cell></row><row><cell></cell><cell></cell><cell cols="3">Supervised Learning</cell><cell></cell></row><row><cell>Lifted [32]</cell><cell>43.6</cell><cell>56.6</cell><cell>68.6</cell><cell>79.6</cell><cell>56.5</cell></row><row><cell>Clustering[38]</cell><cell>48.2</cell><cell>61.4</cell><cell>71.8</cell><cell>81.9</cell><cell>59.2</cell></row><row><cell>Triplet+ [13]</cell><cell>45.9</cell><cell>57.7</cell><cell>69.6</cell><cell>79.8</cell><cell>58.1</cell></row><row><cell>Smart+ [13]</cell><cell>49.8</cell><cell>62.3</cell><cell>74.1</cell><cell>83.3</cell><cell>59.9</cell></row><row><cell></cell><cell></cell><cell cols="3">Unsupervised Learning</cell><cell></cell></row><row><cell>Cyclic [25]</cell><cell>40.8</cell><cell>52.8</cell><cell>65.1</cell><cell>76.0</cell><cell>52.6</cell></row><row><cell>Exemplar [8]</cell><cell>38.2</cell><cell>50.3</cell><cell>62.8</cell><cell>75.0</cell><cell>45.0</cell></row><row><cell>NCE [46]</cell><cell>39.2</cell><cell>51.4</cell><cell>63.7</cell><cell>75.8</cell><cell>45.1</cell></row><row><cell>DeepCluster[3]</cell><cell>42.9</cell><cell>54.1</cell><cell>65.6</cell><cell>76.2</cell><cell>53.0</cell></row><row><cell>MOM [21]</cell><cell>45.3</cell><cell>57.8</cell><cell>68.6</cell><cell>78.4</cell><cell>55.0</cell></row><row><cell>Ours</cell><cell>46.2</cell><cell>59.0</cell><cell>70.1</cell><cell>80.2</cell><cell>55.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results (%) on CUB200 dataset.</figDesc><table><row><cell>Methods</cell><cell>R@1</cell><cell>R@10</cell><cell>R@100</cell><cell>NMI</cell></row><row><cell>Initial (FC)</cell><cell>40.8</cell><cell>56.7</cell><cell>72.1</cell><cell>84.0</cell></row><row><cell>Exemplar [8]</cell><cell>45.0</cell><cell>60.3</cell><cell>75.2</cell><cell>85.0</cell></row><row><cell>NCE [46]</cell><cell>46.6</cell><cell>62.3</cell><cell>76.8</cell><cell>85.8</cell></row><row><cell>DeepCluster[3]</cell><cell>34.6</cell><cell>52.6</cell><cell>66.8</cell><cell>82.8</cell></row><row><cell>MOM [21]</cell><cell>43.3</cell><cell>57.2</cell><cell>73.2</cell><cell>84.4</cell></row><row><cell>Ours</cell><cell>48.9</cell><cell>64.0</cell><cell>78.0</cell><cell>86.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Results (%) on Product dataset.</figDesc><table><row><cell>200 (CUB200) [43] is a fine-grained bird dataset. Follow-</cell></row><row><cell>ing [32], the first 100 categories with 5,864 images are used</cell></row><row><cell>for training, while the other 100 categories with 5,924 im-</cell></row><row><cell>ages are used for testing. Stanford Online Product (Product)</cell></row><row><cell>[32] is a large-scale fine-grained product dataset. Similar-</cell></row><row><cell>ly, 11,318 categories with totally 59,551 images are used</cell></row><row><cell>for training, while the other 11,316 categories with 60,502</cell></row><row><cell>images are used for testing. Cars (Car196) dataset [22] is</cell></row><row><cell>a fine-grained car category dataset. The first 98 categories</cell></row><row><cell>with 8,054 images are used for training, while the other 98</cell></row><row><cell>categories with 8,131 images are used for testing.</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work is partially supported by Research Grants Council (RGC/HKBU12200518), Hong Kong. This work is partially supported by the United States Air Force Research Laboratory (AFRL) and the Defense Advanced Research Projects Agency (DARPA) under Contract No. FA8750-16-C-0166. Any opinions, findings and conclusions or recommendations expressed in this material are solely the responsibility of the authors and does not necessarily represent the official views of AFRL, DARPA, or the U.S. Government.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>R@1 R@2 R@4 R@8 NMI Initial (FC) <ref type="bibr" target="#b34">35</ref> surement. Given a query image from the testing set, R@K measures the probability of any correct matching (with same category label) occurs in the top-k retrieved ranking list <ref type="bibr" target="#b31">[32]</ref>. The average score is reported for all testings samples. Normalized Mutual Information (NMI) <ref type="bibr" target="#b35">[36]</ref> is utilized to measure the clustering performance of the testing set.</p><p>Comparison to State-of-the-arts. The results of all the competing methods on three datasets are listed in Table <ref type="table">3</ref>, 4 and 5, respectively. MOM <ref type="bibr" target="#b20">[21]</ref> is the only method that claims for unsupervised metric learning. We implement the other three state-of-the-art unsupervised methods (Exemplar <ref type="bibr" target="#b7">[8]</ref>, NCE <ref type="bibr" target="#b45">[46]</ref> and DeepCluster <ref type="bibr" target="#b2">[3]</ref>) on three datasets with their released code under the same setting for fair comparison. Note that these methods are originally evaluated for general unsupervised feature learning, where the training and testing set share the same categories. We also list some results of supervised learning (originate from <ref type="bibr" target="#b20">[21]</ref>) on CUB200 dataset as shown in Table <ref type="table">3</ref>.</p><p>Generally, the instance-wise feature learning methods (NCE <ref type="bibr" target="#b45">[46]</ref>, Examplar <ref type="bibr" target="#b7">[8]</ref>, Ours) outperform non-instancewise feature learning methods (DeepCluster <ref type="bibr" target="#b2">[3]</ref>, MOM <ref type="bibr" target="#b20">[21]</ref>), especially on Car196 and Product datasets, which indicates instance-wise feature learning methods have good generalization ability on unseen testing categories. Among all the instance-wise feature learning methods, the proposed method is the clear winner, which also verifies the effectiveness of directly optimizing over feature itself. Moreover, the proposed unsupervised learning method is even competitive to some supervised learning methods on CUB200 dataset.</p><p>Qualitative Result. Some retrieved examples with cosine similarity on CUB200 dataset at different training epochs are shown in Fig. <ref type="figure">4</ref>. The proposed algorithm can iteratively improve the quality of the learned feature and retrieve more correct images. Although there are some wrongly retrieved samples from other categories, most of the top retrieved samples are visually similar to the query.</p><p>Training from Scratch. We also evaluate the performance using a network (ResNet18) without pre-training. The results on the large-scale Product dataset are shown in Table <ref type="table">6</ref>. The proposed method is also a clear winner. Interestingly, MOM <ref type="bibr" target="#b20">[21]</ref> fails in this experiment. The main reason is that the feature from randomly initialized network provides limited information for label mining. Therefore, MOM cannot estimate reliable labels for training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>The proposed method imposes two important properties for instance feature learning: data augmentation invariant and instance spread-out. We conduct ablation study to show the effectiveness of each property on CIFAR-10 dataset. To show the importance of data augmentation invariant property, we firstly evaluate the performance by removing each of the operation respectively from the data augmentation set. The results are shown in Table <ref type="table">7</ref>. We observe that all listed operations contribute to the remarkable performance gain achieved by the proposed algorithm. In particular, RandomResizedCrop contributes the most. We also evaluate the performance without data augmentation (No DA) in Table <ref type="table">8</ref>, and it shows that performance drops significantly from 83.6% to 37.4%. It is because when training without data augmentation, the network does not create any positive concentration property. The features of visually similar images are falsely separated.</p><p>To show the importance of spread-out property, we evaluated two different strategies to choose negative samples: 1) selecting the top 50% instance features that are similar to query instance as negative (hard negative); 2) selecting the bottom 50% instance features that are similar to query instance as negative (easy negative). The results are shown as "Hard" and "Easy" in Table <ref type="table">8</ref>. The performance drops dramatically when only using the easy negative. In comparison, the performance almost remains the same as the full model when only using hard negative. It shows that separating hard negative instances helps to improve the discriminability of the learned embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Understanding of the Learned Embedding</head><p>We calculate the cosine similarity between the query feature and its 5NN features from the same category (Positive)</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to see by moving</title>
		<author>
			<persName><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning for rgb-d based object recognition</title>
		<author>
			<persName><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Experimental Robotics</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="387" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2007">2018. 1, 2, 5, 6, 7</date>
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Selecting receptive fields in deep networks</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2528" to="2536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adversarial feature learning</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Kr√§henb√ºhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09782</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Discriminative unsupervised feature learning with exemplar convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2016. 2, 3, 5, 6, 7, 8</date>
			<biblScope unit="volume">PAMI</biblScope>
			<biblScope unit="page" from="1734" to="1747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="766" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Ishmael</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00704</idno>
		<title level="m">Adversarially learned inference</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Smart mining for deep metric learning</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno>arXiv preprint arX- iv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Significance of softmax-based features in comparison to distance metric learning-based features</title>
		<author>
			<persName><forename type="first">Shota</forename><surname>Horiguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daiki</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
		<idno>arX- iv:1712.10151</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised learning of discriminative attributes and visual representations</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5175" to="5184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised learning of invariant feature hierarchies with applications to object recognition</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y-Lan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Mining on manifolds: Metric learning without labels</title>
		<author>
			<persName><forename type="first">Ahmet</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2018. 1, 2, 5, 6, 7</date>
		</imprint>
	</monogr>
	<note>Yannis Avrithis, and Ondrej Chum</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Citeseer</publisher>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations</title>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="609" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by graph-based consistent constraints</title>
		<author>
			<persName><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="678" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning deep parsimonious representations</title>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5076" to="5084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Classification with noisy labels by importance reweighting</title>
		<author>
			<persName><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="447" to="461" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Surpassing human-level face verification performance on lfw with gaussianface</title>
		<author>
			<persName><forename type="first">Chaochao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3811" to="3819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><surname>Kr√§henb√ºhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2859" to="2867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">No fuss distance metric learning using proxies</title>
		<author>
			<persName><forename type="first">Yair</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName><forename type="first">Hyun</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2007">2016. 1, 2, 4, 5, 6, 7</date>
			<biblScope unit="page" from="4004" to="4012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scaling the scattering transform: Deep hybrid networks</title>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Oyallon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Belilovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5619" to="5628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Introduction to information retrieval</title>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch√ºtze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prabhakar</forename><surname>Raghavan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">39</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multiclass n-pair loss objective</title>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep metric learning via facility location</title>
		<author>
			<persName><forename type="first">Hyun</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2206" to="2214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Robust boltzmann machines for recognition and denoising</title>
		<author>
			<persName><forename type="first">Yichuan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2264" to="2271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Stochastic k-neighborhood selection for supervised and unsupervised learning</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="199" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">The caltech-ucsd birds-200-2011 dataset</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2008">2018. 1, 2, 3, 5, 6, 7, 8</date>
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning deep feature representations with domain guided dropout for person re-identification</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1249" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Robust anchor embedding for unsupervised video person re-identification in the wild</title>
		<author>
			<persName><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyuan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="170" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dynamic graph co-matching for unsupervised videobased person re-identification</title>
		<author>
			<persName><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing (TIP)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Dynamic label graph matching for unsupervised video re-identification</title>
		<author>
			<persName><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5142" to="5150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning spread-out local feature descriptors</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei A Efros ; Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016. 2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4605" to="4613" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Alignedreid: Surpassing human-level performance in person reidentification</title>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilai</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiqi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08184</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Stacked what-where auto-encoders</title>
		<author>
			<persName><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName><surname>Lecun</surname></persName>
		</author>
		<idno>arXiv preprint arX- iv:1506.02351. 6</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
