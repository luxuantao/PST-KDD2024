<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ReGraphxX: NoC-enabled 3D Heterogeneous RERRAM Architecture for Training Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Aqeeb</forename><forename type="middle">Iqbal</forename><surname>Arka</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="institution">Washington State University Pullman</orgName>
								<address>
									<postCode>99164</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Biresh</forename><surname>Kumar Joardar'</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="institution">Washington State University Pullman</orgName>
								<address>
									<postCode>99164</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Janardhan</forename><surname>Rao Doppa'</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="institution">Washington State University Pullman</orgName>
								<address>
									<postCode>99164</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Partha</forename><surname>Pratim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="institution">Washington State University Pullman</orgName>
								<address>
									<postCode>99164</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Krishnendu</forename><surname>Chakrabarty</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="institution">Washington State University Pullman</orgName>
								<address>
									<postCode>99164</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ReGraphxX: NoC-enabled 3D Heterogeneous RERRAM Architecture for Training Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.23919/DATE51398.2021.9473949</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>GNNs</term>
					<term>ReRAM</term>
					<term>3D</term>
					<term>NoC</term>
					<term>Heterogeneous I. INTRODUCTION</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Network (GNN) is a variant of Deep Neural Networks (DNNs) operating on graphs. However, GNNs are more complex compared to traditional DNNs as they simultaneously exhibit features of both DNN and graph applications. As a result, architectures specifically optimized for either DNNs or graph applications are not suited for GNN training. In this work, we propose a 3D heterogeneous manycore architecture for on-chip GNN training to address this problem.</p><p>The proposed architecture, ReGraphX, involves heterogeneous ReRAM crossbars to fulfill the disparate requirements of both DNN and graph computations simultaneously. The RERAMbased architecture is complemented with a multicast-enabled 3D NoC to improve the overall achievable performance. We demonstrate that ReGraphX outperforms conventional GPUs by up to 3.5X (on an average 3X) in terms of execution time, while reducing energy consumption by as much as 11X.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>'Department of ECE, Duke University</head><p>Durham, NC 27708, USA. {bireshkumar.joardar, krish} (@duke.edu between computing cores and the main memory for GNN training/inference. Hence, ReRAM-based architectures are naturally suited for accelerating GNN training/inference (compared to GPUs). However, existing ReRAM-based architectures are optimized specifically for either DNNs (e.g. <ref type="bibr" target="#b5">[6]</ref>. <ref type="bibr" target="#b6">[7]</ref>) or graph computations (e.g. <ref type="bibr" target="#b7">[8]</ref>); they are not effective for GNN training as they exhibit characteristics of both DNNs and graph computations. Hence, there is a pressing need for a heterogeneous architecture that synergistically combines the design principles from both DNNs and graph analytics.</p><p>Unlike traditional DNNs, GNNs involve heavy data exchange due to message-passing operations to accumulate neighbor information in a recursive approach <ref type="bibr" target="#b8">[9]</ref>. This gives rise to significant data exchange among the ReRAM tiles, which limits the overall achievable performance. We show in this work that data exchange during GNN training predominantly exhibits many-to-one-to-many, and multicast characteristics.</p><p>Traditional planar (two-dimensional)</p><p>architectures are not suitable for GNN training due to the large physical separation between the tiles. This will result in significant amount of long-range communication causing a performance bottleneck <ref type="bibr" target="#b9">[10]</ref>.</p><p>To reduce the performance bottlenecks in planar designs, three-dimensional network-onchip (3D NoC) based architectures should be used. By stacking planar dies on top of one another and connecting them with vertical links, the communication latency can be greatly reduced. Prior work has shown that 3D NoCs enable the design of a low latency and high throughput communication backbone for manycore chips <ref type="bibr" target="#b10">[11]</ref>. In addition, 3D NoC enables high-throughput multicast support <ref type="bibr" target="#b11">[12]</ref>. As a result, 3D NoC is more suited for long-range, multicast, and many-to-one-to-many traffic patterns <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>.</p><p>Hence, we conjecture that 3D NoC is a suitable communication backbone to enable the proposed ReRAMbased architecture for training GNNs.</p><p>In this paper, we present the design of a 3D NoC-enabled manycore architecture, referred as ReGraphx, for training GNNs. The proposed manycore architecture consists of (a) heterogeneous ReRAMs as the Processing Elements (PEs) to accelerate the large number of MAC operations in GNNs and (b) a high-throughput NoC architecture as the communication backbone. The main contributions of this work include: e We undertake an in-depth study of the computation and communication patterns when GNNs are trained on ReRAM-based platforms. This study motivates and influences the design of an efficient NoC architecture. e We design an energy efficient and high-performance 3D NoCâ‚¬ architecture by taking into consideration the on-chip traffic pattern associated with GNN training. The 3D NoC architecture enables high performance and energy efficient GNN training.</p><p>To the best of our knowledge, this is the first work that proposes a _ heterogeneous ReRAM-based manycore architecture enabled by 3D NoC for training GNNs. The rest of the paper is organized as follows. Section II describes relevant prior work. In Section III, we discuss the salient features of GNNs, especially the traffic patterns that must be considered for NoC design. In Section IV, we introduce the proposed ReGraphX architecture, the role of the 3D NoC, and the GNN layer mapping strategy. Section V_ presents experimental results and analysis. We conclude the paper in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED PRIOR WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. ReRAM-based architectures</head><p>ReRAMs can be used as memory and also to perform insitu MAC operations <ref type="bibr" target="#b4">[5]</ref>. Both DNN and graph applications involve significant amount of MAC operations. Hence, ReRAM-based accelerators for both DNN training and inference have been extensively studied <ref type="bibr" target="#b5">[6]</ref>  <ref type="bibr" target="#b6">[7]</ref>. Moreover, ReRAM based graph accelerators have been shown to significantly outperform CPU-or GPU-based systems both in terms of execution time and energy <ref type="bibr" target="#b7">[8]</ref> [14] <ref type="bibr" target="#b14">[15]</ref>. However, these solutions focus mainly on accelerating the computation. As mentioned earlier, GNNs involve heavy communication, which limits the maximum achievable performance. In addition, all these RERAM-based accelerators are fine-tuned either for DNN training/inference or graph applications. Our work addresses a key shortcoming of the state-of-the art by proposing an architecture that caters to GNN training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Hardware for GNN Computation</head><p>GNNs exhibit a unique computational kernel that is a combination of traditional DNNs at vertices and graph analytics at edges <ref type="bibr" target="#b3">[4]</ref>. However, GNN computation is memory intensive for large-scale graphs, which necessitates use of efficient graph partitioning <ref type="bibr" target="#b15">[16]</ref>. This divide-and-conquer approach enables scalable GNN training over large graphs with high accuracy and speed <ref type="bibr" target="#b15">[16]</ref>. The design of hardware architectures using commodity processors, FPGAs and custom ASICs has been considered in recent work <ref type="bibr" target="#b1">[2]</ref> [9] <ref type="bibr" target="#b9">[10]</ref>. However, all these architectures primarily focus on GNN inference but nof training. The training of GNNs is considerably more challenging due to additional data exchange between the forward and backward phases. Moreover, all these architectures are limited to relatively small graph structures, which do not require large amounts of memory and computation. In contrast, this paper is focused on a single-chip architecture enabled by 3D integration that leverages the benefits introduced by ReRAM-based processing elements (PEs) to design a GNN training accelerator. Moreover, the proposed architecture leverages graph partitioning to enable acceleration of training GNNs with large-scale graphs that contain millions of nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. COMPUTATIONAL KERNEL FOR TRAINING GNNS</head><p>In this section, we present the salient features of GNN training. Specifically, we study its computation and communication characteristics to motivate the proposed ReRAM-based architecture. Fig. <ref type="figure">1</ref> shows the different computational and communication characteristics of a GNN.</p><p>As shown in Fig. <ref type="figure">1</ref>(a), a graph consists of (i) vertices: each vertex V can be represented using a feature vector (denoted as the vector Xy in Fig. <ref type="figure">1l(a</ref>)); and (ii) edges, which are represented as an adjacency matrix (Adj). The feature vector Xy characterizes a given node V of a graph while Adj indicates the vertex connectivity.</p><p>A GNN consists of multiple cascaded neural layers that in turn can be divided into two sub-layers (or sub-tasks): (i) Vertex-computation layer (V-layer; shown in Fig. <ref type="figure">1(b</ref>)), and (ii) Edge-computation layer (E-layer; shown in Fig. <ref type="figure">1(c</ref>)). The V-layer involves multiplying the feature vector of the nodes/vertices (X,) with the weight matrices (W). It is like MAC operations in a traditional DNN as shown in Fig. <ref type="figure">1(b)</ref>.</p><p>Hence, this operation can be efficiently implemented using</p><p>ReRAM-based architectures <ref type="bibr">[6] [7]</ref>. The output of the V-layer is the updated vertex feature vector (Y, = W -X, ). The Elayer involves the accumulation of the updated neighbor information once the V-layer is completed. This is done via the graph edges and is similar to the message-passing operation between all one-hop neighbors, as shown in Fig. <ref type="figure">1(c</ref>) <ref type="bibr" target="#b3">[4]</ref>. Interestingly, this operation can also be decomposed as a matrix-multiplication operation as shown in Fig. <ref type="figure">1(c)</ref>. The E-layer computation can be envisioned as a sparse matrixvector multiplication (SpMV) involving Adj (sparse matrix <ref type="bibr" target="#b1">[2]</ref>) and the updated vertex feature vectors (Y) as shown in shows, both the V-and E-layers can be decomposed as MAC operations which can be implemented using ReRAMs <ref type="bibr" target="#b4">[5]</ref>.</p><p>Next, Fig. <ref type="figure">1</ref>(d) shows an example GNN with three neural layers, i.e., three V-E layer pairs. As mentioned earlier, the Vlayers have trainable weights similar to a standard DNN. Each V-layer has a unique set of weights which need to be mapped/assigned to different sets of PEs (V-PEs) for computation (discussed in more detail in the next section). The E-layer requires only the graph adjacency matrix, which is represented by Adj. The Adj matrix is fixed for a given graph and is mapped to another set of PEs (E-PEs). Similar to V-PEs, we can have separate E-PEs for each neural layer of GNN. However, as Adj is constant for a given graph, the <ref type="figure">1</ref>. Illustration of the computational components of a GNN: (a) Input graph represented as node features (X;) and adjacency matrix (Adj), (b) Vertex-centric computation layer (V), (c) Edge-centric computation layer (E); Both V-and E-layers together, constitute a neural layer of GNN, and (d) Overall GNN structure with three neural layers as an example; The arrows indicate the data communication pattern ina GNN. Note that each V-layer has its unique set of weights (like traditional DNNs) which need to be mapped to different PEs. However, the E-layer depends on Adj only which is fixed for a given input graph. This results in a many-to-one communication where all the PEs responsible for V-layer computations communicate with the same PEs storing the Adj matrix. different E-PEs will store the same information and is unnecessary. As a result, the E-PEs need to be shared by all the neural layers. As discussed earlier, the output of the Vlayer is used as input for the next E-layer and so on. This results in a many-to-one-to-many communication pattern as multiple sets of V-PEs communicate with the same set of E-PEs as shown in Fig. <ref type="figure">1(d)</ref>. Without a suitable interconnection backbone, the many-to-one-to-many communication can overwhelm the training process, resulting in a performance bottleneck. Hence, a suitable NoC is needed for highperformance training of GNNs.</p><formula xml:id="formula_0">Input Graph asec currene Updated "6(Q 1 1 O)feature} ramus @&gt;Â».. AÃ©sBcoO pelt 02 0 aS w=w-x) 4 "OepAf(ati108 ej1101 ( x |)..TUTUCd 3 Â»|1010 eloo10 al @ BD clz112042 Misses { ots Xats&lt;csX" ( Fh c e vert 0oo10 A i IL = J A Y Input Graph Vertex-Centric computation Edge-Centric computation (a) (b) (Â©) (d) Fig.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TV. OVERALL REGRAPHX ARCHITECTURE</head><p>In this section, we first present the key features of the ReGraphX architecture. Fig. <ref type="figure" target="#fig_2">2</ref> shows the proposed ReGraphxX architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Role of Heterogeneity</head><p>To accelerate the V-layer (which resembles a conventional DNN), we utilize the ReRAM-based tiled architecture for DNNs <ref type="bibr" target="#b5">[6]</ref>. The ReRAM tiles consist of 128x128 ReRAM crossbars (V-PEs) and associated peripheral circuits. The weights (W in Fig. <ref type="figure">1(b)</ref>) are mapped to the ReRAM cells for high-throughput MAC operations. We do not describe how weights are mapped to individual RERAM cells in detail for the sake of brevity. However, the E-layers in GNNs require the storage of the Adj matrix (Fig <ref type="figure">1(c</ref>)), which is sparse.</p><p>Typically, a significant portion (over 90%) of Adj are zeros. Multiplication with zeros is redundant and should be avoided. This can be accomplished by dividing the larger Adj matrix (of size N x N; N being the number of nodes in the graph) into smaller sub-matrices (of size M x M, where M &lt;&lt; N).</p><p>Then, any sub-matrix with all M x M entries being zero is discarded while the rest are mapped to M x M ReRAM crossbars (similar to DNN weights). Note that this method does not eliminate all zero entries of Adj; rather it reduces the number of zeros stored leading to lower E-PE requirements and energy dissipation.</p><p>We show in Fig. <ref type="figure">3</ref> that larger RERAM crossbars (as in <ref type="bibr" target="#b5">[6]</ref>) are not suited for this task. Fig. <ref type="figure">3</ref> shows the number of zeros that are stored (after the reduction step discussed above) for two different ReRAM crossbar sizes (M x M) for the three datasets PPI, Reddit, and Amazon2M considered in this work.</p><p>As we can see from Fig. <ref type="figure">3</ref>, larger RERAMs store up to 7X more zeros than their smaller counterparts, which leads to a significantly higher number of redundant multiplications. This leads to high energy consumption and lower throughput as resources are wasted on redundant multiply-by-zero operations. In addition, more ReRAM cells are necessary to store the extra zeros (higher area/hardware overhead). Hence, smaller REeRAM crossbars are more efficient for the E-layer computations. Overall, ReGraphX is a_ heterogeneous architecture that consists of two types of PEs (ReRAMs in this computations (V-PEs) and smaller crossbars (8x8) for the Elayers (E-PEs) as shown in Fig. <ref type="figure" target="#fig_2">2</ref>. It should be noted that even smaller ReRAM sizes can also be used for E-PEs <ref type="bibr" target="#b13">[14]</ref>. In this work, we adopt (without any loss of generality) the 8x8 tile architecture for E-PE following recent trends <ref type="bibr">[8] [15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Role of 3D NoC</head><p>As shown in Fig. <ref type="figure">1</ref>(d), the V-PEs send the updated node features (Y in Fig. <ref type="figure">1</ref>) to the E-PEs for further processing. The amount of data communicated between the V-PEs and the E-PEs is proportional to the total number of nodes, which considering all the features in the input graph, is often very high. For example, each image in the popular ImageNet more details in Section V). This results in significant amount of data exchange during GNN training. In addition, the data communication exhibits many-to-one-to-many patterns, where multiple sets of V-PEs communicate with the same set of E-PEs, and vice versa. Moreover, training involves data sharing between the forward-and _backward-phase computations of each layer; often the backward-phase computations are implemented on separate set of RERAMs, as described in <ref type="bibr" target="#b6">[7]</ref>. Overall, this results in the output of layer L; being sent to: (a) PEs responsible for the next layer L;,,, and (b) the PEs responsible for the backward phase of layer L;. Therefore, there is a significant amount of multicast traffic on top of the many-to-one-to-many traffic.</p><p>Traditional planar architectures are not suited for such traffic patterns. The heavy many-to-one-to-many and multicast traffic, as well as the large physical separation between PEs, imposes a significant amount of long-range communication requirement in planar architectures. As a result, the communication backbone quickly becomes a performance bottleneck. In addition, the multi-hop nature of a planar-mesh NoC leads to higher communication latencies <ref type="bibr" target="#b12">[13]</ref>, which is not desirable for training GNNs. A 3D architecture can alleviate this problem and be a powerful enabler for the ReGraphX architecture. By stacking multiple layers above each other, the physical distance between PE tiles is reduced significantly <ref type="bibr" target="#b12">[13]</ref>. In addition, 3D NoCs enable high-performance multicast support <ref type="bibr" target="#b11">[12]</ref>; both these characteristics are beneficial for GNN training. In this work, we use a 3D mesh NoC as interconnection backbone because it can support 3D tree multicast. We consider tree-multicast in this paper without any loss of generality. Other multicastaware routing schemes can also be used for ReGraphxX.</p><p>Overall, ReGraphX consists of three phy sical layers (tiers), as shown in Fig. <ref type="figure" target="#fig_2">2</ref>. We need more E-PEs than V-PEs as the number of trainable weights in the V-Layer (few hundred thousands) is smaller than the number of entries in the Adj matrix (which can run into millions) that need to be stored for the E-layer for an input graph. Hence, ReGraphX has two E-PE tiers and one V-PE tier. The middle tier consists of V-PEs m8x8 128x128</p><p>No. of '0's (normalized)</p><p>CoN Fae PPI Reddit Datasets Fig. <ref type="figure">3</ref>. Number of *ZERO's stored (normalized with respect to that of 8x8) for two types of crossbars considering various datasets Amazon2M and has one-hop connections facilitated by the vertical links in both the vertical directions. The top and bottom layers include the E-PEs that store the Adj matrix. This sandwiched structure (Fig. <ref type="figure" target="#fig_2">2</ref>) is critical as it provides one-hop access between the V-PEs and the E-PEs in both directions (a.k.a. vertical traffic, shown in Fig. <ref type="figure">1(d)</ref>). The intra V-PE communication (a.k.a. planar traffic) due to data sharing between the forward and backward phases is addressed by the multicast-aware mapping policy, which maps communicating neural layers to nearby PEs. The E-layer does not have any learnable/trainable parameter. Hence, there is no separate forward-backward traffic as in the case of V-PEs. However, there will be planar traffic in the E-layer as Adj is distributed across multiple E-PEs. Overall, the NoC and the mapping policy complement each other for high-performance GNN training. Note that, three tiers are used as an example only, to show the efficacy of 3D architectures for GNN training. More tiers can also be used. However, adding more tiers can lead to thermal issues and _ investigating thermal-aware 3D architectures for GNN training is part of our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Pipelined GNNs on ReRAMs</head><p>Training GNNs on one big monolithic graph is often impractical due to memory concerns. To address this, clustering-based graph partitioning schemes are used. For instance, in <ref type="bibr" target="#b15">[16]</ref>, the authors use a graph partitioning tool (METIS <ref type="bibr" target="#b16">[17]</ref>) to break a large graph into several smaller subgraphs to reduce memory requirements. In addition, training on large graphs does not exploit the benefits of RERAM-based architectures, which rely on a pipelined implementation <ref type="bibr" target="#b6">[7]</ref>. Pipelining the different layers of a DNN reduces the number of ReRAM writes (which are slow), leading to higher overall throughput. However, this strategy is not amenable to GNNs if the entire graph needs to be processed altogether. Clustering/partitioning can be used to address this problem. By dividing the graph into clusters, we can implement the pipelined training strategy for GNNs (as shown in Fig. <ref type="figure">4</ref>). Fig. <ref type="figure">4</ref> shows how pipelined training can be implemented for a GNN with two neural layers on RERAMS as an example. As discussed earlier, each neural layer can be further divided into two sub-layers (E-and V-layers). Moreover, each E-and Vlayer has a corresponding backward phase computation layer (e.g., BV1 represents the backward phase computation of V1 and so on). Here, V1 is the V-layer computation of the first neural layer of the GNN. Overall, this results in an eight-stage training pipeline as shown in Fig. <ref type="figure">4</ref>.</p><p>The GNN pipelined training works as follows: First, we partition the large input graph into smaller sub-graphs. The size of each sub-graph is chosen based on training time, hardware requirements, and end-to-end accuracy. Here, each input-sub-graph (obtained after partitioning) is the equivalent of one input image in DNNs. The pipelined training strategy for GNNs can then be implemented analogous to DNNs: At time T (Fig. <ref type="figure">4</ref>), the first sub-graph (G)) is loaded for V1-layer computations. The value of T (in Fig. <ref type="figure">4</ref>) will depend on the maximum of computation/communication times for any given layer. At the next timestamp 2T,G; advances for subsequent E-layer computation (E(G1)) while the next sub-graph Go is loaded for V1-layer computation, and so on. Once the pipeline is filled (at time 8T), all the PEs (corresponding to all forward and backward phases) are active all the time, leading to higher throughput and hardware utilization <ref type="bibr" target="#b5">[6]</ref>, as shown in Fig. <ref type="figure">4</ref>.</p><p>However, pipelined training also necessitates the simultaneous processing of multiple sub-graphs. For instance, at time 87 in Fig. <ref type="figure">4</ref>, eight sub-graphs Gi-Gg are being processed. As mentioned earlier in Sec. 3 (and Fig. <ref type="figure">1(d)</ref>), processing each graph results in a many-to-one-to-many traffic pattern. Hence, processing multiple sub-graphs at the same time, as shown in Fig. <ref type="figure">4</ref>, results in several sets of manyto-one-to-many traffic patterns corresponding to each of the sub-graphs Gi-Gs, resulting in high volume of data exchange which needs to be handled by the NoC. It is well known that ina pipeline, the slowest stage is the bottleneck and influences the overall execution time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Overall mapping to PEs</head><p>For an efficient implementation of pipelined GNN training, all the neural layers need to be executed simultaneously (Fig. <ref type="figure">4</ref>). This requires keeping all the neural weights on the chip. Hence, we also need to allocate (map) adequate resources (ReRAMs) to each neural layer based on the requirements. In addition, the mapping strategy influences on-chip traffic and it must complement the underlying NoC design to ensure the best performance. In this work, we employ a simple Simulated Annealing (SA)-based strategy for optimal mapping following <ref type="bibr" target="#b11">[12]</ref> as it can uncover high-quality solutions in a reasonable amount of time. The mapping of weights and the Adj matrix to the PEs can be envisioned as a combinatorial optimization problem: Given a total of P PEs and L layers (V and E), our aim is to distribute all the computation layers such that the highly communicating layers are mapped to nearby PEs. More specifically, our aim is to reduce long-range traffic (as much as possible), while ensuring efficient multicast communication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VV. EXPERIMENTAL RESULTS</head><p>In this section, we first present the experimental setup to evaluate the performance of ReGraphX. Here, we demonstrate the NoC and the full system performance evaluation of the ReGraphX architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A, Experimental Setup</head><p>ReGraphxX is a heterogeneous architecture that consists of two types of ReRAM tiles (V-PEs and the E-PEs, as shown in Fig. <ref type="figure" target="#fig_2">2</ref>). The ReRAM crossbar and tile configurations (shown in Table <ref type="table" target="#tab_1">I</ref>) of both the V-and E-PEs are based on <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b7">[8]</ref> respectively. We consider 64 V-PEs (1 planar tier) and 128 E-PEs (distributed over 2 planar tiers). It should be noted that for both E-PEs and the V-PEs, the area of the ReRAM tiles is dominated by the peripheral circuits instead of the crossbars. The difference between E-PE and V-PE area is not significant, despite the difference in ReRAM crossbar structure. Moreover, for the sake of simplicity in physical floor planning, we assume that the E-PEs and V-PEs are spread over same planar footprint. The ReRAM tiles communicate with each other via the 3D NoC.</p><p>To evaluate the characteristics of the ReGraphx architecture, we use the performance models from <ref type="bibr" target="#b5">[6]</ref>.  ReRAM arrays always execute instructions in-order and the instruction latencies are deterministic <ref type="bibr" target="#b5">[6]</ref>. Hence, deterministic models have been used to evaluate RERAM execution time, on-chip traffic, etc. <ref type="bibr" target="#b6">[7]</ref>. The mapping of DNN layer weights and matrices on the tiles are determined offline.</p><p>The traffic across the NoC is also statically determined to ensure conflict-free routing. We do not discuss the RERRAM execution models in detail for the sake of brevity and as it has been elaborated in <ref type="bibr" target="#b5">[6]</ref>.</p><p>In this work, we use the popular graph convolutional network (GCN) algorithm from <ref type="bibr" target="#b15">[16]</ref> (implemented in TensorFlow), as the representative GNN for performance evaluation. However, our findings and the proposed architecture are equally applicable to other GNNs that rely on the recursive neighborhood aggregation scheme. The GCN configuration in our experiments uses graph partitioning to reduce memory overhead and enable pipelined training. This allows us to evaluate large-sized graphs that are otherwise impossible to process with limited memory, specifically in an on-chip environment. For the evaluation of GNN training on the ReGraphX architecture, we choose three popular graph datasets -PPI, Reddit, and Amazon2M (details provided in Table <ref type="table" target="#tab_1">II</ref>). The GCN configuration for each dataset consists of four neural layers. As Table <ref type="table" target="#tab_1">II</ref> shows, the datasets are diverse in nature (in terms of size, partitioning, etc.), which allows us to comprehensively evaluate the performance of ReGraphx.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Effect of GNN hyper-parameters on ReGraphX</head><p>In this sub-section, we explore how various GNN hyperparameters (particularly batch size) affect ReGraphx performance. Fig. <ref type="figure" target="#fig_5">5</ref> shows the training (Fig. <ref type="figure" target="#fig_5">5(a)</ref>) and validation accuracy (Fig. <ref type="figure" target="#fig_5">5(b</ref>)) when different batch size (f) is used for the Reddit dataset as an example. Note that the notion of B in GNNs is not the same as in traditional DNNs. In a GNN, partitioning a graph (as in <ref type="bibr" target="#b15">[16]</ref>) yields several smaller sub-graphs (referred as NumPart hereafter). However, this often leads to the loss of important information (graph connections) that can cause the training to be unstable. Hence, a stochastic multi-clustering approach is typically used, which involves merging back some of the NumPart sub-graphs. The number of sub-graphs that are merged back together to create an intermediate input sub-graph is defined as Batch Size (f) in GNNs. Hence, the number of effective input sub-graphs (NumiInput) to the GNN is obtained by dividing NumParts by B. In Fig. <ref type="figure" target="#fig_5">5</ref>, we chose NumPart as 1500 (following <ref type="bibr" target="#b15">[16]</ref>) while varying Â§ from 1 to 20. From Fig. <ref type="figure" target="#fig_5">5</ref>, it is clear that the choice of batch size does not affect the accuracy of the GNN significantly for the Reddit dataset. Similar observations were made for the other two datasets. However, it should be noted that smaller Â§ affects convergence, leading to unstable GNN training. For instance, as shown in Fig. <ref type="figure" target="#fig_5">5</ref>, 8 =1 and B = 5 result in sudden drops in accuracy, even after a sufficiently high number of training epochs. On the other hand, larger B leads to more stable training and should be preferred.</p><p>However, larger f is costly to implement from the perspective of hardware overhead. Fig. <ref type="figure">6</ref> shows effects of B on both the training time and hardware requirements (more specifically, E-PE requirements) in ReGraphX. A smaller value of # results in smaller input-subgraphs, which in turn necessitates fewer E-PEs for storage. However, it leads to higher NumJnput (i.e., more input sub-graphs) that need to be processed one-after-another. As shown in Fig. <ref type="figure">6</ref>, this leads to higher training time. On the other hand, with increase in f, Nuninput reduces, which in turn causes training time to decrease. However, higher f} leads to a drastic increase in E-PE requirements as larger graphs need to be stored on-chip. Interestingly, we note from Fig. <ref type="figure">6</ref> that the reduction in training time is relatively insignificant beyond B = 10 while E-PE requirements keeps increasing steadily. From both Fig. <ref type="figure" target="#fig_5">5</ref> and Fig. <ref type="figure">6</ref>, we note that larger B leads to faster and more stable training, which is desirable. However, it also necessitates more E-PEs. Hence, we choose the maximum possible fh whose E-PE requirements can be met by ReGraphxX specifications (Table <ref type="table" target="#tab_1">I</ref>). In this case of Reddit, we set fi = 10.</p><p>The values of Â§ for the other datasets are chosen following same methodology and are listed in Table <ref type="table" target="#tab_1">II</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. NoC Evaluation</head><p>Next, we evaluate the performance of the 3D NoC, which serves as the communication backbone for ReGraphX. As described earlier (Fig. <ref type="figure">4</ref>), training on ReRAMs is implemented in a pipelined fashion. Hence, the overall execution time will be dominated by the slowest computation/communication stages among all the layers. Fig. <ref type="figure">7</ref> shows the worst-case computation and communication delay when a GNN is trained on ReGraphX. The computation delay is the maximum time necessary to finish all the MAC operations of a given layer. Similarly, the communication delay shown in Fig. <ref type="figure">7</ref> represents the maximum time needed to finish sending all output data (messages) between the communicating neural layers.</p><p>As mentioned in Sec. IV, GNN exhibits many-to-one-tomany and multicast nature of traffic. datasets, the communication delay always dominates the overall pipeline stage latency, irrespective of the multicast support. As a result, the overall pipeline stage latency will be dictated by the relatively slower communication. Any further speed-up in computation will be meaningless as communication will not be able to keep up with it. This happens as the massive amount of multicast and many-to-oneto-many traffic between the PEs overwhelms the system, creating a performance bottleneck. Without 3D multicast support, the communication delay is 57.3% worse on average (Communication-U in Fig. <ref type="figure">7</ref>). The communication delay improves significantly when multicast support is incorporated. It should be noted that for the Amazon dataset, the gap between computation and communication is almost non-existent. This happens as the Amazon2M dataset has a higher number of nodes, but smaller weight matrices compared to both PPI and Reddit. This results in higher computation delay, but the smaller weight matrices result in a relatively lower number of messages. Hence, the difference between communication and computation delay reduces. Multicast is used in the ReGraphX architecture hereafter because it helps in reducing the overall pipeline stage delay. This in turn results in lower overall execution time and higher speed-up compared to conventional GPU-based training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Full System Evaluation</head><p>Next, we undertake a full system performance evaluation and compare the overall execution time and energy dissipation of ReGraphX a conventional GPU-based platform. For the GPU execution, we implement the Cluster-GCN algorithm <ref type="bibr" target="#b15">[16]</ref> on NVIDIA Tesla V100 GPU. Fig. <ref type="figure" target="#fig_8">8</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. l(</head><label></label><figDesc>Fig. l(c). For instance, node A in Fig. 1 should accumulate the updated information from its 1-hop neighbors: node B and node C in the E-layer, which can be alternatively accomplished by multiplying the first row of Adj and the vertex feature matrix as shown in Fig. l(c). Hence, as Fig. 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Design, Automation and Test in Europe Conference (DATE 2021) Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 06:59:02 UTC from IEEE Xplore. Restrictions apply.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. ReGraphX architecture. This figure is for illustration purposes only.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 [Fig. 4 .</head><label>14</label><figDesc>Fig. 4. Pipelined implementation of a 2-layer deep GNN, with forward and backward phase. Each layer has two-sublayers -V-layer and E-layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Design, Automation and Test in Europe Conference (DATE 2021) Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 06:59:02 UTC from IEEE Xplore. Restrictions apply.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. GNN accuracy for different batch sizes for the Reddit dataset: (a) training; (b) validation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 Fig. 6 . 7 .</head><label>767</label><figDesc>Fig. 6. Normalized training time and NumInputs for different Batch Size (B) for the Reddit dataset. All numbers have been normalized w.r.t B=1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a)-(c) show the normalized execution time, energy, and energydelay-product (EDP) for GNN training on the ReGraphX architecture and the GPU, respectively. On an average, ReGraphX achieves 3X lower overall execution time on an average compared to the GPU. This speed-up can be attributed to: (a) ReRAM-based high-throughput MAC operations in ReGraphX, and (b) the 3D multicast-enabled NoC that reduces the communication latency. Fig. 8(b) shows that the ReGraphX architecture is on an average 11X more energy efficient than conventional GPUs due to the lower energy consumption of the RERAM tiles. Overall, ReGraphX improves the EDP by 34X on an average and up to 40X compared to the conventional GPU implementation. VI. CONCLUSION Graph neural networks (GNNs) are being increasingly used in multitude of applications such as social media, drug discovery, and recommendation systems. In this work, we propose a novel heterogeneous REeRAM based architecture -ReGraphX, tailor-made to accelerate GNN training. The ReRAM based computation platform is complemented with an efficient 3D NoC. By incorporating efficient many -to-oneto-many and multicast communication, the 3D NoC in ReGraphX reduces the communication bottleneck inherent in GNN training. Overall, ReGraphX outperforms conventional GPUs by up to 40X in terms of achievable EDP. This happens due to faster and more energy-efficient MAC computation enabled by the ReRAM crossbars and_ efficient communication enabled by the 3D NoC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. ReGraphX (a) Execution speed-up, (b) Energy savings and (c) EDP Improvement compared to GPU (normalized with respect to GPU)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I .</head><label>I</label><figDesc>PARAMETERS OF THE REGRAPHX ARCHITECTURE [6] [8]V-PE: 1 Planar Tier, 64 Routers per tier, 4 Tiles per router</figDesc><table><row><cell>ReRAM Tile (12 IMAs)</cell><cell>|</cell><cell cols="5">1 IMA has: 8-ADCs (8-bits), 128x8 DACs (1-bit), 8 crossbars, 128x128 crossbar size, 1OMHz, 2-bit resolution</cell></row><row><cell cols="6">E-PE: 2 Planar Tiers, 64 Routers per tier, 4 Tiles per router</cell></row><row><cell>ReRAM Tiles (12 IMAs)</cell><cell>|</cell><cell cols="5">1 IMA has: 8-ADCs (6-bits), 8x8 DACs (1-bit), 8 crossbars, 8x8 crossbar size, 1OMHz, 2-bit resolution</cell></row><row><cell cols="7">TABLE II. GRAPH DATA STATISTICS &amp; GNN HYPER PARAMETERS</cell></row><row><cell>Datasets</cell><cell cols="2">No. of Nodes</cell><cell>No. of Edges</cell><cell cols="3">No. of Partitions | Size | puts No. of Batch (NumPart) (p) (Num umPa B Input)</cell></row><row><cell>PPI</cell><cell></cell><cell>56,944</cell><cell>818,716</cell><cell>250</cell><cell>5</cell><cell>50</cell></row><row><cell>Reddit</cell><cell cols="2">232,965</cell><cell>11,606,919</cell><cell>1500</cell><cell>10</cell><cell>150</cell></row><row><cell cols="4">Amazon2M | 2,449,029 | 61,859,140</cell><cell>15000</cell><cell>10</cell><cell>1500</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 06:59:02 UTC from IEEE Xplore. Restrictions apply.</note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported, in part by the US National Science Foundation (NSF) grants CNS-1955353, CNS-1564014, CNS-1955196 and USA Army Research Office grant W911NF-17-1-0485. This material is also based upon work supported by the National Science Foundation under Grant # 2030859 to the Computing Research Association for the ClFellows Project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Graph Neural Networks: A Review of Methods</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">AWB-GCN: A Graph Convolutional Network Accelerator with Runtime Workload Rebalancing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM MICRO</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">An End-to-End Deep Learning Architecture for Graph Classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<editor>AAA/</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>New Orleans, LA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JCLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">In-memory Data Flow Processor</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fujiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mahlke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACT</title>
				<meeting><address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ISAAC: a convolutional neural network accelerator with in-situ analog arithmetic in crossbars</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shafiee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JSCA</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">PipeLayer: A Pipelined RERAM-Based Accelerator for Deep Learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EEE HPCA</title>
				<meeting><address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">GraphR: Accelerating Graph Processing Using ReRAM</title>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JEEE HPCA</title>
				<meeting><address><addrLine>Vienna</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">GRIP: A Graph Neural Network Accelerator Architecture</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kiningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Levis</surname></persName>
		</author>
		<idno>arXiv eprint 2007.13828</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hardware Acceleration of Graph Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Auten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tomei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JEEE/ACM DAC</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Networks-on-Chip in a Three-Dimensional Environment: A Performance Evaluation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Feero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="45" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">GRAMARCH: A GPU-ReRAM based Heterogeneous Architecture for Neural Image Segmentation</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Joardar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<pubPlace>DATE, Grennoble</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning-Based Application-Agnostic 3D NoC Design for Heterogeneous Manycore Systems</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Joardar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="852" to="866" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">GraphSAR: a sparsity-aware processing-in-memory architecture for large-scale graph processing on ReRAMs</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPDAC</title>
				<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spara: An Energy-Efficient ReRAM-Based Accelerator for Sparse Graph Analytics Applications</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JEEE IPDPS</title>
				<meeting><address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AC\Z SIGKDD</title>
				<meeting><address><addrLine>Anchorage, AK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Fast and High Quality Multilevel Scheme for Partitioning Irregular Graphs</title>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">S/AM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="359" to="392" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
