<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tuna: A Static Analysis Approach to Optimizing Deep Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yao</forename><surname>Wang</surname></persName>
							<email>yanmwang@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Utah</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rui</forename><surname>Li</surname></persName>
							<email>lirui@cs.utah.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Utah</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Wu</surname></persName>
							<email>yongwu@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Utah</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Tuna: A Static Analysis Approach to Optimizing Deep Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce Tuna, a static analysis approach to optimizing deep neural network programs. The optimization of tensor operations such as convolutions and matrix multiplications is the key to improving the performance of deep neural networks. Many deep learning model optimization mechanisms today use dynamic analysis, which relies on experimental execution on a target device to build a data-driven cost model of the program. The reliance on dynamic profiling not only requires access to target hardware at compilation time but also incurs significant cost in machine resources. We introduce an approach that profiles the program by constructing features based on the target hardware characteristics in order. We use static analysis of the relative performance of tensor operations to optimize the deep learning program. Experiments show that our approach can achieve up to 11? performance compared to dynamic profiling based methods with the same compilation time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Deep neural networks (DNN) are the mainstay of many applications including image classification, natural language processing, speech recognition, and automated driving. Cloud service providers are offering machine learning services that can compile, optimize, and deploy DNN on various target hardware. Optimizing a large set of DNN models, from convolutional neural networks to transformer-based networks, for a wide range of target hardware, from general purpose processors to AI accelerator ASICs, is constrained by two primary factors:</p><p>Compilation time and expense. A long compilation time can be a bad user experience. In addition, the longer the compilation time, the greater the infrastructure costs.</p><p>Cross compilation. A compiler cannot assume that it has access to the target hardware and it must be able to cross-compile from a host machine with a different hardware than the target.</p><p>Current machine learning compilers use two common ways to generate high performance deep learning code for multiple target hardware. The first method is based on auto-tuning, as implemented in AutoTVM <ref type="bibr" target="#b0">[1]</ref>, Ansor <ref type="bibr" target="#b1">[2]</ref>, FlexTensor <ref type="bibr" target="#b2">[3]</ref>,</p><p>and Halide auto-scheduler <ref type="bibr" target="#b3">[4]</ref>. Systems using this approach usually conduct a machine-learning driven auto-tuning search across a large predefined program transformations space. The system generates code samples and measures their performance on the target machine. It trains a machine learning model based on the collected measurement data, and use this model to guide subsequent code sample selection and code generation. The second method uses vendor-supplied kernel libraries (e.g., <ref type="bibr">TensorRT, OneDNN)</ref>. This is widely adopted by current machine learning frameworks such as MXNet, PyTorch, and TensorFlow. Both these methods, however, have several drawbacks that restrict their application at scale. Although the auto-tuning approach significantly reduces the engineering effort needed to hand-craft efficient code for each target platform, training an effective cost model for that target hardware requires collecting profile data from the execution of entire DNN on a real device. This breaks the cross compilation constraints for production compilation service. Furthermore, auto-tuning based compilation requires large number of samplings in program transformation space to converge to local optima, which can be prohibitively long. For example, tuning TensorFlow SSD MobileNet for Amazon Graviton2 target with AutoTVM takes 240 hours. On the other hand, vendor kernel libraries provide efficient handcrafted kernel codes which do not require auto-tuning. However, vendor libraries don't cover every operation used in DNN models. Adding more kernels requires heavy engineering effort. In addition, one vendor library usually only supports a specific hardware architecture. We have to integrate various libraries into production service to support different target hardware, making it hard to manage service infrastructure and extend to new target.</p><p>In order to resolve the challenge of building a compilation service with limited compilation time and cross-compilation mechanism, this paper proposes a system, Tuna, that utilizes the static analysis and a combination of analytical cost modeling to optimizing DNN. Tuna has the following advantages comparing to auto-tuning based compilation:</p><p>? Optimization is done with static analysis and no real hardware is required.</p><p>? Hardware-based cost model is transferable to different micro architectures. Only one single cost model is required for one major architecture, such as CPU and GPU. ? Unlike performance measurement which requires sequential execution on a target device, static analysis tasks can be fully paralleled on a multi-core CPU machine, which largely reduces total analyzing time. Comparing to vendor kernel libraries, Tuna supports a large set of DNN kernels. Tuna provides a uniform compilation stack for various target hardware.</p><p>In this paper, we create cost model for both CPU and Nvidia GPU architectures. We evaluate our approach on cloud servers and edge devices. Experimental results show our approach largely reduces total compilation time while achieving better average performance comparing to dynamic profiled cost model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. OVERVIEW</head><p>Tuna is a deep learning kernel optimization framework fully relying on compile-time static analysis. Figure <ref type="figure">1</ref>  (</p><formula xml:id="formula_0">)<label>1</label></formula><p>Tuna is built upon TVM <ref type="bibr" target="#b4">[5]</ref>, a deep learning compiler stack. While we reuse the tensor expression system, IR and code generator from TVM, the following two key components are the enhancements:</p><p>Hardware related cost model. Existing search-based approaches mainly rely on high-level loop structure features, such as loop unrolling, loop vectorization and number of arithmetic operations. We argue that these high level loop features don't include hardware specifications which have great impact on kernel performance. Tuna analyzes both program IR and low-level generated codes to extract target hardware related features, such as number of SIMD instructions, CPU cache locality, GPU shared memory utilization, etc. With these low-level hardware features, Tuna cost model is accurate enough to predict the relative performance for a set of transformations of a tensor program, rather than relying on experimental execution on real hardware device.</p><p>Multi-threaded search algorithm. A multi-threaded search algorithm which can fully utilize multi-core CPU resources is the key to accelerate searching. We choose Fig. <ref type="figure">1</ref>: Overview of the static analysis approach to optimizing tensor programs.</p><p>evolution strategies <ref type="bibr" target="#b5">[6]</ref> as search algorithm. Evolution strategies is a parallel black-box optimization algorithm. The computation among each iteration of population can be executed simultaneously. With multi-core CPU machine or a multi-machine compilation system, the total searching time can be significantly reduced to be fit into the normal compilation flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. HARDWARE RELATED COST MODEL</head><p>This section describes the analytical cost model to predict tensor program performance. Tuna system can be formulated as following: we extract a series of hardware related features, f 0 , f 1 , ..., f n , which have great impact on deep learning tensor programs performance. These features can fall into two categories:</p><p>Performance related instructions. Tuna counts the number of low-level instructions that dominates tensor program performance, which mainly includes arithmetic and data movement instructions. We propose a unified method which jointly parses high-level program IR and low-level assembly code to get an accurate estimation of instruction numbers.</p><p>General hardware features. Hardware features such as cache locality and instruction level parallelism, have great impact on program performance. Specific algorithms are required to extract such features from program IR and assembly codes.</p><p>The program performance score is computed linearly with respect to features:</p><formula xml:id="formula_1">score = a 0 * f 0 + a 1 * f1 + ... + a n * f n<label>(2)</label></formula><p>The coefficients a 0 , a 1 , ..., a n are generated for each hardware architecture through hardware instruction latency and empirical profiling data. Tuna is generic enough to support different hardware architectures. This paper covers Intel CPU, ARM CPU and Nvidia GPU. We also investigate the transferability of the Tuna cost model across different micro architectures. Experiment results of high-end sever level chips versus resource-limited edge devices show that if two different micro architectures share the same set of Single-Instruction-Multiple-Data (SIMD) related instructions, a single cost model can be applied to both target platforms without modification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. CPU cost model</head><p>This section describes Tuna CPU cost model in detail. To achieve decent inference speed for deep learning program on modern CPU, it is common to do the following optimizations: thread-level parallelism to utilize multi-core CPU resources. Loop tiling to ensure good cache locality. Efficient utilization of SIMD registers to maximize vector computation throughput. Similarly, it's necessary to include these hardware features as parts of our CPU cost model to accurately analyze program performance. Tuna CPU cost model includes the following features: Number of SIMD instructions. Compute-intensive tensor program performance is dominated by vector arithmetic and data movement instructions. For Intel AVX instruction set, vfmadd and vmov are the most common instructions in conv2d and dense operators, while for AARCH64 Neon fmla, ld and st are used. We parse the program IR and assembly codes to get the total number of these significant SIMD instructions.</p><p>Estimation of L1 cache miss. Cache locality is a key factor for tensor program on CPU. An analytical method is proposed in this paper to estimate L1 cache miss for a given program IR.</p><p>Instruction level parallelism. Comparing to handcraft kernel libraries which maximize fma instruction parallelism, search-based approach involves more complicated instruction arrangements. It is significant to evaluate whether the generated codes can keep CPU pipeline busy.</p><p>1) Loop structure mapping: We started from counting the number of SIMD instructions. Program IR represents the transformed tensor program in high level pattern, which preserves the complete loop structures. However, the actual SIMD instruction arrangement is opaque in high-level program IR, due to optimizations done in code generation process, such as register allocation and superword-level parallelism vectorization. On the other side, low-level assembly codes provide detailed instruction information in each basic block. However, it is difficult to restore original loop structure from assembly control-flow graph. It's infeasible to extract instruction information solely from either program IR or assembly code.</p><p>In this paper we propose an algorithm which jointly parse high-level program IR and low-level assembly code. The key idea is to extract full loop information from program IR and low-level instruction quantity from assembly code. A pattern matching is executed to match loop bodies with assembly basic blocks and calculate total number of SIMD instructions.</p><p>Algorithm 1 shows the main procedure of jointly parsing. Loop blocks are extracted from program abstract syntax tree with pre-order depth-first-search. From assembly control flow graph, we identify a local basic block as a loop candidate with the following condition: Traversing from top to bottom of assembly code, there exists a jump instruction j targeting a basic block LBB x , and the position of LBB x is above j . Each pair of loop and basic block are then matched by checking whether they have the same iteration boundary. For matched basic blocks, we count the number of selected SIMD instructions. Finally, the algorithm calculates the total number of SIMD instructions for every loop block in original program IR.</p><formula xml:id="formula_2">LO O P-Map(IR, assembly) ForLoops = PR E O R D E R-DFS-For-Loop(IR) LoopLBBs = ID E N T I F Y-Loop-LBB(assembly) matchedIdx = 0 matchedLBBs = [] for each basicBlock in LoopLBBs do forLoop = ForLoops[matchedIdx] if PA T T E R N-Match-Loop(forLoop, basicBlock) then matchedLBBs.append(basicBlock) matchedIdx += 1 end end return CO U N T-Instruction(F orLoops, matchedLBBs)</formula><p>Algorithm 1: Algorithm for mapping program IR and assembly code 2) Cache locality: In this section, we developed an improved data locality model as an analyisis pass in TVM IR (TIR). This pass can analyze all kinds of computations supported by TVM in a short time. Therefore it provides a fast data locality approximation for the whole DNN network scheduling, and can be easily and systematically combined with other important models and passes together to guide the schedule selection of the whole network. The main idea of the model is that we approximately estimate the data footprint and data movement volume required to move into the cache of the object code. The object code was abstracted as a tree consists of loop-nodes and access-nodes. All loop-nodes are non-leaf nodes and carry all information of a loop statement. All access-nodes are leaf nodes that represent either a tensor load access or a tensor store access. The data footprint and data movement are calculated by traversing the bottom to the top of the tree.</p><p>We use Two Matrix Multiply (2MM) as an example to demonstrate the method for building data movement. Listing 1 shows an example of fused and tiled 2MM code. In this example the first Matmul uses i and j as free index and perform contraction over k . The second Matmul uses i and l as free index and perform contraction over j . The tiling loop of i and j are tiled and fused together, and other loops are / / Ni / Nj / Nk a r e p e r f e c t m u l t i p l e s o f T i / T j / Tk f o r ( i t = 0 ; i t &lt; Ni ; i t += T i ) f o r ( j t = 0 ; j t &lt; Nj ; j t += T j )</p><formula xml:id="formula_3">f o r ( k = 0 ; k &lt; Nk ; k ++) | f o r ( i 1 = 0 ; i 1 &lt; T i ; i 1 ++) | f o r ( j 1 = 0 ; j 1 &lt; T j ; j 1 ++) | C [ i + i t ] [ j + j t ]+= | A[ i + i t ] [ k ] * B [ k ] [ j + j t ] ; f o r ( l = 0 ; l &lt; Nl ; l ++) f o r ( i 2 = 0 ; i 2 &lt; T i ; i 2 ++) f o r ( j 2 = 0 ; j 2 &lt; T j ; j 2 ++) E [ i + i t ] [ l ]+= C [ i + i t ] [ j + j t ] * D[ j + j t ] [ l ]</formula><p>Listing 1: Fused and tiled two matrix multiplication non-tiled and non-fused. The cache capacity and values of all tile-size and problem size are known before the analysis starts.</p><p>We assume the cache capacity S is enough to store all data footprints below tiling loops, and is not enough to store data footprints of any tiling loop. This means Ni &gt; S and Nj &gt; S , but S &gt; Ti Tj + Ti Nl + Tj Nl + Tj Nk + Ti Nk .</p><p>The data footprint and data movement are computed from leaf node to the root of the tree. Since all leaf nodes are tensor accesses, the footprint and data movement for leaf nodes are both 1. The data footprint of a loop node is the number of distinct data elements accessed during all iterations of this loop. The data movement of a loop node is calculated based on its footprint and cache capacity. If the footprint of a single loop iteration is smaller than cache capacity, the data movement of this loop node is equal to its node footprint. Otherwise, the data movement of this loop node is evaluated as product of the number of iterations and the data movement of its single iteration. The data movement of single iteration is correlated to data movement of sub nodes, which is computed earlier in the bottom-up procedure.</p><p>In the 2MM example, based on our capacity assumption, the footprint of a single iteration of loop jt is Ti Tj + Ti Nl + Tj Nl + Tj Nk + Ti Nk which fits in cache. for all sub-loop nodes of jt, the data movement is equal to data footprint. For loop jt, since its single iteration footprint fit in cache, when the control flow goes to the next iteration, tensor A and E could get reuse because their access functions do not include index jt. Therefore, the data movement of loop jt is Ti Nj + Ti Nl + Nj Nl + Nj Nk + Ti Nk , which is still equal to its footprint. However, for loop it, even a single iteration footprint does not fit in cache. So the data movement of loop it will be the product of number of iterations and data movement of a single iteration, which is (Ti Nj +Ti Nl +Nj Nl +Nj Nk + Ti Nk ) * Ni /Ti . Figure <ref type="figure">2</ref> shows the loop structure and data movement calculation of the 2MM example.</p><p>Algorithm 2 shows the main procedure of visiting each node in the TIR node tree. When visiting a loop node, the algorithm recursively visits all its children, and compute the union of data footprint of a single iteration. The algorithm will detect whether the union of data footprint can fit in cache, and track the reuse status of each tensor. If the data footprint of that iteration fits in cache, the data movement volume is same as its footprint. Otherwise, the algorithm will use the tracked reuse status information to calculate the data movement. If the reuse status of the tensor is true, the movement volume of that tensor will be equal to footprint, otherwise the movement volume will be the movement volume of a single iteration multiply the trip-count. The reuse status will be true at the leaf node by default. While our analysis move from the bottom to top, the reuse status will be flipped to false if the following conditions are true. The first case is if tensor footprint exceed cache. The second case is if there exists a set of continuous loop nodes that do not access this tensor, such that their footprints exceed cache. Both implies the reuse distance of the tensor under discussion exceed the cache capacity. The whole analysis module is implemented by using Integer Set Library <ref type="bibr" target="#b6">[7]</ref>.  achieve high performance, the generated assembly code should allow the processor issue as much instructions as possible to keep the pipeline busy. Since the TVM framework includes more types of operators which will generates more types of instructions, a specific model for specific operator is not adequate to apply on the TVM-generated code. To solve this problem, we propose a static analysis model to estimate the instruction level efficiency.</p><formula xml:id="formula_4">VI S I T-</formula><p>The key idea of the model is to design a simplified fast out-of-order instruction scheduler that schedule instructions in each basic block. The scheduler consists of two major components, the data dependency builder and the instruction scheduler. The data dependency builder first scan the whole basic block, and creates two instruction dependency graph for true dependency and false dependency respectively. Then the instruction scheduler will schedule the instructions based on the dependency graph and hardware specifications such as instruction latency and number of different processing unit. During the scheduling, a timestamp will be assigned to each instruction, indicating the time point of the instruction starting executed. The first ready-to-execute instruction will be scheduled at cycle zero, and total cycles required for finalize all instructions will be used as the ILP cost of this basic block. We calculate the product of ILP cost and number of executions for a single basic block, and add up all products of all basic block. The summation is used as the ILP cost of the whole program.</p><p>During the scheduling, the scheduler will manage two different hazard, the structural hazard and data hazard. The structure hazard is controlled by limiting the maximum number of instruction issued at each cycle. If the maximum number of issued instruction reached the number of processing unit, the next instruction to be issued will be delayed to next cycle. The data hazard is identified by analyzing the dependency graph. If there is a read-after-write (RAW) dependency between two instructions, the consumer instruction should be scheduled after the producer instruction finishing execution. If there is a write-after-read (WAR) dependency or write-after-write dependency, the latter instruction who writes to the resource cannot be scheduled before the prior instruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Nvidia GPU cost model</head><p>This section describes our Nvidia GPU cost model in detail. To achieve efficient execution, GPU tensor program needs to exhibit decent thread-level parallelism to utilize GPU thread resources. Data locality is also significant for shared memory efficiency. Our GPU cost model includes the following features:</p><p>Number of PTX instructions. Similar to CPU assembly codes, we select fma, ld and st as the most significant instructions for tensor programs. We parse the Nvidia GPU PTX code to get the total number of these instructions.</p><p>Thread level parallelism. Utilization of GPU threads largely determines the performance of GPU kernels. We evaluate several aspects that directly affect GPU threadlevel parallelism: number of operations in a single thread, Streaming Multiprocessor (SM) occupancy, warp latency hiding and shared memory bank conflict.</p><p>1) Loop structure mapping: The NVCC compilation process unrolls small loops with a known trip count by default, which makes it hard to identify the corresponding loop structure in high-level program IR from the low-level PTX code.</p><p>In this paper we propose an algorithm that parses the PTX code, identifies loop structures and calculates total number of instructions. The key idea is to identify the loop structure from PTX code and loop iterations from registers.</p><p>Algorithm 3 shows the key idea of how we get the number of instructions. We adopt the same idea from Algorithm 1 of identifying loop structures in assembly code since PTX code has similar condition and jump structures. After we have the loop structures, we maintain a register initial value map and register value update map by parsing the PTX code. Since we already know the loop structure, once we reach the line with eligible condition check, we know it is for a certain loop. Based on the condition check, we get the register used for comparing and the end condition. From the two maps we maintain, we can easily calculate the loop iterations by initial value, update value and end condition. Finally, the algorithm calculates the total number of PTX instructions for every loop.</p><p>2) Thread level parallelization: In this section we discuss the details about features regarding to thread-level parallelism.</p><p>Workload per thread We count the total number of PTX instructions for a single thread with Algorithm 3. We calculate the total number of cycles based on instruction cycles from <ref type="bibr" target="#b7">[8]</ref>: The workload per thread is represented by the total number of cycles.</p><formula xml:id="formula_5">P T XInstruction i=1 Count(i) * Cost(i)<label>(3</label></formula><p>Streaming Multiprocessor (SM) occupancy It is important to keep all SMs busy for compute-intensive tensor programs. We check the total number of thread blocks to determine whether it is greater than the total number of SMs, so that all SMs have at least one block to execute. A penalty is added if the total number of thread blocks is too small. Warp latency hiding Warp scheduling is a vital mechanism to hide GPU instruction latency. With more warps on each SM, GPU warp scheduler has a better chance to hide memory latency, thus achieving better performance. We calculate the maximum number of thread blocks that can be concurrently scheduled in one SM by checking the number of registers and shared memory usage per block. These information can be extracted with nvcc ptxas-option command.</p><p>Shared memory bank conflict For modern Nvidia GPUs (compute capability &gt;= 5.0), shared memory has 32 banks that are organized such that successive 32-bit words map to successive banks. Each bank has a bandwidth of 32 bits per clock cycle and any shared memory load or store of n addresses that spans n distinct memory banks can be served simultaneously, yielding an effective bandwidth that is n times as high as the bandwidth of a single bank. However, if multiple addresses of a memory request map to the same memory bank, the accesses are serialized. The hardware splits a memory request that has bank conflicts into as many separate conflict-free requests as necessary, decreasing the effective bandwidth by a factor equal to the number of separate memory requests. The one exception here is when multiple threads in a warp address the same shared memory location, resulting in a broadcast. In this case, multiple broadcasts from different banks are coalesced into a single cast from the requested shared memory locations to the threads. To incorporate the effect of bank conflict, we first numerically evaluate the shared memory access indices of all threads in the first warp from the IR to compute the actual shared memory throughput. We then use the ratio between actual shared memory throughput and requested shared memory throughput to adjust the number of shared memory operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SEARCH ALGORITHM</head><p>In this paper, we need to search in a well-defined large search space to find the optimal configuration. We choose Evolution Strategies (ES) as the search algorithm and treat the search as an arbitrary black-box optimization problem.</p><p>ES works by treating the model of interest as an arbitrary optimization problem. Given parameters of the model and the associated performance of that model on the task of interest, ES is able to optimize and train the model without any knowledge of the structure or architecture of the model itself. Specifically, at each iteration, random gaussian noise is added to the parameters to generate variations of the current model. Each variation is then tested for performance on the task of interest. Finally, a weighted sum is taken, based on performance, to generate an updated model. This becomes the current model for the next iteration. In standard ES, we treat the performance of the model within its environment as a black-box optimization problem. Now we extend that concept to treat ES itself as a black-box optimization problem with respect to the learning rate ? and standard deviation of noise ?.</p><p>Get learning rate ?, noise standard deviation ?, initial policy parameters ? 0 for t = 0, 1, 2, ... do Sample ? 1 , ...? n ? N (0, I) for i = 1, ..., n do</p><formula xml:id="formula_6">F i = F (? t + ?? i ) end ? t+1 = ? t + ? 1 n? n i=1 F i ? i end Algorithm 4: Evolution Strategies</formula><p>The main steps about ES are given in Algorithm 4, F denotes objective function working with parameters ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EVALUATION</head><p>This section evaluates the performance of Tuna. The following hardware platforms are selected to cover the most widely used cases for deep learning inference: Intel Xeon Platinum 8124M CPU (Amazon EC2 C5.9xlarge), AWS Graviton2 (Amazon EC2 M6G.4xlarge, ARM 64-bit CPU), ARM Quadcore Cortex-A53 64-bit CPU (Acer aiSage), Nvidia Tesla V100 GPU (Amazon EC2 P3.2xlarge) and Nvidia Jetson AGX Xavier GPU (512-core Volta GPU with Tensor Cores).</p><p>We evaluate Tuna on two levels: single operator and entire neural network. We use AutoTVM as search-based tuning baseline for both levels of evaluation. We also compare Tuna with state-of-the-art deep learning framework solutions for the entire neural network evaluation.</p><p>We use TVM v0.7.0 release for all target devices. For Intel CPU targets, we chose TensorFlow 1.15.3 and Pytorch 1.6.0 as deep learning framework solutions. For AWS Graviton2 which has AARCH64 CPU, we use ARM Tool Solution docker container which provides AARCH64 optimized version of TensorFlow and Pytorch. For AWS P3 instance which has Nvidia V100 GPU, we use NGC docker container with tag 21.03-tf1-py3 and pytorch:21.03-py3 which optimizes Tensor-Flow and Pytorch for Nvidia GPU. For Nvidia Jetson AGX Xavier GPU, we choose TensorFlow 1.15.2 and Pytorch 1.6.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Entire Network</head><p>We first report Tuna performance on a set of deep learning models pre-trained with TensorFlow and PyTorch, two popular deep learning frameworks: TensorFlow SSD MobileNet v2, TensorFlow SSD Inception v2, PyTorch ResNet50 v1 and PyTorch Bert base uncased. Selected models cover the most popular deep learning applications: image classification, object detection and natural language processing. Three metrics are measures to demonstrate the advantages of Tuna: Compilation cost. We measure the cost in dollar to optimize deep learning models on Amazon EC2 instances to demonstrate the cost reduction provided by Tuna. We multiply instance on-demand price with compilation time to get the cost to compile. We benchmark Tuna on Amazon EC2 C5.24xlarge which takes $4.08 per hour. Table <ref type="table" target="#tab_5">III</ref> shows Tuna reduces compilation cost down to 1.1% of the original cost comparing to AutoTVM.</p><formula xml:id="formula_7">Compilation time.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inference latency.</head><p>Table <ref type="table" target="#tab_4">II</ref> shows the model inference latency from different methods. The row of AutoTVM Partial shows the model inference latency compiled with AutoTVM in the same compilation time of Tuna. Results show that with the same compilation time, Tuna achieves up to 11? performance of AutoTVM. We also compare Tuna latency with AutoTVM full tuning models. On average Tuna achieves 91.5% performance comparing to best possible schedules generated by AutoTVM. This result shows that Tuna is able to achieve similar performance of full tuning with significant less time and cost. Finally we compare Tuna performance with deep learning framework solutions. Tuna achieves up to 17.3? performance. Due to memory resource limitation on Acer aiSage device, directly running inference through deep learning framework is infeasible. We only compare Tuna with AutoTVM on this platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Single Operator</head><p>In this section we report the performance of a set of widely used compute-intensive deep learning operators. We use AutoTVM tuned operator performance as baseline. We reuse the optimization search space defined in Au-toTVM to make apple to apple comparison. We bench-mark conv2d , conv2d winograd , depthwise conv2d and batch matrix multiplication performance on ARM CPU and Nvidia GPU devices. We don't measure conv2d winograd on Intel CPU device since AutoTVM doesn't define optimization space for this operator.</p><p>We define top -k performance ratio as metric. Tuna generates top-k best programs and execution latency of all programs are sum up. Similarly we calculate the latency summation for top-k best programs generated with AutoTVM. We divide two AutoTVM latency value with Tuna to evaluate the effectiveness of Tuna to select decent optimizations from search space. Higher value approaching 1 indicates Tuna can accurately predict the real execution performance. Figure ?? and Figure <ref type="figure" target="#fig_3">4</ref> shows the benchmark result on Intel CPU, ARM CPU and Nvidia GPU. On average We got 0.869 for top 10 and 0.873 for top 50. Experimental data shows Tuna is able to achieve quite close performance comparing to full tuning method with AutoTVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RELATED WORKS</head><p>Performance modeling has been a challenging problem due to the diversities of hardware architectures. Over the years, researchers have proposed many analytical approaches. Here we summarize notable related works.</p><p>CPU performance optimization: Several state-of-theart prior works has focused on analyzing the performance on CPU and generating efficient code. They includes cache modeling tools <ref type="bibr" target="#b8">[9]</ref> [10], static tensor libraries <ref type="bibr" target="#b10">[11]</ref>  <ref type="bibr" target="#b11">[12]</ref> [13], model-driven tensor computation optimizers <ref type="bibr" target="#b13">[14]</ref>  <ref type="bibr" target="#b14">[15]</ref>, polyhedral compilers and polyhedral optimizations <ref type="bibr" target="#b15">[16]</ref> [17] <ref type="bibr" target="#b17">[18]</ref> [19] <ref type="bibr" target="#b19">[20]</ref>  <ref type="bibr" target="#b20">[21]</ref>, and auto-tuning based compilers <ref type="bibr" target="#b4">[5]</ref> [22] <ref type="bibr" target="#b1">[2]</ref>. However, all of the state-of-the-art researches cannot be the off-the-shelf solution to our system for the following reasons. Cache miss analyzers are accurate on analyzing the cache misses but are not able to find the optimal loop schedule. Polyhedral compilers are usually separating the tile-size selection and loop transformation, which makes its performance not competitive to other frameworks. Auto-tuning based compilers are tuning the loop schedule comprehensively for the whole network but results in very long tuning and compiling time. Libraries are achieving high-performance on some important operators but their static optimization may not be optimal for all types of problem sizes, and they could block further graph-level optimizations such as fusion. Model-driven tensor computation optimizers are efficient and comprehensive, but they are not yet fully automated and can only optimize specific computations such as tensor contractions and 2D convolutions.</p><p>GPU performance modeling: Static analysis method has been used for GPU performance modeling. Arafa et al. <ref type="bibr" target="#b7">[8]</ref> developed a GPU performance prediction framework named PPT-GPU, which utilizes a hybrid approach between analytical modeling and cycle-accurate modeling.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>shows the overall architecture of Tuna. The system inputs are a tensor program e together with corresponding loop transformation candidate space T e . For a transformation t ? T e , let i = g(e, t) be the intermediate representation of transformed program. Code generator then generates low-level code a (e.g., assembly, PTX). Hardware-based program feature is extracted via pf = f (i , a). Program performance score is calculated through c(pf ), with cost model function c. The objective of Tuna is formalized as the following: arg min t?Te c(f (g(e, t), a)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3 )Fig. 2 :</head><label>32</label><figDesc>Fig. 2: Loop structure and data movement of 2MM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>)</head><label></label><figDesc>LO O P-Map-PTX(P T X) LoopBBs = ID E N T I F Y-Loop-BB(P T X) regInitMap, regUpdateMap = RE G I S T E R-Match-Loop() LoopIterations = [] for each basicBlock in LoopBBs do loopIteration = GE T-Iterations(regInitM ap, regU pdateM ap) LoopIterations.append(loopIteration) end return CO U N T-Instruction(LoopBBs, LoopIterations) Algorithm 3: Algorithm for identifying loop iterations in PTX code</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Top50 performance ratio for single operators from Tuna VS AutoTVM.</figDesc><graphic url="image-4.png" coords="11,53.69,317.26,241.60,122.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Table II records the compilation time of both Tuna and AutoTVM to optimize neural networks. Experiments show that Tuna can speed up compilation process up to 339? comparing to AutoTVM.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>They used features such as instruction statistics from the PTX code,</figDesc><table><row><cell>Unit: ms</cell><cell cols="4">TF SSD MobileNet TF SSD Inception PT ResNet50 PT Bert</cell></row><row><cell>Framework</cell><cell>22.09</cell><cell>24.29</cell><cell>31.26</cell><cell>253.04</cell></row><row><cell>AutoTVM Partial</cell><cell>30.41</cell><cell>47.7</cell><cell>11.47</cell><cell>86.39</cell></row><row><cell>AutoTVM Full</cell><cell>22.7</cell><cell>30.29</cell><cell>6.37</cell><cell>16.66</cell></row><row><cell>Tuna</cell><cell>23.3</cell><cell>30.16</cell><cell>6.85</cell><cell>15.12</cell></row><row><cell></cell><cell cols="3">(a) Entire network performance on a system with Intel Xeon Platinum 8124M CPU</cell><cell></cell></row><row><cell>Unit: ms</cell><cell cols="4">TF SSD MobileNet TF SSD Inception PT ResNet50 PT Bert</cell></row><row><cell>Framework</cell><cell>37.15</cell><cell>43.39</cell><cell>307.55</cell><cell>56.7</cell></row><row><cell>AutoTVM Partial</cell><cell>46.95</cell><cell>75.37</cell><cell>195.38</cell><cell>63.62</cell></row><row><cell>AutoTVM Full</cell><cell>29.55</cell><cell>45.56</cell><cell>16.11</cell><cell>15.11</cell></row><row><cell>Tuna</cell><cell>30.24</cell><cell>44.3</cell><cell>17.77</cell><cell>16.13</cell></row><row><cell></cell><cell cols="3">(b) Entire network performance on a system with AWS Graviton2 ARM CPU</cell><cell></cell></row><row><cell>Unit: ms</cell><cell>TF SSD MobileNet</cell><cell cols="2">TF SSD Inception PT ResNet50</cell><cell>PT Bert</cell></row><row><cell>AutoTVM Partial</cell><cell>3499.72</cell><cell>4487.97</cell><cell>7111.88</cell><cell>1058.85</cell></row><row><cell>AutoTVM Full</cell><cell>618.45</cell><cell>1419.83</cell><cell>1114.69</cell><cell>574.37</cell></row><row><cell>Tuna</cell><cell>614.43</cell><cell>1389.35</cell><cell>1259.88</cell><cell>541.17</cell></row><row><cell>Unit: ms</cell><cell cols="4">TF SSD MobileNet TF SSD Inception PT ResNet50 PT Bert</cell></row><row><cell>Framework</cell><cell>31.65</cell><cell>33.2</cell><cell>8.65</cell><cell>16.34</cell></row><row><cell>AutoTVM Partial</cell><cell>44.77</cell><cell>70.97</cell><cell>105.16</cell><cell>13.03</cell></row><row><cell>AutoTVM Full</cell><cell>27.43</cell><cell>30.69</cell><cell>2.34</cell><cell>3.24</cell></row><row><cell>Tuna</cell><cell>28.45</cell><cell>34.87</cell><cell>2.65</cell><cell>4.62</cell></row><row><cell></cell><cell cols="3">(d) Entire network performance on a system with Nvidia V100 GPU</cell><cell></cell></row><row><cell>Unit: ms</cell><cell cols="4">TF SSD MobileNet TF SSD Inception PT ResNet50 PT Bert</cell></row><row><cell>Framework</cell><cell>126.7</cell><cell>124.13</cell><cell>26.04</cell><cell>19.66</cell></row><row><cell>AutoTVM Partial</cell><cell>202.21</cell><cell>426.44</cell><cell>21.16</cell><cell>51.32</cell></row><row><cell>AutoTVM Full</cell><cell>50.78</cell><cell>57.25</cell><cell>15.21</cell><cell>7.64</cell></row><row><cell>Tuna</cell><cell>59.81</cell><cell>80.05</cell><cell>18.82</cell><cell>11.49</cell></row><row><cell></cell><cell cols="3">(e) Entire network performance on a system with Nvidia Jetson AGX Xavier GPU</cell><cell></cell></row></table><note><p>(c) Entire network performance on a system with ARM Quad-core Cortex-A53 64-bit CPU(Acer aiSage)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I :</head><label>I</label><figDesc>Entire network performance of Tuna and the selected baselines. Comparing to AutoTVM and Ansor, Tuna applies hardware feature related cost model which doesn't require any training and accurately predict relative performance. Halide auto-scheduler [4] generates schedules for the whole Unit: hour TF SSD MobileNet TF SSD Inception PT ResNet50 Entire network compilation time for Intel Xeon Platinum 8124M CPU. Entire network compilation time for ARM Quad-core Cortex-A53 64-bit CPU(Acer aiSage).</figDesc><table><row><cell>instruction latencies, and L2 cache miss rate. Guerreiro et al. [23] proposed a GPU performance evaluation method based on recurrent neural networks and features extracted from PTX code. We need to emphasize the goal of Tuna is to sort and find the best kernels in a defined search space, while these approaches aim to accurately predict the performance of different kernels. One significant advantage of Tuna is that AutoTVM 53 Tuna 0.7 (a) Unit: hour TF SSD MobileNet TF SSD Inception PT ResNet50 Search-based auto-tuning: Search-based tuning approach has become an effective method to optimize deep learning programs. AutoTVM [1] relies on user-specified search space. Ansor [2] automatically generates search space for small sub-graphs. Both AutoTVM and Ansor create machine-learning based cost model and do training during 50 12 2.92 1 0.13 0.012 PT Bert AutoTVM 240 280 49 3.2 Tuna 3 5 0.45 0.062 (b) Entire network compilation time for AWS Graviton2 ARM CPU. Unit: hour TF SSD MobileNet TF SSD Inception PT ResNet50 PT Bert AutoTVM 299 316 64 5.14 Tuna 3 5 0.45 0.062 (c) Unit: hour TF SSD MobileNet TF SSD Inception PT ResNet50 PT Bert AutoTVM 93.5 167.5 37 4.4 Tuna 1.6 1.04 0.13 0.05 (d) Entire network compilation time for AWS P3 V100 GPU. Unit: hour TF SSD MobileNet TF SSD Inception PT ResNet50 PT Bert AutoTVM 202.5 356.3 83 4.4 exploration. PT Bert Tuna 1.7 1.05 0.13 0.05</cell></row><row><cell>it leverages information from both low level assembly code</cell></row><row><cell>and high level intermediate representations.</cell></row></table><note><p>(e) Entire network compilation time for Nvidia Jetson AGX Xavier GPU.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II :</head><label>II</label><figDesc>Entire network compilation time of Tuna VS AutoTVM. input program. All these search-based tuning methods require experimental execution on target device, while Tuna fully relies on static analysis.VII. CONCLUSIONIn this paper, we proposed Tuna as an analytical approach to analyze and optimize deep learning programs on modern CPU and GPU hardware. The experiments show that we are able to achieve 99.21% average throughput for three categories of deep learning models on various kinds of hardware platforms, with merely 1.65% average compilation time comparing to search-based tuning approach. Tuna achieves 1.54x average performance comparing to state-of-the-art deep learning framework inference solutions. Similar methodology can be applied to ASIC hardware. Extending Tuna to other architecture is a future work. Entire network compilation cost for Amazon EC2 C5.9xlarge(price $1.53 per hour). Entire network compilation cost for AWS Graviton2 ARM CPU(price $0.616 per hour). Entire network compilation cost for Amazon EC2 P3.2xlarge(price $3.06 per hour).</figDesc><table><row><cell>Unit: dollar</cell><cell cols="3">TF SSD MobileNet TF SSD Inception PT ResNet50</cell><cell>PT Bert</cell></row><row><cell>AutoTVM</cell><cell>81.09</cell><cell>76.5</cell><cell>18.36</cell><cell>4.47</cell></row><row><cell>Tuna</cell><cell>2.86</cell><cell>4.08</cell><cell>0.53</cell><cell>0.05</cell></row><row><cell cols="4">(a) Unit: dollar TF SSD MobileNet TF SSD Inception PT ResNet50</cell><cell>PT Bert</cell></row><row><cell>AutoTVM</cell><cell>147.84</cell><cell>172.48</cell><cell>30.18</cell><cell>1.97</cell></row><row><cell>Tuna</cell><cell>12.24</cell><cell>20.4</cell><cell>1.84</cell><cell>0.25</cell></row><row><cell cols="4">(b) Unit: dollar TF SSD MobileNet TF SSD Inception PT ResNet50</cell><cell>PT Bert</cell></row><row><cell>AutoTVM</cell><cell>286.32</cell><cell>512.61</cell><cell>113.22</cell><cell>13.46</cell></row><row><cell>Tuna</cell><cell>6.53</cell><cell>4.24</cell><cell>0.53</cell><cell>0.2</cell></row><row><cell></cell><cell>(c)</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>Entire network compilation cost of Tuna VS AutoTVM.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08166</idno>
		<title level="m">Learning to optimize tensor programs</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ansor: Generating high-performance tensor programs for deep learning</title>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengfan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minmin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cody</forename><surname>Hao Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameer</forename><surname>Haj-Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danyang</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koushik</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th {USENIX} Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="863" to="879" />
		</imprint>
	</monogr>
	<note>{OSDI} 20</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Flextensor: An automatic schedule exploration and optimization framework for tensor computation on heterogeneous system</title>
		<author>
			<persName><forename type="first">Size</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiwen</forename><surname>Sheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="859" to="873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatically scheduling halide image processing pipelines</title>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Teja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mullapudi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dillon</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kayvon</forename><surname>Fatahalian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">{TVM}: An automated end-to-end optimizing compiler for deep learning</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meghan</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th {USENIX} Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="578" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Tensor comprehensions: Framework-agnostic high-performance machine learning abstractions</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Szymon</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03864</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">isl: An integer set library for the polyhedral model</title>
		<author>
			<persName><forename type="first">Sven</forename><surname>Verdoolaege</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Congress on Mathematical Software</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="299" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ppt-gpu: Scalable gpu performance modeling</title>
		<author>
			<persName><forename type="first">Yehia</forename><surname>Arafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdel-Hameed A</forename><surname>Badawy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gopinath</forename><surname>Chennupati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nandakishore</forename><surname>Santhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Eidenbenz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Architecture Letters</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="58" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Analytical modeling of cache behavior for affine programs</title>
		<author>
			<persName><forename type="first">Wenlei</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriram</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Noel</forename><surname>Pouchet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ponnuswamy</forename><surname>Sadayappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Programming Languages</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A fast analytical model of fully associative caches</title>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Gysi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Grosser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurin</forename><surname>Brandner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation</title>
		<meeting>the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="816" to="829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Analytical modeling is enough for high-performance blis</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Tze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><forename type="middle">D</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><forename type="middle">M</forename><surname>Igual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enrique</forename><forename type="middle">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><surname>Quintana-Orti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Mathematical Software (TOMS)</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Blis: A framework for rapidly instantiating blas functionality</title>
		<author>
			<persName><forename type="first">Van</forename><surname>Field</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Zee</surname></persName>
		</author>
		<author>
			<persName><surname>Van De Geijn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Mathematical Software (TOMS)</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Intel oneapi deep neural network library (onednn)</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<ptr target="https://software.intel.com/content/www/us/en/develop/documentation/oneapi-programming-guide/top/api-based-programming/intel-oneapi-deep-neural-network-library-onednn.html" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Analytical cache modeling and tilesize optimization for tensor contractions</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Sukumaran-Rajam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Veras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Tze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrice</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atanas</forename><surname>Rastello</surname></persName>
		</author>
		<author>
			<persName><surname>Rountev</surname></persName>
		</author>
		<author>
			<persName><surname>Sadayappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Analytical characterization and design space exploration for optimization of cnns</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Sukumaran-Rajam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atanas</forename><surname>Rountev</surname></persName>
		</author>
		<author>
			<persName><surname>Sadayappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="928" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Vasilache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Zinenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodoros</forename><surname>Theodoridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">S</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Verdoolaege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04730</idno>
		<title level="m">Tensor comprehensions: Framework-agnostic high-performance machine learning abstractions</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A practical automatic polyhedral parallelizer and locality optimizer</title>
		<author>
			<persName><forename type="first">Uday</forename><surname>Bondhugula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Hartono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jagannathan</forename><surname>Ramanujam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ponnuswamy</forename><surname>Sadayappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM SIGPLAN Conference on Programming Language Design and Implementation</title>
		<meeting>the 29th ACM SIGPLAN Conference on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="101" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Polyhedral parallel code generation for cuda</title>
		<author>
			<persName><forename type="first">Sven</forename><surname>Verdoolaege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Juega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ignacio</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Tenllado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francky</forename><surname>Catthoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization (TACO)</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Polly-polyhedral optimization in llvm</title>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Grosser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongbin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghesh</forename><surname>Aloor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Simb?rger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armin</forename><surname>Gr??linger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-No?l</forename><surname>Pouchet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First International Workshop on Polyhedral Compilation Techniques (IMPACT)</title>
		<meeting>the First International Workshop on Polyhedral Compilation Techniques (IMPACT)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">2011</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">When polyhedral transformations meet simd code generation</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Veras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franz</forename><surname>Franchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-No?l</forename><surname>Pouchet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ponnuswamy</forename><surname>Sadayappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th ACM SIGPLAN conference on Programming language design and implementation</title>
		<meeting>the 34th ACM SIGPLAN conference on Programming language design and implementation</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="127" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Code generation in the polyhedral model is easier than you think</title>
		<author>
			<persName><forename type="first">C?dric</forename><surname>Bastoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 13th International Conference on Parallel Architecture and Compilation Techniques</title>
		<meeting>13th International Conference on Parallel Architecture and Compilation Techniques</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004. 2004. 2004</date>
			<biblScope unit="page" from="7" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Optimizing {CNN} model inference on cpus</title>
		<author>
			<persName><forename type="first">Yizhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruofei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vin</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 {USENIX} Annual Technical Conference ({USENIX}{ATC} 19)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1025" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gpu static modeling using ptx and deep structured learning</title>
		<author>
			<persName><forename type="first">Jo?o</forename><surname>Guerreiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nuno</forename><surname>Roma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Tom?s</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="159150" to="159161" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
