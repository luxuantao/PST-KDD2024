<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generation Probabilities Are Not Enough: Exploring the Effectiveness of Uncertainty Highlighting in AI-Powered Code Completions *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-02-14">14 Feb 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Helena</forename><surname>Vasconcelos</surname></persName>
							<email>helenav@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gagan</forename><surname>Bansal</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adam</forename><surname>Fourney</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Q</forename><surname>Vera Liao</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Generation Probabilities Are Not Enough: Exploring the Effectiveness of Uncertainty Highlighting in AI-Powered Code Completions *</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-02-14">14 Feb 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2302.07248v1[cs.HC]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large-scale generative models enabled the development of AI-powered code completion tools to assist programmers in writing code. However, much like other AI-powered tools, AI-powered code completions are not always accurate, potentially introducing bugs or even security vulnerabilities into code if not properly detected and corrected by a human programmer. One technique that has been proposed and implemented to help programmers identify potential errors is to highlight uncertain tokens. However, there have been no empirical studies exploring the effectiveness of this technique-nor investigating the different and not-yet-agreed-upon notions of uncertainty in the context of generative models. We explore the question of whether conveying information about uncertainty enables programmers to more quickly and accurately produce code when collaborating with an AI-powered code completion tool, and if so, what measure of uncertainty best fits programmers' needs. Through a mixed-methods study with 30 programmers, we compare three conditions: providing the AI system's code completion alone, highlighting tokens with the lowest likelihood of being generated by the underlying generative model, and highlighting tokens with the highest predicted likelihood of being edited by a programmer. We find that highlighting tokens with the highest predicted likelihood of being edited leads to faster task completion and more targeted edits, and is subjectively preferred by study participants. In contrast, highlighting tokens according to their probability of being generated does not provide any benefit over the baseline with no highlighting. We further explore the design space of how to convey uncertainty in AI-powered code completion tools, and find that programmers prefer highlights that are granular, informative, interpretable, and not overwhelming. This work contributes to building an understanding of what uncertainty means for generative models and how to convey it effectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The increasing power and availability of generative models has made it possible to create previously unimaginable tools for human-AI collaboration on tasks like writing <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12]</ref> and generating art or music <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>. Building on the same technology as large language models like GPT-3 <ref type="bibr" target="#b6">[7]</ref>, AI-powered code completion tools such as GitHub's Copilot <ref type="bibr" target="#b15">[16]</ref> (built on OpenAI's Codex model <ref type="bibr" target="#b10">[11]</ref>), Amazon's CodeWhisperer <ref type="bibr" target="#b0">[1]</ref>, and DeepMind's AlphaCode <ref type="bibr" target="#b12">[13]</ref>, recommend code completions within an integrated development environment (IDE) to help programmers author software. These tools are projected to have a profound impact on the productivity and experience of professional programmers and more broadly anyone engaging in programming tasks <ref type="bibr" target="#b26">[27]</ref>. However, models can be imperfect, and large language models in particular are often criticized for their inability to distinguish correct outputs from outputs that are seemingly coherent and plausible <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b24">25]</ref>-a topic that has received widespread public attention since the release of ChatGPT in November 2022 <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b54">55]</ref>. In the context of code completion, erroneous recommendations can introduce bugs or even security vulnerabilities into a code base <ref type="bibr" target="#b44">[45]</ref>.</p><p>For effective human-AI collaboration on code, programmers should be able to detect and correct any errors introduced by the code completion tool. This can be challenging as even experts may be susceptible to automation bias, overreliance, and automation-induced complacency <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b53">54]</ref>. To help operators detect and override errors in medical <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b35">36]</ref>, legal <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23]</ref>, hiring <ref type="bibr" target="#b31">[32]</ref>, and other high-stakes domains, conveying AI uncertainty and providing explanations has become of paramount importance <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref>. However, prior work often focuses on decision-support scenarios in which the AI system's prediction is typically limited to categorical output, such as a medical diagnosis. It is not clear how to translate these strategies to generative scenarios with complex outputs, where every generation may give rise to dozens or hundreds of small decisions for the operator-for instance, whether to keep, modify, or delete each token of recommended code.</p><p>Prior works have proposed <ref type="bibr" target="#b47">[48]</ref> and implemented <ref type="bibr" target="#b41">[42]</ref> techniques that help programmers identify potential errors by highlighting uncertain tokens. Similar to in-line spell-check in text editors, highlighted tokens are meant to draw attention to regions of the code that would benefit most from human oversight. However, to the best of our knowledge, there have been no empirical studies exploring the effectiveness of this technique. Furthermore, there are multiple ways of quantifying uncertainty and its unclear which is most effective. First, code generation models, like all large language models, have a notion of uncertainty baked into them-specifically, the likelihood that the model would generate a specific token given its surrounding context <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, what we refer to in this paper as the "generation probability" of the token. This is the notion of uncertainty upon which previous proposals and implementations were based <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b47">48]</ref>, but it may not align with programmers' intuition or needs, as we discuss in Section 3.1. Another notion of uncertainty more directly inspired by programmers' goals would be the likelihood that a programmer would need to modify or delete a token in order to solve the given problem. This cannot be obtained directly from the code generation model, but potentially approximated by building a separate "edit model" based on logs of programmers' actions in similar contexts.</p><p>In this paper, we explore the question of whether conveying information about uncertainty can enable programmers to more quickly and accurately produce code in collaboration with an AI-powered code completion tool, and, if so, which measure of uncertainty is most effective. Through a mixed-methods study with N = 30 programmers, we compare three conditions: providing the AI system's code completion alone, highlighting tokens with the lowest generation probability, and highlighting tokens with the highest predicted likelihood of being edited by a programmer. To implement the final condition, we build a simple version of an edit model to directly predict this likelihood, trained on a closed-world set of programming tasks. While the model we train is simplistic, there are paths to generalize the approach by learning from existing large-scale telemetry data (e.g., user logs), as discussed in Section 7.</p><p>We find that uncertainty highlighting can lead to improved speed and accuracy, but that the way in which uncertainty is measured matters. Specifically, highlighting tokens with the highest predicted likelihood of being edited leads to faster task completion and more targeted edits. It additionally improves accuracy across all coding tasks that we consider, though this improvement is not statistically significant with our relatively small sample size. In contrast, Figure <ref type="figure">1</ref>: Overview of two approaches to uncertainty highlighting explored in our study: using generation probabilities (bottom path) and directly predicting the likelihood of an edit (top path). The coding task ("Write a function to reverse a string") and generation provided are simplified, human-generated examples. In this example, the generated code with edit model highlights points the programmer to the location in which there is an error: it highlights the list comprehension which erroneously has "1" in place of "-1." The highlights based on generation probability, on the other hand, highlight the function name, which, while being high variance, does not necessarily have any errors. across a variety of measures, highlighting tokens with the lowest generation probability does not provide any benefit over the baseline condition in which no highlights are shown. Highlighting tokens according to the edit model increases the likelihood that participants edit those particular tokens. Participants also report a higher subjective utility for highlights generated using the edit model compared with those generated using generation probabilities.</p><p>In follow-up interviews, we further explore the design space of uncertainty highlighting for code generation by showing participants alternative design probes. We find that participants prefer highlights that are sufficiently granular to allow them to narrow in on potential errors, but without overwhelming them with unnecessary details-for example, shading is strongly preferred to presenting precise probabilities that may be distracting and slow programmers down. Participants also requested explanations for uncertainty highlighting to help them diagnose the underlying bugs and to have more control, such as setting the uncertainty highlighting threshold.</p><p>To summarize, our research makes the following contributions:</p><p>? We further our understanding of what uncertainty means for generative models and how to best convey it by conducting a mixed-methods, preregistered study comparing two different forms of uncertainty highlighting for AI-powered code completions, along with a baseline condition with no uncertainty highlighting, for a total of three conditions.</p><p>? We find that it is possible to meaningfully change programmer behavior by highlighting uncertain tokens; however, the type of uncertainty matters. Highlighting tokens according to their generation probability does not provide any benefit over the baseline with no highlighting across several different metrics, while highlighting tokens with the highest predicted likelihood of being edited leads to faster task completion and more targeted edits, and is subjectively preferred by study participants.</p><p>? We leverage design probes to further explore the design space of uncertainty highlighting, and find that participants prefer highlights that are granular, informative, interpretable, and not overwhelming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>AI systems can make mistakes for various reasons, and users may not always be able to detect their errors. This can lead to overreliance on AI, which prior work has shown can decrease task performance <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24]</ref>, reduce trust <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b55">56]</ref>, and worsen user experience <ref type="bibr" target="#b27">[28]</ref>. It is therefore important to help users detect errors in AI predictions, especially in high-stakes domains (such as healthcare and criminal justice) where erroneous predictions can have serious consequences <ref type="bibr" target="#b17">[18]</ref>. Previous research has shown that techniques such as communicating uncertainty <ref type="bibr" target="#b56">[57]</ref>, explaining AI predictions <ref type="bibr" target="#b16">[17]</ref>, and providing appropriate onboarding and user training <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b38">39]</ref> can help users detect errors in AI predictions. Communicating uncertainty, which is our focus, has been shown to be an effective method for improving task performance <ref type="bibr" target="#b2">[3]</ref>, detecting errors <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17]</ref>, and increasing trust <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b56">57]</ref>. However, it is still an open question how to best present uncertainty, how to compute it, and whether it helps for the increasingly important generative scenario of AI-powered code completion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Computing and Communicating Uncertainty</head><p>There are various techniques for computing and communicating uncertainty in AI predictions, such as using gauges, percentages, highlights, discretization, and deferring to the user (e.g., withholding uncertain predictions) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b52">53]</ref>. Post hoc calibration methods and auxiliary models can also be used to compute and improve uncertainty scores <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b39">40]</ref>. However, it is unclear how these techniques from classification-like tasks can be applied to generative tasks or the specific scenario of AI-powered code completion. This lack of generalization may be due to differences in the nature of the tasks. In classification, predictions are typically atomic, structured, and simple, whereas for text or code generation, predictions are often tokens of varying length and are unstructured. There is more to double-check and more to fix in an AI-generated code completion scenario, which motivates our research to explore how to effectively compute and present uncertainty in this context. We discuss specific methods to compute uncertainty for large language models in Section 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Highlighting Uncertainty</head><p>A few prior works have studied the use of inline highlights to communicate uncertainty in the generative scenario of AI-powered code completion. Sun et al. <ref type="bibr" target="#b47">[48]</ref> conducted qualitative studies to gain insight into the explainability-related needs of programmers, and found that their design, which highlighted uncertainty in a line of code using a wavy line, revealed the need for: alternative outputs for uncertain code suggestions, explanations for why the AI was uncertain, and the ability to display different levels of uncertainty (e.g., using more colors or hues). Similarly, Vaithilingam et al. <ref type="bibr" target="#b48">[49]</ref> conducted qualitative studies and found that programmers found it challenging to read and debug suggestions from Copilot, particularly when its suggestions were long. Their participants expressed a desire for highlighting uncertain code completions. Our work builds on these findings by quantitatively measuring the effect of highlighting uncertainty.</p><p>Inline highlights have been used to communicate uncertainty in many other related domains. Lank et al. <ref type="bibr" target="#b29">[30]</ref> studied the task of handwriting recognition and observed that errors in recognition can pass undetected by the user <ref type="bibr" target="#b29">[30]</ref>, but that highlighting the errors can help slow down the user and decrease the error rate. The use of static code analysis tools, such as pylint, <ref type="foot" target="#foot_0">1</ref>have also been popular in software development to improve programmer productivity. These tools are deployed at scale inside IDEs and can, for example, highlight errors in code, such as violations of syntax, coding conventions, or undefined variables. However, they may not catch errors in the logic of the program (e.g., an incorrect regex). Similarly, spell checkers in word processors like MS Word and Google Docs can detect and highlight spelling and common grammatical errors, which can improve incidental learning <ref type="bibr" target="#b32">[33]</ref>. AI-based services such as Grammarly can also be used to highlight grammatical and spelling errors in written text. Our work builds on this finding by incorporating highlighting in our AI-powered code completion tool to improve accuracy and user experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Generative Models and Programmer Performance</head><p>There is a growing body of research on the use of generative models to support people in a variety of tasks. In this paper, we focus on the use of generative models for code completion, but it is worth noting that there are many other generative tasks in which these models have been shown to be effective. These tasks include, but are not limited to, machine translation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, co-creative writing <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12]</ref>, and the generation of art or music with AI <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>. In all of these domains, there is evidence that generative models can help improve human performance.</p><p>Measuring performance in these domains can be challenging, as it is often multi-dimensional and may include subjective elements <ref type="bibr" target="#b14">[15]</ref>. For example, in the context of programming, it is difficult to capture programmer productivity in a single metric. However, there are dimensions that are clearly important and should be captured. These include the correctness of the code produced and the effort put in by the programmer. Therefore, in this study, we focus on capturing performance through metrics such as the number of unit tests passed and the time taken. While these metrics may not capture the full complexity of programmer productivity, they do provide valuable insights and can help inform future research on the use of generative models in this context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Two Notions of Uncertainty</head><p>In this section, we lay out the two notions of uncertainty that we consider in the study, and the reasoning behind each. We explain how each is calculated and walk through examples of highlights generated using the different notions. We end with a list of the hypotheses we explore in our study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Uncertainty in Code Generation Models</head><p>Code generation models, like the ones we consider in this paper, are large language models that have been trained or fine-tuned for the task of generating computer code. A language model is a system that predicts the conditional probability of a token (which may be a character, word, or other string) given either the preceding context or, in some cases, its surrounding context <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b46">47]</ref>. In the case of code generation models, the context may include, for example, prompts, comments, and previously written code. Blocks of recommended code are generated by sampling one token at a time. Language models range from simple n-gram models to vastly more sophisticated and expressive models based on modern self-attention architectures like the transformer <ref type="bibr" target="#b51">[52]</ref>, but all share this common structure.</p><p>The conditional probability of the model producing a particular token in a given contextwhat we refer to in this paper as the generation probability of the token-can be viewed as one particular localized notion of the model's uncertainty. Thus it is natural to consider revealing the generation probability to end users in order to convey the uncertainty of generated code. Indeed, Sun et al. <ref type="bibr" target="#b47">[48]</ref> propose highlighting low probability lines of code, and OpenAI's online playground interface <ref type="bibr" target="#b41">[42]</ref> includes an option to highlight individual tokens with either high or low generation probability.</p><p>While past work has shown generation probabilities to be predictive of which code suggestions programmers are likely to accept <ref type="bibr" target="#b37">[38]</ref>, this notion of uncertainty may not line up with programmers' intuition. For example, when it is time to introduce a new variable name, the model will have many choices, all of which may have low generation probability simply because none is significantly more statistically probable than the rest. However, programmers may incorrectly attribute the uncertainty as indicating a potential error. Likewise, if there are multiple correct ways to implement a function, they cannot all simultaneously have high generation probability, meaning that some will necessarily appear "uncertain." The disconnect between the meaning of a generation probability and a programmer's intuition may be exacerbated by the fact that language models can be confidently wrong, producing fluently inadequate <ref type="bibr" target="#b36">[37]</ref> outputs that are merely statistically plausible linguistically, but that lose or hallucinate information [e.g., <ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>Given that the ultimate goal of a programmer working with a code generation tool is generally to produce high quality code that does what it is intended to do, we hypothesize that a more useful notion of the uncertainty of a generated token is the likelihood that a programmer would need to modify or delete this token in order to arrive at code that satisfied their needs. This is not something that can be obtained directly from the code generation model. However, it could potentially be approximated by building a separate model based on logs of programmers' actions when presented with AI-generated code in similar contexts. Therefore, we propose that learning an edit model, which predicts the aforementioned interactions, may better capture a notion of uncertainty aligned with what programmers need to correct errors and improve the generated output.</p><p>In this paper, we learn a closed-world edit model to achieve two goals: (1) to show a proof-of-concept implementation of such an edit model, and (2) as a probe to determine whether revealing such a notion of uncertainty to programmers would enable them to more quickly and accurately produce code when collaborating with a code generation model. We next describe how we built this model for the purposes of our study. In Section 7, we discuss the feasibility of building a more general edit model, suitable for open-world settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Building the Closed-World Edit Model</head><p>For the purposes of our study, we did not build a general-purpose edit model, but rather a model that predicts what code programmers would be most likely to edit for the three specific coding tasks and code completions we use in the main study. These tasks are described in detail in Section 4.1. For each of these tasks (as well as two additional tasks that we piloted; see Section 4.1), we ran the task instructions through OpenAI's Codex model to obtain completions and generation probabilities. To generate data for the edit model, we then provided these completions to participants who were given instructions to "change the completions to ensure they have completed the task properly." Participants were also provided with unit tests to check their code.</p><p>We recruited nine participants. As with the main study, all participants were employees of a large technology company located in the United States, and had experience coding in Python. They were recruited through a mix of direct emails, posts on message boards, and word of mouth, and were paid $50. All interviews were conducted over a video-conferencing platform and lasted approximately one hour. Because of this time constraint, not all participants were able to complete all coding tasks; each coding task was completed by six participants. We created our closed-world edit model based on which tokens participants edited for each coding task. We programmatically tracked which tokens from each completion were edited, considering a token edited if at least one character had been changed, deleted, or commented out. For example, if a token was "==" and the user changed it to "!=," the token was considered edited. For each token, we then set the probability of an edit to be the fraction of participants who edited it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Setting Thresholds</head><p>In order to highlight the most uncertain tokens, it was necessary to choose two thresholds. In the condition using the generation probabilities as our notion of uncertainty, any token with generation probability lower than a specified threshold would be highlighted. In the condition using the edit model, any token with edit probability higher than a specified threshold would be highlighted. In order to make a fair comparison, we aimed to set these thresholds in such a way that the number of characters highlighted across the three coding tasks would be comparable between the two conditions -though the number of characters highlighted for any particular task might vary.</p><p>For the edit model, we chose to highlight characters that were edited by at least 4 out of 6 of the participants, resulting in a total of 203 highlighted characters across the three coding tasks. The amount of text highlighted was not too sensitive to this choice; moving to 3 out of 6 would not have changed the highlights at all for two of the three coding tasks, while the highlights in the third coding task would change for only one token.</p><p>To set the threshold for the generation probabilities, we aimed to highlight a similar number of characters. This could be achieved by highlighting tokens with probability less than 0.694, which would lead to 205 characters highlighted across the three conditions. <ref type="foot" target="#foot_1">2</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Differences and Imperfections in Generation Probabilities and the Edit Model</head><p>Figure <ref type="figure" target="#fig_1">2</ref> shows the code generations output by Codex for each of the three coding tasks, along with the highlights generated using generation probabilities (left) and the edit model (right). As noted earlier, an approximately equal number of characters are highlighted in both conditions. However, in the edit model condition the highlights concentrate in the Ugly Number task, reflecting that the nine participants changed this generation the most to get it working. This distribution of highlights helps restate an important characteristic of the edit model: it tends to correspond to coding errors, since highlighted tokens are those that participants most frequently changed or deleted in pursuit of correcting or improving the initial code completion. For example, in Ugly Number, the "end" keywords are extraneous, and are not required (or valid) in Python, and thus were deleted by participants. Likewise, in Most Common Word,  the "List" identifier is not defined, and most participants replaced it with the built-in "list" type (Python is case-sensitive).</p><p>Conversely, when using generation probabilities, highlights seem to be triggered by that start and end of important sequences (e.g., defining a new class, initiating a new test or loop, or returning a value). Likewise, comparator and Boolean operators are often highlighted (e.g., "&lt;=," "not," "and"), perhaps reflecting some momentary uncertainty about the construction and directionality of these expressions. Finally, in several cases, newly introduced variable names are also highlighted (e.g., in the Ugly Number and Most Common Word coding tasks). This pattern of highlights reflects our characterization of generation probabilities discussed in Section 3.1: At various moments in the generation process, decision points are reached where the model has several possible paths it can pursue. Since tokens are generated one at a time, all the uncertainty of choosing a path is imbued upon the token generated in that moment (yielding a low token generation probability). Once a path is decided, and the corresponding token is output, the generation probabilities of subsequent tokens tend to recover.</p><p>Since the generation probabilities and the edit model have different distributions and characteristics, we posit that they will similarly have different effects on programmers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Pre-registered Hypotheses</head><p>We posited and pre-registered nine hypotheses, <ref type="foot" target="#foot_3">3</ref> which we state informally here; the bolded variables in each hypothesis are defined more formally in Section 4.3.</p><p>First, we wanted to explore how much benefit uncertainty highlighting provides to participants in terms of their task performance and efficiency. Ideally, the highlights would point participants to errors in the code and therefore speed up the process of accurately completing the task. However, highlights may also distract participants, potentially increasing time spent and mistakes made.</p><p>? [H1] Highlight condition will affect the time it takes to complete the coding task.</p><p>? [H2] Highlight condition will affect accuracy on unit tests.</p><p>Secondly, because one goal of working with an AI-powered code completion tool is to reduce the amount of work that programmers need to do, we wanted to explore whether highlighting would affect how much participants have to edit or add to the code. We also examine whether the particular tokens edited vary with different forms of highlighting to see whether highlighted tokens are more likely to be edited.</p><p>? [H3] Highlight condition will affect the number of characters added to the code.</p><p>? [H4] Highlight condition will affect the overall survival rate of tokens in the code.</p><p>? [H5] The interaction between highlight condition and whether a given token is highlighted will affect the token-level survival rate.</p><p>Finally, we wanted to measure participants' subjective preferences for the tools. Since one of our goals is to reduce the effort required to get to working code, we hypothesized that highlights would affect cognitive load. Additionally, we hypothesized that participants would feel more favorably towards the completions, highlights, and the whole code completion tool in some conditions.</p><p>? [H6] Highlight condition will affect self-reported cognitive load.</p><p>? [H7] Highlight condition will affect self-reported completion utility.</p><p>? [H8] Highlight condition will affect self-reported highlight utility.</p><p>? [H9] Highlight condition will affect self-reported rankings of the code completion tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods</head><p>We conducted a mixed-methods study with 30 participants consisting of coding tasks and a post-task interview. All participants were employees of a large technology company located in the United States with experience of coding in Python. None had previously been exposed to our study. Participants were recruited through a mix of direct emails, posts on message boards, Figure <ref type="figure">3</ref>: Screenshot of the interface used in our study. The "Task Overview" section provides general instructions for the task and interface. The "Coding Task" section provides a description of the current coding task. The "Expected Format" section describes the code format (e.g., function names and definitions) expected by unit tests. The "Suggestions" section provides some suggestions for debugging in our interface. Participants have the option to run their code as a file ("Run your code") or run our unit tests on their code ("Run unit tests"). When satisfied with their code, they can select "Submit."</p><p>and word of mouth. The study was IRB approved with voluntary participation and paid $50. All interviews were conducted over a video-conferencing platform and lasted approximately one hour.</p><p>Of the 30 participants, 15 reported having more than 5 years of experience writing Python code, 11 participants reported having 1-5 years of experience, and 4 reported less than one year of experience. Likewise, 15 reported that they worked with Python code at least once a week, while 12 reported only using Python "when needed." Comparatively, participants were far less experienced with AI code completion tools: 16 participants never used such a system before. A detailed listing of participant experience is provided in Appendix A.</p><p>Finally, among our participants, 5 identified as women, 22 identified as men, and 3 declined to report. 16 were age 24-29, 8 were aged 30-39, 3 were aged 40-49, and 3 declined to report. Additionally, 2 identified as Black, 2 identified as Hispanic/Latino, 5 identified as White, 13 identified as Asian, and 8 declined to report.</p><p>We used a within-subjects design to compare different highlighting options. Each participant was asked to complete three coding tasks with three "different" AI-powered code completion tools (in actuality, the code generation was the same, only differing in the highlights shown). The three highlight conditions we used are as follows:</p><p>? No highlights: Only the generated code completion was displayed.</p><p>? Generation probability: The code completion was displayed with highlights on tokens with generation probability ? 69.4%.</p><p>? Edit model: The code completion was displayed with highlights on tokens with likelihood ? 66.7% of being edited or removed according to the edit model.</p><p>The order of tasks as well as assignments of highlight conditions to tasks were randomized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Coding Tasks and OpenAI's Completions</head><p>We aimed to select coding tasks that would satisfy the following criteria: (1) they could be completed in about ten minutes by a non-beginner Python programmer, and (2) they had reasonable but imperfect code completions given by the Codex model, ideally with a diversity of error types and frequencies among the tasks. We initially selected fifteen potential coding tasks from Leetcode <ref type="bibr" target="#b30">[31]</ref> using the "easy" setting. After piloting with three participants, we narrowed these down to a set of five tasks and gave the completions of these five tasks to nine additional participants in the edit model training phase, as described in Section 3.2. We finally selected three tasks from these five that best satisfied our selection criteria. These were Ugly Number, Base 7, and Most Common Word.</p><p>To generate the code completion, we ran the instructions provided by Leetcode as the prompt to Codex (retrieved in June 2022) with the following (mostly default) parameters: model: Code Davinci 002, temperature: 0.5, maximum tokens: 4000, top p:1, frequency penalty: 0, presence penalty: 0, and logprobs: 5. We collected the log probabilities of each token and converted these to generation probabilities by exponentiating. We trimmed the output of Codex to exclude any strings before the first Python keyword or comment that are not in Python (e.g., starting the output with "."), as productized versions of code completion tools, such as GitHub's Copilot, are likely to do this automatically. Below, we outline the three coding tasks and their completions (shown in Figure <ref type="figure" target="#fig_1">2</ref>).</p><p>Ugly Number <ref type="foot" target="#foot_4">4</ref> This coding task is described as follows: "An ugly number is a positive integer whose prime factors are limited to 2, 3, and 5. Given an integer n, return true if n is an ugly number." The output returned by Codex and provided to participants has many errors, including some syntax that is not in Python (e.g., "end" as a keyword) and the incorrect function name (i.e., "is_ugly" instead of "isUgly"). Despite these errors, it has the correct logic-for example, the function correctly uses the idea of dividing by 2 until it no longer is evenly divisible by 2. In other words, the completion provides useful conceptual help, but a participant would need to correct the many syntax errors to arrive at correct code.</p><p>Base 7 <ref type="foot" target="#foot_5">5</ref> This task is described as follows: "Given an integer num, return a string of its base 7 representation." The output returned by Codex and provided to participants has no syntax errors and only minimal conceptual errors; specifically, it appends an extra "0" to every output. Therefore, to arrive at correct code, a participant would need only to remove the extra "0." Most Common Word 6 This coding task is described in the following way: "Given a string paragraph and a string array of the banned words banned, return the most frequent word that is not banned. It is guaranteed there is at least one word that is not banned, and that the answer is unique. The words in paragraph are case-insensitive and the answer should be returned in lowercase." The output returned by Codex and provided to participants has minimal syntax errors (e.g., using "List" which has not been imported-this could be fixed by instead using "list" since the non-native Python version is unnecessary). It also has minimal logic errors, making one simple mistake where it tries to change a dictionary in place instead of setting it equal to a new variable. A participant can arrive at correct code by fixing these two small errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Study Procedure</head><p>At the start of the study, participants were given general instructions about the coding tasks they would be asked to complete as well as the interface. The interface is shown in Figure <ref type="figure">3</ref>. Participants were able to view the coding task description along with some examples of inputs and outputs. They were able to run their code for debugging, and also run a set of provided unit tests on their code at any time. For each coding task, once participants were satisfied with their solution, or after a limit of 10 minutes, they were asked a series of questions rating their experience with the tool and encouraged to explain their responses to the interviewer. First, they were asked to rate these statements regarding their subjective perception of the AI tool, each on a 7-point Likert scale:</p><p>1. I found the AI's code completions helpful as a starting point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">(If applicable) I found the AI's highlights helpful in determining what to edit.</head><p>3. I would be willing to pay to access the AI's code completions.</p><p>4. (If applicable) I would be willing to pay to access the AI's highlights. 5. I found the AI's code completions distracting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">(If applicable) I found the AI's highlights distracting.</head><p>They were also asked to complete the following reduced <ref type="foot" target="#foot_7">7</ref> NASA RAW TLX (written as "TLX" in the rest of the paper) questionnaire <ref type="bibr" target="#b21">[22]</ref>, which aims to measure subjective workload: After completing all three coding tasks, participants were prompted by the interface to "Please rank the three AIs you used in terms of how satisfied you were with it. Each AI must have a unique rank." The interface also reminded the participants which coding task they completed with each of the three code completion tools.</p><p>After completing the ranking question, a semi-structured interview was conducted. In the first part, participants discussed their perception of, and experience with, the three versions of AI tool, including ranking and comparing them. The second part aimed to further explore the design space of highlighting uncertainty in AI-powered code generation tools more generally. Participants were shown images of five alternative designs for discussions. We created these Figure <ref type="figure">4</ref>: An example of the completions provided in the three experimental conditions. Top left: No Highlights. Top right: Generation Probability. Bottom: Edit Model. On this particular coding task, more tokens were highlighted in the Generation Probability condition compared with the Edit Model condition; recall thresholds were chosen such that the sum of highlighted tokens across the three tasks was the same for the two conditions. alternative designs to explore the following dimensions: uncertainty granularity (e.g., about the token or the whole output), specificity (e.g., categorical or precise quantity), and interactivity (e.g., suggesting alternative tokens or automatically updating uncertainty). The first two are informed by Van der Bles et al. <ref type="bibr" target="#b49">[50]</ref>, which surveyed psychology and communication literature on uncertainty communication. Images of these alternative designs are included in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Measured Variables</head><p>We now formally define the measured variables that we consider in each of the hypotheses stated in Section 3.5:</p><p>? Time (H1) is measured by the length of time taken to complete the coding task, either by hitting the "Submit" button or by reaching the cap of 10 minutes.</p><p>? Accuracy (H2) is measured by the percentage of participant-facing unit tests the participant's code passed for a given coding task.</p><p>? Number of characters added (H3) is measured by how many additional characters the participant added to the code.</p><p>? Overall survival rate (H4) is measured by the aggregate percentage of tokens in the provided code completion that "survived," where survival constitutes not being edited, removed, or commented out.</p><p>? Token-level survival rate (H5) is measured by whether a given token in the provided code completion survived or not. As before, survival constitutes not being edited, removed, or commented out.</p><p>? Cognitive load (H6) is measured by the average response to the provided TLX questions, reverse-coded as appropriate.</p><p>? Completion utility (H7) is measured by the average response to questions 1, 3, and 5 (reverse-coded) on subjective perceptions of the tool.</p><p>? Highlight utility (H8) is measured by the average response to questions 2, 4, and 6 (reverse-coded) on subjective perception of the tool.</p><p>? Rank (H9) is measured by the raw rank score provided when the interface prompts the participants to rank the tools at the end of the study, with 1 being best and 3 worst.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Quantitative Results</head><p>In this section, we analyze each of the pre-registered hypotheses described in Section 3.5.</p><p>We provide additional exploratory analyses and participant quotes to shed light on potential reasons why our hypotheses do or do not hold. Our key quantitative results are summarized in Table <ref type="table" target="#tab_0">1</ref>. For each measured variable, unless stated otherwise, we ran a mixed-effects regression model (linear or logistic depending on the data type), using the highlight condition as the fix-effects independant variable, and participant ID and coding task as the random-effects variables. An omnibus p-value of the highlight condition was obtained through an ANOVA test, and if significant, we conducted pairwise comparisons with a post-hoc Tukey test. The left column of Table <ref type="table" target="#tab_0">1</ref> provides p-values for the omnibus tests, while the right column shows only those pairs of conditions for which the results were statistically significant or marginally significant in the post-hoc Tukey test.</p><p>We note that because we randomized both the order in which participants completed coding tasks and the assignment of coding tasks to conditions, the number of participants assigned to each coding task varied by condition, as shown in Table <ref type="table" target="#tab_1">2</ref>. Since we controlled for task assignment in all of our analyses, this randomization should not have a major impact on significance tests. However, it may have a small impact on the reported means since our sample size is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Impact of Highlight Condition on Time and Accuracy</head><p>We begin by analyzing the impact of the highlight condition on two quantitative measures of participants' performance: the time it took them to complete the task and their accuracy on unit tests. We find that participants are significantly faster at completing tasks in the Edit Model condition compared with the Generation Probability condition, even though this effect may be dampened by the 10-minute time limit that we set for each coding task. Participants are also most accurate in the Edit Model condition, though this difference is not statistically significant with our relatively small sample size. Below we discuss these results in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H1: Highlight condition will affect the time it takes to complete the coding task</head><p>To test hypothesis H1, we used the previously mentioned mixed-effects regression model with time (capped at 10 minutes) as the dependent variable and highlight condition as the independent variable (participant ID and coding task were always used as random-effects independent variables in all mixed-effect models in this paper, which we omit mentioning for the remaining analyses). An omnibus test suggests the effect of highlight condition is significant (p = 0.002) and therefore H1 is supported by our results. On average, participants spent 9.27 minutes per task in the No Highlights condition, 9.61 minutes in the Generation Probability condition, and 8.59 minutes in the Edit Model condition. A post-hoc Tukey test found that the mean value of the time taken was significantly different between the Edit Model condition and the Generation Probability condition (p = 0.003), with participants in the Edit Model condition completing their tasks in 89.9% of the time it took those in the Generation Probability condition on average. There was only a marginally  significant difference between the Edit Model condition and the No Highlights condition (p = 0.066), with participants completing tasks faster on average in the Edit Model condition; but, as shown in Figure <ref type="figure" target="#fig_3">5</ref>, this difference is consistent across tasks. We found no significant difference in time taken between the Generation Probability condition and the No Highlights condition (p = 0.511).</p><p>One potential limitation of our study design is that we capped the amount of time a participant could spend on each coding task at 10 minutes. This was done in order to make time for participants to engage with all three highlight conditions without causing fatigue. The time constraint affected the distribution of completion times for participants. In particular, we observed that in both the No Highlights and Generation Probability conditions, 70% of participants were impacted by the 10-minute cutoff, failing to complete the task on time. This number drops to just 44% in the Edit Model condition. While this makes our results harder to interpret, we conjecture that the difference in the average time taken between the Edit Model condition and the Generation Probability condition would be even more pronounced without the time cap since more participants were affected by the cap in the Generation Probability condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H2: Highlight condition will affect accuracy on unit tests</head><p>To test the effect of highlight condition on accuracy, we used a mixed-effects logistic regression model, with the fraction of unit tests passed as the dependent variable, highlight condition as the fixed-effects independent variable. An omnibus test showed the effect of highlight condition was not significant (p = 0.145) and therefore H2 is not supported by our data.</p><p>On average, participants achieved 38.3% accuracy in the No Highlights condition, 30.0% accuracy in the Generation Probability condition, and 50.0% accuracy in the Edit Model condition. Although the effect was not significant with our relatively small sample size, we do observe a trend across coding tasks, with participants obtaining the highest accuracy in the Edit Model condition and the lowest accuracy in the Generation Probability condition across all tasks, as shown in Figure <ref type="figure" target="#fig_4">6</ref>.</p><p>Our exploratory analyses shed more light on why accuracy was relatively low for simple coding tasks. Since accuracy may be impacted by the 10-minute time cap, we calculated the average number of unit tests passed over just the subset of participants who submitted each task before the time limit was up. We find that, for the Most Common Word and Base 7 coding tasks, all participants who completed the task in under 10 minutes had 100% accuracy on the unit tests across all three highlight conditions. For Ugly Number, participants who completed the task on time in the No Highlights condition had 100% accuracy, while those in the Generation Probability condition had 33% accuracy and those in the Edit Model condition had an accuracy of 50%. This suggests that, at least for some tasks, low accuracy arose from participants who timed out on the task.</p><p>We also explore the extent to which participants overrely on the highlights when given an incorrect or incomplete edit. Specifically, by editing only the tokens highlighted in the Edit Model condition, participants would be able to pass 100% of the provided unit tests for the Base 7 coding task. However, the unit tests are not comprehensive, and editing the highlighted tokens alone would result in improperly handling an edge case. Three out of the nine participants who were assigned to the Edit Model condition for Base 7 made such an edit and submitted their code. However, a similar number of participants made such an edit in the Generation Probability condition (three out of ten) and the No Highlights condition (two out of eleven), bringing into question whether this is truly overreliance or an error that programmers would make with or without the presence of highlights. Participants who made this suggested change in the Edit Model condition did submit their code much faster than those who made the same change in other conditions (6.84 minutes on average, compared with 9.05 minutes in the Generation Probability condition and 7.53 minutes in the No Highlights condition). Because the Edit Model condition was the condition in which this change was explicitly suggested and participants were much faster to submit their code, this may indicate a transfer of overreliance from the code completion onto the highlights. We discuss this more in Section 7, but because this analysis is exploratory and inconclusive, we leave this question for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Impact of Highlight Condition on Edits Made</head><p>Our second set of hypotheses concerns the specific changes that participants made to the code completions. We wanted to understand the extent to which the survival and modification of the AI-generated code were affected by our intervention. We found that the highlight condition has a significant effect on the number of characters added to the code, with the least characters added in the Edit Model condition. Interestingly, while there is no significant effect of highlight condition on overall token survival rate, the specific tokens edited varied by condition. Our analysis shows that the tokens highlighted in the Edit Model condition are not only (already) more likely to be edited by participants, but that highlighting them further boosts the chance that they will be edited or removed.</p><p>H3: Highlight condition will affect the number of characters added to the code To test H3, we used the same mixed-effects linear regression model with the number of characters added as the dependent variable. An omnibus test suggests the effect of highlight condition is significant (p = 0.010) and therefore H3 is supported by our results. However, in a post-hoc Tukey test, only the pairwise difference between the No Highlights condition and the Edit Model condition is significant (p = 0.011). If we take characters added as a proxy for (one component of) programmers' effort, this implies that the Edit Model can reduce programmers' required effort, which is in line with our results on time (H1 ). We acknowledge that this metric can only be considered "beneficial" to programmers in conjunction with other metrics, such as accuracy. This is because, in isolation, the metric of edits made could be wrongly optimized to a minimum (e.g., tricking a programmer into thinking an incorrect AI-generated code completion is correct, which leads to little edits). Because accuracy, for example, is shown to be (albeit not significantly) higher for the Edit Model across all tasks, we do not believe that the aforementioned pitfall of optimizing for this metric is the case in this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H4: Highlight condition will affect the overall survival rate of tokens in the code</head><p>To test H4, we used the same mixed-effects logistic regression model with token survival rate as the dependent variable. An omnibus test suggests the effect of highlight condition is not significant (p = 0.164) and therefore H4 is not supported by our results.</p><p>We found that, on average, the token survival rate in the No Highlights condition was 81.0%, while the rate was 82.7% in the Generation Probability condition, and 77.8% in the Edit Model condition. Although the differences between these rates are not significant, our analysis of H5 below adds nuance to the story, showing that the specific tokens edited does vary by highlight condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H5:</head><p>The interaction between highlight condition and whether a given token is highlighted will affect the token-level survival rate To test this hypothesis we used a mixed-effects logistic regression model, with token survival rate as the dependent variable, the interaction between highlight condition and whether a token was highlighted or not as the fixed-effects independent variables. An omnibus test suggests the effect is significant (p &lt; 0.001) and therefore H5 is supported by our results.</p><p>We observe that for both the Generation Probability condition and the Edit Model condition, tokens that have been highlighted are significantly less likely to survive (i.e., more likely to be edited or removed) than those that have not been highlighted (p = 0.001 and p &lt; 0.001, respectively). Specifically, for the Generation Probability condition, highlighted tokens have a 73.9% survival rate, while not-highlighted tokens have a survival rate of 80.9%. This difference is even more striking for the Edit Model condition, where highlighted tokens have a survival rate of only 35.3% compared with 87.1% for not-highlighted tokens. Tokens highlighted in the Edit Model condition are significantly less likely to survive than those highlighted in the Generation Probability condition (p &lt; 0.001). Similarly, the tokens that are not highlighted in the Edit Model condition are significantly more likely to survive (less likely to be edited or removed) than those not highlighted in the Generation Probability condition (p &lt; 0.001).</p><p>Additionally, tokens that are not highlighted in the Edit Model condition are more likely to survive than tokens in the No Highlights condition, where, by definition, all tokens are not highlighted (87.1% chance of survival compared with 79.3%, p &lt; 0.001). This effect does not exist for the Generation Probability condition; there is no significant difference in whether a token that is not highlighted will survive in the Generation Probability condition compared with a (not highlighted by definition) token in the No Highlights condition. All of these results are summarized in Table <ref type="table" target="#tab_0">1</ref>.</p><p>There are two possible factors that might contribute to these results. One is that highlighting a token causes participants to edit that token more often. The other is that the highlighted tokens are tokens that programmers would be more likely to edit anyway. Indeed, the tokens highlighted using the Edit Model are, by definition, those that we would expect programmers to edit most often. To tease apart these factors, we make two additional exploratory comparisons. First, we limit attention to the set of tokens highlighted in the Edit Model condition and compare the survival rate of these specific tokens in the Edit Model condition and the No Highlights condition. We find that their survival rate is 35.3% in the Edit Model condition, as reported above, compared with 53.5% in the No Highlights condition. <ref type="foot" target="#foot_8">8</ref> This suggests that the effect is coming from a mix of the two factors. The tokens highlighted in the Edit Model condition are inherently more likely to be edited by programmers, and highlighting them further increases their likelihood of being edited.</p><p>Similarly, we can limit attention to the set of tokens highlighted in the Generation Probability condition. We observe that their survival rate is 73.9% in the Generation Probability condition, as reported above, compared with 76.8% in the No Highlights condition. Interestingly, this suggests that the presence of highlights alone is not enough to meaningfully change participants' behavior. Rather, the highlights must reflect plausible changes that a programmer might be reasonably willing to explore, which generation probabilities do not appear to capture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Impact of Highlight Condition on Cognitive Load and Utility</head><p>Our third set of hypotheses concern self-reported measures of cognitive load and the utility of the completions and highlights. We wanted to measure how much, if at all, our interventions affected participants' preference towards the completion, the highlights, and the code completion tool on the whole. We found that the highlight condition has a significant effect on self-reported utility for the highlights, with higher utility reported in the Edit Model condition compared with the Generation Probability condition. However, we found no significant effect on cognitive load or code completion utility. Further, we observe that participants' ranking of the tools are dominated by their opinion of the quality of the code completions rather than the highlights. H6: Highlight condition will affect self-reported cognitive load. We defined cognitive load to be the average response across the TLX questions and tested this hypothesis using a mixed-effects linear regression model with cognitive load as the dependent variable. An omnibus test shows that the effect of highlight condition is not significant (p = 0.228) and therefore H6 is not supported by our results.</p><p>Participants reported a cognitive load of 49.5 on average in the No Highlights condition, 46.4 in the Generation Probability condition, and 43.5 in the Edit Model condition. Through exploratory analysis, we found that there is a sizable difference in the cognitive load reported by participants who completed a task before the 10-minute mark (35.6 on average) compared with participants who reached the 10-minute mark <ref type="bibr">(53.4)</ref>. This suggests that whether or not a participant felt they were able to complete the task may have influenced their perceived cognitive load more than the specifics of the highlight condition they were assigned. H7: Highlight condition will affect self-reported code completion utility. Defining code completion utility as in Section 4.2, we tested this hypothesis using a mixed-effects linear regression model with code completion utility as the dependent variable. An omnibus test suggests the effect of highlight condition is not significant (p = 0.679) and therefore H7 is not supported by our results.</p><p>The average code completion utility, on a 1-7 scale with higher being better, was 4.13 in the No Highlights condition, 4.27 in the Generation Probability condition, and 4.31 in the Edit Model condition. The null result may reflect the fact that the completions provided for each of the three coding tasks were the same across conditions. Any perceived differences in their utility would have to come from the highlights rather from the completions themselves.</p><p>H8: Highlight condition will affect self-reported highlight utility. Defining highlight utility as in Section 4.2, we tested H8 with a mixed-effects linear regression model with highlight utility as the dependent variable. An omnibus test suggests the effect of highlight condition is significant (p &lt; 0.001) and therefore H8 is supported by our results.</p><p>Specifically, the average highlight utility, on a 1-7 scale with higher being better, was 2.93 for the Generation Probability condition, compared with 3.92 for the Edit Model condition. This sizable difference in utility was reflected in participants' comments, which shed more light on the reasons why they preferred the highlights produced with the Edit Model. As P4 put it, "the highlights in the [Edit Model condition] identified the places where there were compilation or other errors. And so that was particularly useful especially since the editor did not really give good feedback about what the issues were, in particular, when running the unit tests." In contrast, reflecting on the highlights produced using generation probabilities, participants remarked that they were more distracting: "There were so many there. I think I recall there being one in every two or three lines, if I'm recalling correctly. So it was to the amount that it was noise, so I completely ignored it" [P11] and "it feels like the highlights in the [Generation Probability condition] were even sort of less useful because they were sort of on stuff that it had gotten right" [P18].</p><p>H9: Highlight condition will affect self-reported rankings of the code completion tools. We analyzed this using a cumulative link mixed model<ref type="foot" target="#foot_9">9</ref> with rank (1 for the best, 2 for the second best, and 3 for the worst) as the dependent variable, highlight condition as the fixed-effects independent variable, and participant ID and coding task as random effects. An omnibus test showed the result was only marginally significant (p = 0.068) and H9 is marginally supported.</p><p>Participants ranked the code completion tool from the No Highlights condition 2.03 on average, the tool from the Generation Probability condition 2.10, and the tool from the Edit Model condition 1.87 (here, as previously mentioned, a lower number is better). None of the pairwise differences are significant with our small sample of participants, though the difference between the Generation Probability condition and Edit Model condition is marginally significant (p = 0.063). In total, 9 participants ranked the tool from the No Highlights condition highest, 8 ranked the tool from the Generation Probability condition highest, and the remaining 13 ranked the tool from the Edit Model condition highest.</p><p>Taking this result together with the results for completion utility and highlight utility, we conjecture that participants based their ranking more on the quality of code completions than the perceived value of the highlights. (Recall that completions varied by tasks, and tasks were randomly assigned to highlighting conditions.) Indeed, we observed 4 cases in which participants who reported higher highlight utility for the Edit Model condition still ranked the tool from the Generation Probability condition higher, and 2 cases where the reverse was true. This is in line with participants' comments as they reflected on the code completion tools. For example, P7 stated that "the [tool from the Edit Model condition] would be dead last by a long shot... the [tool from the Generation Probability condition] would be number one because it gave me Python code that worked." Similarly, P4 noted that "the [tool from the Generation Probability condition] was reasonable code and bad highlights." P18 put it particularly bluntly, saying the tool from the Edit Model condition "was just sort of annoying. And, also, I can't remember if the solution did anything useful when I first read it. And the other two seemed like they were basically already written and running."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Further Exploration of the Uncertainty Highlighting Design Space</head><p>Our post-task interview included discussing design probes to further explore the design space of uncertainty highlighting. Our design probes showed alternative designs of uncertainty highlighting that differ in granularity, specificity, and interactivity as described in Section 4.2 and shown in Appendix B. We analyzed the transcribed content using an inductive approach following our interview structure. We first discuss participants' comments around these three dimensions, then we summarize additional themes regarding participants' preferences regarding uncertainty highlighting and code completion in general.</p><p>Granularity of uncertainty highlighting Unlike discriminative models that output a single value for a task, language models output a series of tokens. Because of this, uncertainty about the outputs of language models-and generative models more broadly-can be defined and presented at various levels, from token-level uncertainty (like the generation probabilities and edit model used in our study) to uncertainty about the output as a whole (full code completions, in the case of code completion tools) and anything in between. When being shown an alternative design that displayed uncertainty at the level of the full code completion (Figure <ref type="figure">9</ref> in Appendix B), participants generally had negative reactions. Many commented that this method is not granular enough and would not help them find errors. They found that a number without context is abstract, hard to interpret, and doesn't necessarily help them fix or find the errors: "running this number like 90 percent or 80 percent, I don't feel too much difference to me" [P9] and "It's just, if it is not 100 percent correct, we still need to debug it, right? " [P12]. Some called out that a single number may bias their overall perception of, and interaction with, the generation: "...I think my behavior would depend on the score... if the score is low, I would be pretty frustrated. I would probably just trash the entire thing and rewrite the code myself " [P14].</p><p>On the other hand, this comparison prompted some participants to call out that, while they preferred the token-level uncertainty highlighting, it comes with a tradeoff of potentially highlighting too many things and "visually it looks a bit distracting" [P14]. Some further commented that with the granular highlighting, the threshold that determines tokens with how much uncertainty to highlight is critical for the user experience: "I don't want to see a sea of color in front of me. I'll just assume it thinks it's fine if it's not highlighted " [P2].</p><p>Moreover, some participants offered a middle ground by suggesting the tool could highlight uncertainty at the line level or block level, highlighting digestible blocks of code that are "doing a specific task " [P8], or "if you can chunk out the whole code to separate blocks, and for each separate part give a score then I think that could be helpful " [P9].</p><p>Overall, the discussions reflect that participants prefer more granular uncertainty highlights to guide them to make changes to specific tokens or lines. Specific for code generation, there are also opportunities to explore highlighting at the granularity of meaningful blocks that accomplish particular subtasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Specificity of Uncertainty Highlighting</head><p>We showed two alternative designs to explore the dimension of specificity (Figure <ref type="figure">10</ref> in Appendix B). First, in contrast to the single threshold used in our study to determine whether or not a token should be highlighted, one design showed highlights with multiple levels of opacity to convey different levels of uncertainty. The second design revealed the exact numerical quantification of uncertainty (e.g., probability the token would be edited when using the edit model) when hovering over the highlights. Participants almost unanimously pushed back on the idea of displaying numerical values since it is difficult to make meaning out of numbers without context and they do not have the need or capacity to process the additional specificity. For example, P19 commented that "I personally think it's horrible. First of all... I pay very little attention...I wouldn't spend more than, like, say, a minute or even half a minute to interact with the autocompletion... you may know exactly what these numbers are or what exactly these things mean. But, to me, as a user, I wouldn't put in that much of an effort to parse that result... there's so many things going on in my VS code, I have no idea! My attention's been drawn everywhere."</p><p>A number of participants (but not all) preferred the multi-level opacity approach as "a nice way of conveying a little bit more information" [P2], which can better guide their attention especially working with long code: "I've got 100 lines of code, I would look at the ones with less opacity first, and then it'll definitely help me prioritize" [P3].</p><p>Overall, participants preferred categorical information on uncertainty over being shown the exact quantification of uncertainty. They also highlighted the need to prioritize not overwhelming users as code generation is often used in time-constrained or cognitively demanding settings.</p><p>Interactivity of Uncertainty Highlighting We showed participants two example designs with interactivity (Figure <ref type="figure" target="#fig_7">11</ref> in Appendix B) and encouraged them to come up with other kinds of interactivity they might desire. In one design we showed, alternative tokens were suggested when hovering over the highlights. In the other, the uncertainty highlights were updated as the user made changes to the generated code. Surprisingly, participants pushed back on both ideas. The alternative token suggestion was seen as unhelpful because participants considered token-level changes to be easy fixes much of the time: "it's faster for me to go there and insert the colon than it is for me to use, like, let me go to the alternatives" [P17]. However, they pointed out that they would find it more useful if appropriate alternative suggestions could happen at line or block level.</p><p>Participants generally disliked the idea of the tool taking the initiative to auto-update the uncertainty highlights, finding it "too confusing and disruptive" [P4] or "too busy" [P15]. They suggested to either lower the update frequency so that only significant updates are made, or let the user take the initiative by providing "a button to click on" [P14] or the ability "to toggle ... have them recomputed " [P7].</p><p>Participants suggested a few other interactivity designs. One feature is to allow users to control the threshold of uncertainty for highlights, such as providing a slider to "tune the sensitivity of displaying the highlight" [P7] or buttons to choose if "you want it to be very sensitive, slightly sensitive, or not sensitive" [P16]. Throughout the discussions, many participants requested explanations-not just about how the highlights were computed, but ultimately to diagnose what might have caused this uncertainty. For example, "if there's like a clear rule set...clear categories that define and contribute to its level of certainty...to explain why it thinks it's uncertain about this could be useful " [P1].</p><p>These discussions around interactivity suggest that users desire interaction features around uncertainty that facilitate their end goal of producing correct code efficiently, and these features must also allow sufficient user controls.</p><p>Need to better convey the notion of uncertainty We highlight a theme from participants about the challenge of conveying the concept of uncertainty. First, a few participants equated uncertainty with errors and found it hard to grasp that "AI was highlighting its own mistakes, which was kind of weird " [P15], preferring that "if it's not correct, then the code should not be [shown] " [P12]. Second, some participants found it counter-intuitive in a way that diminished their overall trust to see the uncertainty highlighting at the level of single tokens, as best illustrated by P15's comment: " when you write something like this, it's tied really closely to what happened before and after and there's a lot of nuances that happen. If the AI is saying like this token might be wrong that basically makes it so the whole function might be wrong because it might be doing something like-if anything's wrong at this point then like everything after that could be wrong." These comments suggest that people may not have a clear mental model about what uncertainty means for generative models and could benefit from onboarding information that clearly communicates the notion of uncertainty as potential errors to be corrected and how they are inferred.</p><p>Limitations of Highlighting Lastly, we find that our participants struggled with the idea that highlighting is not necessarily comprehensive with regards to the potential errors in an AI code completion. Participants cited the fact that highlighting relies on information that is already present in the code completion and will not be able to indicate when an error may be due to missing content: "...[this is] an example where highlighting the tokens become kind of tricky because like what was wrong with the solution wasn't necessarily that any part of it was wrong, is that it was missing a case" [P14]. This is a limitation of all highlighting techniques, and should be clearly communicated when introducing programmers to such systems, perhaps in tutorials or onboarding materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>Our study finds that highlighting appropriate tokens in generated code can meaningfully impact and improve programmer behavior, and points to a clear path in favor of edit models for this purpose. With the edit model, participants completed tasks faster, while making more targeted edits to the suggested code, and reporting higher utility, than when in the Generation Probability condition. Conversely, the Generation Probability condition was often statistically indistinguishable from the baseline No Highlights condition.</p><p>Our qualitative findings further suggest that highlighting helps when the highlights are granular, informative, interpretable, and not overwhelming (i.e., when there aren't too many, and participants understand what changes need to be made). Here, a careful balance must be struck: often the preferred design is to do, or show, less rather than more. For example, if model uncertainty is very high, participants preferred suppressing suggestions altogether rather than showing one with many highlights. Likewise, when uncertainty is very low, participants preferred not to see highlights at all, as the base assumption is that only problem areas are flagged (e.g., similar to spell check).</p><p>Taken together these findings chart an encouraging, yet cautionary, path for future work in this space. However, there a numerous limitations with this work, and there remain many unknowns. We discuss these limitations and open problems below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Feasibility of Building an Edit Model</head><p>First and foremost, our work relies on a closed-world edit model, learned on the very set of code completions used in our study. This clearly represents a best-case highly-calibrated scenario. It remains to be demonstrated that we can learn an open-world general-purpose edit model, and that such a model would similarly impact user interaction. Fortunately, systems like GitHub Copilot already consider edits to their code completions as a form of performance metric <ref type="bibr" target="#b57">[58]</ref> and this product-scale data stream could be directly repurposed to this end.</p><p>Despite the uncertainties in generalizing our edit model approach, we are encouraged by our positive findings-had we observed no benefits under these conditions, it would have called into question the prospect of using highlighting as a strategy for conveying model uncertainty, and for directing human attention for providing oversight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Representativeness of Tasks, Scenarios, and Participants</head><p>Similarly, the tasks we include in our study were adapted from the Leetcode problem set. These are timed, standalone programming challenges, and may not be representative of the day-to-day coding tasks or debugging scenarios that our participants regularly encounter. Likewise, by focusing on unit tests and algorithmic correctness, we may miss other important aspects of code quality such as maintainability and security. Finally, our participants are all professionals in the field of software development, and work for the same US-based technology company. Their experiences may not generalize. For example, students, or those with less experience, may benefit more from the assistance afforded by highlights, or alternatively, may find the lack of explanations more problematic.</p><p>To address these limitations, future work should focus on longitudinal in-situ studies, with broader audiences, and should track outcomes beyond algorithmic correctness. As an example, a future experiment might examine how highlighting strategies impact online metrics such as acceptance rates, or the total proportion of code contributed by the AI system <ref type="bibr" target="#b57">[58]</ref>. Likewise, recent work has suggested that people write less secure code when using such AI systems <ref type="bibr" target="#b45">[46]</ref>, so future work could examine whether highlighting strategies ameliorate this risk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Impact on Automation Bias</head><p>In the introduction, we motivated this work by presenting the hypothesis that, by highlighting uncertainty, we can help direct human attention to problematic AI generations, and thus reduce automation bias. However, it remains to be demonstrated that the observed benefits (task completion time, targeted edits, preference), translate to increased oversight and decreased automation bias. Nevertheless, we are encouraged that past work has shown that reducing the effort needed to interpret model explanations, or expressions of uncertainty, can increase the likelihood of people overriding AI-induced errors <ref type="bibr" target="#b50">[51]</ref>. We hope to explore these questions in immediate future work.</p><p>However, like the code generations themselves, highlights are not perfect, are prone to error, and lack the expressivity needed communicate all classes of error (e.g., errors of omission). In at least one task (Base 7), we observed that an important base case was missing. Participants often attended to other errors flagged by highlights, but submitted their code without addressing the omission. In interviews, several participants mentioned that they interpreted a lack of highlights as signal that the code was correct. We may find that we are simply shifting the automation bias such that people are applying an insufficient level of skepticism to the highlights, where before they were insufficiently skeptical of the code completion itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>Effective human oversight of AI-powered code completion tools is essential, and requires that programmers be able to efficiently detect and correct generation errors. To this end, prior work-and various commercial systems-use token highlighting to signal which tokens have the lowest likelihood of being generated. However, there have been no empirical studies exploring whether such highlights change programmer behavior in any meaningful way.</p><p>In this paper, we explored whether conveying information about uncertainty via highlights enables programmers to more quickly and accurately produce code when collaborating with an AI-powered code completion tool. Specifically, we considered uncertainty with respect to two distributions: (1) the likelihood of generating the token from the underlying model, and (2) the likelihood of a programmer editing (or deleting) the token once generated. In separate systems, we highlighted tokens that were least likely to be generated, and most likely to be edited, respectively.</p><p>Our mixed-methods study with 30 programmers found that highlighting tokens with the highest likelihood of being edited can lead to faster task completion times and more targeted edits, and was subjectively preferred by study participants. In contrast, highlighting tokens according to their generation probabilities did not provide any benefit over a baseline with no highlights.</p><p>Further, we explored the design space of token highlighting strategies in post-task interviews with participants. We found significant resistance to designs that highlighted too much, or conveyed too fine-grained a measure of uncertainty. Instead, programmers prefer highlights that are granular, informative, interpretable, and not overwhelming. Participants additionally noted that all highlighting strategies are limited because they cannot communicate errors of omission.</p><p>We hope that our positive results, when highlighting tokens most likely to be edited, encourage future and continued work in this important space. At the same time, we hope our results discourage the blind use of generation probabilities for this purpose. Most of all, we hope that research and tooling for human oversight keeps pace with the extraordinarily rapid advancements and releases of code generation models, and generative models more broadly. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Highlights in the Generation Probability condition (left) and the Edit Model condition (right) for the three coding tasks. The No Highlights condition displays the code completion alone.</figDesc><graphic url="image-4.png" coords="8,88.66,316.32,437.51,148.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 . 5 .</head><label>15</label><figDesc>How mentally demanding was the task? (Very low to very high; step-size of 20) 2. How hurried or rushed was the pace of the task? (Very low to very high; step-size of 20) 3. How successful were you in accomplishing what you were asked to do? (Prefect to Failure; step-size of 20) 4. How hard did you have to work to accomplish your level of performance? (Very low to very high; step-size of 20) How insecure, discouraged, irritated, stressed, and annoyed were you? (Very low to very high; step-size of 20)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Average time taken per task in minutes (capped at 10) by highlight condition and coding task.</figDesc><graphic url="image-7.png" coords="16,129.23,56.69,353.52,135.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Average accuracy on unit tests by highlight condition and coding task.</figDesc><graphic url="image-8.png" coords="16,129.23,243.63,353.54,131.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: (a): Token-level survival rate for tokens that are highlighted (yellow) or not highlighted (gray) in each of the three experimental conditions. (b): Token-level survival rate in the No Highlights condition of tokens that would have been highlighted in the Generation Probability condition or Edit Model condition, respectively.</figDesc><graphic url="image-9.png" coords="18,88.66,56.69,212.12,144.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Self-reported highlight utility by condition.</figDesc><graphic url="image-11.png" coords="20,184.47,56.69,243.05,139.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Alternative designs that differ in interactivity of uncertainty highlighting.</figDesc><graphic url="image-16.png" coords="33,195.52,388.87,220.96,131.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="3,85.04,56.69,441.92,243.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-5.png" coords="10,129.23,56.69,353.53,271.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-6.png" coords="13,85.04,56.69,441.93,301.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Hypothesis</cell><cell>No Highlights (Mean)</cell><cell>Generation Prob (Mean)</cell><cell>Edit Model (Mean)</cell><cell cols="2">Pairwise Significance</cell></row><row><cell>H1: Time (Minutes) p = 0.002**</cell><cell>9.28</cell><cell>9.58</cell><cell>8.61</cell><cell cols="2">Gen Prob &gt; Edit Model** No Highlights &gt; Edit Model ?</cell></row><row><cell>H2: Accuracy p = 0.145</cell><cell>38.3%</cell><cell>30.0%</cell><cell>50.0%</cell><cell>-</cell></row><row><cell>H3: Chars Added p = 0.010*</cell><cell>146.9</cell><cell>120.5</cell><cell>90.7</cell><cell cols="2">No Highlights &gt; Edit Model*</cell></row><row><cell>H4: Overall Token</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Survival Rate</cell><cell>79.3%</cell><cell>79.8%</cell><cell>75.3%</cell><cell>-</cell></row><row><cell>p = 0.164</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Edit Model, Not-HL &gt; Edit Model, HL***</cell></row><row><cell>H5: Token-Level Survival Rate p &lt; 0.001***</cell><cell>Not-HL: 79.3%</cell><cell>HL: 73.9% Not-HL: 80.9%</cell><cell>HL: 35.3% Not-HL: 87.1%</cell><cell cols="2">Gen Prob, Not-HL &gt; Gen Prob, HL** Gen Prob, HL &gt; Edit Model, HL*** Edit Model, Not-HL &gt; Gen Prob, Not-HL***</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Edit Model, Not-HL &gt; No Highlights***</cell></row><row><cell>H6: Cognitive</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Load (0-100)</cell><cell>49.5</cell><cell>46.4</cell><cell>43.5</cell><cell>-</cell></row><row><cell>p = 0.228</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>H7: Code Completion</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Utility (1-7, higher is better)</cell><cell>4.13</cell><cell>4.27</cell><cell>4.31</cell><cell>-</cell></row><row><cell>p = 0.679</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>H8: Highlight</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Utility (1-7, higher is better)</cell><cell>N/A</cell><cell>2.93</cell><cell>3.92</cell><cell cols="2">Edit Model &gt; Gen Prob**</cell></row><row><cell>p &lt; 0.001***</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>H9: Rank (1-3,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>lower is better)</cell><cell>2.03</cell><cell>2.10</cell><cell>1.87</cell><cell cols="2">Gen Prob &gt; Edit Model ?</cell></row><row><cell>p = 0.068 ?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">No Highlights Generation Probability Edit Model</cell></row><row><cell cols="2">Ugly Number</cell><cell>11</cell><cell></cell><cell>8</cell><cell>11</cell></row><row><cell cols="2">Base 7</cell><cell>11</cell><cell></cell><cell>10</cell><cell>9</cell></row><row><cell cols="2">Most Common Word</cell><cell>8</cell><cell></cell><cell>12</cell><cell>10</cell></row></table><note><p>Summary of quantitative results. The left column shows p-values obtained via omnibus tests for each hypothesis. The right column shows pairs of conditions that are statistically significantly different or marginally significant. Significance is marked as p &lt; 0.1 ( ?), p &lt; 0.05 (*), p &lt; 0.01 (**), or p &lt; 0.001 (***).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Number of participants randomly assigned to each highlight condition for each coding task.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://www.pylint.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Due to a bug in our code, our interface showed 2 tokens that should have been highlighted but were not and</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>tokens that were highlighted but should not have been, highlighting a total of 204 characters across the three tasks. These errors were spread among the three coding tasks. Four of those five tokens had generation probability between 0.692 and 0.701, or within 0.007 of the 0.694 threshold; the fifth had probability 0.624. We do not believe this discrepancy meaningfully impacted the results, since this would imply extreme sensitivity to minor shifts in the threshold used.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>Pre-registration link: https://osf.io/tymah</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>https://leetcode.com/problems/ugly-number/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5"><p>https://leetcode.com/problems/base-7/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6"><p>https://leetcode.com/problems/most-common-word/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_7"><p>We reduced the number of TLX questions asked to participants because one of the questions surrounds the idea of physical demand, which we do not think relates meaningfully to our research questions.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_8"><p>Note that both of these values are higher than we might expect given that tokens were only highlighted if they had a survival rate of no more than 33.3% based on the original six participants who completed each task. We believe this is due to the small sample size on which the edit model was trained and perhaps the presence of the 10-minute time limit.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_9"><p>This analysis differs from the analysis that was pre-registered. We chose to make this switch after determining that a cumulative link mixed model is more appropriate for comparing rankings.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Study Participants</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Alternative designs shown in post-task interview</head><p>The figures in this section show a selection of the alternative designs that were displayed to participants as design probes.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">ML-powered coding companion -Amazon CodeWhisperer</title>
		<ptr target="https://aws.amazon.com/codewhisperer/" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Amazon Web Services</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Machine bias: There&apos;s software across the country to predict future criminals and it&apos;s biased against blacks</title>
		<author>
			<persName><forename type="first">Julia</forename><surname>Angwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Mattu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lauren</forename><surname>Kirchner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Does the whole exceed its parts? the effect of ai explanations on complementary team performance</title>
		<author>
			<persName><forename type="first">Gagan</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongshuang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joyce</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Fok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Besmira</forename><surname>Nushi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ece</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2021 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?</title>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shmargaret</forename><surname>Shmitchell</surname></persName>
		</author>
		<idno type="DOI">10.1145/3442188.3445922</idno>
		<ptr target="https://doi.org/10.1145/3442188.3445922" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT)</title>
		<meeting>the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="610" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?jean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02">2003. Feb (2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Uncertainty as a form of transparency: Measuring, communicating, and using uncertainty</title>
		<author>
			<persName><forename type="first">Umang</forename><surname>Bhatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Antor?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Vera Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasanna</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Fogliato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabrielle</forename><surname>Melan?on</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ranganath</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omesh</forename><surname>Tickoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society</title>
		<meeting>the 2021 AAAI/ACM Conference on AI, Ethics, and Society</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="401" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hello AI&quot;: Uncovering the Onboarding Needs of Medical Practitioners for Human-AI Collaborative Decision-Making</title>
		<author>
			<persName><forename type="first">Carrie</forename><forename type="middle">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samantha</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lauren</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Terry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Human-computer Interaction</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Onboarding Materials as Cross-functional Boundary Objects for Developing AI Assistants</title>
		<author>
			<persName><forename type="first">Carrie</forename><forename type="middle">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samantha</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">F</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lauren</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Terry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Tuhin</forename><surname>Chakrabarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishakh</forename><surname>Padmakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.13669</idno>
		<title level="m">Help me write a poem: Instruction Tuning as a Vehicle for Collaborative Poetry Writing</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrique</forename><surname>Ponde De Oliveira Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heidy</forename><surname>Khlaaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brooke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alethea</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Tillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felipe</forename><forename type="middle">Petroski</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fotios</forename><surname>Chantzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Hebgen Guss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Paino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolas</forename><surname>Tezak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchir</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">N</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Morikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miles</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mira</forename><surname>Murati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2107.03374</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.2107.03374" />
		<title level="m">Evaluating Large Language Models Trained on Code</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Creative writing with a machine in the loop: Case studies on slogans and stories</title>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><surname>Spencer Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenhao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd International Conference on Intelligent User Interfaces</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="329" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><surname>Deepmind</surname></persName>
		</author>
		<ptr target="https://alphacode.deepmind.com/" />
		<imprint>
			<date type="published" when="2022-09">2022. September, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Communicating uncertainty using words and numbers</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mandeep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Dhami</surname></persName>
		</author>
		<author>
			<persName><surname>Mandel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The SPACE of Developer Productivity: There&apos;s more to it than you think</title>
		<author>
			<persName><forename type="first">Nicole</forename><surname>Forsgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret-Anne</forename><surname>Storey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Maddila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Houck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenna</forename><surname>Butler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Queue</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="20" to="48" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">GitHub Copilot -Your AI pair programmer</title>
		<author>
			<persName><surname>Github</surname></persName>
		</author>
		<ptr target="https://github.com/features/copilot/" />
		<imprint>
			<date type="published" when="2022-09">2022. September, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Do Explanations Help Users Detect Errors in Open-Domain QA? An Evaluation of Spoken vs. Visual Explanations</title>
		<author>
			<persName><forename type="first">Ana</forename><surname>Valeria Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gagan</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srini</forename><surname>Iyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of ACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Algorithmic Risk Assessments Can Alter Human Decision-Making Processes in High-Stakes Government Contexts</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiling</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Predictive translation memory: A mixed-initiative system for human language translation</title>
		<author>
			<persName><forename type="first">Spence</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Heer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th annual ACM symposium on User interface software and technology</title>
		<meeting>the 27th annual ACM symposium on User interface software and technology</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="177" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The efficacy of human post-editing for language translation</title>
		<author>
			<persName><forename type="first">Spence</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Heer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">NASA-task load index (NASA-TLX); 20 years later</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sandra</surname></persName>
		</author>
		<author>
			<persName><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the human factors and ergonomics society annual meeting</title>
		<meeting>the human factors and ergonomics society annual meeting<address><addrLine>Angeles, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Los</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="904" to="908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Can AI become reliable source to support human decision making in a court scene</title>
		<author>
			<persName><forename type="first">Yugo</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kosuke</forename><surname>Wakabayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="195" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">How machine-learning recommendations influence clinician treatment selections: the example of antidepressant selection</title>
		<author>
			<persName><forename type="first">Maia</forename><forename type="middle">L</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><forename type="middle">Fernandes</forename><surname>Pradier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">H</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><forename type="middle">H</forename><surname>Perlis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Finale</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><forename type="middle">Z</forename><surname>Gajos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Translational Psychiatry</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Survey of Hallucination in Natural Language Generation</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nayeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rita</forename><surname>Frieske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiezheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etsuko</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<idno type="DOI">10.1145/3571730</idno>
		<ptr target="https://doi.org/10.1145/3571730" />
	</analytic>
	<monogr>
		<title level="j">Comput. Surveys</title>
		<imprint>
			<date type="published" when="2022-11">2022. nov 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">How can we know when language models know? on the calibration of language models for question answering</title>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibo</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="962" to="977" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Research: quantifying GitHub Copilot&apos;s impact on developer productivity and happiness</title>
		<author>
			<persName><forename type="first">Eirini</forename><surname>Kalliamvakou</surname></persName>
		</author>
		<ptr target="https://github.blog/2022-09-07-research-quantifying-github-copilots-impact-on-developer-productivity-and-happ" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Kocielnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saleema</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<title level="m">Will You Accept an Imperfect AI?: Exploring Designs for Adjusting End-user Expectations of AI Systems. Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Why is &apos;Chicago&apos; deceptive?</title>
		<author>
			<persName><forename type="first">Vivian</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenhao</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Towards Building Model-Driven Tutorials for Humans. Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Estimating residual error rate in recognized handwritten documents using artificial error injection</title>
		<author>
			<persName><forename type="first">Edward</forename><surname>Lank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Stedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Terry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">The world&apos;s leading online programming learning platform</title>
		<author>
			<persName><surname>Leetcode</surname></persName>
		</author>
		<ptr target="https://leetcode.com/" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Psychology Meets Machine Learning: Interdisciplinary Perspectives on Algorithmic Job Candidate Screening</title>
		<author>
			<persName><forename type="first">Cynthia Cs</forename><surname>Liem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Langer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Demetriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Annemarie</forename><forename type="middle">Mf</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Achmadnoer</forename><surname>Sukma Wicaksana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marise</forename><surname>Ph Born</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cornelius</forename><forename type="middle">J</forename><surname>K?nig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Explainable and Interpretable Models in Computer Vision and Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="197" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Effects of spell checkers on English as a second language students&apos; incidental spelling learning: a cognitive load perspective</title>
		<author>
			<persName><forename type="first">Tzu-Chien</forename><surname>Po-Han Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Paas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reading and Writing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1501" to="1525" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Opal: Multimodal Image Generation for News Illustration</title>
		<author>
			<persName><forename type="first">Vivian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lydia</forename><forename type="middle">B</forename><surname>Chilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology</title>
		<meeting>the 35th Annual ACM Symposium on User Interface Software and Technology</meeting>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Novice-AI Music Co-Creation via AI-Steering Tools for Deep Generative Models</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Louie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Coenen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Cheng-Zhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carrie</forename><forename type="middle">J</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2020 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Explainable machine-learning predictions for the prevention of hypoxaemia during surgery</title>
		<author>
			<persName><forename type="first">Bala</forename><surname>Scott M Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><forename type="middle">S</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayumi</forename><surname>Vavilala</surname></persName>
		</author>
		<author>
			<persName><surname>Horibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Eisses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">E</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Liston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">King-Wai</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu-Fang</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature biomedical engineering</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="749" to="760" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Identifying fluently inadequate output in neural and statistical machine translation</title>
		<author>
			<persName><forename type="first">Marianna</forename><surname>Martindale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marine</forename><surname>Carpuat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Translation Summit</title>
		<meeting>Machine Translation Summit</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">XVII</biblScope>
			<biblScope unit="page" from="233" to="243" />
		</imprint>
	</monogr>
	<note>Research Track</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Hussein</forename><surname>Mozannar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gagan</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fourney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.14306</idno>
		<title level="m">Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Teaching Humans When To Defer to a Classifier via Examplars</title>
		<author>
			<persName><forename type="first">Hussein</forename><surname>Mozannar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvindmani</forename><surname>Satyanarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Predicting good probabilities with supervised learning</title>
		<author>
			<persName><forename type="first">Alexandru</forename><surname>Niculescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Mizil</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on Machine learning</title>
		<meeting>the 22nd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="625" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><surname>Nyt</surname></persName>
		</author>
		<ptr target="https://www.nytimes.com/2022/12/05/technology/chatgpt-ai-twitter.html" />
		<title level="m">The Brilliance and Weirdness of ChatGPT</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<ptr target="https://beta.openai.com/playground" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<ptr target="https://openai.com/blog/chatgpt/" />
		<title level="m">ChatGPT: Optimizing Language Models for Dialogue</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Complacency and bias in human use of automation: An attentional integration</title>
		<author>
			<persName><forename type="first">Raja</forename><surname>Parasuraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dietrich</forename><forename type="middle">H</forename><surname>Manzey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human factors</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="381" to="410" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Asleep at the Keyboard? Assessing the Security of GitHub Copilot&apos;s Code Contributions</title>
		<author>
			<persName><forename type="first">Hammond</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baleegh</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Dolan-Gavitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Karri</surname></persName>
		</author>
		<idno type="DOI">10.1109/SP46214.2022.9833571</idno>
		<ptr target="https://doi.org/10.1109/SP46214.2022.9833571" />
	</analytic>
	<monogr>
		<title level="m">2022 IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="754" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Neil</forename><surname>Perry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Megha</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Boneh</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2211.03622</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.2211.03622" />
		<title level="m">Do Users Write More Insecure Code with AI Assistants?</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>OpenAI white paper</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Investigating Explainability of Generative AI for Code through Scenario-Based Design</title>
		<author>
			<persName><forename type="first">Jiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Vera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Houde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><forename type="middle">D</forename><surname>Talamadupula</surname></persName>
		</author>
		<author>
			<persName><surname>Weisz</surname></persName>
		</author>
		<idno type="DOI">10.1145/3490099.3511119</idno>
		<ptr target="https://doi.org/10.1145/3490099.3511119" />
	</analytic>
	<monogr>
		<title level="m">27th International Conference on Intelligent User Interfaces (Helsinki, Finland) (IUI &apos;22)</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="212" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models</title>
		<author>
			<persName><forename type="first">Priyan</forename><surname>Vaithilingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><forename type="middle">L</forename><surname>Glassman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI Conference on Human Factors in Computing Systems Extended Abstracts</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Communicating uncertainty about facts, numbers and science</title>
		<author>
			<persName><forename type="first">Anne</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Marthe</forename><surname>Van Der Bles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Van Der Linden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra Lj</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><forename type="middle">B</forename><surname>Galvao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Zaval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Spiegelhalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Royal Society open science</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">181870</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Explanations Can Reduce Overreliance on AI Systems During Decision-Making</title>
		<author>
			<persName><forename type="first">Helena</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>J?rke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madeleine</forename><surname>Grunde-Mclaughlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Gerstenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2212.06823</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.2212.06823" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Show or suppress? Managing input uncertainty in machine learning model explanations</title>
		<author>
			<persName><forename type="first">Danding</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wencan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">Y</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">294</biblScope>
			<biblScope unit="page">103456</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Complacency and automation bias in the use of imperfect automation</title>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">A</forename><surname>Christopher D Wickens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">Z</forename><surname>Clegg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelia</forename><forename type="middle">L</forename><surname>Vieane</surname></persName>
		</author>
		<author>
			<persName><surname>Sebok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human factors</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="728" to="739" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName><surname>Wikipedia</surname></persName>
		</author>
		<ptr target="https://en.wikipedia.org/wiki/ChatGPT" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Understanding the Effect of Accuracy on Trust in Machine Learning Models</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2019 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making</title>
		<author>
			<persName><forename type="first">Yunfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingzi</forename><surname>Vera Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><forename type="middle">K E</forename><surname>Bellamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2020 Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Productivity Assessment of Neural Code Completion</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eirini</forename><surname>Kalliamvakou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Alice</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Rifkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Simister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Sittampalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Aftandilian</surname></persName>
		</author>
		<idno type="DOI">10.1145/3520312.3534864</idno>
		<ptr target="https://doi.org/10.1145/3520312.3534864" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming</title>
		<meeting>the 6th ACM SIGPLAN International Symposium on Machine Programming<address><addrLine>San Diego, CA, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
	<note>MAPS 2022)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
