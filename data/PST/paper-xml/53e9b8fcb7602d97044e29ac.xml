<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RDIP: Return-address-stack Directed Instruction Prefetching</title>
				<funder>
					<orgName type="full">ARM</orgName>
				</funder>
				<funder ref="#_7nPwPcU">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Aasheesh</forename><surname>Kolli</surname></persName>
							<email>akolli@umich.edu</email>
						</author>
						<author>
							<persName><forename type="first">Ali</forename><surname>Saidi</surname></persName>
							<email>ali.saidi@arm.com</email>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
							<email>twenisch@umich.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RDIP: Return-address-stack Directed Instruction Prefetching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>B.3.2 [Memory Structures]: Design Styles -cache memories Design</term>
					<term>Performance RAS</term>
					<term>caching</term>
					<term>instruction fetch</term>
					<term>prefetching</term>
					<term>accuracy</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>L1 instruction fetch misses remain a critical performance bottleneck, accounting for up to 40% slowdowns in server applications. Whereas instruction footprints typically fit within last-level caches, they overwhelm L1 caches, whose capacity is limited by latency constraints. Past work has shown that server application instruction miss sequences are highly repetitive. By recording, indexing, and prefetching according to these sequences, nearly all L1 instruction misses can be eliminated. However, existing schemes require impractical storage and considerable complexity to correct for minor control-flow variations that disrupt sequences.</p><p>In this work, we simplify and reduce the energy requirements of accurate instruction prefetching via two observations: (1) program context as captured in the call stack correlates strongly with L1 instruction misses, and (2) the return address stack (RAS), already present in all high performance processors, succinctly summarizes program context. We propose RAS-Directed Instruction Prefetching (RDIP), which associates prefetch operations with signatures formed from the contents of the RAS. RDIP achieves 70% of the potential speedup of an ideal L1 cache, outperforms a prefetcherless baseline by 11.5% and reduces energy and complexity relative to sequence-based prefetching. RDIP's performance is within 2% of the state-of-the-art Proactive Instruction Fetch, with nearly 3X reduction in storage and 1.9X reduction in energy overheads.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Recent research shows that L1 instruction fetch misses remain a critical performance bottleneck in traditional server workloads <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b32">32]</ref>, cloud computing workloads <ref type="bibr">[7,</ref><ref type="bibr" target="#b19">19]</ref>, and even smartphone applications <ref type="bibr" target="#b10">[10]</ref>, accounting for up to 40% slowdowns. Whereas instruction footprints typically fit comfortably within last-level caches, they overwhelm L1 caches, whose capacity is tightly limited by latency constraints. Intermediate instruction caches may reduce miss penalties, but are either too small to capture the entire instruction footprint or have high access latencies, which are then exposed on the execution critical path <ref type="bibr">[7,</ref><ref type="bibr" target="#b11">11]</ref>. Unlike L1 data misses, out of order mechanisms are ineffective in hiding instruction miss penalties. Figure <ref type="figure" target="#fig_0">1</ref> shows the performance gap between a 32kB L1 cache without prefetching ("NoP"), the same cache with a conventional next-2line prefetcher ("N2L"), and an idealized (unbounded size, but single-cycle latency; "Ideal") cache for a range of cloud computing and server applications. Instruction prefetching has the potential to improve performance by an average of 16.2%, but next-line prefetching falls well short of this potential. (For a brief description of the workloads and corresponding performance metrics, see <ref type="bibr">Section 4)</ref>.</p><p>The importance of instruction prefetching has long been recognized. Nearly all shipping processors, from embeddedto server-class systems, include some form of next-line or stream-based instruction prefetcher <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b37">37]</ref>. Such prefetchers are simple and require negligible storage, but fail to prefetch across fetch discontinuities (e.g., function calls and returns), which incur numerous stalls. Early research attempts at more sophisticated instruction prefetch leverage the branch predictor to run ahead of fetch <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b36">36]</ref>. However, poor branch predictability and insufficient lookahead limit the effectiveness of these designs <ref type="bibr" target="#b9">[9]</ref>. To our knowledge, such instruction prefetchers have never been commercially deployed.</p><p>More recent hardware prefetching proposals rely on the observation that instruction cache miss or instruction execution sequences are highly repetitive <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9]</ref>. These designs log the miss/execution streams in a large circular buffer and maintain an index on this buffer to locate and replay past sequences on subsequent triggers. The most recent proposal, Proactive Instruction Fetch (PIF) <ref type="bibr" target="#b8">[8]</ref>, can eliminate nearly all L1 instruction misses, but requires impractical storage and substantial complexity to correct for minor control-flow variations that disrupt otherwise-repetitive sequences. Storage (and hence, energy) requirements grow rapidly with code footprint because these mechanisms log all instruction-block fetches and must maintain an index by block address.</p><p>In this work, we seek to simplify and reduce the storage and energy requirements of accurate hardware instruction prefetching. We exploit the observation that program context, as captured in the call stack, correlates strongly with L1 instruction misses. Moreover, we note that modern high-performance processors already contain a structure that succinctly summarizes program context: the return address stack (RAS). Upon each call or return operation, RAS-Directed Instruction Prefetching (RDIP) encodes the RAS state into a compact signature comprising the active call stack, and direction/destination of the current call or return. We find a strong correlation between these signatures and L1 instruction misses, and associate each signature with a sequence of misses recently seen for that signature. By generating signatures on both calls and returns, we construct signatures with fine-grained context information that can prefetch not only on traversals down the call graph, but can also prefetch accurately within large functions and when deep call hierarchies unwind-a key advantage over past hardware and software prefetching schemes that exploit caller-callee relationships <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b20">20]</ref>. Furthermore, we find that RAS signatures are highly predictable-current signatures accurately predict upcoming signatures, enabling higher prefetch lookahead.</p><p>RDIP reduces area and overheads relative to past sequencebased prefetchers because it records less state and requires simpler indexing; we find that coverage saturates with only 64kB of storage (relative to over 200kB for <ref type="bibr" target="#b8">[8]</ref>). Using the gem5 <ref type="bibr" target="#b4">[4]</ref> full-system simulation infrastructure running a suite of cloud computing and server applications, we show that RDIP achieves a performance improvement of up to 36% (avg 11.5%) over a prefetcher-less design, as compared to 13.5% (avg 4.8%) for a carefully tuned next-line prefetcher and 40.1% (avg 12.9%) for PIF. PIF incurs over 3X higher storage overhead and 1.9X dynamic energy overhead than RDIP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Due to the performance criticality of instruction cache misses, there is a rich body of prior work on instruction prefetching. Even early computer systems included nextline instruction prefetchers to exploit the common case of sequential instruction fetch <ref type="bibr" target="#b1">[1]</ref>. This early concept evolved into next-N-line and instruction stream prefetchers <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b37">37]</ref>, which make use of a variety of trigger events and control mechanisms to modulate the aggressiveness and lookahead of the prefetcher. However, next-line prefetchers provide poor accuracy and lack prefetch lookahead for codes with frequent branching and function calls. Nevertheless, because of their simplicity, next-line and stream prefetchers are widely deployed in industrial designs (e.g., <ref type="bibr" target="#b13">[13]</ref>). To our knowledge, more sophisticated hardware prefetchers proposed in the literature have never been deployed.</p><p>To improve prefetch accuracy and lookahead in branchand call-heavy code, researchers have proposed several branch predictor based prefetchers <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b33">33]</ref>. Run-ahead execution <ref type="bibr" target="#b22">[22]</ref>, wrong path instruction prefetching <ref type="bibr" target="#b24">[24]</ref>, and speculative threading mechanisms <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b39">39]</ref> also prefetch instructions by using control flow speculation to explore ahead of the instruction fetch unit. As shown by Ferdman et al. <ref type="bibr" target="#b8">[8]</ref>, these prefetchers suffer from interference caused by wrong path execution and insufficient lookahead when the branch predictor traverses loop branches. RDIP instead bases its predictions on non-speculative RAS state, which is more stable. It is important to note that execution-path based correlation has been shown to be effective for branch prediction <ref type="bibr" target="#b23">[23]</ref>, dead-block prediction <ref type="bibr" target="#b17">[17]</ref> and last touch prediction <ref type="bibr" target="#b16">[16]</ref>, but none of these leverage the RAS to make their respective predictions.</p><p>The discontinuity prefetcher <ref type="bibr" target="#b32">[32]</ref> handles fetch discontinuities but its lookahead is limited to one fetch discontinuity at a time to prevent run-away growth in the number of prefetch candidates. The branch history guided prefetcher (BHGP) <ref type="bibr" target="#b33">[33]</ref> keeps track of the branch instructions executed and near future instruction cache misses, and prefetches them upon next occurrence of the branch. BHGP cannot differentiate among invocations of the same branch, which results in either unnecessary prefetches or reduced coverage. In RDIP, by using the RAS (instead of a single entry) to make prefetching decisions, different invocations of a function call are uniquely identified leading to more accurate prefetching.</p><p>TIFS <ref type="bibr" target="#b9">[9]</ref> and PIF <ref type="bibr" target="#b8">[8]</ref> address the limitations of branchpredictor-directed prefetching by directly recording the instruction fetch miss and instruction commit sequences, respectively. Hence, their accuracy and lookahead are independent of the predictability of individual branches. Both designs maintain an ordered log of instruction block addresses and an index that maps particular fetch events (called trigger accesses) to locations in the log. Whenever a trigger access results in a hit in the index, the prefetcher begins issuing prefetch requests according to the recorded history in the log. PIF improves over TIFS by recording the committed instruction sequence, and hence is immune to disruptions from wrong-path execution that harm the accuracy and coverage of TIFS. To similarly avoid disruptions from wrong-path calls and returns, RDIP updates its signatures as call and return instructions commit (rather than at fetch as for a conventional RAS), which substantially improves its ability to use current signatures to predict future signatures.  Both TIFS and PIF require considerable on-chip storage, which in turn leads to high energy requirements. TIFS adds a 15-bit overhead to every L2 tag array entry to maintain its index (totaling to 240kB in an 8MB L2) and requires an additional 156kB (also within the L2) to store its instruction miss log. Hence, each access to prefetcher meta-data incurs up to two L2 accesses, at a considerable cost in energy. PIF employs dedicated storage structures, but even those structures total over 200kB, and are accessed frequently. In contrast, as we show later, RDIP requires only 64kB of dedicated storage. We analyze energy overheads of each approach in greater detail in Section 5.4.3.</p><p>SHIFT <ref type="bibr" target="#b15">[15]</ref> is a concurrent work that employs a similar record and replay mechanism as PIF, while reducing storage overhead per core. The key insight behind SHIFT is that different cores in a multi-core server system usually execute similar instruction sequences. By adopting shared prefetcher-related storage for the different cores, SHIFT reduces overall storage overhead per core. In contrast, RDIP does not rely on the assumption that neighboring cores execute the same programs, and is more storage efficient when a server system allocates less than 4 cores per workload. Some prior hardware-software hybrid prefetchers track function caller-callee relationships and initiate next-N-line instruction prefetching for a function's callees upon entry to the caller <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b20">20]</ref>. Like RDIP, these techniques exploit relationships in a program's control flow graph. However, RDIP uses additional RAS history, rather than just its leaf, which allows it to distinguish multiple invocations of the same functions from different call sites, improving accuracy. Moreover, RDIP prefetches on traversals both up and down the call stack, enabling timely and accurate prefetch when control returns into the body of a large function.</p><p>Other software techniques involve relocating infrequently executed code to increase sequential code execution <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b38">38]</ref> or compiler-inserted instruction prefetches (e.g., <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b21">21]</ref>). These techniques are orthogonal to RDIP and can provide synergistic benefits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DESIGN</head><p>We next explain the design and rationale for RDIP, which is based on three key observations:</p><p>? Instruction cache misses correlate strongly to program context; that is, the same misses recur each time a context is revisited.</p><p>? Program call graphs follow repetitive patterns, hence a current program context can be used to accurately predict upcoming program contexts.</p><p>? The state of the return address stack succinctly summarizes program context.</p><p>The current call stack accurately reflects the program execution path taken to reach a particular point in the code. The contents of the instruction cache are determined by the instruction fetches encountered along this execution path. So, both the initial cache state upon entering a particular context and the misses incurred during that context will tend to be similar to prior invocations of the same context. The central idea of RDIP is to record the instruction cache misses seen during a particular program context and prefetch these upon the next occurrence of the same program context.</p><p>However, issuing prefetch requests upon entering a program context (i.e., after executing the call instruction that branches into a function) is too late-the processor will likely stall on a miss to the call's target. Instead, prefetch requests arising during a particular program context must be issued during a preceding context. Fortunately, the call graphs of server applications tend to be predictable and hence nearfuture contexts can be predicted accurately, providing adequate lookahead.</p><p>RDIP operates in three steps:</p><p>1. Record the instruction cache misses seen during a particular program context. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Program Context Representation</head><p>The active program context is reflected by the current content of the RAS. As the number of bits in the RAS is large and contains a great deal of redundancy, we can compact it into a smaller representation through hashing. In our implementation we simply XOR the bit-string representing the RAS state to a desired bit width (we find that 32 bits suffice). We call the resulting hash value a signature. Because we use a commutative operation, signatures can maintained in a single register and updated incrementally in response to RAS pushes, pops, and overflows.</p><p>The call stack reflected in the RAS, and its corresponding signature, is an indication of the execution path taken to reach a particular point in the program, which in turn will determine the content of the L1 instruction cache. However, when a single caller function invokes many callees, upon return from each callee, the RAS state (and signature) is identical, but subsequent instructions will differ as they correspond to different segments of the caller. While each segment could suffer different instruction cache misses, they all associate with the same signature.</p><p>To distinguish these return sites, we form signatures before processing return instructions (i.e., before popping the RAS) but after call instructions (i.e., after pushing the RAS) and further extend the signature with a bit to indicate the direction of the control transfer (call or return). Hence, signatures always make use of all available information, and RDIP triggers prefetches on traversals both up and down the call stack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Logging Misses</head><p>The Miss Table records the association between a signature and a set of instruction cache misses that have been recently observed for that signature. It is a set-associative structure accessed by signature, with least-recently-used replacement. During execution, instruction cache miss addresses are recorded in a circular buffer, the Current Signature Misses buffer. Whenever a call or return instruction is retired, and therefore the active RAS signature changes, we first record the misses logged in the Current Signature Misses buffer into an appropriate entry in the Miss Table (we discuss the signature to which these misses should be associated in the next subsection). We then clear the Current Signature Misses buffer. The Current Signature Misses buffer is organized as a circular buffer, hence, misses will be discarded if the buffer overflows.</p><p>To reduce the size of miss table entries, we first compress the sequence of misses using a scheme similar to that pro-posed for PIF <ref type="bibr" target="#b8">[8]</ref>. The key observation is that the misses associated with a particular signature usually comprise a small number of contiguous blocks separated by discontinuities due to local control flow. For each nearby region of blocks, we record a single base address and then a bit vector indicating the other nearby blocks (at higher and lower addresses) that should also be prefetched. We provision storage for two (or more) such regions per signature, to account for a larger discontinuity within a program context. We examine sensitivity to the bit vector encoding and number of regions in Section 5.</p><p>When updating an existing miss table entry, we merge newly recorded misses with the existing encoding of regions when possible (i.e., setting additional bits for an existing base address), replacing regions in round-robin fashion when merging is impossible (e.g., if the newly recorded misses are not near the previously recorded regions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Program Context Prediction</head><p>Nominally, we might associate misses with the active signature when those misses were observed. However, under such a design, when we later try to use the Miss Table to predict future misses, our prefetches will not be timely; the interval between the signature change (call or return) and the misses is too short to hide their latency. Instead, when we log misses, we record them with a Previous Signature, as shown in Figure <ref type="figure" target="#fig_1">2</ref>. By maintaining a FIFO queue of previous signatures, rather than a single previous signature, we can increase RDIP's prefetch lookahead. We examine sensitivity to the number of signatures of lookahead in our evaluation. We will show that a lookahead of one signature is sufficient to assure timely prefetch.</p><p>Alternatively, one might design RDIP with separate signature and miss prediction tables (as in two-level branch predictors), which would allow signature lookahead to be varied dynamically at run time. However, such a design requires far more storage for the additional signature table, hence, we do not consider it further.</p><p>Upon every signature change, RDIP consults the miss table. If it finds a match, it issues the prefetch requests indicated in the table entry. It is important to note that accessing the Miss Table and issuing prefetch requests lie off the critical path of instruction fetch and hence the hardware structures can be pipelined to meet cycle time constraints. Pipelining the Miss Table delays the issue of prefetches by up to a few cycles. However, RDIP provides sufficient lookahead to hide small increases in prefetch issue latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Example</head><p>Figure <ref type="figure">3</ref> shows an example of how RDIP constructs signatures and logs misses. Each row of the table represents the execution of a single instruction. PC indicates the instruction PC, while Type indicates the type of instruction (Call, Return, or other). RAS shows the state of a 2-entry RAS, while Signature shows a representation of the new signature value each time the signature changes. The signatures in the table are shown as a list of return addresses, separated by ? operators, followed by a 0 indicating a call or 1 indicating a return operation caused the signature change. ICache Miss indicates if a miss is incurred when fetching a particular instruction, while the Description describes the actions taken by RDIP.</p><p>Initially, the RAS has a single valid return address (0x048), execution begins at address 0x400, and the initial active signature is (0x048|0), while the previous signature (not shown) is (0x0|0). When the instruction at address 0x408 is fetched, an instruction cache miss occurs, and address 0x408 is logged. The call instruction (0x410) causes the active signature to change to (0x414?0x48|0), and the logged misses are recorded in the signature table in the entry for the previous signature (0x0|0). We again log a miss for instruction 0x100. Upon execution of the return instruction (0x104), the active signature again changes, to (0x414?0x48|1). As before, the logged misses are entered into the miss table and associated with the previous signature (0x048|0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">METHODOLOGY</head><p>We next describe the simulation infrastructure and workloads we use to investigate RDIP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Simulation Infrastructure</head><p>We use the gem5 <ref type="bibr" target="#b4">[4]</ref> simulation infrastructure for all of our analysis. All results were obtained by simulating an out-of-order core executing the ARM instruction set, the configuration details of which can be found in Table <ref type="table" target="#tab_2">1</ref>. Our prefetcher was evaluated using a combination of trace-based studies for the sensitivity analyses and detailed full-system simulation for performance studies. In all cases, we capture a checkpoint once steady state behavior has been reached. From this checkpoint, we either generate an instruction fetch trace for offline analysis or run performance simulations with a detailed CPU model and memory system.</p><p>For our trace-based studies, we collect execution traces of one billion instructions for each benchmark. We analyze these traces to guide the final design of RDIP. In particular, we use this trace-based methodology to tune the size of the miss table, choose a hashing algorithm for the signature compression, and experiment with techniques for compressing a sequence of misses. We validate these design decisions using execution-driven full-system simulation.</p><p>All performance results derive from full-system simulation. We launch the simulation from a checkpoint and simulate two billion instructions. We report the metric used to measure performance for each workload in the descriptions of the workloads.</p><p>We obtain energy-per-access, static power, and access time estimates for RDIP's miss table and structures used in other prefetchers using CACTI 5.3 <ref type="bibr" target="#b35">[35]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Workloads</head><p>We study a number of server and cloud workloads that have large instruction footprints. Several of our workloads use operating system services intensively, hence, we use fullsystem simulation and run our workloads on Ubuntu 12.04 (Linux kernel v3.3). Unlike oft-used user-level CPU-intensive benchmark suites (e.g., SPEC), our workloads comprise many thousands of code lines overwhelming L1 instruction caches. Descriptions of each follow: gem5. To investigate a design automation workload, we simulate the gem5 simulator running within itself. The simulated gem5 instance is simulating the twolf benchmark from the SPEC2000 suite, using a simple CPU model and a timing-accurate memory system. The gem5 simulator is a large C++ program with over 200k lines of code that makes extensive use of virtual function calls. The code size coupled with pervasive virtual function indirection renders simple prefetchers ineffective. The performance metric is IPC when simulating gem5 (which is, in turn, simulating twolf).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hadoop.</head><p>To analyze massive amounts of unstructured data, companies have turned to MapReduce implementations such as Apache's Hadoop. Hadoop is a shared-nothing parallel processing framework designed to run on scale-out systems and its use over the last few years has exploded. For our simulations we use an in-house proxy for hadoop (written in Java), and carrie out the following Hadoop mapping tasks:</p><p>? Teraread (hdtr): This hadoop workload reads through terasort input data and writes it back to a different part of the filesystem. This workload represents small mapping tasks and exercises the I/O sub-system and associated APIs.</p><p>? Word Count (hdwc): Word Count builds a map of words to counts found in the input corpus, useful for building indexes.</p><p>The performance metric for Hadoop is the rate at which data is processed and written back to the file system.</p><p>Memcached. In memory key-value stores such as memcached are widely used by internet services operators to enable scalable web services. Memcached clusters reduce pressure on back-end databases and drastically reduce the average latency to retrieve cached data. Memcached clusters generally serve as a best-effort cache for a database backing store, as memcached does not offer the same resiliency guarantees provided by a database. Facebook reports using memcached clusters of over 800 servers that cache over 28 TB of data <ref type="bibr" target="#b29">[29]</ref>. The behavior of a memcached server varies significantly with the kind of traffic it services, in particular the distribution of requests sent by clients and the size distribution of the cached objects <ref type="bibr" target="#b3">[3]</ref>. Note that, although memcached itself contains relatively little code (under 10,000 lines), it makes extensive use of operating system services and the networking stack, resulting in a kernel instruction footprint that overwhelms the L1 cache <ref type="bibr" target="#b18">[18]</ref>. We simulate two different workloads running on top of Memcached:</p><p>? MicroBlog (mcmb): This workload represents queries for short snippets of text (e.g., user status updates). We base the object size and popularity distribution on a sample of "tweets" (brief messages shared between Twitter users) collected from Twitter. The text of a tweet is restricted to 140 characters, however, associated meta-data brings the average object size to 1kB with little variance. Even the largest tweet objects are under 2.5kB in size.</p><p>? FriendFeed (mcff ): Our second workload demonstrates heavy use of MULTI-GET requests. Facebook has disclosed that MULTI-GETs play a central role in its use of memcached <ref type="bibr" target="#b6">[6]</ref>. This workload seeks to emulate the requests required to construct a user's Facebook Wall, a list of a user's friends' most recent posts and activity. We use distribution of requests from the MicroBlog workload, as Facebook status updates and tweets have similar size and popularity characteristics <ref type="bibr" target="#b3">[3]</ref>. However, instead of issuing single GET requests individually, a group of roughly 100 GETs is batched together to approximate a user having as many friends. MULTI-GETs improve efficiency because they incur fewer trips through the TCP/IP stack, which reduces pressure on the ICache.</p><p>We quantify performance by reporting the number of requests per second the server achieves when offered a load that saturates the CPU (no idle time), but does not saturate the network interface.</p><p>SSJ. Server Side Java (SSJ) tests Java performance in the SPECpower benchmark, and is similar to the SPECjbb benchmark. The benchmark is written in Java and runs on top of a commercial JVM. The JVM we study performs justin-time compilation and aggressively inlines method invocations when generating native code sequences in its code cache. Hence, the JIT process flattens much of the control flow hierarchy present in the original Java code, with control transfers through branches and jumps rather than calls and returns. As such, this workload represents a particularly challenging case for RDIP. The workload contains a mix of six different transaction types and we quantify performance by measuring the transaction completion rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RESULTS</head><p>We next evaluate the effectiveness and impact of RDIP relative to existing prefetching approaches and the potential gains from an ideal instruction cache. First, we validate the key premises underlying RDIP. Then, we perform sensitivity studies to size the hardware structures in RDIP. Next, we show the coverage and performance achieved by RDIP and contrast them with the state-of-the-art PIF prefetcher and a basic Next-2-Line prefetcher. We finally compare our storage and energy overheads against PIF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Prefetch Accuracy &amp; Coverage</head><p>We begin by validating the key premises on which RDIP rests, namely that instruction misses correlate strongly to signatures and that signatures themselves are predictable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Potential of Signature Based Prefetching</head><p>The key premise of RDIP is that the set of instruction cache misses incurred each time a signature recurs is repetitive. This phenomenon allows us to record previously seen misses for a signature and prefetch them upon the next occurrence of the signature, thus eliminating potential misses. The intuition underlying this premise is that the signature is an indication both of the past control-flow, which determines the L1 instruction cache content, and upcoming control flow, which determines the set of cache blocks that will be needed in the near future.</p><p>We examine this key issue in Figure <ref type="figure" target="#fig_2">4</ref>. The graph shows, for all of our benchmarks, potential RDIP coverage as a function of the history depth. Here, we define coverage as the fraction of misses incurred while a signature is active that appear within the previously recorded history for that signature. For this experiment, we do not restrict the storage in the Miss Table, rather, we assume an unbounded number of entries and maintain up to 50 distinct miss addresses per signature, updated in an LRU fashion. Figure <ref type="figure" target="#fig_2">4</ref> shows that recording as few as 10 cache block addresses achieves nearly 80% coverage for most benchmarks. These results demonstrate that, with the notable exception of SSJ, our main premise holds-misses are strongly correlated to signatures, and RDIP has the potential to eliminate most of the instruction cache misses. We explore the Miss Table entry organization in greater detail in subsequent sections.</p><p>In SSJ, the impact of the JVM's just-in-time compilation can be seen: far more misses must be tracked for each signature to achieve high coverage. Because of inlining and loop unrolling, the JIT tends to generate long functions without </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Signature Prediction Accuracy</head><p>LookAhead-1 LookAhead-2 LookAhead-3</p><p>Figure <ref type="figure" target="#fig_5">5</ref>: Future Signature Prediction Accuracy: The accuracy with which future signatures can be predicted given the current signature. LookAhead-X indicates a prediction X signatures into the future.</p><p>internal calls and returns. Control is returned to the JVM upon a method invocation from within generated code. The RAS state therefore reflects the state of the JVM, rather than the executing Java program, when control is transferred into the code cache. Hence, the control flow structure of the original Java code is obscured. Nevertheless, RDIP still has the potential to eliminate about half of instruction cache misses within practical storage constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Signature Prediction</head><p>The second premise of RDIP is that the current RAS signature effectively predicts upcoming signatures. That is, that a program's call graph is repetitive. It is this property that enables RDIP to achieve sufficient lookahead to hide prefetch latency. Inaccurate signature prediction leads to spurious prefetches-the wrong blocks will be fetched from L2, wasting bandwidth and polluting the cache. We begin our evaluation by demonstrating that signatures can be predicted.</p><p>Figure <ref type="figure" target="#fig_5">5</ref> shows the RAS signature prediction accuracy as a function of lookahead, that is, how many signatures into the future we attempt to predict. Predicting the next RAS signature is synonymous with predicting the next program context-the next call or return operation. The three bars for each benchmark correspond to lookaheads of one, two, and three signatures, respectively. The further into the future we predict, the less accurate the prediction, as a considerable number of control flow decisions may be made between three consecutive call/return operations. The vertical axis indicates the prediction accuracy, where 1.0 indicates that future signatures are always correctly predicted.</p><p>We find that signature prediction accuracy is high-80% for a one-signature lookahead on average. Greater lookahead reduces accuracy. Fortunately, however, we find in our timing simulations that a lookahead of a single signature is sufficient to hide the L2 instruction fetch latency.</p><p>For SSJ, though our previous results indicated instruction cache misses correlate poorly to RAS state for generated code, we nevertheless find here that the call graph within the JVM remains predictable, as demonstrated by the high signature prediction accuracy.</p><p>As discussed in Section 3, we do not explicitly predict signatures using a signature table. Rather, we associate misses in the Miss Table with the preceding signature, this association implicitly relies on signature predictability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Practical Design</head><p>In this subsection, we optimize the storage requirements of RDIP to arrive at a practical design. The storage requirements for RDIP are determined by two factors: (1) the number of signatures to be tracked, and (2) the number of associated misses per signature. Below, we analyze both these factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Few Signatures Account for Most Misses</head><p>The number of distinct signatures encountered in each benchmark is quite high (16000 for SSJ). Tracking all of these signatures would require prohibitive storage. Fortunately, we observe that only a few signatures account for the vast majority of instruction cache misses in each benchmark, as shown in Figure <ref type="figure" target="#fig_4">6</ref>. Each graph shows the cumulative fraction of misses attributable to each distinct signature, with distinct signatures sorted along the x-axis in descending frequency of occurrence. The figure demonstrates that a few thousand most frequently occurring signatures account for the vast majority of misses in all cases. Thus, tracking just these signatures is sufficient to achieve near-peak coverage. In the following subsection, we look at the actual number of signatures that must be tracked in the Miss Table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Miss Table Size</head><p>Clearly, an unbounded Miss Table is infeasible. The Miss Table is indexed in a fashion similar to a cache in that the last N bits of the signature are used to index the table (where N is log2 size of the table) and the remaining bits are used as a tag. If a signature misses in the table, no prefetches are issued.</p><p>We sweep a range of practical sizes for the Miss Table, from 256 to 64K entries. We assume a 4-way set-associative structure. The 64K-entry table achieves 99% hit rate and results in no performance loss when compared to an infinitestorage Miss Table . We tried all power-of-two sizes between 256 and 64K and report the most relevant subset in Figure <ref type="figure">7</ref>. We select a 4K-entry miss table for our final design, as it achieves a 96% hit rate on average while reducing storage by 94% when compared to a 64K-entry miss table. Next we look at the number of misses that are associated with each signature in the Miss Table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Miss Table Entry Encoding</head><p>As shown in Figure <ref type="figure" target="#fig_2">4</ref>, storing just a few instruction cache misses (about 15) can result in coverages &gt;80% for RDIP. However, storing complete addresses for 15 cache blocks in each Miss Table entry remains prohibitive. Fortunately, as observed by Ferdman et al. <ref type="bibr" target="#b8">[8]</ref>, misses tend to be closely clustered with only a few discontinuities. Hence, we adopt a compression scheme similar to theirs to reduce the number of bits required in each Miss Table Entry. Each Miss Table Entry stores one or more trigger addresses (each 26 bits long) and an associated bit vector (8 bits long) that indicates which surrounding blocks to prefetch. </p><p>Miss We perform a sensitivity study to discover the impact of varying the number of trigger addresses. We also study the width of the bit vector indicating the surrounding blocks, but found no improvement when increasing the bit vector length above eight bits. The coverage achieved for a varying number of trigger addresses is presented in Figure <ref type="figure">8</ref>. Coverage, here, is defined as the fraction of misses that can be eliminated by RDIP when restricted to an encoding scheme with the specified number of trigger addresses, varying from one to five. Increasing the number of trigger addresses allows the prefetcher to correctly capture miss sequences for functions with more complex internal control flow (i.e., more jumps and fetch discontinuities) and also account for virtual function calls. As can be seen in the graph, adding additional trigger addresses substantially improves performance </p><p>Figure <ref type="figure">8</ref>: Miss Table <ref type="table">Entry</ref>: No. of Trigger Addresses vs Coverage. Average coverage achieved is around 75% when 3 or more triggers are used. We use 3 trigger addresses and associated bit-vectors for each Miss Table <ref type="table">Entry</ref>.</p><p>over a single trigger address. However having additional triggers implies additional storage. We select 3 triggers as a sensible trade-off between storage cost and coverage. As expected, SSJ has the lowest coverage among all benchmarks.</p><p>We have found that even 10 triggers is insufficient to achieve perfect coverage for SSJ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Sensitivity to RAS size</head><p>Next, we analyze the sensitivity of RDIP to the number RAS entries used to form a signature. RDIP only considers non-speculative call/return instructions when constructing the signature to minimize spurious signature changes and prefetch requests. Processors maintain some notion of non-speculative RAS to recover from mispredicted branches.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.5">Practical Configuration</head><p>In summary, we configure RDIP with a 4K-entry miss table, each having three trigger addresses and an 8-bit vector of blocks to prefetch. The total storage requirement is about 63kB. In the following section we analyze the performance of RDIP relative to alternative prefetching schemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performance</head><p>In this section, we contrast the performance of RDIP with other relevant prefetcher designs. Throughout this section, we consider five prefetcher designs:</p><p>? NoP represents a system lacking an I-Cache prefetcher.</p><p>? N2L is a standard next-2-line prefetcher, typical of current processors. We have tried several other prefetch depths, and found next-2-line to perform best.</p><p>? PIF is our implementation of Proactive Instruction Fetch proposed by Ferdman et al. <ref type="bibr" target="#b8">[8]</ref>, which is the most effective instruction prefetching design reported to date.</p><p>? RDIP is our prefetcher design.  ? Ideal is an unrealizable 1MB instruction cache with the same latency as the 32kB cache. It easily captures working sets for all workloads and acts as an upper bound for performance of history-based schemes like RDIP and PIF.</p><p>We first discuss Figure <ref type="figure" target="#fig_0">10</ref>, which shows the decrease in instruction cache miss rate for various prefetcher configurations across benchmarks. While RDIP reduces overall miss rate by 72.4%, N2L reduces it by only 28.5% and PIF by 74.2% over the baseline NoP case. These reductions result in corresponding improvements in performance as shown later.</p><p>Next, we discuss the overall effectiveness of the prefetchers (N2L, PIF, RDIP) in terms of the fraction of misses eliminated and the number of erroneously prefetched blocks transferred from L2 into the L1 cache. In Figure <ref type="figure" target="#fig_8">11</ref>, we show the number of prefetch hits, misses, and erroneous prefetches.  Erroneous prefetches hurt performance in two ways: (1) they waste L1-L2 bandwidth, and (2) they displace potentially useful blocks in the L1 cache. In all benchmarks except gem5, RDIP issues fewer erroneous prefetches than even N2L. PIF almost always issues many more prefetch requests than RDIP. This result is to be expected as PIF has no way of predicting when the current temporal stream being prefetched terminates. To provide timely prefetches, the PIF prefetch engine traverses well-ahead of the actual commit stream. Thus, erroneous prefetches are issued past the end of every temporal stream. Additionally, PIF uses the PC of the head of a temporal stream to identify/distinguish streams. However, the same PC sometimes results in a miss in different program contexts; PIF's stream detection mechanisms is unable to distinguish such differing contexts, leading to a decrease in accuracy (but, generally, little decrease in coverage since the correct stream is often identified on the next miss). RDIP achieves much of the coverage possible with PIF, while issuing far fewer requests. RDIP's greater accuracy leads to significant energy savings, as we show in Section 5.4.</p><p>As can be seen from Figure <ref type="figure" target="#fig_1">12</ref>, averaged across the workloads, the ideal cache (Ideal) improves performance by 16.2%, which is substantially better than the 4.8% available from a next-2-line prefetcher. The frequent function calls and fetch discontinuities in these benchmarks give rise to the relatively poor performance of a next-2-line prefetcher. PIF improves performance by as much as 40% and by 12.9% on average. RDIP outperforms the next-2-line prefetcher and performs comparably to PIF in all cases except SSJ. N2L is relatively effective for SSJ because the JIT frequently generates long sequential functions, due to inlining and loop-unrolling. PIF is similarly able to learn these long miss sequences, since it relies on miss addresses rather than calls and returns to distinguish streams. A hybrid of RDIP and N2L might reclaim some of the lost opportunity on SSJ. Overall, RDIP improves performance by as much as 33% (11.5% on average), and realizes over 70% of the possible performance achieved with an Ideal instruction cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Energy and Storage Overheads</head><p>In this section, we present the storage and energy requirements of RDIP and contrast them with the requirements of  PIF. The original report on PIF does not explicitly enumerate the storage requirements of the design, so we must infer some structure sizes from other results reported in the work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Hardware Overhead</head><p>RDIP's storage overhead is almost entirely in the Miss Table . Based on studies presented earlier in this section, we size the Miss Table to have 4K entries with each entry as described previously. Each tag is 22 bits long, each trigger address is a cache block address and hence is 26 bits long. We use an 8-bit vector to identify nearby misses. Thus, the total storage per entry is 16 bytes (22+3*(26+8) bits), which results in a total structure size of about 63kB (we use 64kB in CACTI for energy calculations). The storage required by PIF includes a 32K entry history buffer, with each entry storing a cache block region (26+8 bits), which implies a 136kB (128kB in CACTI) history buffer as well as an index table, which we estimate at 68kB (64kB in CACTI; the index table must be large enough to refer to each unique trigger address in the history buffer). The estimate for index table size is based on earlier work <ref type="bibr" target="#b9">[9]</ref>, which employs similar structures. RDIP reduces storage by at least 3X compared to PIF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">ISO-storage Comparison</head><p>In this section, we present an iso-storage comparison between N2L, PIF and RDIP. We configure each prefetcher with a storage overhead budget of 64kB. In case of N2L, we employ a 96kB I-Cache with a hit latency of 3ns as opposed to 32kB and 1ns for PIF and RDIP. PIF was configured to have 8K-entry history buffer and index table. As can be seen from Figure <ref type="figure" target="#fig_10">13</ref>, RDIP performs best of the three.  The performance of N2L with a large I-Cache highlights the criticality of maintaining low I-Cache hit latencies. RDIP comes out better than PIF for the given storage budget as RDIP tracks only I-cache misses where as PIF tracks all the accesses made.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Energy Overhead</head><p>Here we compare the energy and power requirements of RDIP and PIF. In Table <ref type="table" target="#tab_6">2</ref>, we present static power requirements, which we obtain from CACTI <ref type="bibr" target="#b35">[35]</ref>. Based on Table <ref type="table" target="#tab_6">2</ref> and access counts to various structures (including I-Cache and L2-Cache), we estimate the average dynamic energy overhead per instruction for PIF and RDIP. We report them in Table <ref type="table" target="#tab_7">3</ref>.</p><p>The line (right y-axis) in Figure <ref type="figure" target="#fig_11">14</ref> (PIF/RDIP) shows that PIF consumes nearly 1.9X more dynamic energy per instruction, on average, when compared to RDIP. This overhead is due to its large structures and more accesses to these structures and the L2-Cache. Interestingly, RDIP's energy overhead is only 7% more than N2L. Despite the additional storage structure, RDIP's greater accuracy as compared to N2L results in reduced L2 accesses, offsetting the storage energy overhead. Figure <ref type="figure" target="#fig_11">14</ref> also shows the percent of total dynamic energy which was "usefully" spent, for both PIF and RDIP. "Useful" energy is defined as the energy spent in prefetching cache blocks from L2 to L1 that subsequently resulted in prefetch hits. The "non-useful" component of energy arises due to the static energy of prefetcher-specific hardware structures and erroneous prefetches. For RDIP, nearly 67% of the energy spent goes towards transporting useful cache blocks, while for PIF, only 40% of energy is usefully spent.</p><p>These results show that RDIP, though marginally outperformed by PIF, is more energy and power efficient, reducing prefetcher related energy consumption by 1.9X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>Recent research shows that L1 instruction fetch misses remain a critical performance bottleneck, accounting for up to 40% slowdowns in server applications. While instruction footprints comfortably fit in the last-level caches, they overwhelm typical L1 instruction caches, which are limited by strict latency constraints.</p><p>In this work we showed that the RAS captures the program context and correlates strongly with L1 instruction cache misses. This observation allowed us to develop a prefetcher that associates prefetch operations with signatures formed from the RAS state. Our RAS-Directed Instruction Prefetcher is able to realize nearly 70% of the total possible performance improvement of an impractically large L1 instruction cache resulting in a 11.5% improvement in overall performance over a prefetcher-less baseline. RDIP also comes within 2% of the performance achieved by the state of the art prefetcher (PIF). Unlike PIF, which requires impractical storage and considerable complexity, RDIP is able to realize this performance improvement at one third the hardware overhead and half the energy overhead.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Speedup under next-2-line (N2L) and an idealized instruction cache (Ideal) normalized to a cache without prefetching (NoP).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Prefetcher Design.</figDesc><graphic url="image-19.png" coords="3,130.31,55.24,349.63,170.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Coverage vs. Prefetch Depth. As the number of past misses (and correspondingly, number of prefetches) associated with each signature increases, coverage improves, saturating between 10 and 20 for nearly all workloads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: Not all Signatures are Equal: For each benchmark, we show the cumulative fraction of misses attributable to each distinct signature, with distinct signatures sorted along the x-axis in descending frequency of occurrence. For each benchmark, few frequently occurring signatures account for the vast majority of misses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>g e m 5 H</head><label>5</label><figDesc>D -t e r a r e a d H D -w d c n t S S J M C -f r i e n d f e e d M C -m i c r o b l o g H</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Figure 10: I-Cache Miss Rate across prefetching schemes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Coverage and Erroneous Prefetches. Each bar is divided into three segments, Prefetch Hits eliminated by the prefetching method, remaining Misses, and, above 100%, the number of Erroneous Prefetches normalized to the number of hits+misses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Iso-storage study (64kB).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Energy Efficiency PIF vs RDIP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Simulator Configuration.</figDesc><table><row><cell></cell><cell>2GHz OoO</cell></row><row><cell>Core</cell><cell>6-wide Dispatch, 8-wide Commit 96-entry ROB, 16-entry RAS</cell></row><row><cell></cell><cell>32-entry Issue Queue</cell></row><row><cell></cell><cell>12-entry Skid Buffer</cell></row><row><cell>I-Cache</cell><cell>32kB, 2-way, 64B 1ns cycle hit latency, 4 MSHRs</cell></row><row><cell>D-Cache</cell><cell>64kB, 2-way, 64B 2ns hit latency, 6 MSHRs</cell></row><row><cell>L2-Cache</cell><cell>2MB, 8-way, 64B 12ns hit latency, 16 MSHRs</cell></row><row><cell>Main Memory</cell><cell>2GB, 54ns access latency</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table Hit</head><label>Hit</label><figDesc></figDesc><table><row><cell>Rate</cell><cell>256 1K 4K 16K 64K</cell></row><row><cell cols="2">Figure 7: Miss Table: Size vs Hit Rate. We select a 4K-entry</cell></row><row><cell cols="2">(4-way associative) design, which achieves 96% of possible</cell></row><row><cell cols="2">Miss Table Hit Rate.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>This non-speculative RAS state (which is referred to simply as "RAS" from here on) is used by RDIP for signature generation. For the results presented in Figure9, we use the top N entries of the RAS. RDIP with a larger RAS trades off miss table hit rate (due to more distinct signatures competing for the same miss table capacity) against future signature predictability (a larger RAS implies fewer overflows and more distinct signatures to identify program contexts). As can be seen from Figure9, RDIP's performance is maximized with a 4-entry RAS; a smaller RAS sacrifices too much signature predictability while a larger RAS increases miss table conflicts. We use a 4-entry RAS for all other RDIP results. (Note that the branch predictor continues to use a 16-entry RAS in all cases).</figDesc><table><row><cell>Relative Performance (Relative to NoP)</cell><cell>1.2 1.3 1.4 1.5</cell><cell>RAS01 RAS02 RAS04 RAS08 RAS16</cell></row><row><cell cols="3">Figure 9: RDIP sensitivity to RAS size: A 4-entry RAS</cell></row><row><cell cols="3">performs best.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>). Energy and Power Estimates for Hardware structures in RDIP and PIF.</figDesc><table><row><cell></cell><cell>RDIP</cell><cell>PIF</cell><cell></cell></row><row><cell>Structure</cell><cell>MissTable</cell><cell cols="2">HistoryBuffer IndexTable</cell></row><row><cell>Size</cell><cell>64kB</cell><cell>136kB</cell><cell>68kB</cell></row><row><cell>Access Energy</cell><cell>13pJ</cell><cell>32pJ</cell><cell>11pJ</cell></row><row><cell>Static Power</cell><cell>12mW</cell><cell>39mW</cell><cell>7mW</cell></row><row><cell>Access Time</cell><cell>0.33ns</cell><cell>0.45ns</cell><cell>0.30ns</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Dynamic Energy Overhead per Instruction in pJ.</figDesc><table><row><cell>Benchmark</cell><cell cols="3">PIF RDIP Ratio</cell></row><row><cell>gem5</cell><cell>23.28</cell><cell>29.11</cell><cell>0.80</cell></row><row><cell cols="2">Hadoop-teraread 10.76</cell><cell>4.05</cell><cell>2.65</cell></row><row><cell>Hadoop-wdcnt</cell><cell>9.44</cell><cell>3.16</cell><cell>2.98</cell></row><row><cell>SSJ</cell><cell>22.07</cell><cell>7.17</cell><cell>3.07</cell></row><row><cell>MC-FriendFeed</cell><cell>23.74</cell><cell>16.3</cell><cell>1.46</cell></row><row><cell>MC-Microblog</cell><cell>66.68</cell><cell>41.84</cell><cell>1.59</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was funded by grants from <rs type="funder">ARM</rs> and <rs type="funder">NSF</rs> <rs type="grantNumber">CCF-0815457</rs>. The authors would like to thank <rs type="person">Faissal Sleiman</rs> and <rs type="person">Steven Pelley</rs> for their help in putting this work together and the anonymous reviewers for their feedback.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_7nPwPcU">
					<idno type="grant-number">CCF-0815457</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The IBM System/360 model 91: Machine philosophy and instruction-handling</title>
		<author>
			<persName><forename type="first">D</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sparacio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tomasulo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Call graph prefetching for database applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Annavaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Computer Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Workload analysis of a large-scale key-value store</title>
		<author>
			<persName><forename type="first">B</forename><surname>Atikoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frachtenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paleczny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGMETRICS/Performance</title>
		<meeting>ACM SIGMETRICS/Performance</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The gem5 simulator</title>
		<author>
			<persName><forename type="first">N</forename><surname>Binkert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sardashti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sewell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shoaib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vaish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2011-08">Aug. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Instruction prefetching using branch prediction information</title>
		<author>
			<persName><forename type="first">I.-C</forename><forename type="middle">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Mudge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Computer Design</title>
		<meeting>of the International Conference on Computer Design</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Facebook. Memcached Tech Talk with M. Zuckerberg</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Clearing the clouds: a study of emerging scale-out workloads on modern hardware</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kocberber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alisafaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>of the International Conf. on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Proactive instruction fetch</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Symposium on Microarchitecture</title>
		<meeting>of the International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Temporal instruction fetch streaming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Symposium on Microarchitecture</title>
		<meeting>of the International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Full-system analysis and characterization of interactive smartphone applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gutierrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dreslinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mudge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Emmons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Paver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symp. on Workload Characterization</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reactive NUCA: near-optimal block placement and replication in distributed caches</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hardavellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Symp. on Computer Architecture</title>
		<meeting>of the International Symp. on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">STEPS towards cache-resident transaction processing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Harizopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Very Large Databases</title>
		<meeting>of the International Conf. on Very Large Databases</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Hegde</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Intel</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Symp. on Computer Architecture</title>
		<meeting>of the International Symp. on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Shift: Shared history instruction fetch for lean-core server processors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 46th International Symposium on Microarchitecture</title>
		<meeting>46th International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Selective, accurate, and timely self-invalidation using last-touch prediction</title>
		<author>
			<persName><forename type="first">A.-C</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 27th International Symposium on Computer Architecture</title>
		<meeting>27th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dead-block prediction &amp; dead-block correlating prefetchers</title>
		<author>
			<persName><forename type="first">A.-C</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 28th International Symposium on Computer Architecture</title>
		<meeting>28th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Thin servers with smart pipes: Designing soc accelerators for memcached</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meisner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th International Symposium on Computer Architecture</title>
		<meeting>40th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scale-out processors</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lotfi-Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kocberber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Picorel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Idgunji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ozer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Symp. on Computer Architecture</title>
		<meeting>of the International Symp. on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cooperative prefetching: Compiler and hardware support for effective instruction prefetching in modern processors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mowry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Symp. on Microarchitecture</title>
		<meeting>of the International Symp. on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Architectural and compiler support for effective instruction prefetching: a cooperative approach</title>
		<author>
			<persName><forename type="first">C</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mowry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Computer Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Runahead execution: An effective alternative to large instruction windows for out-of-order processors</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Symp. on High-Performance Computer Architecture</title>
		<meeting>of the International Symp. on High-Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dynamic path-based branch correlation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 28th International Symposium on Microarchitecture</title>
		<meeting>28th International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Wrong-path instruction prefetching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mudge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Symp. on Microarchitecture</title>
		<meeting>of the International Symp. on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Code layout optimizations for transaction processing workloads</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gharachorloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Larriba-Pey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Lowney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Symp. on Computer Architecture</title>
		<meeting>of the International Symp. on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fetching instruction streams</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Larriba-Pey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Symposium on Microarchitecture</title>
		<meeting>of the International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fetch directed instruction prefetching</title>
		<author>
			<persName><forename type="first">G</forename><surname>Reinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Austin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Symp. on Microarchitecture</title>
		<meeting>International Symp. on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Optimizations enabled by a decoupled front-end architecture</title>
		<author>
			<persName><forename type="first">G</forename><surname>Reinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Austin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>Computers</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Scaling memcached at Facebook</title>
		<author>
			<persName><forename type="first">P</forename><surname>Saab</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Enlarging instruction streams</title>
		<author>
			<persName><forename type="first">O</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sequential program prefetching in memory hierarchies</title>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Effective instruction prefetching in chip multiprocessors for modern commercial applications</title>
		<author>
			<persName><forename type="first">L</forename><surname>Spracklen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Symp. on High-Performance Computer Architecture</title>
		<meeting>of the International Symp. on High-Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Branch history guided instruction prefetching</title>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tyson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Charney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Puzak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Symp. on High-Performance Computer Architecture</title>
		<meeting>of the International Symp. on High-Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Slipstream processors: Improving both performance and fault tolerance</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sundaramoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Purser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rotenburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>of the International Conf. on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Thoziyoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Muralimanohar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">Cacti 5.0, 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Instruction cache prefetching using multilevel branch prediction</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Veidenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Symposium on High Performance Systems</title>
		<meeting>of the International Symposium on High Performance Systems</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hardware-only stream prefetching and dynamic access ordering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mckee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 14th international conference on Supercomputing</title>
		<meeting>of the 14th international conference on Supercomputing</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Buffering database operations for enhanced instruction cache performance</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACM SIGMOD International Conf. on Management of Data</title>
		<meeting>of the ACM SIGMOD International Conf. on Management of Data</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Execution-based prediction using speculative slices</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Zilles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Symp. on Computer Architecture</title>
		<meeting>of the International Symp. on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
