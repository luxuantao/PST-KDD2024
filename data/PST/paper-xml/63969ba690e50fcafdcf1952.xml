<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ROBUST GRAPH REPRESENTATION LEARNING VIA PREDICTIVE CODING</title>
				<funder ref="#_saBfg99">
					<orgName type="full">Alan Turing Institute</orgName>
				</funder>
				<funder>
					<orgName type="full">AXA Research Fund</orgName>
				</funder>
				<funder ref="#_tXSGz2K">
					<orgName type="full">BBSRC</orgName>
				</funder>
				<funder ref="#_vXfhZE6">
					<orgName type="full">EU TAILOR</orgName>
				</funder>
				<funder ref="#_h9wXEgV">
					<orgName type="full">EPSRC</orgName>
				</funder>
				<funder ref="#_HHaVhaj">
					<orgName type="full">MRC</orgName>
				</funder>
				<funder>
					<orgName type="full">Scan Computers International Ltd</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Billy</forename><surname>Byiringiro</surname></persName>
							<email>billy.byiringiro@cs.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tommaso</forename><surname>Salvatori</surname></persName>
							<email>tommaso.salvatori@tuwien.ac.at</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Logic and Computation</orgName>
								<address>
									<postCode>TU</postCode>
									<settlement>Wien</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
							<email>thomas.lukasiewicz@cs.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Logic and Computation</orgName>
								<address>
									<postCode>TU</postCode>
									<settlement>Wien</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ROBUST GRAPH REPRESENTATION LEARNING VIA PREDICTIVE CODING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Predictive coding is a message-passing framework initially developed to model information processing in the brain, and now also topic of research in machine learning due to some interesting properties. One of such properties is the natural ability of generative models to learn robust representations thanks to their peculiar credit assignment rule, that allows neural activities to converge to a solution before updating the synaptic weights. Graph neural networks are also message-passing models, which have recently shown outstanding results in diverse types of tasks in machine learning, providing interdisciplinary state-of-the-art performance on structured data. However, they are vulnerable to imperceptible adversarial attacks, and unfit for out-of-distribution generalization. In this work, we address this by building models that have the same structure of popular graph neural network architectures, but rely on the message-passing rule of predictive coding. Through an extensive set of experiments, we show that the proposed models are (i) comparable to standard ones in terms of performance in both inductive and transductive tasks, (ii) better calibrated, and (iii) robust against multiple kinds of adversarial attacks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Extracting information from structured data has always been an active area of research in machine learning. This, mixed with the rise of deep neural networks as the main model of the field, has led to the development of graph neural networks (GNNs). These models have achieved results in diverse types of tasks in machine learning, providing interdisciplinary state-of-the-art performance in areas such as e-commerce and financial fraud detection <ref type="bibr" target="#b0">[Zhang et al., 2022</ref><ref type="bibr">, Wang et al., 2019]</ref>, drug and advanced material discovery <ref type="bibr" target="#b2">[Bongini et al., 2021</ref><ref type="bibr">, Zhao et al., 2021</ref><ref type="bibr" target="#b4">, Xiong et al., 2019]</ref>, recommender systems <ref type="bibr" target="#b5">[Wu et al., 2021]</ref>, and social networks <ref type="bibr" target="#b6">[Liao et al., 2018]</ref>. Their power lies in a message passing mechanism among vertices of a graph, performed iteratively at different levels of hierarchy of a deep network. Popular examples of these models are graph convolutional networks (GCNs) <ref type="bibr" target="#b7">[Welling and Kipf, 2016]</ref> and graph attention networks <ref type="bibr" target="#b8">[Veli?kovi? et al., 2017]</ref>. Despite the aforementioned results and performance obtained in the last years, these models have been shown to lack robustness. They are in fact vulnerable against carefully-crafted adversarial attacks <ref type="bibr" target="#b9">[Z?gner et al., 2018</ref><ref type="bibr" target="#b10">, G?nnemann, 2022</ref><ref type="bibr" target="#b11">, Dai et al., 2018</ref><ref type="bibr" target="#b12">, Z?gner and G?nnemann, 2019]</ref> and unfit for out-of-distribution generalisation <ref type="bibr" target="#b13">[Hu et al., 2020]</ref>. This prevents GNNs from being used in critical tasks, where misleading predictions may lead to serious consequences, or maliciously manipulated signals may lead to the loss of a large amount of money.</p><p>More generally, robustness has always been a problem of deep learning models, highlighted by the famous example of a panda picture being classified as a gibbon with almost perfect confidence after the addition of a small amount of adversarial noise <ref type="bibr" target="#b14">[Akhtar and Mian, 2018]</ref>. To address this problem, an influential work has shown that it is possible to treat a classifier as an energy-based generative model, and train the joint distribution of a data point and its label to improve robustness and calibration <ref type="bibr" target="#b15">[Grathwohl et al., 2019]</ref>. Justified by this result, this work studies the robustness of GNNs trained using an energy-based training algorithm called predictive coding (PC), originally developed to model information processing in hierarchical generative networks present in the neocortex <ref type="bibr">[Rao and Ballard, 1999]</ref>. Despite not being initially developed to perform machine learning tasks, recent works have been analyzing possible applications of PC in deep learning. This is motivated by interesting properties of PC, as well as similarities with backpropagation (BP) in terms of update of the parameters: when used to train classifiers, PC is able to approximate the weight update of BP on any neural network <ref type="bibr" target="#b16">[Whittington and</ref><ref type="bibr">Bogacz, 2017, Millidge et al., 2021]</ref>, and a variation of it is able to exactly replicate the weight update of BP <ref type="bibr" target="#b18">[Song et al., 2020</ref><ref type="bibr">, Salvatori et al., 2022a]</ref>. It has been shown that PC is able to train powerful image classifiers <ref type="bibr" target="#b20">[He et al., 2016]</ref>, is able to perform generation tasks <ref type="bibr" target="#b21">[Ororbia and Kifer, 2022]</ref>, continual learning <ref type="bibr" target="#b22">[Ororbia et al., 2020]</ref>, associative memories <ref type="bibr" target="#b23">[Salvatori et al., 2021</ref><ref type="bibr" target="#b24">, Tang et al., 2022]</ref>, reinforcement learning <ref type="bibr" target="#b25">[Ororbia and Mali, 2022]</ref>, and train neural networks with any structure <ref type="bibr">[Salvatori et al., 2022b]</ref>. It is, however, the unique credit assignment rule of predictive coding, where errors are dynamically redistributed throughout the network and concentrated where they are most needed before performing a weight update, which is interesting to us. It has in fact been shown that this allows PC models to perform better than standard ones in many biologically relevant scenarios <ref type="bibr" target="#b27">[Song et al., 2022]</ref>. In this work, we extend the study of PC to structured data, and show that PC is naturally able to train robust classifiers due to its energy-based formulation. To show that, we first show that PC is able to match the performance of BP on small and medium tasks, hence showing that the results on image classification <ref type="bibr" target="#b16">[Whittington and Bogacz, 2017]</ref> extend to graph data, and then showing the improved calibration and robustness against adversarial attacks of models trained this way. Summarizing, our contributions are briefly as follows:</p><p>? We introduce and formalize a new class of message passing models, which we call graph predictive coding networks (GPCN). We show that these models achieve a performance comparable with equivalent GCNs trained using BP in multiple tasks, and propose a general recipe to train any message-passing GNN with PC.</p><p>? We empirically show that GPCNs are less confident in their prediction, and hence produce models that are better calibrated than equivalent GCNs. Our results show large improvements in expected calibration error and maximumum calibration error on the CORA, CiteSeer, and PubMed datasets. This proves the ability of GPCNs to estimate the likelihood close to the true probability of a given data point and capacity to better capture uncertainty in its prediction.</p><p>? We further conduct an extensive robustness evaluation using advanced graph adversarial attacks on various dimensions: poisoning and evasion, global and targeted, and direct and indirect. In these evaluations, (i) we introduce PC-based graph attention networks (PC-GATs), and we show that (ii) GPCNs outperform standard GCNs on all kinds of evasion attacks, (iii) GPCNs and PC-GATs outperform their counterpart on poisoning attacks and random-poisoning attacks on large datasets, and (iv) they naturally obtain a better performance on various datasets than other complex methods that use tricks designed to make the model more robust <ref type="bibr" target="#b28">[Zhu et al., 2019]</ref>. Note that the goal of these experiments is not to provide state-of-the-art results, but to show that GPCNs have a natural predisposition towards learning robust representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>In this section, we review the general framework of message-passing neural networks (MPNNs) <ref type="bibr" target="#b29">[Gilmer et al., 2017]</ref>. Assume a graph G = (V, E, X) with a set of nodes V , a set of edges E, and a set of attributes or properties of each node in the graph, described by a matrix X ? R |V |?d . The idea behind MPNNs is to begin with certain initial node characteristics and iteratively modify them over the course of k iterations using information gained from neighbours of each node. The representation h</p><formula xml:id="formula_0">(k)</formula><p>u of a node u ? V at layer k is iteratively modified as follows:</p><formula xml:id="formula_1">h (k) u = update (k) h (k-1) u , aggregate (t) h (k-1) v | v ? N (u) ,<label>(1)</label></formula><p>where N (u) is a set of neighbors of node u, and update and aggregate are differentiable functions. The aggregate function has to be a permutation-invariant to maintain symmetries necessary when operating on graph data, such as locality and invariance properties. In this work, we mainly focus on graph convolutional networks (GCNs). Here, the aggregation function is a weighted combination of neighbour characteristics with predetermined fixed weights, and the update function is a linear transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Predictive Coding Graphs</head><p>Predictive coding networks (PCNs) were first introduced for unsupervised feature learning <ref type="bibr">[Rao and Ballard, 1999]</ref>, and later extended to supervised learning <ref type="bibr" target="#b16">[Whittington and Bogacz, 2017]</ref>. Here, we describe a recent formulation, called PC graphs, that allows to use PC to train on graphs with any topology <ref type="bibr">[Salvatori et al., 2022b]</ref>. What results, is a message passing mechanism that is similar to that of GNNs, but with no multilayer structure. Let us consider a directed graph G = (V, E), where V is a set of n vertices, and E the set of directed edges. Every vertex u is equipped with a value node h u,t , which denotes the neural activity of the node u at time t, and is a variable of the model, and every edge has a weight w u,v . Every node has a prediction ? u,t , given by the incoming signals from other layers processed by an aggregation function (in practice, always the sum), and prediction error ? u,t , given by the difference between the real value of a node and its prediction. In detail,</p><formula xml:id="formula_2">? u,t = v?p(u) w v,u f (h v,t ) and ? u,t = h u,t -? u,t ,<label>(2)</label></formula><p>where p(u) denotes the set parent nodes of u. As PC graphs are energy-based models, training happens through minimization of the global energy in each layer. This global energy F t is the sum of the prediction errors of the network:</p><formula xml:id="formula_3">F t = 1 2 u (? u,t ) 2 .</formula><p>(3)</p><p>Learning happens in two phases, called inference and weight update. The inference phase is a message passing process, where the weights are fixed, and the values are continuously updated according to neighbour information. Differently from GNNs, where the update rule is given, here it follows an objective, that is to minimize the energy of Equation <ref type="formula">3</ref>. This is done via gradient descent until convergence. The update rule is the following:</p><formula xml:id="formula_4">?h u,t ? ?E t /?x u,t = -? u,t + f (h u,t ) v?c(u) ? v,t w v,u ,<label>(4)</label></formula><p>where c(u) is the set of children vertices of u. To perform a weight update, we fix all the value nodes, and update the weights for one iteration by minimizing the same energy function via gradient descent as follows:</p><formula xml:id="formula_5">?w i,j ? ?E t /?? i,j = ? ? ? l i,t f (h j,t ) .</formula><p>(5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Graph Predictive Coding Networks</head><p>We now propose graph predictive coding networks (GPCNs), obtained by using the multilayer structure of GNNs, and the learning mechanism of PC. To design GPCNs, we introduce two different message passing rules, and incorporate them inside the same training algorithm. Both rules are derived by minimizing the same energy function of Equation 3 via gradient descent. Here, the PC graph has an hierarchical structure and intra-layer and inter-layer operations.</p><p>Inter-layer operations update neural activities and prediction according to information coming from the layer above, while intra-layer ones do it accordingly to the predictions computed from neighbour nodes according to an aggregation mechanism.</p><p>Inter-layer operation: We follow the formulation of PC graphs that we described in Section 2.1. For a node u ? V and a message passing layer k, we have neural activity state denoted as h k u,t and corresponding prediction-error state, ? k u,t . t denotes the inference phase time step during energy minimization. The predicted representation, ? k u,t , at layer k is calculated as follows:</p><formula xml:id="formula_6">? k i,t = update (k) h (k-1) u , aggregate (t) h (k-1) v | v ? N (u) and ? k u,t = h k u,t -? k u,t .<label>(6)</label></formula><p>The embeddings of the nodes u are obtained through global energy minimization during inference stage h k i,t as described in Section 2.1.</p><p>Intra-layer operation: For intra-layer operations, we similarly apply the predictive coding mechanism to neighbourhood aggregation stage, where the neural activity state of neighboring nodes of node u is denoted as h (k) N (u) and its corresponding prediction-error state and predicted state are denoted as ? agg k u,t and ? agg k u,t , respectively. The equations governing the dynamic of this model are the following:</p><formula xml:id="formula_7">h k u,t = update (k) h (k-1) u , h (k) N (u) (7) ? agg k u,t = aggregate (k) h (k-1) v | v ? N (u) (8) ? agg k u,t = h (k) N (u) -? agg k u,t .<label>(9)</label></formula><p>In a similar fashion, h</p><p>N (u) is not updated directly, rather, it is updated during inference stage. In what follows, we test GPCNs on some standard benchmarks on both inductive and transductive tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we perform extensive experiments to assess the performance of our proposed method on common benchmark tasks, their calibration, and the ability of our energy-based models to counter graph adversarial attacks. More details on the experimental setup, a description to reproduce all the results presented in this section, as well as further results, are given in the supplementary material. We now provide some information about the models and datasets used, as well as a description of the baselines that we will compare against.</p><p>Datasets. Following many related works <ref type="bibr" target="#b7">[Welling and Kipf, 2016</ref><ref type="bibr" target="#b8">, Veli?kovi? et al., 2017</ref><ref type="bibr" target="#b9">, Z?gner et al., 2018</ref><ref type="bibr">, Zhu et al., 2019]</ref>, we conduct experiments using the standard citation graph benchmark datasets: CORA, CiteSeer, and PubMed. We also employ the inherently inductive large-scale protein-to-protein interaction (PPI) <ref type="bibr" target="#b30">[Hamilton et al., 2017</ref><ref type="bibr" target="#b8">, Veli?kovi? et al., 2017]</ref> dataset to validate the scalability of our model. The dataset statistics are summarised in Table <ref type="table" target="#tab_4">5</ref> in the supplementary material.</p><p>Baselines. To evaluate the performance of our framework, we first compare GPCNs against GCNs <ref type="bibr" target="#b7">[Welling and Kipf, 2016]</ref>, as our proposed method is based on the same message-passing scheme. To make sure that the comparison is as fair as possible, all the models are identical in structure (number of parameters, depth, and width), and are trained without dropout or batch-normalisation. Furthermore, in some tasks, we have also trained PC-based graph attention networks (PC-GATs), and compared against the standard formulation trained with backpropagation <ref type="bibr" target="#b8">[Veli?kovi? et al., 2017]</ref>. We also refer to results from Robust-GCN (RGCN) <ref type="bibr" target="#b28">[Zhu et al., 2019]</ref>, a popular model specifically developed to increase the robustness of GCNs.</p><p>Evaluation metrics. We are interested in learning calibrated models, that is, the ability of a model to produce probability estimations that are accurate reflections of the correct likelihood of an event. To do that, we employ scalar quantification metrics, such as expected calibration error (ECE) and maximum calibration error (MCE). The first captures the notion of average miscalibration in a single number, and can be obtained by the expected difference between the accuracy and the confidence of a model, while the second quantifies worst-case expected miscalibration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiments on General Performance</head><p>To study where we stand in terms of performance against standard GNNs, we now test our newly proposed model against GCNs trained with BP on both transductive and inductive tasks. As shown in Tables <ref type="table" target="#tab_0">1</ref> and<ref type="table">2</ref>, our method is always comparable to the baseline in terms of performance. This is important, as it allows to make the improvements in robustness meaningful, which is the main goal of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Calibration Analysis</head><p>Here, we investigate the calibration robustness of our GPCNs in comparison to GCNs. To do that, we use reliability diagrams introduced in <ref type="bibr" target="#b31">[Guo et al., 2017]</ref>. Reliability diagrams are a visual representation of model calibration that plots the expected sample accuracy as a function of confidence. That is, if a model is perfectly calibrated, the diagrams would plot an identity function, while any variation implies miscalibration. The reliability diagrams of both GCNs and GPCNs are in Fig. <ref type="figure" target="#fig_0">1</ref>. GCNs are highly overconfident on most prediction confidence levels on the CORA dataset, and highly under-confident on 0.4 prediction confidence. Conversely, on CiteSeer, GCNs tend to be under-confident on most predictions, which proves the miscalibration of GCN models. Our model, on the other hand, tends to approximate a perfect calibration with very small variations on both CORA and CiteSeer. A similar trend is seen on PubMed,   and further evidence is supported by histograms of prediction confidence distributions provided in the supplementary material.</p><p>We have also quantified the calibration error using ECE and MCE. The results, reported inside the plots in Fig. <ref type="figure" target="#fig_0">1</ref>, again show that our models are better calibrated than GCNs. These results are interesting, as they show that our models can effectively quantify uncertainty, which is crucial in highly critical settings. As different learning rates have different impact on calibration <ref type="bibr" target="#b31">[Guo et al., 2017]</ref>, in the supplementary material, we have provided plots of how ECE and MCE change over time with different learning rates, as well as further details on the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evasion Attacks with Nettack</head><p>We now evaluate our model on one of the advanced graph adversarial evasion attacks, Nettack <ref type="bibr" target="#b12">[Z?gner and G?nnemann, 2019]</ref>. In targeted evasion attacks, the model parameters are kept fixed. Then, we employ Nettack, which uses a surrogate model trained on the same training set to attack selected victim nodes. Here, all experiments are repeated five times under different random seeds.</p><p>Following the experimental setting of <ref type="bibr" target="#b32">[Chen et al., 2021]</ref>, we assess the robustness against structural attacks. Here, we randomly select 1000 victims nodes from both the validation and the test set. As in previous works <ref type="bibr">[Z?gner and</ref><ref type="bibr">G?nnemann, 2019, Jin et al., 2020]</ref>, the perturbations budget ranges from 1 to 5, and each victim nodes is attacked separately. We employ Figure <ref type="figure">2</ref>: Robustness against direct attacks with respect to the number of perturbations on CORA (left), CiteSeer (centre), and PubMed (right). In orange, GPCNs, and in blue, GCNs.</p><p>metric corresponds to higher robustness. The results are displayed in Table <ref type="table" target="#tab_2">3</ref>, where GPCNs outperform GCNs and R-GCNs with drop-out and batch-normalisation. Interestingly, the highest robustness is achieved on PubMed, the largest dataset among the three. The result on each perturbation budget are also plotted in Fig. <ref type="figure">2</ref>, and from the figure, we observe that GPCNs outperform GCNs for all budgets, especially for large numbers of perturbations.</p><p>We have also performed a semi-qualitative analysis of robustness using classification margin and box-plots. Here, victim nodes are selected in similar fashion as in the Nettack paper <ref type="bibr" target="#b12">[Z?gner and G?nnemann, 2019]</ref>: we have selected 10 nodes with highest classification margins, 10 other nodes with lowest classification margins, and 20 nodes that are randomly selected from the test set. We then perform a range of attacks such as direct and indirect attacks and feature or/and structure attacks, and evaluate the robustness on varying perturbation budgets. In the box-plots represented in Fig. <ref type="figure" target="#fig_1">3</ref>, each point represents one victim node, and the color of each point indicates the random seed on which an experiment is performed. The suffix '(u)' indicates the performance of a model on clean graphs. A more robust model is one that retains higher classification margins after an attack.</p><p>1. Structure and feature attack: Figure <ref type="figure" target="#fig_1">3</ref> (c) shows that with only 10 perturbations on the neighbourhood structure and features of victim nodes, the classification margin of victim nodes collapses to -1 on GCNs, while GPCNs stay relatively robust with many victim nodes retaining positive classification margins, meaning that they were not adversarial affected by the attack. This trend is reflected for all numbers of perturbations (2, 5, 10), as reported in the supplementary material. In particular, when the number of perturbations is 2, the median of classification margin for GCNs fall closer to -1, while GPCNs protectaround 50 percent of the victim nodes retaining a positive classification margin after the attack.</p><p>2. Feature attacks: Since feature attacks do not affect GNNs as much as structure attacks, we use a high perturbation budget for features with perturbation numbers in {1, 5, 10, 30, 50, 100}. We observe a similar trend as above, where a small perturbation on features does not affect the model much. However, when the perturbation rate becomes large, GPCNs are much better in resisting the attacks. In detail, when the perturbation rate is equal to 30 (see Fig. <ref type="figure" target="#fig_1">3</ref> (a)), GCNs misclassify around 70% of the victim nodes, while GPCNs less than 30%. In the supplementary material we show that when the perturbation budget is increased to 100, GCNs mislassify all victim nodes, while GPCNs are still able to correctly classify most victim nodes in the upper quartile.</p><p>3. Structure attacks: GPCNs also consistently outperform GCNs under structure-only attacks, as it can be observed in Fig. <ref type="figure" target="#fig_1">3</ref> (b) under 10 perturbations. More figures under different numbers of perturbations (1, 2, 3, 5), which show similar results, are provided in supplementary material.</p><p>4. Indirect Attacks: For indirect attacks, we choose 5 influencing/neighboring nodes to attack for each victims node. Again, GPCNs consistently outperform GCNs. Interestingly, we observed that GPCNs are hardly affected on all perturbation budgets as the lower quartiles of all box plots stay in the positive half of classification margin for all attacks, as shown in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Global Poisoning Attacks using Meta Learning (Mettack)</head><p>Finally, we perform global poisoning attacks using the Mettack technique <ref type="bibr" target="#b12">[Z?gner and G?nnemann, 2019]</ref>. In poisoning attacks, only the training data are corrupted and are tempered with in a manner that renders the target model fail to learn. This is the most common type of graph attacks in the real world, as malicious individuals can change the training data, but do not have access to the parameters of the model <ref type="bibr" target="#b34">[Jin et al., 2021]</ref>. As Mettack has several variants, we use the same setting as in the Pro-GNN paper <ref type="bibr" target="#b33">[Jin et al., 2020]</ref> and employ the most destructive variant known as Meta-Self on CORA and CiteSeer, and apply A-Meta-Self (approximate faster version of Meta-Self) on PubMed due to computation limitations. The perturbation rate is varied from 0% to 25% with a step of 5, and the results are reported in Table <ref type="table" target="#tab_3">4</ref>, where we also compare with the results obtained in the Pro-GNN work. Note that the reason for the accuracy for 0 perturbations to be different from the one we reported earlier, is that here we only use the largest component of a graph instead of using all nodes. As it can be seen from Table <ref type="table" target="#tab_3">4</ref>, GPCNs consistently perform better than GCNs on all datasets with more than 10% increase in robustness on CORA when the perturbation rate is 25%. GPCNs and PC-GATs also outperform other methods under various perturbation rates on PubMed, the most challenging dataset, with more than 7% improvement over both GATs and RGCNs when the perturbation rate is 25%. In the supplementary material, we report similar results for global random attacks and evasion attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Adversarial attacks on graphs. The recent revelation of lacks of robustness of the current graph learning methods inspired a body of work that attempts to enhance the robustness of graph machine learning. Those techniques can generally be classified into three categories: robust representation, robust detection, and robust optimisation <ref type="bibr" target="#b35">[Ma et al., 2021]</ref>. Robust representation entails techniques that seek to map a graph representation into a resilient embedding space by minimising the loss objective function of anticipated worst-case perturbation approximations such as robustness certificates <ref type="bibr" target="#b36">[Bojchevski and G?nnemann, 2019]</ref> and known adversarial samples <ref type="bibr" target="#b37">[Xu et al., 2019]</ref>. Robust detection techniques, on the other hand, recognise that the dearth of robustness of GNNs stems from the local message-passing aggregation phase; thus, they selectively choose which neighbourhood nodes to include in the aggregation based on some properties. Popular techniques in this category include the Jaccard method <ref type="bibr" target="#b38">[Wu et al., 2019]</ref>, which removes edges of some nodes whose Jaccard similarity is below a certain threshold, and the singular value decomposition method <ref type="bibr" target="#b39">[Entezari et al., 2020]</ref>, which preprocesses a graph by generating a low-rank approximation of it. Finally, robust optimisation is concerned with regularisation techniques that avoid extreme embeddings. GCN-LFR (Low-Frequency  <ref type="bibr" target="#b41">[Geisler et al., 2020]</ref>, which introduces the soft medoid function as a message-aggregation method to produce a robust representation. Robust-GCN (RGCN) <ref type="bibr" target="#b28">[Zhu et al., 2019</ref>] embeds a node representation as a Gaussian distribution and utilises a variance-based attention mechanism during the neighbourhood message aggregation phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Energy-based Models (EBMs).</head><p>Although there is a considerable interest in integrating the energy-based view into deep learning <ref type="bibr" target="#b42">[Xie et al., 2016</ref><ref type="bibr" target="#b43">, Nijkamp et al., 2019</ref><ref type="bibr" target="#b15">, Grathwohl et al., 2019</ref><ref type="bibr" target="#b44">, Song and Kingma, 2021]</ref>, only a handful of works have transferred it to graph machine learning <ref type="bibr" target="#b45">[Di Giovanni et al., 2022]</ref>. Here, most lines of research have largely concentrated on graph generation tasks with models such as GNN-EBMS <ref type="bibr" target="#b46">[Liu et al., 2020]</ref> and GraphEBM <ref type="bibr" target="#b47">[Liu et al., 2021]</ref>. Only one nascent work has recently attempted to expand the GCN classifier to an energy-based model named <ref type="bibr">GCN-JEMO [Shin and Dharangutte, 2021]</ref>. GCN-JEMO derives its energy from graph properties and was demonstrated to achieve a comparable discriminative performance to classic GCN but with increased robustness. In contrast to our work, GCN-JEMO relies on a non-standard training method and is not tested on adversarial attacks.</p><p>Predictive coding. Recently, many works have been developed that use PC to address machine learning problems. A first example is computer vision, where recent works have performed image classification with simple experiments on MNIST <ref type="bibr" target="#b16">[Whittington and Bogacz, 2017]</ref>, or more complex ones on ImageNet <ref type="bibr" target="#b20">[He et al., 2016]</ref>. Other examples are image generation <ref type="bibr" target="#b21">[Ororbia and Kifer, 2022]</ref>, associative memories <ref type="bibr" target="#b23">[Salvatori et al., 2021]</ref>, continual learning <ref type="bibr" target="#b22">[Ororbia et al., 2020]</ref>, reinforcement learning <ref type="bibr" target="#b25">[Ororbia and Mali, 2022]</ref>, and NLP <ref type="bibr" target="#b49">[Pinchetti et al., 2022]</ref>. We conclude by referring to a more theoretical direction, that is, Friston's free energy principle and active inference <ref type="bibr" target="#b50">[Friston, 2010</ref><ref type="bibr" target="#b51">, Friston et al., 2006</ref><ref type="bibr">, 2016]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Summary and Outlook</head><p>In this work, we have explored a new framework to perform machine learning on structured data, inspired from the neuroscience theory of predictive coding. First, we have defined the model, and then we have shown that it is able to reach competitive performance in both inductive and transductive tasks, with respect to similar models trained with BP.</p><p>We have then tested this framework on robustness tasks, with extensive results showing that simply training GNNs using PC instead of BP, results in models that are better calibrated, and more robust against adversarial attacks. As we have used the original formulation adapted to GNNs, with no further effort put in increasing the robustness of the trained models, future work should focus on scaling up the results of this paper to large-scale models, and research on variations of the proposed framework that make these models even more robust. More generally, this work shrinks the gap between computational neuroscience and machine learning, by showing that biologically plausible methods are able to reach competitive performance on complex tasks.</p><p>Reliability diagrams are visual representation tools for model calibration, as Equation 10 is intractable, because P is a continuous random variable. They characterise the average accuracy level inside points from a given confidence level bin.</p><p>Classification margin is simply the difference between the model output probability of the ground-truth class and the model probability of the predicted most-likely class, i.e., the probability of the best second class. Thus, this metric is between -1 and 1, where values close to -1 indicate that a model is overconfident in wrong predictions, and values closer to 1 indicate that the model is confident in its correct prediction. In our reported class margin diagrams, we average these values over many samples and repeated trials with different random seeds to draw box-plot diagrams of our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Reproducibility</head><p>We use standard splits on all datasets. We report the average results of 5 runs on different seeds; the hyperparameters are selected using the validation set. Following the results from the original papers on the baseline models, we evaluate our model on 2 GNN layers. The models are trained for 300 epochs on citation graphs and 50 epochs on PPI. We also use the Adam optimiser. The reported results on the calibration analysis were performed with the initial learning rate of 0.001 for both GCNs and GPCNs, as it provided the best performances for both models. For adversarial attacks, we set the initial learning rate to 0.01 and the number of epochs to 200 to compare our results to other works. For the general performance experiment, we use grid search for the hyperparameters, as described in Table <ref type="table">6</ref>. Inductive tasks were trained using a GCN version of GraphSage <ref type="bibr" target="#b30">[Hamilton et al., 2017]</ref> with the neighborhood sample size of 25 and 10 for the first and the second GNN layer, respectively (see <ref type="bibr" target="#b30">[Hamilton et al., 2017]</ref> for more details).</p><p>Our experiments are performed using the PyTorch Geometric library <ref type="bibr" target="#b53">[Fey and Lenssen, 2019]</ref>. To do an extensive experiment, we build on GraphGym <ref type="bibr" target="#b54">[You et al., 2020]</ref>, a research platform for designing and evaluating GNNs, and we seamlessly integrate it with the predictive coding learning algorithm. In addition, we employ another PyTorch library for adversarial attacks and defenses known as DeepRobust <ref type="bibr" target="#b55">[Li et al., 2020]</ref> for various type of adversarial attacks that we perform on graphs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Calibration Analysis: Confidence in Prediction</head><p>As deep networks tend to be overconfident even if they are wrong, we compared the confidence distribution of GPCNs with GCNs in Figs 4, 5, and 6, where confidence is the maximum of the softmax of the model output. The result in this section correspond to the reported results in the body of the paper on calibration analysis in Section 4.2. Both models are run for 300 epochs using the same parameters (i.e., learning rate on weights equal to 0.001), and we select the best model based on the validation set for GCNs. For GPCNs, we select the best model based on the best accuracy on the validation set as well as the lowest energy on the training set, as the energy minimization can be interpreted as likelihood maximisation. We consider the energy while selecting the best model for evaluation, because we discovered a high correlation between energy and robustness, as we will demonstrate in the following section (see Fig. <ref type="figure" target="#fig_3">7</ref>). Interestingly, the results on the prediction distribution provide another dimension to communicate the same results that we witness in Section 4.2 using reliability diagrams. We see that on the prediction distribution on the CORA dataset in Fig. <ref type="figure">6</ref>, GPCNs are relatively less confident in predictions, while GCNs are overly confident. Fig. <ref type="figure">5</ref> on the CiteSeer dataset similarly shows that most prediction confidences of the GCN model are less than 0.5, showing that GCNs are overly under-confident as we saw in the body of the paper. GPCNs, on the other hand, provide a well-behaved prediction confidence distribution on Citeseer: demonstrating that GCNs are either under-confident despite a high-performance accuracy or over-confident in the manner that is disproportional to the performance accuracy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Calibration Strengthening through Energy Minimization</head><p>We observed a high positive correlation between calibration and training energy level, thus we further investigated the role of the energy to the robustness and calibration of our learned representation. Differently from standard predictive coding networks, we observed that GPCNs require several inference steps to reach the lowest training energy possible, i.e., this can seen as reaching a local optimum of the likelihood maximisation function. More importantly, we also observed that the lower the energy the better the calibration is, i.e., the better the model can estimate uncertainty in its prediction. Figure <ref type="figure" target="#fig_3">7</ref> on the CORA dataset and Fig. <ref type="figure">8</ref> on the CiteSeer dataset showcase this correlation. The plots on the left show that the inference steps correlate to the energy level, i.e., the longer the inference is, the more likely that the model converges to a lower energy. The middle diagram and right diagrams, similarly, present the correlation between the inference steps and ECE and MCE, respectively, which from the left diagrams, implies the correlation of the energy level and ECE and MCE. We see that the lower the energy the better the calibration performance reached.</p><p>As it has been shown that calibration can be affected by learning rates <ref type="bibr" target="#b31">[Guo et al., 2017]</ref>, we track the ECE and MCE throughout training, and we see that the lower the learning rate, the better and more stable calibration GPCN is able to attain based on the ECE and MCE metrics (see Figs. 9 and 10). Left: Demonstrates that increasing inference steps leads to a lower energy. Middle: Shows that the lower energy level determines the expected error (ECE). Right: Also showcases the correlation between the energy and the maximum calibration error (MCE).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D GPCN Architecture</head><p>Using a simple graph in Fig. <ref type="figure" target="#fig_0">11</ref>, here, we demonstrate the idea of graph predictive coding as opposed to standard graph neural networks. In typical graph convolution networks (GCNs) <ref type="bibr" target="#b7">[Welling and Kipf, 2016]</ref>, the node representation is obtained by the recursive aggregation of representations of its neighbours, after which a learned linear transformation and non-linear activation function are applied. After the k t h round of aggregation (with k denoting the number of the GCN layer), the representation of a node reflects the underlying structure of its nearest neighbours within k hops. Note that the GCN is one of the simplest GNN models, as the update function equates to only neighbourhood aggregation, that is why in Fig. <ref type="figure" target="#fig_0">12</ref> (left), we only depict the aggregate function, as it captures the update function altogether.</p><p>Our GPCN model (see Fig. <ref type="figure" target="#fig_0">12</ref> (right)) differs from the standard GNN in three aspects. First, node representation are not a mere result of neighborhood aggregation. Rather, each node has a unique neural state that is updated through energy minimization using the theory of predictive coding described in the main body of this work. Specifically, each neighborhood aggregation at each hop, k, passes through a predictive coding module that predicts the incoming aggregated neighborhood representation. Second, GPCNs have a different concept of what neighborhood messages are (see Fig. <ref type="figure" target="#fig_7">13</ref>). Rather than transmitting raw messages, the instead forward the residual error of the difference between the predicted representation and the aggregation, which reduces the dynamic range of the message being transmitted, hence acting as a low pass filter. Lastly, unlike standard GNNs that are trained using BP, where the update of weights corresponding to a given neighborhood are dependent, which creates large computation graphs, GPCN learning rules are local and the model weight are updated through energy minimization, as we described in the methodology section.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional Results on Evasion Attacks with Nettack</head><p>The following plots demonstrate how both GCNs and GPCNs perform under various perturbation budgets on the four types of attacks, namely, feature, structure, feature-structure, and indirect attacks.</p><p>(1) Structure and feature attack: Figure <ref type="figure" target="#fig_8">14</ref> shows that with only 2 perturbations on the neighbourhood structure and features of victim nodes, the median classification margin approaches -1 on the GCN model, while the GPCN model stays relatively robust and with more robustness on a lower energy model (PCx3) where most of the victim nodes have positive classification margins, or in other worlds, they are not adversarially affected by the attack. This trend is even more pronounced when the perturbation rate is increased to 5 (Fig. <ref type="figure" target="#fig_9">15</ref>) and 10 (Fig. <ref type="figure" target="#fig_10">16</ref>), where, except for outliers, the margin of classification of all victim nodes falls to -1 for both the GCN and the PC models, and PCx3 stays lately more robust.   (2) Feature attacks: Since feature attacks do not highly affect GNNs as much as structure attacks, we perform large corruptions of features with the perturbation rate of 1, 5, 10, 30, 50, and 100. We observe similar trends, where small perturbations on features do not affect the model, however, when the perturbation rate becomes large, our GPCNs display an unparalleled performance resisting the attacks. When the perturbation rate is 30 (see Fig. <ref type="figure" target="#fig_14">20</ref>), while the GCN misclassifies around 70% of the victim nodes, the GPCN is still able to classify more than 70% correctly after perturbation. The highly superior performance is observed when the perturbation rate is increased to 100 in Fig. <ref type="figure" target="#fig_16">22</ref>, the GCN mislassifies all victim nodes, while the GPCN still classifies correctly those nodes with most victim nodes in the upper quartile having positive classification margins.      (3) Structure attacks: GPCNs also consistently outperform GCNs under structure-only attacks on 1, 2, 5, and 10 perturbations, as it can be observed in <ref type="bibr">Figs. 17,</ref><ref type="bibr">24,</ref><ref type="bibr">25,</ref><ref type="bibr">18,</ref><ref type="bibr">and 19</ref>.     (4) Indirect attacks: For indirect attacks, we choose 5 influencing/neighboring nodes to attack for each victim node. We observe a similar trend that was found in the Nettack paper <ref type="bibr" target="#b9">[Z?gner et al., 2018]</ref>. We found that indirect attacks do not affect GNNs as much as other attacks, as it can be witnessed from the box plots below. However, we also found that GPCNs, especially with smaller inference steps, consistently outperform all models, but all GPCN models are strictly better than GCNs under all perturbations.</p><p>Note that for Figs. 28, 29, and 30, on the x-axis, BP indicates the GCN model, PC denotes the GPCN model trained using 12 inference steps, and PCx3 indicate the GPCN model trained using 36 inference steps, hence achieving a lower training energy. The suffix "untempered" indicates the performance of the model on a clean graph. To interpret the plots, a more robust model is one that retains higher classification margins after the attacks.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F More Attacks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Global Poisoning Attacks</head><p>Following the same experimental setup as in Section 4.4 for the global poisoning attack, we perform global random attacks <ref type="bibr" target="#b33">[Jin et al., 2020]</ref>, which randomly insert fake edges into a graph, thus it can be viewed as adding a random noise to a clean graph. We evaluate our GPCN-GCN and GPCN-GAT against their similar architectural counterparts. Note that the results on the GAT model were taken from <ref type="bibr" target="#b33">[Jin et al., 2020]</ref>, which had both batchnorm and dropout, unlike our GPCN-GAT. Table <ref type="table">7</ref> shows the results under different ratios of random noise from 0% to 100% with a step size of 20%. Each experiment is repeated five times under different random seeds, and we found that our GPCN-GCN and GPCN-GAT strictly outperform their counterparts, with GPCN outperforming all models on CORA and PubMed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Reliability diagrams on multiple datasets. In the top row, we have GCNs, and in the bottom row, GPCNs. Inside every figure, we have the MCE and ECE of every model (the lower the better). Any deviation from the diagonal line indicates miscalibration. In all cases, PC outperforms standard GCNs trained with BP.</figDesc><graphic url="image-1.png" coords="5,402.53,218.71,132.36,126.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Classification margin diagrams on different types of attacks on CORA. GCN(u) and GPCN(u) labels indicate the results of GCN and GPCN on clean/untempered graph data.Figure (a) corresponds to adversarial attacks on node feature attacks; Figure (b) corresponds to adversarial attacks on graph structures; Figure (c) corresponds to adversarial attacks on both structure and features; and Figure (d) corresponds to indirect attacks where adversarial attack targets neighboring nodes of a victim node</figDesc><graphic url="image-17.png" coords="7,56.50,194.59,280.12,124.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :Figure 5 :Figure 6 :</head><label>456</label><figDesc>Figure 4: Histogram of the prediction confidence distribution on the CORA dataset. The x-axis indicates the confidence of the model on the samples, and the y-axis is the count on a normal scale of data points that fall into a given confidence bin. Left: GCN. Right: GPCN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Figure7: Evaluation of the impact of the energy on the calibration performance on CORA (learning rate = 0.001). Left: Demonstrates that increasing inference steps leads to a lower energy. Middle: Shows that the lower energy level determines the expected error (ECE). Right: Also showcases the correlation between the energy and the maximum calibration error (MCE).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :Figure 10 :Figure 8 :</head><label>9108</label><figDesc>Figure 9: Evolution of expected calibration error (ECE) on the test set during training on various learning rates (lr). Left: GCN. Right: GPCN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 11 :Figure 12 :</head><label>1112</label><figDesc>Figure 11: Example of a graph that we use to illustrate the architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 13 :</head><label>13</label><figDesc>Figure13: Difference between message aggregation between standard GNNs and GPCNs. Left: standard aggregation method; right: intra-layer GPCNs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Classification margin diagram of targeted attack on both features and structure when the perturbation rate is 2. On the x-axis, BP indicates the GCN model, PC denotes the GPCN model trained using 12 inference steps, and PCx3 indicate the GPCN model trained using 36 inference steps, hence achieving a lower training energy.</figDesc><graphic url="image-18.png" coords="17,72.00,473.86,468.00,193.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Classification margin diagram of targeted attack on both features and structure when the perturbation rate is 5.</figDesc><graphic url="image-19.png" coords="18,72.00,72.00,468.00,193.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Classification margin diagram of targeted attack on both features and structure when the perturbation rate is 10.</figDesc><graphic url="image-20.png" coords="18,72.00,364.77,468.00,193.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Classification margin diagram of targeted attack on features when the perturbation rate is 1.</figDesc><graphic url="image-21.png" coords="19,72.00,72.00,468.00,193.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: Classification margin diagram of targeted attack on features when the perturbation rate is 5.</figDesc><graphic url="image-22.png" coords="19,72.00,495.68,468.00,193.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: Classification margin diagram of targeted attack on features when the perturbation rate is 10.</figDesc><graphic url="image-23.png" coords="20,72.00,72.00,468.00,193.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: Classification margin diagram of targeted attack on features when the perturbation rate is 30.</figDesc><graphic url="image-24.png" coords="20,72.00,495.68,468.00,193.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 21 :</head><label>21</label><figDesc>Figure 21: Classification margin diagram of targeted attack on features when the perturbation rate is 50.</figDesc><graphic url="image-25.png" coords="21,72.00,72.00,468.00,193.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 22 :</head><label>22</label><figDesc>Figure 22: Classification margin diagram of targeted attack on features when the perturbation rate is 100.</figDesc><graphic url="image-26.png" coords="21,72.00,411.36,468.00,193.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 23 :</head><label>23</label><figDesc>Figure 23: Classification margin diagram of targeted attack on structure when the perturbation rate is 1.</figDesc><graphic url="image-27.png" coords="22,73.25,72.00,468.00,193.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 24 :</head><label>24</label><figDesc>Figure 24: Classification margin diagram of targeted attack on structure when the perturbation rate is 2.</figDesc><graphic url="image-28.png" coords="22,73.25,495.68,468.00,193.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 25 :</head><label>25</label><figDesc>Figure 25: Classification margin diagram of targeted attack on structure when the perturbation rate is 3.</figDesc><graphic url="image-29.png" coords="23,73.25,72.00,468.00,193.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 26 :</head><label>26</label><figDesc>Figure 26: Classification margin diagram of targeted attack on structure when the perturbation rate is 5.</figDesc><graphic url="image-30.png" coords="23,73.25,495.68,468.00,193.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 27 :</head><label>27</label><figDesc>Figure 27: Classification margin diagram of targeted attack on structure when the perturbation rate is 10.</figDesc><graphic url="image-31.png" coords="24,73.25,72.00,468.00,193.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 28 :</head><label>28</label><figDesc>Figure 28: Classification margin diagram on influence attack with perturbation equal to the degree of target node and 5 influencing neighboring nodes.</figDesc><graphic url="image-32.png" coords="24,72.00,484.77,468.00,193.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 29 :</head><label>29</label><figDesc>Figure 29: Classification margin diagram on influence attack with perturbation equal to the degree of target node and 5 influencing neighboring nodes.</figDesc><graphic url="image-33.png" coords="25,72.00,72.00,468.00,193.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 30 :</head><label>30</label><figDesc>Figure 30: Classification margin diagram on influence attack with perturbation equal to 1 and 5 influencing neighboring nodes.</figDesc><graphic url="image-34.png" coords="25,72.00,486.82,468.00,193.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 31 :</head><label>31</label><figDesc>Figure 31: Classification margin diagram on on influence attack with perturbation 10 and 5 influencing neighboring nodes.</figDesc><graphic url="image-35.png" coords="26,72.00,72.00,468.00,193.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Test accuracy on transductive tasks.</figDesc><table><row><cell></cell><cell>Method</cell><cell>CORA</cell><cell>CiterSeer</cell><cell>PubMed</cell><cell></cell></row><row><cell></cell><cell>GCN</cell><cell cols="3">80.72 ? 1.05% 67.12 ? 1.53% 77.1 ? 1.45%</cell><cell></cell></row><row><cell></cell><cell>GPCN</cell><cell>80.7 ? 1.09%</cell><cell cols="2">67.26 ? 1.28% 76.2 ? 2.44%</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Table 2: F-1 score on inductive tasks.</cell><cell></cell></row><row><cell>Method</cell><cell>CORA</cell><cell>CiterSeer</cell><cell>PubMed</cell><cell>PPI(Sup.)</cell><cell>PPI(Unsup.)</cell></row><row><cell>GCN</cell><cell>80 ? 0.41%</cell><cell>67.64 ? 1.14%</cell><cell>77.0 ? 0.46%</cell><cell>76.45 ? 0.39%</cell><cell>52.44 ? 0.37%</cell></row><row><cell>GPCN</cell><cell cols="5">79.66 ? 0.75% 69.68 ? 0.37% 77.12 ? 0.47% 78.31 ? 0.47% 54.41 ? 0.31%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Robustness against perturbations.</figDesc><table /><note><p><p><p><p><p>GCN</p><ref type="bibr" target="#b32">[Chen et al., 2021]</ref> </p>RGCN</p><ref type="bibr" target="#b32">[Chen et al., 2021</ref></p>] GCPN CORA 2.05 ? 0.07 2.79 ? 0.10 3.26 ? 0.18 CiterSeer 1.98 ? 0.12 2.02 ? 0.23 2.73 ? 0.08 PubMed 1.14 ? 0.02 1.48 ? 0.02 4.21 ? 0.32</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Classification performance of the models under poisoning global attack with Metattack.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Poisoning Attack with Mettack</cell><cell></cell></row><row><cell cols="2">Dataset Ptb Rate (%)</cell><cell>GCN</cell><cell cols="4">GPCN-GCN GAT [Jin et al., 2020] GPCN-GAT RGCN [Jin et al., 2020]</cell></row><row><cell></cell><cell>0</cell><cell cols="2">82.87 ? 0.75 83.17 ? 0.78</cell><cell>83.97 ? 0.65</cell><cell>82.8?1.21</cell><cell>83.09 ? 0.44</cell></row><row><cell></cell><cell>5</cell><cell>76.4 ? 0.88</cell><cell>78.17 ? 1.13</cell><cell>80.44 ? 0.74</cell><cell>78.91 ? 0.71</cell><cell>77.42 ? 0.39</cell></row><row><cell>CORA</cell><cell>10</cell><cell cols="2">67.98 ? 0.99 71.21 ? 1.13</cell><cell>75.61 ? 0.59</cell><cell>72.22 ? 1.21</cell><cell>72.22 ? 0.38</cell></row><row><cell></cell><cell>15</cell><cell>60.3 ? 1.73</cell><cell>65.14 ? 1.84</cell><cell>69.78 ? 1.28</cell><cell>66.77 ? 1.15</cell><cell>66.82 ? 0.39</cell></row><row><cell></cell><cell>20</cell><cell cols="2">50.31 ? 1.69 55.83 ? 3.39</cell><cell>59.94 ? 0.92</cell><cell>54.7 ? 1.34</cell><cell>59.27 ? 0.37</cell></row><row><cell></cell><cell>25</cell><cell cols="2">44.16 ? 0.88 54.27 ? 7.25</cell><cell>54.78 ? 0.74</cell><cell>49.68 ? 1.08</cell><cell>50.51 ? 0.78</cell></row><row><cell></cell><cell>0</cell><cell cols="2">72.43 ? 0.05 72.64 ? 0.51</cell><cell>73.26 ? 0.83</cell><cell>73.58 ? 0.13</cell><cell>71.20 ? 0.83</cell></row><row><cell></cell><cell>5</cell><cell>71.53 ? 0.43</cell><cell>72.1 ? 1.07</cell><cell>72.89 ? 0.83</cell><cell>72.61 ? 0.57</cell><cell>70.50 ? 0.43</cell></row><row><cell>CiteSeer</cell><cell>10</cell><cell cols="2">68.59 ? 0.65 69.04 ? 0.45</cell><cell>70.63 ? 0.48</cell><cell>70.34 ? 0.25</cell><cell>67.71 ? 0.30</cell></row><row><cell></cell><cell>15</cell><cell cols="2">65.02 ? 1.16 65.95 ? 1.21</cell><cell>69.02 ? 1.09</cell><cell>68.12 ? 0.28</cell><cell>65.69 ? 0.37</cell></row><row><cell></cell><cell>20</cell><cell cols="2">53.77 ? 0.73 56.73 ? 1.67</cell><cell>61.04 ? 1.52</cell><cell>59.12 ? 0.84</cell><cell>62.49 ? 1.22</cell></row><row><cell></cell><cell>25</cell><cell>57.49 ? 2.13</cell><cell>58.43 ? 1.7</cell><cell>61.85 ? 1.12</cell><cell>62.02 ? 0.73</cell><cell>55.35 ? 0.66</cell></row><row><cell></cell><cell>0</cell><cell>85.37 ? 0.06</cell><cell>85.3 ? 0.3</cell><cell>83.73 ? 0.40</cell><cell>84.56 ? 0.22</cell><cell>86.16 ? 0.18</cell></row><row><cell></cell><cell>5</cell><cell cols="2">81.4 ? 0.12 81.99 ? 0.24</cell><cell>78.00 ? 0.44</cell><cell>81.16 ? 0.36</cell><cell>81.08 ? 0.20</cell></row><row><cell>PubMed</cell><cell>10</cell><cell cols="2">79.73 ? 0.27 80.47 ? 0.73</cell><cell>74.93 ? 0.38</cell><cell>76.7 ? 0.74</cell><cell>77.51 ? 0.27</cell></row><row><cell></cell><cell>15</cell><cell cols="2">77.03 ? 0.11 79.38 ? 0.43</cell><cell>71.13 ? 0.51</cell><cell>73.87 ? 0.62</cell><cell>73.91 ? 0.25</cell></row><row><cell></cell><cell>20</cell><cell cols="2">75.59 ? 0.26 78.01 ? 0.37</cell><cell>68.21 ? 0.96</cell><cell>72.14 ? 0.54</cell><cell>71.18 ? 0.31</cell></row><row><cell></cell><cell>25</cell><cell cols="2">73.34 ? 0.19 75.69 ? 1.43</cell><cell>65.41 ? 0.77</cell><cell>68.94 ? 0.6</cell><cell>67.95 ? 0.15</cell></row></table><note><p><p><p><p><p>based Regularisation)</p><ref type="bibr" target="#b40">[Chang et al., 2021]</ref> </p>adopts a robust co-training paradigm that derive the robustness from the eligible low-frequency components, while MedianGCN</p><ref type="bibr" target="#b32">[Chen et al., 2021</ref></p>] leverages robust aggregation functions (i.e., the median and trimmed mean) that ignore outliers based on a breakdown point characterisation. MedianGCN is very similar to SMGCN</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>An overview of the data sets used in our experiments</figDesc><table><row><cell></cell><cell cols="2">CORA CiteSeer</cell><cell>PubMed</cell><cell>PPI</cell><cell>Reddit</cell></row><row><cell>Type</cell><cell cols="2">Citation Citation</cell><cell>Citation</cell><cell>Protein interaction</cell><cell>Communities</cell></row><row><cell>#Nodes</cell><cell>2708</cell><cell>3327</cell><cell cols="3">19717 (1 graph) 56944 (24 graphs) 232965 (1 graph)</cell></row><row><cell>#Edges</cell><cell>5429</cell><cell>4732</cell><cell>44338</cell><cell>818716</cell><cell>114615892</cell></row><row><cell>#Features/Nodes</cell><cell>1433</cell><cell>3703</cell><cell>500</cell><cell>50</cell><cell>602</cell></row><row><cell>#Classes</cell><cell>7</cell><cell>6</cell><cell>3</cell><cell>121(multilabel)</cell><cell>41</cell></row><row><cell>#Training Nodes</cell><cell>140</cell><cell>120</cell><cell>60</cell><cell>44906(20 graphs)</cell><cell>153431</cell></row><row><cell>#Validation Nodes</cell><cell>500</cell><cell>500</cell><cell>500</cell><cell>6513 (2 graphs)</cell><cell>23831</cell></row><row><cell>#Testing Nodes</cell><cell>1000</cell><cell>1000</cell><cell>1000</cell><cell>5524 (2 graphs)</cell><cell>55703</cell></row><row><cell></cell><cell></cell><cell cols="3">Table 6: Hyper-Parameter Search</cell><cell></cell></row><row><cell cols="2">Parameter Type</cell><cell></cell><cell></cell><cell>Grid</cell><cell></cell></row><row><cell cols="3">values nodes update rate</cell><cell></cell><cell>0.05, 0.1, 0.5, 1.0</cell><cell></cell></row><row><cell cols="3">Weight update learning rate</cell><cell></cell><cell cols="2">1e -2,1e -3,1e -4,1e -5</cell></row><row><cell cols="3">Number of GNN Layer</cell><cell></cell><cell>2, 4</cell><cell></cell></row><row><cell cols="3">Inference steps,T,</cell><cell></cell><cell>12, 32, 50, 100</cell><cell></cell></row><row><cell cols="6">PC synaptic weight update rate at the end of ,T, inference steps, and at every inference step</cell></row><row><cell cols="3">aggregation functions</cell><cell></cell><cell>sum, add, max</cell><cell></cell></row><row><cell cols="3">Graphsage sampling</cell><cell cols="3">10, 25 for first and second GNN layer respectively</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_0"><p>q=1 q ? p q as holistic robustness metric, where q denotes the number of perturbations, and p q is the classification accuracy corresponding to the perturbation budget q<ref type="bibr" target="#b32">[Chen et al., 2021]</ref>. A larger value of this</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported by the <rs type="funder">Alan Turing Institute</rs> under the <rs type="grantName">EPSRC grant</rs> <rs type="grantNumber">EP/N510129/1</rs>, by the <rs type="funder">AXA Research Fund</rs>, the <rs type="funder">EPSRC</rs> grant <rs type="grantNumber">EP/R013667/1</rs>, the <rs type="funder">MRC</rs> grant <rs type="grantNumber">MC_UU_00003/1</rs>, the <rs type="funder">BBSRC</rs> grant <rs type="grantNumber">BB/S006338/1</rs>, and by the <rs type="funder">EU TAILOR</rs> grant. We also acknowledge the use of the EPSRC-funded Tier 2 facility JADE (<rs type="grantNumber">EP/P020275/1</rs>) and GPU computing support by <rs type="funder">Scan Computers International Ltd</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_saBfg99">
					<idno type="grant-number">EP/N510129/1</idno>
					<orgName type="grant-name">EPSRC grant</orgName>
				</org>
				<org type="funding" xml:id="_h9wXEgV">
					<idno type="grant-number">EP/R013667/1</idno>
				</org>
				<org type="funding" xml:id="_HHaVhaj">
					<idno type="grant-number">MC_UU_00003/1</idno>
				</org>
				<org type="funding" xml:id="_tXSGz2K">
					<idno type="grant-number">BB/S006338/1</idno>
				</org>
				<org type="funding" xml:id="_vXfhZE6">
					<idno type="grant-number">EP/P020275/1</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Details on Evaluation Metrics</head><p>In this section, we explain the metrics used in the main body of this work in more detail. We employ scalar quantification metrics, such as expected calibration error (ECE) and maximum calibration error, together with visual tools, such as reliability diagrams and confidence distribution histograms <ref type="bibr" target="#b31">[Guo et al., 2017]</ref> and classification margin diagrams <ref type="bibr" target="#b11">[Dai et al., 2018]</ref> to evaluate model calibration.</p><p>The term "confidence calibration" is used to describe the ability of a model to produce probability estimations that are accurate reflections of the correct likelihood of an event, which is imperative especially in real-world applications. Considering a K-class classification task, let X ? X and Y ? Y = {1, . . . , K} be input and true ground-truth label random variables, respectively. Let ? denote a class prediction and P be its associated confidence, i.e., probability of correctness. We would like the confidence estimate P to be calibrated, which intuitively means that P represents a true probability. The perfect calibration can be described as follows:</p><p>ECE captures the notion of average miscalibration in a single number, and can be obtained by the expected difference between accuracy and confidence of the model:</p><p>MCE is crucial in safety-and security-critical settings, as it quantifies the worst-case expected miscalibration:</p><p>Random Poisoning Attack: 7 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Evasion</head><p>Fast Gradient Sign Method (FGSM/FGA):</p><p>First, following the experimental setup in Section 4.3, we assess the robustness against structural evasion attacks, known as Fast Gradient Sign Method (FGSM/FGA) <ref type="bibr" target="#b56">[Chen et al., 2020]</ref>. We randomly select 1000 victim nodes from both the validation and the test set. As in previous works <ref type="bibr">[Z?gner and</ref><ref type="bibr">G?nnemann, 2019, Jin et al., 2020]</ref>, the perturbations budget ranges from 1 to 5, and each victim node is attacked separately. The results are shown in Table <ref type="table">8</ref>, where GPCNs strictly perform better than GCNs on both CORA and CiteSeer. Due to computation limitations, we test on PubMed and only using the initial Adam learning rate of 0.01, as it was done in multiple similar bodies of previous work <ref type="bibr">[Z?gner and</ref><ref type="bibr">G?nnemann, 2019, Jin et al., 2020]</ref>. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">eFraudCom: An e-commerce fraud detection system via competitive graph neural networks</title>
		<author>
			<persName><forename type="first">Ge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianliang</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A semi-supervised graph attentive network for financial fraud detection</title>
		<author>
			<persName><forename type="first">Daixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanhui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanming</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="598" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Molecular generative graph neural networks for drug discovery</title>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Bongini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><surname>Bianchini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">450</biblScope>
			<biblScope unit="page" from="242" to="252" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">CSGNN: Contrastive self-supervised graph neural network for molecular interaction prediction</title>
		<author>
			<persName><forename type="first">Chengshuai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shichao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3756" to="3763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pushing the boundaries of molecular representation for drug discovery with the graph attention mechanism</title>
		<author>
			<persName><forename type="first">Zhaoping</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feisheng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhe</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xutong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaojun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaomin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hualiang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Medicinal Chemistry</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="8749" to="8760" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Self-supervised graph learning for recommendation</title>
		<author>
			<persName><forename type="first">Jiancan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="726" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attributed social network embedding</title>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2257" to="2270" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016">2017. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adversarial attacks on neural networks for graph data</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Z?gner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Akbarnejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2847" to="2856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph neural networks: Adversarial robustness</title>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graph Neural Networks: Foundations, Frontiers, and Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="149" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adversarial attack on graph structured data</title>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1115" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adversarial attacks on graph neural networks via meta learning</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Z?gner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08412</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22118" to="22133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Threat of adversarial attacks on deep learning in computer vision: A survey</title>
		<author>
			<persName><forename type="first">Naveed</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajmal</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="14410" to="14430" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Predictive coding in the visual cortex: A functional interpretation of some extra-classical receptive-field effects</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan-Chieh</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?rn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; Dana</forename><surname>Ballard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.03263</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="87" />
			<date type="published" when="1999">2019. 1999</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Your classifier is secretly an energy based model and you should treat it like one</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An approximation of the error backpropagation algorithm in a predictive coding network with local Hebbian synaptic plasticity</title>
		<author>
			<persName><forename type="first">James</forename><surname>Whittington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Bogacz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1229" to="1262" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Predictive coding: A theoretical and experimental review</title>
		<author>
			<persName><forename type="first">Beren</forename><surname>Millidge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anil</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Buckley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.12979</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Can the brain do backpropagation?-Exact implementation of backpropagation in predictive coding networks</title>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Bogacz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22566" to="22579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reverse differentiation via predictive coding</title>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Salvatori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Bogacz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Conference on Artificial Intelligence</title>
		<meeting>the 36th Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The neural coding framework for learning generative models</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Ororbia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kifer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Continual learning of recurrent neural networks by locally aligning distributed representations</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Ororbia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Mali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kifer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4267" to="4278" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Associative memories via predictive coding</title>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Salvatori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujian</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Frieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Bogacz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3874" to="3886" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recurrent predictive coding models for associative memory employing covariance learning</title>
		<author>
			<persName><forename type="first">Mufeng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Salvatori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beren</forename><surname>Millidge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Bogacz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Active predicting coding: Brain-inspired reinforcement learning for sparse reward robotic control problems</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Ororbia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Mali</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.09174</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Salvatori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Pinchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beren</forename><surname>Millidge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Bogacz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.13180</idno>
		<title level="m">Learning on arbitrary graph topologies via predictive coding</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Inferring neural activity before plasticity: A foundation for learning beyond backpropagation</title>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beren</forename><surname>Gray Millidge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Salvatori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Bogacz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Robust graph convolutional networks against adversarial attacks</title>
		<author>
			<persName><forename type="first">Dingyuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1399" to="1407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Understanding structural vulnerability in graph convolutional networks</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jintang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibiao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zibin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.06280</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph structure learning for robust graph neural networks</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="66" to="74" />
		</imprint>
	</monogr>
	<note>Xianfeng Tang, Suhang Wang, and Jiliang Tang</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adversarial attacks and defenses on graphs</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaxing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charu</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="19" to="34" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Understanding adversarial attacks on deep learning based medical image analysis systems</title>
		<author>
			<persName><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yitian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page">107332</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Certifiable robustness to graph perturbations</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Topology attack and defense for graph neural networks: An optimization perspective</title>
		<author>
			<persName><forename type="first">Kaidi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongge</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsui-Wei</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04214</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Adversarial examples on graph data: Deep insights into attack and defense</title>
		<author>
			<persName><forename type="first">Huijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuriy</forename><surname>Tyshetskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Docherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liming</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.01610</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">All you need is low (rank) defending against adversarial attacks on graphs</title>
		<author>
			<persName><forename type="first">Negin</forename><surname>Entezari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saba</forename><forename type="middle">A</forename><surname>Al-Sayouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirali</forename><surname>Darvishzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evangelos</forename><forename type="middle">E</forename><surname>Papalexakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Web Search and Data Mining</title>
		<meeting>the 13th International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="169" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Not all low-pass filters are robust in graph convolutional networks</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yatao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiji</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="25058" to="25071" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Reliable graph neural networks via robust aggregation</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Geisler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Z?gner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="13272" to="13284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A theory of generative convnet</title>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingnian</forename><surname>Wu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2635" to="2644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning non-convergent non-persistent short-run mcmc toward energy-based model</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Nijkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitch</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">How to train your energy-based models</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.03288</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Rowbottom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Benjamin P Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Markovich</surname></persName>
		</author>
		<author>
			<persName><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.10991</idno>
		<title level="m">Graph neural networks as gradient flows</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graph generation with energy-based models</title>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Graph Representation Learning and Beyond (GRL+)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">GraphEBM: Molecular graph generation with energy-based models</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keqiang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bora</forename><surname>Oztekin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.00546</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">An energy-based view of graph neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prathamesh</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><surname>Dharangutte</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13492</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Luca</forename><surname>Pinchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Salvatori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beren</forename><surname>Millidge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yordan</forename><surname>Yordanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<title level="m">Predictive coding beyond gaussian assumptions. 36th Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The free-energy principle: A unified brain theory?</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Friston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="138" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A free energy principle for the brain</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Friston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Kilner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><surname>Harrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Physiology</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Active inference and learning</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Friston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Rigoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Schwartenbeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Pezzulo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroscience &amp; Biobehavioral Reviews</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="862" to="879" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Fast graph representation learning with Pytorch Geometric</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Eric Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Design space for graph neural networks</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17009" to="17021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">DeepRobust: A PyTorch library for adversarial attacks and defenses</title>
		<author>
			<persName><forename type="first">Yaxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.06149</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Graph representation learning: a survey</title>
		<author>
			<persName><forename type="first">Fenxiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Cheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C-C Jay</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">APSIPA Transactions on Signal and Information Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
