<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Dimpled Manifold Model of Adversarial Examples in Machine Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-06-18">18 Jun 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Adi</forename><surname>Shamir</surname></persName>
							<email>adi.shamir@weizmann.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Math&amp;CS Weizmann Institute of Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Odelia</forename><surname>Melamed</surname></persName>
							<email>odelia.melamed@weizmann.ac.il</email>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Math&amp;CS Weizmann Institute of Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Oriel</forename><surname>Benshmuel</surname></persName>
							<email>oriel.benshmuel@weizmann.ac.il</email>
							<affiliation key="aff2">
								<orgName type="department">Faculty of Math&amp;CS Weizmann Institute of Science</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The Dimpled Manifold Model of Adversarial Examples in Machine Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-06-18">18 Jun 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2106.10151v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The extreme fragility of deep neural networks when presented with tiny perturbations in their inputs was independently discovered by several research groups in 2013, but in spite of enormous effort these adversarial examples remained a baffling phenomenon with no clear explanation. In this paper we introduce a new conceptual framework (which we call the Dimpled Manifold Model) which provides a simple explanation for why adversarial examples exist, why their perturbations have such tiny norms, why these perturbations look like random noise, and why a network which was adversarially trained with incorrectly labeled images can still correctly classify test images. In the last part of the paper we describe the results of numerous experiments which strongly support this new model, and in particular our assertion that adversarial perturbations are roughly perpendicular to the low dimensional manifold which contains all the training examples.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In 2013 <ref type="bibr" target="#b19">Szegedy et al. [2013]</ref> and <ref type="bibr" target="#b1">Biggio et al. [2013]</ref> independently demonstrated the surprising fact that even the best trained deep neural networks were extremely fragile when presented with tiny adversarial perturbations. This discovery naturally attracted a lot of interest, and many attempts to explain this phenomenon had been proposed over the last 8 years: that DNN's are too nonlinear, that they are too linear, that they were trained with an insufficient number of training examples, that adversarial examples are just the rare cases where DNNs err, that images contain robust and nonrobust features, etc. However, none of these vague qualitative ideas seems to provide a simple intuitive explanation for the existence and bizarre properties of adversarial examples.</p><p>The purpose of this paper is not to propose new adversarial attacks or defenses, but to change the way we think about adversarial examples, which is usually based on the highly misleading 2D image on the left side of Fig. <ref type="figure" target="#fig_0">1</ref>. In this mental image, the square [0, 1] × [0, 1] contains multiple clusters of training images from two classes (denoted by red and blue, respectively). The purpose of the training is to create a 1D curved decision boundary (denoted by the grey line) which splits the input space into two (not necessarily connected) parts, and the optimization goal is to place each training example on the correct side of the decision boundary and as far as possible from it in order to maximize the confidence level in its provided label, subject to the limited expressive power of the given DNN. In this mental image, adversarial examples are created by moving the given images along the green arrows towards some kind of centroid of the nearest training images with the opposite label (as stated, for example, by Ian Goodfellow in his lecture <ref type="bibr" target="#b3">Goodfellow [2017]</ref> at time 1:11:54).</p><p>For the sake of simplicity, we consider in this paper only 2-class classifiers, and use L 2 norms. Note that in the high dimensional case, the input space is the n-dimensional cube [0, 1] n which is split into two complementary regions by an n − 1 dimensional curved decision boundary, and the standard interpretation of this decision boundary is that it passes roughly halfway along and perpendicularly to the line that connects the centroids of nearby clusters of opposite classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The new mental image of adversarial examples</head><p>The fundamental observation which motivates the new conceptual framework is that all the natural images are located on or near some low dimensional manifold (from <ref type="bibr" target="#b14">Ruderman [1994]</ref> throughout <ref type="bibr" target="#b13">Pope et al. [2021]</ref>). We can approximate this manifold by using a high quality autoencoding DNN <ref type="bibr" target="#b2">(Bourlard and Kamp [1988]</ref>, <ref type="bibr" target="#b20">Wang et al. [2014]</ref>) which first compresses the given image into a k-dimensional latent space, and then decompresses this latent space into a k-dimensional image manifold in the n-dimensional input space. Note that by using a DNN with ReLU activation functions, we can guarantee that the approximated manifold will be a continuous piecewise linear surface within the image cube, with a well defined k-dimensional basis for the local linear subspace at any given point on it. In typical autoencoders k can be one to two order of magnitude smaller than n, and their outputs are visually very similar to their inputs. While we do not fully understand the global structure of this manifold, we expect it to be fairly benign, since image compression is achieved primarily by eliminating some high spacial frequencies and exploiting the non-uniform distribution and similarity between small patches in natural images.</p><p>Next we note that while decision boundaries are n − 1 dimensional objects, their quality is judged only by their performance on natural images within the tiny k-dimensional image manifold. Our main claim is that the existence of nearby adversarial examples is a natural consequence of the mismatch between the tiny k and the large n, where trained networks try to optimally utilize the vast perpendicular subspace in order to make it easier for them to learn the class partition within the image manifold. We visually demonstrate this new mental image for the low dimensional case of k = 2 and n = 3 in the middle part of Fig. <ref type="figure" target="#fig_0">1</ref>, in which the same 2D image manifold floats in the middle of the 3D cube at height z = 0.5. According to the old mental image, we could expect the one dimensional grey decision boundary in the left image to be extended into a vertical 2D wall for any value of 0 ≤ z ≤ 1. However, we claim that the decision boundary which will actually be chosen by the DNN after the training will be the one depicted on the right side of  The new model can also help to understand (at least intuitively) why the training phase of a given network typically converges to the same global optimal placement of the decision boundary, regardless of its random initialization. To demonstrate this point, consider the old model in which you sprinkle at random locations in the two dimensional square a large number of clusters of two classes, as depicted in Figure <ref type="figure" target="#fig_3">3</ref>. Our goal is to pass a decision boundary of bounded complexity which will best separate between the red and blue clusters. There is a large number of ways to do this (e.g., the two choices depicted by grey lines), and most of them will be about equally bad. In particular, any decision to pass on one side or the other of some cluster can make it harder to accommodate other clusters elsewhere along the line. Consequently, there are likely to be many local minima of roughly the same quality. In the dimpled manifold model, there is likely to be a single globally best decision boundary shape since there is no conflict between our ability to go above one cluster and below a different cluster when they do not intersect. 1. The mixture mystery: How can it be that a tiny distance away from any cat image there is also an image of guacamole and vice versa? And if these two classes are intertwined in such a fractal way, how can a neural network correctly distinguish between them? Our answer is that all the real cat and guacamole images reside on the tiny image manifold. but "below" the real cat images there is a whole half space of pseudo-guacamole images which are not natural images of guacamole, and "above" the real guacamole images there is a whole half space of pseudo-cat images, which are not natural images of a cat. The adversarial examples we generate are such pseudo images, and the tiny distance is due to the large perpendicular derivative (whose size is proportional to 1/ ). Note that when we consider multi-class classifiers in an n dimensional input space, there are multiple n − 1 dimensional decision boundaries between pairs of classes, where any two decision boundaries have roughly perpendicular "up" and "dowm" directions. 2. The direction mystery: When we use an adversarial attack to modify a cat into a guacamole, why doesn't the perturbation look green and mushy? In fact, most adversarial perturbations look like a featureless small-magnitude random noise. However, in the new mental image we are moving perpendicularly to the direction of the guacamole images. For example, if a unit vector towards the nearest guacamole image is (1, 0, 0, . . . , 0), then a random unit vector in a perpendicular direction has the form (0, x 2 , x 3 , . . . , x n ), in which each x i is a tiny positive or negative value around O(1/ √ n). Such an adversarial perturbation looks like the random salt and pepper perturbations we see in the standard demonstrations of adversarial examples, rather than a depiction of guacamole. 3. The uniformity mystery: In the old mental image, some cats are closer to the decision boundary than other cats. However, when we use the same type of attack to create the nearest adversarial example for a large number of cat images, the L 2 norms of all the generated perturbations turn out to be very similar. This mystery is again solved by the new mental image since the decision boundary is floating just below and roughly in parallel to any cluster of cats. Let us assume that all these adversarial examples were successfully generated, and that they are all shifted the same distance 2 in the vertical direction with respect to the original image ( to reach the decision boundary, and another to get a high confidence in the opposite decision). By adding the adversarial examples (with visually correct labels) to the original training set, we create a new image manifold whose thickness had been increased to 2 . When we adversarially train a new network with both the original images and their adversarial counterparts, its dimples will have to go an extra distance of about 2 deeper than before. This easily explains why it is harder to adversarially train a network (since we have to use more hammer blows to push the dimples further away from the image manifold), and why we may lose accuracy for regular test images (since the decision boundary has to make sharper turns at the deeper dimples and may miss some small clusters).</p><p>A particularly interesting mystery was pointed out in 2019 by <ref type="bibr" target="#b7">Ilyas et al. [2019]</ref>, who ran the following experiment:</p><p>1. Start with the original CIFAR10 training set of images s 1 , . . . s m , and train a deep neural network N 1 to recognize the ten standard CIFAR10 classes.</p><p>2. Use N 1 to create from each training example s i an adversarial example t i of a consistently modified permuted type (for example, use tiny perturbations to turn all the birds (B) into cats (C), all the cats (C) into deer (D), etc).</p><p>3. Train a fresh deep neural network N 2 (with a possibly different architecture/size and new random initialization) using only the adversarially modified images t 1 , . . . , t m , along with the visually wrong (ie, target) class labels.</p><p>The new network N 2 never saw anything that visually resembles a cat being labelled a cat: all the slightly modified cats were labelled as deer. and all the slightly modified birds were labelled as cats. However, when the new network N 2 is applied to the original test images, it has good accuracy, labelling a large fraction of the cat images as cats.</p><p>The original paper tried to explain this highly surprising result by distinguishing between robust and non-robust features in any given image, where some of them are preserved by the adversarial change and some of them are not. However, it is not clear what makes some of the features more robust than others. Our new model provides a very simple alternative explanation (which does not necessarily contradict the original one) which is summarized in Fig. <ref type="figure" target="#fig_4">4</ref>. To simplify the description, we will use a 2D vertical cut through the input space, and consider only the decision boundary that separates between "cats" (C) and "anything else" (B,D,E, etc). When N 1 is trained on the original training examples, it creates the grey cat'ness decision boundary depicted in part 1 of the figure, in the form of a dimpled line which passes a distance under all the cats and the same distance above all the other classes. 5 Why DNNs are so sensitive and humans so insensitive to adversarial perturbations</p><p>One of the great mysteries of adversarial examples is why they are so successful even on our best trained deep neural networks, while they almost never fool the human visual system (even young babies will recognize a printed image of an adversarial cat as a cat). Our fundamental answer to this question is that through a combination of millions of years of evolution and individual experience during the first few years of life (during which the baby is only exposed to natural images), the human visual system had learned to recognize large parts of the manifold of natural images, which includes regions consisting of faces, animals, landscapes, etc. Once the visual system learns this low dimensional manifold, it can perform a projection of any given image to this manifold during its processing. For example, in all cat-like regions of the image manifold, the human visual system expects to see in a natural image certain shapes of eyes and ears, a certain form of hair texture, etc, and can correct imperfectly shaped, occluded, or cartoonishly-drawn eyes into conceptual cat-like eyes, while ignoring tiny fluctuations in individual pixel values. Similarly, when the human visual perception system encounters a handwritten 7 digit, it does not notice the exact value of each pixel -it only records the fact that it was a 7, and perhaps a few other distinguishing features such as whether it had a middle stroke or not, and whether the angle between the strokes was unusually sharp. It is this projection into the low dimensional image manifold during the recognition process which makes our human visual system immune to adversarial examples, since according to the new model presented here, adversarial cats differ from real cats by moving almost perpendicularly to the image manifold, and thus the projection operation eliminates any such perturbation. Notice that according to the old model of adversarial examples, an adversarial perturbation moves a cat within the image manifold, and thus the projection back to the image manifold will have no effect on the adversarial example.</p><p>This projection of a given image into the real manifold of natural images (which works so well for humans) could be our best defense against adversarial examples for future deep neural networks. However, at the moment we do not know how to define this manifold with sufficient precision. The best we can currently do is to eliminate some of the extra dimensions, which only partially removes the adversarial perturbation, However, we believe that this promising type of defense mechanism (which had already been proposed elsewhere, e.g., in <ref type="bibr" target="#b18">Sun et al. [2019]</ref> and in <ref type="bibr" target="#b11">Meng and Chen [2017]</ref>) should be studied further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental results</head><p>Among the three testable claims we made in Section 2, the first one (that natural images lie on a low dimensional manifold) is essentially common knowledge (see <ref type="bibr" target="#b13">Pope et al. [2021]</ref>), and the second one (that the decision boundary passes very close to this manifold) is equivalent to the statement that almost any natural image has a nearby adversarial example, which is known to be true. In the following sections we experimentally verify the third one (that adversarial directions are almost perpendicular to this manifold) by measuring the off-manifold and on-manifold norms of adversarial perturbations. Given a correctly classified test-set image x and a local manifold M around x, we first found an adversarial example x + d (where d 2 ≤ for some small , and the classifier classifies x and x + d differently). We then projected the adversarial perturbation d onto M , and computed the ratio R(d, M ) between the norm of d and the norm of its projection d on . Note that for a random vector r = (r 1 , ..., r n ) with r i ∼ N (0, 1) of dimension n, its norm is equally distributed among all the dimensions. Therefore, the expected ratio between r and r on for a random vector r (which we denote by R) will be about n k (see details in Appendix B). For k &lt;&lt; n this expected ratio is large, making most vectors almost perpendicular to small manifolds by definition.</p><p>In our experiments we measured the average of the ratio R(d, M ) where d is an adversarial perturbation and M is the natural image manifold, and compared it to R (note that values larger than R indicate actual preference of off-manifold over on-manifold dimensions by the DNN, but any ratio larger than √ 2 indicates an angle larger than 45 degrees). In this paper we use the Euclidean norm in all our attacks; experiments with L ∞ -norm adversarial perturbations can be found in appendix G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments with synthetic images</head><p>The advantage of synthetic experiments is that we know the precise dimension and location of the image manifold. In the following experiments, we linearly partitioned the latent space [0, 1] k for k = 128 into two classes, and chose the decompressed versions of random vectors in it as training examples. The decompression into [0, 1] n for n = 3072 was done in three different ways:</p><p>1. Linear: This experiment used a linear mapping from the latent space to the samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Trigonometric:</head><p>This experiment used a linear mapping followed by trigonometric functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Polynomial:</head><p>This experiment used a linear mapping followed by Chebyshev polynomials.</p><p>We chose these particular mappings to guarantee that the latent cube [0, 1] k will not be mapped to images x which are either outside the cube [0, 1] n or too concentrated at its center. After the training, we sampled additional such x's, computed for each one the local linear manifold M x , and took a single PGD <ref type="bibr" target="#b10">(Madry et al. [2018]</ref>) step towards the opposite class (denoted by d 1</p><p>x ). Each result in Table <ref type="table" target="#tab_1">1</ref> is the average of R(d 1</p><p>x , M x ) for 100 random samples x for three different random initializations of the DNN and the standard deviations of these experiments. As shown in Appendix B, a random vector is expected to have a norm ratio of about n k = 3072 128 ≈ 4.8989. In the Linear experiment, the adversarial step almost always pointed within the manifold (ratio ∼ 1). This was expected, since when everything except the DNN is linear, there is no advantage in going off manifold or creating dimples (when all the training examples are in a linear subspace and there is a linear separator between the classes, all the adversarial directions should be within the subspace and perpendicular to the separator).</p><p>In our two nonlinear experiments, the norms of the adversarial steps were about 33 and 18 times larger than their on-manifold norms, demonstrating that the adversarial directions were almost perfectly perpendicular to the image manifold (way beyond what we could expect from a randomly oriented vector). For some carefully chosen parameters, we even got ratios exceeding 100, but these cases were rare. For more information about these synthetic experiments, see Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Experiments with natural images</head><p>We conducted two types of experiments on each one of the MNIST, CIFAR10 and ImageNet datasets <ref type="bibr" target="#b9">(LeCun et al. [2010]</ref>, <ref type="bibr" target="#b8">Krizhevsky [2009]</ref>, <ref type="bibr" target="#b15">Russakovsky et al. [2015]</ref>). For each data set, we approximated the low-dimensional natural-image manifold M using an autoencoder with a high compression rate, and computed its local linearization around each test-set image x (see Appendix D for further details and dimension specifications). This approximation may in fact overestimate k (and thus underestimate the real norm ratio), yet it demonstrates well the dimpled manifold model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Experiment 1: Projection of an adversarial vector</head><p>For an image x, we started with calculating an adversarial example x + d using a multi-step PGD attack <ref type="bibr" target="#b10">(Madry et al. [2018]</ref>). We projected d onto the locally approximated linear manifold M , measured the aforementioned average ratio R(d, M ) = d P roj M (d) , and compared it to the ratio R which is expected for random vectors.</p><p>The relative effect of the projected vector on the change in classification To measure the effect of the on-manifold projection on the change of classification for the classifier function f (which is the neural network without the SoftMax final layer), an original class i, and an adversarial class t, we first define the total effect of the adversarial vector d as</p><formula xml:id="formula_0">E d = f (x + d) t − f (x + d) i − (f (x ) t − f (x ) i )</formula><p>Finally, we calculate the on-manifold relative effect out of the total adversarial effect as</p><formula xml:id="formula_1">E P roj M (d) = f (x + P roj M (d)) t − f (x + P roj M (d)) i − (f (x ) t − f (x ) i ) E d</formula><p>Table <ref type="table">2</ref>: The norm ratio and on-manifold relative effect for natural images</p><formula xml:id="formula_2">Dataset R R(d, M ) E P roj M (d) MNIST n k = 7 3.68 8% CIFAR10 n k = 4.89 5.59 4% ImageNet n k = 6.48 5.5 2%</formula><p>In Table <ref type="table">2</ref> one can see that for MNIST dataset the norm ratio was smaller than expected. The reason for it may be that the MNIST dataset can be successfully classified using a linear classifier, making it similar to the linear synthetic case in which the ratio was 1. Note that in all three cases, the on-manifold relative effect was tiny, demonstrating that the networks primarily used the off-manifold perturbations rather than the on-manifold perturbations in order to switch their classification.</p><p>In Figures 5, 6  In Figure <ref type="figure" target="#fig_6">5</ref> we can see that the max logit is switching at x+d and almost switching at the off-manifold x + (d − P roj M (d)). However, along the linear line between x and the on-manifold x + P roj M (d), we can see that there is almost no difference in the logits. Note that while the large off-manifold perturbation is hard to interpret, the small on-manifold perturbation is doing exactly what the human visual system would expect, trying to open the two loops in the "8" to get a "2" by blackening the relevant white segments in them.  In Figures <ref type="figure" target="#fig_8">7 and 8</ref> we show the PGD attack and its projections on a CIFAR10 test-set image, demonstrating the same effects. In Figure <ref type="figure" target="#fig_8">7</ref>, the on-manifold projection tries to change the plane visually into a ship -painting the grass in blue to resemble the sea. In Figures 8, the on-manifold perturbation is a bird-shaped and bird-colored perturbation added to the plane in order to "paint" it like a bird while keeping the overall shape and background. Additional examples from these three datasets can be found in appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Experiment 2: Adversarial examples with on/off manifold constraints</head><p>In this experiment, starting from an image x, we used the PGD attack with an extra constraint -before normalizing each step onto the epsilon-step sphere, we projected it on or off the approximated local linear manifold M (see Appendix E for attack details). In this way, we generated on and off manifold adversarial examples, and compared the resultant adversarial distances between the three attacks: unconstrained PGD, on-manifold PGD and off-manifold PGD. In Table <ref type="table" target="#tab_2">3 one</ref>   We show in Figure <ref type="figure" target="#fig_10">9</ref> the on and off adversarial attacks results. One can see that while the off manifold attack creates "cloudy" noise around the figure, the on-manifold attack is carefully creating a circle making the 7 figure look like 9. Indeed, the on-manifold adversarial example looks like the figure <ref type="figure" target="#fig_10">9</ref> to the human eye.</p><p>Once again, we show in Figure <ref type="figure" target="#fig_12">10</ref> that the on-manifold adversarial perturbations are visually meaningful, while the off-manifold perturbations consist of low level noise surrounding the object's general area: At the top of Figure <ref type="figure" target="#fig_12">10</ref> one can see that the network is creating a bird by coloring the plane and shorten its wings.</p><p>Figure <ref type="figure" target="#fig_0">11</ref> presents an additional interesting phenomenon. The on-manifold and the off-manifold attacks are using two different strategies: The on-manifold perturbation tries to control the line's width, whereas the off-manifold perturbation tries to add two black holes at the top and bottom of the 1. Additional examples for this experiment can be found in appendix A.  It is important to emphasize the following points:</p><p>1. To define a 2-class classifier, we used new labels which were the original labels modulo 2 (i.e., all the even labels were mapped to class 1, and all the odd labels were mapped to class 2).</p><p>2. The PGD algorithm tried to flip this classification rather than to flip among the original ten classes.</p><p>3. Following that, we included all the samples regardless of whether they were misclassified after the final epoch).</p><p>A similar two-phase behavior of the training process was also observed by Shwartz-Ziv and Tishby <ref type="bibr">[2017]</ref>, who showed that the DNN first increased its information about the available inputs, and only later increased its information about the desired outputs. In this paper we demonstrated that the new DMM conceptual framework can provide simple and intuitive explanations for most of the mysteries of adversarial examples. The main remaining mystery is why adversarial perturbations are often transferable among classifiers of different types, structures, and sizes. We hope that the common shape of the image manifold in all these cases will help explain this mystery as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix -additional natural image examples</head><p>In this section we provide additional examples from our natural image experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Projected adversarial vector visual examples</head><p>In    Figure <ref type="figure" target="#fig_7">16</ref>: CIFAR10 -a plane is changed into a bird. As in Figure <ref type="figure">8</ref>, the plane in "painted" like a bird.</p><p>Figure <ref type="figure" target="#fig_8">17</ref>: CIFAR10 -a plane is changed into a ship. In the on-manifold projection's row, the wings of the plane are emphasized to turn the plane into a diagonal ship viewed from above.</p><p>Figure <ref type="figure" target="#fig_0">18</ref>: CIFAR10 -a plane is changed into a ship. The on-manifold perturbation is "painting" the plane's runway in blue, to resemble a sea.</p><p>Figure <ref type="figure" target="#fig_10">19</ref>: ImageNet -a white shark is changed into a goldfish. A small orange cluster is created on the left. See further explanation in A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Separate on-manifold and off-manifold PGD attack examples</head><p>For each dataset, for a test-set image x, we run all three attacks -regular PGD, on-manifold PGD, and off-manifold PGD. We look at the different adversarial perturbations for each attack. We note that the on-manifold attack creates a perturbation that tries to visually change the image to belong to the target class, but its adversarial distance is much larger than for the off-manifold attack.  Figure <ref type="figure" target="#fig_2">22</ref>: ImageNet separate attacks -changing white shark into goldfish. A smaller fish-shaped noise is added. See further explanation in A.3.</p><p>Figure <ref type="figure" target="#fig_3">23</ref>: ImageNet separate attacks -changing white shark into goldfish. The small object in the shark's jaws is painted in orange to resemble a goldfish. See further explanation in A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 ImageNet visual examples</head><p>We have shown visual examples from MNIST CIFAR10 and ImageNet dataset which have similar characteristics when projected on and off the natural image manifold. First, the on-manifold component of the perturbation has a much smaller norm since the perturbation is almost perpendicular to the image manifold. Consequently, it has very little adversarial effect on the classification. Finally, we showed that the adversarial noise on the manifold (in both experiments) is trying to change the image in a visually meaningful way, making it look more like a similarly shaped object from the target class.</p><p>One big difference between the ImageNet dataset and the two others is the image dimensions:</p><p>The images in ImageNet are of dimension 224 × 224 × 3 = 150, 528, which is 192 and 49 times bigger than in MNIST and CIFAR10 datasets, respectively. Therefore, in order to change the visual appearance of the image we have to change many more pixels, causing a very large L 2 distance. Even though the adversarial perturbation calculated using the on-manifold PGD attack has a much larger norm than the unrestricted adversarial perturbation, it is still too small to create a visually noticeable difference in such high-dimensional images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Appendix -random-vector projection ratio</head><p>We look at a random vector r = (r 1 , ..., r n ) for r i ∼ N (0, 1) with dimension n. For convenience, we normalize r by dividing it by its L 2 norm.</p><p>Due to symmetry, We can rotate r so that its first k entries represent it's on-manifold projection -</p><formula xml:id="formula_3">r on = P roj N (r) = (r 1 , ..., r k , 0, ..., 0)</formula><p>and its next n − k entries represent its off-manifold projection -</p><formula xml:id="formula_4">r of f = P roj N ⊥ (r) = r − (r 1 , ..., r k , 0, ..., 0) = (0, ..., 0, r k+1 , ..., r n ) Note, 1 = r 2 = r 2 1 + ... + r 2 n , r on 2 = r 2 1 + ... + r 2 k , r of f 2 = r 2 k+1 + ... + r 2 n .</formula><p>When looking at the ratio between r and its projection on the manifold r on we get -</p><formula xml:id="formula_5">r r on = r 2 1 + ... + r 2 n r 2 1 + ... + r 2 k = 1 + r 2 k+1 + ... + r 2 n r 2 1 + ... + r 2 k</formula><p>All r i s are independent and have the same distribution, so all r 2 i s are also i.i.d. Therefore, if we look at the expectation of the squared ratio we get -</p><formula xml:id="formula_6">E[ r 2 r on 2 ] = E[1 + r 2 k+1 + ... + r 2 n r 2 1 + ... + r 2 k ] = 1 + E[r 2 k+1 ] + ... + E[r 2 n ] E[r 2 1 ] + ... + E[r 2 k ] = 1 + (n − k) • E[r 2 i ] k • E[r 2 j ] = 1 + n − k k = n k</formula><p>In conclusion, we can upper bound the expectation of the ratio between the norms -</p><formula xml:id="formula_7">E[ r r on ] ≤ n k C Appendix -synthetic experiments C.1 Network architecture</formula><p>The experiments were conducted using the Fully-Connected network described in Figure <ref type="figure" target="#fig_20">24</ref>, with the following hyperparameters:</p><p>• Optimizer: Adam</p><p>• Learning Rate: 0.001</p><p>• Training Batch-Size: 64</p><p>• Weight-Decay: 1e-6</p><p>• Loss: NLL CNN and FC In this section we use Fully-Connected networks, since in our synthetic constructions nearby pixels are not correlated with each other. Convolutional networks act differently and produce different ratios since they give structures great importance: In fact, a set of input values could achieve a large norm ratio in a specific order, and a low norm ratio in a different order, whereas Fully-Connected networks typically yield the same ratio for any order of the data. With CNN and trigonometric functions in a specific order, we could easily reach norm ratios above 100, indicating an almost perfectly perpendicular direction for the perturbation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Technical details</head><p>The synthetic experiments were conducted with GPU K80 using Pytorch framework <ref type="bibr" target="#b12">(Paszke et al. [2019]</ref>) and Numpy python-library <ref type="bibr" target="#b5">(Harris et al. [2020]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 The dataset generation process</head><p>Let n, k be the dimensions of the input space and the latent space respectively, and let X be the allowed pixels' range [0, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Labels generation:</head><p>In our experiments, we linearly partitioned the latent space into two classes, and used the nonlinearities in our trigonometric and polynomial decompression algorithms to create a complex labeling pattern on the manifold. This choice encouraged the network to learn how to map training examples back to the latent space in which determining the labels was straightforward. In all our experiments, we used the following simple partitioning of the latent space: Given x ∈ X k , we assigned it a label based on the average value of its entries:</p><formula xml:id="formula_8">CLASS(h(x)) = 1 M ean(x) = k j=1 xj k &lt; 0.5 2 otherwise</formula><p>The label of a decompressed sample on the manifold was the same as the label given to its latent space source (note that with very high probability, no point on the manifold would be given two conflicting labels in this way). 2. First layer mapping: The first step in the sample creation process was to linearly expand the k latent space coordinates into the n manifold coordinates. This linear mapping f had to satisfy three requirements:</p><p>(a) To make the mapping nontrivial, each entry in the output f (x) should be influenced by a large number of entries in x. (b) All output entries should be within the range [0, 1] whenever all the input entries are within that range. (c) The images of f should be well distributed in [0, 1] n rather than concentrated in a tiny subset in it (for example, a linear mapping with randomly chosen entries in [−1, 1] would map essentially the whole latent space cube [0, 1] k into a tiny volume around the origin of the n-dimensional space).</p><p>These considerations led us to the following choice of the linear mapping f . For each coordinate 1 ≤ i ≤ n in the output do the following: (a) Choose a random permutation of k indices σ i . (b) For any input x ∈ X k and output y ∈ X n , define y i -the output of the i'th coordinate for y = f (x) as:</p><formula xml:id="formula_9">y i = k j=1 0.5 j • (σ i (x)) j</formula><p>where (σ i (x)) j is the j'th coordinate of (σ i (x)) and k is an integer between 1 and k, normally k would be set to no more than k = 10 (since the impact of a larger number is not noticeable). 3. Second layer mapping: To create more realistic image manifolds that continuously bends within the input space, we used an additional non-linear mapping g : X n −→ X n such that the full mapping from the latent space would be composed of g • f . We used two families of such second layer functions in the experiments: (a) Trigonometric functions: A vital property of g is that it should be sparse in the input space. A good way of doing so is to use trigonometric functions with different numbers of cycles in each pixel, which creates a variety of high dimensional Lissajous patterns within [0, 1] n . Note that we should not use extremely large frequencies to prevent a random-looking mapping and numerical instability, and should not use extremely low frequencies since then the mapping will be almost linear. Define g as the following function: i. Choose randomly n numbers between BOU N D L to BOU N D U (some reasonable numbers) to be the numbers of cycles in each coordinate in the output space, denote this vector of numbers as f requency. ii. Set phase to be a random vector of n numbers between 0 and 2π, it would be the phase for each trigonometric function (one for each of the n coordinates). iii. For each coordinate i between 1 to n, define g i : X −→ X :</p><formula xml:id="formula_10">g i (x) = 0.5 + 0.5 • sin(2π • f requency i • x + phase i )</formula><p>such that f requency i and phase i are the i'th elements in f requency and phase respectively. iv. Finally, given x ∈ X k from the latent space define g : X n −→ X n :</p><formula xml:id="formula_11">g(x) = (g 1 (f (x) 1 ), ..., g n (f (x) n ))</formula><p>where f (x) i is the i'th element in f (x). (b) Polynomial functions: In this synthetic experiment, we use the same first layer mapping and the same class partitioning, but replace the trigonometric functions in the second layer mapping by a set of polynomial functions. A suitable choice of such polynomial functions is the first kind of Chebyshev polynomials since they have the following desirable properties:</p><p>• They are polynomial mappings of arbitrarily high degrees from</p><formula xml:id="formula_12">[−1, 1] to [−1, 1],</formula><p>which could be easily shifted to mappings from [0, 1] to [0, 1]. • Except for the first two degrees, all these polynomial functions have the same lower and upper bounds, −1 and 1, respectively (unlike the second kind of Chebyshev polynomials). The Chebyshev polynomials in this experiment had a maximum degree of 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Adversarial examples as neighbors:</head><p>Since our synthetic manifold is sparse in the input space, one could claim that in these experiments, the adversarial direction is not necessarily described by the Dimpled Manifold Model; instead, it could just point to nearby training examples from the opposite class.</p><p>A simple way to contradict this claim is to find the shortest L 2 distance among pairs of samples in the dataset on the one hand, and compare it to the average L 2 norm of adversarial perturbations (where the adversarial perturbation is defined as the difference between a sample that classified with at least 90% confidence level as one class, and it's adversarial example which is classified as the other class with at least 90% confidence level.</p><p>When the average norm of adversarial perturbations is much smaller than the shortest distance among pairs of samples, it means that the adversarial example cannot be near any other sample from the dataset.</p><p>The synthetic experiment with the sparsest manifold is the one that uses trigonometric functions. We used it to test 300 samples under two different initializations. In order to talk about on-manifold and off-manifold projections, we have to determine the local orientation of the natural image manifold around some natural image x. Though it is commonly believed that such a manifold exists, there is no known way to find its exact representation. However, there are many methods to approximate the natural image manifold, and we decided to use autoencoders to approximate it in the vicinity of each x.</p><p>Autoencoders consists of an encoder and a decoder. Given a natural image x the encoder computes a "code" c representing this image in a much lower dimensional latent space. The decoder then translates c back to an image x , which is as close as possible to the original image x. In this way, the latent space created by the encoder is translated by the decoder into a manifold contained in the image input space. We denote this manifold by M . Notice that the decoder and the encoder are continuous functions (as they are a composition of continuous functions), and thus the vicinity of c is translated into the vicinity of x via the decoder's continuous function.</p><p>We know that the autoencoder is trained such that the auto-encoded images (which are on M by definition), will be as close as possible to the original images that are on the real natural image manifold. As the autoencoder is assumed to generalize well, we can say that the manifold M is close to the natural image manifold around all images from that specific distribution (particularly train-set and test-set images). Therefore, in this paper, we use M as a local approximation of the natural-images manifold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Local linear approximation of the manifold</head><p>For a small epsilon the classifier function in the multi-dimensional epsilon ball around a natural image is almost linear. We use this property when we calculate the projections on and off the manifold.</p><p>Given a test-set image x, we first calculate it's code c using the encoder, and the auto-encoded image x using the decoder. The code c is a much lower dimensional vector, whose dimension is determined by the dimension k of the latent space. In this notation, x, x ∈ [0, 1] n and c ∈ R k for k &lt;&lt; n.</p><p>We want to calculate the linear approximation of the manifold M around x . For each i ∈ {0, ..., k−1} and for some small , we define i to be the k-dimensional vector with on the i-th entry and zero elsewhere. Next, we look at the code</p><formula xml:id="formula_13">c i = c + i</formula><p>We decode c i and denote the resulted image as x + s i . We next use these k s i 's to span the k-dimensional local linear approximation of the manifold around x , denoted by M = span(s 1 , ..., s k−1 ). As x and x are optimally close, we use M as an approximation for the local natural image manifold around x. When looking for the on-manifold projection of a short vector originated in x, we project the vector on the local linear manifold M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Image manifold dimensions</head><p>For MNIST dataset, containing images of n = 28 × 28 = 784 dimensions we trained an autoencoder with k = 16-dimensional latent space <ref type="bibr" target="#b0">(Atzmon et al. [2020]</ref>). For the CIFAR10 and the ImageNet experiments we trained an autoencoder built using the VGG architecture <ref type="bibr" target="#b17">(Simonyan and Zisserman [2014]</ref>). For simplicity, we trained it only on 2 arbitrarily chosen classes. Therefore, we run these experiments only on correctly classified images from these classes (instead of all test-set images). For CIFAR10 (whose images have n = 3 × 32 × 32 = 3072 dimensions) we chose the classes "plane" and "car" (indices 0 and 1), encoded into 128-dimensional latent space. For ImageNet (whose images have n = 3 × 224 × 224 = 150, 528 dimensions) we trained it on "goldfish" and "white sharks" (indices 1 and 2), resulting in 3584-dimensional latent space.</p><p>For the CIFAR10 autoencoder we used an autoencoder created for cars dataset from Stanford by Yang Liu (found here https://github.com/foamliu/Autoencoder) and based on the VGG16 architecture.</p><p>In order to get a low-dimensional latent space, we replaced the max-up-pooling decoder layers (that leak additional information from the encoder side to the decoder side in the form of max-pool indices, thus increasing the effective width of the bottleneck) with regular up-sampling layers with the same sample ratio. We trained it only for the classes of plains (0) and cars (1) using Adam optimizer for 400 epochs, with learning rate of 0.001, batch size of 32 and a latent space of 128 dimensions. The result mean MSE loss for the 2000 test-set images is 0.0001, and the average L 2 distance between these images and their matching autoencoded images is 7.88.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 ImageNet dataset</head><p>For the ImageNet experiments we used a pre-trained ResNet50 network from the pytorch package <ref type="bibr" target="#b6">(He et al. [2016]</ref>).</p><p>For the ImageNet autoencoder we used the same architecture as in CIFAR10. It was trained using Adam optimizer for 325 epochs, with learning rate of 0.0001 and batch size of 32. The auto encoder was trained to use a latent space of dimension 3584. The result mean MSE loss for the 100 test-set images is 0.0001, and the average L 2 distance between these images and its matching autoencoded images is 38.7601.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Appendix -comparison to L ∞ norm</head><p>This paper discusses adversarial attacks using the Euclidean norm, while most of the literature describes and discusses L ∞ norm attacks. The common depiction of an adversarial example consists of some natural image to which we add a tiny randomly-looking perturbation. A typical example is the panda bear (from <ref type="bibr" target="#b4">Goodfellow et al. [2014]</ref>), where the adversarial perturbation added to the image looks like random noise which is multiplied by a very small constant factor (Figure <ref type="figure" target="#fig_21">25</ref>).  When discussing an L 2 adversarial attack, the adversarial perturbation seems much less random. In the paper of <ref type="bibr" target="#b19">Szegedy et al. [2013]</ref> there are adversarial examples created by an L 2 attack, presenting a perturbation that fits the object's general outline and expands in a halo around it (Figure <ref type="figure" target="#fig_23">27</ref>). Why does the L ∞ adversarial perturbation look like random noise? The answer is related to the L ∞ normalization of the gradient-step. When using FGSM attack <ref type="bibr" target="#b4">(Goodfellow et al. [2014]</ref>) or PGD attack <ref type="bibr" target="#b10">(Madry et al. [2018]</ref>) with perturbation size (L ∞ -norm), we first calculate the best adversarial direction using gradient -that is the local optimal noise we should add to the natural image in order to change its classification. The gradients in both attacks are calculated for each pixel of the natural image. When performing L ∞ normalization, we take any pixel change -tiny or large -to be -sized. Note, the L ∞ -normalized gradient is no longer the optimal direction, as normalization changes the vector's original direction.</p><p>The L 2 -normalizations performed in the L 2 versions of the attacks, of course, don't change the gradient direction. Therefore, we can understand that the optimal perturbation applies more significant changes to pixels that play more prominent roles in the classification. Consequently, most of the changes happen within the figure, especially around its edges, which makes the perturbation have the general appearance of the original image.</p><p>L ∞ attacks behave differently. Due to L ∞ normalization, the background pixel's tiny noise and the object-surrounding pixel's larger noise are both becoming -sized noise. The L ∞ perturbations thus look like random noise with almost uniform amplitude, which appears both inside and outside the classified object.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The old mental image on the left, the green image manifold in the center, and the newly proposed grey decision boundary on the right, with red Class 1 clusters and blue class 2 clusters.</figDesc><graphic url="image-1.png" coords="2,108.00,72.00,396.02,129.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig. 1, where it clings to the image manifold, except for shallow dimples that gently undulate below the training examples of class 1, and above the training examples of class 2. We call this conceptual framework the Dimpled Manifold Model (DMM), and note that it makes three testable claims about the kind of decision boundaries created by trained DNN's: 2. DNN decision boundaries pass very close to this image manifold 3. The gradient of the classification's confidence level has a large norm and points roughly perpendicularly to the image manifold To support this model, we will do three things: Explain intuitively why the training process is likely to prefer such a decision boundary over other structures, show that it provides simple explanations for the mysteries of adversarial examples, and finally verify it by direct experimental measurements. 3 Why DNN's are likely to create dimpled manifolds as decision boundaries A randomly initialized DNN is likely to create a randomly oriented initial decision boundary. Let us consider once again our new mental image of a flat horizontal 2D image manifold at middle height in a three dimensional input cube, and let us assume that in a certain region of this manifold (which contains training images of both classes) the initial decision boundary passes well above the image manifold.In the first epoch, we try to push the decision boundary "downwards" in the gradient's direction (which is roughly perpendicular to the current decision boundary) at any 2D point which is labelled as class 1, while we do nothing at points labelled as class 2 (since they are already on the right side of the boundary). Similarly, in regions where the initial decision boundary passes well below the image manifold, we will want to either leave the decision boundary unchanged or move it "upwards" (see Figure2). Thinking about the decision boundary as a thin sheet of pliable metal, we bend it over multiple epochs by applying tiny hammer blows to regions we want to move up or down. Eventually, we expect this process to yield a metal sheet which conforms to the shape of the image manifold, except in the vicinity of training examples of class 1 (where we create downward pointing dimples) or class 2 (where we create upward pointing dimples). Note that by developing a large derivative in the vertical direction, the network can bend the decision boundary more gently, making it easier to gain accuracy with a simpler decision boundary that uses shallower dimples which pass on the correct sides of neighboring training examples of opposite classes. Such large gradients are made possible by the fact that DNNs have a huge number of inputs in their linear combinations, which may be one of the secrets for the exceptional success of this computational model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Bending the initial decision boundary to accommodate misclassified training examples moves the boundary closer to the image manifold</figDesc><graphic url="image-2.png" coords="3,207.00,434.19,198.00,71.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: When there are many randomly located clusters, separating them with an n − 1 dimensional dimpled manifold is much easier than finding a simple k − 1 dimensional decision boundary within the manifold</figDesc><graphic url="image-3.png" coords="4,207.00,72.00,197.99,192.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: A schematic side view of green image manifolds and grey decision boundaries</figDesc><graphic url="image-4.png" coords="6,108.00,72.00,396.00,190.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>, 7 and 8, for a test-set image x we show the original adversarial example x + d (first row), the projected on-manifold example x + P roj M (d) (second row), and the projected off-manifold example x + (d − P roj M (d)) (last row). The columns in the figure from left to right are: the natural image x, the perturbed image, the perturbation itself (maximally amplifying its entries to the full range of [0, 1] to make it visually clearer), and the evolution of the logit's values for the various classes along the linear line x + αd for α ∈ [0, 1 100 , 2 100 , ..., 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: MNIST -changing 8 to 2</figDesc><graphic url="image-5.png" coords="9,108.00,129.71,396.00,201.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: MNIST -changing 2 to 3In Figure6we see similar behaviour. The on-manifold projection has a minor effect on the logits while trying to delete (blacken) the left part of the circle in the 2 figure in order to open it. It also tries to increase the distance between the two bottom horizontal lines left from the circle by adding a white line outside and a black line inside and tries to eliminate the short protrusion on the right side of the 2.</figDesc><graphic url="image-6.png" coords="9,108.00,438.96,396.00,197.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: CIFAR10 -changing plane to ship</figDesc><graphic url="image-7.png" coords="10,108.00,72.00,395.99,198.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>can see a big difference in the adversarial distances. In all datasets, on-manifold adversarial examples are much further away from the original image x while the off-manifold adversarial examples are much closer (up to 6 times closer).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: MNIST separate attacks -changing 7 into 9. The on-manifold perturbation is adding a circle at the top of the 7.</figDesc><graphic url="image-9.png" coords="11,108.00,212.11,395.99,212.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>The clinging and dimpling phases of DNN training Finally, we studied the evolution of the decision boundary during the training of the DNN. According to our model, the training consists of two separate phases: first, the decision boundary has to cling to the image manifold, and then it can grow shallow dimples which will have little effect on this average distance. In order to demonstrate it, we measure the average distance between the training images and the decision boundary after each training epoch. In Figure12, we describe the results of training a DNN on CIFAR10 data, where the horizontal axis denotes the epoch number, and the vertical axis represents the average distance of 100 randomly chosen samples from the decision boundary at the end of each epoch (slightly overestimated by following the adversarial directions until the classification is flipped).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: CIFAR10 -on and off manifold attacks changing a plane into a bird</figDesc><graphic url="image-10.png" coords="12,108.00,72.00,396.00,194.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: During training, the decision boundary is getting closer to the manifold during the initial epochs, and then staying at roughly the same distance during later epochs</figDesc><graphic url="image-12.png" coords="13,192.61,72.00,226.77,174.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>the following figures, we show the on-manifold and off-manifold projections of adversarial perturbations d created by a PGD attack for typical test-set images x. In all these examples, the image created by the off-manifold projection x + (d − P roj M (d)) is almost adversarial (i.e., it almost flips the class associated with the max logit), while the on-manifold projection x + P roj M (d) barely changes the output logits (i.e., it has almost no adversarial effect). In addition, in all the examples, one can see that the on-manifold projection of the adversarial perturbation is visually meaningful, trying to modify x into an image with the characteristic features of the target class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: MNIST -The figure 9 changed into 7. The on manifold projection is trying to delete (blacken) the bottom part of the ring in the 9 in order to create a 7-like figure.</figDesc><graphic url="image-13.png" coords="15,108.00,229.08,395.99,204.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: MNIST -The figure 6 changed into 0. The on-manifold projection is trying to close the 6 figure's circle to create a 0 figure.</figDesc><graphic url="image-14.png" coords="15,108.00,482.74,396.00,199.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: MNIST -The figure 8 changed into 1. The on-manifold projection is trying to fill-in with white the top ring of the 8 figure to create a solid vertical line, which is the characteristic feature of a 1 figure.</figDesc><graphic url="image-15.png" coords="16,108.00,113.87,395.99,205.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: MNIST separate attacks -changing 7 into 1. The on-manifold perturbation is shortening the upper horizontal line.</figDesc><graphic url="image-20.png" coords="19,108.00,148.78,396.00,208.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 21 :</head><label>21</label><figDesc>Figure 21: CIFAR separate attacks -changing plane into bird. The on-manifold perturbation is changing the shape of the wings by adding blue to the front part of it. The plane's head is turning into a bird's head by coloring it and creating a narrow neck.</figDesc><graphic url="image-21.png" coords="19,108.00,406.29,395.99,194.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 24 :</head><label>24</label><figDesc>Figure 24: Structure of the network. Input size: 3072, Number of classes: 2.</figDesc><graphic url="image-24.png" coords="22,192.61,238.71,226.78,388.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 25 :</head><label>25</label><figDesc>Figure 25: Goodfellow et al. [2014] L ∞ adversarial exampleRunning the multi-step PGD attack<ref type="bibr" target="#b10">(Madry et al. [2018]</ref>) on an arbitrary test-set image from the ImageNet dataset also generates a similar randomly-looking perturbation (Figure26).</figDesc><graphic url="image-25.png" coords="27,192.61,416.72,226.77,92.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 26 :</head><label>26</label><figDesc>Figure 26: multi-step PGD L ∞ adversarial example</figDesc><graphic url="image-26.png" coords="27,192.61,575.26,226.77,81.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 27 :</head><label>27</label><figDesc>Figure 27: Szegedy et al. [2013] L 2 adversarial example</figDesc><graphic url="image-27.png" coords="28,192.61,72.00,226.78,77.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-17.png" coords="17,108.00,116.70,395.99,202.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-18.png" coords="17,108.00,447.80,396.00,196.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-19.png" coords="18,108.00,275.11,395.99,210.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-22.png" coords="20,108.00,104.83,396.00,224.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-23.png" coords="20,108.00,434.04,396.00,222.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>4. The vanishing gap mystery: According to the old mental image, when we place a decision boundary halfway between a cluster of cat images and a cluster of guacamole images, then the adversarial examples created from one cluster cross the boundary towards the other cluster. When we adversarially train a new network with both the original and the adversarial images, there is no gap left between the expanded cat and guacamole clusters, so where can we pass the new decision boundary? This mystery is again explained in the new mental image, since the newly created adversarial examples are moving in opposite perpendicular directions, and this shearing movement does not close the original gap between the clusters.</figDesc><table><row><cell>5. The accuracy/robustness tradeoff mystery: Note that in the new model, ease of training</cell></row><row><cell>and the existence of nearby adversarial examples are two sides of the same coin: When</cell></row><row><cell>we train a network, we keep the images stationary and move the decision boundary around</cell></row><row><cell>them by creating dimples; when we create adversarial examples, we keep the decision</cell></row><row><cell>boundary stationary and move the images to its other side. By allowing a large perpendicular</cell></row><row><cell>derivative we make the training easier since we do not have to sharply bend the decision</cell></row><row><cell>boundary around the training examples. However, such a large derivative also creates very</cell></row><row><cell>close adversarial examples. Any attempt to robustify a network by limiting all its directional</cell></row><row><cell>derivatives will make the network harder to train and thus less accurate.</cell></row></table><note>The new conceptual model enables us to analyze the effect of adversarial training of DNNs from a new perspective. During an adversarial training, we take each training image and create from it a nearby adversarial example.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Synthetic results</figDesc><table><row><cell>Experiment</cell><cell cols="3">Network accuracy R(d 1 x , M x ) Standard deviation</cell></row><row><cell>Linear</cell><cell>97%</cell><cell>1.0001</cell><cell>0.0001</cell></row><row><cell>Trigonometric</cell><cell>94%</cell><cell>32.77</cell><cell>1.18</cell></row><row><cell>Polynomial</cell><cell>93%</cell><cell>18.11</cell><cell>0.66</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The mean distance of an adversarial example</figDesc><table><row><cell>Dataset</cell><cell cols="3">no-constraint PGD On-manifold PGD Off-manifold PGD</cell></row><row><cell>MNIST</cell><cell>1.01</cell><cell>3.68 (364%)</cell><cell>1.05 (103%)</cell></row><row><cell cols="2">CIFAR10 0.3</cell><cell>1.87 (623%)</cell><cell>0.305 (101%)</cell></row><row><cell cols="2">ImageNet 0.149</cell><cell>0.849 (570%)</cell><cell>0.152 (102%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The results of this experiment are summarized in 4, and clearly demonstrate that adversarial examples are much closer than training examples with the opposite label. Network accuracy Shortest distance Adversarial distance</figDesc><table><row><cell>Initialization 1</cell><cell>90%</cell><cell>25.39</cell><cell>0.55</cell></row><row><cell>Initialization 2</cell><cell>92%</cell><cell>25.48</cell><cell>0.49</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Michal Irani and Yaron Lipman for many interesting discussions and for contributing computational resources and models.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Appendix -separate on and off manifold PGD attack details</head><p>The multi-step PGD attack <ref type="bibr" target="#b10">Madry et al. [2018]</ref> as implemented in the Advertorch python package, consists of 4 main parts in each step:</p><p>1. Calculate the network's gradient with respect to the input image x.</p><p>2. Normalize the gradient to be -step sized vector with respect to the chosen norm.</p><p>3. Add the current calculated step to the accumulative adversarial vector.</p><p>4. Clip the total adversarial vector to be at most sized vector (and each pixel to be ∈ [0, 1]).</p><p>For a multi-step PGD attack on natural images we used the L 2 norm, up to 50 steps, the margin loss function and epsilon of 2, 0.5, and 2 for MNIST, CIFAR10, and ImageNet respectively. The step size is 0.04 for MNIST and CIFAR10 and 0.8 for ImageNet. We run a greedy version of the attack -we stop as soon as we get to an adversarial example (change in classification).</p><p>In order to get separate on and off manifold adversarial examples, we project each gradient step respectively on or off the manifold before normalization. Therefore, the new step is:</p><p>1. Calculate the network's gradient with respect to the input image x.</p><p>2. Project the gradient step vector on or off the manifold.</p><p>3. Normalize the gradient to be -step sized vector with respect to the chosen norm.</p><p>4. Add the current calculated step to the accumulative adversarial vector.</p><p>5. Clip the total adversarial vector to be at most sized vector (and each pixel to be ∈ [0, 1]).</p><p>For the multi-step on or off PGD attack on natural images we used the L 2 norm, up to 1000 steps, the margin loss function and epsilon of 10, 3, and 5 for MNIST, CIFAR10, and ImageNet respectively. The step size is 0.02, 0.12 and 0.01 for MNIST, CIFAR10, and ImageNet respectively for the regular and the off-manifold attacks. The step size is multiplied by 10 for the on-manifold attack, in order to reach further. Here also, we run a greedy version of the attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Appendix -image auto-encoders and image classifiers details F.1 MNIST dataset</head><p>We used two hidden layers classification network:</p><p>1. Linear layer (in=784, out=256), ReLU layer, Dropout layer w.p. 0.2 (drop out entries with 0.2 probability) 2. Linear layer (in=256, out=256), ReLU layer, Dropout layer w.p. 0.2 (drop out entries with 0.2 probability)</p><p>3. Linear output layer (in=256, out=10)</p><p>The NN was trained using SGD optimizer with learning rate of 0.01, weight decay of 0.0001, batch size of 200 and for 40 epochs, reaching 98% success in classification.</p><p>The MNIST encoder and decoder are described in <ref type="bibr" target="#b0">Atzmon et al. [2020]</ref>. They were trained using Adam optimizer for 1000 epochs, with learning rate of 0.001 and batch size of 144. The chosen latent space dimension is 16. The result mean MSE loss for the 10, 000 test-set images is 0.00003, and the average L 2 distance between the MNIST images and its matching autoencoded images is 2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 CIFAR10 dataset</head><p>For the CIFAR10 experiments we used a pre-trained 7 layered convolutional classifier network (which can be found at https://github.com/aaron-xichen/pytorch-playground) achieving 93% success rate in its classification.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gropp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09289</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Isometric autoencoders. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evasion attacks against machine learning at test time</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maiorca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Šrndić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European conference on machine learning and knowledge discovery in databases</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="387" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Auto-association by multilayer perceptrons and singular value decomposition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="291" to="294" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Stanford university school of engineering -lecture 16 | adversarial examples and adversarial training</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<ptr target="https://www.youtube.com/watch?v=CIfsB_EYsVI&amp;t=3s" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Array programming with NumPy</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Millman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gommers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Picus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Van Kerkwijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Haldane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Del Río</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gérard-Marchant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sheppard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Weckesser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gohlke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Oliphant</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-020-2649-2</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">585</biblScope>
			<biblScope unit="page" from="357" to="362" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02175</idno>
		<title level="m">Adversarial examples are not bugs, they are features</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Mnist handwritten digit database. ATT Labs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Magnet: a two-pronged defense against adversarial examples</title>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The intrinsic dimension of images and its impact on learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abdelkader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goldblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The statistics of natural images</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Ruderman</surname></persName>
		</author>
		<idno type="DOI">10.1088/0954-898X_5_4_006</idno>
		<ptr target="https://doi.org/10.1088/0954-898X_5_4_006" />
	</analytic>
	<monogr>
		<title level="j">Network: Computation in Neural Systems</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="517" to="548" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Opening the black box of deep neural networks via information</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shwartz-Ziv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00810</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adversarial defense by stratified convolutional sparse coding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>-H. Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<title level="m">Intriguing properties of neural networks</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generalized autoencoder: A neural network framework for dimensionality reduction</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW.2014.79</idno>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="496" to="503" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
