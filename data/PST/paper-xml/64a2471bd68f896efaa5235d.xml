<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SUBGRAPH STATIONARY HARDWARE-SOFTWARE INFERENCE CO-DESIGN</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Payman</forename><surname>Behnam</surname></persName>
							<email>&lt;payman.behnam@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>Georgia</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>Georgia</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianming</forename><surname>Tong</surname></persName>
							<email>jianming.tong@gatech.edu&gt;.</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>Georgia</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>Georgia</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alind</forename><surname>Khare</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>Georgia</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yangyu</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>Georgia</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yue</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>Georgia</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pranav</forename><surname>Gadikar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>Georgia</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Abhimanyu</forename><forename type="middle">Rajeshkumar</forename><surname>Bambhaniya</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>Georgia</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>Georgia</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexey</forename><surname>Tumanov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>Georgia</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SUBGRAPH STATIONARY HARDWARE-SOFTWARE INFERENCE CO-DESIGN</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A growing number of applications depend on Machine Learning (ML) functionality and benefits from both higher quality ML predictions and better timeliness (latency) at the same time. A growing body of research in computer architecture, ML, and systems software literature focuses on reaching better latency/accuracy tradeoffs for ML models. Efforts include compression, quantization, pruning, early-exit models, mixed DNN precision, as well as ML inference accelerator designs that minimize latency and energy, while preserving delivered accuracy. All of them, however, yield improvements for a single static point in the latency/accuracy tradeoff space. We make a case for applications that operate in dynamically changing deployment scenarios, where no single static point is optimal. We draw on a recently proposed weight-shared SuperNet mechanism to enable serving a stream of queries that uses (activates) different SubNets within this weight-shared construct. This creates an opportunity to exploit the inherent temporal locality with our proposed SubGraph Stationary (SGS) optimization. We take a hardware-software co-design approach with a real implementation of SGS in SushiAccel and the implementation of a software scheduler SushiSched controlling which SubNets to serve and what to cache in real-time. Combined, they are vertically integrated into SUSHI-an inference serving stack. For the stream of queries SUSHI yields up to 25% improvement in latency, 0.98% increase in served accuracy. SUSHI can achieve up to 78.7% off-chip energy savings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The number of applications leveraging Machine Learning (ML) functionality continues to grow, as ML is successfully applied beyond image classification <ref type="bibr" target="#b31">(Ovtcharov et al., 2015)</ref>, object detection/recognition <ref type="bibr">(Chen et al., 2017a;</ref><ref type="bibr" target="#b0">Ali et al., 2018)</ref>, sentiment analysis <ref type="bibr" target="#b22">(Jiang et al., 2020)</ref>, and next word prediction <ref type="bibr" target="#b41">(Sundermeyer et al., 2012)</ref>. These applications are also increasingly latency sensitive. Their interactive experience depends on what fraction of prediction tasks are satisfied within the application-specified latency budget (typically in the 10-100 ms interactive latency range). Examples of such applications include self-driving cars <ref type="bibr" target="#b17">(Gog et al., 2022)</ref>, specifically the on-board software responsible for multi-modal sensory data processing, street sign detection <ref type="bibr" target="#b42">(Tabernik &amp; Sko?aj, 2019)</ref>, pedestrian detection <ref type="bibr" target="#b28">(Liu et al., 2019)</ref>, vehicle trajectory tracking <ref type="bibr" target="#b13">(Deo &amp; Trivedi, 2018)</ref>, lane tracking <ref type="bibr" target="#b12">(Datta et al., 2020)</ref>, and Intensive Care Unit stability score prediction <ref type="bibr" target="#b19">(Hong et al., 2020)</ref>. These applications require the ability to serve trained ML models in a way that maximizes the fraction of queries completed within the application specified latency budget-defined as latency Service Level Objective (SLO) attainment. A unifying characteristic for this class of applications is that they simultaneously care about the quality (accuracy) and timeliness (latency) of ML inference served.</p><p>There has been a body of work successfully improving achievable latency/accuracy tradeoffs for specific Deep Learning models. Examples include multiple forms of quantization <ref type="bibr" target="#b1">(Bai et al., 2018;</ref><ref type="bibr" target="#b50">Zhang et al., 2018;</ref><ref type="bibr" target="#b32">Pouransari et al., 2020;</ref><ref type="bibr" target="#b15">Fang et al., 2020)</ref>, mixed DNN precision <ref type="bibr" target="#b0">(Abdelaziz et al., 2021)</ref>, compression <ref type="bibr" target="#b21">(Iandola et al., 2016)</ref>, pruning <ref type="bibr" target="#b29">(Liu et al., 2018)</ref>, latency-aware neural architecture search <ref type="bibr" target="#b2">(Cai et al., 2018;</ref><ref type="bibr" target="#b14">Eriksson et al., 2021)</ref>, just to name a few. However, fundamentally, all of these techniques optimize for a single static point in the latency/accuracy tradeoff space. Indeed, for a given deployment device, the outcome is typically a single static model that has a specific (latency, accuracy) tuple associated with it. We claim this is no longer sufficient.</p><p>We observe that the applications with acute latency/ accuracy sensitivity typically operate in dynamically variable deployment conditions. These include variable query traffic patterns (e.g., variable number of patients triaged in the ICU or ER), on-device battery power level (e.g. bed-side compute or battery-powered edge device), and query complexity (e.g., autonomous vehicle (AV) navigation of sparse suburban vs dense urban terrain).</p><p>Under such variable deployment conditions, a choice of any single static model from the latency/accuracy tradeoff space will be suboptimal. Indeed, a higher accuracy model may result in dropped queries during periods of transient overloads. The lower accuracy model may yield suboptimal prediction quality under low load-both unnecessarily under-performing. Inherently, the ideal solution would include dynamically picking a "best-fit" model from the latency/accuracy tradeoff space. For a specific latency constraint that varies over time, a just-in-time choice of the highest accuracy model satisfying this constraint is preferred. Thus, the ability to switch (or navigate) between points in the latency/accuracy tradeoff space in real-time is intuitively required for such applications.</p><p>We identify one such mechanism that enables this -weightshared SuperNets <ref type="bibr">(Cai et al., 2019) ( ?2.1)</ref>. This neural network construct consists of multiple convolutional neural networks (CNNs) sharing common model parameters. It simultaneously encapsulates "deep and thin" models as well as "wide and shallow" within the same structure without weight duplication. These SuperNets can be used to activate different SubNets without explicitly extracting them into different independently stored models. This is highly efficient from the systems perspective, as it obviates the need to store these model variants separately (saving memory cost), and enables rapidly switching SubNets that are "activated" to serve different incoming queries.</p><p>On the hardware end, the need for real-time inference has led to a plethora of ML accelerators. A key optimization technique (e.g., "dataflow" <ref type="bibr" target="#b6">(Chen et al., 2016)</ref>) leveraged by most accelerators involves reusing activations and/or weights across multiple computations, leading to architectures that can be classified as weight stationary, output stationary, input stationary, row stationary, and hybrid variations of these <ref type="bibr" target="#b6">(Chen et al., 2016)</ref>. These dataflows rely on neural network layers, specifically 2D convolutions, to be compute-bound. One challenge of serving SubNets with diverse shapes, however, as we identify, is the memory-bound nature of some of the SubNets (smaller FLOPS/Byte).</p><p>To address this challenge, we make a key observation that the weight-shared SuperNet mechanism inherently results in queries activating commonly shared SubGraphs within the same SuperNets structure 1 . Furthermore, we note a significant amount of temporal locality in the weights of the SuperNets re-used across queries. We identify this as an opportunity for a new kind of data reuse, which we 1 We define SubGraph as a subgraph consisting of any subset of weights from the SuperNets connected together into a graph name SubGraph Stationary (SGS) optimizationa technique we haven't seen used or proposed by any existing accelerator. We realize the benefits of SGS by implementing hardware caching support for weight reuse at the granularity of neural network SubGraphs.</p><p>In addition to SGS-aware hardware implementation, we co-design an SGS-aware query scheduler that decides (a) which SubNets to activate for each query and (b) which Sub-Graphs to cache. We propose an algorithmic approach to make these control decisions based on (a) a query's specified accuracy constraint and (b) the current state of the accelerator (which we abstract). We demonstrate that these control decisions benefit from hardware state awareness, as baseline state-unaware caching leaves room for improvement. Finally, we propose an abstraction that enables the query scheduling policy to generalize, while remaining accelerator state-aware. The abstraction is captured by a black-box table (Fig. <ref type="figure">4</ref>) that exposes the latency of activating a SubNet i as a function of a currently cached SubGraph j. We instantiate the concept of SubGraph Stationary (SGS) cross-query optimization in our vertically integrated inference serving stack, SUSHI, which includes (a) SushiAccel-a real FPGA implementation of hardware support for SGS-aware weight-shared SuperNet inference, and (b) SushiSched to make real-time control decisions on a stream of queries executed on SushiAccel, sequentially deciding for each query SubNet i to activate and (periodically) SubGraph j to cache on the accelerator.</p><p>SushiAccel and SushiSched combined in SUSHI enable agile navigation of the latency/accuracy tradeoff space, reaching better latency/accuracy tradeoffs by leveraging the key property of "cross query" temporal locality inherent to weight-shared SuperNets with what we believe to be the first hardware-software co-design for weight-shared inference.</p><p>The key contributions of this paper can be summarized as follows:</p><p>? a concept of SubGraph Stationary (SGS) approach for hardware acceleration of DNN inference on weightshared SuperNets.  Combined, SUSHI is able to achieve up to 25% query serving latency improvement with 0.98% accuracy improvement. SUSHI can also save a significant amount of off-chip energy (78.7%) in simulation with realistic board configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND MOTIVATION</head><p>We start with a background on weight-shared neural networks in ?2.1. Then we motivate and expose the opportunity for hardware support of weight-shared supernet inference ( ?2.2). The need for hardware-software co-design follows from challenges in ?2.3. The hardware-software abstraction in ?2.4 is introduced for generality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Weight-Shared Deep Neural Networks (WS-DNNs)</head><p>Recent advances in deep learning propose weight-shared deep neural networks <ref type="bibr" target="#b3">(Cai et al., 2019;</ref><ref type="bibr" target="#b39">Sahni et al., 2021;</ref><ref type="bibr" target="#b48">Yu et al., 2020)</ref> that propose SuperNet structures can be used to enable inference on Deep Neural Networks (DNNs) across a diverse set of deployment scenarios (both dynamic and static). Weight-shared DNNs (WS-DNN) induce a rich trade-off between accuracy and latency (Fig. <ref type="figure" target="#fig_0">1b</ref>). The inference in WS-DNN fundamentally changes the traditional view of optimizing inference latency, which was focused on a single forward pass query. Instead, WS-DNN's inference makes it possible to satisfy the latency-accuracy requirements for a stream of queries with each query potentially requesting a different point in the trade-off space. This positions WS-DNNs as a salient candidate for a variety of applications <ref type="bibr" target="#b18">(Halpern et al., 2019;</ref><ref type="bibr" target="#b20">Hsieh et al., 2018;</ref><ref type="bibr" target="#b34">Reddi et al., 2020)</ref> and inference-serving systems <ref type="bibr">(Romero et al., 2021a)</ref> that benefit from navigating latency/accuracy trade-off. The key property of these networks is that different DNNs (SubNet), which may differ in several elastic dimensions, including depth and width, partially share their weights as part of a single large DNN (SuperNet). As a result, the SuperNet contains all other SubNets within it (Fig. <ref type="figure" target="#fig_0">1a</ref>). These SubNets can be directly used to render predictions without any further re-training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Need for Hardware Support for WS-DNN Inference</head><p>The goal of hardware acceleration for ML inference is to serve a query with minimal latency and maximal accuracy. This goal becomes even more pronounced for WS-DNN inference, where each query may be served with different latency/accuracy requirements (Fig. <ref type="figure" target="#fig_0">1b</ref>) <ref type="bibr" target="#b3">(Cai et al., 2019;</ref><ref type="bibr" target="#b39">Sahni et al., 2021)</ref>.</p><p>Achieving this goal is challenging due to memoryboundedness of some of the convolutional layers <ref type="bibr" target="#b26">(Kao et al., 2022;</ref><ref type="bibr" target="#b40">Siu et al., 2018)</ref>. This is especially true for the more recent smaller models that have lower arithmetic intensity (FLOPS/Byte) and when they are deployed on bandwidthconstrained embedded boards <ref type="bibr" target="#b45">(Wang et al., 2019;</ref><ref type="bibr" target="#b46">Wei et al., 2019;</ref><ref type="bibr" target="#b6">Chen et al., 2016;</ref><ref type="bibr" target="#b24">Jokic et al., 2020;</ref><ref type="bibr" target="#b40">Siu et al., 2018;</ref><ref type="bibr" target="#b4">Chen et al., 2022)</ref>.</p><p>We quantify this in Fig. <ref type="figure">2</ref>, where we observe that a large fraction of convolution layers running on a canonical edge accelerator are memory-bound<ref type="foot" target="#foot_0">2</ref> . This is problematic, since a significant portion of end-toend inference latency and energy consumption comes from memory-bound layers, given the high latency and energy cost of data movement from memory to the on-chip storage <ref type="bibr" target="#b6">(Chen et al., 2016;</ref><ref type="bibr" target="#b49">Yuan et al., 2021)</ref>.</p><p>Hence, for the same amount of FLOPS it is very important to convert memory-bound layers to compute-bound in order to reduce end-to-end inference latency and energy consumption.</p><p>To do so, we leverage our key insight that WS-DNN inference on a stream of queries exhibits temporal locality.</p><p>As different queries use different SubNets, many of them reuse the same weights shared among those SubNets, by design. We employ this insight to help convert memorybound layers to be more compute-bound. Conceptually, this can be accomplished by reusing the shared weights used by previous queries for the next query in a stream, knowing that they all activate SubNets within the same shared SuperNet structure. This creates an opportunity for reuse across queries, in sharp contrast to techniques commonly explored and exploited in the computer architecture community for a single query for intra-model optimizations, such as weight-stationary, row-stationary, input-stationary, and output-stationary <ref type="bibr">(Chen et al., 2017b;</ref><ref type="bibr">2016;</ref><ref type="bibr" target="#b16">Fleischer et al., 2018;</ref><ref type="bibr" target="#b43">Venkatesan et al., 2019)</ref>.</p><p>We call this novel reuse as SubGraph Reuse, as common shared weights form a SubGraph (e.g., created as the intersection of computational graphs of any two served SubNets).</p><p>Note that in this paper we distinguish between SubGraphs and SubNets. SubNet is a subset of a SuperNet that can be used for forward-pass inference to serve a query, while a SubGraph is a subset of SubNet. Note that any SubNet is a SubGraph, but not vice versa.</p><p>A natural way to leverage SubGraph Reuse is to have a dedicated cache in the hardware accelerator. However, it comes with several challenges that we discuss in ?2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Design Challenges in WS-DNN Inference Specialized Hardware</head><p>The proposed specialized hardware for WS-DNN-inference exploits the temporal locality and enables SubGraph Reuse. However, assigning a dedicated on-chip buffer comes with both software and hardware challenges.</p><p>Hardware Challenges: Due to the resource-restricted nature of many deployment devices, the cache size may be too small to cache entire SubNets. Thus, the hardware must operate at a finer caching granularity of arbitrary SubGraphs instead. Deciding the size of the dedicated on-chip buffer is non-trivial.</p><p>Small buffer size leads to marginalizing the ability to exploit temporal locality. Larger dedicated on-chip buffer limits compute area as well as other on-chip buffer sizes that are leveraged for weight/row/input/output stationary optimizations.</p><p>Furthermore, the SubGraph Stationary depends on the compute/memory boundness of the convolution workload, which is further related to the off-chip bandwidth and throughput of the hardware. Therefore, the variation of the bandwidth and throughput will also affect the best cache size, which introduces more factors for consideration in the trade-off space.</p><p>Software Challenges: We argue that the latency of served SubNets depends on the SubGraph cached in the on-chip buffer. Fig. <ref type="figure">3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Hardware-Agnostic Software Scheduling</head><p>One final goal is to achieve generalizability for the software scheduler while retaining accelerator state awareness. The scheduler policy design could then generalize to any hardware that is able to support WS-DNN inference. Hence, there is a need to decouple the scheduler from the hardware, i.e., the change in the hardware should not require any changes in the scheduler policy code. We propose an abstraction between the software scheduler and the hardware accelerator that exposes latencies of serving a set of SubNets over a set of cached SubGraphs. We show that this gives the policy sufficient information about the hardware state in an accelerator-agnostic fashion. We discuss the mechanism of achieving this while managing the spatial complexity of such a lookup table in ?3. We instantiate this mechanism in SushiSched, which we can now develop and improve upon independently on any hardware accelerator. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Latency</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SYSTEM DESIGN &amp; ARCHITECTURE</head><p>SUSHI serves a stream of queries with different latency/accuracy requirements. It consists of three major components -scheduler (SushiSched), abstraction (SushiAbs), and accelerator (SushiAccel) as shown in Fig. <ref type="figure">4</ref>. SUSHI exploits novel SubGraph Reuse enabled via the interaction of its three components to serve queries with higher accuracy subjected to latency constraints or lower latency subjected to accuracy constraints.</p><p>We describe our proposed SushiAbs and SushiSched below. SushiAccel is described in ?4 in detail. The terminology used in this paper is captured in Fig. <ref type="figure">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SUSHI's System Architecture</head><p>We describe the interaction between SUSHI's components. Fig. <ref type="figure">4</ref> demonstrates a query path in SUSHI. The query enters the system with a certain latency and accuracy constraint. Then, the SushiSched makes a two-part control decision. First, it selects an appropriate SubNet (i.e., SN t ) that can serve the current query q t . It makes this subnet selection with the help of SushiAbs. SushiAbs provides the scheduler with the ability to perform latency estimation when a specific SubNet is served with a given SubGraph cached. SushiAbs exposes this state in an acceleratoragnostic fashion.</p><p>Second, SushiSched decides the next cached-SubGraph.</p><p>The exact algorithm for this control decision is described in Alg. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SushiSched control decision is then enacted by</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SushiAccel.</head><p>The selected SubNet, next cached-SubGraph, and query-data are sent to the SushiAccel. SushiAccel performs inference of the query using the selected SubNet. Model weights that are not already SGScached as part of the cached SubGraph are fetched from off-chip to on-chip buffer space. Finally, the accelerator returns the results of performing inference on SubNet to SushiSched and enacts the SubGraph caching control decision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Abstraction</head><p>SushiAbs abstracts the ability to perform latency estimation for a given SubNet as a function of a cached SubGraph in an accelerator-agnostic fashion. It enables SushiSched to make cachedSubGraph aware control decisions. As these control decisions are performed on the critical path of the query, this enabling abstraction must be efficient both w.r.t. space (R1) and time (R2).</p><p>Indeed, the set of all possible cached-SubGraphs is exponentially large for WS-DNNs (&gt;&gt; 10 19 ) <ref type="bibr" target="#b3">(Cai et al., 2019)</ref>. Thus, to achieve (R1), the abstraction limits the set of all possible cached SubGraphs to a significantly smaller set S, such that |S| &lt;&lt; 10 19 . The size of SubGraphs in S are selected to be close to the cache size.  On the software side, the scheduler receives a stream of queries, where each query is annotated with an (Accuracy, Latency) pair, denoted (A t , L t ). In this section we will describe exactly how the scheduler makes its SubNet selection and SubGraph caching control decisions.</p><p>Per-query SubNet (SN t ) Selection. As shown in Fig. <ref type="figure">4</ref>, the scheduler decision is guided by two primary considerations:</p><p>(i) serve strictly higher accuracy and (ii) serve strictly smaller latency, which can be specified by the user. In case of strictly higher accuracy, the scheduler can choose from the feasibility set of all SubNets with accuracy ? A t . SUSHI serves a SubNet that has minimum latency among all the SubNets that have accuracy ? A t . Note that, it may be possible that the served latency might not satisfy the latency constraint of ? L t . In case of strictly lesser latency, the scheduler serves a SubNet that has maximum accuracy among all the SubNets that have latency ? L t . Similarly, it is possible that the served accuracy might not satisfy the accuracy constraint of ? A t . Notice that the accuracy for a given SubNet is fixed, whereas the latency depends on the SubGraph cached into the PB. The scheduler employs a Latency -T able to get the latency values for SubNet given a cache state.</p><p>Across-query SubGraph Caching (S t+Q ). The scheduler needs to decide what SubGraph to cache after every Q queries (S t+Q ). To make this decision, the scheduler needs to represent the SubGraphs and SubNets, use the information from the past Q queries, and predicts the next SubGraph that should be cached into the PB.</p><p>Encoding SubGraph NN Architecture. The scheduler represents both the SubNets and the SubGraphs as a vector as shown in Fig. <ref type="figure">6</ref>. The scheduler uses the number of kernels K i and the number of channels C i of every layer i to create a vector of size 2N for N layered neural network. For instance, the vectorized representation for a 3-layered neural network would be</p><formula xml:id="formula_0">[K 1 , C 1 , K 2 , C 2 , K 3 , C 3 ].</formula><p>Amortizing Caching Choices. The scheduler keeps a running average of the past Q SubNets that were served by the scheduler as shown in Fig. <ref type="figure">6</ref> (middle). The running average serves as a good indicator of the kernels and the channels that were frequently used in the SubNets that were served for the past Q queries. If some kernels or channels were frequently used in the past Q SubNets, the values corresponding to these kernels or channels will be high in the vectorized representation. Notice that, the running average can be considered as an approximation of the intersection operation, but with more information. Doing intersection purely loses the information for the kernels and the channels that were frequent but not present in all the SubNets; however, averaging helps us to preserve this information.</p><p>Predicting the Next SubGraph (S t+Q ). The scheduler employs the distance from the running average of the past Q queries to predict the next SubGraph to be cached as shown in Fig. <ref type="figure">6</ref>. The scheduler caches the SubGraph that has the minimum distance from the average SubNet. Minimum distance ensures that the most frequent kernels and channels will be cached into the PB. In case fitting all of them is not possible, minimum distance from average Sub-Net ensures that we are picking the best fit SubGraph in terms of frequently occurring channels and kernels in the SubNets served by the scheduler. The algorithm for performing both the scheduler decisions is described briefly in Algorithm 1. SushiSched receives input from the user Figure <ref type="figure">6</ref>. The scheduler represents each neural network as a vector using the number of kernels and channels for each layer. The scheduler maintains a running average of the SubNets that were served for the past Q queries. For every Q queries, the scheduler caches the SubGraph that is the closest to the average SubNet.</p><p>including SubGraphs, SubNets, LatencyT able. AvgNet is the running average of the served SubNets. The cache state is set to a random SubGraph initially. The SushiSched decides the SubNet to be served for a given query when the accuracy is a hard constraint i.e. serving strictly better accuracy. The SushiSched can also decide the SubNet to be served if the latency is a hard constraint i.e. serving strictly lesser latency. It updates the running average of the SubNets.</p><p>Finally, the SushiSched determines the SubGraph that is closest to the AvgNet and caches it into the PB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SU S H IAC C E L IMPLEMENTATION 4.1 Hardware Design Challenges</head><p>As discussed earlier in ?2 and ?3, to support SubGraph Stationary, we propose to augment DNN accelerators with a custom cache called Persistent Buffer (PB). The introduction of PB leads to a new design space because it competes for a finite on-chip buffer capacity (that needs to be partitioned across input activation, weight, and output activation tiles, and also shared weights).</p><p>To guarantee the best performance of hardware design on such a design space, we have to develop the parameterizable hardware template with the support of different hardware configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Architectural Components</head><p>In this part, we introduce components of SushiAccel (Fig. <ref type="figure">7</ref>) and how it supports all proposed data reuse in Fig. <ref type="figure">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Compute Array</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dot Product Engine (DPE).</head><p>The key building block of DNN accelerators is the ability to compute dot-products. For example, the Google TPU systolic array <ref type="bibr" target="#b25">(Jouppi et al., 2017)</ref> computes fixed-size dot products in each column by keeping weights stationary and forwarding (streaming) inputs from one column to the other, NVDLA (NVIDIA, 2016) employs dedicated dot product engines (DPEs) of size 64, while flexible accelerators <ref type="bibr" target="#b27">(Kwon et al., 2018;</ref><ref type="bibr" target="#b33">Qin et al., 2020)</ref> have DPEs of configurable sizes (enabled via all-toall connectivity between the buffers and PEs). In this work, we picked fixed-size DPEs of size 9. Larger kernels will be breakdown into a serial of 3 ? 3 kernels and get flattened across the multipliers for reduction using the adder tree. As for small kernels (1 ? 1), C dimension will be flattened across multipliers to leverage input channel parallelism.</p><p>Parallelism. To further increase the throughput, we instantiate a 2D array of DPEs to boost the throughput by leveraging parallelism and reuse as shown in the Fig. <ref type="figure">8</ref>. As for the parallelism, the number of row indicates the total number of kernels being processed in parallel in DPE Array, i.e. kernellevel parallelism (K P ). While the number of column stands for total number of input activation (iAct) channels being processed in parallel, i.e. channel-level parallelism (C P ).</p><p>Both iActs and weights take the same interface to save the wire cost and improve scalability. In the vertical axis, both weights and iActs pass through DPEs of different rows in the store-and-forward fashion. During the weights forwarding, DPE will keep targeted weights stationary. Then, iActs will be streamed and get processed. In the horizontal axis, we replicate the same DPE independently to process different iActs channels and add an extra adder tree to reduce results from DPEs in the same row.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">On-chip Buffers and Supported Data Reuse</head><p>We designed a custom on-chip buffer hierarchy to both store data in the layout preferred by the DPE array and support reuse opportunities not leveraged by the DPE array. The entire on-chip storage is divided into multiple separate buffers for different types of data as illustrated by different colors in Fig. <ref type="figure">7</ref>.</p><p>Persistent Buffer (PB). The PB is designed to enable Sub-Graph Reuse. For example, SushiAccel loads the Sub-Graph (kernel 1) in Fig. <ref type="figure">8d</ref> from off-chip memory only once and stores it inside PB, such that it could be reused when switching between SubNet 1 and SubNet 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic Buffer (DB).</head><p>The DB is a typical on-chip storage to store the distinct weights of the requested SubNet. By adopting a PB, only non-common weights need to be fetched from the off-chip to the on-chip storage. For example, in Fig. <ref type="figure">8d</ref>, all kernels except the common part (kernel 2 to kernel N ) will be loaded into DB when targeting at SubNet 1, and will be replaced by kernel M to kernel M + N when switching into SubNet 2. The DB is implemented as a pingpong buffer, as indicated by DB1 and DB2 in Fig. <ref type="figure">7</ref>, to hide the latency of fetching distinct weights from the off-chip DRAM.</p><p>Streaming Buffer (SB). SB is designed to store entire iActs and support iAct Reuse -Multiple kernels. (Fig. <ref type="figure">8b</ref>).</p><p>Line Buffer (LB). LB works as a serial to parallel conversion <ref type="bibr" target="#b49">(Wang et al., 2021)</ref> because the line buffer takes a single pixel from SB and moves it internally. Therefore,  iActs data among different sliding windows will be reused inside the LB, i.e. LB supports iAct Reuse -Sliding Window Overlap (Fig. <ref type="figure">8a</ref>). We augment the naive line buffer to support stride by enabling sliding windows skipping.</p><p>Output Buffer (OB). OB provides in-place accumulation for oActs of different channels such that only the final oActs will be sent off-chip to save data movement of partial sums.</p><p>ZP/Scale Buffer (ZSB). ZSB serves as the on-chip storage for zero point and scale for quantized inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">SushiAccel Dataflow</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Latency Reduction from Inter-Query Dataflow</head><p>The inter-query processing timeline of SushiAccel is shown in Fig. <ref type="figure" target="#fig_3">9a</ref> where stage B indicates the movement of the common SubGraph from off-chip to on-chip PB. The latency saving of SushiAccel comes from eliminating the redundant off-chip SubGraph access, as illustrated in Fig. <ref type="figure" target="#fig_3">9a</ref> where SushiAccel reduces common SubGraph off-chip access (stage B) to only once in the critical path instead of multiple times in design w/o PB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Hiding Latency from Intra-layer Dataflow</head><p>Within each convolution layer, SushiAccel processes a convolution layer in the granularity of weight tiles shown in Fig. <ref type="figure" target="#fig_3">9b</ref>. Different stages (i.e., A-L) are defined in Fig. <ref type="figure">7</ref> that represent the movement of specific data. To further hide off-chip data access latency from critical path, we implement a double distinct weights buffer (ping-pong dynamic buffers DB1 and DB2 shown in Fig. <ref type="figure">7</ref>) to hide the off-chip latency of fetching distinct weights behind the computation latency. This is indicated by stages D1 and D2 that are hidden from stages F-G-J-K shown with arrows in Fig. <ref type="figure" target="#fig_3">9b</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">System Setup</head><p>Workload: We choose weight shared version of ResNet50 and MobV3 as two SuperNets <ref type="bibr" target="#b3">(Cai et al., 2019)</ref>. To evaluate SUSHI with full range on the pareto-frontier, we pick a sequence of 6 and 7 SubNets from ResNet50 and MobV3, respectively.</p><p>The sizes of ResNet50 SubNets range from the <ref type="bibr">[7.58 MB,</ref><ref type="bibr">27.47 MB]</ref> while the sizes of MobV3 SubNets range from [2.97 MB, 4.74 MB]. Shared weights take up 7.55 MB and 2.90 MB for ResNet50 and MobV3, separately<ref type="foot" target="#foot_1">3</ref> . SubNets are obtained using the procedure mentioned in OFA <ref type="bibr" target="#b3">(Cai et al., 2019)</ref>.</p><p>Metrics: Latency in this section refers to the end-to-end serving latency of a given model, while accuracy refers to the top-1 accuracy. Both accuracy and latency are defined for SubNets only. SubGraphs are only used for the caching purpose as a subset of SubNets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture Analytic Model:</head><p>We have developed an analytic model which estimates the behavior of SushiAccel to explore design space by configuring the architecture with parameters. Our model accurately predicts the latency trend of SushiAccel using profiled latency of SushiAccel on both workloads, enabling us to perform an exhaustive search of all parameter combinations within specified constraints. This approach allows for the identification of optimal configurations for improved performance in both simulation and real-world deployment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Roofline Analysis</head><p>We also extended a roofline analysis tool to study the effect of PB on the boundness of SushiAccel under different workloads.</p><p>Deployment Platforms: We implemented the proposed SushiAccel on two FPGA including ZCU104 (5 W) and Alveo U50 (75 W). We compare our SushiAccel w/ PB and w/o PB with Xilinx DPU and CPU (Intel i7 10750H, 45 W).</p><p>Scheduler Simulator: We have developed SushiSched, which runs on the CPU and guides the SushiAccel on how to serve the current query and (a) what SubGraph to serve and (b) SubNet to be placed in PB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">SUSHI Impact on Arithmetic Intensity</head><p>To understand the benefits of SGS, we perform roofline analysis as shown in Fig. <ref type="figure" target="#fig_4">10</ref> and Fig. <ref type="figure" target="#fig_6">11</ref>, where roofline represents the normal roofline curve. And SGS-roofline virtually improves the overall off-chip bandwidth by saving off-chip data access, leading to an improved roofline curve shown by SGS roofline. The experiments are performed with a system with 19.2 GB/s off-chip memory bandwidth and 1.296 Tflops throughput running at 100 MHz <ref type="bibr" target="#b35">(Reuther et al., 2022)</ref>.</p><p>The latency breakdown results in Fig. <ref type="figure" target="#fig_4">10</ref> shows that SGS can potentially remove the off-weights access latency from the critical path, such that the individual latency of serving a stream of queries from pareto-frontiers could be reduced by [6%, 23.6%] for MobV3 and [5.7%, 7.92%] for ResNet50. Such latency reduction essentially comes from the model boundedness shifting. The SGS pushes models towards compute-bound, which increases the utilization of the avail-   able compute resources for higher throughput and reduces latency and energy consumption. The shifting is illustrated by blue dots being pushed toward the red dots in Fig. <ref type="figure" target="#fig_6">11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">SushiAccel Configuration Impact</head><p>In this subsection, we explore the impact of three main factors (i.e., bandwidth, throughput, and PB size) of SushiAccel on the overall end-to-end serving latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Bandwidth -Buffers Arrangement</head><p>Different types of data require different bandwidths. A unified buffer for all different data types demands the controller to handle potentially all-to-all connections between buffers and all compute units. While the design of the splitting buffer only needs a direct connection between a buffer and compute units, which saves the complexity of both the datapath and the controller. The buffer is a 2D array and its size equals width ? height. The width refers to the bandwidth a buffer could supply every cycle. The bandwidth demand of different buffers is shown in Tab. 1, which is determined by both workloads and hardware specifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">PB Size -Sizes of Buffers</head><p>All buffers compete on the same total storage budget so that a balance of them is preferred to achieve good performance. The addition of a persistent buffer also introduces a new factor of common weights reuse, leading to a trade-off between inter-layer data reuse and intra-layer data reuse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Throughput -Parallelism of the Compute Array</head><p>The parallelism of the 2D DPE Array is also a controllable knob. Within the same computation engine budget, a change in parallelism indicates a change in throughput, yielding different performances on different workloads. For example,   the parallelism of 16 and 32 in K and C dimensions deliver a peak throughput of 512 data per clock cycle. Therefore, we use this throughput as the factor to abstract parallelism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4">Design Space Exploration</head><p>As Fig. <ref type="figure" target="#fig_9">12</ref> shows with larger PB sizes, more on-chip computation, and less off-chip bandwidth, the latency is improved. However, for MobV3, due to the smaller size, having depthwise conv layers, and less reuse, the amount of improvement is lesser for MobV3 compared with the ResNet50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">SushiAccel Evaluation</head><p>In this subsection, we evaluate how SushiAccel will impact the latency and energy reduction. We evaluate different scales of SushiAccel on two real FPGAs with different budgets running the 3x3 convolution layers of ResNet50. The SushiAccel on Alveo U50 has off-chip bandwidth of 14.4 GB/s, PB size of 1.69 MB, and throughput of 0.9216 TFlops running at 100 MHz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Resources Allocation among Buffers</head><p>The resource utilization of SushiAccel w/ PB and w/o PB under optimal configurations on both Xilinx ZCU104 and Alveo U50 are shown in Tab. 2 with a breakdown on-chip storage allocation shown in Tab. 3. Both SushiAccel w/ PB and SushiAccel w/o PB use the same amount of overall on-chip storage for a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Latency Evaluation</head><p>The real-board latency and energy consumption results are shown in Fig. <ref type="figure" target="#fig_10">13a</ref> with resources shown in Tab. 2. On ZCU104, compared with CPU, SushiAccel w/o PB achieves 1.81X ? 3.04X speedup and SushiAccel w/ PB achieves 1.87X ? 3.17X for different SubNets. While on Alveo U50, compared with CPU, SushiAccel w/o PB achieves 1.43X ? 2.54X speedup and SushiAccel w/ PB achieves 1.57X ? 2.61X for different SubNets. Fig. <ref type="figure" target="#fig_10">13a</ref> also shows that the scale-up design on Alveo U50 performs worse than the small-scale design on ZCU104 under small SubNets because of higher off-chip DRAM competition in data center cluster hosting Alveo U50 than simple embedded ZCU104. Thus, off-chip data access dominates latency in Alveo U50, resulting in the slow down for small SubNets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Energy Evaluation</head><p>Energy in data movement has been proved to dominate the entire power consumption of neural network accelerator <ref type="bibr" target="#b11">(Dally et al., 2020)</ref> and thus we estimate the overall energy through profiling the off-chip DRAM data access for all different platforms shown in Figure <ref type="figure" target="#fig_10">13b</ref>.</p><p>We estimate the off-chip energy by profiling the DRAM data access and compute it as N umberAccess ? EnergyP erAccess. With the proposed SubGraph Reuse, we could save [14%, 52.6%] off-chip data access energy saving for ResNet50 and [43.6%, 78.7%] for MobV3 compared to SushiAccel w/o PB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Comparing with DPU</head><p>We compared SushiAccel against Xilinx DPU using real layer-wise end-to-end inference latency of min-SubNet on ZCU104 as shown in Fig. <ref type="figure" target="#fig_0">14</ref>. We consider convolution layers with 3 ? 3 kernel sizes. SushiAccel w/o PB achieved 0.5?1.95? faster execution time than Xilinx DPU (25.1% GeoMean speedup). This quantitative comparison lends credence to the proposal of adding a Persistent Buffer (PB) to a state-of-the-art ML accelerator design.</p><p>There are also seldom cases when SushiAccel performs worse than Xilinx DPU, because SushiAccel takes less parallelism in height (X) and width (Y) dimensions (Fig. <ref type="figure">5</ref>), leading to higher latency under workload with higher X and Y values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">SushiSched Functional Evaluation</head><p>In this section, we evaluate the performance of SushiSched for both ResNet50 and MobV3.</p><p>Fig. <ref type="figure" target="#fig_12">15</ref> shows that the SushiSched is able to serve queries with strictly lesser latency and/or better accuracy where blue dots represent served queries by employing SushiSched.  In Fig. <ref type="figure" target="#fig_12">15a</ref> and Fig. <ref type="figure" target="#fig_12">15c</ref>, blue dots are almost always below the line y = x manifesting that the SushiSched can serve strictly lesser latency if the latency is a hard constraint that needs to be satisfied. Similarly, all blue dots above the line y = x in Fig. <ref type="figure" target="#fig_12">15b</ref> and Fig. <ref type="figure" target="#fig_12">15d</ref> show that the SushiSched can serve strictly better accuracy if accuracy is a hard constraint that needs to be met.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">End-to-End SUSHI Evaluation</head><p>In this section, we compare the latency-accuracy tradeoff results among SUSHI w/o PB, SUSHI w/ PB (state-unaware caching), and SUSHI. The blue dots in Fig. <ref type="figure" target="#fig_13">16</ref> illustrate how SUSHI serves random queries 4 .</p><p>For ResNet50 in all cases, SUSHI w/o scheduler consistently outperforms No-SUSHI. For random queries, SUSHI is also able to decrease the latency by 21% on average given the same accuracy compared to not having SUSHI.</p><p>In the case of MobV3, due to its small size, a relatively larger fraction of a SubNet fits in PB, resulting in a higher cache-hit ratio (Appendix A.4). SUSHI offers better accuracy-latency tradeoff than SUSHI w/o scheduler, with the exception of only a few points. In the case of MobV3, SUSHI is also able to decrease the latency by 25% on average given the same accuracy compared to not having SUSHI.</p><p>4 Due to the overlap, only limited points in the figures are visible Finally, SUSHI increases the serving accuracy by up to 0.98% for the same latency, which is significant for ML serving applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>Various accelerator designs such as Maeri <ref type="bibr" target="#b27">(Kwon et al., 2018)</ref>, Eyeriss <ref type="bibr" target="#b8">(Chen et al., 2018)</ref>, NVDLA <ref type="bibr" target="#b30">(NVIDIA, 2016)</ref>, and DPU <ref type="bibr" target="#b47">(Xilinx, 2022)</ref> support different types of reuse Fig. <ref type="figure">8</ref>. A comparison of them is shown in Tab. 4. However, all of these works achieve intra-model cross-layer reuse in contrast to the cross-query reuse we propose with SushiAccel.</p><p>Clipper <ref type="bibr" target="#b9">(Crankshaw et al., 2017)</ref> serves single model queries without exposing a latency/accuracy tradeoff. Inferline <ref type="bibr" target="#b10">(Crankshaw et al., 2018)</ref> serves multiple models but in a pipeline, there's no latency/accuracy tradeoff per model. INFaaS <ref type="bibr">(Romero et al., 2021b)</ref> provides a query-time latency/accuracy tradeoff mechanism and policy but suffers from expensive model switching mechanisms. This also translates into a policy that minimizes model switching as a result. The vertically integrated inference serving stack provided by SUSHI naturally plugs into existing inference serving frameworks, enabling agile navigation of the latency/accuracy tradeoff at query time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>SUSHI is a vertically integrated hardware-software infer-   ence serving stack that takes advantage of the temporal locality induced by serving inference queries on the same weight-shared supernetwork structure. To the best of our knowledge, the concept of SubGraph Stationary (SGS) optimization across queries is novel. We demonstrate that to achieve the best temporal locality benefit, the proposed hardware implementation SushiAccel must work in tandem with the software scheduler SushiSched to control what SubNets to serve for each query and how to update the accelerator state. We further ensure generalizability of SushiSched by abstracting the effect of hardware state on the latency (and energy) of served SubNets with a black box SubGraph latency table. This decouples SushiSched from any accelerator implementation, while maintaining its state-awareness implicitly. SUSHI can be naturally integrated in state-of-the-art ML inference serving frameworks and enables better latency/accuracy tradeoffs for a stream of queries with latency/accuracy constraints. For a stream of queries, our results show 0.98% improvement in the served accuracy, and up to 25% latency reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">ACKNOWLEDGMENT</head><p>This material is based upon work partially supported by the National Science Foundation under Grant Number CCF-2029004. Additional support was provided by a sponsored research award by Cisco Research. We would like to further acknowledge the insightful comments of the review panel as well as the skillful guidance of our shepherd, Dr. Qijing Jenny Huang, which greatly contributed to the quality of this paper. We thank the anonymous reviewers of MLSys, and the SAIL Research Group members for valuable feedback and the stimulating intellectual environment they provide.</p><p>We also thank Taekyung Heo from Synergy lab for his feedback on the initial version of the paper. Disclaimer: Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. WS-DNN properties.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Figure4. System architecture overview. Given a stream of queries annotated with (Accuracy, Latency) pairs q1, .., qQ and the current cache state C1, the scheduler chooses the SubNet to be served SNi for each i'th query and next cache state G2 after every Q queries.Input Activations (iActs)Output Activations (oActs)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>Figure 7. The overall SushiAccel architecture (KP = 2, CP = 3)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. SushiAccel dataflow overview.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 10 .</head><label>10</label><figDesc>Potential latency reduction with SGS (two bar per Sub-Graph, left: w/o PB; Right: w PB)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 11 .</head><label>11</label><figDesc>SGS pushes memory-bound to compute-bound layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 .</head><label>12</label><figDesc>Latency reduction (Time Save in legend) improvement exploration on SushiAccel using Analytic Model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. Real board latency and energy reduction for ResNet50. (left and right bars in (b) are SushiAccel w/o PB and w/ PB)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 15 .</head><label>15</label><figDesc>Serve strictly better accuracy and lesser latency for ResNet50 and MobV3 using SUSHI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 16 .</head><label>16</label><figDesc>Comparing delivering latency-vs-accuracy of No-SUSHI and SUSHI w/o scheduler and baselines for ResNet50 and MobV3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Latency of two different SubNets as a function of different cached SubGraphs. Different cached SubGraphs are optimal for different served SubNets with a non-trivial relationship based on the similarity of NN architecture parameters. SushiSched captures this similarity with a distance measure in ?3.</figDesc><table><row><cell>Shallow &amp; Wide SubNet</cell><cell>Deep &amp; Thin SubNet</cell></row><row><cell cols="2">Cached SubGraph</cell></row><row><cell>More Layers</cell><cell>More Width</cell></row><row><cell>Figure 3.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Latency Accuracy SubNets to be Served LatencyTable Query SubGraph Cached Cache State Inputs Accuracy Latency 80% DPE DPE DPE DPE queris PB-Cache On-Chip Buffers Change Per queries Cache State Output SubGraph SubGraph SubGraph Scheduler DRAM</head><label></label><figDesc>Hence, at any point in time, SushiAccel always caches SubGraphs from S and SushiSched also selects a Sub-Graph to cache from S as well. The abstraction achieves (R2) by using a lookup table data structure with SubNets as rows and SubGraphs as columns. Hence, it takes the least amount of time to get latency-estimate of SubNet i for a given SubGraph j. The size of the lookup table is given by O(|S|.|X |) ? O(|S|) where X denotes the set of serving SubNets, since we expect O(|X |) ? O(1). SubNet to be served and SubGraph to be cached. Calculate SubNet to be served for every query q t = (A t , L</figDesc><table><row><cell>SuperNet</cell><cell>SubNet 1</cell><cell>SubNet 2</cell></row><row><cell>Query 1</cell><cell></cell><cell></cell></row><row><cell>&lt;100 ms</cell><cell></cell><cell></cell></row><row><cell>Acc&gt;80%</cell><cell></cell><cell></cell></row><row><cell>Query N</cell><cell></cell><cell></cell></row><row><cell>&lt; 10 ms</cell><cell></cell><cell></cell></row><row><cell>Acc&gt;70%</cell><cell></cell><cell></cell></row><row><cell cols="3">3.3 SushiSched Design</cell></row><row><cell cols="3">Algorithm 1: Scheduling Algorithm</cell></row></table><note><p>Input: SubNet to be served SN i , i ? [1...N ], SubGraph to be cached G j , j ? [1...M ], Latency table L[i][j]. Result: t ), t ? [0...Q] and SubGraph to be cached every Q iterations; AvgN et = [0,0,0...0]; CacheState = ?; while q t do if policy == STRICT ACCURACY then id x = argmin latency (L[i][CacheState] ?i ? [0...N ] s.t. SN i .accuracy &gt;= A t ); else id x = argmax accuracy (L[i][CacheState] ?i ? [0...N ] s.t. SN i .latency &lt;= L t ); end for every Q queries do AvgN et.update(SN idx , Q) CacheState = argmin Dist (Dist(G j ,AvgNet)) ?j ? [0...M ]); end end</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 .</head><label>1</label><figDesc>Bandwidth requirement of on-chip buffers Buffer Minimal Bandwidth Requirement DB LCM ( max off-chip BW , DPE Array demanded on-chip BW ) SB LCM ( max off-chip BW , C P ? R ? S? iActs DataWidth) LB DPE Array demanded on-chip BW OB K P ? oAct DataWidth PB LCM ( max off-chip BW , DPE Array demanded on-chip BW ) Note: BW = bandwidth, LCM (x1, x2): Least Common Multiple of x1 and x2.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 .</head><label>2</label><figDesc>Resources comparison of SushiAccel with DPU</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>SushiAccel</cell><cell>SushiAccel</cell><cell>Xilinx DPU</cell><cell>SushiAccel</cell><cell>SushiAccel</cell></row><row><cell></cell><cell></cell><cell></cell><cell>w/o PB</cell><cell>w/ PB</cell><cell>DPUCZDX8G</cell><cell>w/o PB</cell><cell>w/ PB</cell></row><row><cell></cell><cell></cell><cell>Device</cell><cell>ZCU104</cell><cell>ZCU104</cell><cell>ZCU104</cell><cell>Alveo U50</cell><cell>Alveo U50</cell></row><row><cell></cell><cell></cell><cell>LUT</cell><cell>61180 (26.6%)</cell><cell>64307 (27.9%)</cell><cell>41640 (18.1%)</cell><cell>231668 (26.63%)</cell><cell>244969 (28.16%)</cell></row><row><cell></cell><cell></cell><cell>Register</cell><cell>107216 (23.3%)</cell><cell>117724 (25.5%)</cell><cell>69180 (15%)</cell><cell>435071 (24.96%)</cell><cell>445602 (25.56%)</cell></row><row><cell></cell><cell></cell><cell>BRAM</cell><cell>192.5 (61.7%)</cell><cell>198.5 (63.6%)</cell><cell>0</cell><cell>452.5 (33.67%)</cell><cell>452.5 (33.67%)</cell></row><row><cell></cell><cell></cell><cell>URAM</cell><cell>48 (50%)</cell><cell>96 (100%)</cell><cell>60 (62.5%)</cell><cell>48 (7.5%)</cell><cell>96 (15%)</cell></row><row><cell></cell><cell></cell><cell>DSP</cell><cell>1507 (87.2%)</cell><cell>1459 (87.2%)</cell><cell>438 (25.35%)</cell><cell>4739 (79.78%)</cell><cell>4740 (79.79%)</cell></row><row><cell></cell><cell></cell><cell>PeakOps/cycle</cell><cell>2592</cell><cell>2592</cell><cell>2304</cell><cell>9216</cell><cell>9216</cell></row><row><cell></cell><cell></cell><cell>GFlops (100MHz)</cell><cell>259.2</cell><cell>259.2</cell><cell>230.4</cell><cell>921.6</cell><cell>921.6</cell></row><row><cell>latency (ms)</cell><cell>1 2 3 4 5</cell><cell cols="3">SushiAccel speedup 25.1% over Xilinx DPU on average Xilinx DPU SushiAccel w/o PB Xilinx DPU (GeoMean)</cell><cell cols="2">SushiAccel w/o PB (GeoMean)</cell></row><row><cell></cell><cell>0</cell><cell cols="5">Convolution Layer ID in ResNet50 0 1 2 4 8 11 14 17 21 24 27 30 34 37 40 43 46 49 53 56 59</cell></row></table><note><p>Figure 14. The latency comparison between SushiAccel w/o PB and Xilinx DPU for ResNet50.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 .</head><label>3</label><figDesc>Buffer configurations of SushiAccel (ZCU104 board)</figDesc><table><row><cell></cell><cell cols="2">SushiAccel w/o PB</cell><cell cols="2">SushiAccel w/ PB</cell></row><row><cell></cell><cell>BRAM (KB)</cell><cell>URAM (KB)</cell><cell>BRAM (KB)</cell><cell>URAM (KB)</cell></row><row><cell>DB-Ping</cell><cell>0</cell><cell>1152</cell><cell>0</cell><cell>576</cell></row><row><cell>DB-Pong</cell><cell>0</cell><cell>1152</cell><cell>0</cell><cell>576</cell></row><row><cell>SB</cell><cell>8</cell><cell>1152</cell><cell>8</cell><cell>576</cell></row><row><cell>LB</cell><cell>54</cell><cell>0</cell><cell>54</cell><cell>0</cell></row><row><cell>OB</cell><cell>327</cell><cell>0</cell><cell>327</cell><cell>0</cell></row><row><cell>ZSB</cell><cell>8</cell><cell>0</cell><cell>8</cell><cell>0</cell></row><row><cell>PB</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1728</cell></row><row><cell>Overall</cell><cell>397</cell><cell>3456</cell><cell>397</cell><cell>3456</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 .</head><label>4</label><figDesc>Reuse comparison (prior works v.s. SUSHI).</figDesc><table><row><cell>Work</cell><cell>iActs Reuse Fig. 8a &amp; 8b</cell><cell>oAct Reuse Partial Sum</cell><cell>Weights Reuse iAct Tiling</cell><cell>SubGraph Reuse</cell></row><row><cell>MAERI (Kwon et al., 2018)</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>temporal ?</cell></row><row><cell>NVDLA (NVIDIA, 2016)</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>temporal ?</cell></row><row><cell>Eyeriss (Chen et al., 2016)</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>temporal ?</cell></row><row><cell>Xilinx DPU (Xilinx, 2022)</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>temporal ?</cell></row><row><cell>SUSHI</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>spatial ? temporal ?</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>In the same network, relatively lower arithmetic intensity corresponds to higher chances of becoming memory bound.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Weights, input activations, and zero points are quantized to int8, and the quantization scale is quantized into int32.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX -ABLATION STUDIES A.1 Temporal Analysis of Subgraph Caching</head><p>In this section, we explore the impact of a number of vectorized SubGraphs employed in the running average results as well as the size of Latency -T able on the accuracylatency results. Making cache update decisions after each query improves both latency and accuracy results (Fig. <ref type="figure">17</ref>), but is prohibitively expensive as the new SubGraph must be fetched from off-chip memory.</p><p>For ResNet50, increasing the number of queries to two, the results worsen. Increasing the number of queries to 4 and 8 yields better results. Eventually, there's a point when the performance starts to get worse (e.g., at 10+ queries) as the benefit of temporal locality will be reduced. So there's a tradeoff between the staleness of query history over which the cached SubGraph is computed and the cost of updating cache frequently.</p><p>Following the same methodology for MobV3 (18), we observe that averaging over 10 queries gives us the best tradeoff, leading to better accuracy-latency results.</p><p>A.2 Impact of Latency -T able size</p><p>The results in Tab. 5 show the average latency improvement by increasing the size of Latency -T able compared with SUSHI w/o scheduler. As the results for ResNet50 show, increasing the size of the table improves performance, but is quickly saturated. This is consistent with the important property of SushiSched table (rapid lookups on the critical query path).</p><p>For MobV3, we see almost no improvement in latency with increased table size, which shows that if the PB is large enough to hold a large portion of the SubNet (and, with other on-chip buffers-the whole SubNet), the small table size can capture most of the required information by the scheduler. Thus, for smaller models, we keep the horizontal size of Latency -T able minimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Lookup Latency</head><p>We used the lookup table as a fast-search data structure. For the largest model, (ResNet-50) the latency in microseconds is shown in table Tab. 6. These results show that the lookup table time is less than 1 1000 of the inference time and, thus, doesn't significantly interfere with the query's critical path. A.4 Cache Hit Ratio SUSHI leverages temporal locality across queries as the SubNets they induce share some weights that are common to these SubNets. The benefit of SubGraph Reuse thus fundamentally is a function of the workload. For instance, if all queries used the exact same SubNet and we cache the largest SubGraph of that SubNet, then the probability of its reuse is 1. To generalize this intuition, SUSHI makes caching decisions based on the intersection of SubNets used by the last Q queries. Thus, we define the cache hit ratio as the fraction of the cached SubGraph that was "hit" or present in the SubNet served, because those weights don't need to be fetched.</p><p>For a given query trace, we log (SN t , G t ) series of tuples where SN t is the SubNet that the scheduler decided to serve at time t, and G t is a SubGraph cached in PB at time t. We find the overlap between SN t and G t using</p><p>where SN t is already vectorized using C and K. We average this over t to get the average cache hit ratio. ||?|| 2 is used as a proxy to calculate vector overlap. Thus defined, SUSHI reaches a hit ratio of 66% (78%) for ResNet50 (MobV3).</p><p>It is instructive that the cache hit ratio is higher for smaller models, as the intersection of common weights used by Sub-Nets over a past window of Q queries is a larger fraction of the served SubNet. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Yolo3d: End-to-end real-time 3d oriented object bounding box detection from lidar point cloud</title>
		<author>
			<persName><forename type="first">H</forename><surname>Abdelaziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pedram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hassoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abdelkarim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zidan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zahran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>El Sallab</surname></persName>
		</author>
		<ptr target="https://proceedings.mlsys.org/paper/2021/file/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) Workshops</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Dimakis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</editor>
		<meeting>the European Conference on Computer Vision (ECCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2018">2021. 2018</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning and Systems</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Proxquant: Quantized neural networks via proximal operators</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Liberty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR&apos;19</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Direct neural architecture search on target task and hardware</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><surname>Proxylessnas</surname></persName>
		</author>
		<idno>CoRR, abs/1812.00332</idno>
		<ptr target="http://arxiv.org/abs/1812.00332" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Once for all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<idno>CoRR, abs/1908.09791</idno>
		<ptr target="http://arxiv.org/abs/1908.09791" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haberle</surname></persName>
		</author>
		<author>
			<persName><surname>Holtz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.08279</idno>
		<title level="m">Communication bounds for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="127" to="138" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using Dataflow to Optimize Energy Efficiency of Deep Neural Network Accelerators</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="12" to="21" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.07928</idno>
		<title level="m">Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Clipper: A {Low-Latency} online prediction serving system</title>
		<author>
			<persName><forename type="first">D</forename><surname>Crankshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th USENIX Symposium on Networked Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="613" to="627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Inferline: Ml prediction pipeline provisioning and management for tight latency objectives</title>
		<author>
			<persName><forename type="first">D</forename><surname>Crankshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-E</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tumanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01776</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domain-specific hardware accelerators</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Turakhia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="48" to="57" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Real-time tracking and lane line detection technique for an autonomous ground vehicle system</title>
		<author>
			<persName><forename type="first">T</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Swain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Computing and Smart Communication</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019. 2020</date>
			<biblScope unit="page" from="1609" to="1625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional social pooling for vehicle trajectory prediction</title>
		<author>
			<persName><forename type="first">N</forename><surname>Deo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1468" to="1476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Latency-aware neural architecture search with multi-objective bayesian optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">I</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Daulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Balandat</surname></persName>
		</author>
		<idno>CoRR, abs/2106.11890</idno>
		<ptr target="https://arxiv.org/abs/2106.11890" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Near-lossless post-training quantization of deep neural networks via a piecewise linear approximation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shafiee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Abdel-Aziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thorsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Georgiadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hassoun</surname></persName>
		</author>
		<idno>CoRR, abs/2002.00104</idno>
		<ptr target="https://arxiv.org/abs/2002.00104" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A scalable multi-teraops deep learning processor core for ai trainina and inference</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fleischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Babinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Symposium on VLSI Circuits</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="35" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">D3: a dynamic deadline-driven approach for building autonomous vehicles</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kalra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schafhalter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth European Conference on Computer Systems</title>
		<meeting>the Seventeenth European Conference on Computer Systems</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="453" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">One size does not fit all: Quantifying and exposing the accuracy-latency trade-off in machine learning cloud service apis via tolerance tiers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boroujerdian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mummert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duesterwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISPASS.2019.00012</idno>
		<ptr target="https://doi.ieeecomputersociety.org/10.1109/ISPASS.2019.00012" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
		<meeting><address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-03">mar 2019</date>
			<biblScope unit="page" from="34" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Health OnLine Model Ensemble Serving for Deep Learning Models in Intensive Care Units</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Priambada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Maher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aljiffry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tumanov</surname></persName>
		</author>
		<author>
			<persName><surname>Holmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1614" to="1624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Focus: Querying large video datasets with low latency and low cost</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bodik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Philipose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/osdi18/presentation/hsieh" />
	</analytic>
	<monogr>
		<title level="m">13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)</title>
		<meeting><address><addrLine>Carlsbad, CA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2018-10">October 2018</date>
			<biblScope unit="page" from="269" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and &lt;1mb model size</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno>CoRR, abs/1602.07360</idno>
		<ptr target="http://arxiv.org/abs/1602.07360" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SMART: Robust and efficient fine-tuning for pre-trained natural language models through principled regularized optimization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<idno>doi: 10.18653</idno>
		<ptr target="1/2020.acl-main" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07">July 2020</date>
			<biblScope unit="page" from="2177" to="2190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<ptr target="https://aclanthology.org/2020.acl-main.197" />
		<title level="m">URL</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving memory utilization in convolutional neural network accelerators</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jokic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Emery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Embedded Systems Letters</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="77" to="80" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Borchers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th annual international symposium on computer architecture</title>
		<meeting>the 44th annual international symposium on computer architecture</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Flat: An optimized dataflow for mitigating attention bottlenecks</title>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yazdanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">MAERI: Enabling Flexible Dataflow Mapping over DNN Accelerators via Reconfigurable Interconnects</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Samajdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the 23rd International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<publisher>ASP-LOS</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Highlevel semantic feature detection: A new perspective for pedestrian detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5187" to="5196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Rethinking the value of network pruning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>CoRR, abs/1810.05270</idno>
		<ptr target="http://arxiv.org/abs/1810.05270" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<ptr target="http://nvdla.org/primer.html" />
		<title level="m">Deep Learning Accelerator (NVDLA)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Accelerating deep convolutional neural networks using specialized hardware</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ovtcharov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Microsoft Research Whitepaper</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Least squares binary quantization of neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pouransari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In CVPRW&apos;20</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sigma: A sparse and irregular gemm accelerator with flexible interconnects for dnn training</title>
		<author>
			<persName><forename type="first">E</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Samajdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nadella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<idno type="DOI">10.1109/HPCA47549.2020.00015</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="58" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mlperf inference benchmark</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kanter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mattson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schmuelling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Breughe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Charlebois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="446" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Ai and ml accelerator survey and trends</title>
		<author>
			<persName><forename type="first">A</forename><surname>Reuther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Michaleas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gadepally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kepner</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2210.04055" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Automated model-less inference serving</title>
		<author>
			<persName><forename type="first">F</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Yadwadkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><surname>Infaas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 USENIX Annual Technical Conference (USENIX ATC 21)</title>
		<imprint>
			<date type="published" when="2021-07">July 2021</date>
			<biblScope unit="page" from="397" to="411" />
		</imprint>
	</monogr>
	<note>USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<ptr target="https://www.usenix.org/conference/atc21/presentation/romero" />
		<title level="m">URL</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Automated model-less inference serving</title>
		<author>
			<persName><forename type="first">F</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Yadwadkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><surname>{infaas}</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 USENIX Annual Technical Conference (USENIX ATC 21)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="397" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">CompOFA -compound once-for-all networks for faster multi-platform deployment</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sahni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Varshini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tumanov</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=IgIk8RRT-Z" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Memory requirements for convolutional neural network hardware accelerators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Siu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahmoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Symposium on Workload Characterization (IISWC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="111" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Lstm neural networks for language modeling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schl?ter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirteenth annual conference of the international speech communication association</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep learning for large-scale traffic-sign detection and recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tabernik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sko?aj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on intelligent transportation systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1427" to="1440" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Magnet: A modular accelerator generator for neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clemons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fojtik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klinefelter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pinckney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2019">2019. 2019</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">ac 2 slam: Fpga accelerated high-accuracy slam with heapsort and parallel keypoint extractor</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 International Conference on Field-Programmable Technology (ICFPT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Benchmarking tpu, gpu, and cpu platforms for deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">E</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10701</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Overcoming data transfer bottlenecks in fpga-based dnn accelerators via layer conscious memory management</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 56th ACM/IEEE Design Automation Conference (DAC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><surname>Xilinx</surname></persName>
		</author>
		<ptr target="https://docs.xilinx.com/r/en-US/ug1414-vitis-ai/Deep-Learning-Processor-Unit" />
		<title level="m">Xilinx Deep Learning Unit (DPU)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Bignas: Scaling up neural architecture search with big singlestage models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="702" to="717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Forms: Fine-grained polarized reram-based in-situ computation for mixed-signal dnn accelerator</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Behnam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shafiee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Bojnordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="265" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Lq-nets: Learned quantization for highly accurate and compact deep neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
