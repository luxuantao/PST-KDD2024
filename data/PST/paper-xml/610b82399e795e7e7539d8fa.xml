<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-08-04">4 Aug 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Huadong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-08-04">4 Aug 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2108.02035v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>science</term>
					<term>mathematics</term>
					<term>biology</term>
					<term>research</term>
					<term>knowledge</term>
					<term>physics</term>
					<term>electron</term>
					<term>scientist</term>
					<term>capabilities</term>
					<term>labs</term>
					<term>knowledge</term>
					<term>innovate</term>
					<term>calculation</term>
					<term>...</term>
					<term>agrobiology</term>
					<term>mscience</term>
					<term>euclid</term>
					<term>orthogon</term>
					<term>einstein</term>
					<term>bioacoustics</term>
					<term>chemo</term>
					<term>axiom</term>
					<term>nomenclature What&apos;s the relation between speed and acceleration? A [MASK] : SCIENCE Construct Knowledgeable Verbalizer Refine Knowledgeable Verbalizer science</term>
					<term>mathematics</term>
					<term>biology</term>
					<term>research</term>
					<term>knowledge</term>
					<term>physics</term>
					<term>electron</term>
					<term>scientist</term>
					<term>capabilities</term>
					<term>labs</term>
					<term>knowledge</term>
					<term>innovate</term>
					<term>calculation</term>
					<term>...</term>
					<term>agrobiology</term>
					<term>mscience</term>
					<term>euclid</term>
					<term>orthogon</term>
					<term>einstein</term>
					<term>bioacoustics</term>
					<term>chemo</term>
					<term>axiom</term>
					<term>nomenclature physics mathematics science calculation research MLM Head</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Tuning pre-trained language models (PLMs) with task-specific prompts has been a promising approach for text classification. Particularly, previous studies suggest that prompttuning has remarkable superiority in the lowdata scenario over the generic fine-tuning methods with extra classifiers. The core idea of prompt-tuning is to insert text pieces, i.e., template, to the input and transform a classification problem into a masked language modeling problem, where a crucial step is to construct a projection, i.e., verbalizer, between a label space and a label word space. A verbalizer is usually handcrafted or searched by gradient descent, which may lack coverage and bring considerable bias and high variances to the results. In this work, we focus on incorporating external knowledge into the verbalizer, forming a knowledgeable prompttuning (KPT), to improve and stabilize prompttuning. Specifically, we expand the label word space of the verbalizer using external knowledge bases (KBs) and refine the expanded label word space with the PLM itself before predicting with the expanded label word space. Extensive experiments on zero and few-shot text classification tasks demonstrate the effectiveness of knowledgeable prompt-tuning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent years have witnessed the prominence of Pretrained Language Models (PLMs) <ref type="bibr" target="#b20">(Peters et al., 2018;</ref><ref type="bibr" target="#b23">Radford et al., 2018;</ref><ref type="bibr" target="#b4">Devlin et al., 2019;</ref><ref type="bibr" target="#b24">Raffel et al., 2020;</ref><ref type="bibr" target="#b7">Xu et al., 2021)</ref> due to their superior performance on a wide range of languagerelated downstream tasks such as text classification <ref type="bibr" target="#b9">(Kowsari et al., 2019)</ref>, question answering <ref type="bibr" target="#b25">(Rajpurkar et al., 2016)</ref>, and machine reading comprehension <ref type="bibr" target="#b18">(Nguyen et al., 2016)</ref>. To fathom the principles of such effectiveness of PLMs, researchers have conducted extensive studies and suggested that PLMs have obtained rich knowledge during pre-training <ref type="bibr" target="#b22">(Petroni et al., 2019;</ref><ref type="bibr" target="#b3">Davison et al., 2019;</ref><ref type="bibr" target="#b26">Roberts et al., 2020)</ref>. Hence, how to stimulate and exploit such knowledge is receiving increasing attention.</p><p>One conventional approach to achieve that is fine-tuning <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, where we add extra classifiers on the top of PLMs and further train the models under classification objectives. Fine-tuning has achieved satisfying results on supervised tasks. However, since the extra classifier requires adequate training instances to tune, it is still challenging to apply fine-tuning in few-shot learning <ref type="bibr" target="#b0">(Brown et al., 2020)</ref> and zero-shot learning <ref type="bibr" target="#b38">(Yin et al., 2019)</ref> scenarios. Originated from <ref type="bibr">GPT-3 (Brown et al., 2020)</ref> and LAMA <ref type="bibr" target="#b22">(Petroni et al., 2019</ref><ref type="bibr" target="#b21">(Petroni et al., , 2020))</ref>, a series of studies using prompts <ref type="bibr" target="#b28">(Schick and Schütze, 2020a;</ref><ref type="bibr" target="#b13">Liu et al., 2021)</ref> for model tuning bridge the gap between pre-training objective and down-stream tasks, and demonstrate that such discrete or continuous prompts induce better performances for PLMs on few-shot and zero-shot tasks.</p><p>A typical way to use prompt-tuning is to wrap the input sentence into a natural language template and let the PLM conduct masked language modeling. For instance, to classify the topic of a sentence x: "What's the relation between speed and acceleration?" into the "SCIENCE" category, we wrap it into a template: "A [MASK] question: x". The prediction is made based on the probability that the word "science" is filled in the "[MASK]" token. The mapping from label words (e.g., "science" ) to the specific class (e.g., class SCIENCE) is called the verbalizer <ref type="bibr" target="#b28">(Schick and Schütze, 2020a)</ref>. Verbalizer bridges a projection between the vocabulary and the label space and is proven to have a great influence on the performance of classification <ref type="bibr" target="#b5">(Gao et al., 2020)</ref>.</p><p>Most existing works use human-written verbalizers <ref type="bibr">(Schick and</ref><ref type="bibr">Schütze, 2020a, 2021)</ref>, in which the designers manually think up a single word to indicate each class. However, the humanwritten verbalizers usually determine the predictions based on limited information. For instance, in the above mentioned example, the naive verbalizer {science}→ SCIENCE means that only predicting the word "science" for the <ref type="bibr">[MASK]</ref> token is regarded as correct during inference, regardless of the predictions on other relevant words such as "physics" and "maths", which are also informative. Such handcrafted one-one mapping limits the coverage of label words, thus lacking enough information for prediction and also inducing bias into the verbalizer. Therefore, the handcrafted verbalizers are hard to be optimal in prompt-tuning, where the semantics of label words are crucial for predictions. Some works try to mitigate the disadvantage of handcrafted verbalizer, and propose to search for the best verbalizer(s) using gradient descent <ref type="bibr" target="#b13">(Liu et al., 2021;</ref><ref type="bibr" target="#b27">Schick et al., 2020)</ref> and induce a few words that are similar to the class name in terms of word sense but differ in terms of surface forms. However, such optimization-based expansion is difficult to infer words across granularities (e.g. from "science" to "physics"). If we expand the verbalizer of the above example into {science, physics} → SCIENCE, the probability of predicting the true label will be considerably enhanced. Therefore, to improve the coverage and reduce the bias of the verbalizer we present to incorporate external knowledge into the verbalizers to facilitate prompt-tuning, namely, knowledgeable prompt-tuning (KPT). Since our expansion is not based on optimization, it will be more favorable for zero-shot learning. Specifically, KPT contains three steps: construction, refinement, and utilization. (1) Firstly, in the construction stage, we use external KBs to generate a set of label words for each label (in § 3.2). Note that the expanded label words are not simply synonyms of each other, but covers different granularities and perspectives, thus are more comprehensive and unbiased than the class name. (2) Secondly, we use the PLM itself to denoise the expanded label words. For zero-shot learning, we propose to use a contextualized prior to remove those words with low prior probability. Since the words from the KB can have dramatically different prior probabilities, we propose a robust calibration method, namely, contextualized calibration, to boost the zero-shot performance (in § 3.3). For few-shot learning, we assign a learnable weight to each label word to denoise the knowledgeable verbalizer. (3) Finally, we apply either a vanilla average loss function or a weighted average loss function for the utilization of expanded verbalizers, which map the scores on a set of label words to the scores of the labels.</p><p>We conduct extensive experiments on zero-shot and few-shot text classification tasks. The empirical results show the effectiveness of KPT (in § 4). In addition to the promising improvements than regular prompt-tuning, KPT also reduces the prediction variances in few-shot experiments and yields more stable performances (in § 5). We will make the source code publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>This work focuses on incorporating knowledge into prompt verbalizers. Thus, three groups of research are related to KPT: prompt-tuning, the verbalizer construction, and knowledge-enhanced PLMs. Since we conduct experiments on text classification tasks, we introduce several works of zeroshot and few-shot text classification in § 4.</p><p>Prompt-tuning. Since the emergence of <ref type="bibr">GPT-3 (Brown et al., 2020)</ref>, prompt-tuning has received considerable attention. <ref type="bibr">GPT-3 (Brown et al., 2020)</ref> demonstrates that with prompt-tuning and incontext learning, the large-scale language models can achieve superior performance in the low-data regime. The following works <ref type="bibr">(Schick and Schütze, 2020a,b)</ref> argue that small-scale language models <ref type="bibr" target="#b23">(Radford et al., 2018;</ref><ref type="bibr" target="#b4">Devlin et al., 2019;</ref><ref type="bibr" target="#b14">Liu et al., 2019;</ref><ref type="bibr" target="#b10">Lan et al., 2019)</ref> can also achieve decent performance using prompt-tuning. While most of the researches are conducted on text classification or the tasks in SuperGLUE <ref type="bibr" target="#b34">(Wang et al., 2019)</ref>, some works extend the impact of prompt-tuning into other tasks, e.g., relation extraction <ref type="bibr" target="#b36">(Han et al., 2021;</ref><ref type="bibr" target="#b2">Chen et al., 2021)</ref>. In addition to using prompt-tuning for various down-stream tasks, prompt is also used to probe knowledge from the PLMs <ref type="bibr" target="#b22">(Petroni et al., 2019</ref><ref type="bibr" target="#b21">(Petroni et al., , 2020))</ref>.</p><p>Verbalizer Construction. As introduced in § 1, the verbalizer is an important component in prompttuning, and existing studies have shown that verbalizers have a strong influence on the performance of prompt-tuning <ref type="bibr" target="#b8">(Holtzman et al., 2021;</ref><ref type="bibr" target="#b5">Gao et al., 2020)</ref>. Most works use human-written verbalizers <ref type="bibr" target="#b28">(Schick and Schütze, 2020a)</ref>, which are highly bi- ased towards personal vocabulary and do not have enough coverage. Some other studies <ref type="bibr" target="#b5">(Gao et al., 2020;</ref><ref type="bibr" target="#b31">Shin et al., 2020;</ref><ref type="bibr" target="#b13">Liu et al., 2021;</ref><ref type="bibr" target="#b27">Schick et al., 2020)</ref> design automatic verbalizer searching methods for better verbalizer choices, however, their methods require adequate training set and validation set for optimization. Moreover, the automatically determined verbalizers are usually synonym of the class name, which differs from our intuition of expanding the verbalizer with a set of diverse and comprehensive label words using external KB. <ref type="bibr" target="#b27">Schick et al. (2020)</ref>; <ref type="bibr" target="#b31">Shin et al. (2020)</ref> also try multiple label words for each class. The optimal size of their label words set for each class is generally less than 10. In this work, we propose KPT , which uses external knowledge to boost the performance of prompt-tuning. Compared to the previous strategies, our method can generate and effectively utilize more than 100 related label words across granularities for each class, and can be effectively applied to a zero-shot setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original Input Template</head><p>Knowledge Enhanced PLMs. Using external knowledge to enhance the performance of PLMs has been extensively studied in recent years, and it is usually applied to the pre-training stage <ref type="bibr" target="#b41">(Zhang et al., 2019b;</ref><ref type="bibr" target="#b12">Liu et al., 2020)</ref> and the fine-tuning stage <ref type="bibr" target="#b37">(Yang et al., 2019;</ref><ref type="bibr" target="#b6">Guan et al., 2020)</ref>. Specifically, in text classification tasks, Chen et al. ( <ref type="formula">2019</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Knowledgeable Prompt-tuning</head><p>In this section, we present our methods to incorporate external knowledge into a prompt verbalizer. We first introduce the overall paradigm of prompttuning and then elucidate how to construct, refine and utilize the knowledgeable prompt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>Let M be a language model pre-trained on large scale corpora. In text classification task, an input sequence x = (x 0 , x 1 , ..., x n ) is classified into a class label y ∈ Y. Prompt-tuning formalizes the classification task into a masked language modeling problem. Specifically, prompt-tuning wraps the input sequence with a template, which is a piece of natural language text. For example, assuming we need to classify the sentence x ="What's the relation between speed and acceleration?" into label SCIENCE (labeled as 1) or SPORTS (labeled as 2), we wrap it into</p><formula xml:id="formula_0">x p = [CLS] A [MASK] question : x</formula><p>Then M gives the probability of each word v in the vocabulary being filled in [MASK] token P M ([MASK] = v|x p ). To map the probabilities of words into the probabilities of labels, we define a verbalizer as a mapping f from a few words in the vocabulary, which form the label word set V,</p><p>to the label space Y, i.e., f : V → Y. We use V y to denote the subset of V that is mapped into a specific label y, ∪ y∈Y V y = V. Then the probability of label y, i.e., P (y|x p ), is calculated as</p><formula xml:id="formula_1">P (y|x p )=g P M ([MASK]= v|x p )|v ∈ V y , (1)</formula><p>where g is a function transforming the probability of label words into the probability of the label.</p><p>In the above example, regular prompt-tuning may define V 1 = {"science"}, V 2 = {"sports"} and g as an identity function, then if the probability of "science" is larger than "sports", we classify the instance into SCIENCE.</p><p>We propose KPT, which mainly focuses on using external knowledge to improve verbalizers in prompt-tuning. In KPT , we use KBs to generate multiple label words related to each class y, e.g., V 1 = {"science","physics", ...}. And we propose a contextualized calibration method to eliminate noise in the expanded V. Finally, we explore the vanilla average and weighted average approaches for the utilization of the expanded V. The details are in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Verbalizer Construction</head><p>The process of predicting masked words based on the context is not a single-choice procedure, that is, there is no standard correct answer, but abundant words may fit this context. Therefore, the label words mapped by a verbalizer should be equipped by two attributes: wide coverage and little subjective bias. Such a comprehensive projection is crucial to the imitation of pre-training, i.e., prompttuning. Fortunately, external structured knowledge could simultaneously meet both requirements. In this section, we introduce how we use external knowledge for two text classification tasks: topic classification and sentiment classification.</p><p>For topic classification, the core issue is to extract label words related to the topic from all aspects and granularities. From this perspective, we choose Related Words 1 , a knowledge graph G aggregated from multiple resources, including word embeddings, ConceptNet <ref type="bibr" target="#b33">(Speer et al., 2017)</ref>, WordNet <ref type="bibr" target="#b19">(Pedersen et al., 2004)</ref>, etc., as our external KB. The edges denote "relevance" relations and are annotated with relevance scores which could be used to measure the correlations between label words and topics. We use the name of each topic v as the anchor node to get the neighborhood nodes 1 https://relatedwords.org N G (v) whose scores are larger than a threshold η as the related words. Thus, each class is mapped into a set of label words V y = N G (v) ∪ {v}. For binary sentiment classification, the primary goal is to select as many expressions as possible that tend to be positive or negative. And we use the sentiment dictionary summarized by previous researchers 2,3 . Thus, we get a knowledgeable verbalizer mapping multiple label words to a class label, which enhances the handcrafted verbalizer with external knowledge. Several examples of the label words in the KPT are in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Verbalizer Refinement</head><p>Although we have constructed a knowledgeable verbalizer that contains comprehensive label words, the collected knowledgeable verbalizer can be very noisy since the vocabulary of the KB is not tailored for the PLM. Thus it is necessary to further refine such verbalizer by retaining high-quality words and removing low-relevance words. In this section, we introduce the refinement of verbalizers in zero-shot and few-shot settings.</p><p>Zero-shot Refinement. In zero-shot learning, three problems need to be addressed to facilitate the use of knowledgeable verbalizers. First of all, some of the words recommended by the KB are out-of-vocabulary (OOV) for the PLM, however, these words may also provide information for classification and should not be removed completely. To support the prediction of these words, we simply use the average probability of each token in their tokenizations being filled in the masked position as the probability for these words.</p><p>The second problem is to handle the rare words. We assume that several words in the KB are rare to the PLM, thus the prediction probabilities on these words tend to be inaccurate. Instead of using a word-frequency dictionary, we propose to use contextualized prior of the label words to remove these words. Specifically, given a text classification task, we denote the distribution of the sentences x in the corpus as D. For each sentence in the distribution, we wrap it into the template and calculate the predicted probability for each label word v in the masked position P M ([MASK]= v|x p ). By taking the expectation of the probability over the entire distribution of sentences, we can get the prior dis- tribution of the label words in the masked position. We can formalize it as</p><formula xml:id="formula_2">P D (v) = E x∼D P M ([MASK]= v|x p ). (2)</formula><p>Empirically, we found that using a small-size unlabeled support set C sampled from the training set and with labels removed, will yield a satisfying estimate of the above expectation. Thus, assuming that the input samples {x ∈ C} have a uniform prior distribution, the contextualized prior is approximated by</p><formula xml:id="formula_3">P D (v) ≈ 1 | C| x∈ C P M ([MASK]= v|x p ).</formula><p>(3) Then we remove the label words whose prior probabilities are less than a threshold.</p><p>The third problem is the drastic difference in the prior probabilities of label words. As previous works <ref type="bibr" target="#b42">(Zhao et al., 2021;</ref><ref type="bibr" target="#b8">Holtzman et al., 2021)</ref> have shown, some label words are less likely to be predicted than the others, regardless of the label of input sentences, resulting in a biased prediction. In our setting, the label words in the KB tend to have more diverse prior probabilities. Therefore, we use the contextualized prior of label words to calibrate the predicted distribution, namely, contextualized calibration (CC):</p><formula xml:id="formula_4">PM([MASK]= v|xp) = PM([MASK]= v|xp) PD(v) .<label>(4)</label></formula><p>Compared to the contextual calibration <ref type="bibr" target="#b42">(Zhao et al., 2021)</ref> and PMI DC <ref type="bibr" target="#b8">(Holtzman et al., 2021)</ref>, our method utilizes a small unlabeled support set but yields better and stabler results (see § 5.1).</p><p>Few-shot Refinement. In few-shot learning, the refinement is easier since we can identify each label word's influence on the prediction. After collecting the label words from the KB, we first remove the label words that are split into multiple tokens, since they tend to be more tricky to handle in the training objective. To mitigate the problem of noisy label words, we assign a learnable weight w v to each label word v. The weights form a vector w ∈ R |V| , which is initialized to be a zero vector. The weights are normalized within each V y :</p><formula xml:id="formula_5">α v = exp(w v )</formula><p>u∈Vy exp(w u )</p><p>.</p><p>(5)</p><p>Intuitively, in the training process, a small weight is expected to be learned for a noisy label word to minimize its influence on the prediction. Note that in few-shot setting, we do not conduct calibration since the probability of a label word can be trained to the desired magnitude, i.e., PM ([MASK] = v|x p ) = P M ([MASK]= v|x p ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Verbalizer Utilization</head><p>The final problem is how to map the predicted probability on each refined label word to the decision of the class label y, i.e., the objective function g of the knowledgeable verbalizer. Moreover, in few-shot learning, an additional question is how to optimize the knowledgeable verbalizer.</p><p>Average. After refinement, we can assume that each label word of a class contributes equally to predicting the label. Therefore, we use the average of the predicted scores on V y as the predicted score for label y. The predicted label ŷ is</p><formula xml:id="formula_6">ŷ = argmax y∈Y 1 |Vy| v∈Vy PM([MASK]= v|xp) . (6)</formula><p>We use this method in zero-shot learning since there is no parameter to be trained.</p><p>Weighted Average. In few-shot text classification, we adopt a weighted average of label words' scores as the prediction score. We use the refinement weights α i as the weights for averaging. Thus, the predicted label ŷ is</p><formula xml:id="formula_7">ŷ = argmax y∈Y exp s(y|x p ) y exp s(y |x p ) ,<label>(7)</label></formula><p>where s(y|x p ) is</p><formula xml:id="formula_8">s(y|x p ) = v∈Vy α v log P M ([MASK]= v|x p ). (8)</formula><p>This objective function is suitable for continuous optimization by applying a cross-entropy loss on the predicted probability.</p><p>We evaluate KPT on four text classification datasets to demonstrate the effectiveness of incorporating external knowledge into prompt-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Templates</head><p>We carry out experiments on two topic classification datasets: AG's News <ref type="bibr" target="#b40">(Zhang et al., 2015)</ref> and DBPedia <ref type="bibr" target="#b11">(Lehmann et al., 2015)</ref>, and two sentiment classification datasets: IMDB <ref type="bibr" target="#b15">(Maas et al., 2011)</ref> and Amazon <ref type="bibr" target="#b16">(McAuley and Leskovec, 2013)</ref>.</p><p>The statistics of the datasets are shown in Table <ref type="table" target="#tab_1">2</ref>. The detailed information is in Appendix A. Due to the rich expert knowledge contained, the manual templates are proven to be competitive with or better than auto-generated templates <ref type="bibr" target="#b5">(Gao et al., 2020)</ref> even though they are simpler to be constructed. Therefore we use manual templates in our experiments. Manual templates are also more applicable than auto-generated templates in the zero-shot setting. To mitigate the influence of different templates, we test KPT under multiple templates for each dataset. Specifically, we use four manual templates for each dataset which are either introduced by <ref type="bibr" target="#b28">(Schick and Schütze, 2020a)</ref> or tailored to fit the dataset. We report both the average results of the four templates and the results of the best template. The specific templates we use for each dataset are in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment Settings</head><p>For the PLM, we use RoBERTa large <ref type="bibr" target="#b14">(Liu et al., 2019)</ref> for all experiments. For test metrics, we use Micro-F1 in all experiments. We have different settings for zero-shot and few-shot experiments.</p><p>Zero-shot Experiments. The size of the unlabeled support set | C| is 200. For topic classification, threshold for removing rare words are 0.5. For the sentiment classification dataset, we find that our sentiment dictionary is of high quality, thus we do not remove the words based on prior probability. Since the choices of C will influence the test performance, we repeat 5 times of each experiment in KPT and PT+CC using different random seeds.</p><p>Few-shot Experiments. We conduct 5, 10 and 20-shot experiments. For a k-shot experiment, we sample k instances of each class from the original training set to form the few-shot training set and sample another k instances per class to form the validation set. We tune the entire model for 5 epochs and choose the checkpoint with the best validation performance to test. Since the different choices of the few-shot training set and validation set affect the test performance heavily, we repeat the experiments on 5 random seeds.</p><p>Other hyper-parameters for tuning the Roberta model can be found in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>In this subsection, we introduce the baselines we compare with, including the regular prompttuning, prompt-tuning combined with contextualized calibration, and fine-tuning. We also include the reported scores of LOTClass and UDA since they are state-of-the-art of unsupervised and semisupervised text classification. However, they use much more training resource than KPT , which may lead to unfair comparisons.</p><p>Prompt-tuning (PT). The regular prompttuning method wraps an input sentence into a handcrafted template. Different from KPT , it uses the class name as the only label word for each class, which is adopted by PET and most existing works. Note that PET uses several other tricks such as self-training, prompt ensemble, etc. We do not use any of these tricks since we want to study the effect of knowledgeable verbalizers alone. These tricks are orthogonal to our contributions and can be combined into ours in future work.</p><p>Prompt-tuning + Contextualized Calibration (PT + CC). This approach is the prompt-tuning combined with the proposed contextualized calibration. We use the same unlabeled support set as KPT to calculate the contextualized prior of label words. This baseline is to see how much improvement is made by contextualized calibration instead of knowledgeable verbalizers. In few-shot learning experiments, we do not include this baseline since we find that calibration is less important for few-shot learning.</p><p>Fine-tuning (FT). The traditional fine-tuning method inputs the hidden embedding of [CLS] token of the PLM into the classification layer to make predictions. Note that fine-tuning can not be applied to the zero-shot setting, since the classifica- tion layer is randomly initialized. LOTClass. LOTClass <ref type="bibr" target="#b17">(Meng et al., 2020</ref>) uses a PLM to extract the topic-related words from the whole unlabeled training corpus. Then it uses a Masked Category Prediction task to train on the unlabeled corpus with pseudo labels.</p><p>UDA. UDA <ref type="bibr" target="#b35">(Xie et al., 2019)</ref> uses a small labeled corpus and a large unlabeled corpus. To leverage the unlabeled corpus, they use advanced dataaugmentation methods, such as back-translation, to encourage the consistency of predictions over augmented data samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Main Results</head><p>In this subsection, we introduce the specific results and provide possible insights of KPT .</p><p>Zero-shot. From Table <ref type="table" target="#tab_2">3</ref>, we see that KPT consistently outperforms PT and PT+CC baselines, which indicates the effectiveness of our methods. We achieve superior performance to LOTClass either with average performance of all templates or the best-performance template, even though we do not leverage the large unlabeled training set. Specifically, we observe that the performance boost compared to the baselines in topic classification is higher than sentiment classification, which we conjecture that topic classification requires more external knowledge than sentiment classification. While CC offers huge improvement over PT baseline, the incorporation of external knowledge improves over PT+CC up to 7.4 on DBPedia.</p><p>Few-shot. From Table <ref type="table" target="#tab_4">5</ref>, we find KPT consistently outperforms baseline method PT, especially in 5-shot and 10-shot experiments. For 20-shot, we hypothesis that the number of labeled instances is enough to optimize the label words' embeddings away from their original word embeddings so that the rich semantics in the knowledgeable verbalizer may bring less assistance. However, KPT still achieves improvement on three datasets. From the table, we can see that FT is highly unstable in lowshot regime, but with enough data, e.g., 280 data points in total for DBPedia, it is superior to the best template of PT. However, KPT still compares favorably to FT under this setting. Another notable feature of KPT is that it achieves significantly low variances compared with the baseline methods. It is probably because the ensemble of different label words provides a more stable training target. Compared with UDA, although we use significantly less training resource, we are superior to them on AG's News and IMDB. Table <ref type="table">4</ref>: The results of KPT using CC in 10-shot learning. The ∆ column shows the differences between the model using CC and the model not using CC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>In this section, we conduct several ablation studies including the effect of contextualized calibration and the diversity of predicted label words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Effect of Contextualized Calibration on</head><p>Zero-shot Learning Existing methods propose Domain Conditional PMI <ref type="bibr" target="#b8">(Holtzman et al., 2021)</ref>   probability of label words predicted in the [MASK] position given the raw template without filling the template with the instances in the corpus. To compare our method with PMI DC and further assess how many instances are needed to yield a satisfying calibration, we draw the impact of the unlabeled support set's size | C| on the test performance in Figure <ref type="figure" target="#fig_2">2</ref>, and draw the performance of PMI DC at | C| = 0 for comparison. From Figure <ref type="figure" target="#fig_2">2</ref>, we find that | C| ∼ 100 is enough to yield a satisfying calibration, and utilizing such a small unlabeled support set produces much better results than PMI DC .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Is Calibration Important for Few-shot</head><p>Learning?</p><p>Although calibration is crucial for the zero-shot setting, we do not perform calibration for the fewshot setting because we assume that the posterior probability of the label words can be trained to the desired magnitude with only a few training instances. To verify the assumption empirically, we try a 10-shot classification with contextualized calibration. The results and the gap between the methods with and without calibration are reported in Table <ref type="table">4</ref>, which indicate that contextualized calibration has little impact in the few-shot scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Diversity of Top Predicted Words</head><p>One advantage of KPT is that it can generate diverse label words across different granularities. To specifically quantify such diversity, we conduct a case study. For the correctly predicted sentences of a class y, we count the frequency of label words v ∈ V y appearing in the top-5 predictions for the [MASK] position. Then we report the top-15 frequent label words in Figure <ref type="figure" target="#fig_3">3</ref>. Due to space limit, only the results of POLITICS and SPORTS category of AG's News are shown. As shown in Figure <ref type="figure" target="#fig_3">3</ref>, a diversity of label words, instead of mainly the original class names, are predicted. And the predicted label words cover various aspects of the corresponding topic. For example, for the topic POLITICS, the predicted "diplomatic", "republic", "parliament" are related to it from different angles.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose KPT , which expands the verbalizer in prompt-tuning using the external KB. To better utilize the KB, we propose refinement methods for the knowledgeable verbalizer. The experiments show the potential of KPT in both zero-shot settings and few-shot settings. For future work, there are open questions related to our research for investigation. (1) Sophisticated ways to select the informative label words in the verbalizers. (2) Better approaches for combining KB and prompt-tuning in terms of template construction and verbalizer design. (3) Incorporating external knowledge into prompt-tuning for other tasks such as text generation. We are looking forward to more novel works in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Datasets and Templates</head><p>In this section, we introduce the information of datasets and the templates we use in detail.</p><p>AG's News. AG's News is a news' topic classification dataset. In this dataset, we follow PET <ref type="bibr" target="#b28">(Schick and Schütze, 2020a)</ref> to design the templates. However, their best performance pattern T 1 (x) = "[MASK] news : x" requires the [MASK] token to be capitalized, which is not suitable for the label words in KB. And some of their templates are not informative and yield low performances. Therefore, we define four slightly changed templates:</p><formula xml:id="formula_9">T 1 (x) = A [MASK] news : x T 2 (x) = x This topic is about [MASK]. T 3 (x) = [ Category : [MASK] ] T 4 (x) = [ Topic : [MASK] ] x DBPedia.</formula><p>In a DBPedia sample, we are given a paragraph b paired with a title a, in which the title is the subject of paragraph. The task is to determine the topic (or the type) of the subject. Different from other topic classifications, the paragraph can emphasize topics that are different from the title. For example, in a paragraph about an audio company, the main paragraph talks about music, albums, etc., but the correct label is "company" rather than "music". Therefore, we define the following templates: where ã means removing the last punctuate in the title.</p><formula xml:id="formula_10">T 1 (a, b) = a b ã is a [MASK] .</formula><p>IMDB. IMDB is a sentiment classification dataset about movie reviews.</p><p>Similar to the template defined in <ref type="bibr" target="#b28">(Schick and Schütze, 2020a)</ref> for sentiment classification, we define the following template: Amazon. Amazon is another sentiment classification dataset , we define the following template: Since the test set of amazon is unnecessarily large for efficient testing, we randomly sample 10,000 samples from the 400,000 test samples to test, which is proven to have tiny influence on the performance in our pilot experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experimental Settings</head><p>We list the hyper-parameters in Table <ref type="table" target="#tab_6">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Detailed Results</head><p>To have a close look at the performance of KPT and baselines on each template, we report the performance of each template on zero-shot and 10-shot experiments in Table <ref type="table">7</ref>. Table <ref type="table">7</ref>: Results of each template in zero-shot and 10-shot text classification.Not that the variances are small within different choices of random seeds for the same template in zero-shot learning. In order not to let the differences across the templates dominate the variances, in the "Avg" row of zero-shot classification, the variance is the average of the variances of different templates, instead of the variance of all experiments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FilteringFigure 1 :</head><label>1</label><figDesc>Figure1: The illustration of KPT , the knowledgeable verbalizer maps the predictions over label words into labels. And the above part is the construction, refinement and utilization processes of KPT .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>);<ref type="bibr" target="#b39">Zhang et al. (2019a)</ref>;<ref type="bibr" target="#b32">Sinoara et al. (2019)</ref> also explore utilizing KBs to enhance the input text. Different from these methods, KPT incorporates external knowledge in the prompt-tuning stage and yields remarkable improvements in zero-shot and few-shot text classification tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Size of unlabeled support set C w.r.t. test performance. The points at | C| = 0 are the performances of PMI DC .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Frequent words appearing in the top-5 predictions. The results for two classes: POLITICS (left) and SPORTS (right) are drawn.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>T 2</head><label>2</label><figDesc>(a, b) = a b In this sentence, ã is a [MASK] . T 3 (a, b) = a b The type of ã is [MASK]. T 4 (a, b) = a b The category of ã is [MASK].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>T 1</head><label>1</label><figDesc>(x) = It was [MASK] .x T 2 (x) = Just [MASK] ! x T 3 (x) = x All in all, it was [MASK].T 4 (x) = x In summary, the film was [MASK].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>T 1</head><label>1</label><figDesc>(x) = It was [MASK] .x T 2 (x) = Just [MASK] ! x T 3 (x) = x All in all, it was [MASK].T 4 (x) = x In summary, it was [MASK]".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>NewsPOLITICS politics, government, diplomatic, law, aristotle, diplomatical,  governance, ... SPORTS sports, athletics, gymnastics, sportsman, competition, cycling, soccer, ... IMDB NEGATIVE abysmal, adverse, alarming, angry, annoy, anxious, apathy, appalling, ... POSITIVE absolutely, accepted, acclaimed, accomplish, accomplishment, ... Examples of the expanded label words. In topic classification (e.g. AG's News), they are expanded by knowledge graphs, and in sentiment classification (e.g. IMDB), they are expanded by sentiment dictionary.</figDesc><table><row><cell>Dataset</cell><cell>Label</cell><cell>Label Words</cell></row><row><cell>AG's</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The statistics of each dataset.</figDesc><table><row><cell>Name</cell><cell>Type</cell><cell cols="2"># Class Test Size</cell></row><row><cell>AG's News</cell><cell>Topic</cell><cell>4</cell><cell>7600</cell></row><row><cell>DBPedia</cell><cell>Topic</cell><cell>14</cell><cell>70000</cell></row><row><cell>Amazon</cell><cell>Sentiment</cell><cell>2</cell><cell>10000</cell></row><row><cell>IMDB</cell><cell>Sentiment</cell><cell>2</cell><cell>25000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results of zero-shot text classification. Average results and the variances of four templates are shown. The results of the best templates are shown in the brackets. Note that for PT+CC and KPT, we repeat each experiment five times using different random seeds. †means they use different training resources than our setting.</figDesc><table><row><cell>Method</cell><cell>AG's News</cell><cell>DBPedia</cell><cell>Amazon</cell><cell>IMDB</cell></row><row><cell>LOTClass †</cell><cell>82.2</cell><cell>86.0</cell><cell>85.3</cell><cell>80.2</cell></row><row><cell>PT</cell><cell cols="4">75.1 ± 6.2 (79.1) 67.4 ± 3.6 (71.1) 80.5 ± 9.3 (88.2) 86.4 ± 4.2 (92.5)</cell></row><row><cell>PT + CC</cell><cell cols="4">80.0 ± 0.8 (81.1) 75.1 ± 5.4 (82.3) 91.1 ± 1.8 (93.7) 90.6 ± 3.1 (93.7)</cell></row><row><cell>KPT</cell><cell cols="4">83.0 ± 1.7 (85.9) 82.5 ± 4.4 (87.2) 92.5 ± 1.3 (94.7) 91.5 ± 3.0 (94.2)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>(PMI DC ) to calibrate the distribution, which directly measures the prior</figDesc><table><row><cell cols="2">shot Method</cell><cell>AG's News</cell><cell>DBPedia</cell><cell>Amazon</cell><cell>IMDB</cell></row><row><cell></cell><cell>UDA †</cell><cell>86.4</cell><cell>98.6</cell><cell>96.0</cell><cell>88.7</cell></row><row><cell></cell><cell>FT</cell><cell>37.9 ± 10.0</cell><cell>95.8 ± 1.3</cell><cell>52.1 ± 1.3</cell><cell>51.4 ± 1.4</cell></row><row><cell>5</cell><cell>PT</cell><cell cols="4">83.8 ± 3.1 (85.7) 96.5 ± 0.7 (96.8) 92.8 ± 2.0 (94.6) 92.1 ± 2.4 (94.2)</cell></row><row><cell></cell><cell>KPT</cell><cell cols="4">85.3 ± 0.9 (85.9) 97.2 ± 0.6 (97.4) 93.3 ± 2.0 (94.6) 92.5 ± 2.4 (94.3)</cell></row><row><cell></cell><cell>FT</cell><cell>75.9 ± 8.4</cell><cell>93.8 ± 2.2</cell><cell>83.0 ± 7.0</cell><cell>76.2 ± 8.7</cell></row><row><cell>10</cell><cell>PT</cell><cell cols="4">86.3 ± 1.8 (86.5) 97.1 ± 0.8 (97.5) 94.2 ± 1.2 (94.6) 92.8 ± 1.2 (93.8)</cell></row><row><cell></cell><cell>KPT</cell><cell cols="4">87.2 ± 0.9 (87.5) 98.0 ± 0.3 (98.1) 94.4 ± 1.1 (94.8) 93.3 ± 0.7 (93.6)</cell></row><row><cell></cell><cell>FT</cell><cell>85.4 ± 1.8</cell><cell>97.9 ± 0.2</cell><cell>71.4 ± 4.3</cell><cell>78.5 ± 10.1</cell></row><row><cell>20</cell><cell>PT</cell><cell cols="4">87.2 ± 1.8 (88.4) 97.5 ± 0.4 (97.6) 94.6 ± 0.9 (94.9) 93.9 ± 1.0 (94.7)</cell></row><row><cell></cell><cell>KPT</cell><cell cols="4">87.4 ± 0.9 (88.0) 98.0 ± 0.2 (98.1) 95.0 ± 0.4 (95.3) 93.8 ± 1.4 (94.5)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Results of few-shot text classification. Average Micro-F1 scores and variances using four templates are shown. The Micro-F1 scores of the best templates are shown in the brackets. Note that each experiment is repeated five times using different random seeds. †means they use more training resource than our setting.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>. Most of the hyper-parameters are the default parameters from Huggingface Transformers 4 . Hyper-parameter settings.</figDesc><table><row><cell>Hyper-parameter</cell><cell>Value</cell></row><row><cell cols="2">maximum sequence length 512</cell></row><row><cell>warmup steps</cell><cell>500</cell></row><row><cell>learning rate</cell><cell>3e-5</cell></row><row><cell>maximum epochs</cell><cell>5</cell></row><row><cell>adam epsilon</cell><cell>1e-8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>± 0.3 82.3 ± 0.6 90.8 ± 0.5 86.6 ± 0.1 3 80.3 ± 0.3 67.5 ± 0.3 90.7 ± 1.7 93.4 ± 0.5 4 79.3 ± 0.2 74.3 ± 0.2 93.7 ± 0.3 93.7 ± 0.3 Avg 80.0 ± 0.8 75.1 ± 0.4 91.1 ± 0.7 90.6 ± 0.3 KPT 1 85.9 ± 0.1 86.5 ± 0.2 91.6 ± 0.0 91.0 ± 0.1 2 82.3 ± 0.4 87.2 ± 0.4 91.4 ± 0.1 87.0 ± 0.2 3 82.5 ± 0.3 78.8 ± 0.2 92.2 ± 0.5 93.8 ± 0.3 4 81.4 ± 0.3 77.8 ± 0.5 94.7 ± 0.1 94.2 ± 0.1 Avg 83.0 ± 0.3 82.5 ± 0.3 92.5 ± 0.2 91.5 ± 0.2</figDesc><table><row><cell cols="3">Shot Method Template ID Agnews</cell><cell>DBPedia</cell><cell>Amazon</cell><cell>IMDB</cell></row><row><cell></cell><cell>1</cell><cell cols="3">81.1 ± 0.2 76.3 ± 0.6 89.2 ± 0.3 88.8 ± 0.1</cell></row><row><cell cols="5">0 79.3 10 PT+CC 2 PT 1 86.0 ± 2.7 97.1 ± 0.9 94.4 ± 1.2 93.2 ± 0.6 2 85.1 ± 2.0 97.5 ± 0.6 93.6 ± 1.7 91.2 ± 0.5 3 86.9 ± 1.1 97.3 ± 0.7 94.6 ± 0.5 93.2 ± 0.8 4 86.9 ± 1.1 96.7 ± 1.1 94.1 ± 1.0 93.8 ± 0.6 Avg 86.3 ± 1.8 97.1 ± 0.8 94.2 ± 1.2 92.8 ± 1.2</cell></row><row><cell></cell><cell>1</cell><cell cols="3">87.4 ± 0.6 97.8 ± 0.3 94.5 ± 1.6 93.5 ± 0.4</cell></row><row><cell></cell><cell>2</cell><cell cols="3">87.5 ± 0.7 97.9 ± 0.2 93.8 ± 1.3 92.7 ± 0.3</cell></row><row><cell>KPT</cell><cell>3</cell><cell cols="3">86.6 ± 1.1 98.0 ± 0.3 94.8 ± 0.5 93.6 ± 0.4</cell></row><row><cell></cell><cell>4</cell><cell cols="3">87.1 ± 1.1 98.1 ± 0.2 94.5 ± 0.6 93.5 ± 1.0</cell></row><row><cell></cell><cell>Avg</cell><cell cols="3">87.2 ± 0.9 98.0 ± 0.3 94.4 ± 1.1 93.3 ± 0.7</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">https://www.enchantedlearning.com/ wordlist/positivewords.shtml</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">https://www.enchantedlearning.com/ wordlist/negativewords.shtml</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">https://huggingface.co/transformers/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep short text classification with knowledge powered attention</title>
		<author>
			<persName><forename type="first">Jindong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghua</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyun</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
				<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6252" to="6259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Adaprompt: Adaptive promptbased finetuning for relation extraction</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahuan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07650</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Commonsense knowledge mining from pretrained models</title>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
				<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15723</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A knowledge-enhanced pretraining model for commonsense story generation</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="93" to="108" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.11259</idno>
		<title level="m">Ptr: Prompt tuning with rules for text classification</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vered</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08315</idno>
		<title level="m">Surface form competition: Why the highest probability answer isn&apos;t always right</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Text classification algorithms: A survey</title>
		<author>
			<persName><forename type="first">Kamran</forename><surname>Kowsari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiana</forename><forename type="middle">Jafari</forename><surname>Meimandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mojtaba</forename><surname>Heidarysafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjana</forename><surname>Mendu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">150</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Isele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anja</forename><surname>Jentzsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Kontokostas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Hellmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Morsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Van Kleef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
		<title level="m">Dbpedia-a large-scale, multilingual knowledge base extracted from wikipedia. Semantic web</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="167" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">K-bert: Enabling language representation with knowledge graph</title>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiruo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
				<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2901" to="2908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10385</idno>
		<title level="m">Gpt understands, too</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hidden factors and hidden topics: understanding rating dimensions with review text</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of RecSys</title>
				<meeting>RecSys</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Text classification using label names only: A language model self-training approach</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ms marco: A human generated machine reading comprehension dataset</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoCo@ NeurIPS</title>
				<meeting>CoCo@ NeurIPS</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Wordnet:: Similarity-measuring the relatedness of concepts</title>
		<author>
			<persName><forename type="first">Ted</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Michelizzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
				<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="25" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
				<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">How context affects language models&apos; factual predictions</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Automated Knowledge Base Construction</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases?</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2463" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">How much knowledge can you pack into the parameters of a language model?</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5418" to="5426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automatically identifying words that can serve as labels for few-shot text classification</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
				<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5569" to="5578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Exploiting cloze questions for few-shot text classification and natural language inference</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07676</idno>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">It&apos;s not just size that matters: Small language models are also few-shot learners</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.07118</idno>
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">It&apos;s not just size that matters: Small language models are also few-shot learners</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
				<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2339" to="2352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Autoprompt: Eliciting knowledge from language models with automatically generated prompts</title>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.15980</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Knowledge-enhanced document embeddings for text classification</title>
		<author>
			<persName><forename type="first">Roberta</forename><forename type="middle">A</forename><surname>Sinoara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><forename type="middle">G</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Solange</forename><forename type="middle">O</forename><surname>Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="955" to="971" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Conceptnet 5.5: An open multilingual graph of general knowledge</title>
		<author>
			<persName><forename type="first">Robyn</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
				<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Superglue: a stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NeurIPS</title>
				<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3266" to="3280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
		<title level="m">Unsupervised data augmentation for consistency training</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Han</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Zhengyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gu</forename><surname>Yuxian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huo</forename><surname>Yuqi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiu</forename><surname>Jiezhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Wentao</surname></persName>
		</author>
		<author>
			<persName><surname>Huang Minlie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07139</idno>
		<title level="m">Pre-trained models: Past, present and future</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Enhancing pre-trained language representations with rich knowledge for machine reading comprehension</title>
		<author>
			<persName><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaoqiao</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2346" to="2357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach</title>
		<author>
			<persName><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamaal</forename><surname>Hay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3914" to="3923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Integrating semantic knowledge to tackle zero-shot text classification</title>
		<author>
			<persName><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyawat</forename><surname>Lertvittayakumjorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yike</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
				<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="page" from="1031" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07129</idno>
		<title level="m">Ernie: Enhanced language representation with informative entities</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Calibrate before use: Improving few-shot performance of language models</title>
		<author>
			<persName><forename type="first">Tony</forename><forename type="middle">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09690</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
