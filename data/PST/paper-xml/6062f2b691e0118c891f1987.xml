<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reducing Load Latency with Cache Level Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-03-27">27 Mar 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Majid</forename><surname>Jalili</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Mattan</forename><surname>Erez</surname></persName>
							<email>mattan.erez@utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Reducing Load Latency with Cache Level Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-03-27">27 Mar 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2103.14808v1[cs.AR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>High load latency that results from deep cache hierarchies and relatively slow main memory is an important limiter of single-thread performance. Data prefetch helps reduce this latency by fetching data up the hierarchy before it is requested by load instructions. However, data prefetching has shown to be imperfect in many situations. We propose cachelevel prediction to complement prefetchers. Our method predicts which memory hierarchy level a load will access allowing the memory loads to start earlier, and thereby saves many cycles. The predictor provides high prediction accuracy at the cost of just one cycle added latency to L1 misses. Experimental results show speedup of 7.8% on generic, graph, and HPC applications over a baseline with aggressive prefetchers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Low memory-load latency is critical for high-performance computing applications. Achieving low load latency is challenging because latency has been trending up as cache hierarchies grow in capacity and complexity. Recent Intel processors, for example, have estimated second-and third-level cache (L2 and L3) latencies of 12 and 40 cycles, respectively <ref type="bibr" target="#b14">[15]</ref>. The levels of the hierarchy are typically looked up in sequence, starting from the first-level cache (L1) and proceeding through second-and third-level caches. If the data is not found in any cache, it is fetched from memory. Deep cache hierarchies generally improve performance, but can result in higher load latencies when caches do not successfully filter requests, only adding lookup delays <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b36">[37]</ref>.</p><p>Prefetchers somewhat mitigate the latency impact of levelby-level lookup by moving data between levels prior to the execution of load instructions <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>. Although prefetchers improve system performance, many loads are still exposed to sequential-lookup delays. Previous work has demonstrated miss coverage of just 24% and 40% for SPEC CPU 2006 <ref type="bibr" target="#b31">[32]</ref> and CloudSuite <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, respectively.</p><p>A naive approach to reduce cache hierarchy latency is to look up caches in parallel. However, this increases energy consumption and requires over-provisioning tag array ports and possibly on-chip bandwidth. Alternatively, the entire hierarchy, tag store, TLBs, and coherence protocol can be completely redesigned to allow Direct-to-Data, lookup-free cache access <ref type="bibr" target="#b25">[26]</ref>. Unlike such costly and radical solutions, we propose a memory hierarchy level predictor that enables directly looking up the cache level where a block resides with minimal changes to the memory hierarchy.</p><p>The level predictor gives us the best of both worlds: the reduced access latency of non-sequential (parallel) lookup while maintaining the low access costs and simplicity of a sequential hierarchy. In this way, level prediction (LP) improves performance for those loads that inevitably miss in the cache despite advanced prefetchers and replacement policies. Our analysis of a large set of benchmarks running on an Intel Skylake processor demonstrates that many applications, including from the graph analytics and scientific computing domains are likely to benefit from non-sequential cache access, even when cache hit rates are high.</p><p>On an L1 miss, the level predictor (within each core) predicts which memory levels to target, bypassing some levels and occasionally indicating partial parallel lookups across memory levels. The bypass and parallel access hardware mechanisms reuse structures that already exist in current sequential lookup implementations, requiring only small modifications to control logic. Any mispredictions that incorrectly bypass a level or that perform unnecessary parallel accesses are also handled with cache controller modifications and reuse existing structures: unnecessary parallel accesses are terminated by modifying existing address matching-logic on the return path, and incorrect bypasses are rectified by the cache-coherence directory reissuing requests to the correct level.</p><p>We propose a novel per-core level predictor comprised of two components: (1) a metadata cache of a global location map (LocMap) that holds possibly-stale information about the level of each memory block, and (2) a compact history-based Popular Level Detector predictor that is used on metadata cache misses. The LocMap requires 2 bits per 512-bit cache block to indicate its memory level. It is organized as a flat table in system-reserved physical memory. It is accessed with the same granularity as the data cache-information for 256 cache blocks is fetched on every LocMap access after a metadata cache miss. We find that just a 2 KiB metadata cache of the LocMap provides a high hit rate while keeping access energy and energy low.</p><p>The compact Popular-Level Detector generates a level prediction when level information is not available in the LocMap metadata cache (a metadata miss). This is required because the long delay of fetching LocMap information would otherwise render the level prediction useless. The history predictor requires just a 32-bit per-level register for counting the number of hits to each level and simple logic that implements a heuristic of combining the counters to generate a level prediction.</p><p>Level prediction is distinct from prior work on cache miss prediction (also referred to as memory access prediction) <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b35">[36]</ref>. While at a high level LP is analogous to simultaneous miss prediction at all cache levels (excluding L1, which has a very short latency), a practical level predictor must be architected from scratch. We find that combining or stacking miss predictors squanders opportunities to utilize cross-level information, necessitating a new predictor design. Notably, prior work did not consider the impact of advanced prefetchers. When combined with better prefetchers, prior miss predictors exhibit poor accuracy and much larger resource requirements. Given the stringent area and power constraints necessary for the gains of level prediction to outweigh its costs, again, a new predictor architecture is required.</p><p>To summarize our main contributions:</p><p>• We demonstrate that many graph analytics and scientific applications benchmarks can benefit from non-sequential lookup; including applications with a high cache hit ratio. • We architect and evaluate an effective, yet low-cost level predictor that operates in each core on the L1 cache miss path and substantially outperforms cache miss predictors when incorportaing prefetchers. • We incorporate level prediction keeping with minimal microarchitectural changes; level misrepdictions are handled by utilizing the cache coherence directory and existing address-matching logic. • We evaluate our proposed method and compare it to an ideal baseline and two state-of-the-art <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b25">[26]</ref>. Overall, level prediction improves the performance of both general, scientific, and graph analytics applications by 7.8% on average over a baseline with aggressive prefetchers. Level prediction also reduces the cache hierarchy energy consumption by 18% on average, as level prediction eliminates many unnecessary miss lookups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MOTIVATION AND BACKGROUND</head><p>Modern processors employ three or more levels of cache. The cache levels are accessed in sequence. The first level cache (L1, the highest level cache) is looked up first, and in the case of a miss the request is forwarded to L2, and then L3 (the lowest level cache). If the data is not found in any cache, it is read from main memory. To provide non-blocking access, each cache has miss status holding registers (MSHRs). MSHRs track outstanding misses, allowing the processor to continue execution while a request is serviced. MSHRs also coalesce different targets to the same cache block.</p><p>Current processors use a distributed directory to maintain coherence. The directory tracks the locations of any block in all on-chip caches to redirect requests and force writebacks and invalidations when necessary. The directory plays a significant role in our design and we rely on it to resolve level mispredictions. Note that directory lookup does not replace cache level prediction. First it is located in or near the LLC tags, and thus far from the L2 cache. Second, the cost of the directory lookup is high and thus not beneficial from both latency and energy perspectives. Each application is plotted according to the ratio of misses filtered by each level compared to the level above it. Applications further to the top and right work best with sequential access, while those toward the bottom left stand to benefit most from non-sequential level-predicted lookups.</p><p>Many applications are memory-latency bound. To study the effectiveness of the level-by-level lookup strategy, we compare the number of misses at different levels. If the application shows good locality or has access patterns that are detected by prefetchers, the number of demand load misses decreases significantly from one level to the next. Our analysis below demonstrates that while sequential level lookup indeed works well for many applications that exhibit this type of behavior, other applications suffer from unnecessary lookups as either L2 does not successfully filter requests, or L3 provides no substantial additional benefit over the L1 and L2 caches.</p><p>The high-level insights from our analysis are summarized in Figure <ref type="figure" target="#fig_0">1</ref>. The figure plots each evaluated application in terms of its L2 and L3 effectiveness-the filtering capability of each level of cache. The x-axis (log scale) is the ratio of L1 to L2 misses and points further on the right indicate applications for which L2 more effectively filters L1 misses from reaching L3, thus indicating that looking up L2 before L3 is the right strategy. Similarly, the y-axis (log scale) represents the effectiveness of L3 and higher points are for applications where misses from L2 are mostly hits in L3. Data on cache effectiveness is collected on a 3.2GHz Intel Core i7-8700 CPU for SPEC CPU 2017 and NAS Parallel Benchmarks applications, the GAPBS graph analytics benchmark suite, and for the hpcg, gups, stream, spmv, and bmt kernels (see Section IV for more details on methodology).</p><p>Using this figure, we roughly classify applications into three categories: (1) applications outside the red box are a good fit for sequential level lookup and are unlikely to benefit from level prediction, (2) applications inside the green box, for which non-sequential lookup is likely to offer significant latency reductions, and (3) applications between the boxes where we expect that L2, L3, or levels can be occasionally bypassed for modest performance gains. We observe that not only graph analytics applications exhibit poor cache effectiveness, but that many other applications are likely to benefit from level prediction. We provide a more detailed analysis below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Sequential Lookup Effectiveness</head><p>Figure <ref type="figure" target="#fig_1">2</ref> provides more detail on the number of misses at each cache level across the execution duration of several applications. We expect sequential level lookup to be the best design choice when caches reduce the number of misses significantly. Figure <ref type="figure" target="#fig_1">2</ref>(a) exemplifies such behavior and shows that for hpcg (which falls outside the red box of Figure <ref type="figure" target="#fig_0">1</ref>), the number of misses significantly decreases after L2 (3× reduction) and also after L3 (a further 2× decrease). This behavior is consistent throughout execution, meaning serial lookup performs well over the course of execution.</p><p>Many other applications, however, suffer from serial lookup. Figures <ref type="figure" target="#fig_1">2(b-f</ref>) show applications where either L2, L3, or both are not effective. If the miss rate at any level is very high, the access latency increases unnecessarily by looking up that level. This is especially a problem when an intermediate level, has a very high miss rate. This behavior is common in graph applications, which frequently exhibit poor hit rates at L2 and moderate hit rates at L3 <ref type="bibr" target="#b2">[3]</ref>. For example, for GAPBS Triangle Count (tc in Figure <ref type="figure" target="#fig_1">2(b</ref>)), we observe a similar rate of L2 and L1 misses, indicating that L2 is ineffective. L3 only moderately reduces the number of misses. Therefore, almost all L2 accesses and the majority of L3 lookups are redundant and only increase memory access latency.</p><p>A case where L3 cache is ineffective is shown in Figure <ref type="figure" target="#fig_1">2</ref>(c) where the number of misses at L3 is roughly the same as at L2, despite the fact that L3 is 48× larger.Considering the fact that the L3 is large and has a high access latency, levelby-level lookup only squanders CPU cycles by looking up L3 for every single access. This problem can be exacerbated when the application has a random access pattern as in gups (Figure <ref type="figure" target="#fig_1">2(d)</ref>). Generally, random behavior impairs both prefetchers and caches, and thus almost all references to caches waste cycles that could otherwise be spent directly looking up main memory.</p><p>While it intuitively seems that sequential accesses should be the best choice for applications with simple access patterns, that is not necessarily the case. For example, Figure <ref type="figure" target="#fig_1">2</ref>(e) shows that despite both L2 and L3 reducing the misses for an easyto-prefetch application like 619.lbm, the number of misses at each level is still high. Meaning a substantial fraction of the memory requests still needs to traverse the memory hierarchy level-by-level wasting many cycles.</p><p>Application behavior may change during execution, and thus using a static strategy to lookup the caches is not optimal. Figure <ref type="figure" target="#fig_1">2</ref>(f) shows the miss rates for 602.gcc from SPEC CPU 2017. L2 is not very effective at the early stages of execution (0-25 seconds), is beneficial in filtering out requests from 20-400 seconds, and decreases to lower effectiveness for the rest of execution. The hardware level predictor can exploit this phase-dependent behavior and skip looking up levels of the hierarchy (L2 in this case) when they do not provide benefit. Prefetchers. Despite progress in designing prefetchers, many misses are left uncovered. Figure <ref type="figure">3</ref> shows the simulated coverage and accuracy of numerous state-of-the-art academic prefetchers <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>. As a matter of fact, in the best-case scenarios, prefetchers can eliminate up to 50% of LLC misses, meaning many accesses still access slow main memory. The average accuracy is also low (50%-60%), meaning many unnecessary blocks are fetched and evicted with a possible negative impact on performance. We offer a new mechanism that complements advanced prefetchers and replacement policies, rather than replacing them. We attempt to handle those misses that even state-of-the-art prefetchers leave for main memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. LEVEL PREDICTION MICROARCHITECTURE</head><p>A. Predictor Design Considerations Levels to Predict. Without loss of generality, assume that there are four memory hierarchy levels, and any given block can reside in L1, L2, L3, and main memory. Out of these, we exclude L1 as a prediction target for two reasons. First, many processors use virtually-indexed physically-tagged (VIPT) L1 caches (L1 and TLB are accessed concurrently), meaning the physical address needed for skipping L1 is not available during the L1 lookup itself. Second, the L1 is tightly integrated with the core pipeline, and we choose not to disrupt its timing or design. We do include L2 as a level prediction target because our evaluation results indicate that skipping L2 lookups offers substantial speedup in many of the applications that benefit from level prediction. Additionally, our level predictor is simple and scalable, and thereby can be simply extended to predict more levels, if required. Resource and power constraints. A level predictor is integrated with each and is queried on every L1 miss. As a result, it must be area-efficient to be realistically implemented and operate with very low power to provide benefits even when skipping the fairly low-power L2 lookup operation. Our analysis shows that larger predictors increase energy consumption higher than the baseline, even if the prediction accuracy is high. Hence, we target predictor structures that are small and lower power. Level Prediction Approach. A level predictor must determine whether to skip L2 lookup, L3 lookup, or both. While, this is distinct from prior work on miss predictors, our LP architecture is inspired by such work. We group miss predictors into three categories: PC + miss history <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b35">[36]</ref>, address + miss history <ref type="bibr" target="#b28">[29]</ref>, and a "miss map" approach <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b22">[23]</ref>.</p><p>The miss-history predictors rely on classic binary predictors (e.g., TAGE <ref type="bibr" target="#b27">[28]</ref>) to predict the level of a cache block based on its observed hit/miss history. However, LP is conceptually more straightforward because the location of a cache block is available at any time within the tag arrays. The miss-map approach, therefore, uses a table <ref type="bibr" target="#b19">[20]</ref> or a Bloom filter <ref type="bibr" target="#b22">[23]</ref> to track which blocks are in a specific cache.</p><p>One approach for LP is to "stack" separate L2 and L3 miss predictors. However, after extensive experimentation, we conclude that this stacking approach performs poorly with both lower accuracy and high power consumption than our proposed level predictor (detailed analysis in Section IV). Instead, it is beneficial to extend miss prediction to true level </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ratio</head><p>Coverage Accuracy</p><p>Figure <ref type="figure">3</ref>: Coverage and accuracy of LLC prefetchers. Coverage is the fraction of misses eliminated by prefetching, and accuracy is the fraction of useful prefetches. In the best cases (e.g., DCPT <ref type="bibr" target="#b12">[13]</ref>), 40% of misses are eliminated but with high overhead of 40% inaccuracy. High inaccuracy of prefetchers makes level prediction very challenging.</p><p>prediction by combining multi-level information in a single structure and tracking per-level history and level location, rather than single-level history and presence. Specifically, we enhance the TAGE-based address+history miss predictor of Sim et al. <ref type="bibr" target="#b28">[29]</ref>. We replace each TAGE counter entry with three counters in each entry to keep track of each level (L2, L3, and main memory). On prediction, if an entry is found in a TAGE table, we use a Popular Levels Detector heuristic that uses the three counters to predict a cache level; we describe this heuristic in greater detail later in this section. If an entry is not found in any TAGE table, we follow a level-by-level traversal.</p><p>Our design-space exploration identifies several additional crucial issues with all prior designs, even when extended to LP. We find that the MissMap approach requires tables that are too large for the tight resource budget of the LP while the historybased predictors work poorly in the presence of advanced prefetchers, unless significant additional storage resources are made available. Impact of Prefecthers. One important reason for the poor performance of stacked miss predictors is the presence of advanced prefetchers. While prior publications report very high prediction accuracy for a range of history-based PC-indexed <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b35">[36]</ref> and address-indexed <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b28">[29]</ref> miss predictors, prior evaluations did not include sophisticated prefetchers. We find that prefetchers add "noise" to the hit/miss history and substantially degrade history-based predictor accuracy. We evaluate a range of predictors with and without advanced prefetchers and find that prefetchers necessitate much larger prediction structures.</p><p>Furthermore, there is an opportunity for coordinating the prefetcher and level predictor: the prefetcher may update the level predictor about data movement within the hierarchy to improve prediction accuracy. We add such updates to both our LP-extensions of prior miss predictors and to our own LP design described below. We find that even with such updates, history-based predictors perform poorly because either the additional histories from prefetches or the additional entries introduced by prefetch updates overwhelm the prediction tables at sizes that are reasonable for the tight LP resource constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Cache Level Predictor Microarchitecture</head><p>Given the level prediction considerations described above, we opt for a novel LP microarchitecture that extends the MissMap <ref type="bibr" target="#b19">[20]</ref> approach in three critical ways (Figure <ref type="figure" target="#fig_2">4</ref>). First, we extend the MissMap to a location map (LocMap), which provides the location of each block (L2, L3, or MEM), requiring 2 bits of metadata per block. Second, the original MissMap is implemented as a cache and loses all information about a block when a MissMap entry is evicted. We find that this either requires a very large MissMap table or leads to low LP accuracy. Instead, we implement the LocMap as an inmemory table containing location information for every block in physical memory, which is cached in a small metadata cache. The long-term location information is then available even after an eviction. Third, because the metadata cache is small and full LocMap access has high latency, we add a small history-based Popular Levels Detector that provides fast level prediction on a metadata cache miss. This history predictor requires just three counters.</p><p>The level predictor is attached to the L2 bus and communicates with the L2 and L3 caches, which report fill and dirty eviction events to the level predictor. These events are used to update the LocMap. In addition, the three counters of the Popular Levels Detector receive hit and miss signals from the caches. We describe the components and their operation below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. LocMap and Location Tracking</head><p>The LocMap is a flat table in system-reserved physical memory and is accessed with the same granularity as the data cache. The LocMap holds the level information of all memory blocks using 2 bits of metadata (there are 3 possible levels to predict: L2, LLC and main memory). Each 64-bit LocMap entry holds location information of 32 blocks. To provide fast access to LocMap, hot metadata is cached on-chip. For 64B cache blocks, this metadata scheme incurs only 2 512 = 0.39% overhead. LocMap Access. The LocMap is accessed (through the metadata cache) on every L1 cache miss, and when it is updated. Each block in physical memory is mapped to an entry in LocMap. Hence, to access the LocMap, we need to generate an address from the block's physical address. To do so, we employ a simple one-to-one mapping. We assume that the base address to the LocMap table is set by the operating system and that the memory access granularity and cache blocks are 64B. Each 64B cache block requires 2 bits in the LocMap such that information for 256 cache blocks fits into a single 64B block of the LocMap (matching the memory access and cache granularity). Hence, the memory address corresponding to a LocMap entry isLocMap Address = Base Address + Physical Address &gt;&gt; 14. This physical address of a LocMap block is first looked up in the per-core LocMap metadata cache, which is filled on a miss through the data cache hierarchy and main memory. LocMap Update. Level prediction does not need to be 100% accurate. Hence, we can carefully trade accuracy for powerefficiency. This can be achieved by updating the catalog on certain events. We update the catalog only on demand cache fills, dirty evictions, and prefetch fills that are metadata cache hits. Thus, the LocMap may hold possibly-stale information because it is not updated on all events happening in the cache. Prefetch fills that are metadata cache misses do not update the LocMap because the traffic this would incur with our aggressive prefetchers is substantial and not worth improving the already-high prediction accuracy (see Section V). Because of the aggressive prefetchers, there are frequent clean evictions and these do not update the LocMap as well. Finally, to avoid changes to the coherence protocol and actions, coherenceinduced level changes (i.e., invalidations) are also ignored. Again, staleness is tolerable because the predictor already performs well and because misprediction recovery is inexpensive. Metadata Cache. The sizing of the metadata cache is important. If the miss ratio is too high, the problem is two-fold. First, many off-chip requests are issued to update the LocMap. Second, the prediction accuracy may degrade as we have to rely on the statistical Popular Level Predictor, which is less </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Directory</head><p>Fill Eviction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L3 Cache</head><p>Fill Eviction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L3 Cache</head><p>Fill Eviction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L3 Cache</head><p>Fill Eviction accurate than the LocMap. At the same time, the size of the LocMap metadata cache is constrained to maintain low access latency and energy-the benefits of level prediction can easily be overwhelmed by an expensive predictor. We show this in the evaluation by demonstrating the detrimental impact of a higher-energy predictor, for example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L3 Cache</head><note type="other">Popular Level Detector</note><p>In order to find a good size for the metadata cache, we measure the average energy for various capacities. We simulate a 4-way 64KB set associative L1 cache along with a 2way metadata cache. We sweep over metadata cache sizes to study the average energy consumption. Figure <ref type="figure" target="#fig_3">5</ref> shows the normalized average energy for sizes of 1KB, 2KB, 4KB, and 8KB for 4 benchmark suites: SPEC CPU 2017, GAPBS, NAS, and other applications (bmt, hpcg, spmv, gups, and stream; annotated as Others). We observe that a 2KB metadata cache provides a reasonable tradeoff between the extra overhead and the coverage provided by the cache. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Popular Levels Detector</head><p>When there is a miss in the metadata cache, it is not possible to wait to fetch the LocMap entry from main memory because this takes longer than the cache lookup itself. One option is to follow a serial access pattern and predict L2 as the location of the block. However, this option is too conservative and does not cover applications with high metadata cache miss ratios such as pr, bc, tc of the GAPBS benchmark suite. Hence, in conjunction with the LocMap, we devise a simple counterbased mechanism to find the most frequently accessed levels and suggest those as the prediction target(s).</p><p>Because the metadata hit rate is relatively high (95% on average across 37 applications), we can be more aggressive on (relatively rare) misses and predict more than one level to increase the prediction accuracy. This is particularly helpful if the counters are not strongly biased toward one level.</p><p>If one level is suggested, we call the prediction a singleway prediction, and a multi-way prediction otherwise. Multiway prediction for uncertain cases increases the prediction accuracy, but requires a more complicated lookup. We use 3 counters, one per cache level and main memory. Upon a hit at a level the corresponding counter is incremented by 1 and others are decremented by 1. This helps to rapidly find popular levels and prevents counter saturation. When a prediction is required, candidates are selected as follows. The counters are sorted and the topmost is picked as the first candidate. If its counter is higher than a threshold, then only this level is selected as the level to look up. Otherwise, the level with the second highest counter is also considered to be a possible destination (two parallel lookup targets). If again the sum of the first and the second counters does not reach a predefined threshold, the third level is also included (three parallel lookup targets). Hence, depending on the counter values the locality predictor may issue single-or multi-way predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Misprediction Detection and Recovery</head><p>Mispredicting a level that is closer to the core than the actual cache level does not require detection and recovery, and is simply a lost opportunity to reduce latency. For example, if the block is only in main memory and LLC is suggested, then not looking up L2 saves cycles, but looking up the LLC wastes CPU cycles; MSHR entries are still allocated along the request path and will be eventually filled as the response arrives. Therefore, this type of misprediction is safe as it does not violate correctness or functionality.</p><p>However, mispredicting a miss and bypassing a level that has more up to date data does require recovery as stale data is fetched. For example, if data is present in L2, but only L3 is suggested by the level predictor, then stale data may be incorrectly fetched from L3. To cope with this problem, we slightly modify the directory controller. Normally, the directory is checked to make sure that private caches of other processors are not holding the data block. When level prediction bypasses L2, we also check the directory, as we may have skipped the private cache. Fortunately, this can be done effectively for free. Recent processors collocate the directory with LLC tags, meaning when the cache is looked up, the information of the directory is also available <ref type="bibr" target="#b33">[34]</ref>. This enables simple misprediction detection.</p><p>Another misprediction type is where main memory is predicted while the block is actually cached. This misprediction is also detected by the directory, which is queried in any case before accessing main memory.</p><p>We change the cache controller to raise a signal when a misprediction is detected. Misprediction recovery first issues a packet to the actual level, requesting to satisfy the pending request and then deallocates all MSHR entries past the actual level. This can be implemented as a new transaction over the shared bus.</p><p>We present examples of detection and recovery schemes from the simplest (one-way correct prediction) to the most complex (multi-way wrong prediction) case below. One-way correct prediction. Figure <ref type="figure" target="#fig_4">6</ref>(a) shows an example for one-way correct prediction. Assume the predicted location is LLC, and the block is present in the LLC (green box). When an L1 miss occurs, an MSHR entry is allocated in L1 and the request is sent to L2 ( 1 ). Then without accessing L2, an MSHR entry (denoting an L2 miss) is allocated in L2. Hence, L2 is bypassed and the request is forwarded to the LLC ( 2 ). Note that allocating an entry in the MSHRs for bypassed levels (in this case L2) is necessary, otherwise it would be impossible to fulfill the request later on the fill path. At the LLC, the tag-store is checked ( 3 ). Since, the block already exists in the cache and the directory confirms that the block is not stale, the LLC responds to the request ( 4 ). The block is sent to L2 and the L2 MSHR is deallocated, and it is filled into L2 ( 5 ). Finally, the block is forwarded to L1 which responds to the CPU request ( 6 ). One-way wrong prediction. Figure <ref type="figure" target="#fig_4">6</ref>(b) shows an example where a block is present in L2, but the predictor suggests LLC. In this example, steps 1 , 2 , and 3 are the same as before; MSHR entries are allocated in L1 and L2 and the request is subsequently sent to the LLC. However, when the tag store is looked up in the LLC, the extended way information indicates that the block is present in L2 4 . Thus, the cache controller sends a new request to L2 to fulfill the request 5 . In L2, the block is found and is forwarded to L1 6 . Also, a signal is raised to deallocate the L2 MSHR entry 7 . The only modification here is to add the ability to cache controller to deallocate the MSHR entry. Multi-way wrong prediction. Figure <ref type="figure" target="#fig_4">6</ref>(b) shows an example where LLC and DRAM have been suggested as targets by the predictor, but the block is present in DRAM. Steps 1 , 2 , and 3 are the same as in both the previous examples. However, when the packet reaches to LLC, both LLC and directory are accessed, and after finding the location of the block which is the main memory, the request is forwarded to the main memory. Finally, when the request comes back, it can follow the normal path that any miss sourced in DRAM can take. Note that in our design the directory and LLC tags are collocated, thus as soon as the tag is accessed, the request is sent to main memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EVALUATION METHODOLOGY A. Simulated System</head><p>We use the gem5 full-system cycle-level simulator to conduct the experiments <ref type="bibr" target="#b9">[10]</ref>. We model a 3-level cache hierarchy where L1 and L2 are inclusive and private and L3 is noninclusive and shared. L1 and L2 are parallel caches where tag and data stores are accessed in parallel with access latencies of 4 and 8 cycles, respectively. L3 is a sequential cache with latencies of 20 cycles and 55 cycles for tag and tag+data, respectively. There is a first-level TLB of 64 entries, a secondlevel TLB with 3072 entries that are equally partitioned between 4KB and 2MB pages (1536 each). The L2TLB is 4-way set associative with a 4-cycles access latency. There are 2 page per core.</p><p>Our goal is to evaluate level prediction with an advanced prefetch scheme. We therefore experimented extensively with a wide range of state-of-the-art prefetchers and their combinations. The highest-performing scheme overall in our experiments uses the DCPT prefetcher <ref type="bibr" target="#b12">[13]</ref> with degree 2 in L3 and tagged next-line prefetchers of degrees 1 and 2 for L1 and L2, respectively. DCPT exhibits the highest coverage and high accuracy (Figure <ref type="figure">3</ref>) and worked well in combination with the L1 and L2 prefetchers. While these next-line prefetchers may appear simplistic, they work remarkably well given the tighter timing and accuracy needs of the smaller cache levels.</p><p>We also find that always enabling these prefetchers significantly degrades system performance for some applications (e.g., 605.mcf) because the prefetchers contend too strongly with demand requests. We therefore implement two prefetch throttling mechanisms. In the first scheme, we reserve 25% of MSHR entries for demand accesses, which decreases the prefetch rate and maintains some minimum demand request service. The second throttling mechanism is that we monitor the performance of the prefetcher periodically and disable a prefetcher when its accuracy drops below 40%. Specifically, in each epoch of 10 million accesses, the prefetchers operate for the first 1 million accesses, then the prefetcher accuracy determines if the prefetcher remains enabled for the following 9 million accesses.</p><p>We simulate out-of-order cores with a fetch width of 4 instructions, 192 ROB entries, and 32-entry store and 32-entry load queues. The frequency of the system is set to 4GHz. We use a single DDR4-2400 x64 channel (one command and address bus), with timings based on a DDR4-2400 8 Gbit datasheet (Micron MT40A1G8) in an 8×8 configuration. Total channel capacity is 16GB. This maintains a reasonable coreto-memory ratio for the simulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Benchmarks</head><p>We evaluate the applications of: (1) SPEC CPU 2017 <ref type="bibr" target="#b30">[31]</ref>,</p><p>(2) GAPBS <ref type="bibr" target="#b8">[9]</ref> (pr, tc, cc, bfs, and bc), (3) NAS (cg, ft, is, mg, and ua) and ( <ref type="formula">4</ref>) bmt, hpcg, stream-copy, and gups. In the evaluation section we report averages for the full benchmark suites, but choose to highlight 21 applications to maintain readability of figures. We pick 12 applications that we expect to highly-benefit from level prediction (within the green box of Figure <ref type="figure" target="#fig_0">1</ref>) and 9 applications that we expect to exhibit smaller benefits (from within the red box).</p><p>All SPEC CPU applications are run with the reference inputs. We use the Twitter <ref type="bibr" target="#b0">[1]</ref> dataset for GAPBS, with the exception of tc that uses a synthetic graph of 2 25 nodes. For NAS, input class C is used. For gups, we replace the random generator with the C++ built-in random generator to ensure that the table is randomly accessed. The table size is 8GB and 4 million locations are accessed. We compile all benchmarks with gcc/gfortran and -O3 flags.</p><p>We use the SimPoint methodology <ref type="bibr" target="#b13">[14]</ref> to find representative regions of each application. We use 2 SimPoints of 250 million instructions each and 250 million instructions of warmup. For kernels (gups, stream, bmt), we annotate the code with gem5 pragmas to simulate just the region of interest.</p><p>For multi-core evaluation we use a set of multi-program and multi-threaded applications listed in Table <ref type="table" target="#tab_2">II</ref>. We use level prediction accuracy from single core simulation to observe how different applications with high, medium, and low prediction accuracy interact. From application mixes, mix1, mix3, and mix5 have 2 high expected benefit applications (green box) and 2 expected medium benefit applications (red box), mix2 has 1 high-benefit application and 3 medium-benefit applications (red box), and mix4 has 4 expected mediumbenefit applications. For multi threads, we focus on GAPBS.pr with 2 and 4 threads. Given the GAPBS.pr has one the lowest single-core hit prediction accuracy, we can observe how level prediction accuracy changes as the contention increases, and how the accuracy is impacted as the LocMap does not update the prediction table on snoop invalidations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. System Comparisons</head><p>To evaluate the level prediction, we compare LP to 4 systems: (1) 2KB TAGE (energy competitor) <ref type="bibr" target="#b28">[29]</ref>, (2) 8KB  TAGE (prediction accuracy competitor) <ref type="bibr" target="#b28">[29]</ref>, (3) Direct-to-Data (high implementation cost, but energy and accuracy competitor) <ref type="bibr" target="#b25">[26]</ref>, and (4) Ideal system where misses do not incur any performance penalty. In Ideal, we use 0 cycle for the miss latency with other functionalities of the simulator remaining the same. For example, bus and MSHR latencies are still counted for misses. For D2D, we assume that the Hub is an 8-way 4KB cache. Additionally, we assume that the eTLB requires 10% higher energy per access as it increases the length entries <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EVALUATION RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Prediction Accuracy</head><p>The LP predictions may be: (1) correctly predicted sequential (sequential); (2) correctly predicted skip (skip); (3) wrongly predicted sequential (opportunity loss); or wrongly predicted skip requiring recovery (harmful). Additionally, some predictions are multi-way and add some overhead despite reducing access latency. The overall prediction accuracy (Figure <ref type="figure" target="#fig_6">7</ref>) is very high. Only 605.mcf, 609.foto, 620.omnetpp, 654.roms, and nas.ft exhibit accuracies under 90%, and only gups and and nas.is exhibit non-negligible lost opportunities. It is also clear that LP correctly identifies a large number of useful non-sequential lookup opportunities.</p><p>Interestingly, the lower accuracies are not a result of high LocMap metadata cache miss rates. As shown in Figure <ref type="figure">8</ref>, those applications with lower accuracies still exhibit reasonable metadata cache hit ratios and high accuracy LDP predictions. Instead, the mispredictions are a result of stale LocMap information originating from a combination of aggressive prefetchers and poor metadata cache locality. When the prefetchers are aggressive, more clean lines are evicted without updating the LocMap. Furthermore, when the metadata cache replacement rate is high, a larger number of prefetch cache fills miss in the metadata cache and also do not update the LocMap.</p><p>Figure <ref type="figure">8</ref> also demonstrates the importance of the Popular Levels Detector (PLD). Several benchmarks (605.mcf, 620.omnet, gapbs.bc, gapbs.pr, gups, and nas.is) exhibit high LocMap metadata cache miss rates, but those misses use the PLD, which offers high accuracy for these benchmarks. Note that the figure shows the PLD accuracy only considering those accesses that use it (metadata cache misses).</p><p>There are two reasons for the high accuracy of the PLD. The first is that some applications exhibit clear cache-level usage patterns, as discussed in Section II and summarized in Figure <ref type="figure" target="#fig_0">1</ref>. For example, gapbs.bc and gapbs.tc exhibit very low hit rates in both L2 and L3, allowing the PLD to frequently suggest skipping both levels.</p><p>The second reason is that the PLD can suggest multi-way access to multiple levels in parallel, and thus not mispredict but with access overhead. This is shown in Figure <ref type="figure">9</ref> with good examples being: 620.omnet with 25% of PLD predictions suggesting all levels in parallel, gapbs.pr with 35% of PLD predictions of parallel accesses to memory and L3, and nas.is with 50% of PLD predictions requesting simultaneous access to L2 and L3. However, for the most part multi-way prediction is rare.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Cache Energy Dissipation</head><p>We use CACTI to obtain energy per access and then accumulate the total energy. There are two major contributors to the energy consumption of predictors: (1) how frequent the structures are accessed to update and for prediction, and (2) how frequently we have to refer to the directory because of mispredictions. D2M also has two sources of energy overhead:</p><p>(1) accessing larger TLB entries, and (2) updating the Hub on TLB misses and new insertions. While there are no mispredictions because D2M is a precise scheme, applications with high TLB miss rate (e.g. is) need to access the hub more frequently, and thereby the energy consumption increases. Note that our energy analysis here refers to access energy alone and does not account for the additional energy savings resulting from the higher performance provided by level prediction.</p><p>Figure <ref type="figure" target="#fig_0">10</ref> shows the energy consumption of LP normalized to the baseline system and also compares this energy to the 2KB and 8KB address+history TAGE variants. We make four important observations. First, our LocMap + PLD predictor is substantially better than either TAGE variant. The 2KB TAGE has the same access energy as the LP, but its accuracy is far lower, which increases recovery overhead. In contrast, the 8KB TAGE offers similar (just slightly lower) prediction accuracy, but its access energy is far higher, resulting in significant additional cache-hierarchy energy.</p><p>The second observation is that our LP saves energy in all but two cases, resulting in an average energy saving of 16%. The predictor is so accurate and the recovery scheme simple, such that on average only 1% of the cache-hierarchy energy is spent on recovery.   The third observation is that in the two cases where energy is slightly increased, the overhead was a result of the very small benefit opportunity available. In 620.omnet, the energy overhead was the result of frequent all-level predictions by the PLD, while for nas.ft the overhead was a result of the low potential coupled with a relatively low prediction accuracy of just 08%, and nearly all of those were for sequential lookup.</p><p>Finally, while not suffering from any mispredictions, D2M also exhibits overheads and can only improve on our LP by 3% on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance</head><p>Figure <ref type="figure" target="#fig_0">11</ref> shows the IPC improvement for the 2KB and 8KB TAGE predictors, D2M, our LP, and the idealized system. The geomean speedups are 4.3%, 6.9%, 8.2%, 7.8% and 8.4%, respectively for each system. LP is within 10% of the ideal speedup and trails the far more intrusive D2M architecture by just 5% on average. At the same time, LP offers far better performance than building directly on the miss-predictor approach. We make four main observations on these results.</p><p>First, with two exceptions, speedup correlates well with the level-prediction potential discussed in Section II and summarized in Figure <ref type="figure" target="#fig_0">1</ref>. The largest speedup is achieved for those applications for which sequential lookup is harmful (application within the green box in Figure <ref type="figure" target="#fig_0">1</ref>): 619.lbm, 649.foton, all the gpabs applications, and gups. The first exception is 605.mcf where speedup is just 3%. The results of a top-down microarchitecture analysis <ref type="bibr" target="#b34">[35]</ref> show that 619.lbm is bound by both memory bound (35%) and the front-end (31%). This, together with relatively high memory-level parallelism, limits the potential benefit of level prediction. The second exception is nas.is, which falls outside the green box of Figure <ref type="figure" target="#fig_0">1</ref> but still exhibits high speedup. We attribute this anomaly to the better prefetchers available on the commercial Intel core compared to our simulated processor. Indeed, when performing the same analysis with our simulated result, nas.is falls within the green box.</p><p>Our second important observation is that LP nearly matches the speedup of Ideal and D2M in all but two cases: 650.roms and nas.is. Both benchmarks exhibit high speedup potential and lower prediction accuracy compared to other applications with high-speedup potential. As shown in Figure <ref type="figure" target="#fig_6">7</ref>, LP only successfully skips a relatively small fraction of accesses (40%) while also requiring recovery relatively frequently (20%). The other two similar applications are 605.mcf and nas.ft, but those offer minimal speedup opportunity. The reason for the large speedup difference with nas.is is different. For nas.is, the LP relies heavily on the PLD, which frequently suggests parallel L2 and L3 access. We argue that this increases pressure on the cache ports, realizing lower speedup than Ideal and D2D which do not attempt parallel accesses.</p><p>Our third observation is that while skipping L3 lookup and directly accessing memory instead has higher speedup benefits than skipping the much-lower latency L2, skipping L2 is still very useful. Just skipping L2 offers &gt; 5% speedup for many applications.</p><p>Finally, Figure <ref type="figure" target="#fig_1">12</ref> shows the average memory access latency for LP and Ideal. The baseline is shown with a red line. The average memory access latency is improved by 20% on average. Graph applications obtain lower memory access latency with level prediction because they have high miss ratios at all levels and avoiding those unnecessary lookups helps reduce memory access latency. The trends match the speedup trends overall. Relative Energy   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TAGE-2KB TAGE-8KB D2D LP</head><note type="other">Baseline Energy L2+L3 Predictor Misprediction</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Multi-Core Results</head><p>For multi-core simulation, we enable one LP per core. Figure <ref type="figure" target="#fig_10">13</ref> shows the predictor accuracy for the five multiprocess mixes and the multi-threaded applications. Overall, LP accuracy with four cores is lower than with a single core. This is because contention on LLC is greater while prefetch aggressiveness is also substantially larger because more prefetchers operate in parallel. Still, accuracy is high with the exception of mix4. For multi-threaded applications, we run gapbs.pr with 2 and 4 threads on a 4-core processor. While accuracy changes little between 2 and 4 threads, both harmful and opportunity-loss mispredictions are more frequent than with  a single thread. This is expected because not only is there the greater LLC contention and prefetch aggressiveness, but there is also some degradation in LocMap accuracy because the LocMap is not updated with coherence events and because the LocMap uses a single entry per block even though it may or may not be cached in multiple private L2 caches. Figure <ref type="figure" target="#fig_2">14</ref> shows the speedup and relative energy-efficiency improvement for the mixes. Notably, level prediction always provides some speedup and cache-energy improvement. For multi-program mixes, mix1 has the highest IPC improvement (13%), and mix4 has the lowest (1.2%). The main reason is that mix4 is composed of four low-MPKI applications and offers minimal speedup potential, whereas mix1 has 4 very high MPKI applications and offers high speedup potential. Overall, the speedup geomean with LP is 6%, achieving a large fraction of the potential 7% geomean speedup of the idealized system. Energy efficiency is improved by an average (geomean) of 8%, which is again, more than 85% of the potential energy benefits of the ideal case.</p><p>For multi-threaded applications, speedup improves as the thread count increases. This is despite the prediction accuracy slightly decreasing because of higher LLC contention. This same LLC contention, however, also increases speedup potential because memory is accessed mmore frequently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Sensitivity Analysis</head><p>Figure <ref type="figure" target="#fig_11">15</ref> compares the average normalized IPC across all applications for 5 systems: (1) a system with the configuration of Section IV, (2) a system with fast sequential LLC (45 cycles), (3) a system with parallel LLC (40 cycles of access latency), (4) a system with a parallel LLC, and a larger LSQ (96 entries), and (5) a system with a very aggressive core (ROB=224, and LSQ=96) and a parallel LLC. To be fair, for each scenario, we normalize the IPC to the baseline with that configuration without level prediction. Speedup geomeans of the 5 systems are 7.8%, 6.6%, 6.4%, 5.9%, and 5.6%. As can be seen, the overall improvement decreases as the systems become more aggressive. However, even for the most aggressive configuration, level prediction provides 5.6% speedup compared to the clearly aggressive baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Overhead Analysis</head><p>Our design requires only a 2KiB metadata cache per core as well as three 32-bit wide counters. For each 64B-block in physical memory 2 bits are assigned leading to a memory overhead of 0.39%. The directory remains unchanged, and only the cache controller is notified with a mechanism to deallocate the MSHR entries on a misprediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RELATED WORK</head><p>Per level hit/miss predictor is either used in front of L1 cache <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b35">[36]</ref> to handle instruction scheduling better, or only employed at L4 caches <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b28">[29]</ref>. The insight behind L4 hit/miss prediction that the miss penalty is high, and blindly accessing cache incurs high performance and power consumption costs. Our proposal extends such a solution to all memory hierarchy, which is getting deeper recently.</p><p>D2M <ref type="bibr" target="#b26">[27]</ref> finds the actual location of a block in the memory hierarchy with a single lookup. Authors separate the metadata from the data hierarchy, and then using pointers, the request is forwarded to its destination. This technique requires significant changes to the current processor, such as enlarging TLB entries and adopting a new coherence scheme. D2D is another technique to provide a single lookup in cache hierarchy <ref type="bibr" target="#b25">[26]</ref>, however, it also needs to enlarge the TLB entries and OS changes.</p><p>Cache bypassing technique <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b32">[33]</ref> selectively inserts data blocks in the cache. Because many applications with streaming behavior have no data resue, and they better to be written directly to the main memory, and keep the valuable on-chip space for incoming requests. Way prediction reduces energy consumption by avoiding searching all ways to match the tag <ref type="bibr" target="#b23">[24]</ref>.</p><p>Software prefetching is an appealing solution to reduce memory access latency for applications with complex address patterns <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref>. This technique is useful when the memory access pattern is complex and thereby cannot be captured by hardware prefetchers <ref type="bibr" target="#b2">[3]</ref>. Many previous works attempted to handle indirect memory access patterns by inserting the software prefetch instruction in the code. Also, properly adjusting the prefetch distance is a crucial for a timely prefetch. Proactively finding the best time slot to issue the software prefetch request has been studied in <ref type="bibr" target="#b1">[2]</ref>. Event-driven software prefe tch generation has been proposed in <ref type="bibr" target="#b3">[4]</ref>. Although software prefetch can increase the coverage to almost 100%, it severely suffers from lack of timeliness. its effectiveness.</p><p>Many prefetchers have been proposed in the past decade [3]- <ref type="bibr" target="#b4">[5]</ref> [11]- <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>. Spatial prefetchers rely on spatial behavior of access patterns to predict the next address. However, temporal prefetchers record the past addresses and use them for future predictions. Spatial prefetchers require complex logics to detect the address patterns, while temporal prefetchers need a significant amount of metadata to record the past addresses. achieve at most 40% of coverage <ref type="bibr" target="#b7">[8]</ref>.</p><p>Cache level prediction can achieve high accuracy with a simple table. Also, in contrast to D2M <ref type="bibr" target="#b26">[27]</ref>, the overall system design can remain untouched because it utilizes the available resources and schemes. Additionally, compared to a sequential per level predictors where separate predictors for each level can sit close to CPU, our unified predictor takes smaller space and attain better accuracy as it aggregates information all in one table, and observes the system as whole.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>We propose a cache level predictor in order to enhance the lookup strategy in multiple-cache setting. This technique filters unnecessary accesses to intermediate levels, and thus reduces the cumulative miss latency. The proposed system enhances the IPC by 7.8% compared to the baseline.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: The x-and y-axes on this log-log plot represent the effectiveness of L2 and L3 at filtering misses, respectively. Each application is plotted according to the ratio of misses filtered by each level compared to the level above it. Applications further to the top and right work best with sequential access, while those toward the bottom left stand to benefit most from non-sequential level-predicted lookups.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Miss trace of several applications across their execution. A gap between the miss rate (number of misses per time window) of different cache levels indicates effective miss filtering, while lines that are close to one another suggest a cache level lookup can be bypassed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The proposed architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Normalized average energy for different cache sizes. Numbers are normalized to 1KB cache.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Example of parallel access with cache level prediction: (a) One-way correct prediction (prediction=LLC, actual location=LLC); (b) One-way wrong prediction (prediction=LLC, actual location=L2); (c) Multi-way with a wrong prediction (prediction=LLC and main memory, actual location=main memory) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Breakdown of level prediction accuracy. Useful is the accesses that skip at least one level correctly. Opportunity loss is the fraction of lookups the could have avoided. Harmful is the fraction misprediction in all lookups.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Figure 8: Metadata cache miss ratio and misprediction ratio of the Popular Level Detector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>x a l a n 6 2 7 . c a m 6 4 9 . f o t o n 6 5 4 . r o m s b m t g a p b s . b c g a p b s . b f s g a p b s . c c g a p b s . p r g a p b s . t c g u p s n a s . c g n a s . f t n a s .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Figure 10: Energy consumption normalized to the baseline. For each benchmark, the bars are 8KB-TAGE, 2KB-TAGE, D2M, and level prediction from left to right. Ideal is "L2+L3" cache energy only.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Multi-core level prediction accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Sensitivity analysis to ROB size, LSQ size and LLC latency for a single core evaluation (average of all benchmarks).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Evaluated System Configuration.</figDesc><table><row><cell>Processor</cell><cell>Single and Quad-core, 4.0GHz, Ubuntu 16.04 OS.</cell></row><row><cell></cell><cell>ROB:192, LQ:64, SQ:64, Fetch-width=4</cell></row><row><cell>L1 Cache</cell><cell>32kB, Split I and D cache; 32KB private; 4-way;</cell></row><row><cell></cell><cell>64B line size; LRU; write-back; 1 port; 4 cycles.</cell></row><row><cell></cell><cell>Tagged next line prefetcher with degree=1</cell></row><row><cell>L1 and L2</cell><cell>MOESI directory; L1 and L2 are inclusive, L3 is</cell></row><row><cell>Coherency</cell><cell>non-inclusive</cell></row><row><cell>L2 Cache</cell><cell>256KB; 8-way; 64B line size; LRU; write-back; 2</cell></row><row><cell></cell><cell>ports; 12 cycles, tagged Next Line prefetcher with</cell></row><row><cell></cell><cell>degree=2</cell></row><row><cell>L3 Cache</cell><cell>2MB single-core and 8MB multi-core; 16-way; 64B</cell></row><row><cell></cell><cell>line size; LRU; write-back; 4 ports; Sequential</cell></row><row><cell></cell><cell>cache (20+35). DCPT prefetcher degree of 2</cell></row><row><cell>Main Mem-</cell><cell>16 GB: DDR4-2400 x64, Micron MT40A1G8 in</cell></row><row><cell>ory</cell><cell>an 8x8 configuration</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Multi-program and multi-threaded applications.</figDesc><table><row><cell>mix1</cell><cell>GAPBS.bfs, SPEC.619.lbm, NAS.lu, bmt</cell></row><row><cell>mix2</cell><cell>SPEC.654.roms, NAS.mg, SPEC.649.fotonik3d, SPEC.602.gcc</cell></row><row><cell>mix3</cell><cell>SPEC.620.omnetpp, GAPBS.pr, SPEC.627.cam, NAS.cg</cell></row><row><cell>mix4</cell><cell>SPEC.627.cam, NAS.cg, SPEC.621.wrf, NAS.bt</cell></row><row><cell>mix5</cell><cell>GAPBS.bfs, SPEC.619.lbm, SPEC.621.wrf, NAS.bt</cell></row><row><cell>MT1</cell><cell>GAPBS.pr with 2 threads</cell></row><row><cell>MT2</cell><cell>GAPBS.pr with 4 threads</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Twitter graph</title>
		<ptr target="http://an.kaist.ac.kr/∼haewoon/release/twittersocialgraph/twitterrv.tar.gz" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graph prefetching using data structure knowledge</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ainsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
		<idno type="DOI">10.1145/2925426.2926254</idno>
		<ptr target="https://doi.org/10.1145/2925426.2926254" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 International Conference on Supercomputing, ser. ICS &apos;16</title>
				<meeting>the 2016 International Conference on Supercomputing, ser. ICS &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Software prefetching for indirect memory accesses</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ainsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 International Symposium on Code Generation and Optimization, ser. CGO &apos;17</title>
				<meeting>the 2017 International Symposium on Code Generation and Optimization, ser. CGO &apos;17</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="305" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An event-triggered programmable prefetcher for irregular workloads</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ainsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
		<idno type="DOI">10.1145/3173162.3173189</idno>
		<ptr target="https://doi.org/10.1145/3173162.3173189" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems, ser. ASPLOS &apos;18</title>
				<meeting>the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems, ser. ASPLOS &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="578" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Software prefetching for indirect memory accesses: A microarchitectural perspective</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ainsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
		<idno type="DOI">10.1145/3319393</idno>
		<ptr target="https://doi.org/10.1145/3319393" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019-06">Jun. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Filter caching for free: The untapped potential of the store-buffer</title>
		<author>
			<persName><forename type="first">R</forename><surname>Alves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Black-Schaffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaxiras</surname></persName>
		</author>
		<idno type="DOI">10.1145/3307650.3322269</idno>
		<ptr target="https://doi.org/10.1145/3307650.3322269" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th International Symposium on Computer Architecture, ser. ISCA &apos;19</title>
				<meeting>the 46th International Symposium on Computer Architecture, ser. ISCA &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="436" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domino temporal data prefetcher</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bakhshalipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lotfi-Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sarbazi-Azad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
				<imprint>
			<date type="published" when="2018-02">Feb 2018</date>
			<biblScope unit="page" from="131" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bingo spatial data prefetcher</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bakhshalipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shakerinava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lotfi-Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sarbazi-Azad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
				<imprint>
			<date type="published" when="2019-02">Feb 2019</date>
			<biblScope unit="page" from="399" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The GAP benchmark suite</title>
		<author>
			<persName><forename type="first">S</forename><surname>Beamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<idno>abs/1508.03619</idno>
		<ptr target="http://arxiv.org/abs/1508.03619" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The gem5 simulator</title>
		<author>
			<persName><forename type="first">N</forename><surname>Binkert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sardashti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sewell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shoaib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vaish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011-08">Aug. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Sandbox based optimal offset estimation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">C2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Proactive instruction fetch</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<idno type="DOI">10.1145/2155620.2155638</idno>
		<ptr target="http://doi.acm.org/10.1145/2155620.2155638" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual IEEE/ACM International Symposium on Microarchitecture, ser. MICRO-44</title>
				<meeting>the 44th Annual IEEE/ACM International Symposium on Microarchitecture, ser. MICRO-44<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="152" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-level hardware prefetching using low complexity delta correlating prediction tables with partial matching</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grannaes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jahre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Natvig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on High Performance Embedded Architectures and Compilers, ser. HiPEAC&apos;10</title>
				<meeting>the 5th International Conference on High Performance Embedded Architectures and Compilers, ser. HiPEAC&apos;10<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="247" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Simpoint 3.0: Faster and more flexible program phase analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hamerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<ptr target="http://www.jilp.org/vol7/v7paper14.pdf" />
	</analytic>
	<monogr>
		<title level="j">J. Instruction-Level Parallelism</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Intel® 64 and ia-32 architectures optimization reference manual</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Access map pattern matching for data cache prefetch</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Inaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hiraki</surname></persName>
		</author>
		<idno type="DOI">10.1145/1542275.1542349</idno>
		<ptr target="http://doi.acm.org/10.1145/1542275.1542349" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Supercomputing, ser. ICS &apos;09</title>
				<meeting>the 23rd International Conference on Supercomputing, ser. ICS &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="499" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Linearizing irregular memory accesses for improved correlated prefetching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2540708.2540730</idno>
		<ptr target="http://doi.acm.org/10.1145/2540708.2540730" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual IEEE/ACM International Symposium on Microarchitecture, ser. MICRO-46</title>
				<meeting>the 46th Annual IEEE/ACM International Symposium on Microarchitecture, ser. MICRO-46<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="247" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Lookahead prefetching with signature path</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L N</forename><surname>Reddy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">C2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Runtime cache bypassing</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Connors</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Merten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">. W</forename><surname>Hwu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1338" to="1354" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficiently enabling conventional block sizes for very large die-stacked dram caches</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 44th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
				<imprint>
			<date type="published" when="2011-12">Dec 2011</date>
			<biblScope unit="page" from="454" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A best-offset prefetcherr</title>
		<author>
			<persName><forename type="first">P</forename><surname>Michaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Criticality aware tiered cache hierarchy: A fundamental relook at multi-level cache hierarchies</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="96" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bloom filtering cache misses for accurate data speculation and prefetching</title>
		<author>
			<persName><forename type="first">J.-K</forename><surname>Peir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Supercomputing 25th Anniversary Volume</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="347" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reducing set-associative cache energy via way-prediction and selective direct-mapping</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Vijaykumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 34th ACM/IEEE International Symposium on Microarchitecture. MICRO-34</title>
				<meeting>34th ACM/IEEE International Symposium on Microarchitecture. MICRO-34</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="54" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fundamental latency trade-off in architecting dram caches: Outperforming impractical sram-tags with a simple and practical design</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 45th Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="235" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Navigating the cache hierarchy with a single lookup</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sembrant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hagersten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Black-Schaffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 ACM/IEEE 41st International Symposium on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="133" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A split cache hierarchy for enabling data-oriented optimizations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sembrant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hagersten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Black-Schaffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
				<imprint>
			<date type="published" when="2017-02">Feb 2017</date>
			<biblScope unit="page" from="133" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A new case for the tage branch predictor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 44th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A mostly-clean dram cache for effective hit speculation and self-balancing dispatch</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oconnor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thottethodi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 45th Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="247" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Spatiotemporal memory streaming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<idno type="DOI">10.1145/1555754.1555766</idno>
		<ptr target="http://doi.acm.org/10.1145/1555754.1555766" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual International Symposium on Computer Architecture, ser. ISCA &apos;09</title>
				<meeting>the 36th Annual International Symposium on Computer Architecture, ser. ISCA &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="69" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Experiments with spec cpu 2017 : Similarity , balance , phase behavior and simpoints</title>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Flolid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient metadata management for irregular data prefetching</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nathella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sunwoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th International Symposium on Computer Architecture, ser. ISCA &apos;19</title>
				<meeting>the 46th International Symposium on Computer Architecture, ser. ISCA &apos;19</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="449" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Coordinated static and dynamic cache bypassing for gpus</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE 21st International Symposium on High Performance Computer Architecture (HPCA)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="76" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attack directories, not caches: Side channel attacks in a non-inclusive world</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sprabery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gopireddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Torrellas</surname></persName>
		</author>
		<idno type="DOI">10.1109/SP.2019.00004</idno>
		<ptr target="https://doi.ieeecomputersociety.org/10.1109/SP.2019.00004" />
	</analytic>
	<monogr>
		<title level="m">2019 2019 IEEE Symposium on Security and Privacy (SP)</title>
				<meeting><address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2019-05">may 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A top-down method for performance analysis and counters architecture</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yasin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Speculation techniques for improving load related instruction scheduling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yoaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ronen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jourdan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Symposium on Computer Architecture (Cat. No.99CB36367)</title>
				<meeting>the 26th International Symposium on Computer Architecture (Cat. No.99CB36367)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Accord: Enabling associativity for gigascale dram caches by coordinating way-install and way-prediction</title>
		<author>
			<persName><forename type="first">V</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qureshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Towards bandwidth-efficient prefetching with slim ampm</title>
		<author>
			<persName><forename type="first">V</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krisshna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">C2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Imp: Indirect memory prefetcher</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Devadas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 48th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
				<imprint>
			<date type="published" when="2015-12">Dec 2015</date>
			<biblScope unit="page" from="178" to="190" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
