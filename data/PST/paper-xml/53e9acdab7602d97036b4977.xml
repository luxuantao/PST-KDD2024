<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Affine Structure from Line Correspondences With Uncalibrated Affine Cameras</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Long</forename><surname>Quan</surname></persName>
							<email>long.quan@imag.fr</email>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">CNRS-GRAVIR-INRIA</orgName>
								<address>
									<addrLine>ZIRST 655, avenue de l&apos;Europe</addrLine>
									<postCode>38330</postCode>
									<settlement>Montbonnot</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Robotics Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Affine Structure from Line Correspondences With Uncalibrated Affine Cameras</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CDF336C9BFB9F74F92C105C615FAF29A</idno>
					<note type="submission">received 6 May 1996; revised 13 June 1997. Recommended for acceptance by L. Shapiro.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Structure from motion</term>
					<term>affine structure</term>
					<term>factorization method</term>
					<term>line correspondence</term>
					<term>affine camera</term>
					<term>one-dimensional camera</term>
					<term>uncalibrated image</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a linear algorithm for recovering 3D affine shape and motion from line correspondences with uncalibrated affine cameras. The algorithm requires a minimum of seven line correspondences over three views. The key idea is the introduction of a one-dimensional projective camera. This converts 3D affine reconstruction of "line directions" into 2D projective reconstruction of "points." In addition, a line-based factorization method is also proposed to handle redundant views. Experimental results both on simulated and real image sequences validate the robustness and the accuracy of the algorithm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>SING line segments instead of points as features has attracted the attention of many researchers <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b42">[43]</ref> for various tasks such as pose estimation, stereo, and structure from motion. In this paper, we are interested in structure from motion using line correspondences across multiple images. Line-based algorithms are generally more difficult than point-based ones for the following two reasons. The parameter space of lines is nonlinear, though lines themselves are linear subspaces, and a line-to-line correspondence contains less information than a point-to-point one, as it provides only one component of the image plane displacement instead of two for a point correspondence. A minimum of three views is essential for line correspondences, whereas two views suffice for point ones. In the case of calibrated perspective cameras, the main results on structure from line correspondences were established in <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b7">[8]</ref>: With at least six line correspondences over three views, nonlinear algorithms are possible. With at least 13 lines over three views, a linear algorithm is possible. The basic idea of the 13-line linear algorithm is similar to the "eight-point" one <ref type="bibr" target="#b17">[18]</ref> in that it is based on the introduction of a redundant set of intermediate parameters. This significant over-parameterization of the problem leads to the instability of the algorithm reported in <ref type="bibr" target="#b16">[17]</ref>. The 13-line algorithm was extended to uncalibrated camera case in <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b42">[43]</ref>. The situation here might be expected to be better, as more free parameters are introduced. However, the 27 tensor components that are introduced as intermediate parameters are still subject to eight complicated algebraic constraints. The algorithm can hardly be stable. A subsequent nonlinear optimization step is al-most unavoidable to refine the solution <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b11">[12]</ref>.</p><p>In parallel, there has been a lot of work <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b41">[42]</ref> on structure from motion with simplified camera models varying from orthographic projections via weak and para-perspective to affine cameras, almost exclusively for point features. These simplified camera models provide a good approximation to perspective projection when the width and depth of the object are small compared to the viewing distance. More importantly, they expose the ambiguities that arise when perspective effects diminish. In such cases, it is not only easier to use these simplified models but also advisable to do so, as, by explicitly eliminating the ambiguities from the algorithm, one avoids computing parameters that are inherently ill-conditioned. Another important advantage of working with uncalibrated affine cameras is that the reconstruction is affine, rather than projective, as with uncalibrated projective cameras.</p><p>Motivated, on the one hand, by the lack of satisfactory line-based algorithms for projective cameras and, on the other, by the fact that the affine camera is a good model for many practical cases, we investigate the properties of projection of lines by affine cameras and propose a linear algorithm for affine structure from line correspondences. The key idea is the introduction of a one-dimensional projective camera. This converts the 3D affine reconstruction of "line directions" into 2D projective reconstruction of "points." The linear algorithm requires a minimum of seven lines over three images. We also prove that seven lines over three images is the strict minimum data needed for affine structure from uncalibrated affine cameras and that there are always two possible solutions. This result extends the previous results of Koenderink and Van Doorn <ref type="bibr" target="#b13">[14]</ref> for affine structure with a minimum of two views and five points. To deal with redundant views, we also present a line-based factorization algorithm, which extends the previous pointbased factorization methods <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b28">[29]</ref>. A preliminary version of this work was presented in <ref type="bibr" target="#b29">[30]</ref>.</p><p>The paper is organized as follows. In Section 2, the affine camera model is briefly reviewed. Then, we investigate the properties of projection of lines with the affine camera and introduce the one-dimensional projective camera in Section 3. Section 4 is focused on the study of the uncalibrated onedimensional camera, and, in this section, we present also a linear algorithm for 2D projective reconstruction which is equivalent to the 3D affine reconstruction of line directions. Later, the linear estimation of the translational component of the uncalibrated affine camera is given in Section 5 and the affine shape recovery is described in Section 6. To handle redundant views, a line-based factorization method is proposed in Section 9. The passage to metric structure from the affine structure using known camera parameters will be described in Section 11. Finally, in Section 13, discussions and some concluding remarks are given.</p><p>(Throughout the paper, tensors and matrices are denoted in uppercase boldface, vectors in lowercase boldface, and scalars in either plain letters or lowercase Greek.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">REVIEW OF THE AFFINE CAMERA MODEL</head><p>For a projective (pin-hole) camera, the projection of a point x = (x, y, z, t)</p><p>T of 3 3 to a point w = (u, v, w) T of 3 2 can be described by a 3 4 homogeneous projection matrix P 3 4 :</p><formula xml:id="formula_0">Ow = P 3 4 x.<label>(1)</label></formula><p>For a restricted class of camera models, by setting the third row of the perspective camera P 3 4 to (0, 0, 0, O), we obtain the affine camera initially introduced by Mundy and Zisserman <ref type="bibr" target="#b22">[23]</ref>, A M 0 t </p><formula xml:id="formula_1">0 0 0 ¥ ¥ ¥ ¥ = F H G G I K J J = F H G I K J p p p p p p p p p<label>(2)</label></formula><p>The affine camera A 3 4 encompasses the uncalibrated versions of the orthographic, weak perspective and paraperspective projection models. These reduced camera models provide a good approximation to the perspective projection model when the depth of the object is small compared to the viewing distance. For more detailed relations and applications, one can refer to <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b40">[41]</ref>.</p><p>For points in the affine spaces ‫ޒ‬ 3 and ‫ޒ‬ 2 , they are naturally embedded into 3 3 and 3 2 by the mappings w a ‫ۋ‬ w = (w a , 1)</p><p>T and x a ‫ۋ‬ x = (x a , 1) T . We have thus</p><formula xml:id="formula_2">w a = M 2 3 x a + t 0 ,</formula><p>where t 0 = (t 1 /t 3 , t 2 /t 3 ) T = (p 14 /p 34 , p 24 /p 34 )</p><p>T . If we further use relative coordinates of the points with respect to a given reference point (for instance, the centroid of the set of points), the vector t 0 is canceled and we obtain the following linear mapping between space points and image points:</p><formula xml:id="formula_3">D D w M x a a = ¥ 2 3 . (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>This is the basic equation of the affine camera for points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE AFFINE CAMERA FOR LINES</head><p>Now consider a line in ‫ޒ‬ 3 through a point x 0 , with direction d x :</p><formula xml:id="formula_5">x x d a x = + OE 0 l l , for ‫ޒ‬ .</formula><p>The affine camera A 3 4 projects this to an image line</p><formula xml:id="formula_6">A x M x t M d w M d 3 4 2 3 0 0 2 3 0 2 3 1 ¥ ¥ ¥ ¥ F H G I K J= + + ∫ + a x x c h l l passing through the image point w M x t 0 2 3 0 0 = + ¥ , with<label>direction</label></formula><formula xml:id="formula_7">d w = OM 2 3 d x .<label>(4)</label></formula><p>This equation describes a linear mapping between direction vectors of 3D lines and those of 2D lines, and reflects a key property of the affine camera: lines parallel in 3D remain parallel in the image. It can be derived even more directly using projective geometry by considering that the line direction d x is the point at infinity</p><formula xml:id="formula_8">x d • = x T T</formula><p>, 0 e j of the projective line in 3 3 and the line direction d w is the point at</p><formula xml:id="formula_9">infinity w d • = w T T</formula><p>, 0 e j of the projective line in 3 2 . Equation (4) immediately follows as the affine camera preserves the points at infinity by its very definition. Comparing ( <ref type="formula" target="#formula_7">4</ref>) with (1)-a projection from 3 3 to 3 2we see that ( <ref type="formula" target="#formula_7">4</ref>) is nothing but a projective projection from 3 2 to 3 1 if we consider the 3D and 2D "line directions" as 2D and 1D projective "points." This key observation allows us to establish the following: The affine reconstruction of line directions with a two-dimensional affine camera is equivalent to the projective reconstruction of points with a one-dimensional projective camera.</p><p>One of the major remaining efforts will be concerned with 2D projective reconstruction from the points in 3 1 . There have been many recent works <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b2">[3]</ref> on projective reconstruction and the geometry of multiviews of two dimensional uncalibrated projective cameras. Particularly, the tensorial formalism developed by Triggs <ref type="bibr" target="#b39">[40]</ref> is very interesting and powerful. We now extend this study to the case of the one-dimensional camera. It turns out that there are some nice properties which were absent in the 2D case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">UNCALIBRATED ONE-DIMENSIONAL CAMERA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Trilinear Tensor of the Three Views</head><p>First, rewrite (4) in the following form:</p><formula xml:id="formula_10">Ou = M 2 3 x<label>(5)</label></formula><p>in which we use u = (u 1 , u 2 )</p><p>T and x = (x 1 , x 2 , x 3 ) T instead of d w and d x to stress that we are dealing with "points" in the projective spaces 3 2 and 3 1 rather than "line directions" in the vector spaces ‫ޒ‬ 3 and ‫ޒ‬ 2 .</p><p>We now examine the matching constraints between multiple views of the same point. Since two viewing lines in the projective plane always intersect in a point, no con-straint is possible for less than three views. There is one constraint only for the case of three views. Let the three views of the same point x be given as follows:</p><formula xml:id="formula_11">l l l u Mx u M x u M x = ¢ ¢ = ¢ ¢¢ ¢¢ = ¢¢ R S | T | (6)</formula><p>These can be rewritten in matrix form as</p><formula xml:id="formula_12">M u M u M u x 0 0 0 0 0 0 0 ¢ ¢ ¢¢ ¢¢ F H G I K J - -¢ -¢¢ F H G G I K J J = l l l ,<label>(7)</label></formula><p>which is the basic reconstruction equation for a onedimensional camera. The vector x, , , --¢ -¢¢ l l l c h T can- not be zero, so</p><formula xml:id="formula_13">M u M u M u 0 0 0 0 0 0 0 ¢ ¢ ¢¢ ¢¢ = . (<label>8</label></formula><formula xml:id="formula_14">)</formula><p>The expansion of this determinant produces a trilinear constraint of three views</p><formula xml:id="formula_15">T u u u ijk i j k i j k ¢ ¢¢ = = Â 0 1 2 , , ,<label>(9)</label></formula><p>or, in short,</p><formula xml:id="formula_16">T uuu 2 2 2 0 ¥ ¥ ¢ ¢¢ = ,</formula><p>where T 2 2 2 = (T ijk ) is a 2 2 2 homogeneous tensor whose components T ijk are 3 3 minors of the following 6 3 joint projection matrix:</p><formula xml:id="formula_17">M M M ¢ ¢¢ F H G I K J = ¢ ¢ ¢¢ ¢¢ F H G G G G G I K J J J J J 1 2 1 2 1 2 . (<label>10</label></formula><formula xml:id="formula_18">)</formula><p>The components of the tensor can be made explicit as</p><formula xml:id="formula_19">T i jk i j k ijk = ¢ ¢¢ ¢ ¢¢ = , , , , for<label>1 2, (11)</label></formula><p>where the bracket [ijk] denotes the 3 3 minor of ith, jth, and kth row vector of the above joint projection matrix and bar " -"in i , j , and k denotes the dualization</p><formula xml:id="formula_20">(1, 2) ‫ۋ‬ (2, 1). (<label>12</label></formula><formula xml:id="formula_21">)</formula><p>It can easily be seen that any constraint obtained by adding further views reduces to a trilinearity. This proves the uniqueness of the trilinear constraint. Moreover, the 2 2 2 homogeneous tensor T 2 2 2 has 7 = 2 2 2 1 d.o.f., so it is a minimal parameterization of three views since three views have exactly</p><formula xml:id="formula_22">3 (2 3 1) (3 3 1) = 7 d.o.f. up to a projective transformation in 3 2 .</formula><p>Each point correspondence over three views gives one linear constraint on the tensor components T ijk . We can establish the following: The tensor components T ijk can be estimated linearly with at least seven points in 3 1 .</p><p>At this point, we have obtained a remarkable result that for a one-dimensional projective camera, the trilinear tensor encapsulates exactly the information needed for projective reconstruction in 3 2 . Namely, it is the unique matching constraint, it minimally parameterizes the three views and it can be estimated linearly. Contrast this to the 2D projective camera case, in which the multilinear constraints are algebraically redundant and the linear estimation is only an approximation based on over-parameterization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Retrieving Normal Forms for Projection Matrices</head><p>The geometry of the three views is most conveniently, and completely, represented by the projection matrices associated with each view. In the previous section, the trilinear tensor was expressed in terms of the projection matrices. Now we seek a map from the trilinear tensor representation back to the projection matrix representation of the three views.</p><p>Without loss of generality, we can always take the following normal forms for the three projection matrices</p><formula xml:id="formula_23">M I M A c abc M D f def = ¢ = = ¢¢ = = ¥ ¥ ¥ 2 2 2 2 2 2 0 c h c h c h c h c h<label>(13)</label></formula><p>Actually, the set of projection matrices M M M , , ¢ ¢¢ m r parameterized this way has 10 d.o.f.-still three more than the minimum of seven. Further constraints can be imposed. We can observe that any projective transformation in 3 2 of the form</p><formula xml:id="formula_24">H I v = F H G I K J ¥ 2 2 0 1</formula><p>T for an arbitrary two-vector v leaves M invariant and transforms</p><formula xml:id="formula_25">M into ~¢ = ¢ = + = M M H A cv c A c T e j e j .</formula><p>As c cannot be a zero vector, it can be normalized such that c</p><p>T c = 1. If we further choose an arbitrary vector v to be</p><formula xml:id="formula_26">A T c, then Ã A cc A = - T .</formula><p>It can now be easily verified that Ã c T = 0. This amounts to saying that Ã in ~¢ M can be taken to be a rank one matrix up to a projective transformation, i.e.:</p><formula xml:id="formula_27">Ã = F H G I K J a a a a 1 1 2 2 r r ,</formula><p>for a nonzero scalar U. The two-vector c is then (a 2 , a 1 ) T .</p><p>Hence, M can be represented as</p><formula xml:id="formula_28">¢ = - F H G I K J M a a a a a a 1 1 2 2 2 1 r r (<label>14</label></formula><formula xml:id="formula_29">)</formula><p>by two parameters, the ratio a 1 : a 2 and U. Therefore, a minimal seven parameter representation for the set of projection matrices M M M , , ¢ ¢¢ m r has been obtained.</p><p>With the projection matrices given by ( <ref type="formula" target="#formula_23">13</ref>), the trilinear tensor (T ijk ) defined by <ref type="bibr" target="#b10">(11)</ref> becomes</p><formula xml:id="formula_30">lT dc af ijk ijk i ki j j i k = - - = + 1 1 2 1</formula><p>a f e j for , , , , (15)  where bar " -" in j and k represents the dualization <ref type="bibr" target="#b11">(12)</ref>.</p><p>If we consider the tensor (T ijk ) as an eight-vector (t 1 , , t l , , t 8 ) T , for l = 4(i 1) + 2(j 1) + k and i, j, k = 1, 2, the eight homogeneous equations of ( <ref type="formula">15</ref>) can be rearranged into seven nonhomogeneous ones by taking the ratios t l : t 8 for l = 1, , 7. By separating the entries of M from those of M, we obtain</p><formula xml:id="formula_31">G d e f 7 6 0 ¥ F H G I K J = , (<label>16</label></formula><formula xml:id="formula_32">)</formula><p>where the matrix G 7 6 ¥ is given by Since the parameter vector (d, e, f) T of M cannot be zero, the 7 6 matrix in ( <ref type="formula" target="#formula_31">16</ref>) has at most rank five. Thus, all of its 6 6 minors must vanish. There are 2 = (6 5) (7 <ref type="formula" target="#formula_10">5</ref>) such minors which are algebraically independent, and each of them gives a quadratic polynomial in a 1 , a 2 , and U as follows:</p><p>t a t a t a t a t a t a t a t a </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R S T</head><p>By eliminating U, we obtain a homogeneous quadratic equation in a 1 and a 2 :</p><formula xml:id="formula_33">a b g a aa a 1 2 1 2 2 2 0 + + = ,<label>(17)</label></formula><p>where</p><formula xml:id="formula_34">D = t 3 t 8 t 4 t 7 , E = t 2 t 7 + t 4 t 5 t 1 t 8 t 3 t 6 J = t 1 t 6 t 2 t 5 .</formula><p>This quadratic equation may be easily solved for a 1 /a 2 . Then, U is given by the following linear equation for each of two solutions of a 1 /a 2 t a t a t a t a</p><formula xml:id="formula_35">3 1 1 2 2 2 4 1 0 - + - = c h c h r . (<label>18</label></formula><formula xml:id="formula_36">)</formula><p>Thus, we obtain two possible solutions for the projection matrix M. Finally, the six-vector (d, e, f) T for the projection matrix M is linearly solved from (16) (for instance, using SVD) in terms of M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">2D Projective Reconstruction-3D Affine Line Direction Reconstruction</head><p>With the complete determination of the projection matrices {M, M, M} of the three views, the projective reconstruction of "points" in 3 2 , which is equivalent to the affine reconstruction of "line directions" in ‫ޒ‬ 3 , can be performed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">UNCALIBRATED TRANSLATIONS</head><p>To recover the full affine structure of the lines, we still need to find the vectors t 3 1 of the affine cameras defined in <ref type="bibr" target="#b1">(2)</ref>. These represent the image translations and magnification components of the camera. Recall that line correspondences from two views-now a 2D view instead of 1D view-do not impose any constraints on camera motion: The minimum number of views required is three. If the interpretation plane of an image line for a given view is defined as the plane going through the line and the projection center, the well-known geometric interpretation of the constraint available for each line correspondence across three views (cf. <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b7">[8]</ref>) is that the interpretation planes from different views must intersect in a common line in space.</p><p>If the equation of a line in the image is given by l T u = 0, then substituting Ou = A 3 4 x into it produces the equation of the interpretation plane of l in space:</p><formula xml:id="formula_37">l T A 3 4 x = 0.</formula><p>The plane is therefore given by the four-vector p T = l T A 3 4 , which can also be expressed as p T = (n x , p) T where n x is the normal vector of the plane. An image line of direction n w can be written as l = (n w , l) T , with its interpretation plane being</p><formula xml:id="formula_38">p l P M n l t T T T w T T = = , e j . (<label>19</label></formula><formula xml:id="formula_39">)</formula><p>The 2 3 submatrices M 2 3 representing uncalibrated camera orientations have already been obtained from the two-dimensional projective reconstruction. Now we proceed to recover the uncalibrated translations.</p><p>For each interpretation plane (n x , p)</p><p>T of each image line, its direction component is completely determined by the previously computed {M, M, M} as</p><formula xml:id="formula_40">n x = M T n w .</formula><p>Only its fourth component p = l T t remains undetermined. This depends linearly on t. Notice that, as the direction vector can still be arbitrarily and individually rescaled, the interpretation plane should be properly written as</p><formula xml:id="formula_41">p T = (OM T n w , Pl T t) T .</formula><p>Hence, the ratio O/P is significant, and this justifies the homogenization of the vector t. So far we have made explicit the equation of the interpretation planes of lines in terms of the image line and the projection matrix, the geometric constraint of line correspondences on the camera motion gives a 3 4 matrix whose rows are the three interpretation planes</p><formula xml:id="formula_42">p p p n M l t n M l t n M l t T T T w T T w T T w T T ¢ ¢¢ F H G G G I K J J J = ¢ ¢ ¢ ¢ ¢¢ ¢¢ ¢¢ ¢¢ F H G G G I K J J J</formula><p>which has rank at most two. Hence, all of its 3 3 minors vanish. Only two of the total of four minors are algebraically independent, as they are connected by the quadratic identities <ref type="bibr" target="#b36">[37]</ref>.</p><p>The vanishing of any two such minors provides the two constraints on camera motion for a given line correspondence of three views. The minor formed by the first three columns contains only known quantities. It provides the constraint on the directions. It is easy to show that it is equivalent to the tensor by using suitable one-dimensional projective transformations.</p><p>By taking any two of the first three columns, say the first two, and the last column, we obtain the following vanishing determinant:</p><formula xml:id="formula_43">* * * * * * l t l t l t T T T ¢ ¢ ¢¢ ¢¢ = 0 ,</formula><p>where the "" designates a constant entry.</p><p>Expanding this minor by cofactors in the last column gives a homogeneous linear equation in t, t, and t:</p><formula xml:id="formula_44">¥ ¥ ¥ ¢ ¢¢ F H G I K J = ¥ c h 1 9 0 t t t</formula><p>, where the "" designates a constant three-vector in a row. Collecting all these vanishing minors together, we obtain</p><formula xml:id="formula_45">¥ ¥ ¥ ¥ ¥ ¥ F H G I K J ¢ ¢¢ F H G I K J = ¥ M M M n 9 0 t t t</formula><p>for n line correspondences in three views. At this stage, since the origin of the coordinate frame in space is not yet fixed, we may take t = (0, 0, 1)</p><p>T up to a scaling factor, say t 0 , so the final homogeneous linear equations to solve for (t 0 , t, t)</p><formula xml:id="formula_46">T is * * ¥ ¥ ¥ ¥ F H G I K J ¢ ¢¢ F H G G I K J J = ¥ M M M n t 7 0 0 t t . (<label>20</label></formula><formula xml:id="formula_47">)</formula><p>This system of homogeneous linear equations can be nicely solved by SVD factorization. The least squares solution for (t 0 , t, t)</p><p>T subject to ʈ(t 0 , t, t) T ʈ = 1 is the right singular vector corresponding to the smallest singular value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">AFFINE SHAPE</head><p>The projection matrices of the three views are now completely determined up to a common scaling factor. From now on, it is a relatively easy task to compute the affine shape. Two methods to obtain the shape will be described, one based on the projective representation of lines and another on the minimal representation of lines, inspired by <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Method 1: Projective Representation</head><p>A projective line in space can be defined either by a pencil of planes (a pencil of planes is defined by two projective planes) or by any two of its points. The matrix</p><formula xml:id="formula_48">W p p p P T T T = ¢ ¢¢ F H G G G I K J J J</formula><p>should have rank two, so its kernel must also have dimension two. The range of W P defines the pencil of planes and the null space defines the projective line in space. Once again, using SVD to factorize W P gives us everything we want. Let W U V</p><formula xml:id="formula_49">P P P P T = S</formula><p>be the SVD of W P with ordered singular values. Two points of the line might be taken to be v 3 and v 4 , so the line is given by</p><formula xml:id="formula_50">Ov 3 + Pv 4 for O, P ° ‫.ޒ‬<label>(21)</label></formula><p>One advantage of this method is that, using subset selection <ref type="bibr" target="#b8">[9]</ref>, near singular views can be detected and discarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Method 2: Minimal Representation</head><p>As a space line has four d.o.f., it can be minimally represented by four parameters. One such possibility is suggested by <ref type="bibr" target="#b7">[8]</ref> which uses a four-vector l T = (a, b, x 0 , y 0 ) T such that the line is defined as the intersection of two planes (1, 0, a, x 0 ) T and (0, 1, b, y 0 ) T with equations:</p><p>x az x y bz y</p><formula xml:id="formula_51">= + = + R S T 0 0</formula><p>Geometrically, this minimal representation gives a 3D line with direction (a, b, 1)</p><p>T and passing through the point (x 0 , y 0 , 0) T . This representation excludes, therefore, the lines of direction (a, b, 0) T , parallel to the xy plane. Two other representations are needed, each excluding either the directions (0, b, c) T or (a, 0, c) T . These three representations together completely describe any line in space.</p><p>In our case, we have no problem in automatically selecting one of the three representations, as the directions of lines have been obtained in the first step of factorization, allowing us to switch to one of the three representations. There remain only two unknown parameters x 0 and y 0 for each line.</p><p>To get a solution for x 0 and y 0 , as the two planes (1, 0, a, x 0 )</p><p>T and (0, 1, b, y 0 ) T defining the line belong to the pencil of planes defined by W P , we can still stack these two planes on the top of W P to get the matrix ¢ W P : Since this matrix still has rank two, all its 3 3 minors vanish. Each minor involving x 0 and y 0 gives a linear equation in x 0 and y 0 . With n views, a linear equation system is obtained</p><formula xml:id="formula_52">¢ = F H G G G I K J J J = F H G G G G I K J J J J</formula><formula xml:id="formula_53">A b n x y ¥ F H G I K J= 2 0 0 . (<label>22</label></formula><formula xml:id="formula_54">)</formula><p>This can be nicely solved using least squares for each line.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">AFFINE-STRUCTURE-FROM-LINES THEOREM</head><p>Summarizing the results obtained above, we have established the following: For the recovery of affine shape and affine motion from line correspondences with an uncalibrated affine camera, the minimum number of views needed is three and the minimum number of lines required is seven for a linear solution. There are always two solutions for the recovered affine structure. This result can be compared with that of Koenderink and Van Doorn <ref type="bibr" target="#b13">[14]</ref> for affine structure with a minimum of two views and five points.</p><p>We should also note the difference with the well-known results established for both calibrated and uncalibrated projective cameras <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>: A minimum of 13 lines in three views is required to have a linear solution. It is important to note that with the affine camera and the method presented in this paper, the number of line correspondences for achieving a linear solution is reduced from 13 to seven, which is of great practical importance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">OUTLINE OF THE SEVEN-LINE THREE-VIEW ALGORITHM</head><p>The linear algorithm to recover 3D affine shape/motion from at least seven line correspondences over three views with uncalibrated affine cameras may be outlined as follows:</p><p>1) If an image line segment is represented by its endpoints w 1 = (u 1 , v 1 ) T and w 2 = (u 2 , v 2 ) T , compute the direction vector of the line n w = u 2 u 1 . View this as the homogeneous coordinates of a point in 3 1 . 2) Compute the tensor components (T ijk ) defined by <ref type="bibr" target="#b8">(9)</ref> linearly with at least seven lines in three views. 3) Retrieve the projection matrices {M, M, M} of the onedimensional camera from the estimated tensor using ( <ref type="formula" target="#formula_33">17</ref>), <ref type="bibr" target="#b17">(18)</ref>, and ( <ref type="formula" target="#formula_31">16</ref>). There are always two solutions.</p><p>4) Perform 2D projective reconstruction using <ref type="bibr" target="#b6">(7)</ref>, which recovers the directions of the affine lines in space and the uncalibrated rotations of the camera motion. 5) Solve the uncalibrated translation vector (t, t, t) T using ( <ref type="formula" target="#formula_46">20</ref>) by linear least squares. 6) Compute the final affine lines in space using <ref type="bibr" target="#b20">(21)</ref> or <ref type="bibr" target="#b21">(22)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">LINE-BASED FACTORIZATION METHOD FROM AN IMAGE STREAM</head><p>The linear affine reconstruction algorithm described above deals with redundant lines, but is limited to three views. In this section, we discuss redundant views, extending the algorithm from the minimum of three to any number N &gt; 3 of views.</p><p>In the past few years, a family of algorithms for structure from motion using highly redundant image sequences called factorization methods have been extensively studied <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b28">[29]</ref> for point correspondences for affine cameras. Algorithms of this family directly decompose the feature points of the image stream into object shape and camera motion. More recently, a factorization based algorithm has been proposed by Triggs and Sturm <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b35">[36]</ref> for 3D projective reconstruction. We will accommodate our line-based algorithm to this projective factorization schema to handle redundant views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">2D Projective Reconstruction by Rescaling</head><p>According to <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b35">[36]</ref>, 3D projective reconstruction is equivalent to the rescaling of the 2D image points. We have already proven that recovering the directions of affine lines in space is equivalent to 2D projective reconstruction from one-dimensional projective images. Therefore, a reconstruction of the line directions in 3D can be obtained by rescaling the direction vectors, viewed as points of 3 1 .</p><p>For each 1D image point in three views [cf. ( <ref type="formula">6</ref> T is to use the basic reconstruction <ref type="bibr" target="#b6">(7)</ref> directly or alternatively to observe the following matrix identity:</p><formula xml:id="formula_55">M u M u M u M M M I x l l l ¢ ¢ ¢ ¢¢ ¢¢ ¢¢ F H G I K J = ¢ ¢¢ F H G I K J ¥ 3 3 c h.</formula><p>The rank of the left matrix is, therefore, at most three. All 4 4 minors vanish, and three (3 = ( <ref type="formula" target="#formula_7">4</ref>3) (6 3)) of them are algebraically independent, for instance,</p><formula xml:id="formula_56">M u M u l l ¢ ¢ ¢ = 0, ¢ ¢ ¢ ¢¢ ¢¢ ¢¢ = M u M u l l 0, M u m u m u l l l ¢ ¢ ¢ ¢¢ ¢¢ ¢¢ = 1 1 0.</formula><p>Each of them can be expanded by cofactors in the last column to obtain a linear homogeneous equation in O, O, O.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Therefore (O, O, O)</head><p>T can be solved linearly using</p><formula xml:id="formula_57">* * * * * * * * * F H G I K J ¢ ¢¢ F H G I K J = l l l 0 . (<label>23</label></formula><formula xml:id="formula_58">)</formula><p>where designates a known constant entry in the matrix.</p><p>For each triplet of views, the image points can be consistently rescaled according to <ref type="bibr" target="#b22">(23)</ref>. For the case of n &gt; 3 views, we can take appropriate triplets among n views such that each view is contained in at least two triplets. Then, the rescaling equations of all triplets of views for any given point can be chained together over n views to give a con- <ref type="figure">sistent (O,</ref><ref type="figure">O,</ref><ref type="figure">,</ref><ref type="figure">O</ref> (n) ) T . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Direction Factorization</head><formula xml:id="formula_59">= ¢ ¢ ¢ ¢ ¢ ¢ F H G G G G I K J J J J l l l l l l l l l 1 2 1 2 1 2 1 2 1 2 1 2 L L M M O M L a f a f a f a f a f a f</formula><p>. Since the following matrix equation holds for the measurement matrix W D :</p><formula xml:id="formula_60">W M D M M M d d d D D D n x x x m = = ¢ F H G G G I K J J J M L a f e j 1 2</formula><p>, the rank of W D is at most of three. The factorization method can then be applied to</p><formula xml:id="formula_61">W D . Let W U V D D D D T = S</formula><p>be the SVD factorization (cf. <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b26">[27]</ref>) of W D . The 3 3 diagonal matrix 6 D3 is obtained by keeping the first three singular values (assuming that singular values are ordered) of 6 and U D3 (V D3 ) are the first three columns (rows) of U(V).</p><p>Then, the product U V are also valid solutions, as we have</p><formula xml:id="formula_62">$ $ $ $ $ $ MAA D M D MD - = ¢ ¢ = 1 .</formula><p>This means that the recovered direction matrix $ D and the rotation matrix $ M are only defined up to an affine transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Translation Factorization-Step 2</head><p>We can stack all of the interpretation planes from different views of a given line to form the following n 4 measurement matrix of planes:</p><formula xml:id="formula_63">W l t l t l t P T T n n T = ¢ ¢ F H G G G I K J J J * * * * * * * * * M M M M a f a f</formula><p>. This matrix W P geometrically represents a pencil of planes, so it still has rank at most two. For any three rows i, j, and k of W P , taking any minor involving the t (i) , we obtain</p><formula xml:id="formula_64">* * * * * * l t l t l t i i j j k k T T T a f a f b g b g b f b f = 0 .</formula><p>Expanding this minor by cofactors in the last column gives a homogeneous linear equation in t (i) , t (j) , and t (k) :</p><formula xml:id="formula_65">¥ ¥ ¥ F H G G G I K J J J = a f a f b g b f t t t i j k 0 ,</formula><p>where each "" designates a constant three-vector in a row. Collecting all these minors together, we obtain</p><formula xml:id="formula_66">¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ F H G G I K J J ¢ F H G G G I K J J J = 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 L L M M M M M O M M M L M t t t n a f</formula><p>. We may take t = (0, 0, 1) T up to a scaling factor, say t 0 , so the final homogeneous linear equations to solve for</p><formula xml:id="formula_67">t t t 0 , , ... , ¢ n T a f e j are W t t t t T n n t t 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ¢ F H G G G I K J J J = ¥ ¥ ¥ ¥ ¥ ¥ ¥ ¥ F H G G I K J J ¢ F H G G G I K J J J = M L L M M M M M O M M M L M a f a f</formula><p>* Once again, this system of equations can be nicely solved by SVD factorization of W T . The least squares solution for</p><formula xml:id="formula_68">t t t 0 , , ... , ¢ n T</formula><p>a f e j , subject to t t t 0 1 , , , ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>¢ ¢¢ =</head><p>c h T , is the singu- lar vector corresponding to the smallest singular value of W T . Note that the efficiency of the computation can be further improved if the block diagonal structure of W T is exploited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.4">Shape Factorization-Step 3</head><p>The shape reconstruction method developed for three views extends directly to more than three views. Given n views, for each line across n views, we just augment the matrix W p from a 3 4 to n 4 matrix, then apply exactly the same method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">OUTLINE OF THE LINE-BASED FACTORIZATION ALGORITHM</head><p>The line-based factorization algorithm can be outlined as follows:</p><p>1) For triplets of views, compute the tensor (T ijk ) associated with each triplet, then rescale the directions of lines of the triplet using <ref type="bibr" target="#b22">(23)</ref>. . However it may happen that the linear estimate of XX T is not positive-definite due to noise. An alternative nonlinear solution using Cholesky parameterization that ensures the positive-definiteness can be found in <ref type="bibr" target="#b28">[29]</ref>.</p><p>Once we obtain the appropriate $ X , then $ $ MX and $ $ X D -1</p><p>carry the rotations of the camera and the directions of lines. The remaining steps are the same as the uncalibrated affine camera case.</p><p>If we take the weak perspective as a particular affine camera model, with only the aspect ratio of the camera, Euclidean structure is obtained this way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12">EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.1">Simulation Setup</head><p>We first use simulated images to validate the theoretical development of the algorithm. To preserve realism, the simulation is set up as follows. First, a real camera is calibrated by placing a known object of about 50 cm 3 in front of the camera. The camera is moved around the object through different positions. A calibration procedure gives the projection matrices at different positions, and these projection matrices are rounded to affine projection matrices. Three different positions which cover roughly 45 o of the field of view are selected. A set of 3D line segments within a cube of 30 cm 3 is generated synthetically and projected onto the different image planes by the affine projection matrices. All simulated images are of size 512 512. Both 3D and 2D line segments are represented by their endpoints.</p><p>The noise-free line segments are then perturbed as follows. To take advantage of the relatively higher accuracy of line position obtained by the line fitting process in practice, each 2D line segment is first resampled into a list of evenly spaced points of the line segment. The position of each point is perturbed by varying levels of noise of uniform distribution. The final perturbed line is obtained by a least squares fit to the perturbed point data.</p><p>Reconstruction is performed with 21 line segments and two different resample rates. The average residual error is defined to be the average distance of the midpoint of the image line segment to the reprojected line in the image plane from the 3D reconstructed line. In Table <ref type="table" target="#tab_1">1</ref>, the average residual errors of reconstruction are given with various noise levels. The number of points used to fit the line is the length of the line segment in pixels, this re-sample rate corresponds roughly to the digitization process. Table <ref type="table" target="#tab_2">2</ref> shows the results with the number of points used to fit the line equal to only one fourth the length of the line segment. We can notice that the degradation with the increasing noise level is very graceful and the reconstruction results remain acceptable with up to ±5.5 pixel noise. These good results show that the reconstruction algorithm is numerically stable. While comparing Tables <ref type="table" target="#tab_1">1</ref> and<ref type="table" target="#tab_2">2</ref>, it shows that higher resample rate gives better results, this confirms the importance of the line fitting procedure-the key advantage of line features over point features.</p><p>Another influential factor for the stability of the algorithm is the number of lines used. Table <ref type="table" target="#tab_3">3</ref> confirms that the more lines used, the better the results obtained. In this test, the pixel error is set to ±1.5.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.2">The Experiment With Real Images</head><p>A Fujinon/Photometrics CCD camera is used to acquire a sequence of images of a box of size 12 12 12.65 cm. The image resolution is 576 384. Three of the frames in the sequence used by the experiments are shown in Fig. <ref type="figure" target="#fig_4">1</ref>.</p><p>A Canny-like edge detector is first applied to each image. The contour points are then linked and fitted to line segments by least squares. The line correspondences across three views are selected by hand. There are a total of 46 lines selected, as shown in Fig. <ref type="figure" target="#fig_8">2</ref>.</p><p>The reconstruction algorithm generates infinite 3D lines, each defined by two arbitrary points on it. 3D line segments are obtained as follows. We reproject 3D lines into one image plane. In the image plane selected, the corresponding original image line segments are orthogonally projected onto the reprojected lines to obtain the reprojected line segments. Finally, by back-projecting the reprojected line segments to space, we obtain the 3D line segments, each defined by its two endpoints.</p><p>Excellent reconstruction results are obtained. An average residual error of one tenth of a pixel is achieved. Fig. <ref type="figure" target="#fig_5">3</ref> shows two views of the reconstructed 3D line segments. We notice that the affine structure of the box is almost perfectly recovered.</p><p>Table <ref type="table" target="#tab_4">4</ref> shows the influence of the number of line segments used by the algorithm. The reconstruction results degrade gracefully with decreasing number of lines.</p><p>Table <ref type="table" target="#tab_5">5</ref> shows the influence of the distribution of line segments in space. For instance, one degenerate case for structure from motion is that when all line segments in space lie on the same plane. Actually, in our images, line segments lie on three different planes-pentagon face, star shape face,  and rectangle face-of the box. We also performed experiments with line segments lying on only two planes. Table <ref type="table" target="#tab_5">5</ref> shows the results with various different two-plane configurations. Compared with the three-plane configuration, the reconstruction algorithm does almost equally well.</p><p>To illustrate the effect of using affine camera model as an approximation to the perspective camera, we used a bigger cube of size 30 30 30 cm, which is two and a half times the size of the first smaller cube. The affine approximation to the perspective camera is becoming less accurate than it was with the smaller cube. A sequence of images of this cube is acquired in almost the same conditions as for the smaller cube. The perspective effect of the big cube is slightly more pronounced as shown in Fig. <ref type="figure" target="#fig_10">4</ref>. The configuration of line segments is preserved. A total of 39 line segments of three views is used to perform the reconstruction. Fig. <ref type="figure" target="#fig_11">5</ref> illustrates two reprojected views of the reconstructed 3D line segments. Compared with Fig. <ref type="figure" target="#fig_5">3</ref>, the reconstruction is slightly degraded: In the top view of Fig. <ref type="figure" target="#fig_11">5</ref>, we notice that one segment falls a little apart from the pentagon face of the cube. Globally, the degradation is quite graceful, as the average residual error is only 0.3 pixel, compared with 0.12 pixel for the smaller cube.</p><p>The affine structures obtained can be converted to Euclidean ones (up to a global scaling factor) as soon as we know the aspect ratio <ref type="bibr" target="#b28">[29]</ref>, which is actually one for the camera used. Fig. <ref type="figure" target="#fig_12">6</ref> shows the rectified affine shape illustrated in Fig. <ref type="figure" target="#fig_5">3</ref>. The two sides of the box are accurately orthogonal to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13">DISCUSSION</head><p>A linear line-based structure from motion algorithm for uncalibrated affine cameras has been presented. The algorithm requires a minimum of seven line correspondences over three views. It has also been proven that seven lines over three views are the strict minimum data needed to    recover affine structure with uncalibrated affine cameras. In other words, in contrast to projective cameras, the linear algorithm is not based on the over-parametrization. This gives the algorithm intrinsic stability. The previous results of Koenderink and Van Doorn <ref type="bibr" target="#b13">[14]</ref> on affine structure from motion using point correspondences are therefore extended to line correspondences. To handle the case of redundant views, a factorization method was also developed. The experimental results based on real and simulated image sequences demonstrate the accuracy and the stability of the algorithms.</p><p>As the algorithms presented in this paper are developed within the same framework as suggested in <ref type="bibr" target="#b28">[29]</ref> for points, it is straightforward to integrate both points and lines into the same framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>m 1 T and m 2 T</head><label>12</label><figDesc>From the projection equation Ou = Mx, each point of a view u = (u 1 , u 2 ) T gives one homogeneous linear equation in the unknown point x in 3 2 are the first and second row vector of the matrix M. With one point correspondence in three views u u u, we have the following homogeneous linear equation system, designates a constant entry. This equation system can be easily solved for x, either considered as a point in 3 2 or as an affine line direction in ‫ޒ‬ 3 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>)], the scale factors O, O, and O-taken individually-are arbitrary. However, taken as a whole (O, O, O) T , they encode the projective structure of the points x in 3 2 . One way to recover the scale factors (O, O, O)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>-Step 1</head><label>1</label><figDesc>Suppose we are given m line correspondences in n views. The view number is indexed by a superscript and the line number by a subscript. We can now create the 2n m measurement matrix W D of all lines in all views by stacking all the direction vectors d w j i b g properly rescaled by l i j b g of m lines in n views as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>For any nonsingular 3 3</head><label>3</label><figDesc>matrix A 3 3 -either considered as a projective transformation in 3 2 or as an affine transformation in ‫ޒ‬ 3 ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>2 ) 4 ) 5 )</head><label>245</label><figDesc>Chain together all the rescaling factors (O, O, , O (n) ) T for each line across the sequence. 3) Factorize the rescaled measurement matrix of directions W D = U6V T to get the uncalibrated rotations and the directions of the affine lines Factorize the measurement matrix using the constraints on the motion W T = U6V T to get the uncalibrated translation vector $ , $ , ... , Factorize the measurement matrix of the interpretation planes for each line correspondence over all views W P = U6V T to get two points of the line $ far we have worked with an uncalibrated affine camera, the recovered shape and motion are defined up to an affine transformation in space. If the cameras are calibrated, then the affine structure can be converted into a Euclidean one up to an unknown global scale factor. Following the decomposition of the submatrix M 2 3 of the affine camera A 3 4 as M = KR introduced in [29], the metric information from the calibrated affine camera is completely contained in the affine intrinsic parameters KK T . Each view with the associated uncalibrated rotation matrix for the unknown affine transformation X which upgrades the affine structure to a Euclidean one. A linear solution may be expected as soon as we have three views if we solve for the entries of XX T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 1 .</head><label>1</label><figDesc>Three original images of the box used for the experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Line segments selected across the image sequence.</figDesc><graphic coords="9,329.41,429.21,160.02,152.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Reconstructed 3D line segments: a general view and a top view.</figDesc><graphic coords="10,72.74,198.40,151.23,139.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. One original image of the big cube image sequence.</figDesc><graphic coords="10,62.96,371.48,170.69,111.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Two views of the reconstructed line segments for the big box: (top) a general view and (bottom) a top view.</figDesc><graphic coords="10,348.62,174.96,121.00,116.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. A side view of the Euclidean shape obtained by using the known aspect ratio of the camera.</figDesc><graphic coords="10,323.14,327.97,171.86,172.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 AVERAGE</head><label>1</label><figDesc>RESIDUAL ERRORS WITH VARIOUS NOISE LEVELS FOR THE RECONSTRUCTION WITH 21 LINES OVER THREE VIEWS The number of points to fit the line is the length of the line segment in pixels.</figDesc><table><row><cell>Noise</cell><cell>± 0.5</cell><cell>± 1.5</cell><cell>± 2.5</cell><cell>± 3.5</cell><cell>± 4.5</cell><cell>± 5.5</cell></row><row><cell>Average</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>residual</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>error</cell><cell>0.045</cell><cell>0.061</cell><cell>0.10</cell><cell>0.15</cell><cell>0.20</cell><cell>0.25</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 AVERAGE</head><label>2</label><figDesc>The number of points to fit the line segment is one fourth the length of the line segment.</figDesc><table><row><cell></cell><cell cols="6">RESIDUAL ERRORS OF RECONSTRUCTION WITH</cell></row><row><cell></cell><cell cols="4">VARIOUS NOISE LEVELS</cell><cell></cell><cell></cell></row><row><cell>Noise</cell><cell>± 0.5</cell><cell>± 1.5</cell><cell>± 2.5</cell><cell>± 3.5</cell><cell>± 4.5</cell><cell>± 5.5</cell></row><row><cell>Average</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>residual</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>error</cell><cell>0.077</cell><cell>0.26</cell><cell>0.31</cell><cell>0.44</cell><cell>0.65</cell><cell>1.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3 AVERAGE</head><label>3</label><figDesc>RESIDUAL ERRORS OF RECONSTRUCTION WITH ±1.5 PIXEL NOISE AND VARIOUS NUMBER OF LINES</figDesc><table><row><cell>Lines #</cell><cell>8</cell><cell>13</cell><cell>17</cell><cell>21</cell></row><row><cell>Average</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>residual</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>error</cell><cell>1.9</cell><cell>1.6</cell><cell>0.59</cell><cell>0.26</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4 RESIDUAL</head><label>4</label><figDesc>ERRORS OF RECONSTRUCTION WITH DIFFERENT NUMBER OF LINE SEGMENTS</figDesc><table><row><cell>Lines #</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>46</cell></row><row><cell>Average</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>residual</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>error</cell><cell>1.3</cell><cell>0.88</cell><cell>0.28</cell><cell>0.12</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5 RESIDUAL</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">ERRORS OF RECONSTRUCTION</cell><cell></cell></row><row><cell></cell><cell cols="3">WITH DIFFERENT DATA</cell><cell></cell></row><row><cell>Line con-</cell><cell>star+rect.+</cell><cell>star+rect.</cell><cell>pent.+rect.</cell><cell>star+pent.</cell></row><row><cell>figuration</cell><cell>pent.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Average</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>residual</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>error</cell><cell>0.12</cell><cell>0.078</cell><cell>0.14</cell><cell>0.28</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by CNRS and the French Ministère de l'Education, which is gratefully acknowledged. We would like to thank D. Morris and N. Chiba for their help during the development of this work.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Long Quan received the BSc degree in telecommunication from the Northern Jiao-Tong University, Beijing, China, in 1984, the MSc degree in computer science from the University of Nancy I in 1986, and the PhD degree in computer science from the Institut National Polytechnique de Lorraine, Nancy, France, in 1989. He joined Centre National de la Recherche Scientifique in 1990, where he is currently a senior research scientist at GRAVIR laboratory in Grenoble, France. He was a visiting scientist at the Robotics Institute, Carnegie Mellon University, during 1996. His current main research interests are 3D reconstruction, vision geometry, and applications of computer vision. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Takeo</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Perspective Approximations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="179" to="192" />
			<date type="published" when="1990-08">Aug. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
		<title level="m">Stereovision and Sensor Fusion</title>
		<meeting><address><addrLine>Cambridge, Mass</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Active Visual Navigation Using Non-Metric Structure</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Beardsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Fifth Int&apos;l Conf. Computer Vision</title>
		<meeting>Fifth Int&apos;l Conf. Computer Vision<address><addrLine>Cambridge, Mass.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-06">June 1995</date>
			<biblScope unit="page" from="58" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dual Computation of Projective Shape and Camera Positions from Multiple Images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weinshall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. Computer Vision</title>
		<imprint/>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Determination of the Attitude of 3D Objects From Single Perspective View</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dhome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Richetin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Lapresté</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1" to="265" />
			<date type="published" when="1989-12">Dec. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">What Can Be Seen in Three Dimensions With an Uncalibrated Stereo Rig?</title>
		<author>
			<persName><forename type="first">O</forename><surname>Faugeras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Second European Conf. Computer Vision</title>
		<meeting>Second European Conf. Computer Vision<address><addrLine>Santa Margherita Ligure, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-05">May 1992</date>
			<biblScope unit="page" from="563" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the Geometry and Algebra of the Point and Line Correspondences Between n Images</title>
		<author>
			<persName><forename type="first">O</forename><surname>Faugeras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mourrain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Fifth Int&apos;l Conf. Computer Vision</title>
		<meeting>Fifth Int&apos;l Conf. Computer Vision<address><addrLine>Cambridge, Mass.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-06">June 1995</date>
			<biblScope unit="page" from="951" to="956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Motion and Structure From Point and Line Matches</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">D</forename><surname>Faugeras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lustman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toscani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. First Int&apos;l Conf. Computer Vision</title>
		<meeting>First Int&apos;l Conf. Computer Vision<address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987-06">June 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Van Loan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Matrix Computation</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Johns Hopkins Univ. Press</publisher>
			<pubPlace>Baltimore</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lines and Points in Three Views-an Integrated Approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">G.E. CRD</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stereo From Uncalibrated Cameras</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Computer Vision and Pattern Recognition</title>
		<meeting>Conf. Computer Vision and Pattern Recognition<address><addrLine>Urbana-Champaign, Ill</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="761" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Projective Reconstruction From Line Correspondences</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Computer Vision and Pattern Recognition</title>
		<meeting>Conf. Computer Vision and Pattern Recognition<address><addrLine>Seattle, Wash</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="903" to="907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Object Pose: The Link Between Weak Perspective, Para Perspective, and Full Perspective</title>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Christy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dornaika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INRIA</title>
		<imprint>
			<date type="published" when="1994-09">Sept. 1994</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Affine Structure From Motion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Koenderink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Doorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Optical Soc. Am. A</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="377" to="385" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Finding Point Correspondences and Determining Motion of a Rigid Object From Two Weak Perspective Views</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, Graphics, and Image Processing</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="309" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Estimation of Rigid Body Motion Using Straight Line Correspondences</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, Graphics, and Image Processing</title>
		<imprint>
			<date type="published" when="1988-07">July 1988</date>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="37" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Linear Algorithm for Motion Estimation Using Straight Line Correspondences</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, Graphics, and Image Processing</title>
		<imprint>
			<date type="published" when="1988-10">Oct. 1988</date>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="35" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Computer Program for Reconstructing a Scene From Two Projections</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Longuet-Higgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">293</biblScope>
			<biblScope unit="page" from="133" to="135" />
			<date type="published" when="1981-09">Sept. 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Matrice fondamentale et autocalibration en vision par ordinateur</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992-12">Dec. 1992</date>
			<pubPlace>Orsay, France</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Université de Paris-Sud</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Thèse de doctorat</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Canonic Representations for the Geometries of Multiple Projective Views</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vieville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Dept. of Electrical Engr. and Computer Science</title>
		<imprint>
			<date type="published" when="1993-10">Oct. 1993</date>
		</imprint>
		<respStmt>
			<orgName>Univ. of California, Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recursive Affine Structure and Motion From Image Sequences</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Mclauchlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Third European Conf. Computer Vision</title>
		<meeting>Third European Conf. Computer Vision<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="217" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Relative 3D Reconstruction Using Multiple Uncalibrated Images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mohr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Veillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. Robotics Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="619" to="632" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Geometric Invariance in Computer Vision</title>
		<editor>J.L. Mundy and A. Zisserman</editor>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, Mass</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Obtaining Surface Orientation From Texels Under Perspective Projection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Otha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Maenobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sakai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Seventh Int&apos;l Joint Conf</title>
		<imprint>
			<biblScope unit="page" from="746" to="751" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
	<note>Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Optimal Estimation of Object Pose From a Single Perspective View</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Q</forename><surname>Phong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yassine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Fourth Int&apos;l Conf. Computer Vision</title>
		<meeting>Fourth Int&apos;l Conf. Computer Vision<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-05">May 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Paraperspective Factorization Method for Shape and Motion Recovery</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Poelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Third European Conf. Computer Vision</title>
		<meeting>Third European Conf. Computer Vision<address><addrLine>Stockholm</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-05">May 1994</date>
			<biblScope unit="page" from="97" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Flannery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Teukolsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Vetterling</surname></persName>
		</author>
		<title level="m">Numerical Recipes in C</title>
		<meeting><address><addrLine>Cambridge, England</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge Univ. Press</publisher>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Invariants of Six Points and Projective Reconstruction From Three Uncalibrated Images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="46" />
			<date type="published" when="1995-01">Jan. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Self-Calibration of an Affine Camera From Multiple Views</title>
		<author>
			<persName><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="93" to="105" />
			<date type="published" when="1996-05">May 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A Factorization Method for Affine Structure From Line Correspondences</title>
		<author>
			<persName><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Computer Vision and Pattern Recognition</title>
		<meeting>Conf. Computer Vision and Pattern Recognition<address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="803" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Affine Shape Representation from Motion Through Reference Points</title>
		<author>
			<persName><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mohr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="249" to="254" />
			<date type="published" when="1991">1992. 1991</date>
			<pubPlace>New Jersey</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">3D Motion Recovery via Affine Epipolar Geometry</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="182" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Algebraic Functions for Recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="779" to="789" />
			<date type="published" when="1995-08">Aug. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Structure From Motion Using Line Correspondences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Spetsakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="171" to="183" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A Unified Theory of Structure From Motion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Spetsakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. DARPA Image Understanding Workshop</title>
		<meeting>DARPA Image Understanding Workshop</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="271" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A Factorization Based Algorithm for Multi-Image Projective Structure and Motion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Fourth European Conf. Computer Vision</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Buxton</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</editor>
		<meeting>Fourth European Conf. Computer Vision<address><addrLine>Cambridge, England</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="1996-04">Apr. 1996</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="709" to="720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Algorithms in Invariant Theory</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sturmfels</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Shape and Motion From Image Streams Under Orthography: A Factorization Method</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Motion Segmentation and Outlier Detection</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
		<respStmt>
			<orgName>Univ. of Oxford, Dept. of Engr. Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Matching Constraints and the Joint Image</title>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Fifth Int&apos;l Conf. Computer Vision, E. Grimson</title>
		<meeting>Fifth Int&apos;l Conf. Computer Vision, E. Grimson<address><addrLine>Cambridge, Mass., USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-06">June 1995</date>
			<biblScope unit="page" from="338" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">The Interpretation of Visual Motion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979">1979</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, Mass</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Recognition by Linear Combinations of Models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="1991-10">Oct. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Motion of Points and Lines in the Uncalibrated Case</title>
		<author>
			<persName><forename type="first">T</forename><surname>Viéville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">D</forename><surname>Faugeras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Linear and Incremental Acquisition of Invariant Shape Models From Image Sequences</title>
		<author>
			<persName><forename type="first">D</forename><surname>Weinshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Fourth Int&apos;l Conf. Computer Vision</title>
		<meeting>Fourth Int&apos;l Conf. Computer Vision<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Motion and Structure From Line Correspondences: Closed-Form Solution, Uniqueness, and Optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="318" to="336" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">3D Dynamic Scene Analysis, a Stereo Based Approach</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">D</forename><surname>Faugeras</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
