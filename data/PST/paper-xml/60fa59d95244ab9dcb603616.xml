<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-07-22">22 Jul 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
							<email>xintaowang@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Applied Research Center (ARC)</orgName>
								<address>
									<country>Tencent PCG</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liangbin</forename><surname>Xie</surname></persName>
							<email>lb.xie@siat.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
							<email>chao.dong@siat.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
							<email>yingsshan@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Applied Research Center (ARC)</orgName>
								<address>
									<country>Tencent PCG</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-07-22">22 Jul 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2107.10833v1[eess.IV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>https://github.com/xinntao/Real-ESRGAN Bicubic ESRGAN RealSR Real-ESRGAN Figure 1: Comparisons of bicubic-upsampled, ESRGAN [44], RealSR [17], and our Real-ESRGAN results on real-life images. The Real-ESRGAN model trained with pure synthetic data is capable of enhancing details while removing annoying artifacts for common real-world images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Single image super-resolution (SR) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b24">25]</ref> is an active research topic, which aims at reconstructing a highresolution (HR) image from its low-resolution (LR) counterpart. Since the pioneering work of SRCNN <ref type="bibr" target="#b6">[7]</ref>, deep convolution neural network (CNN) approaches have brought prosperous developments in the SR field. However, most approaches <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b43">44]</ref> assume an ideal bicubic downsampling kernel, which is different from real degradations. This degradation mismatch makes those approaches unpractical in real-world scenarios.</p><p>Blind super-resolution <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b48">49]</ref>, on the contrary, aims to restore low-resolution images suffering from unknown and complex degradations. Existing approaches can be roughly categorized into explicit modeling and implicit modeling, according to the underlying degradation process. Classical degradation model <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27]</ref>, which consists of blur, downsampling, noise and JPEG compression (more details in Sec. 3.1), is widely adopted in explicit modeling meth-ods <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b30">31]</ref>. However, the real-world degradations are usually too complex to be modeled with a simple combination of multiple degradations. Thus, these methods will easily fail in real-world samples. Implicit modeling methods <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b39">40]</ref> utilize data distribution learning with Generative Adversarial Network (GAN) <ref type="bibr" target="#b11">[12]</ref> to obtain the degradation model. Yet, they are limited to the degradations within training datasets, and could not generalize well to out-of-distribution images. Readers are encouraged to refer to a recent blind SR survey <ref type="bibr" target="#b25">[26]</ref> for a more comprehensive taxonomy.</p><p>In this work, we aim to extend the powerful ESR-GAN <ref type="bibr" target="#b43">[44]</ref> to restore general real-world LR images by synthesizing training pairs with a more practical degradation process. The real complex degradations usually come from complicate combinations of different degradation processes, such as imaging system of cameras, image editing, and Internet transmission. For example, when we take a photo with our cellphones, the photos may have several degradations, such as camera blur, sensor noise, sharpening artifacts, and JPEG compression. We then do some editing and upload to a social media APP, which introduces further compression and unpredictable noises. The above process becomes more complicated when the image is shared several times on the Internet. This motivates us to extend the classical "first-order" degradation model to "high-order" degradation modeling for real-world degradations, i.e., the degradations are modeled with several repeated degradation processes, each process being the classical degradation model. Empirically, we adopt a second-order degradation process for a good balance between simplicity and effectiveness. A recent work <ref type="bibr" target="#b47">[48]</ref> also proposes a random shuffling strategy to synthesize more practical degradations. However, it still involves a fixed number of degradation types, and the rationality behind is unclear. Instead, high-order degradation modeling is more flexible and attempts to mimic the real degradation generation process. We further incorporate sinc filters in the synthesis process to simulate the common ringing and overshoot artifacts.</p><p>As the degradation space is much larger than ESRGAN, the training also becomes challenging. Specifically, 1) the discriminator requires a more powerful capability to discriminate realness from complex training outputs, while the gradient feedback from the discriminator needs to be more accurate for local detail enhancement. Therefore, we improve the VGG-style discriminator in ESRGAN to an U-Net design. 2) The U-Net structure and complicate degradations also increase the training instability. Thus, we employ the spectral normalization (SN) regularization to stabilize the training dynamics. Equipped with the dedicated improvements, we are able to easily train our Real-ESRGAN and achieve a good balance of local detail en-hancement and artifact suppression.</p><p>To summarize, in this work, 1) we propose a high-order degradation process to model practical degradations, and utilize sinc filters to model common ringing and overshoot artifacts. 2) We employ several essential modifications (e.g., U-Net discriminator with spectral normalization) to increase discriminator capability and stabilize the training dynamics. 3) Real-ESRGAN trained with pure synthetic data is able to restore most real-world images and achieve better visual performance than previous works, making it more practical in real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The image super-resolution field <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b27">28]</ref> has witnessed a variety of developments since SRCNN <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. To achieve visually-pleasing results, generative adversarial network <ref type="bibr" target="#b12">[13]</ref> is usually employed as loss supervisions to push the solutions closer to the natural manifold <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b42">43]</ref>. Most methods assume a bicubic downsampling kernel and usually fail in real images. Recent works also incorporate reinforcement learning or GAN prior to image restoration <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>There have been several excellent explorations in blind SR. The first category involves explicit degradation representations and typically consists of two components: degradation prediction and conditional restoration. The above two components are performed either separately <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b48">49]</ref> or jointly (iteratively) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b39">40]</ref>. These approaches rely on predefined degradation representations (e.g., degradation types and levels), and usually consider simple synthetic degradations. Moreover, inaccurate degradation estimations will inevitably result in artifacts.</p><p>Another category is to obtain/generate training pairs as close to real data as possible, and then train a unified network to address blind SR. The training pairs are usually 1) captured with specific cameras followed by tedious alignments <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b44">45]</ref>; 2) or directly learned from unpaired data with cycle consistency loss <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b29">30]</ref>; 3) or synthesized with estimated blur kernels and extracted noise patches <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b16">17]</ref>. However, 1) the captured data is only constrained to degradations associated with specific cameras, and thus could not well generalize to other real images; 2) learning finegrained degradations with unpaired data is challenging, and the results are usually unsatisfactory. Degradation models. Classical degradation model <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27]</ref> is widely adopted in blind SR methods <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b30">31</ref>]. Yet, real-world degradations are usually too complex to be explicitly modeled. Thus, implicit modeling attempts to learn a degradation generation process within networks <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b39">40]</ref>. In this work, we propose a flexible high-order degradation model to synthesize more practical degradations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Classical Degradation Model</head><p>Blind SR aims to restore high-resolution images from low-resolution ones with unknown and complex degradations. The classical degradation model <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27]</ref> is usually adopted to synthesize the low-resolution input. Generally, the ground-truth image y is first convolved with blur kernel k. Then, a downsampling operation with scale factor r is performed. The low-resolution x is obtained by adding noise n. Finally, JPEG compression is also adopted, as it is widely-used in real-world images.</p><formula xml:id="formula_0">x = D(y) = [(y k) ↓ r +n] JPEG ,<label>(1)</label></formula><p>where D denotes the degradation process. In the following, we briefly revisit these commonly-used degradations. The detailed settings are specified in Sec. 4.1. More descriptions and examples are in Appendix. A.</p><p>Blur. We typically model blur degradation as a convolution with a linear blur filter (kernel). Isotropic and anisotropic Gaussian filters are common choices. For a Gaussian blur kernel k with a kernel size of 2t + 1, its (i, j) ∈ [−t, t] element is sampled from a Gaussian distribution, formally:</p><formula xml:id="formula_1">k(i, j) = 1 N exp(− 1 2 C T Σ −1 C), C = [i, j] T , (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where Σ is the covariance matrix; C is the spatial coordinates; N is the normalization constant. The covariance matrix could be further represented as follows:</p><formula xml:id="formula_3">Σ = R σ 2 1 0 0 σ 2 2 R T , (R is the rotation matrix) (3) = cosθ −sinθ sinθ cosθ σ 2 1 0 0 σ 2 2 cosθ sinθ −sinθ cosθ ,<label>(4)</label></formula><p>where σ 1 and σ 2 are the standard deviation along the two principal axes (i.e., eigenvalues of the covariance matrix); θ is the rotation degree. When σ 1 = σ 2 , k is an isotropic Gaussian blur kernel; otherwise k is an anisotropic kernel. Discussion. Though Gaussian blur kernels are widely used to model blur degradation, they may not well approximate real camera blur. To include more diverse kernel shapes, we further adopt generalized Gaussian blur kernels <ref type="bibr" target="#b28">[29]</ref> and a plateau-shaped distribution. Their probability density function (pdf) are</p><formula xml:id="formula_4">1 N exp(− 1 2 (C T Σ −1 C) β ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and</head><p>Noise. We consider two commonly-used noise types -1) addictive Gaussian noise and 2) Poisson noise. Addictive Gaussian noise has a probability density function equal to that of the Gaussian distribution. The noise intensity is controlled by the stand deviation (i.e., sigma value) of the Gaussian distribution. When each channel of RGB images has independent sampled noise, the synthetic noise is color noise. We also synthesize gray noise by employing the same sampled noise to all three channels. Poisson noise (or shot noise) follows the Poisson distribution. It can model the sensor noise caused by statistical quantum fluctuations, that is, variation in the number of photons sensed at a given exposure level. Poisson noise has an intensity proportional to the image intensity, and the noises at different pixels are independent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Resize (Downsampling).</head><p>Downsampling is a basic operation for synthesizing low-resolution images in SR. More generally, we consider both downsamping and upsampling, i.e., the resize operation. There are several resize algorithms -nearest-neighbor interpolation, area resize, bilinear interpolation, and bicubic interpolation. Different resize operations bring in different effects -some produce blurry results while some may output over-sharp images with overshoot artifacts.</p><p>In order to include more diverse and complex resize effects, we consider a random resize operation from the above choices. As nearest-neighbor interpolation introduces the misalignment issue, we exclude it and only consider the area, bilinear and bicubic operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>JPEG compression.</head><p>JPEG compression is a commonly used technique of lossy compression for digital images. It first converts images into the YCbCr color space and downsamples the chroma channels. Images are then split into 8 × 8 blocks and each block is transformed with a two-dimensional discrete cosine transform (DCT), followed by a quantization of DCT coefficients. More details of JPEG compression algorithms can be found in <ref type="bibr" target="#b36">[37]</ref>. Unpleasing block artifacts are usually introduced by the JPEG compression.</p><p>The quality of compressed images is determined by a quality factor q ∈ [0, 100], where a lower q indicates a higher compression ratio and worse quality. We use the Py-Torch implementation -DiffJPEG<ref type="foot" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">High-order Degradation Model</head><p>When we adopt the above classical degradation model to synthesize training pairs, the trained model could indeed handle some real samples. However, it still can not resolve some complicated degradations in the real world, especially the unknown noises and complex artifacts (see Fig. <ref type="figure" target="#fig_1">3</ref>). It is because that the synthetic low-resolution images still have  Yet, they amplify noises or introduce ringing artifacts for complex real-world images (Right). Zoom in for best view a large gap with realistic degraded images. We thus extend the classical degradation model to a high-order degradation process to model more practical degradations. The classical degradation model only includes a fixed number of basic degradations, which can be regarded as a first-order modeling. However, the real-life degradation processes are quite diverse, and usually comprise a series of procedures including imaging system of cameras, image editing, Internet transmission, etc. For instance, when we want to restore a low-quality image download from the Internet, its underlying degradation involves a complicated combination of different degradation processes. Specifically, the original image might be taken with a cellphone many years ago, which inevitably contains degradations such as camera blur, sensor noise, low resolution and JPEG compression. The image was then edited with sharpen-ing and resize operations, bringing in overshoot and blur artifacts. After that, it was uploaded to some social media applications, which introduces a further compression and unpredictable noises. As the digital transmission will also bring artifacts, this process becomes more complicated when the image spreads several times on the Internet.</p><p>Such a complicated deterioration process could not be modeled with the classical first-order model. Thus, we propose a high-order degradation model. An n-order model involves n repeated degradation processes (as shown in Eq. 5), where each degradation process adopts the classical degradation model (Eq. 1) with the same procedure but different hyper-parameters. Note that the "high-order" here is different from that used in mathematical functions. It mainly refers to the implementation time of the same operation. In order to keep the image resolution in a reasonable range, the downsampling operation in Eq. 1 is replaced with a random resize operation. Empirically, we adopt a second-order degradation process, as it could resolve most real cases while keeping simplicity. Fig. <ref type="figure" target="#fig_0">2</ref> depicts the overall pipeline of our pure synthetic data generation pipeline.</p><formula xml:id="formula_5">x = D n (y) = (D n • • • • • D 2 • D 1 )(y).<label>(5)</label></formula><p>It is worth noting that the improved high-order degradation process is not perfect and could not cover the whole degradation space in the real world. Instead, it merely extends the solvable degradation boundary of previous blind SR methods through modifying the data synthesis process. Several typical limitation scenarios can be found in Fig. <ref type="figure">11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Ringing and overshoot artifacts</head><p>Ringing artifacts often appear as spurious edges near sharp transitions in an image. They visually look like bands or "ghosts" near edges. Overshoot artifacts are usually combined with ringing artifacts, which manifest themselves as an increased jump at the edge transition. The main cause of   Zoom in for best view these artifacts is that the signal is bandlimited without high frequencies. These artifacts are very common and usually produced by a sharping algorithm, JPEG compression, etc. Fig. <ref type="figure" target="#fig_4">5</ref> (Top) shows some real samples suffering from ringing and overshoot artifacts. We employ the sinc filter, an idealized filter that cuts off high frequencies, to synthesize ringing and overshoot artifacts for training pairs. The sinc filter kernel can be expressed as 2 :</p><formula xml:id="formula_6">k(i, j) = ω c 2π i 2 + j 2 J 1 (ω c i 2 + j 2 ),<label>(6)</label></formula><p>where (i, j) is the kernel coordinate; ω c is the cutoff frequency; and J 1 is the first order Bessel function of the first kind. Fig. <ref type="figure" target="#fig_4">5</ref> (Bottom) shows sinc filters with different cutoff frequencies, and their corresponding filtered images. It is observed that it could well synthesize ringing and overshoot artifacts (especially introduced by over-sharp effects). These artifacts are visually similar to those in the first two real samples in Fig. <ref type="figure" target="#fig_4">5</ref> (Top). We adopt sinc filters in two places: the blurring process and the last step of the synthesis. The order of the last sinc filter and JPEG compression is randomly exchanged 2 We use the implementation in this url.  to cover a larger degradation space, as some images may be first over-sharpened (with overshoot artifacts) and then have JPEG compression; while some images may first do JPEG compression followed by sharpening operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Networks and Training</head><p>ESRGAN generator. We adopt the same generator (SR network) as ESRGAN <ref type="bibr" target="#b43">[44]</ref>, i.e., a deep network with several residual-in-residual dense blocks (RRDB), as shown in Fig. <ref type="figure" target="#fig_2">4</ref>. We also extend the original ×4 ESRGAN architecture to perform super-resolution with a scale factor of ×2 and ×1. As ESRGAN is a heavy network, we first employ the pixel-unshuffle (an inverse operation of pixelshuffle <ref type="bibr" target="#b35">[36]</ref>) to reduce the spatial size and enlarge the channel size before feeding inputs into the main ESRGAN architecture. Thus, the most calculation is performed in a smaller resolution space, which can reduce the GPU memory and computational resources consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>U-Net discriminator with spectral normalization (SN).</head><p>As Real-ESRGAN aims to address a much larger degradation space than ESRGAN, the original design of discriminator in ESRGAN is no longer suitable. Specifically, the discriminator in Real-ESRGAN requires a greater discriminative power for complex training outputs. Instead of discriminating global styles, it also needs to produce accurate gradient feedback for local textures. Therefore, we improve the VGG-style discriminator in ESRGAN to an U-Net design with skip connections <ref type="bibr" target="#b34">[35]</ref> (Fig. <ref type="figure" target="#fig_6">6</ref>). The U-Net outputs realness values for each pixel, and can provide detailed perpixel feedback to the generator.</p><p>In the meanwhile, the U-Net structure and complicate degradations also increase the training instability. We employ the spectral normalization regularization <ref type="bibr" target="#b32">[33]</ref> to stabi-lize the training dynamics. Moreover, we observe that spectral normalization is also beneficial to alleviate the oversharp and annoying artifacts introduced by GAN training. With those adjustments, we are able to easily train the Real-ESRGAN and achieve a good balance of local detail enhancement and artifact suppression.</p><p>The training process is divided into two stages. First, we train a PSNR-oriented model with the L1 loss. The obtained model is named by Real-ESRNet. We then use the trained PSNR-oriented model as an initialization of the generator, and train the Real-ESRGAN with a combination of L1 loss, perceptual loss <ref type="bibr" target="#b17">[18]</ref> and GAN loss <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Implementation</head><p>Training details.</p><p>Similar to ESRGAN, we adopt DIV2K <ref type="bibr" target="#b0">[1]</ref>, Flickr2K <ref type="bibr" target="#b38">[39]</ref> and OutdoorSceneTraining <ref type="bibr" target="#b42">[43]</ref> datasets for training. The training HR patch size is set to 256. We train our models with four NVIDIA V100 GPUs with a total batch size of 48. We employ Adam optimizer <ref type="bibr" target="#b20">[21]</ref>. Real-ESRNet is finetuned from ESR-GAN for faster convergence. We train Real-ESRNet for 1000K iterations with learning rate 2 × 10 −4 while training Real-ESRGAN for 400K iterations with learning rate 1 × 10 −4 . We adopt exponential moving average (EMA) for more stable training and better performance. Real-ESRGAN is trained with a combination of L1 loss, perceptual loss and GAN loss, with weights {1, 1, 0.1}, respectively. We use the {conv1, ...conv5} feature maps (with weights {0.1, 0.1, 1, 1, 1}) before activation in the pre-trained VGG19 network <ref type="bibr" target="#b17">[18]</ref> as the perceptual loss. Our implementation is based on the BasicSR <ref type="bibr" target="#b41">[42]</ref>. Degradation details. we employ a second-order degradation model for a good balance of simplicity and effectiveness. Unless otherwise specified, the two degradation processes have the same settings. We adopt Gaussian kernels, generalized Gaussian kernels and plateau-shaped kernels, with a probability of {0.7, 0.15, 0.15}. The blur kernel size is randomly selected from {7, 9, ...21}. Blur standard deviation σ is sampled from [0.2, 3] ([0.2, 1.5] for the second degradation process). Shape parameter β is sampled from [0.5, 4] and <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> for generalized Gaussian and plateau-shaped kernels, respectively. We also use sinc kernel with a probability of 0.1. We skip the second blur degradation with a probability of 0.2.</p><p>We employ Gaussian noises and Poisson noises with a probability of {0.5, 0.5}. The noise sigma range and Poisson noise scale are set to <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b29">30]</ref> and [0.05, 3], respectively ( <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">25]</ref> and [0.05, 2.5] for the second degradation process). The gray noise probability is set to 0.4. JPEG compression quality factor is set to <ref type="bibr" target="#b29">[30,</ref><ref type="bibr">95]</ref>. The final sinc filter is applied with a probability of 0.8. More details can be found in the released codes. Training pair pool. In order to improve the training efficiency, all degradation processes are implemented in Py-Torch with CUDA acceleration, so that we are able to synthesize training pairs on the fly. However, batch processing limits the diversity of synthetic degradations in a batch. For example, samples in a batch could not have different resize scaling factors. Therefore, we employ a training pair pool to increase the degradation diversity in a batch. At each iteration, the training samples are randomly selected from the training pair poor to form a training batch. We set the pool size to 180 in our implementation. Sharpen ground-truth images during training. We further show a training trick to visually improve the sharpness, while not introducing visible artifacts. A typical way of sharpening images is to employ a post-process algorithm, such as unsharp masking (USM). However, this algorithm tends to introduce overshoot artifacts. We empirically find that sharpening ground-truth images during training could achieve a better balance of sharpness and overshoot artifact suppression. We denote the model trained with sharped ground-truth images as Real-ESRGAN+ (comparisons are shown in Fig. <ref type="figure" target="#fig_7">7</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparisons with Prior Works</head><p>We compare our Real-ESRGAN with several stateof-the-art methods, including ESRGAN <ref type="bibr" target="#b43">[44]</ref>, DAN <ref type="bibr" target="#b30">[31]</ref>, CDC <ref type="bibr" target="#b44">[45]</ref>, RealSR <ref type="bibr" target="#b16">[17]</ref> and BSRGAN <ref type="bibr" target="#b47">[48]</ref>. We test on several diverse testing datasets with real-world images, including RealSR <ref type="bibr" target="#b3">[4]</ref>, DRealSR <ref type="bibr" target="#b44">[45]</ref>, OST300 <ref type="bibr" target="#b42">[43]</ref>, DPED <ref type="bibr" target="#b15">[16]</ref>, ADE20K validation <ref type="bibr" target="#b51">[52]</ref> and images from Internet. Since existing metrics for perceptual quality cannot well reflect the actual human perceptual preferences on the fine-grained scale <ref type="bibr" target="#b2">[3]</ref>, we present several representative visual samples in Fig. <ref type="figure" target="#fig_7">7</ref>.</p><p>It can be observed from Fig. <ref type="figure" target="#fig_7">7</ref> that our Real-ESRGAN outperforms previous approaches in both removing artifacts and restoring texture details. Real-ESRGAN+ (trained with sharpened ground-truths) can further boost visual sharpness. Specifically, the first sample contains overshoot artifacts (white edges around letters). Directly upsampling will inevitably amplify those artifacts (e.g., DAN and BSR-GAN). Real-ESRGAN takes such common artifacts into consideration and simulates them with sinc filter, thus effectively removing ringing and overshoot artifacts. The second sample contains unknown and complicated degradations. Most algorithms can not effectively eliminate them while Real-ESRGAN trained with second-order degradation processes could. Real-ESRGAN is also capable of restoring more realistic textures (e.g., brick, mountain and tree textures) for real-world samples, while other methods either fail to remove degradations or add unnatural textures (e.g., RealSR and BSRGAN).  More complicated blur kernels. We remove the generalized Gaussian kernel and plateau-shaped kernel in blur synthesis. As shown in Fig. <ref type="figure">10</ref>, on some real samples, the model cannot remove blur and recover sharp edges as Real-ESRGAN do. Nevertheless, on most samples, their differences are marginal, indicating that the widely-used Gaussian kernels with a high-order degradation process can already cover a large real blur space. As we can still observe slightly better performance, we adopt those more complicated blur kernels in Real-ESRGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Limitations</head><p>Though Real-ESRGAN is able to restore most realworld images, it still has some limitations. As shown in Fig. <ref type="figure">11, 1</ref>) some restored images (especially building and indoor scenes) have twisted lines due to aliasing issues. 2)  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we train the practical Real-ESRGAN for real-world blind super-resolution with pure synthetic training pairs. In order to synthesize more practical degradations, we propose a high-order degradation process and employ sinc filters to model common ringing and overshoot artifacts. We also utilize a U-Net discriminator with spectral normalization regularization to increase discriminator capability and stabilize the training dynamics. Real-ESRGAN trained with synthetic data is able to enhance details while removing annoying artifacts for most real-world images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. JPEG compression</head><p>We use the PyTorch implementation -DiffJPEG. We observe that the compressed images by DiffJPEG are a bit different from those compressed by the cv2 package. Fig. <ref type="figure" target="#fig_12">16</ref> shows the typical JPEG compression artifacts and the difference caused by using different packages. Such a difference may bring an extra gap between synthetic and real samples. In this work, we only adopt DiffJPEG for simplicity, and this difference will be addressed later.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of the pure synthetic data generation adopted in Real-ESRGAN. It utilizes a second-order degradation process to model more practical degradations, where each degradation process adopts the classical degradation model. The detailed choices for blur , resize, noise and JPEG compression are listed. We also employ sinc filter to synthesize common ringing and overshoot artifacts.</figDesc><graphic url="image-15.png" coords="4,170.36,286.58,55.85,55.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Models trained with synthetic data of classical degradation model could resolve some real samples (Left). Yet, they amplify noises or introduce ringing artifacts for complex real-world images (Right). Zoom in for best view</figDesc><graphic url="image-28.png" coords="4,170.36,400.87,56.15,56.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Real-ESRGAN adopts the same generator network as that in ESRGAN. For the scale factor of ×2 and ×1, it first employs a pixel-unshuffle operation to reduce spatial size and re-arrange information to the channel dimension.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Input</head><label></label><figDesc>𝜔 c = 𝜋/5 𝜔 c = 𝜋/3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Top: Real samples suffering from ringing and overshoot artifacts. Bottom: Examples of sinc kernels (kernel size 21) and the corresponding filtered images. Zoom in for best view</figDesc><graphic url="image-39.png" coords="5,51.67,272.77,58.31,58.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Architecture of the U-Net discriminator with spectral normalization.</figDesc><graphic url="image-40.png" coords="5,226.12,272.77,58.31,58.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Qualitative comparisons on several representative real-world samples with upsampling scale factor of 4. Our Real-ESRGAN outperforms previous approaches in both removing artifacts and restoring texture details. Real-ESRGAN+ (trained with sharpened ground-truths) can further boost visual sharpness. Other methods may either fail to remove overshoot (the 1st sample) and complicated artifacts (the 2nd sample), or fail to restore realistic and natural textures for various scenes (the 3rd, 4th, 5th samples). (Zoom in for best view)</figDesc><graphic url="image-88.png" coords="7,55.85,533.87,122.50,105.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Top: Real-ESRNet results w/ and w/o secondorder degradation process. Bottom: Real-ESRNet results w/ and w/o sinc filters. Zoom in for best view4.3. Ablation StudiesSecond-order degradation model. We conduct ablation studies of degradations on Real-ESRNet, as it is more controllable and can better reflect the influence of degradations. We replace the second-order process in Real-ESRNet with the classical degradation model to generate training pairs. As shown in Fig.8(Top), models trained with classical firstorder degradation model cannot effectively remove noise on the wall or blur in the wheat field, while Real-ESRNet can handle these cases. sinc filters. If sinc filters are not employed during training, the restored results will amplify the ringing and overshoot artifacts that existed in the input images, as shown in Fig.8(Bottom), especially around the text and lines. In contrast, models trained with sinc filters can remove those artifacts. U-Net discriminator with SN regularization. We first employ the ESRGAN setting including the VGG-style discriminator and its loss weights. However, we can observe from Fig.9, this model cannot restore detailed textures (bricks and bushes) and even brings unpleasant artifacts in bush branches. Using a U-Net design could improve local details. Yet, it introduces unnatural textures and also increases training instability. SN regularization could improve restored textures while stabilizing training dynamics. More complicated blur kernels. We remove the generalized Gaussian kernel and plateau-shaped kernel in blur synthesis. As shown in Fig.10, on some real samples, the model cannot remove blur and recover sharp edges as Real-ESRGAN do. Nevertheless, on most samples, their differences are marginal, indicating that the widely-used Gaussian kernels with a high-order degradation process can already cover a large real blur space. As we can still observe slightly better performance, we adopt those more complicated blur kernels in Real-ESRGAN.</figDesc><graphic url="image-102.png" coords="8,50.31,126.83,57.97,57.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Ablation on the discriminator design. Zoom in for best view Gaussian blur kernels More blur kernels Gaussian blur kernels More blur kernels</figDesc><graphic url="image-113.png" coords="8,427.68,211.01,57.96,58.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Figure 10: Ablation on using more blur kernels (generalized blur and plateau-shaped kernels). Zoom in for best view</figDesc><graphic url="image-114.png" coords="8,486.99,211.01,57.96,58.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: JPEG compressed images by cv2 and DiffJPEG, with factor q = 50. They produces slightly different results. Zoom in for best view</figDesc><graphic url="image-184.png" coords="12,50.28,197.86,57.63,57.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>, our Real-ESRGAN outperforms previous approaches in both removing artifacts and restoring texture details. Real-ESRGAN+ (trained with sharpened ground-truths) can further boost visual sharpness. Other methods typically fail to remove complicated artifacts (the 1st sample) and overshoot artifacts (the 2nd, 3rd sample), or fail to restore realistic and natural textures for various scenes (the 4th, 5th samples).</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/mlomnitz/DiffJPEG</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Details of Classical Degradation Model</head><p>In this section, we provide more details (especially examples) of each degradation type used in the classical degradation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Blur</head><p>Isotropic and anisotropic Gaussian filters are the common choices for blur kernels. We show several Gaussian kernels and their corresponding blurry images in Fig. <ref type="figure">12</ref>.   To include more diverse kernel shapes, we further adopt generalized Gaussian blur kernels <ref type="bibr" target="#b28">[29]</ref> and a plateau-shaped distribution. Fig. <ref type="figure">13</ref> shows how the shape parameter β controls kernel shapes. Empirically, we found that including these blur kernels produces sharper outputs for several real samples.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Resize</head><p>There are several resize algorithms. We compare the following resize operations: nearest-neighbor interpolation, area resize, bilinear interpolation and bicubic interpolation. We examine the different effects of these resize operations. We first downsample an image by a scale factor of four and then upsample to its original size. Different downsampling and upsampling algorithms are performed, and the results of different combinations are shown in Fig. <ref type="figure">15</ref>. It is observed that different resize operations result in very different effects -some produce blurry results while some may output over-sharp images with overshoot artifacts.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Dataset and study</title>
		<author>
			<persName><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Blind super-resolution kernel estimation using an internal-gan</title>
		<author>
			<persName><forename type="first">Sefi</forename><surname>Bell-Kligler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Assaf</forename><surname>Shocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tomer Michaeli, and Lihi Zelnik-Manor. The 2018 pirm challenge on perceptual image super-resolution</title>
		<author>
			<persName><forename type="first">Yochai</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roey</forename><surname>Mechrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Toward real-world single image super-resolution: A new benchmark and a new model</title>
		<author>
			<persName><forename type="first">Jianrui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zisheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Glean: Generative latent bank for large-factor image super-resolution</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Second-order attention network for single image super-resolution</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianrui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Restoration of a single superresolution image from several blurred, noisy, and undersampled measured images</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arie</forename><surname>Feuer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Frequency separation for real-world super-resolution</title>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Fritsche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Superresolution from a single image</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Blind super-resolution with iterative kernel correction</title>
		<author>
			<persName><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep backprojection networks for super-resolution</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dslr-quality photos on mobile devices with deep convolutional networks</title>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Ignatov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Kobyshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Vanhoey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Real-world super-resolution via kernel estimation and noise injection</title>
		<author>
			<persName><forename type="first">Xiaozhong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyoung</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deeplyrecursive convolutional network for image super-resolution</title>
		<author>
			<persName><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyoung</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Photorealistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Photorealistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyoung</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Blind image super-resolution: A survey and beyond</title>
		<author>
			<persName><forename type="first">Anran</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiaoand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03055</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On bayesian adaptive video super resolution</title>
		<author>
			<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Non-local recurrent network for image restoration</title>
		<author>
			<persName><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bihan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Estimating generalized gaussian blur kernels for out-of-focus image deblurring</title>
		<author>
			<persName><forename type="first">Yu-Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui-Liang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu-Jie</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on circuits and systems for video technology</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Unsupervised learning for real-world super-resolution</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<editor>IC-CVW</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unfolding the alternating optimization for blind super resolution</title>
		<author>
			<persName><forename type="first">Zhengxiong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Nonparametric blind super-resolution</title>
		<author>
			<persName><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Enhancenet: Single image super-resolution through automated texture synthesis</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A u-net based discriminator for generative adversarial networks</title>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Schonfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Andrew P Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehan</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Jpeg-resistant adversarial images</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop on Machine Learning and Computer Security</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image superresolution via deep recursive residual network</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Methods and results</title>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyoung</forename><surname>Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPRW</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised degradation representation learning for blind superresolution</title>
		<author>
			<persName><forename type="first">Longguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Towards real-world blind face restoration with generative facial prior</title>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><forename type="middle">C K</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<ptr target="https://github.com/xinntao/BasicSR" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Recovering realistic in image super-resolution by deep spatial feature transform</title>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
				<imprint>
			<date type="published" when="2006">2018. 1, 2, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Qixiang Ye amd Wangmeng Zuo, and Liang Lin. Component divide-and-conquer for real-world image super-resolution</title>
		<author>
			<persName><forename type="first">Pengxu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongyuan</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10343</idno>
		<title level="m">Path-restore: Learning network path selection for image restoration</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised image superresolution using cycle-in-cycle generative adversarial networks</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPRW</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Designing a practical degradation model for deep blind image super-resolution</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14006</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning a single convolutional super-resolution network for multiple degradations</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Semantic understanding of scenes through the ade20k dataset</title>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Kernel modeling superresolution on real low-resolution images</title>
		<author>
			<persName><forename type="first">Ruofan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabine</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
