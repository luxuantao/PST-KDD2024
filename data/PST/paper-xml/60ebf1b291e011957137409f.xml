<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UNIRE: A Unified Label Space for Entity Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yijun</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">AI Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Changzhi</forename><surname>Sun</surname></persName>
							<email>sunchangzhi@bytedance.com</email>
							<affiliation key="aff3">
								<orgName type="laboratory">ByteDance AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
							<email>ybwu@cs.ecnu.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">ByteDance AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">ByteDance AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
							<email>yanjunchi@sjtu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">AI Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">UNIRE: A Unified Label Space for Entity Relation Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many joint entity relation extraction models setup two separated label spaces for the two sub-tasks (i.e., entity detection and relation classification). We argue that this setting may hinder the information interaction between entities and relations. In this work, we propose to eliminate the different treatment on the two sub-tasks' label spaces. The input of our model is a table containing all word pairs from a sentence. Entities and relations are represented by squares and rectangles in the table. We apply a unified classifier to predict each cell's label, which unifies the learning of two sub-tasks. For testing, an effective (yet fast) approximate decoder is proposed for finding squares and rectangles from tables. Experiments on three benchmarks (ACE04, ACE05, SciERC) show that, using only half the number of parameters, our model achieves competitive accuracy with the best extractor, and is faster.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Extracting structured information from plain texts is a long-lasting research topic in NLP. Typically, it aims to recognize specific entities and relations for profiling the semantic of sentences. An example is shown in Figure <ref type="figure" target="#fig_0">1</ref>, where a person entity "David Perkins" and a geography entity "California" have a physical location relation PHYS.</p><p>Methods for detecting entities and relations can be categorized into pipeline models or joint models. In the pipeline setting, entity models and relation models are independent with disentangled feature spaces and output label spaces. In the joint setting, on the other hand, some parameter sharing of feature spaces <ref type="bibr" target="#b13">(Miwa and Bansal, 2016;</ref><ref type="bibr" target="#b5">Katiyar and</ref>  Each cell corresponds to a word pair. Entities are squares on diagonal, relations are rectangles off diagonal. Note that PER-SOC is a undirected (symmetrical) relation type, while PHYS and ORG-AFF are directed (asymmetrical) relation types. The table exactly expresses overlapped relations, e.g., the person entity "David Perkins" participates in two relations, ("David Perkins", "wife", PER-SOC) and ("David Perkins", "California", PHYS). For every cell, a same biaffine model predicts its label. The joint decoder is set to find the best squares and rectangles. <ref type="bibr" target="#b5">Cardie, 2017)</ref> or decoding interactions <ref type="bibr" target="#b25">(Yang and Cardie, 2013;</ref><ref type="bibr" target="#b17">Sun et al., 2019)</ref> are imposed to explore the common structure of the two tasks. It was believed that joint models could be better since they can alleviate error propagations among sub-models, have more compact parameter sets, and uniformly encode prior knowledge (e.g., constraints) on both tasks. However, <ref type="bibr" target="#b31">Zhong and Chen (2020)</ref> recently show arXiv:2107.04292v1 [cs.CL] 9 Jul 2021 that with the help of modern pre-training tools (e.g., BERT), separating the entity and relation model (with independent encoders and pipeline decoding) could surpass existing joint models. They argue that, since the output label spaces of entity and relation models are different, comparing with shared encoders, separate encoders could better capture distinct contextual information, avoid potential conflicts among them, and help decoders making a more accurate prediction, that is, separate label spaces deserve separate encoders.</p><formula xml:id="formula_0">PER PER ‚ä• ‚ä• PER- SOC ‚ä• ‚ä• ‚ä• ‚ä• PHYS PER PER ‚ä• ‚ä• PER- SOC ‚ä• ‚ä• ‚ä• ‚ä• PHYS ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• PER PER- SOC ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• PER- SOC PER- SOC ‚ä• PER- SOC PER ‚ä• ‚ä• ‚ä• ‚ä• PHYS ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• GPE ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ORG- AFF PER ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• ‚ä• GPE</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>David</head><p>In this paper, we pursue a better joint model for entity relation extraction. After revisiting existing methods, we find that though entity models and relation models share encoders, usually their label spaces are still separate (even in models with joint decoders). Therefore, parallel to <ref type="bibr" target="#b31">(Zhong and Chen, 2020)</ref>, we would ask whether joint encoders (decoders) deserve joint label spaces?</p><p>The challenge of developing a unified entityrelation label space is that the two sub-tasks are usually formulated into different learning problems (e.g., entity detection as sequence labeling, relation classification as multi-class classification), and their labels are placed on different things (e.g., words v.s. words pairs). One prior attempt <ref type="bibr" target="#b30">(Zheng et al., 2017)</ref> is to handle both sub-tasks with one sequence labeling model. A compound label set was devised to encode both entities and relations. However, the model's expressiveness is sacrificed: it can detect neither overlapping relations (i.e., entities participating in multiple relation) nor isolated entities (i.e., entities not appearing in any relation).</p><p>Our key idea of defining a new unified label space is that, if we think <ref type="bibr" target="#b30">Zheng et al. (2017)</ref>'s solution is to perform relation classification during entity labeling, we could also consider the reverse direction by seeing entity detection as a special case of relation classification. Our new input space is a two-dimensional table with each entry corresponding to a word pair in sentences (Figure <ref type="figure" target="#fig_0">1</ref>). The joint model assign labels to each cell from a unified label space (union of entity type set and relation type set). Graphically, entities are squares on the diagonal, and relations are rectangles off the diagonal. This formulation retains full model expressiveness regarding existing entity-relation extraction scenarios (e.g., overlapped relations, directed relations, undirected relations). It is also different from the current table filling settings for entity relation extraction <ref type="bibr" target="#b15">(Miwa and Sasaki, 2014;</ref><ref type="bibr" target="#b4">Gupta et al., 2016;</ref><ref type="bibr" target="#b29">Zhang et al., 2017;</ref><ref type="bibr" target="#b22">Wang and Lu, 2020)</ref>, which still have separate label space for entities and relations, and treat on/off-diagonal entries differently.</p><p>Based on the tabular formulation, our joint entity relation extractor performs two actions, filling and decoding. First, filling the table is to predict each word pair's label, which is similar to arc prediction task in dependency parsing. We adopt the biaffine attention mechanism <ref type="bibr" target="#b3">(Dozat and Manning, 2016)</ref> to learn interactions between word pairs. We also impose two structural constraints on the table through structural regularizations. Next, given the table filling with label logits, we devise an approximate joint decoding algorithm to output the final extracted entities and relations. Basically, it efficiently finds split points in the table to identify squares and rectangles (which is also different with existing table filling models which still apply certain sequential decoding and fill tables incrementally).</p><p>Experimental results on three benchmarks (ACE04, ACE05, SciERC) show that the proposed joint method achieves competitive performances comparing with the current state-of-the-art extractors <ref type="bibr" target="#b31">(Zhong and Chen, 2020</ref>): it is better on ACE04 and SciERC, and competitive on ACE05.<ref type="foot" target="#foot_0">1</ref> Meanwhile, our new joint model is fast on decoding (10x faster than the exact pipeline implementation, and comparable to an approximate pipeline, which attains lower performance). It also has a more compact parameter set: the shared encoder uses only half the number of parameters comparing with the separate encoder <ref type="bibr" target="#b31">(Zhong and Chen, 2020</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Definition</head><p>Given an input sentence s = x 1 , x 2 , . . . , x |s| (x i is a word), this task is to extract a set of entities E and a set of relations R. An entity e is a span (e.span) with a pre-defined type e.type ‚àà Y e (e.g., PER, GPE). The span is a continuous sequence of words. A relation r is a triplet (e 1 , e 2 , l), where e 1 , e 2 are two entities and l ‚àà Y r is a pre-defined relation type describing the semantic relation among two entities (e.g., the PHYS relation between PER and GPE mentioned before). Here Y e , Y r denote the set of possible entity types and relation types respectively.</p><p>We formulate the joint entity relation extraction as a table filling task (multi-class classification between each word pair in sentence s), as shown in Figure <ref type="figure" target="#fig_0">1</ref>. For the sentence s, we maintain a table T |s|√ó|s| . For each cell (i, j) in table T , we assign a label y i,j ‚àà Y, where Y = Y e ‚à™ Y r ‚à™ {‚ä•} ( ‚ä• denotes no relation). For each entity e, the label of corresponding cells y i,j (x i ‚àà e.span, x j ‚àà e.span) should be filled in e.type. For each relation r = (e 1 , e 2 , l), the label of corresponding cells y i,j (x i ‚àà e 1 .span, x j ‚àà e 2 .span) should be filled in l. 2 While others should be filled in ‚ä•.</p><p>In the test phase, decoding entities and relations becomes a rectangle finding problem. Note that solving this problem is not trivial, and we propose a simple but effective joint decoding algorithm to tackle this challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>In this section, we first introduce our biaffine model for table filling task based on pre-trained language models (Section 3.1). Then we detail the main objective function of the table filling task (Section 3.2) and some constraints which are imposed on the table in training stage (Section 3.3). Finally we present the joint decoding algorithm to extract entities and relations (Section 3.4). Figure <ref type="figure" target="#fig_6">2</ref> shows an overview of our model architecture. 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Biaffine Model</head><p>Given an input sentence s, to obtain the contextual representation h i for each word, we use a pre-trained language model (PLM) as our sentence encoder (e.g., BERT). The output of the encoder is</p><formula xml:id="formula_1">{h 1 , . . . , h |s| } = PLM({x 1 , . . . , x |s| }),</formula><p>where x i is the input representation of each word x i . Taking BERT as an example, x i sums the corresponding token, segment and position embeddings.</p><p>To capture long-range dependencies, we also employ cross-sentence context following <ref type="bibr" target="#b31">(Zhong and Chen, 2020)</ref>, which extends the sentence to a fixed window size W (W = 200 in our default settings).</p><p>To better encode direction information of words in table T , we use the deep biaffine attention mechanism <ref type="bibr" target="#b3">(Dozat and Manning, 2016)</ref>, which achieves impressive results in the dependency parsing task. Specifically, we employ two dimension-reducing 2 Assuming no overlapping entities in one sentence. 3 We only show three labels of Y in Figure <ref type="figure" target="#fig_6">2</ref> for simplicity and clarity.</p><p>MLPs (multi-layer perceptron), i.e., a head MLP and a tail MLP, on each h i as</p><formula xml:id="formula_2">h head i = MLP head (h i ), h tail i = MLP tail (h i ),</formula><p>where h head i ‚àà R d and h tail i ‚àà R d are projection representations, allowing the model to identify the head or tail role of each word. Next, we calculate the scoring vector g i,j ‚àà R |Y| of each word pair with biaffine model,</p><formula xml:id="formula_3">g i,j = Biaff(h head i , h tail j ), Biaff(h 1 , h 2 ) = h T 1 U 1 h 2 + U 2 (h 1 ‚äï h 2 ) + b,</formula><p>where</p><formula xml:id="formula_4">U 1 ‚àà R |Y|√ód√ód and U 2 ‚àà R |Y|√ó2d are weight parameters, b ‚àà R |Y| is the bias, ‚äï denotes concatenation.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Table Filling</head><p>After obtaining the scoring vector g i,j , we feed g i,j into the softmax function to predict corresponding label, yielding a categorical probability distribution over the label space Y as P (y i,j |s) = Softmax(dropout(g i,j )).</p><p>In our experiments, we observe that applying dropout in g i,j , similar to de-noising autoencoding, can further improve the performance. <ref type="foot" target="#foot_1">4</ref> . We refer this trick to logit dropout And the training objective is to minimize</p><formula xml:id="formula_5">L entry =‚àí 1 |s| 2 |s| i=1 |s| j=1 log P (y i,j = y i,j |s), (1)</formula><p>where the gold label y i,j can be read from annotations, as shown in Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Constraints</head><p>In fact, Equation 1 is based on the assumption that each label is independent. This assumption simplifies the training procedure, but ignores some structural constraints. For example, entities and relations correspond to squares and rectangles in the table. Equation 1 does not encode this constraint explicitly. To enhance our model, we propose two intuitive constraints, symmetry and implication, which are detailed in this section. Here we introduce a new notation P ‚àà R |s|√ó|s|√ó|Y| , denoting the stack of P (y i,j |s) for all word pairs in sentence s.  </p><formula xml:id="formula_6">F V 0 = " &gt; A A A C J X i c b V D L S s N A F J 3 U V 6 2 v q E s 3 g 0 V w V R J R d O G i 6 M Z l F f u Q J p b J d N I O n U z C z E Q o a X 7 G j b / i x o V F B F f + i t M 0 F G 0 9 M H D m n H u 5 9 x 4 v Y l Q q y / o y C k v L K 6 t r x f X S x u b W 9 o 6 5 u 9 e Q Y S w w q e O Q h a L l I U k Y 5 a S u q G K k F Q m C A o + R p j e 4 n v j N J y I k D f m 9 G k b E D V C P U 5 9 i p L T U M S + d A K k + R i y p p d C h H G Z / z 0 v u 0 s f k F D q K B k T C G R n N y h / S U d o x y 1 b F y g A X i Z 2 T M s h R 6 5 h j p x v i O C B c Y Y a k b N t W p N w E C U U x I 2 n J i S W J E B 6 g H m l r y p G e 6 S b Z l S k 8 0 k o X + q H Q j y u Y q b 8 7 E h R I O Q w 8 X T l Z U s 5 7 E / E / r x 0 r / 8 J N K I 9 i R T i e D v J j B l U I J 5 H B L h U E K z b U B G F B 9 a 4 Q 9 5 F A W O l g S z o E e / 7 k R d I 4 q d h n F e v 2 t F y 9 y u M o g g N w C I 6 B D c 5 B F d y A G q g D D J 7 B K 3 g H Y + P F e D M + j M 9 p a c H I e / b B H x j f P 0 1 h p c U = &lt; / l a t e x i t &gt; P 2 R 4‚á•4‚á•|Y| Decoding &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E 9 v V m N i i R V p 9 h e X z A + c 7 U a j G K I Q = " &gt; A A A B 8 3 i c b V B N S 8 N A F H y p X 7 V + V T 1 6 W S y C p 5 K I o s e i F 4 8 V b C 0 0 p W y 2 L + 3 S z S b s b s Q S + j e 8 e F D E q 3 / G m / / G T Z u D t g 4 s D D P v 8 W Y n S A T X x n W / n d L K 6 t r 6 R n m z s r W 9 s 7 t X 3 T 9 o 6 z h V D F s s F r H q B F S j 4 B J b h h u B n U Q h j Q K B D 8 H 4 J v c f H l F p H s t 7 M 0 m w F 9 G h 5 C F n 1 F j J 9 y N q R k G Y P U 3 7 X r 9 a c + v u D G S Z e A W p Q Y F m v / r l D 2 K W R i g N E 1 T r r u c m p p d R Z T g T O K 3 4 q c a E s j E d Y t d S S S P U v W y W e U p O r D I g Y a z s k 4 b M 1 N 8 b G Y 2 0 n k S B n c w z 6 k U v F / / z u q k J r 3 o Z l 0 l q U L L 5 o T A V x M Q k L 4 A M u E J m x M Q S y h S 3 W Q k b U U W Z s T V V b A n e 4 p e X S f u s 7 l 3 U 3 b v z W u O 6 q K M M R 3 A M p + D B J T T g F p r Q A g Y J P M M r v D m p 8 + K 8 O x / z 0 Z J T 7 B z C H z i f P y u y k c Q = &lt; / l a t e x i t &gt; x 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 k H u V 1 Y P Z Z x 7 F 2 U O x I G i D U G o 7 V M = " &gt; A A A B 8 3 i c b V D L S g M x F L 2 p r 1 p f V Z d u g k V w V W a K o s u i G 5 c V 7 A M 6 Q 8 m k m T Y 0 k x m S j F i G / o Y b F 4 q 4 9 W f c + T d m 2 l l o 6 4 H A 4 Z x 7 u S c n S A T X x n G + U W l t f W N z q 7 x d 2 d n d 2 z + o H h 5 1 d J w q y t o 0 F r H q B U Q z w S V r G 2 4 E 6 y W K k S g Q r B t M b n O / + 8 i U 5 r F 8 M N O E + R E Z S R 5 y S o y V P C 8 i Z h y E 2 d N s 0 B h U a 0 7 d m Q O v E r c g N S j Q G l S / v G F M 0 4 h J Q w X R u u 8 6 i f E z o g y n g s 0 q X q p Z Q u i E j F j f U k k i p v 1 s n n m G z 6 w y x G G s 7 J M G z 9 X f G x m J t J 5 G g Z 3 M M + p l L x f / 8 / q p C a / 9 j M s k N U z S x a E w F d j E O C 8 A D 7 l i 1 I i p J Y Q q b r N i O i a K U G N r q t g S 3 O U v r 5 J O o + 5 e 1 p 3 7 i 1 r z p q i j D C d w C u f g w h U 0 4 Q 5 a 0 A Y K C T z D K 7 y h F L 2 g d / S x G C 2 h Y u c Y / g B 9 / g A t N p H F &lt; / l a t e x i t &gt; x 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q z j e s N / A d n F j x n N r n 2 q 3 S u / R Z l U = " &gt; A A A B 8 3 i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s z 4 Q J d F N y 4 r 2 A d 0 h p J J M 2 1 o J h m S j F i G / o Y b F 4 q 4 9 W f c + T d m 2 l l o 6 4 H A 4 Z x 7 u S c n T D j T x n W / n d L K 6 t r 6 R n m z s r W 9 s 7 t X 3 T 9 o a 5 k q Q l t E c q m 6 I d a U M 0 F b h h l O u 4 m i O A 4 5 7 Y T j 2 9 z v P F K l m R Q P Z p L Q I M Z D w S J G s L G S 7 8 f Y j M I o e 5 r 2 z / v V m l t 3 Z 0 D L x C t I D Q o 0 + 9 U v f y B J G l N h C M d a 9 z w 3 M U G G l W G E 0 2 n F T z V N M B n j I e 1 Z K n B M d Z D N M k / R i V U G K J L K P m H Q T P 2 9 k e F Y 6 0 k c 2 s k 8 o 1 7 0 c v E / r 5 e a 6 D r I m E h S Q w W Z H 4 p S j o x E e Q F o w B Q l h k 8 s w U Q x m x W R E V a Y G F t T x Z b g L X 5 5 m b T P 6 t 5 l 3 b 2 / q D V u i j r K c A T H c A o e X E E D 7 q A J L S C Q w D O 8 w p u T O i / O u / M x H y 0 5 x c 4 h / I H z + Q M u u p H G &lt; / l a t e x i t &gt;</formula><p>x 3</p><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e w a 9 l 9</p><formula xml:id="formula_7">C 3 R G Q n r q U R Z 0 k 2 6 7 8 y 1 r Q = " &gt; A A A B 8 3 i c b V D L S g M x F L 2 p r 1 p f V Z d u g k V w V W a k o s u i G 5 c V 7 A M 6 Q 8 m k m T Y 0 k x m S j F i G / o Y b F 4 q 4 9 W f c + T d m 2 l l o 6 4 H A 4 Z x 7 u S c n S A T X x n G + U W l t f W N z q 7 x d 2 d n d 2 z + o H h 5 1 d J w q y t o 0 F r H q B U Q z w S V r G 2 4 E 6 y W K k S g Q r B t M b n O / + 8 i U 5 r F 8 M N O E + R E Z S R 5 y S o y V P C 8 i Z h y E 2 d N s 0 B h U a 0 7 d m Q O v E r c g N S j Q G l S / v G F M 0 4 h J Q w X R u u 8 6 i f E z o g y n g s 0 q X q p Z Q u i E j F j f U k k i p v 1 s n n m G z 6 w y x G G s 7 J M G z 9 X f G x m J t J 5 G g Z 3 M M + p l L x f / 8 / q p C a / 9 j M s k N U z S x a E w F d j E O C 8 A D 7 l i 1 I i p J Y Q q b r N i O i a K U G N r q t g S 3 O U v r 5 L O R d 2 9 r D v 3 j V r z p q i j D C d w C u f g w h U 0 4 Q 5 a 0 A Y K C T z D K 7 y h F L 2 g d / S x G C 2 h Y u c Y / g B 9 / g A w P p H H &lt; / l a t e x i t &gt;</formula><p>x 4</p><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P h 5 A S l q 5 G g</p><formula xml:id="formula_8">J A C 7 X x D k i T Q e n f V S g = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i S i 6 L L o x m U F + 4 A m l M l 0 0 g 6 d T M L M j V B C f 8 O N C 0 X c + j P u / B u n b R b a e m D g c M 6 9 3 D M n T K U w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z J p x l s s k Y n u h t R w K R R v o U D J u 6 n m N A 4 l 7 4 T j u 5 n f e e L a i E Q 9 4 i T l Q U y H S k S C U b S S 7 8 c U R 2 G U j 6 Z 9 r 1 + t u X V 3 D r J K v I L U o E C z X / 3 y B w n L Y q 6 Q S W p M z 3 N T D H K q U T D J p x U / M z y l b E y H v G e p o j E 3 Q T 7 P P C V n V h m Q K N H 2 K S R z 9 f d G T m N j J n F o J 2 c Z z b I 3 E / / z e h l G N 0 E u V J o h V 2 x x K M o k w Y T M C i A D o T l D O b G E M i 1 s V s J G V F O G t q a K L c F b / v I q a V / U v a u 6 + 3 B Z a 9 w W d Z T h B E 7 h H D y 4 h g b c Q x N a w C C F Z 3 i F N y d z X p x 3 5 2 M x W n K K n W P 4 A + f z B x N C k b Q = &lt; / l a t e x i t &gt; h 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 6 g i V N 2 O f 6 5 h 4 5 3 7 x z A K Y 9 8 0 2 S s s = " &gt; A A A B 8 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y B F c l a Q o u i y 6 c V n B P q A p Z T K d t E M n k z B z I 5 T Q 3 3 D j Q h G 3 / o w 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 w J E i k M u u 6 3 s 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 7 b J k 4 1 4 y 0 W y 1 h 3 A 2 q 4 F I q 3 U K D k 3 U R z G g W S d 4 L J X e 5 3 n r g 2 I l a P O E 1 4 P 6 I j J U L B K F r J 9 y O K 4 y D M x r N B f V C p u j V 3 D r J K v I J U o U B z U P n y h z F L I 6 6 Q S W p M z 3 M T 7 G d U o 2 C S z 8 p + a n h C 2 Y S O e M 9 S R S N u + t k 8 8 4 y c W 2 V I w l j b p 5 D M 1 d 8 b G Y 2 M m U a B n c w z m m U v F / / z e i m G N / 1 M q C R F r t j i U J h K g j H J C y B D o T l D O b W E M i 1 s V s L G V F O G t q a y L c F b / v I q a d d r 3 l X N f b i s N m 6 L O k p w C m d w A R 5 c Q w P u o Q k t Y J D A M 7 z C m 5 M 6 L 8 6 7 8 7 E Y X X O K n R P 4 A + f z B x T G k b U = &lt; / l a t e x i t &gt; h 2</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " h w 8 s B J N J 6 + q g 1 3</p><formula xml:id="formula_9">C 7 k X V m k v 6 i s 3 8 = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q + 0 G X R j c s K 1 h a a U i b T m 3 b o Z B J m J k I J / Q 0 3 L h R x 6 8 + 4 8 2 + c t F l o 6 4 G B w z n 3 c s + c I B F c G 9 f 9 d k o r q 2 v r G + X N y t b 2 z u 5 e d f / g U c e p Y t h i s Y h V J 6 A a B Z f Y M t w I 7 C Q K a R Q I b A f j 2 9</formula><p>x v P 6 H S P J Y P Z p J g L 6 J D y U P O q L G S 7 0 f U j I I w G 0 3 7 5 / 1 q z a 2 7 M 5 B l 4 h W k B g W a / e q X P 4 h Z G q E 0 T F C t u 5 6 b m F 5 G l e F M 4 L T i p x o T y s Z 0 i F 1 L J Y 1 Q 9 7 J Z 5 i k 5 s c q A h L G y T x o y U 3 9 v Z D T S e h I F d j L P q B e 9 X P z P 6 6 Y m     Symmetry We have several observations from the table in the tag level. Firstly, the squares corresponding to entities must be symmetrical about the diagonal. Secondly, for symmetrical relations, the relation triples (e 1 , e 2 , l) and (e 2 , e 1 , l) are equivalent, thus the rectangles corresponding to two counterpart relation triples are also symmetrical about the diagonal. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, the rectangles corresponding to ("his", "wife", PER-SOC) and ("wife", "his", PER-SOC) are symmetrical about the diagonal. We divide the set of labels Y into a symmetrical label set Y sym and an asymmetrical label set Y asym . The matrix P :,:,t should be symmetrical about the diagonal for each label t ‚àà Y sym . We formulate this tag-level constraint as symmetrical loss,</p><formula xml:id="formula_10">v O 5 l X C a p Q c n m h 8 J U E B O T v A A y 4 A q Z E R N L K F P c Z i V s R B V l x t Z U s S V</formula><formula xml:id="formula_11">f + V 3 k K F c i 0 K 4 = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i S i 6 L L o x m U F + 4 C m l M l 0 0 g 6 d T M L M j V B C f 8 O N C 0 X c + j P u / B s n b R b a e m D g c M 6 9 3 D M n S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T u 9 z v P H F t R K w e c Z r w f k R H S o S C U b S S 7 0 c U x 0 G Y j W e D y 0 G 1 5 t b d O c g q 8 Q p S g w L N Q f X L H 8 Y s j b h C J q k x P c 9 N s J 9 R j Y J J P q v 4 q e E J Z R M 6 4 j 1 L F Y 2 4 6 W f z z D N y Z p U h C W N t n 0 I y V 3 9 v Z D Q y Z h o F d j L P a J a 9 X P z P 6 6 U Y 3 v Q z o Z I U u W K L Q 2 E q C c Y k L 4 A M h e Y M 5 d Q S y r S w W Q k b U 0 0 Z 2 p o q t g R v + c u r p H 1 R 9 6 7 q 7 s N l r X F b 1 F G G E z i F c / D g G h p w D 0 1 o A Y M E n u E V 3 p z U e</formula><formula xml:id="formula_12">j S j v N t V Z a W V 1 b X q u u 1 j c 2 t 7 R 1 7 d 6 + j 4 l R i 0 s Y x i 2 U v Q I o w K k h b U 8 1 I L 5 E E 8 Y C R b j C + L v z u A 5 G K x u J O T x L i c T Q U N K I Y</formula><formula xml:id="formula_13">Y z Q k f U M F 4 k R 5 W X l C D o + N E s I o l u Y J D U v 1 d 0 e G u F I T H p j K Y k c 1 7 x X i f 1 4 / 1 d G l l 1 G R p J o I P B 0 U p Q z q G B Z 5 w J B K g j W b G I K w p G Z X i E d I I q x N a j U T g j t / 8 i L p n D b c 8 4 Z z e 1 Z v X s 3 i q I J D c A R O g A s u Q B P c g B Z o A w w e w T N 4 B W / W k / V i v V s f 0 9 K K N e v Z B 3 9 g f f 4 A Y + 2 Y E Q = = &lt; / l a t e x i t &gt; h end i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 f X t M F a a M 7 + i W x C i p 4 3 a n 4 J n 3 a E = " &gt; A A A C N X i c b V D L S s N A F J 3 U V 6 2 v q E s 3 w S I I Q k l E 0 W X R j Y s u K t g H t C F M p p N 2 6 M w k z E y E E P J T b v w P V 7 p w o Y h b f 8 F J m o W 2 v X D h c M 6 9 3 H O P H 1 E i l W 2 / G Z W V 1 b X 1 j e p m b W t 7 Z 3 f P 3 D / o y j A W C H d Q S E P R 9 6 H E l H D c U U R R 3 I 8 E h s y n u O d P b 3 O 9 9 4 i F J C F / U E m E X Q b H n A Q E Q a U p z 2 w N G V Q T B G n a y r w C C 5 Z i r k S S n S 2 T Z M K W C 4 R F m W f W 7 Y Z d l L U I n B L U Q V l t z 3 w Z j k I U M 3 0 Q U S</formula><formula xml:id="formula_14">L sym = 1 |s| 2 |s| i=1 |s| j=1 t‚ààYsym |P i,j,t ‚àí P j,i,t |.</formula><p>We list all Y sym in Table <ref type="table" target="#tab_0">1</ref> for our adopted datasets.</p><p>Implication A key intuition is that if a relation exists, then its two argument entities must also exist. In other words, it is impossible for a relation to exist without two corresponding entities. From the perspective of probability, it implies that the probability of relation is not greater than the probability of each argument entity. Since we model entity and relation labels in a unified probability space, this idea can be easily used in our model as the implication constraint. We impose this constraint on P: for each word in the diagonal, its maximum possibility over the entity type space Y e must not be lower than the maximum possibility for other words in the same row or column over the relation type space Y r . We formulate this table-level constraint as implication loss,</p><formula xml:id="formula_15">L imp = 1 |s| |s| i=1 max l‚ààYr {P i,:,l , P :,i,l } ‚àí max t‚ààYe {P i,i,t } *</formula><p>where [u] * = max(u, 0) is the hinge loss. It is worth noting that we do not add margin in this loss function. Since the value of each item is a probability and might be relatively small, it is meaningless to set a large margin. Finally, we jointly optimize the three objectives in the training stage as L entry + L sym + L imp .<ref type="foot" target="#foot_3">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Decoding</head><p>In the testing stage, given the probability tensor P ‚àà R |s|√ó|s|√ó|Y| of the sentence s,<ref type="foot" target="#foot_4">7</ref> how to decode all rectangles (including squares) corresponding to entities or relations remains a non-trivial problem. Since brute force enumeration of all rectangles is intractable, a new joint decoding algorithm is needed. We expect our decoder to have,</p><formula xml:id="formula_16">„ÄÅ „ÄÅ „ÄÅ „ÄÅ „ÄÅ „ÄÅ „ÄÅ „ÄÅ „ÄÅ</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J 7 H U 8 H i 5 r x 5 A N p w e e N X Y E S u F F   ‚Ä¢ Simple implementation and fast decoding.</p><formula xml:id="formula_17">V 0 = " &gt; A A A C J X i c b V D L S s N A F J 3 U V 6 2 v q E s 3 g 0 V w V R J R d O G i 6 M Z l F f u Q J p b J d N I O n U z C z E Q o a X 7 G j b / i x o V F B F f + i t M 0 F G 0 9 M H D m n H u 5 9 x 4 v Y l Q q y / o y C k v L K 6 t r x f X S x u b W 9 o 6 5 u 9 e Q Y S w w q e O Q h a L l I U k Y 5 a S u q G K k F Q m C A o + R p j e 4 n v j N J y I k D f m 9 G k b E D V C P U 5 9 i p L T U M S + d A K k + R i y p p d C h H G Z / z 0 v u 0 s f k F D q K B k T C G R n N y h / S U d o x y 1 b F y g A X i Z 2 T M s h R 6 5 h j p x v i O C B c Y Y a k b N t W p N w E C U U x I 2 n J i S W J E B 6 g H m l r y p G e 6 S b Z l S k 8 0 k o X + q H Q j y u Y q b 8 7 E h R I O Q w 8 X T l Z U s 5 7 E / E / r x 0 r / 8 J N K I 9 i R T i e D v J j B l U I J 5 H B L h U E K z b U B G F B</formula><formula xml:id="formula_18">K 1 r v h / 9 I A G L n L H a / f B F y 7 F W H w a M = " &gt; A A A C L n i c b V D L S g M x F M 3 U V 6 2 v q k s 3 w S L U T Z m R i i 6 L I r i s 0 p d 0 p i W T Z t r Q T G Z I M k K Z z h e 5 8 V d 0 I a i I W z / D d F p E W w 8 E D u f c m 3 v v c U N G p T L N V y O z t L y y u p Z d z 2 1 s b m 3 v 5 H f 3 G j K I B C Z 1 H L B A t F w k C a O c 1 B V V j L R C Q Z D v M t J 0 h 5 c T v 3 l P h K Q B r 6 l R S B w f 9 T n 1 K E Z K S 9 3 8 V d H 2 k R p g x O J q 0 k m 5 8 G P 9 b 3 L c q U G b c p h q r h v f J p 2 4 D G 1 F f S J h e f z T d p e M k 2 6 + Y J b M F H C R W D N S A D N U u / l n u x f g y C d c Y Y a k b F t m q J w Y C U U x I 0 n O j i Q J E R 6 i P m l r y p E e 6 s T p u Q k 8 0 k o P e o H Q j y u Y q r 8 7 Y u R L O f J d X T l Z U s 5 7 E / E / r x 0 p 7 9 y J K Q 8 j R T i e D v I i B l U A J 9 n B H h U E K z b S B G F B 9 a 4 Q D 5 B A W O m E c z o E a / 7 k R d I 4 K V m n J f O m X K h c z O L I g g N w C I r A A m e g A q 5 B F d Q B B</formula><formula xml:id="formula_19">L 1 W O N 2 w 0 O Y F Y = " &gt; A A A C K n i c b V D L T s J A F J 3 i C / F V d e l m I j F x R V q D 0 S X q x i U a e R h a y H Q Y Y M J 0 2 s x M N a T 0 e 9 z 4 K 2 5 Y a I h b P 8 S h N E b B k 9 z k 5 J x 7 c + 8 9 X s i o V J Y 1 N X I r q 2 v r G / n N w t b 2 z u 6 e u X 9 Q l 0 E k M K n h g A W i 6 S F J G O W k p q h i p B k K g n y P k Y Y 3 v J n 5 j S c i J A 3 4 g x q F x P V R n 9 M e x U h p q W N e O T 5 S A 4 x Y X E 3 a K R d + L I L n B D q U w 1 T w v P g + a c d l 6 C j q E w n L 4 5 + Z x 2 S c d M y i V b J S w G V i Z 6 Q I M l Q 7 5 s T p B j j y C V e Y I S l b t h U q N 0 Z C U c x I U n A i S U K E h 6 h P W p p y p J e 6 c f p q A k + 0 0 o W 9 Q O j i C q b q 7 4 k Y + V K O f E 9 3 z o 6 U i 9 5 M / M 9 r R a p 3 6 c a U h 5 E i H M 8 X 9 S I G V Q B n u c E u F Q Q r N t I E Y U H 1 r R A P k E B Y 6 X Q L O g R 7 8 e V l U j 8 r 2 e c l 6 6 5 c r F x n c e T B E T g G p 8 A G F 6 A C b k E V 1 A A G L + A N v I M P 4 9 W Y G F P j c 9 6 a M 7 K Z Q / A H x t c 3 8 s 2 o z A = = &lt; / l a t e x i t &gt; P row 2 R 4‚á•4|Y| ùëô!-dist ùëô!-dist ùë• ! ùë• " ùë• # ùë• $ ùë• ! ùë• " ùë• # ùë• $ ùë• ! ùë• " ùë• # ùë• $</formula><p>We permit slight decoding accuracy drops for scalability.</p><p>‚Ä¢ Strong interactions between entities and relations. When decoding entities, it should take the relation information into account, and vice versa.</p><p>Inspired by the procedures of <ref type="bibr" target="#b17">(Sun et al., 2019)</ref>, We propose a three-steps decoding algorithm: decode span first (entity spans or spans between entities), and then decode entity type of each span, and at last decode relation type of each entity pair (Figure <ref type="figure" target="#fig_9">3</ref>). We consider each cell's probability scores on all labels (including entity labels and relation labels) and predict spans according to a threshold. Then, we predict entities and relations with the highest score. Our heuristic decoding algorithm could be very efficient. Next we will detail the entire decoding process, and give a formal description in the Appendix A.</p><p>Span Decoding One crucial observation of a ground-truth table is that, for an arbitrary entity, its corresponding rows (or columns) are exactly the same in the table (e.g., row 1 and row 2 of Figure 1 are identical), not only for the diagonal entries (entities are squares), but also for the off-diagonal entries (if it participates in a relation with another entity, all its rows (columns) will spot that relation label in the same way). In other words, if the adjacent rows/columns are different, there must be an entity boundary (i.e., one belonging to the entity and the other not belonging to the entity). Therefore, if our biaffine model is reasonably trained, given a model predicted table, we could use this property to find split positions of entity boundary. As expected, experiments (Figure <ref type="figure">4</ref>) verify our assumption. We adapt this idea to the 3-dimensional probability tensor P.  Specifically, we flatten P ‚àà R |s|√ó|s|√ó|Y| as a matrix P row ‚àà R |s|√ó(|s|‚Ä¢|Y|) from row perspective, and then calculate the Euclidean distances (l 2 distances) of adjacent rows. Similarly, we calculate the other Euclidean distances of adjacent columns according to a matrix P col ‚àà R (|s|‚Ä¢|Y|)√ó|s| from column perspective, and then average the two distances as the final distance. If the distance is larger than the threshold Œ± (Œ± = 1.4 in our default settings), this position is a split position. In this way, we can decode all the spans in O(|s|) time complexity.</p><p>Entity Type Decoding Given a span (i, j) by span decoding,<ref type="foot" target="#foot_5">8</ref> we decode the entity type t according to the corresponding square symmetric about the diagonal: t = arg max t‚ààYe‚à™{‚ä•} Avg(P i:j,i:j,t ). If t ‚àà Y e , we decode an entity. If t = ‚ä•, the span (i, j) is not an entity.</p><p>Relation Type Decoding After entity type decoding, given an entity e 1 with the span (i, j) and another entity e 2 with the span (m, n), we decode the relation type l between e 1 and e 2 according to the corresponding rectangle. Formally, l = arg max l‚ààYr‚à™{‚ä•} Avg(P i:j,m:n,l ). If l ‚àà Y r , we decode a relation (e 1 , e 2 , l). If l = ‚ä•, e 1 and e 2 have no relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets We conduct experiments on three entity relation extraction benchmarks: ACE04 (Doddington et al., 2004),<ref type="foot" target="#foot_6">9</ref> ACE05 <ref type="bibr" target="#b21">(Walker et al., 2006)</ref>, <ref type="foot" target="#foot_7">10</ref>and SciERC (Luan et al., 2018). 11 Table <ref type="table" target="#tab_2">2</ref> shows the dataset statistics. Besides, we provide detailed dataset specifications in the Appendix B.</p><p>Evaluation Following suggestions in <ref type="bibr" target="#b19">(Taill√© et al., 2020)</ref>, we evaluate Precision (P), Recall (R), and F1 scores with micro-averaging and adopt the Strict Evaluation criterion. Specifically, a predicted entity is correct if its type and boundaries are correct, and a predicted relation is correct if its relation type is correct, as well as the boundaries and types of two argument entities are correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We tune all hyperparameters based on the averaged entity F1 and relation F1 on ACE05 development set, then keep the same settings on ACE04 and SciERC. For fair comparison with previous works, we use three pre-trained language models: bert-base-uncased <ref type="bibr" target="#b1">(Devlin et al., 2019)</ref>, albert-xxlarge-v1 <ref type="bibr" target="#b6">(Lan et al., 2019)</ref> and scibert-scivocab-uncased <ref type="bibr" target="#b0">(Beltagy et al., 2019)</ref> as the sentence encoder and fine-tune them in training stage. <ref type="foot" target="#foot_9">12</ref>For the MLP layer, we set the hidden size as d = 150 and use GELU as the activation function. We use AdamW optimizer <ref type="bibr" target="#b9">(Loshchilov and Hutter, 2017)</ref> with Œ≤ 1 = 0.9 and Œ≤ 2 = 0.9, and observe a phenomenon similar to <ref type="bibr" target="#b3">(Dozat and Manning, 2016)</ref> in that setting Œ≤ 2 from 0.9 to 0.999 causes a significant drop on final performance. The batch size is 32, and the learning rate is 5e-5 with weight decay 1e-5. We apply a linear warm-up learning rate scheduler with a warm-up ratio of 0.2. We train our model with a maximum of 200 epochs (300 epochs for SciERC) and employ an early stop strategy. We perform all experiments on an Intel(R) Xeon(R) W-3175X CPU and a NVIDIA Quadro RTX 8000 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Performance Comparison</head><p>Table <ref type="table" target="#tab_3">3</ref> summarizes previous works and our UNIRE on three datasets. <ref type="foot" target="#foot_10">13</ref> In general, UNIRE achieves the best performance on ACE04 and Sci-ERC and a comparable result on ACE05. Comparing with the previous best joint model <ref type="bibr" target="#b22">(Wang and Lu, 2020)</ref>, our model significantly advances both entity and relation performances, i.e., an absolute F1 of +0.9 and +0.7 for entity as well as +3.4 and +1.7 for relation, on ACE04 and ACE05 respectively. For the best pipeline model <ref type="bibr" target="#b31">(Zhong and Chen, 2020</ref>) (current SOTA), our model achieves superior performance on ACE04 and SciERC and comparable performance on ACE05. Comparing with ACE04/ACE05, SciERC is much smaller, so entity performance on SciERC drops sharply. Since <ref type="bibr" target="#b31">(Zhong and Chen, 2020</ref>) is a pipeline method, its relation performance is severely influenced by the poor entity performance. Nevertheless, our model is less influenced in this case and  <ref type="bibr" target="#b8">(Li et al., 2019)</ref> and ALBERT XXLARGE <ref type="bibr" target="#b22">(Wang and Lu, 2020)</ref>. These results confirm the proposed unified label space is effective for exploring the interaction between entities and relations. Note that all subsequent experiment results on ACE04 and ACE05 are based on BERT BASE for efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>In this section, we analyze the effects of components in UNIRE with different settings ( From the ablation study, we get the following observations.</p><p>‚Ä¢ When one of the additional losses is removed, the performance will decline with varying de- grees (line 2-3). Specifically, the symmetrical loss has a significant impact on SciERC (decrease 1.1 points and 1.4 points for entity and relation performance). While removing the implication loss will obviously harm the relation performance on ACE05 (1.0 point). It demonstrates that the structural information incorporated by both losses is useful for this task.</p><p>‚Ä¢ Comparing with the "Default", the performance of "w/o logit dropout" and "w/o crosssentence context" drop more sharply (line 4-5).</p><p>Logit dropout prevents the model from overfitting, and cross-sentence context provides more contextual information for this task, especially for small datasets like SciERC.</p><p>‚Ä¢ The "hard decoding" has the worst performance (its relation performance is almost half of the "Default") (line 6). The major reason is that "hard decoding" separately decodes entities and relations. It shows the proposed decoding algorithm jointly considers entities and relations, which is important for decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Inference Speed</head><p>Following (Zhong and Chen, 2020), we evaluate the inference speed of our model (  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Impact of Different Threshold Œ±</head><p>In Figure <ref type="figure">4</ref>, the distance between adjacent rows not at entity boundary ("Non-Ent-Bound") mainly concentrates at 0, while that at entity boundary ("Ent-Bound") is usually greater than 1. This phenomenon verifies the correctness of our span decoding method. Then we evaluate the performances, with regard to the threshold Œ± in Figure <ref type="figure" target="#fig_10">5</ref>. <ref type="foot" target="#foot_11">14</ref> Both span and entity performances sharply decrease when Œ± increases from 1.4 to 1.5, while the relation performance starts to decline slowly from Œ± = 1.5. The major reason is that relations are so sparse that many entities do not participate in any relation, so the threshold of relation is much higher than that of entity. Moreover, we observe a similar phenomenon on ACE04 and SciERC, and Œ± = 1.4 is a general best setting on three datasets. It shows the stability and generalization of our model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Context Window and Logit Dropout Rate</head><p>In Table <ref type="table" target="#tab_5">4</ref>, both cross-sentence context and logit dropout can improve the entity and relation performance. Table <ref type="table">6</ref> shows the effect of different context window size W and logit dropout rate p. The entity and relation performances are significantly improved from W = 100 to W = 200, and drop sharply from W = 200 to W = 300. Similarly, we achieve the best entity and relation performances when p = 0.2. So we use W = 200 and p = 0.2 in our final model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Error Analysis</head><p>We further analyze the remaining errors for relation extraction and present the distribution of five errors: span splitting error (SSE), entity not found (ENF), entity type error (ETE), relation not found (RNF), and relation type error (RTE) in Figure <ref type="figure" target="#fig_11">6</ref>. The proportion of "SSE" is relatively small, which proves the effectiveness of our span decoding method. Moreover, the proportion of "not found error" is significantly larger than that of "type error" for both entity and relation. The primary reason is that the table filling suffers from the class imbalance issue, i.e., the number of ‚ä• is much larger than that of other classes. We reserve this imbalanced classification problem in the future. Finally, we give some concrete examples in Figure <ref type="figure" target="#fig_12">7</ref> to verify the robustness of our decoding algorithm. There are some errors in the biaffine model's prediction, such as cells in the upper left corner (first example) and upper right corner (second example) in the intermediate table. However, these errors are corrected after decoding, which demonstrates that our decoding algorithm not only recover all entities and relations but also corrects errors leveraging table structure and neighbor cells' information.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Entity relation extraction has been extensively studied over the decades. Existing methods can be roughly divided into two categories according to the adopted label space.</p><p>Separate Label Spaces This category study this task as two separate sub-tasks: entity recognition and relation classification, which are defined in two separate label spaces. One early paradigm is the pipeline method <ref type="bibr" target="#b26">(Zelenko et al., 2003;</ref><ref type="bibr" target="#b14">Miwa et al., 2009)</ref> that uses two independent models for two sub-tasks respectively. Then joint method handles this task with an end-to-end model to explore more interaction between entities and relations. The most basic joint paradigm, parameter sharing <ref type="bibr" target="#b13">(Miwa and Bansal, 2016;</ref><ref type="bibr" target="#b5">Katiyar and Cardie, 2017)</ref>, adopts two independent decoders based on a shared encoder. Recent span-based models <ref type="bibr" target="#b12">(Luan et al., 2019b;</ref><ref type="bibr" target="#b20">Wadden et al., 2019)</ref> also use this paradigm.</p><p>To enhance the connection of two decoders, many joint decoding algorithms are proposed, such as ILP-based joint decoder <ref type="bibr" target="#b25">(Yang and Cardie, 2013)</ref>, joint MRT <ref type="bibr" target="#b18">(Sun et al., 2018)</ref>, GCN-based joint inference <ref type="bibr" target="#b17">(Sun et al., 2019)</ref>. Actually, table filling method <ref type="bibr" target="#b15">(Miwa and Sasaki, 2014;</ref><ref type="bibr" target="#b4">Gupta et al., 2016;</ref><ref type="bibr" target="#b29">Zhang et al., 2017;</ref><ref type="bibr">Wang et al., 2020</ref>) is a special case of parameter sharing in table structure. These joint models all focus on various joint algorithms but ignore the fact that they are essentially based on separate label spaces.</p><p>Unified Label Space This family of methods aims to unify two sub-tasks and tackle this task in a unified label space. Entity relation extraction has been converted into a tagging problem <ref type="bibr" target="#b30">(Zheng et al., 2017)</ref>, a transition-based parsing problem <ref type="bibr" target="#b23">(Wang et al., 2018)</ref>, and a generation problem with  Seq2Seq framework <ref type="bibr" target="#b27">(Zeng et al., 2018;</ref><ref type="bibr" target="#b16">Nayak and Ng, 2020)</ref>. We follow this trend and propose a new unified label space. We introduce a 2D table to tackle the overlapping relation problem in <ref type="bibr" target="#b30">(Zheng et al., 2017)</ref>. Also, our model is more versatile as not relying on complex expertise like <ref type="bibr" target="#b23">(Wang et al., 2018)</ref>, which requires external expert knowledge to design a complex transition system.</p><note type="other">Gold Table Intermediate Table Decoded Table</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we extract entities and relations in a unified label space to better mine the interaction between both sub-tasks. We propose a novel table that presents entities and relations as squares and rectangles. Then this task can be performed in two simple steps: filling the table with our biaffine model and decoding entities and relations with our joint decoding algorithm. Experiments on three benchmarks show the proposed method achieves not only state-of-the-art performance but also promising efficiency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Example of a table for joint entity relation extraction. Each cell corresponds to a word pair. Entities are squares on diagonal, relations are rectangles off diagonal. Note that PER-SOC is a undirected (symmetrical) relation type, while PHYS and ORG-AFF are directed (asymmetrical) relation types. The table exactly expresses overlapped relations, e.g., the person entity "David Perkins" participates in two relations, ("David Perkins", "wife", PER-SOC) and ("David Perkins", "California", PHYS). For every cell, a same biaffine model predicts its label. The joint decoder is set to find the best squares and rectangles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>5  </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " J 7 H U 8 H i 5 r x 5 A N p w e e N X Y E S u F</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>4 i 1 9 e J o 9 n d e + y 7 t 5 f 1 B o 3 R R 1 l O I J j O A U P r q A B d 9 C E F j B I 4 B l e 4 c 1 J n R f n 3 f m Y j 5 a c Y u c Q / s D 5 / A E W S p G 2 &lt; / l a t e x i t &gt; h 3 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F d / C w 9 i n / Z G g w y 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>X H e n Y / F a M k p d o 7 h D 5 z P H x f O k b c = &lt; / l a t e x i t &gt; h 4 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y 3V V k f Y 2 h C A 6 c 4 a / a 1 X 7 i d e q r R g = " &gt; A A A C A 3 i c b V D L S s N A F J 3 U V 6 2 v q D v d D B b B V U l E 0 W X R j c s K 9 g F t D J P J p B k 6 m Y S Z i V B C w I 2 / 4 s a F I m 7 9 C X f + j Z M 0 C 2 0 9 M H D m n H u 5 9 x 4 v Y V Q q y / o 2 a k v L K 6 t r 9 f X G x u b W 9 o 6 5 u 9 e T c S o w 6 e K Y x W L g I U k Y 5 a S r q G J k k A i C I o + R v j e 5 L v z + A x G S x v x O T R P i R G j M a U A x U l p y z Y N R h F T o B V m Y u / S + / I g o C w n y c 9 d s W i 2 r B F w k d k W a o E L H N b 9 G f o z T i H C F G Z J y a F u J c j I k F M W M 5 I 1 R K k m C 8 A S N y V B T j i I i n a y 8 I Y f H W v F h E A v 9 u I K l + r s j Q 5 G U 0 8 j T l c W S c t 4 r x P + 8 Y a q C S y e j P E k V 4 X g 2 K E g Z V D E s A o E + F Q Q r N t U E Y U H 1 r h C H S C C s d G w N H Y I 9 f / I i 6 Z 2 2 7 P O W d X v W b F 9 V c d T B I T g C J 8 A G F 6 A N b k A H d A E G j + A Z v I I3 4 8 l 4 M d 6 N j 1 l p z a h 6 9 s E f G J 8 / H V W Y d g = = &lt; / l a t e x i t &gt; h head i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f f H X m u u j K D P L N p u s 5 p y w e J z l + 1 g = " &gt; A A A C A n i c b V D L S s N A F J 3 U V 6 2 v q C t x M 1 g E V y U R R Z d F N y 4 r 2 A e 0 M U w m k 3 b o z C T M T I Q S g h t / x Y 0 L R d z 6 F e 7 8 G y d p F 9 p 6 Y O D M O f d y 7 z 1 B w q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>a S P 5 9 s G A I z 0 K o m y U + / S + / E i e E R H m v l 1 3 G k 4 J u E j c G a m D G V q + / T U I Y 5 x y I j R m S K m + 6 y T a y 5 D U F D O S 1 w a p I g n C</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of our model architecture. One main objective (L entry ) and two additional objectives (L sym , L imp ) are imposed on probability tensor P and optimized jointly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>9 a 4 Q 9 5 F A W O l g S z o E e / 7 k R d I 4 q d h n F e v 2 t F y 9 y u M o g g N w C I 6 B D c 5 B F d y A G q g D D J 7 B K 3 g H Y + P F e D M + j M 9 p a c H I e / b B H x j f P 0 1 h p c U = &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>g / g C b y B d + P R e D E + j M 9 p a c a Y 9 e y D P z C + v g E y H K n d &lt; / l a t e x i t &gt; (P col ) T 2 R 4‚á•4|Y| &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S 6 k P p o f d 6 6 e l l 1 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Overview of our joint decoding algorithm. It consists of three steps: span decoding, entity type decoding, and relation type decoding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Performances with respect to the threshold Œ± on ACE05 dev set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Distribution of five relation extraction errors on ACE05 and SciERC test data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Examples showing the robustness of our decoding algorithm. "GoldTable" presents the gold label. "Intermediate Table" presents the biaffine model's prediction (choosing the label with the highest probability for each cell). "Decoded Table" presents the final results after decoding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Symmetrical label set Y sym for used datasets.</figDesc><table><row><cell>Dataset</cell><cell>Ent</cell><cell>Ysym</cell><cell>Rel</cell></row><row><cell>ACE04/ ACE05</cell><cell cols="2">PER,ORG,LOC, FAC,WEA,VEH,GPE</cell><cell>PER-SOC</cell></row><row><cell>SciERC</cell><cell cols="2">Task,Method,Metric, Material,Generic, OtherScientificTerm</cell><cell>COMPAREP, CONJUNCTION</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The statistics of the adopted datasets.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Overall evaluation. means that the model leverages cross-sentence context information.</figDesc><table><row><cell cols="2">Dataset Model</cell><cell>Encoder</cell><cell>P</cell><cell>Entity R</cell><cell>F1</cell><cell>P</cell><cell>Relation R</cell><cell>F1</cell></row><row><cell></cell><cell>Li and Ji (2014)</cell><cell>-</cell><cell cols="6">83.5 76.2 79.7 60.8 36.1 45.3</cell></row><row><cell></cell><cell>Miwa and Bansal (2016)</cell><cell>LSTM</cell><cell cols="6">80.8 82.9 81.8 48.7 48.1 48.4</cell></row><row><cell></cell><cell cols="2">Katiyar and Cardie (2017) LSTM</cell><cell cols="6">81.2 78.1 79.6 46.4 45.3 45.7</cell></row><row><cell></cell><cell>Li et al. (2019)</cell><cell>BERTLARGE</cell><cell cols="6">84.4 82.9 83.6 50.1 48.7 49.4</cell></row><row><cell>ACE04</cell><cell>Wang and Lu (2020)</cell><cell>ALBERTXXLARGE</cell><cell>-</cell><cell>-</cell><cell>88.6</cell><cell>-</cell><cell>-</cell><cell>59.6</cell></row><row><cell></cell><cell>Zhong and Chen (2020)</cell><cell>BERTBASE</cell><cell>-</cell><cell>-</cell><cell>89.2</cell><cell>-</cell><cell>-</cell><cell>60.1</cell></row><row><cell></cell><cell>Zhong and Chen (2020)</cell><cell>ALBERTXXLARGE</cell><cell>-</cell><cell>-</cell><cell>90.3</cell><cell>-</cell><cell>-</cell><cell>62.2</cell></row><row><cell></cell><cell>UNIRE</cell><cell>BERTBASE</cell><cell cols="6">87.4 88.0 87.7 62.1 58.0 60.0</cell></row><row><cell></cell><cell>UNIRE</cell><cell cols="7">ALBERTXXLARGE 88.9 90.0 89.5 67.3 59.3 63.0</cell></row><row><cell></cell><cell>Li and Ji (2014)</cell><cell>-</cell><cell cols="6">85.2 76.9 80.8 65.4 39.8 49.5</cell></row><row><cell></cell><cell>Miwa and Bansal (2016)</cell><cell>LSTM</cell><cell cols="6">82.9 83.9 83.4 57.2 54.0 55.6</cell></row><row><cell></cell><cell cols="2">Katiyar and Cardie (2017) LSTM</cell><cell cols="6">84.0 81.3 82.6 55.5 51.8 53.6</cell></row><row><cell></cell><cell>Sun et al. (2019)</cell><cell>LSTM</cell><cell cols="6">86.1 82.4 84.2 68.1 52.3 59.1</cell></row><row><cell></cell><cell>Li et al. (2019)</cell><cell>BERTLARGE</cell><cell cols="6">84.7 84.9 84.8 64.8 56.2 60.2</cell></row><row><cell>ACE05</cell><cell>Wang et al. (2020)</cell><cell>BERTBASE</cell><cell>-</cell><cell>-</cell><cell>87.2</cell><cell>-</cell><cell>-</cell><cell>63.2</cell></row><row><cell></cell><cell>Wang and Lu (2020)</cell><cell>ALBERTXXLARGE</cell><cell>-</cell><cell>-</cell><cell>89.5</cell><cell>-</cell><cell>-</cell><cell>64.3</cell></row><row><cell></cell><cell>Zhong and Chen (2020)</cell><cell>BERTBASE</cell><cell>-</cell><cell>-</cell><cell>90.2</cell><cell>-</cell><cell>-</cell><cell>64.6</cell></row><row><cell></cell><cell>Zhong and Chen (2020)</cell><cell>ALBERTXXLARGE</cell><cell>-</cell><cell>-</cell><cell>90.9</cell><cell>-</cell><cell>-</cell><cell>67.8</cell></row><row><cell></cell><cell>UNIRE</cell><cell>BERTBASE</cell><cell cols="6">88.8 88.9 88.8 67.1 61.8 64.3</cell></row><row><cell></cell><cell>UNIRE</cell><cell cols="7">ALBERTXXLARGE 89.9 90.5 90.2 72.3 60.7 66.0</cell></row><row><cell></cell><cell>Wang et al. (2020)</cell><cell>SciBERT</cell><cell>-</cell><cell>-</cell><cell>68.0</cell><cell>-</cell><cell>-</cell><cell>34.6</cell></row><row><cell>SciERC</cell><cell>Zhong and Chen (2020)</cell><cell>SciBERT</cell><cell>-</cell><cell>-</cell><cell>68.2</cell><cell>-</cell><cell>-</cell><cell>36.7</cell></row><row><cell></cell><cell>UNIRE</cell><cell>SciBERT</cell><cell cols="6">65.8 71.1 68.4 37.3 36.6 36.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Table" presents the gold label. "Intermediate Table" presents the biaffine model's prediction (choosing the label with the highest probability for each cell). "Decoded Table" presents the final results after decoding.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Source code and models are available at https://github. com/Receiling/UniRE.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1">We set dropout rate p = 0.2 by default.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2">P without logit dropout mentioned in Section 3.2 to preserve learned structure.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3">We directly sum the three losses to avoid introducing more hyper-parameters.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4">For the symmetrical label t ‚àà Ysym, we set Pi,j,t = Pj,i,t = (Pi,j,t + Pj,i,t)/2.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5">i and j denote start and end indices of the span.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6">https://catalog.ldc.upenn.edu/LDC2005T09</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_7">https://catalog.ldc.upenn.edu/LDC2006T06</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_8">http://nlp.cs.washington.edu/sciIE/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_9">The first two are for ACE04 and ACE05, and the last one is for SciERC.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_10">Since<ref type="bibr" target="#b11">(Luan et al., 2019a;</ref><ref type="bibr" target="#b20">Wadden et al., 2019)</ref> neglect the argument entity type in relation evaluation and underperform our baseline<ref type="bibr" target="#b28">(Zhang et al., 2020)</ref>, we do not compare their results here.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_11">We use an additional metric to evaluate span performance, "Span F1", is Micro-F1 of predicted split positions.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The authors wish to thank the reviewers for their helpful comments and suggestions. This work was (partially) supported by National Key Research and Development Program of China (2018AAA0100704), NSFC (61972250, 62076097), STCSM (18ZR1411500), Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102), and the Fundamental Research Funds for the Central Universities.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Datasets</head><p>The ACE04 and ACE05 corpora are collected from various domains, such as newswire and online forums. Both corpora annotate 7 entity types and 6 relation types. we use the same data splits and preprocessing as <ref type="bibr" target="#b7">(Li and Ji, 2014;</ref><ref type="bibr" target="#b13">Miwa and Bansal, 2016)</ref>, i.e., 5-fold cross-validation for ACE04, and 351 training, 80 validating, and 80 testing for ACE05. 15 Besides, we randomly sample 10% of training set as the development set for ACE04.</p><p>The SciERC corpus collects 500 scientific abstracts taken from AI conference/workshop proceedings. This dataset annotates 6 entity types and 7 relation types. We adopt the same data split protocol as in <ref type="bibr">(Luan et al., 2019b) (350 training, 50 validating, and 100 testing)</ref>. Detailed dataset specifications are shown in Table <ref type="table">2</ref>. 15 We use the pre-processing scripts provided by <ref type="bibr" target="#b22">(Wang and Lu, 2020)</ref> at https://github.com/LorrinWWW/ two-are-better-than-one/tree/master/datasets. Moreover, we correct the annotations of undirected relations for three datasets, regarding each undirected relation as two directed relation instances, e.g., for the undirected relation PER-SOC, only one relation triplet ("his", wife", PER-SOC) is annotated in the original dataset, we will add another relation triplet ("wife", "his", PER-SOC) in our corrected datasets for symmetry. In this case, each undirected relation corresponds to two rectangles, which are symmetrical about the diagonal.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10676</idno>
		<title level="m">Scibert: A pretrained language model for scientific text</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The automatic content extraction (ACE) program -tasks, data, and evaluation</title>
		<author>
			<persName><forename type="first">George</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Przybocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC&apos;04)</title>
				<meeting>the Fourth International Conference on Language Resources and Evaluation (LREC&apos;04)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01734</idno>
		<title level="m">Deep biaffine attention for neural dependency parsing</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Table filling multi-task recurrent neural network for joint entity and relation extraction</title>
		<author>
			<persName><forename type="first">Pankaj</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch√ºtze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Andrassy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
				<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2537" to="2547" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Going out on a limb: Joint extraction of entity mentions and relations without dependency trees</title>
		<author>
			<persName><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1085</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="917" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Incremental joint extraction of entity mentions and relations</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-1038</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="402" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Entity-relation extraction as multi-turn question answering</title>
		<author>
			<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiayu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arianna</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duo</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1129</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1340" to="1350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1360</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3219" to="3232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A general framework for information extraction using dynamic span graphs</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03296</idno>
		<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A general framework for information extraction using dynamic span graphs</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1308</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019b</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3036" to="3046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-to-end relation extraction using LSTMs on sequences and tree structures</title>
		<author>
			<persName><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1105</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1105" to="1116" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A rich feature vector for protein-protein interaction extraction from multiple corpora</title>
		<author>
			<persName><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rune</forename><surname>Saetre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun'ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="121" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modeling joint entity and relation extraction with table representation</title>
		<author>
			<persName><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Sasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1858" to="1869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Effective modeling of encoder-decoder architecture for joint entity and relation extraction</title>
		<author>
			<persName><forename type="first">Tapas</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8528" to="8535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Joint type inference on entities and relations via graph convolutional networks</title>
		<author>
			<persName><forename type="first">Changzhi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiliang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1131</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1361" to="1370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Extracting entities and relations with joint minimum risk training</title>
		<author>
			<persName><forename type="first">Changzhi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiliang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuang-Chih</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kewen</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1249</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Let&apos;s stop incorrect comparisons in end-to-end relation extraction!</title>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Taill√©</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Guigue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Scoutheeten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3689" to="3701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Entity, relation, and event extraction with contextualized span representations</title>
		<author>
			<persName><forename type="first">David</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulme</forename><surname>Wennberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1585</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5784" to="5789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><surname>Medero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<title level="m">Ace 2005 multilingual training corpus. Linguistic Data Consortium</title>
				<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page">45</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Two are better than one: Joint entity and relation extraction with tablesequence encoders</title>
		<author>
			<persName><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.133</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1706" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Joint extraction of entities and relations based on a novel graph scheme</title>
		<author>
			<persName><forename type="first">Shaolei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4461" to="4467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pre-training entity relation encoder with intra-span and inter-span information</title>
		<author>
			<persName><forename type="first">Yijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changzhi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guotong</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.132</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1692" to="1705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Joint inference for fine-grained opinion extraction</title>
		<author>
			<persName><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1640" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Kernel methods for relation extraction</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Zelenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chinatsu</forename><surname>Aone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Richardella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1083" to="1106" />
			<date type="published" when="2003-02">2003. Feb</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Extracting relational facts by an end-to-end neural model with copy mechanism</title>
		<author>
			<persName><forename type="first">Xiangrong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="506" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Minimize exposure bias of seq2seq models in joint entity and relation extraction</title>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aysa</forename><surname>Xuemo Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.07503</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-to-end neural relation extraction with global optimization</title>
		<author>
			<persName><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guohong</forename><surname>Fu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1182</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1730" to="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Joint extraction of entities and relations based on a novel tagging scheme</title>
		<author>
			<persName><forename type="first">Suncong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuexing</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05075</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A frustratingly easy approach for joint entity and relation extraction</title>
		<author>
			<persName><forename type="first">Zexuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12812</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
