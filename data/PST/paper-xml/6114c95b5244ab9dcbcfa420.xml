<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EDITS: Modeling and Mitigating Data Bias for Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-08-11">11 Aug 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yushun</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Virginia Charlottesville</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ninghao</forename><surname>Liu</surname></persName>
							<email>ninghao.liu@uga.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Georgia Athens</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Brian</forename><surname>Jalaian</surname></persName>
							<email>brian.a.jalaian.civ@mail.mil</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Army Research Laboratory Arlington</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
							<email>jundong@virginia.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Virginia Charlottesville</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EDITS: Modeling and Mitigating Data Bias for Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-08-11">11 Aug 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2108.05233v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) have recently demonstrated superior capability of tackling graph analytical problems in various applications. Nevertheless, with the wide-spreading practice of GNNs in high-stake decision-making processes, there is an increasing societal concern that GNNs could make discriminatory decisions that may be illegal towards certain demographic groups. Although some explorations have been made towards developing fair GNNs, existing approaches are tailored for a specific GNN model. However, in practical scenarios, myriads of GNN variants have been proposed for different tasks, and it is costly to train and fine-tune existing debiasing models for different GNNs. Also, bias in a trained model could originate from training data, while how to mitigate bias in the graph data is usually overlooked. In this work, different from existing work, we first propose novel definitions and metrics to measure the bias in an attributed network, which leads to the optimization objective to mitigate bias. Based on the optimization objective, we develop a framework named EDITS to mitigate the bias in attributed networks while preserving useful information. EDITS works in a model-agnostic manner, which means that it is independent of the specific GNNs applied for downstream tasks. Extensive experiments on both synthetic and real-world datasets demonstrate the validity of the proposed bias metrics and the superiority of ED-ITS on both bias mitigation and utility maintenance. Open-source implementation: https://github.com/yushundong/EDITS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph-structured data, such as social networks <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b51">52]</ref>, chemical reaction networks <ref type="bibr" target="#b12">[13]</ref> and traffic networks <ref type="bibr" target="#b48">[49]</ref>, has become ubiquitous in a plethora of realms. To better understand graph data, various graph mining models have been proposed. Among them, the recently emerged Graph Neural Networks (GNNs) have demonstrated superior capability of tackling graph analytical problems in various tasks, such as node classification <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55]</ref> and link prediction <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b61">62]</ref>. Despite the superior performance, GNNs usually do not consider fairness issues in the learning process <ref type="bibr" target="#b11">[12]</ref>. Extensive research has been carried out to demonstrate that graph mining algorithms, especially those recently proposed GNNs <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b58">59]</ref>, could deliver discriminatory decisions dominated by sensitive attributes such as gender <ref type="bibr" target="#b16">[17]</ref> and political preference <ref type="bibr" target="#b42">[43]</ref> in many decision-making systems. In real-world scenarios, people are unwilling to experience such discriminatory decisions, which sometimes are even in conflict with anti-discrimination laws <ref type="bibr" target="#b25">[26]</ref>. As a consequence, the wide-spreading practice of GNNs has turned it into a crucial problem to mitigate bias in real applications.</p><p>Various efforts have been made to mitigate the bias exhibited in graph mining algorithms. For example, random walk algorithm can be modified via improving the appearance rate of minorities <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b47">48]</ref>. Adversarial learning is another popular approach, which aims to learn node embeddings that are not distinguishable on sensitive attributes <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b39">40]</ref>. Some recent efforts have also been made to mitigate bias in the outcome of GNNs. For example, adversarial learning can also be adapted to GNNs for outcome bias mitigation <ref type="bibr" target="#b11">[12]</ref>. Nevertheless, existing approaches to debias GNN outcome are tailored for a specific GNN model on a certain downstream task. In practical scenarios, different tasks could adopt different GNN variants, and it is costly to train and fine-tune the debiasing models for diverse GNN backbones. As a consequence, to mitigate bias more efficiently for different GNNs and tasks, developing an one-size-fits-all approach becomes highly desired. Then the question is: how can we perform debiasing regardless of specific GNNs and downstream tasks? Considering that a model trained on biased datasets also tends to be biased <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15]</ref>, directly debiasing the dataset itself can be a straightforward solution. There are already debiasing approaches modifying original datasets via perturbing data distributions or reweighting the data points in the dataset <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b56">57]</ref>. These approaches obtain less biased datasets, which help to mitigate bias in downstream tasks. In this regard, considering that debiasing for different GNNs is costly, it is also highly desired to mitigate the bias in attributed networks before they are fed into GNNs. Nevertheless, to the best of our knowledge, despite its fundamental importance, no existing literature has taken such a step forward.</p><p>In this paper, we aim to make an initial investigation on debiasing attributed networks as the input of GNNs. Specifically, we tackle the following challenges in debiasing attributed networks.</p><p>(1) Data Bias Modeling. Traditionally, bias modeling is coupled with the outcomes of a specific downstream task, e.g., node classification. Based on such outcomes, bias can then be modeled via metrics focusing on different notions, e.g., Statistical Parity <ref type="bibr" target="#b14">[15]</ref> and Equality of Opportunity <ref type="bibr" target="#b20">[21]</ref>, to determine whether the outcome is biased towards some specific demographic groups. Nevertheless, if debiasing is carried out directly based on the attributed network data, the first and foremost challenge is how to appropriately model such data bias. (2) Multi-modality Debiasing. In fact, attributed networks contain both graph structure and node attributes information. Correspondingly, bias may exist with diverse formats across different data modalities. In this regard, how to debias attributed networks with the existence of different data modalities is the second challenge that needs to be tackled. (3) Model-agnostic Debiasing. Existing debiasing approaches for GNNs commonly require supervision signal in the training process. Different from these approaches, model-agnostic debiasing for GNNs does not require the supervision over model output, as they are decoupled from each other. Nevertheless, the ultimate goal of debiasing still lies in mitigating the bias in model output of a specific downstream task. Such a contradiction poses the challenge of how to properly formulate a debiasing objective that can be universally applied to different GNNs in downstream tasks.</p><p>To tackle the challenges discussed above, we present novel data bias modeling approaches and a principled debiasing framework named EDITS (modEling anD mItigating daTa biaS) to achieve model-agnostic attributed network debiasing for GNNs. Specifically, we first carry out preliminary analysis to illustrate how bias exists in the two data modalities (i.e., node attributes and network structure) and affects each other in the information propagation of GNNs. Then, we formally define attribute bias and structural bias, together with the corresponding metrics for data bias modeling. Besides, we formulate the problem of debiasing attributed networks for GNNs, and propose a novel framework named EDITS for bias mitigation. It should be noted that EDITS is model-agnostic for different GNNs, meaning that we directly mitigate bias for attributed networks. Finally, empirical evaluations on both synthetic and real-world datasets corroborate the validity of the proposed bias metrics and the effectiveness of EDITS. Our contributions are summarized as:</p><p>• Problem Formulation. We formulate and make an initial investigation on a novel problem: debiasing attributed networks for GNNs based on the analysis of the information propagation mechanism of GNNs. • Metric and Algorithm Design. We design new metrics to model the bias existed in both data modalities of attributed networks, and propose a debiasing framework named EDITS to mitigate bias in attributed networks for GNNs. • Theoretical Analysis. We provide theoretical analysis for the proposed metrics and framework, which not only builds connections between the two kinds of bias, but also reveal the essence of the proposed framework EDITS. • Experimental Evaluation. We conduct comprehensive experiments on both synthetic and real-world datasets to verify the validity of the proposed bias metrics and the effectiveness of the proposed framework EDITS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARY ANALYSIS</head><p>To show how the two data modalities (i.e., attribute and structure) introduce bias together in an attributed network during information propagation -the most common operation in GNNs, here we provide two exemplary cases. These two cases also bring insights on tackling the three challenges mentioned in Section 1. Following previous literature on algorithmic bias <ref type="bibr" target="#b14">[15]</ref>, the attribute distribution difference between groups is regarded as the bias in attribute, while the group membership distribution difference of the neighbors for nodes between groups is regarded as the bias in structure. Such bias in attribute and structure can be regarded as the bias existed in two data modalities in an attributed network. To better illustrate how biased and unbiased data modalities affect each other, we only introduce bias to one data modality in each case, and leave the other data modality unbiased. Now we explain how synthetic attributed networks are generated. Without loss of generality, we assume the sensitive attribute is gender, and 1,000 nodes are generated with half males (blue) and half females (orange) for both cases. In addition to the sensitive attribute, each node is with an extra two-dimensional attribute vector, which will be initialized and utilized for information propagation. To introduce bias to either of the data modalities, different strategies are adopted to generate the attribute vector and the network structure in the two cases. To study how biased and unbiased data modalities affect each other during information propagation, we compare the distribution difference of attribute between groups before and after propagation. Here the information propagation mechanism of GCN <ref type="bibr" target="#b28">[29]</ref>   <ref type="formula" target="#formula_1">1d</ref>) with (1f), we find that even if the original attributes are unbiased, the biased structure still turns the attributes into biased ones after information propagation. This implies that the bias contained in the network structure is also a significant source of bias. Based on the above discussions, we draw three preliminary conclusions to help us tackle the challenges mentioned in Section 1.</p><p>(1) For Data Bias Modeling, bias in attributes can be modeled based on the difference of attribute distribution between groups. Also, bias in network structure can be modeled based on the difference of attribute distribution between groups after information propagation. (2) For Multi-modality Debiasing in an attributed network, at least two debiasing processes should be carried out targeting the two data modalities (i.e., attributes and structure).</p><p>(3) For Model-agnostic Debiasing, if the attribute distributions between groups can be less biased both before and after general information propagation, the learned node representations tend to be indistinguishable between groups. Then, GNNs trained on such data could also be less biased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MODELING DATA BIAS FOR GNNS</head><p>In this section, consistent with traditional algorithmic bias notion <ref type="bibr" target="#b14">[15]</ref>, we define attribute bias and structural bias in attributed networks together with their metrics. Without loss of generality, only binary sensitive attributes are considered, which can be easily generalized to more complicated scenarios. Theoretical analysis corresponding to the metrics will be presented in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>In this paper, without further specification, bold uppercase letters (e.g., X), bold lowercase letters (e.g., x), and normal lowercase letters (e.g., 𝑥) represent matrices, vectors, and scalars, respectively. For any matrix, e.g., X, we use X 𝑖 denote its 𝑖-th row.</p><p>Let G = (A, X) be an undirected attributed network. A ∈ R 𝑁 ×𝑁 is the adjacency matrix, and X ∈ R 𝑁 ×𝑀 is the node attribute matrix, where 𝑁 is the number of nodes and 𝑀 is the attribute dimension. Let diagonal matrix D be the degree matrix of A, where its (𝑖,𝑖)-th entry D 𝑖,𝑖 = 𝑗 A 𝑖 𝑗 , and D 𝑖,𝑗 = 0 (𝑖 ≠ 𝑗). L = D − A is the graph Laplacian. Denote the normalized adjacency matrix and the normalize Laplacian matrix as</p><formula xml:id="formula_0">A norm = D − 1 2 AD − 1 2 and L norm = D − 1 2 LD − 1 2 . |.</formula><p>| is the absolute value operator for scalars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Definitions of Bias</head><p>We consider two types of bias on attributed networks, i.e., attribute bias and structural bias. We first define attribute bias as follows.</p><p>Definition 1. Attribute bias. Given an undirected attributed network G = (A, X) and the corresponding group indicator (w.r.t. the sensitive attribute) for each node s = [𝑠 1 , 𝑠 2 , ..., 𝑠 𝑁 ], where 𝑠 𝑖 ∈ {0, 1} (1 ≤ 𝑖 ≤ 𝑁 ). For any attribute, if the distributions between different demographic groups are different, then attribute bias exists in G.</p><p>Besides, as shown in the second example in Section 2, bias can also emerge after attributes are propagated in the network even when original attributes are unbiased. Therefore, an intuitive idea to identify structural bias is whether information propagation in the network introduces or exacerbates bias <ref type="bibr" target="#b23">[24]</ref>. Formally, we give the definition of structural bias on attributed networks as follows. Apart from these definitions, it is also necessary to quantitatively measure the attribute bias and structural bias. In the sequel, we introduce our proposed metrics for the two types of bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Bias Metrics</head><p>Here we take the first step to define metrics for both attribute bias and structural bias for an undirected attributed network G. We defer a detailed theoretical analysis that builds the connections between these two bias metrics in Section 5. Attribute bias metric. Let X norm ∈ R 𝑁 ×𝑀 be the column normalized attribute matrix. For the 𝑚-th dimension (1 ≤ 𝑚 ≤ 𝑀) of X norm , we use X 0 𝑚 and X 1 𝑚 to denote attribute value set for nodes with 𝑠 𝑖 = 0 and 𝑠 𝑖 = 1 (1 ≤ 𝑖 ≤ 𝑁 ), respectively. Then we have</p><formula xml:id="formula_1">X 𝑡𝑜𝑡𝑎𝑙 = {(X 0 1 , X 1 1 ), (X 0 2 , X<label>1</label></formula><p>2 ), ..., (X 0 𝑀 , X 1 𝑀 )} for node attributes from two groups. We measure attribute bias with Wasserstein distance between the distributions of the two groups:</p><formula xml:id="formula_2">𝑏 attr = 1 𝑀 ∑︁ 𝑚 𝑊 (𝑝𝑑 𝑓 (X 0 𝑚 ), 𝑝𝑑 𝑓 (X 1 𝑚 )).<label>(1)</label></formula><p>Here 𝑝𝑑 𝑓 (•) is the probability density function for a set of values, and 𝑊 (., .) is the Wasserstein distance between two distributions. Intuitively, 𝑏 attr describes the average Wasserstein distance between attribute distributions of different groups across all dimensions, which is consistent with traditional algorithmic bias notion <ref type="bibr" target="#b14">[15]</ref>. </p><formula xml:id="formula_3">M 𝐻 = 𝛽 1 P norm + 𝛽 2 P 2 norm + ... + 𝛽 𝐻 P 𝐻 norm ,<label>(2)</label></formula><p>where 𝛽 ℎ (1 ≤ ℎ ≤ 𝐻 ) is the re-weighting parameter. The rationale behind the formulation above is to measure the aggregated reaching likelihood from each node to other nodes within a distance of 𝐻 .</p><p>To achieve localized effect for each node, a desired choice is to let 𝛽 1 ≥ 𝛽 2 ≥ ... ≥ 𝛽 𝐻 , i.e., emphasizing small-hop terms and reducing the weights of large-hop terms. For example, assume 𝐻 = 3, so the value (M 3 ) 𝑖,𝑗 is the aggregated reaching likelihood from node 𝑖 to node 𝑗 within 3 hops with re-weighting parameters being 𝛽 1 , 𝛽 2 and 𝛽 3 . Also, given attributes X norm , we define the reachability matrix R ∈ R 𝑁 ×𝑀 as R = M 𝐻 X norm . Intuitively, R 𝑖,𝑚 is the aggregated reachable attribute value for attribute 𝑚 of node 𝑖. Similar to attribute bias, we utilize R 0 𝑚 and R 1 𝑚 to represent the set of values of the 𝑚-th dimension in R for nodes with 𝑠 𝑖 = 0 and 𝑠 𝑖 = 1 (1 ≤ 𝑖 ≤ 𝑁 ), respectively. Then, the entries in R can also be divided into tuples according to attribute dimensions:</p><formula xml:id="formula_4">R 𝑡𝑜𝑡𝑎𝑙 = {(R 0 1 , R 1 1 ), (R 0 2 , R 1 2 ), ..., (R 0 𝑀 , R 1 𝑀 )}.</formula><p>We define structural bias as:</p><formula xml:id="formula_5">𝑏 stru = 1 𝑀 ∑︁ 𝑚 𝑊 (𝑝𝑑 𝑓 (R 0 𝑚 ), 𝑝𝑑 𝑓 (R 1 𝑚 )).<label>(3)</label></formula><p>Here 𝑏 stru is defined in a similar way as 𝑏 attr , except that the former uses R 0 𝑚 and R 1 𝑚 instead of X 0 𝑚 and X 1 𝑚 . In this way, structural bias 𝑏 stru describes the average difference between aggregated attribute distributions of different groups after several rounds of propagation, which also agrees with traditional algorithmic bias notion <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Problem Statement</head><p>Based on the definitions and metrics in Section 3.2 and Section 3.3, we argue that if both 𝑏 𝑎𝑡𝑡𝑟 and 𝑏 𝑠𝑡𝑟𝑢 are reduced, bias in an attributed network can be mitigated. As a result, if GNNs are trained on such data, the bias issues in downstream tasks could also be alleviated. Formally, we define the debiasing problem as follows.</p><p>Problem 1. Debiasing attributed networks for GNNs. Given an attributed network G = (A, X), our goal is to debias G by reducing 𝑏 𝑎𝑡𝑡𝑟 and 𝑏 𝑠𝑡𝑟𝑢 to obtain G = ( Ã, X), so that the bias of GNNs trained on G is also mitigated. The debiasing process is indepdent of detailed architecture and parameterization of GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MITIGATING DATA BIAS FOR GNNS</head><p>In this section, we discuss how to tackle Problem 1 with our proposed framework EDITS. We first present an overview of EDITS, followed by the formulation of the objective function. Finally, we present the optimization process for the objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Framework Overview</head><p>An overview of the proposed framework EDITS is shown in Fig. <ref type="bibr" target="#b1">(2)</ref>. Specifically, EDITS consists of three learnable modules. Gradient descent based optimization is executed alternatively in training.</p><p>• Attribute debiasing. This module learns a debiasing function 𝑔 𝜽 with learnable parameter 𝜽 ∈ R 𝑀 . The debiased version of X is obtained as output where X = 𝑔 𝜽 (X). • Structural debiasing. This module outputs Ã as the debiased A. Specifically, Ã is initialized with A at the beginning of the optimization process. The entries in Ã are optimized via gradient descent with clipping and binarization.</p><p>• Wasserstein distance approximator. This module learns a function 𝑓 for each attribute dimension. Here 𝑓 is utilized to estimate the Wasserstein distance between the distributions of different groups for any attribute dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Objective Function</head><p>In this subsection, we introduce the details of our framework. Following the Definition 1 and Definition 2, our goal is to reduce 𝑏 attr and 𝑏 stru simultaneously. For the ease of understanding, we first only consider the 𝑚-th attribute dimension as an example, and then extend it to all 𝑀 dimensions to obtain our final objective. Let 𝑥 0,𝑚 ∼ 𝑃 0,𝑚 and 𝑥 1,𝑚 ∼ 𝑃 1,𝑚 be the random variables that represent the value of the 𝑚-th attribute dimension in X for nodes with sensitive attribute 𝑠 = 0 and 𝑠 = 1, respectively. Assume that we have a function 𝑔 𝜃 𝑚 : R → R (1 ≤ 𝑚 ≤ 𝑀) to mitigate attribute bias. For the 𝑚-th dimension, we denote 𝑥 (0)</p><formula xml:id="formula_6">0,𝑚 = 𝑔 𝜃 𝑚 (𝑥 0,𝑚 ) ∼ 𝑃 (0) 0,𝑚 and 𝑥 (0) 1,𝑚 = 𝑔 𝜃 𝑚 (𝑥 1,𝑚 ) ∼ 𝑃 (0)</formula><p>1,𝑚 as the debiasing results for 𝑥 0,𝑚 and 𝑥 1,𝑚 , respectively. Here the superscript (0) indicates that no information propagation is performed in the debaising process. Correspondingly, when such operation is extended to all 𝑀 dimensions, we will have the debiased attribute matrix X. Apart from the goal of mitigating attribute bias, we also want to mitigate structural</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 The Optimization Algorithm for EDITS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input:</head><p>A: Adjacency matrix; X: Attribute matrix; 𝛼, 𝜇 1 to 𝜇 4 : Hyperparameters in objectives; 𝑐: Threshold enforcing lipschitz; 𝑧: Threshold for attribute masking; 𝑟 : Threshold factor for adjacency matrix binarizing; Output:</p><p>Debiased adjacency matrix Ã and attribute matrix X;</p><formula xml:id="formula_7">1: Ã ← A; 𝚯 ← I; 2: while epoch ≤ epoch_max do 3:</formula><p>Compute L 1 following Eq. ( <ref type="formula" target="#formula_14">9</ref>);  Update 𝚯 by PGD following Eq. ( <ref type="formula" target="#formula_16">11</ref>), X ← X𝚯; Update Ã by PGD following Eq. ( <ref type="formula" target="#formula_17">12</ref>), Ã ← 1 2 ( Ã + Ã⊤ ); 8: end while 9: Mask the 𝑧 smallest entries with 0 in 𝑑𝑖𝑎𝑔 (𝚯), X ← X𝚯; 10: Binarize Ã w.r.t. the threshold 𝑟 ; 11: return Ã and X;</p><p>bias. Let Ã be the adjacency matrix from the debiased network structure, and Pnorm denotes the normalized Ã with re-weighted self-loops. Information propagation with ℎ hops using the debiased adjacency matrix could be expressed as Pℎ norm X, where</p><formula xml:id="formula_8">1 ≤ ℎ ≤ 𝐻 . Let 𝑥 (ℎ) 0,𝑚 ∼ 𝑃 (ℎ) 0,𝑚 and 𝑥 (ℎ) 1,𝑚 ∼ 𝑃 (ℎ)</formula><p>1,𝑚 be the random variables of the value at the 𝑚-th column of Pℎ norm X for nodes with sensitive attribute 𝑠 = 0 and 𝑠 = 1, respectively. We hope that Ã could mitigate structural bias. In this work, we combine attribute debiasing and structure debiasing as below.</p><p>Based on the random variables 𝑥  </p><p>where 𝑊 (𝑃 𝐽 𝑜𝑖𝑛𝑡 0,𝑚 , 𝑃 𝐽 𝑜𝑖𝑛𝑡 1,𝑚 ) can be expressed as</p><formula xml:id="formula_10">𝑊 (𝑃 𝐽 𝑜𝑖𝑛𝑡 0,𝑚 ,𝑃 𝐽 𝑜𝑖𝑛𝑡 1,𝑚 ) = (5) inf 𝛾 ∈Π (𝑃 𝐽 𝑜𝑖𝑛𝑡 0,𝑚 ,𝑃 𝐽 𝑜𝑖𝑛𝑡 1,𝑚 ) E (x 0,𝑚 ,x 1,𝑚 )∼𝛾 [∥x 0,𝑚 − x 1,𝑚 ∥ 1 ].</formula><p>Here Π(𝑃 𝐽 𝑜𝑖𝑛𝑡 0,𝑚 , 𝑃 𝐽 𝑜𝑖𝑛𝑡 1,𝑚 ) represents the set of all joint distributions 𝛾 (x 0,𝑚 , x 1,𝑚 ) whose marginals are 𝑃 𝐽 𝑜𝑖𝑛𝑡 0,𝑚 and 𝑃 𝐽 𝑜𝑖𝑛𝑡 1,𝑚 , respectively. After considering all the 𝑀 dimensions, the overall objective is</p><formula xml:id="formula_11">min 𝜽, Ã 1 𝑀 ∑︁ 1≤𝑚 ≤𝑀 𝑊 (𝑃 𝐽 𝑜𝑖𝑛𝑡 0,𝑚 , 𝑃 𝐽 𝑜𝑖𝑛𝑡 1,𝑚 ).<label>(6)</label></formula><p>It is non-trivial to optimize Eq. ( <ref type="formula" target="#formula_11">6</ref>) as the infimum is intractable. Therefore, in the next subsection, we show how to convert it into a tractable optimization problem through approximation, which enables end-to-end gradient-based optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Framework Optimization</head><p>In this subsection, we introduce our optimization algorithm. For simplicity, first we still use the 𝑚-th attribute dimension in X to illustrate the idea. Considering the infimum in Wasserstein distance computation is intractable, we apply the Kantorovich-Rubinstein duality <ref type="bibr" target="#b55">[56]</ref> to convert the problem of Eq. ( <ref type="formula">5</ref>) as:</p><formula xml:id="formula_12">𝑊 (𝑃 𝐽 𝑜𝑖𝑛𝑡 0,𝑚 , 𝑃 𝐽 𝑜𝑖𝑛𝑡 1,𝑚 ) = (7) sup ∥𝑓 ∥ 𝐿 ≤1 E x 0,𝑚 ∼𝑃 𝐽 𝑜𝑖𝑛𝑡 0,𝑚 [𝑓 (x 0,𝑚 )] − E x 1,𝑚 ∼𝑃 𝐽 𝑜𝑖𝑛𝑡 1,𝑚 [𝑓 (x 1,𝑚 )].</formula><p>Here ∥𝑓 ∥ 𝐿 ≤ 1 denotes that the supremum is taken over all 1-Lipschitz functions 𝑓 : R 𝐻 +1 → R. The problem can be solved by learning a neural network as 𝑓 . Nevertheless, it is worth noting that the 1-Lipschitz function is difficult to obtain during optimization. Therefore, here we relax ∥𝑓 ∥ 𝐿 ≤ 1 to ∥𝑓 ∥ 𝐿 ≤ 𝑘 (𝑘 is a constant). In this case, the left side of Eq. ( <ref type="formula">7</ref>) also changes to 𝑘𝑊 (𝑃 𝐽 𝑜𝑖𝑛𝑡 0,𝑚 , 𝑃 𝐽 𝑜𝑖𝑛𝑡 1,𝑚 ). Then, the Wasserstein distance between 𝑃 𝐽 𝑜𝑖𝑛𝑡 0,𝑚 and 𝑃 𝐽 𝑜𝑖𝑛𝑡 1,𝑚 up to a multiplicative constant can be attained via: max</p><formula xml:id="formula_13">𝑓 𝑚 ∈ F E x 0,𝑚 ∼𝑃 𝐽 𝑜𝑖𝑛𝑡 0,𝑚 [𝑓 𝑚 (x 0,𝑚 )] − E x 1,𝑚 ∼𝑃 𝐽 𝑜𝑖𝑛𝑡 1,𝑚 [𝑓 𝑚 (x 1,𝑚 )],<label>(8)</label></formula><p>where F denotes the set of all 𝑘-Lipschitz functions (i.e., ∥𝑓 𝑚 ∥ 𝐿 ≤ 𝑘, 𝑓 𝑚 ∈ F ). Then, extending Eq. ( <ref type="formula" target="#formula_13">8</ref>) to all 𝑀 dimensions leads to our final objective function as:</p><formula xml:id="formula_14">L 1 = ∑︁ 1≤𝑚 ≤𝑀 {E x 0,𝑚 ∼𝑃 𝐽 𝑜𝑖𝑛𝑡 0,𝑚 [𝑓 𝑚 (x 0,𝑚 )] − E x 1,𝑚 ∼𝑃 𝐽 𝑜𝑖𝑛𝑡 1,𝑚 [𝑓 𝑚 (x 1,𝑚 )]},<label>(9)</label></formula><p>where {𝑓 𝑚 : 1 ≤ 𝑚 ≤ 𝑀 } ⊂ F . To model the function 𝑓 in Eq. ( <ref type="formula" target="#formula_14">9</ref>), we use a single-layered neural network for the approximation of each 𝑓 𝑚 (1 ≤ 𝑚 ≤ 𝑀). Therefore, the goal of Wasserstein distance approximators can be formulated as:</p><formula xml:id="formula_15">max {𝑓 𝑚 :1≤𝑚 ≤𝑀 } ⊂ F L 1 .<label>(10)</label></formula><p>The weights of neural networks are clipped within [−𝑐, 𝑐] (𝑐 is a pre-defined constant), which has been proved to be a simple but effective way to enforce the Lipschitz constraint for every 𝑓 𝑚 <ref type="bibr" target="#b2">[3]</ref>. For attribute debiasing 𝑔 𝜃 , we choose a linear function, i.e., 𝑔 𝜃 𝑚 (𝑥 𝑠,𝑚 ) = 𝜃 𝑚 𝑥 𝑠,𝑚 (𝑠 ∈ {0, 1}). One advantage is that it acts as the role of feature re-weighting by assigning a feature weight for each attribute, which enables better interpretability for the debiased result. In matrix form, assume 𝚯 is a diagonal matrix with the 𝑚-th diagonal entry being 𝜃 𝑚 , we have X = 𝑔 𝜽 (X) = X𝚯. Then the optimization goal for attribute debiasing is:</p><formula xml:id="formula_16">min 𝚯 L 1 + 𝜇 1 ∥ X − X∥ 2 𝐹 + 𝜇 2 ∥𝚯∥ 1 ,<label>(11)</label></formula><p>where 𝜇 1 and 𝜇 2 are hyper-parameters. The second term ensures the debiased attribute after feature re-weighting is close to the original attributes (i.e., preserve as much information as possible).</p><p>The third term controls the sparsity of re-weighting parameters. For structural debiasing, Ã is optimized with the objective as:</p><formula xml:id="formula_17">min Ã L 1 + 𝜇 3 ∥ Ã − A∥ 2 𝐹 + 𝜇 4 ∥ Ã∥ 1 𝑠.𝑡 ., Ã = Ã⊤ . (<label>12</label></formula><formula xml:id="formula_18">)</formula><p>where 𝜇 3 and 𝜇 4 are hyper-parameters. The second term ensures the debiased result Ã is close to the original topology A. The third term enforces the debiased network topology is also sparse, which is aligned with the characteristics of real-world networks. To optimize 𝑓 , 𝚯 and Ã, we propose a gradient-based optimization approach for alternatively training as Algorithm 1. Stochastic Gradient Descent (SGD) is utilized for the optimization of Eq. ( <ref type="formula" target="#formula_15">10</ref>), while Proximal Gradient Descent (PGD) is adopted for solving Eq. ( <ref type="formula" target="#formula_16">11</ref>) and Eq. ( <ref type="formula" target="#formula_17">12</ref>), respectively. For the projection operation in PGD, we clip both the parameters in 𝚯 and Ã within [0, 1]. To remove the most biased attribute channels, the 𝑧 smallest weights in the diagonal of 𝚯 are masked with 0, where 𝑧 is a pre-assigned hyper-parameter for attribute debiasing. Besides, we binarize the debiased output Ã as follows. For the entries in Ã that are larger than the corresponding entries in A, we first compute the maximum improvement of the entry pairs in the corresponding positions between Ã and A. Then we re-weight such maximum difference by a factor of 𝑟 , which is a pre-assigned hyper-parameter for structural debiasing. All entries that are improved by the value larger than the re-weighted difference are directly assigned as 1 in Ã, and those do not exceed the re-weighted difference are assigned as 0. For the entries in Ã that are smaller than the corresponding entries in A, we also compute a similar re-weighted maximum decreasing difference, and assign the deceasing entries that exceed this value and those do not as 0 and 1, respectively. To summarize the basic rationale of binarizing Ã, this operation aims to directly flip (i.e., 0 to 1 or 1 to 0) the entries with the largest changes after optimization compared with A, and maintain other entries as their original values. Finally, Algorithm 1 outputs X after the dimension with 𝑧 smallest 𝜃 𝑚 being masked, and Ã after being binarized w.r.t. the threshold factor 𝑟 after multiple epochs of optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">THEORETICAL ANALYSIS</head><p>Here we present theoretical analysis to gain a deeper understanding of debiasing attributed networks. Specifically, we focus on bridging the gap between attribute bias and structural bias from the perspective of Spectral Graph Theory <ref type="bibr" target="#b9">[10]</ref>. Usually, an undirected attributed network is regarded as a signal composed of different frequency components in Graph Signal Processing (GSP) <ref type="bibr" target="#b46">[47]</ref>. An operation preserving lower frequency components more can be regarded as low-pass filtering the input graph signal <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b46">47]</ref>. Theorem 1. Let 𝜆 max be the largest eigenvalue of L norm . Multiplying X by the propagation matrix M 𝐻 can be regarded as low-pass filtering X when 𝛼 = 1 𝜆 𝑚𝑎𝑥 and 𝛽 𝑖 &gt; 0 (1 ≤ 𝑖 ≤ 𝐻 ).</p><p>Proof. Without loss of generality, we present the proof based on Laplacian graph spectrum. By replacing 𝛼 with 1 𝜆 max , we have</p><formula xml:id="formula_19">P norm = 1 𝜆 max A norm + (1 − 1 𝜆 max )I = I − L norm 𝜆 max . (<label>13</label></formula><formula xml:id="formula_20">)</formula><p>Then, by combining Eq. ( <ref type="formula" target="#formula_3">2</ref>) and Eq. ( <ref type="formula" target="#formula_19">13</ref>), we get</p><formula xml:id="formula_21">M 𝐻 = 𝛽 1 (I − L norm 𝜆 max ) + 𝛽 2 (I − L norm 𝜆 max ) 2 + ... + 𝛽 𝐻 (I − L norm 𝜆 max ) 𝐻 . (<label>14</label></formula><formula xml:id="formula_22">)</formula><p>Considering that L norm is a symmetric real matrix, it can be decomposed as L norm = UΛU ⊤ , then Eq. ( <ref type="formula" target="#formula_21">14</ref>) can be rewritten as</p><formula xml:id="formula_23">M 𝐻 = U 𝛽 1 (I − Λ 𝜆 max ) + 𝛽 2 (I − Λ 𝜆 max ) 2 + ... + 𝛽 𝐻 (I − Λ 𝜆 max ) 𝐻 U ⊤ . (<label>15</label></formula><formula xml:id="formula_24">)</formula><p>Here Λ is the diagonal eigenvalue matrix of L norm , and the ℎ-th term (1 ≤ ℎ ≤ 𝐻 ) in Eq. ( <ref type="formula" target="#formula_23">15</ref>) indicates a frequency response function 𝜆 max and all 𝛽 𝑖 &gt; 0. □ Based on Theorem 1, we propose the corollary below to build connections between attribute bias and structural bias.</p><p>Corollary 1. The attribute bias contained in the low frequency components of an attributed network is equivalent to structural bias.</p><p>Considering that structural bias is defined based on the low-pass filtered attribute bias (according to Theorem 1), the corollary is selfevident. At the same time, considering that the frequencies and the corresponding basis of a network data changes when A is optimized to be Ã <ref type="bibr" target="#b9">[10]</ref>, the basic goal of EDITS can also be interpreted as: debiasing the full spectrum of a graph signal, and learning better frequencies together with the corresponding basis to further mitigate the bias existed in the lower frequency components of the graph signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTAL EVALUATIONS</head><p>We conduct experiments on both real-world and synthetic datasets to evaluate the effectiveness of EDITS. In particular, we answer the following research questions. RQ1: How well can EDITS mitigate the bias in attributed networks together with the outcome of different GNN variants for the downstream task? RQ2: How well can EDITS balance utility maximization and bias mitigation compared with other debiasing baselines tailored for a specific GNN?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Downstream Task and Datasets</head><p>Downstream Task. Here we choose the widely adopted node classification task to assess the effectiveness of our proposed framework. Datasets. We use two types of datasets in our experiments, including three real-world datasets and two synthetic datasets. Statistics of the real-world datasets are shown in Table <ref type="table" target="#tab_1">1</ref>. We present the details of the two kinds of datasets as follows.</p><p>• Real-world Datasets. We include three real-world datasets, namely German Credit network, Recidivism network, and Credit Defaulter network <ref type="bibr" target="#b1">[2]</ref>. Specifically, in German Credit, nodes represent clients in a German bank, and edges are formed between clients if their credit accounts are similar.</p><p>With "gender" being the sensitive attribute, the node classification task here aims to classify the credit risk of the clients as high or low. In Recidivism, nodes are defendants released on bail during 1990-2009. Nodes are connected based on the similarity of past criminal records and demographics. The task is to classify defendants into bail vs. no bail with "race" (black and white) being the sensitive attribute. In the Credit Defaulter, nodes are credit card users and they are connected based on the pattern similarity of their purchases and payments. Here "age" is the sensitive attribute, and the task is to predict whether a user will default on credit card payment. • Synthetic Datasets. To carry out ablation study on the effectiveness of each module in EDITS, we also use two datasets created similarly as those in Section 2. One network has biased attributes and unbiased structure, while the other network is on the opposite. We add eight extra attribute dimensions besides the two attribute dimensions for both datasets. The attribute value in the extra attribute dimensions are generated uniformly between 0 and 1. For label generation, we compute the sum of the first two extra attribute dimensions. Then, we add noises to the sum values following N (0, 0.1 2 ), and rank them by the values in a descending order. The labels of the top-ranked 50% individuals are set as 1, while the labels of the other 50% are set as 0. The goal of the downstream task is to predict the labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experimental settings</head><p>GNN Models. To investigate whether the debiased result of EDITS can make outcomes of different GNNs less biased, we adopt three popular GNN variants to make predictions for the downstream task in our experiments: GCN <ref type="bibr" target="#b28">[29]</ref>, GraphSAGE <ref type="bibr" target="#b19">[20]</ref> and GIN <ref type="bibr" target="#b59">[60]</ref>.</p><p>Baselines. Since there is no existing work directly debiasing attributed network in a model-agnostic way, here we choose two state-of-the-art GNN-based debiasing approaches for comparison, namely FairGNN <ref type="bibr" target="#b11">[12]</ref> and NIFTY <ref type="bibr" target="#b1">[2]</ref>. (1) FairGNN. It is a debiasing method based on adversarial training. A discriminator is trained to distinguish the representations between demographic groups. The goal of FairGNN is to train a GNN that fools the discriminator for bias mitigation; (2) NIFTY. It is a recently proposed GNN-based debiasing framework. With counterfactual perturbation on the sensitive attribute, bias is mitigated via learning node representations that are invariant to the sensitive attribute. It should be noted that both of them take GNNs in the downstream task as their backbones.</p><p>While on the other hand, EDITS attempts at debiasing attributed network without referring to the output of downstream GNN models (i.e., EDITS is model-agnostic). The hyper-parameters of EDITS are tuned only based on our proposed bias metrics applied on attributed networks. Therefore, our studied problem is much more difficult than the model-oriented debiasing of the two baselines. Evaluation Metrics. We evaluate model performance from two perspectives: model utility and bias mitigation. A good performance means low bias and high model utility. We introduce the adopted metrics for model utility and bias mitigation: (1) Model Utility Metrics. For node classification, we use the area under receiver operating characteristic curve (AUC) and F1 score as the indicator of model utility;</p><p>(2) Bias Mitigation Metrics. We use two widely-adopted metrics to show to what extent the bias in the output of different GNNs are mitigated <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b37">38]</ref>. For both metrics, a lower value means better bias mitigation performance. The metric measuring bias from </p><formula xml:id="formula_25">Δ 𝑆𝑃 = |𝑃 (ŷ = 1|𝑠 = 0) − 𝑃 (ŷ = 1|𝑠 = 1)|.<label>(16)</label></formula><p>The metric from the perspective of Equality of Opportunity is</p><formula xml:id="formula_26">Δ 𝐸𝑂 = |𝑃 (ŷ = 1|𝑦 = 1, 𝑠 = 0) − 𝑃 (ŷ = 1|𝑦 = 1, 𝑠 = 1)|.<label>(17)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Implementation Details</head><p>EDITS is implemented using Pytorch <ref type="bibr" target="#b43">[44]</ref> and optimized via RM-Sprop optimizer <ref type="bibr" target="#b22">[23]</ref>. For GNN models (i.e., GCN, GraphSAGE and GIN) and baseline methods (i.e., FairGNN and NIFTY) in our experiments, we use their released Pytorch implementations for a fair comparison. In the training of EDITS, we set the training epochs as 100 for Recidivism and 500 for other datasets. The learning rate is set as 3 × 10 −3 for epochs under 400 and 1 × 10 −3 for those above.</p><p>𝛼 is set as 0.5 considering that 𝜆 max = 2 <ref type="bibr" target="#b9">[10]</ref>. For a more stable optimization, the three modules in EDITS are updated alternatively. The attributed networks output by EDITS are used for GNN training to evaluate the performance of EDITS, while the performance of the Vanilla method is obtained by feeding GNNs with the original networks. To train GNNs, we fix the training epochs to be 1,000, with the learning rate of 1 × 10 −3 . Dropout rate and hidden channel number is set as 0.05 and 16, respectively. All GNNs and baseline methods are optimized with Adam optimizer <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Debiasing Attributed Network for GNNs</head><p>To answer RQ1, we first evaluate the effectiveness of EDITS on reducing the bias measured by the two proposed metrics and traditional bias metrics with different GNN backbones. The attribute and structural bias of the three real-world datasets before and after being debiased by EDITS are shown in Table <ref type="table" target="#tab_3">2</ref>. The quantitative comparison on Δ 𝑆𝑃 and Δ 𝐸𝑂 between GNNs trained on debiased networks from EDITS and original networks is also presented in Table <ref type="table" target="#tab_4">3</ref>. We make the following observations from the two Tables:</p><p>• From the perspective of bias mitigation in the attributed network (i.e., attribute bias and structural bias), EDITS exhibits significant advantages over the Vanilla approach as indicated by Table <ref type="table" target="#tab_3">2</ref>. This verifies the effectiveness of EDITS on reducing the bias existing in the attributed network data. • From the perspective of bias mitigation in the downstream task (i.e., Δ 𝑆𝑃 and Δ 𝐸𝑂 ), we observe from Table <ref type="table" target="#tab_4">3</ref> that EDITS achieves desirable bias mitigation performance with little utility sacrifice in all cases compared with GNNs with original network as input (i.e., the vanilla one). This verifies that attributed networks debiased by EDITS can generally mitigate the bias in the outcome of different GNNs. • When comparing bias mitigation performance indicated by Table <ref type="table" target="#tab_3">2</ref> and Table <ref type="table" target="#tab_4">3</ref>, we can find that the bias in the outcome of GNNs is also mitigated after EDITS mitigates attribute bias and structural bias in the attributed networks. Such consistency verifies the validity of our proposed metrics on measuring the bias existed in the attributed network data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Comparison with Other Debiasing Models</head><p>To answer RQ2, we then compare the balance between model utility and bias mitigation with other baselines on a given GNN. Here we present the comparison of AUC and Δ 𝑆𝑃 based on GCN in Fig. <ref type="bibr" target="#b2">(3)</ref>. Similar results can be obtained for other GNNs, which are omitted due to space limit. The following observations can be made:</p><p>• From the perspective of model utility (indicated by Fig. <ref type="figure" target="#fig_7">(3a)</ref>), EDITS and the baselines methods achieve comparable results with the vanilla GCN. This implies that the debiasing process of EDITS preserves as much useful information for the downstream task as the original attributed network. • From the perspective of bias mitigation (indicated by Fig. <ref type="figure" target="#fig_7">(3b)</ref>), all baselines achieve effective bias mitigation. For debiasing in the downstream task, NIFTY performs better than FairGNN. Compared with debiasing in the downstream task, debiasing the attributed network is more difficult due to the lack of supervision signals from the downstream task. Interestingly, EDITS performs better than both two baselines on bias mitigation. This verifies the superior performance of EDITS on debiasing the attributed network data. • From the perspective of balancing the model utility and bias mitigation, EDITS achieves comparable model utility with vanilla GNNs and other baselines, but exhibits better bias mitigation performance. Consequently, we argue that EDITS achieves superior performance on balancing the model utility and bias mitigation compared with other baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Ablation Study</head><p>To evaluate the effectiveness of the two debiasing modules (i.e., attribute debiasing module and structural debiasing module) in EDITS, here we investigate how each of them individually contributes to bias mitigation under our proposed bias metrics and the bias metrics in the downstream task. We choose GCN as the GNN model in our downstream task. For better visualization purpose, the two datasets showing large attribute bias and structural bias (i.e., German and Credit) are selected for experiments. Besides, to better demonstrate the functionality of the two debiasing modules, we also adopt the two synthetic datasets we mentioned in Section 2 (i.e., the networks with either only biased attributes or only biased structure), which are further modified according to Section 6.1. Based on the four selected datasets, four different variants of EDITS are tested, namely EDITS with both debiasing modules, EDITS without the structural debiasing module (i.e., *w/o-SD), EDITS without the attribute debiasing module (i.e., *w/o-AD), vanilla GCN model without debiased input (i.e., Vanilla). We present their performance of attribute bias, structural bias, AUC and Δ 𝑆𝑃 on the four datasets in Fig. <ref type="bibr" target="#b3">(4)</ref>. We make the following observations:</p><p>• The value of attribute bias can be reduced with attribute debiasing module of EDITS, which maintains the model utility (i.e., AUC) but reduces Δ 𝑆𝑃 in the downstream task. • The value of structural bias can be reduced with both attribute debiasing and structural debiasing module. With only structural debiasing, EDITS still maintains comparable model utility but reduces Δ 𝑆𝑃 in the downstream task. • Although both attribute debiasing and structural debiasing module help mitigate structural bias, only debiasing the network structure achieves better bias mitigation performance on all four datasets compared with only debiasing the attributes as implied by Fig. <ref type="figure" target="#fig_8">(4d</ref>). This demonstrates the indispensability of the structural debiasing module in EDITS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK</head><p>Mitigating Bias in Machine Learning. Bias can be defined differently in machine learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b57">58]</ref>. Commonly used bias notions can be broadly categorized into group fairness and individual fairness <ref type="bibr" target="#b15">[16]</ref>. Group fairness emphasizes that algorithms should not yield discriminatory outcomes for any specific demographic group <ref type="bibr" target="#b15">[16]</ref>. Such group are usually determined by attributes that are reluctant or illegal to be shared (e.g., gender or race) <ref type="bibr" target="#b25">[26]</ref>, i.e., sensitive attributes. Existing debiasing approaches work in any of the three data flow stages, i.e., pre-processing, processing and post-processing stage. In pre-processing stage, a common method is to re-weight training samples from different groups to mitigate bias before model training <ref type="bibr" target="#b25">[26]</ref>. Perturbing data distributions between groups is another popular approach to debias the data in the pre-processing stage <ref type="bibr" target="#b56">[57]</ref>. In processing stage, a popular method is to add regularization terms to disentangle the outcome from sensitive attribute <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b49">50]</ref> or minimize the outcome difference between groups <ref type="bibr" target="#b0">[1]</ref>. Besides, utilizing adversarial learning to remove sensitive information from representations is also widely adopted <ref type="bibr" target="#b17">[18]</ref>. In post-processing stage, bias in outcomes is usually mitigated by constraining the outcome to follow a less biased distribution <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b62">63]</ref>. Usually, all above-mentioned approaches are evaluated via measuring how much certain fairness notion is violated, e.g., Statistical Parity <ref type="bibr" target="#b15">[16]</ref>, Equality of Opportunity, Equality of Odds <ref type="bibr" target="#b20">[21]</ref> and Counterfactual Fairness <ref type="bibr" target="#b30">[31]</ref>. Different from group fairness, individual fairness focuses on treating similar individuals similarly <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b60">61]</ref>. The similarity can be given by oracle similarity scores from specialists <ref type="bibr" target="#b31">[32]</ref>. Most existing debiasing methods based on individual fairness work in the processing stage. For example, constraint can enforce similar predictions between similar instances <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b31">32]</ref>, and Consistency is a popular evaluation metric <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>Mitigating Bias in Graph Mining. Efforts have been made to mitigate bias in graph mining algorithms, where these works can be broadly categorized into either focusing on group fairness or individual fairness. For group fairness, adversarial learning can be adopted to learn less biased representations that fool the discriminator <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11]</ref>. Rebalancing between groups is also a popular approach to mitigate bias <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b52">53]</ref>. For example, Rahman et al. mitigate bias via rebalancing the appearance rate of minority groups in random walks <ref type="bibr" target="#b47">[48]</ref>. Projecting the embeddings onto a hyperplane orthogonal to the hyperplane of sensitive attributes is another approach for bias mitigation based on group fairness. Compared with other methods, it theoretically guarantees that embeddings are disentangled from sensitive attributes <ref type="bibr" target="#b41">[42]</ref>. Compared with mitigating bias focusing on group fairness, only few work promotes individual fairness in graphs. To the best of our knowledge, Kang et al. first propose to systematically debias multiple graph mining algorithms based on individual fairness <ref type="bibr" target="#b26">[27]</ref>. Different from all previous debiasing approaches, this paper proposes to directly debias the attributed network data in a model-agnostic way based on the analysis of information propagation mechanism in GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>GNNs are playing increasingly important roles in many practical scenarios. Nevertheless, there is an increasing societal concern that GNNs could yield discriminatory decisions towards certain demographic groups. Such discrimination can be harmful and even illegal under certain circumstances. Existing debiasing approaches are mainly tailored for a specific GNN. Adapting these methods to different GNNs can be costly, as they need to be trained and fine-tuned for different GNNs. Different from them, in this paper, we propose to directly debias the attributed network data for GNNs. With preliminary analysis on the source of bias and interaction between bias in different data modalities, we formally define two kinds of bias together with their corresponding metrics, and formulate a novel problem of debiasing attributed network for GNNs.</p><p>To tackle this problem, we then propose a principled framework named EDITS for model-agnostic debiasing. Experimental results demonstrate that EDITS exhibits superior performance on mitigating the bias and maintaining the model utility in the downstream task compared with other state-of-the-art approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Two exemplary cases illustrating how bias in two data modalities affect each other in GNN information propagation. Here (c) is the attribute distribution after propagation with biased attributes (a) and unbiased structure (b); while (f) is the attribute distribution after propagation with unbiased attributes (d) and biased structure (e).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Definition 2 .</head><label>2</label><figDesc>Structural bias. Given an undirected attributed network G = (A, X) and the corresponding group indicator (w.r.t. sensitive attribute) for each node s = [𝑠 1 , 𝑠 2 , ..., 𝑠 𝑁 ], where 𝑠 𝑖 ∈ {0, 1} (1 ≤ 𝑖 ≤ 𝑁 ). If any information propagation w.r.t. A promotes the distribution difference between different groups at any attribute dimension, then structural bias exists in G.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>WFigure 2 :</head><label>2</label><figDesc>Figure 2: An illustration of the proposed framework EDITS with 𝐻 = 2 as an example. Structural bias metric. As illustrated in Section 2, the key mechanism of GNNs is information propagation, where structural bias could exhibit. Let P norm = 𝛼A norm + (1 − 𝛼)I. Here P norm can be regarded as a normalized adjacency matrix with re-weighted self-loops, where 𝛼 ∈ [0, 1] is a hyper-parameter. Before measuring structural bias, we define the propagation matrix M 𝐻 ∈ R 𝑁 ×𝑁 as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4 : 5 :</head><label>45</label><figDesc>Update the weights of 𝑓 𝑚 by SGD following Eq. (10); Clip the weights of 𝑓 within [-𝑐, 𝑐];</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>1 ,</head><label>1</label><figDesc>𝑚 , we have the (𝐻 + 1)-dimensional vectors x 0,𝑚 = [𝑥</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance comparison between EDITS and baselines on utility (AUC) and bias mitigation (𝚫 𝑺𝑷 ) on GCN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance EDITS and its variants on two real-world datasets and two synthetic datasets. EDITS denotes that both debiasing modules are included; *w/o-SD means EDITS without sturctural debiasing module; *w/o-AD means EDITS is without attribute debiasing module; Vanilla means applying GNN with the original attributed network as input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>1 ,</head><label>1</label><figDesc>𝑚 ] following the joint distribution 𝑃 𝐽 𝑜𝑖𝑛𝑡 0,𝑚 and 𝑃 𝐽 𝑜𝑖𝑛𝑡 1,𝑚 , respectively. To reduce both 𝑏 attr and 𝑏 stru at the 𝑚-th dimension, our goal is to minimize the Wasserstein distance between 𝑃 𝐽 𝑜𝑖𝑛𝑡 0,𝑚 and 𝑃 𝐽 𝑜𝑖𝑛𝑡 1,𝑚 :</figDesc><table><row><cell>min</cell></row><row><cell>𝜃 𝑚 ,</cell></row></table><note>Ã𝑊 (𝑃 𝐽 𝑜𝑖𝑛𝑡 0,𝑚 , 𝑃 𝐽 𝑜𝑖𝑛𝑡 1,𝑚 ),</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>The statistics and basic information about the three real-world datasets adopted for experimental evaluation. . For any 𝜆 𝑖 (1 ≤ 𝑖 ≤ 𝑁 ), 𝜆 𝑖 𝜆 𝑖 . This indicates that, for each term, when it is multiplied by a graph signal, the higher frequency components of the graph signal are more weakened compared with the lower frequency components. In conclusion, multiplying the propagation matrix M 𝐻 with any graph signal equals to the operation of lowpass filtering when 𝛼 = 1</figDesc><table><row><cell>Dataset</cell><cell></cell><cell cols="3">German Credit Recidivism Credit Defaulter</cell></row><row><cell># Nodes</cell><cell></cell><cell>1,000</cell><cell>18,876</cell><cell>30,000</cell></row><row><cell># Edges</cell><cell></cell><cell>22,242</cell><cell>321,308</cell><cell>1,436,858</cell></row><row><cell cols="2"># Attributes</cell><cell>27</cell><cell>18</cell><cell>13</cell></row><row><cell cols="2">Avg. degree</cell><cell>44.48</cell><cell>34.04</cell><cell>95.79</cell></row><row><cell>Sens.</cell><cell></cell><cell>Gender</cell><cell>Race</cell><cell>Age</cell></row><row><cell>Label</cell><cell></cell><cell>good / bad</cell><cell cols="2">bail / no bail default / no default</cell></row><row><cell>of (1 − 𝜆 i 𝜆 max</cell><cell cols="3">) 𝜆 max</cell><cell>≤ 1 holds. Therefore,</cell></row><row><cell cols="5">the frequency response of every term in Eq. (15) monotonically</cell></row><row><cell cols="2">decreases w.r.t.</cell><cell></cell><cell></cell></row></table><note>ℎ </note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Attribute and structural bias comparison between original networks and debiased ones from EDITS. Lower value for both metrics indicates better bias mitigation. All values are in scale of ×10 −3 . Best ones are marked in bold.</figDesc><table><row><cell></cell><cell cols="2">Attribute Bias</cell><cell cols="2">Structural Bias</cell></row><row><cell></cell><cell>Vanilla</cell><cell>EDITS</cell><cell>Vanilla</cell><cell>EDITS</cell></row><row><cell>German</cell><cell>6.33</cell><cell>2.38 (−62.4%)</cell><cell>10.4</cell><cell>3.54 (−66.0%)</cell></row><row><cell>Credit</cell><cell>2.46</cell><cell>0.56 (−77.2%)</cell><cell>4.45</cell><cell>2.36 (−47.0%)</cell></row><row><cell>Recidivism</cell><cell>0.95</cell><cell>0.39 (−58.9%)</cell><cell>1.10</cell><cell>0.52 (−52.7%)</cell></row><row><cell cols="4">the perspective of Statistical Parity is given by</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison on model utility and bias mitigation between GNNs with original network input (denoted as Vanilla) and those with debiased network input from EDITS (denoted as EDITS) on three real-world datasets. ↑ denotes the larger, the better; ↓ denotes the opposite. Best results are marked in bold.</figDesc><table><row><cell></cell><cell cols="2">GCN</cell><cell cols="2">GraphSAGE</cell><cell cols="2">GIN</cell></row><row><cell></cell><cell>Vanilla</cell><cell>EDITS</cell><cell>Vanilla</cell><cell>EDITS</cell><cell>Vanilla</cell><cell>EDITS</cell></row><row><cell></cell><cell>AUC ↑ 74.46 ± 0.7%</cell><cell>71.01 ± 1.3%</cell><cell cols="2">75.28 ± 2.1% 73.21 ± 0.5%</cell><cell cols="2">71.35 ± 1.7% 71.51 ± 0.6%</cell></row><row><cell>German</cell><cell cols="2">F1 ↑ 81.54 ± 0.9% 82.43 ± 0.69% 𝚫 𝑺𝑷 ↓ 43.14 ± 2.5% 2.04 ± 1.3%</cell><cell cols="2">81.52 ± 1.0% 80.62 ± 1.5% 26.83 ± 0.5% 8.30 ± 3.1%</cell><cell cols="2">83.08 ± 0.9% 83.78 ± 0.4% 18.55 ± 2.0% 1.26 ± 0.7%</cell></row><row><cell></cell><cell>𝚫 𝑬 𝑶 ↓ 33.75 ± 0.4%</cell><cell>0.63 ± 0.39%</cell><cell>20.66 ± 3.0%</cell><cell>3.75 ± 3.3%</cell><cell>11.27 ± 3.5%</cell><cell>2.87 ± 1.4%</cell></row><row><cell></cell><cell>AUC ↑ 73.62 ± 0.3%</cell><cell>70.16 ± 0.6%</cell><cell cols="2">74.99 ± 0.2% 75.28 ± 0.5%</cell><cell cols="2">73.82 ± 0.4% 72.06 ± 0.9%</cell></row><row><cell>Credit</cell><cell>F1 ↑ 81.86 ± 0.1% 𝚫 𝑺𝑷 ↓ 12.93 ± 0.1%</cell><cell>81.44 ± 0.2% 9.13 ± 1.2%</cell><cell cols="2">82.31 ± 0.7% 83.39 ± 0.3% 17.03 ± 3.3% 12.25 ± 0.2%</cell><cell cols="2">82.11 ± 0.1% 85.10 ± 0.7% 12.18 ± 0.3% 8.79 ± 5.6%</cell></row><row><cell></cell><cell>𝚫 𝑬 𝑶 ↓ 10.65 ± 0.0%</cell><cell>7.88 ± 1.0%</cell><cell>15.31 ± 4.0%</cell><cell>9.58 ± 0.1%</cell><cell>9.48 ± 0.3%</cell><cell>7.19 ± 3.8%</cell></row><row><cell></cell><cell>AUC ↑ 86.91 ± 0.4%</cell><cell>85.96 ± 0.3%</cell><cell cols="2">88.12 ± 1.4% 88.15 ± 0.9%</cell><cell cols="2">82.40 ± 0.8% 81.55 ± 1.5%</cell></row><row><cell>Recidivism</cell><cell>F1 ↑ 78.30 ± 1.0% 𝚫 𝑺𝑷 ↓ 7.89 ± 0.3%</cell><cell>75.80 ± 0.5% 5.39 ± 0.2%</cell><cell cols="2">76.23 ± 2.8% 76.30 ± 1.4% 2.42 ± 1.2% 0.79 ± 0.5%</cell><cell cols="2">70.36 ± 1.9% 71.09 ± 2.3% 9.97 ± 0.7% 4.98 ± 0.9%</cell></row><row><cell></cell><cell>𝚫 𝑬 𝑶 ↓ 5.58 ± 0.2%</cell><cell>3.36 ± 0.3%</cell><cell>2.98 ± 2.2%</cell><cell>1.01 ± 0.5%</cell><cell>6.10 ± 1.2%</cell><cell>5.47 ± 0.7%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">ACKNOWLEDGEMENTS</head><p>This material is, in part, supported by the National Science Foundation (NSF) under grants #2006844.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A reductions approach to fair classification</title>
		<author>
			<persName><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alina</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miroslav</forename><surname>Dudík</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="60" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Towards a Unified Framework for Fair and Stable Graph Representation Learning</title>
		<author>
			<persName><forename type="first">Chirag</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Himabindu</forename><surname>Lakkaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.13186</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Martín</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<ptr target="http://arxiv.org/abs/1701.07875" />
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Wasserstein GAN. CoRR</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fairness in machine learning</title>
		<author>
			<persName><forename type="first">Solon</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS Tutorial</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.00075</idno>
		<title level="m">Data decisions and theoretical implications when adversarially learning fair representations</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Compositional Fairness Constraints for Graph Embeddings</title>
		<author>
			<persName><forename type="first">Avishek</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th ICML (ICML &apos;19)</title>
				<meeting>the 36th ICML (ICML &apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Compositional Fairness Constraints for Graph Embeddings</title>
		<author>
			<persName><forename type="first">Joey</forename><surname>Avishek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th ICML (ICML &apos;19)</title>
				<meeting>the 36th ICML (ICML &apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Balanced neighborhoods for fairness-aware collaborative recommendation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sonboli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mansoury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ordoñez-Gauger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Building classifiers with independency constraints</title>
		<author>
			<persName><forename type="first">Toon</forename><surname>Calders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Kamiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mykola</forename><surname>Pechenizkiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE International Conference on Data Mining Workshops</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Spectral graph theory</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Rk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>American Mathematical Soc</publisher>
			<biblScope unit="volume">92</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">FairGNN: Eliminating the Discrimination in Graph Neural Networks with Limited Sensitive Attribute Information</title>
		<author>
			<persName><forename type="first">Enyan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.01454</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Say No to the Discrimination: Learning Fair Graph Neural Networks with Limited Sensitive Attribute Information</title>
		<author>
			<persName><forename type="first">Enyan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.01454[cs.LG]</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Graph transformation policy network for chemical reaction prediction</title>
		<author>
			<persName><forename type="first">Kien</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Truyen</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetha</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="750" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Mengnan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Na</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08843</idno>
		<title level="m">Fairness in deep learning: A computational perspective</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fairness through awareness</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toniann</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Reingold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd ITCS (ITCS &apos;12)</title>
				<meeting>the 3rd ITCS (ITCS &apos;12)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fairness through awareness</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toniann</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Reingold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Innovations in Theoretical Computer Science</title>
				<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-01-08">2012. 2012. January 8-10, 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploring author gender in book rating and recommendation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ekstrand</surname></persName>
		</author>
		<author>
			<persName><surname>Kluver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">User Modeling and User-Adapted Interaction</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06640</idno>
		<title level="m">Adversarial removal of demographic attributes from text data</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A Fairness-aware Hybrid Recommender System</title>
		<author>
			<persName><forename type="first">F</forename><surname>Golnoosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Spencer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><forename type="middle">G</forename></persName>
		</author>
		<idno>CoRR abs/1809.09030</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st NeurIPS (NIPS &apos;17)</title>
				<meeting>the 31st NeurIPS (NIPS &apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Equality of Opportunity in Supervised Learning</title>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nati</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th NeurIPS (NeurIPS &apos;16)</title>
				<meeting>the 30th NeurIPS (NeurIPS &apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Equality of opportunity in supervised learning</title>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nati</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th NeurIPS (NIPS &apos;16)</title>
				<meeting>the 30th NeurIPS (NIPS &apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Neural networks for machine learning lecture 6a overview of mini-batch gradient descent</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
	<note>Cited on</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the Information Unfairness of Social Networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zeinab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weixiang</forename><surname>Jalali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myunghwan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hema</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sucheta</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><surname>Soundarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM International Conference on Data Mining. SIAM</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="613" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Eliciting and enforcing subjective individual fairness</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Neel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Stapleton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><forename type="middle">Steven</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10660</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Data preprocessing techniques for classification without discrimination</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kamiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Calders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">InFoRM: Individual Fairness on Graph Mining</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Maciejewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th SIGKDD</title>
				<meeting>the 26th SIGKDD</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd ICLR (ICLR &apos;15)</title>
				<meeting>the 3rd ICLR (ICLR &apos;15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Counterfactual fairness</title>
		<author>
			<persName><forename type="first">Matt</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Loftus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4066" to="4076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Operationalizing Individual Fairness with Pairwise Fair Representations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Preethi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Gummadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
				<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">ifair: Learning individually fair data representations for algorithmic decision making</title>
		<author>
			<persName><forename type="first">Preethi</forename><surname>Lahoti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">P</forename><surname>Gummadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th ICDE (ICDE &apos;19)</title>
				<meeting>the 35th ICDE (ICDE &apos;19)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to discover social circles in ego networks</title>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th NeurIPS (NIPS &apos;12)</title>
				<meeting>the 26th NeurIPS (NIPS &apos;12)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On dyadic fairness: Exploring and mitigating bias in graph connections</title>
		<author>
			<persName><forename type="first">Peizhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengyu</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongfu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On dyadic fairness: Exploring and mitigating bias in graph connections</title>
		<author>
			<persName><forename type="first">Peizhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengyu</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongfu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
				<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Besim</forename><surname>Avci</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08286</idno>
		<title level="m">Incorporating priors with feature attribution on text classification</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Christos</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00830</idno>
		<title level="m">The variational fair autoencoder</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Ninareh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fred</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nripsuta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kristina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><forename type="middle">G</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09635</idno>
		<title level="m">A survey on bias and fairness in machine learning</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bursting the Filter Bubble: Fairness-Aware Network Link Prediction</title>
		<author>
			<persName><forename type="first">Farzan</forename><surname>Masrour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="841" to="848" />
		</imprint>
	</monogr>
	<note>Pang-Ning Tan, and Abdol Esfahanian</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takanori</forename><surname>Maehara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09550</idno>
		<title level="m">Revisiting graph neural networks: All we have is low-pass filters</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">MONET: Debiasing Graph Embeddings via the Metadata-Orthogonal Training Unit</title>
		<author>
			<persName><forename type="first">John</forename><surname>Palowitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<idno>CoRR abs/1909.11793</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Eli</forename><surname>Pariser</surname></persName>
		</author>
		<title level="m">The filter bubble: How the new personalized web is changing what we read and how we think</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>Penguin</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manish</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.02012</idno>
		<title level="m">On fairness and calibration</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fairwalk: Towards Fair Graph Embedding</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Tahleen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bartlomiej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th IJCAI (IJCAI &apos;19)</title>
				<meeting>the 28th IJCAI (IJCAI &apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Theory and application of digital signal processing</title>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Lawrence R Rabiner</surname></persName>
		</author>
		<author>
			<persName><surname>Gold</surname></persName>
		</author>
		<author>
			<persName><surname>Yuen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Prentice-Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fairwalk: Towards Fair Graph Embedding</title>
		<author>
			<persName><forename type="first">T</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Surma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th IJCAI (IJCAI &apos;19)</title>
				<meeting>the 28th IJCAI (IJCAI &apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning node representations from structural identity</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Saverese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining</title>
				<meeting>the 23rd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="385" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Right for the right reasons: Training differentiable models by constraining their explanations</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Slavin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">C</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Finale</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03717</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Linking bank clients using graph neural networks powered by rich transactional data</title>
		<author>
			<persName><forename type="first">V</forename><surname>Shumovskaia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fedyanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sukharev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Berestnev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Panov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Data Science and Analytics</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Relational learning via latent social dimensions</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th SIGKDD (KDD &apos;09)</title>
				<meeting>the 15th SIGKDD (KDD &apos;09)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Investigating and Mitigating Degree-Related Biases in Graph Convoltuional Networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th CIKM (CIKM &apos;20)</title>
				<meeting>the 29th CIKM (CIKM &apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Petar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guillem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Arantxa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Adriana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pietro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yoshua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Cédric</forename><surname>Villani</surname></persName>
		</author>
		<title level="m">Optimal transport: old and new</title>
				<imprint>
			<publisher>Springer Science &amp; Business Media</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">338</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Repairing without retraining: Avoiding disparate impact with counterfactual distributions</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berk</forename><surname>Ustun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Flavio</forename><surname>Calmon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6618" to="6627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Counterfactual Fairness: Unidentification, Bound and Algorithm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Towards Consumer Loan Fraud Detection: Graph Neural Networks with Role-Constrained Conditional Random Field</title>
		<author>
			<persName><forename type="first">Bingbing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huawei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks?</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning fair representations</title>
		<author>
			<persName><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toni</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ICML (ICML &apos;13)</title>
				<meeting>the 30th ICML (ICML &apos;13)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Link Prediction Based on Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2018/hash/53f0d7c537d99b3824f0f99d62ea2428-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nicolò</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roman</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03">2018. 2018. 2018. December 3-8, 2018</date>
			<biblScope unit="page" from="5171" to="5181" />
		</imprint>
	</monogr>
	<note>Samy Bengio, Hanna M. Wallach</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Men also like shopping: Reducing gender bias amplification using corpuslevel constraints</title>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09457</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
