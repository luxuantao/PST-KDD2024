<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Characterization of data compression across CPU platforms and accelerators</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Laura</forename><surname>Promberger</surname></persName>
							<email>laura.promberger@cern.ch</email>
							<idno type="ORCID">0000-0003-0127-6255</idno>
							<affiliation key="aff0">
								<orgName type="department">EP Department</orgName>
								<orgName type="institution">CERN</orgName>
								<address>
									<settlement>Geneva</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ZITI</orgName>
								<orgName type="institution" key="instit2">Heidelberg University</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">EP Department</orgName>
								<orgName type="institution" key="instit1">Correspondence Laura Promberger</orgName>
								<orgName type="institution" key="instit2">CERN</orgName>
								<address>
									<settlement>Meyrin</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rainer</forename><surname>Schwemmer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">EP Department</orgName>
								<orgName type="institution">CERN</orgName>
								<address>
									<settlement>Geneva</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Holger</forename><surname>Fr?ning</surname></persName>
							<idno type="ORCID">0000-0001-9562-0680</idno>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ZITI</orgName>
								<orgName type="institution" key="instit2">Heidelberg University</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">EP Department</orgName>
								<orgName type="institution" key="instit1">Correspondence Laura Promberger</orgName>
								<orgName type="institution" key="instit2">CERN</orgName>
								<address>
									<settlement>Meyrin</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Characterization of data compression across CPU platforms and accelerators</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1002/cpe.6465</idno>
					<note type="submission">Received: 27 November 2020 Accepted: 19 May 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ever increasing amount of generated data makes it more and more beneficial to utilize compression to trade computations for data movement and reduced storage requirements. Lately, dedicated accelerators have been introduced to offload compression tasks from the main processor. However, research is lacking when it comes to the system costs for incorporating compression. This is especially true for the influence of the CPU platform and accelerators on the compression. This work will show that for general-purpose lossless compression algorithms following can be recommended: (1) snappy for high throughput, but low compression ratio; (2) zstandard level 2 for moderate throughput and compression ratio; (3) xz level 5 for low throughput, but high compression ratio. And it will show that the selected platforms (ARM, IBM or Intel) have no influence on the algorithm's performance. Furthermore, it will show that the accelerator's zlib implementation achieves a comparable compression ratio as zlib level 2 on a CPU, while having up to 17? the throughput and utilizing over 80% less CPU resources. This suggests that the overhead of offloading compression is limited but present. Overall, this work will allow system designers to identify deployment opportunities for compression while considering integration constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K E Y W O R D S</head><p>compression, cross-platform, measurement, offloading</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The ever increasing amount of generated data leads to a growing gap between processing speed and I/O. As a result it becomes more and more beneficial to utilize data compression to trade computation time for data movement and storage requirements. The wish to use compression raises the questions of the optimal compression algorithm for the given tasks, how to integrate it into the system and how it will influence the overall system design and performance.</p><p>Compression algorithms can be classified into lossless und lossy compression. Lossless compression is an one-to-one mapping between original data and compressed data allowing to accurately reconstruct compressed data. Contrary to this, lossy compression is a many-to-one mapping and thus cannot promise accurate reconstruction. This work only analyses lossless compression algorithms, as it is often the only choice in scientific research communities, like the High Energy Physics (HEP) community. In such communities accurate data is necessary to have valid scientific findings. Hence, lossy compression and its potentially inaccurate reconstruction are not wanted. Aside from this classification, different compression</p><p>The authors thank the collaborations of CERN experiments ATLAS, ALICE, and LHCb for providing their data sets. This work has been sponsored by the Wolfgang Gentner Programme of the German Federal Ministry of Education and Research (grant no. 05E18CHA).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>algorithms offer different trade-offs between compression ratio and computation time. The system design on hardware level includes the choice of the CPU platform and the usage of accelerators.</p><p>Even though compression becomes increasingly attractive, little research is available about the costs of integrating compression into existing system and system design choices. Therefore, this work characterizes the performance of software-based general-purpose lossless compression algorithms on three different CPU platforms: ARM Armv8 aarch64, IBM POWER8 ppc64le and Intel Xeon x86_64; and it compares the performance of commercially available compression accelerators to the software-based general-purpose lossless compression algorithms on an Intel Xeon x86_64 server. With this the following contributions are made:</p><p>? Presentation of benchmarks for single-threaded, multi-threaded and compression accelerators ? Performance and power analysis of general-purpose lossless compression algorithms on different CPU platforms (ARM Armv8 aarch64, IBM POWER8 ppc64le and Intel Xeon x86_64)</p><p>? Performance comparison of commercially available compression accelerators with the above mentioned software-based compression algorithms ? Proposal for selection of compression algorithms depending on problem requirements These contributions allow, in particular, system designers to identify deployment opportunities given existing compute systems, data sources, and integration constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>To diminish the implications on (network) data movements and storage, massive utilization of compression can be found, among others, in communities working with databases, graph computations or computer graphics. For these communities a variety of works exists which addresses compression optimizations by utilizing hardware-related features like SIMD and GPGPUs. <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref> Usage of general-purpose compression algorithms can be found in, for example, mobile devices, <ref type="bibr" target="#b4">5</ref> video compression <ref type="bibr" target="#b5">6</ref> or for communication between robots. <ref type="bibr" target="#b6">7</ref> And such algorithms are also compared against newly developed algorithms, like Brotli. <ref type="bibr" target="#b7">8</ref> Furthermore, lossy and lossless compression algorithms were analyzed for data of cosmic particles on an Intel system. <ref type="bibr" target="#b8">9</ref> Similar to HEP data sets, which are used in this work, researching cosmological particles creates huge amounts of data under real-time constraints.</p><p>Moreover, a multitude of research has been conducted to efficiently use Field Programmable Gate Array (FPGA) technology, as they excel at integer computations which are also highly used in compression. Works here range from developing FPGA-specific compression algorithms, <ref type="bibr" target="#b9">10</ref> to integrating the FPGA compression hardware into existing systems, <ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12</ref> to implementing well-known compression algorithms, like gzip, and comparing the achieved performance to other systems (CPU, GPGPU). For example, some work explored CPUs, FPGAs, and CPU-FPGA co-design for LZ77 accelerations, <ref type="bibr" target="#b12">13</ref> while other analyzed the hardware acceleration capabilities of the IBM PowerEN processor for zlib. <ref type="bibr" target="#b13">14</ref> Regarding FPGAs, Abdelfattah et al. <ref type="bibr" target="#b14">15</ref> and Qiao et al. <ref type="bibr" target="#b15">16</ref> implemented their own deflate version on an FPGA.</p><p>However, the large majority of those works focuses on the compression performance in terms of throughput and compression ratio, but not on the resulting integration costs for the entire system. One of those few studies is Matai et al. <ref type="bibr" target="#b16">17</ref> which analyzed the energy efficiency of canonical Huffman coding for Intel, ARM and FPGA, with Intel being the least efficient and the FPGA the most efficient energy-wise.</p><p>Overall, little to no published work can be found covering the topic of system integration costs in the context of compression accelerators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Compression algorithms</head><p>The selected compression algorithms were lossless general-purpose compression algorithms which did not rely on any pre-existent knowledge about the data. The selection was based on their usage in related works and general popularity, including recent developments. Most compression algorithms allow tuning the performance for compression ratio * or throughput ? by setting the compression level. Generally speaking, a higher compression level refers to a higher compression ratio, but also a longer computation time. For this study, compression algorithms run solely on CPU are called software-based (sw-based). In Table <ref type="table">1</ref> all selected sw-based algorithms and their respective compression levels are listed. For them, a pre-study was done to select compression levels where significant changes in either compression ratio or throughput occurred (see Figure <ref type="figure">1</ref>). F I G U R E 1 Pre-study to evaluate the compression algorithms. The throughput is of a single thread for the data sets ALICE 2 and LHCb 1.</p><p>The selected algorithms and their compression levels are circled in red Bzip2 <ref type="bibr" target="#b17">18</ref> combines the Burrows-Wheeler algorithm with a Move-To-Front transform and Huffman coding. Unlike all other algorithms presented here, the level in bzip2 does not refer to the compression function utilized, but the block size of the input data (between 100 and 900 kB). For this study the largest block size (level 9) was chosen.</p><p>Lz4 <ref type="bibr" target="#b18">19</ref> is a LZ77 byte-oriented algorithm. The sub-type Lz4_HC offers 13 levels to increase the compression ratio and automatically builds the frame (header and footer) for the compressed payload. For this study Lz4_HC level 2 was selected. It has a low compression ratio, but a high throughput. For the rest of this study Lz4_HC is referred to as lz4.</p><p>Snappy <ref type="bibr" target="#b19">20</ref> is an algorithm based on LZ77. It was developed at Google with the goal to have a short computation time. It decompresses data significantly faster than it compresses data. Snappy has no tuning parameters.</p><p>Xz <ref type="bibr" target="#b20">21</ref> is a popular tool which offers multiple compression algorithms. The primary algorithm used is LZMA2, which has 9 levels and can achieve a high compression ratio, but requires a long computation time. For this study level 1 and level 5 were selected.</p><p>Zlib <ref type="bibr" target="#b21">22</ref> utilizes the deflate algorithm. <ref type="bibr" target="#b22">23</ref> Deflate is an algorithm based on LZ77, followed by Huffman coding. Zlib is a popular library used, among others, for ZIP and gzip compression. Zlib offers 9 compression levels, of which levels 2 and 4 were selected. This allows to compare against the zlib implementation of the accelerators used in this study.</p><p>Zstandard (zstd) <ref type="bibr" target="#b23">24</ref> is an algorithm based on LZ77, in combination with fast Finite State Entropy and Huffman coding. It was developed by Facebook for real-time compression. Zstd offers 23 levels. Above compression level 20 the configuration differs to achieve a higher compression ratio, but this increases significantly the memory usage. For this study level 2 and level 21 were selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data sets</head><p>Depending on the part within this study, a selection of the input data was chosen from up to four standard compression corpora and six HEP data sets listed in Table <ref type="table">2</ref>. To be representative for big data, compression corpora with at least 200 MB were preferred. One exception was the calgary corpus, as it is one of the most famous compression corpora and thus used in many related works. Tared into a single file, it only sums up to 3.1 MB.</p><p>To conform with the benchmark requirements, it was copied multiple times to reach the 150 MB. The HEP data came from three different CERN experiments. Several files are listed as uncompressed which is the preferred state in this study to be able to analyze the data's full potential. However, this does not mean that the experiments store their production data like this. Many of the experiments apply some form of compression, for example, zero-compression which is similar to the representation of sparse matrices in coordinate format. At CERN, particles are accelerated in a vacuum very close to the speed of light. They are then collided and analyzed by the experiments. The data created consists of geographical locations where particles were registered. Due to the nature of quantum mechanics this process is close to a random number generator. As compression takes advantage of patterns within data, a high compression ratio cannot be expected for them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Compression benchmarks</head><p>A benchmark for each scenario was created: one to analyze the performance of the accelerators, and one each to analyze the single-stream and multi-stream behavior of the sw-based compression. All benchmarks were built upon the same principles to reduce any benchmark-related bias.</p><p>The layout of both multi-stream benchmarks, sw-based and for the accelerator, are shown in Figure <ref type="figure">2</ref>.</p><p>The performance measured was the sustained throughput over the compression function itself. It consists of retrieving the data from RAM, compressing and writing it back to RAM. The tasks of initially loading the data into RAM and writing it out to, for example, the disk, was excluded.</p><p>The following metrics are measured: average compression ratio, total size of uncompressed data, total size of compressed data and the exact run time of each compression thread.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Software-based benchmarks</head><p>Both, the single-stream and multi-stream benchmarks first loaded the entire data set into RAM before calling the streaming compression function of the compression library.</p><p>The single-stream benchmark consists of a single process which executes the compression and runs exclusively on the server. It was used to evaluate and select the compression levels of each algorithm for this study. And it was used to evaluate the stability of throughput and compression ratio when reducing the data size of the input. Based on this and to account for memory limitations on the different servers when using the multi-stream benchmark, the chunk size was set to 150 MB. Larger data sets are iterated multiple times via round-robin, each time compressing 150 MB blocks until the time constraint is reached.</p><formula xml:id="formula_0">F I G U R E 2 Benchmark layout:</formula><p>The mainThread coordinates the progress of the compressThread. The compressThread continuously compresses the input and records the results. For the accelerator benchmark the additional queueThread (in blue) provides the input data chunk-wise to the compressThread</p><p>The multi-stream benchmark consists of two types of threads: the mainThread, which is the thread started by the user and which coordinates the orchestration of all other threads, and multiple compressThreads that execute the compression. Hwloc is used to bind the com-pressThreads equally split onto the NUMA nodes. Because of memory limitation reasons the input data set was loaded only once for each NUMA node and shared with the node's compressThreads. All other resources were uniquely assigned and private to each process (e.g., the output buffer).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Accelerator benchmark</head><p>The accelerator benchmark is a multi-stream benchmark which, compared to the sw-based benchmark, has one additional thread type:</p><p>queueThread. This overlap between the benchmarks can also be seen in Figure <ref type="figure">2</ref>. The queueThread provides the data set chunk-wise to the com-pressThreads via a thread-safe queue. This is necessary as each compression stream needed access to its own allocated input data in order to be able to communicate with the accelerator. To optimize the load for each thread, compressThread was able to run multiple compression streams in a single thread. The compression function used busy waiting to retrieve the results of the streams from the accelerator. Parameters that could be modified were the number of each type of thread, the input buffer size, the size of the thread-safe queue, the number of compression streams within each compressThread, and the type of algorithm and its compression level.</p><p>Overall, the main difference to the sw-based benchmark was that each compression stream needed its own allocated memory for the input data, while the sw-based benchmark moved a pointer on the shared memory of the input data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Performance instrumentation and measurement</head><p>Multiple tools were used to measure the CPU and RAM power consumption and memory bandwidth. The CPU and RAM power consumption was measured for ARM using ipmitool remotely to access the baseboard management controller (BMC), IBM using ipmitool locally and for Intel with the Processor Counter Monitor or likwid. <ref type="bibr" target="#b27">28</ref> Furthermore, for Intel likwid was used to measure the read, write and total memory bandwidth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Hardware</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Servers</head><p>Three servers were selected, each belonging to a different platform: x86_64 (Intel), ARMv8 aarch64 (ARM), and POWER8 ppc64le (IBM). For Intel an older mid-tier server E5-2650 v3 was used. For IBM a similar old, but high-tier server 8335-GTB was used. Only for ARM the Cavium ThunderX2, a state-of-the-art high tier server, was used. The hardware specifications of the servers are listed in Table <ref type="table">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Compression accelerators</head><p>Two commercially available accelerators were considered: Intel QuickAssist 8970 (QAT), <ref type="bibr" target="#b28">29</ref> and AHA378. <ref type="bibr" target="#b29">30</ref> QAT is an ASIC which offers encryption and compression acceleration up to 100 Gb/s. AHA378 is an FPGA accelerator with up to 80 Gb/s, solely designed for compression.</p><p>Unfortunately, the QAT exhibited instabilities in performance and thus did not deliver any consistent results. Therefore, this work will focus on the AHA378 accelerator. The PCIe Gen 3 ? 16 accelerator consists of 4 FPGAs, each providing 20 Gb/s, for a total of up to 80 Gb/s. It offers two compression algorithms: lzs and deflate. For deflate the additional data formats gzip and zlib are provided. The power consumption is specified to not surpass 25 W when the kernel module is not loaded, and not to exceed 75 W during heavy workload, as all the power is provided through the PCIe slot.</p><p>For this study only deflate was analyzed, as a pre-study showed that lzs was inferior in performance compared to the others, and that the performance of deflate, gzip and zlib were equivalent. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TA B L E 3 Servers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Xeon</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PERFORMANCE OF DIFFERENT CPU PLATFORMS</head><p>This section analyses the characteristics of the different sw-based compression algorithms on the three common CPU platforms: ARM (aarch64), IBM (ppc64le), and Intel (x86_64). Following behaviors were characterized: cross-platform, performance per core, scalability, robustness and power consumption. In this section only the HEP data sets were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Cross-platform behavior</head><p>For the cross-platform behavior the best performance of the multi-stream benchmark was compared over all servers. The best performance might differ for each compression algorithm and level in the number of compression streams executed in parallel. Figure <ref type="figure">3</ref> shows that independent of the platform, all servers created a similar performance pattern for the same data set ALICE 2. And Figure <ref type="figure">4</ref> shows that the performance pattern differed for different data sets. For example, bzip2 level 9 had better performance and better compression ratio for ALICE 2 when compared to LHCb 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance per core</head><p>To compare the performance between different platforms, a metric was designed to describe the performance per core. It is described by Throughput per CPU clock speed * #Real Cores. Values for the CPU clock speed were taken from the manufacturer specifications as the CPU clock speed was not measured during the benchmarks.</p><p>In Figure <ref type="figure">5</ref> two plots, one for ATLAS 2 and one for LHCb 1, highlight the performance increase per core compared to the weakest server, the Intel E5-2650 v3. The relative performance increase differed depending on the compression algorithm and level. The Intel E5-2650 v3 had in all the cases the weakest performance per core, followed by ARM ThunderX2, and IBM 8335-GTB which had the best performance. The only exception was zstd level 2. Depending on the data set, it performed equal or worse on the ARM ThunderX2 compared to the Intel E5-2650 v3. Furthermore, the IBM 8335-GTB had a notable higher performance increase for bzip2 level 9, xz level 1 and xz level 5 compared to the ARM ThunderX2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Scalability</head><p>To evaluate the performance scaling both sw-based benchmarks were run with a chunk size of 150 MB. The best performance of the multi-stream benchmark was selected per compression algorithm and level. This might result in different thread configurations for the same server.</p><p>Table <ref type="table">4</ref> lists the performance increase normalized by the number of real cores between the multi-stream and the single-stream benchmark.</p><p>The value 1.0 implies that multi-stream and single-stream have same performance per real core, and thus having no increased performance when using SMT.</p><p>The best scaling factor was achieved by zlib level 2 for Intel E5-2650 v3 (1.19) and ARM ThunderX2 (1.47), while xz level 5 achieved the best for IBM 8335-GTB (2.33). The worst (negative) scaling factor had zstd level 21 on Intel E5-2650 v3 (0.66). The worst scaling factor for ARM ThunderX2 (0.90) and IBM 8335-GTB (1.58) had zstd level 2. On average, the scaling factor for Intel E5-2650 v3 was 0.97, for ARM ThunderX2 was 1.14 and IBM 8335-GTB was 1.87. The scaling was unaffected by the choice of data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Robustness</head><p>In this part of the study, the number of parallel compression streams which achieved maximum throughput was determined. In Figure <ref type="figure">6</ref> a 2D heat map is shown for the evaluation on each platform depending on compression algorithm (left column) and input data set (right column). For every  to use all virtual cores, while IBM 8335-GTB preferred SMT6 for the robustness of the algorithms and SMT4 for the robustness of the data sets combination of algorithm and data set each thread configuration was counted which was within 5% of the maximum performance. Brighter colors on the heat map present a smaller variance in the thread configurations for the optimal performance and thus describing a more robust platform.</p><p>The most robust environment provided the Intel E5-2650 v3. It preferred to use all virtual cores (SMT2). The next robust server was the ARM ThunderX2, which also preferred to use all virtual cores (SMT4), except for snappy which performed better using only SMT3. The least robust server was the IBM 8355-GTB. It had contradicting configuration preferences depending on which kind of robustness was evaluated. For the robustness based on the algorithms it preferred SMT6 (96 cores), with the exception of zstd level 21 which preferred SMT7 (112 cores). While for the robustness based on the data sets it preferred SMT4 (64 cores). Neither of the robustness studies preferred to use all virtual cores (SMT8, 128 cores).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Power consumption</head><p>The CPU and RAM power consumption was evaluated on the multi-stream benchmark. All the values refer to the power consumption of a single socket. It was not possible to measure the RAM power consumption on the ARM ThunderX2, thus the cross-platform comparison will only be based on the CPU power consumption.</p><p>The overall maximum CPU and RAM power consumption for each server is listed in Table <ref type="table">5</ref>. Both, IBM 8335-GTB and Intel E5-2650 v3, achieved the highest power consumption on the data set ALICE 3. They achieved with 99% and 92% a power consumption close to the TDP, while ARM ThunderX2 only reached about 77% of its TDP for the highest power consumption on the data set LHCb 1.</p><p>When looking at the energy efficiency per mega byte, ALICE 3 was the most efficient for Intel E5-2650 v3 and IBM 8335-GTB even though it had the highest power consumption. It reached an energy efficiency of 146 MiB / Joule for IBM 8335-GTB, of 158 MiB / Joule for Intel E5-2650 v3 and of 370 MiB / Joule for ARM ThunderX2. For some data sets the ARM ThunderX2 reached even a energy efficiency &gt; 500 MiB / Joule.</p><p>The energy efficiency scaling behavior for the data set LHCb 1 is shown in Figure <ref type="figure">7</ref>. Other data sets resulted in a similar scaling behavior, namely</p><p>? ARM ThunderX2 always outperformed the Intel E5-2650 v3 and IBM 8335-GTB by a factor of 2 to 5, independent of algorithm and data set</p><p>? Intel E3-2650 v3 performed slightly better than the IBM 8335-GTB</p><p>? A deterioration of the energy efficiency occurred for ARM ThunderX2 and IBM 8335-GTB when increasing the number of threads per core</p><p>? On average the performance decreased for ARM ThunderX2 after SMT3 and for IBM 8335-GTB after SMT4</p><p>Evaluating the power consumption per core in comparison between SMT1 and higher modes of SMTx showed no linear relationship. On the contrary, on the ARM ThunderX2 for some compression algorithms the CPU power consumption was lower for higher modes of SMTx than SMT1.</p><p>And the RAM power consumption of zlib level 2 did not change and was independent of the SMT mode.</p><p>The RAM power consumption was evaluated for Intel E5-2650 v3 and IBM 8335-GTB. The Intel E5-2650 v3 has 4 memory channels per socket, each connected to one 8 GB, 2133 MHz module. The IBM 8335-GTB has 8 memory channels per socket, each connected to two 8 GB, 2400 MHz modules. The maximum RAM power consumption measured on Intel E5-2650 v3 was 10 W and on IBM 8335-GTB 63 W. This would mean that each module needs 2.5 and 3.9 W respectively, if a linear dependency existed between power consumption and number of memory modules.</p><p>Grading the algorithms by throughput and energy efficiency, the following order can be established (first being best) F I G U R E 7 Energy efficiency for LHCb 1. The ARM ThunderX2 has up to a factor 5 better performance per Joule consumed than the Intel E5-2650 v3, which slightly performed better than the IBM 8335-GTB Overall, the biggest CPU power consumer was lz4 level 2 for Intel E5-2650 v3 and for ARM ThunderX, and xz level 1 and xz level 5 for IBM 8335-GTB. xz on IBM 8335-GTB also had the largest increase in RAM power consumption, while at the same time having the best CPU energy efficiency increase. For the other platforms no such connection was visible. The lowest increase in power consumption was achieved on the Intel E5-2650 v3: zstd level 21 and zlib level 2 for the CPU and zlib level 2 and lz4 level 2 for the RAM.</p><formula xml:id="formula_1">snappy &gt; lz4</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">PERFORMANCE OF THE ACCELERATOR</head><p>This section analyzes the characteristics of compression accelerator AHA378 in comparison to sw-based compression algorithms on the Intel E5-2650 v3. The data sets used were all compression corpora and the HEP data sets ALICE 2, ATLAS 1 and LHCb 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Accelerator results</head><p>The accelerator achieved a sustained throughput of 75-81 Gb/s, which is at least 94% of the advertised throughput of 80 Gb/s. This advertised throughput was even surpassed for the calgary corpus. The configuration achieving this was using numactl to bind the entire benchmark on the same NUMA node to which the accelerator was connected to. The input parameters were set to a chunk size of 2 MB, 500 elements in the queue, 3 queueThreads, 4 compressThreads, and 12 compression streams per compressThread. All results for throughput and compression ratio are listed in Table <ref type="table">6</ref>, for memory bandwidth listed in Table <ref type="table">7</ref> and for power consumption listed in Table <ref type="table">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TA B L E 6</head><p>Compression ratio and throughput for the accelerator and sw-based algorithms Note: For the sw-based algorithms only the best and worst performing algorithms are listed. Additionally, zlib level 2 is listed to be able to compare to the accelerator.</p><p>The lowest throughput and compression ratio was achieved by LHCb 1 with 75 Gb/s and a compression ratio of 1.18. The highest compression ratio was achieved by Silesia corpus with a ratio of 2.94 and a throughput of 79 Gb/s. Overall, the data-dependent change of the throughput was less then 4%.</p><p>The memory bandwidth was between 22 and 30 GiB/s. ALICE, ATLAS, and LHCb had the highest memory bandwidth, and calgary corpus the lowest memory bandwidth. Exactly the opposite correlation could be found when looking at the memory bandwidth percentage used for reads.</p><p>Between 51% and 63% of all memory accesses were reads. In general there was a correlation between having a high compression ratio and a high percentage of reads. A higher compression ratio means a lower amount of output to be written and as a result the percentage of reads increases relative to the writes. Only calgary corpus defied this in the final measurement taken. It had the lowest read percentage (51 %), while at the same time the second highest compression ratio. However, multiple measurements showed that in general the read percentage of calgary corpus was with 59% on average significantly higher.</p><p>Similar to the throughput, the power consumption was stable and negligibly influenced by the data sets. For the socket to which the accelerator was connected to the CPU consumed 58 W and the RAM consumed 7 W. While running the benchmark, the power consumption of the entire server was 266 W.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Result comparison</head><p>In this section the results of the accelerator are compared to the results of the sw-based benchmark. The sw-based benchmark was run with 40 compressThreads, using all 40 virtual cores of the server.</p><p>The performance of throughput and compression ratio is listed in Table <ref type="table">6</ref>. The accelerator was compared against the sw-based zlib level 2. Even though zlib level 4 was closer in compression ratio (7% better), the penalty in the throughput was considered too large (13%-37% worse). Depending on the data set, the accelerator achieved an 8-17 times increased throughput compared to zlib level 2 when utilizing all virtual cores. A different algorithm, zstd level 2, had a similar compression ratio as zlib level 2, but a 2 to 3 times higher throughput.</p><p>snappy and lz4 achieved the highest throughput: For the data set PROTEINS they achieved the same throughput as the accelerator. However, overall the throughput was very data-dependent and their compression ratio was only 65%-82% of zlib level 2. The lowest throughput had zstd level 21 and in few cases xz level 5, but these were also the algorithms with the highest compression ratio. For the 150 MB version of the calgary corpus both algorithms, zstd level 21 and xz level 5, were able to recognize the entire calgary corpus sequence as a reoccurring pattern, and thus achieving a compression ratio of over 170.</p><p>The memory bandwidth is listed in Table <ref type="table">7</ref> and was for the sw-based algorithms between 1 and 40 GiB/s. In all cases had zlib level 2 the lowest bandwidth with 1-2 GiB/s. While xz level 5 had the highest bandwidth for the majority of data sets. Otherwise it was zstd level 21.</p><p>The percentage of memory bandwidth being read access was between 49% and 77%. zlib level 2 had here a similar split as the accelerator.</p><p>Listed in Table <ref type="table">8</ref>, the CPU power consumption of the sw-based algorithms was between 72 and 103 W for each socket, with snappy and zstd level 21 having the lowest CPU power consumption and zstd level 2 and zlib level 2 having the highest CPU power consumption.</p><p>zlib level 2 used between 98 and 101 W, which is 30 W more CPU power consumption than the accelerator. The RAM power consumption of sw-based algorithms was between 4 and 11 W. zlib level 2 had the lowest and xz level 5 had the highest RAM power consumption. There was a direct correlation between RAM power consumption and the memory bandwidth measured for the sw-based algorithms.</p><p>However, for the same RAM power consumption of 7 W, the accelerator benchmark achieved twice the memory bandwidth-the value reached when 9 W were consumed by the sw-based algorithms. The server's power consumption was with 266 W for the accelerator within the range also achieved by the different sw-based algorithms (259-301 W).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>This work consists of two sections, the first characterized the performance of general-purpose lossless compression algorithms on the three common CPU platforms: ARM (aarch64), IBM (ppc64le), and Intel (x86_64); and the second characterized the performance of compression accelerators comparing it to sw-based compression algorithms on the Intel (x86_64) platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Performance of different CPU platforms</head><p>It was observed that substantial compression ratios were achieved in spite of inhomogeneous data and already existing compression methods. Furthermore, all three platforms created a similar performance pattern for the same data set. This can be explained by those compression libraries not being performance optimized for a certain platform, but by being optimized for compression accuracy and reliability.</p><p>For nearly all cases, best performance was achieved by utilizing all virtual cores. Only the IBM 8335-GTB was inconsistent with this behavior and had problems finding a robust configuration. The IBM 8335-GTB had two preferred configurations SMT4 or SMT6, even though supporting SMT8. The preference dependent on compression algorithm and data set. As a result, on a IBM platform it is recommended to do a case-by-case study what configuration yields the best performance.</p><p>The ARM ThunderX2 had the best overall performance, both in throughput and energy efficiency. For the throughput, the IBM 8335-GTB had the second best performance, which reached on average about 68% of the ARM ThunderX2 performance. And the Intel E5-2650 v3 had the worst performance, only reaching on average around 40% of the ARM ThunderX2 performance. Energy efficiency-wise the Thun-derX2 significantly outperformed both Intel E5-2650 v3 and IBM 8335-GT, sometimes up to a factor 5. Here, IBM 8335-GTB performed the worst.</p><p>When looking at the performance per core that IBM 8335-GTB outperformed both, ARM ThunderX2 and Intel E5-2650 v3. However, more interesting is that for more than half of the algorithms the relative performance increase between Intel E5-2650 v3 and IBM 8335-GTB was not even close to twice as much, even though the IBM 8335-GTB was newer, and had close to twice the CPU power consumption while having less real cores. This might indicate that most compression libraries do not use the full potential of the IBM 8335-GTB.</p><p>Looking at the scalability, Intel E5-2650 v3 scaled with 0.97 sightly negative. This might indicate that the single core performance boosted by Turbo, increasing the clock frequency, is slightly stronger for a single core than using SMT2 to reduce pipeline stalls. ARM ThunderX2 reached a scaling factor of 1.14, while 8335-GTB reached a scaling factor of 1.87. This is more than four times better than ARM ThunderX2, suggesting that the POWER8 architecture has a better usage efficiency of the SMT modes than the ARMv8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Performance of the accelerator</head><p>The AHA378 accelerator achieved up to 17 times the throughput compared to the sw-based equivalent of the compression algorithm (zlib level 2) while utilizing over 80% less resources. The throughput achieved by the accelerator was at least 94% of the advertised throughput. The accelerator's throughput was stable independent of the data set (&lt;4% variation), while the sw-based zlib level 2 had a throughput reduction of up to 55% depending on the data set. This suggests that the logic on the accelerator could support a higher throughput but some (bandwidth) bottleneck prevents this. During the study it was shown that NUMA binding is important to maintain a stable and good performance.</p><p>Without NUMA binding the same data input could have a decreased throughput of up to 10 Gb/s for the entire benchmark.</p><p>To compare this work with the FPGA zlib implementations of Abdelfattah et al. <ref type="bibr" target="#b14">15</ref> and Qiao et al. <ref type="bibr" target="#b15">16</ref> the geometric mean of the calgary corpus had to be calculated. For this each file of the corpus had to be seperately compressed and afterwards the geometric mean was calculated over the entire corpus. with 80 Gb/s. <ref type="bibr" target="#b15">16</ref> The pipeline-setup of the benchmarks is a realistic model of the planned data acquisition pipeline for the LHCb experiment at CERN. For each captured particle collision the data will be filtered for interesting events, compressed in-flight and sent to storage. The compression will be done by directly streaming the data to the accelerator after the data was filtered. Thus, the data will at all times reside in the server's main memory. Only after the compression will the data be sent to permanent storage, for example, HDDs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Implications</head><p>In general, there is no surprise that the accelerator is faster than the sw-based algorithms while using significantly less resources. However, from the perspective of the system design it is important to understand the underlying requirements to select the optimal setup when it comes to platform and accelerator integration.</p><p>It was shown that when it comes to the sw-based compression algorithms there is no preference of any of the platforms: ARM, IBM or Intel.</p><p>And for the accelerator it was shown that the maximum throughput can be achieved by utilizing only 10% (= 4 compressThreads) of the CPU resources if the data is already residing in memory. However, while compute requirements were low, the memory bandwidth requirements of the accelerator were notable. It used around 80% of the memory bandwidth of one socket ? .</p><p>Hence, compute-intensive tasks would be the most compatible collocation on a system which uses accelerators for compression. This would allow to utilize the 90% of unused CPU time. To improve and maintain a stable performance NUMA binding should be used. Possibly, the best choice would be a task which utilizes the compression as post-processing step, making sure that the data is already available in memory. Alternatively, two accelerators could be put into one server, which would then saturate the memory bandwidth of both sockets.</p><p>As the accelerator benchmark used busy-waiting, it is strongly believed that the CPU overhead can still be reduced further. Future work will focus on implementing a more complex communication model based on polling to reduce the CPU overhead, while also integrating other tasks on the same server.</p><p>Based on the information about time and space implications of the compression algorithms and levels provided by this study, it would be possible to create a scheduler that can select appropriate compression algorithms during run time while still adhering to real-time constraints of for example, Data Acquisition Systems.</p><p>Moreover, Figure <ref type="figure">1</ref> can be used as reference to help with the selection of a fitting compression algorithm. Following general suggestions can be made: (1) Use snappy for high throughput, but low compression ratio; (2) Use zstd level 2 for moderate throughput and compression ratio; (3)   Use xz level 5 for low throughput, but high compression ratio.</p><p>However, as compression is highly data-dependent the authors recommend to conduct similar research before basing expensive hardware and software decisions on this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>This work provides insights on performance characteristics and system integration costs for general-purpose lossless compression algorithms on different CPU platforms and accelerators. For this, the performance on different CPU platforms and the performance of accelerators in comparison ? Including the allocation done in queueThreads to software-based compression algorithms is evaluated. It is shown that the performance of the compression libraries is independent of the evaluated architectures, ARM Armv8 aarch64, IBM POWER8 ppc64le and Intel Xeon x86_64. The ARM ThunderX2 had the best and greenest performance, but was also the most recent server being released. While the IBM 8335-GTB had the best performance per core. Compared to the software-based zlib the accelerator achieved equal compression ratio, but with up to 17 times increased throughput and utilizing over 80% less CPU resources. Considering that compression is often only one element of a data processing pipeline, this overhead cannot be neglected. Moreover, the results support that commercially available compression accelerator are a feasible choice to harvest the advantages of FPGAs of providing high throughput for well-known, reliable compression algorithms without intensive FPGA development effort. For software-based compression algorithms, following can be suggested: use snappy for high-throughput, but low compression ratio;</p><p>(2) zstd level 2 for moderate throughput and compression ratio; and (3) xz level 5 for low throughput, but high compression ratio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>Open Access Funding provided by European Organization for Nuclear Research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><label></label><figDesc>A) Intel E5-2650 v3 (B) ARM Cavium ThunderX2 CN9980 v2.1 (C) IBM POWER8NVL 8335-GTB F I G U R E 3 Cross-platform behavior: on all platforms, the algorithms created a similar performance pattern for the same data set (ALICE 2 shown) (A) Intel E5-2650 v3 -ALICE 2 (B) Intel E5-2650 v3 -LHCb 1 F I G U R E 4 Cross-platform behavior: on all platforms, the algorithms created different performance patterns for different data sets (A) ATLAS 2 (B) LHCb 1 F I G U R E 5 Relative performance per core increase compared to the weakest server, the Intel E5-2650 v3 for the data sets ATLAS 2 and LHCb 1. The IBM 8335-GTB had a notable higher performance increase for bzip2 level 9, xz level 1 and xz level 5 compared to the ARM ThunderX2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc>A) Intel E5-2650 v3 (B) Intel E5-2650 v3 (C) ARM Cavium ThunderX2 CN9980 v2.1 (D) ARM ThunderX2 CN9980 v2.1 (E) IBM POWER8NVL 8335-GTB (F) IBM POWER8NVL 8335-GTB F I G U R E 6 Robustness: The left column (A), (C), (E) shows the robustness of the algorithms. The right column (B), (D), (F) shows the robustness of the data sets. The brighter the color the more preferred the configuration is. On average, both Intel E5-2650 v3 and ARM ThunderX2 preferred</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>TA B L E 4 Scalability factor normalized by number of real cores: best multi-stream compared to single-stream</figDesc><table><row><cell></cell><cell cols="2">Averaged over data sets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Algorithm</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Server</cell><cell>bzip2 level 9</cell><cell>lz4 level 2</cell><cell>snappy</cell><cell>xz level 1</cell><cell>xz level 5</cell><cell>zlib level 2</cell><cell>zstd level 2</cell><cell>zstd level 21</cell></row><row><cell>ARM</cell><cell>1.14</cell><cell>1.17</cell><cell>1.20</cell><cell>0.98</cell><cell>1.19</cell><cell>1.47</cell><cell>0.90</cell><cell>1.05</cell></row><row><cell>IBM</cell><cell>2.06</cell><cell>1.59</cell><cell>1.68</cell><cell>1.91</cell><cell>2.33</cell><cell>2.23</cell><cell>1.58</cell><cell>1.87</cell></row><row><cell>Intel</cell><cell>1.02</cell><cell>1.01</cell><cell>1.06</cell><cell>0.84</cell><cell>0.99</cell><cell>1.19</cell><cell>0.97</cell><cell>0.66</cell></row><row><cell></cell><cell cols="4">Averaged over algorithms and experiments</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Experiment</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Server</cell><cell>ALICE 1</cell><cell>ALICE 2</cell><cell>ALICE 3</cell><cell>ATLAS 1</cell><cell>ATLAS 2</cell><cell>LHCb 1</cell><cell></cell><cell>Average</cell></row><row><cell>ARM</cell><cell>1.16</cell><cell>1.11</cell><cell>1.14</cell><cell>1.15</cell><cell>1.15</cell><cell>1.15</cell><cell></cell><cell>1.14</cell></row><row><cell>IBM</cell><cell>1.89</cell><cell>1.86</cell><cell>1.92</cell><cell>1.93</cell><cell>1.92</cell><cell>1.92</cell><cell></cell><cell>1.91</cell></row><row><cell>Intel</cell><cell>0.93</cell><cell>0.97</cell><cell>0.96</cell><cell>0.99</cell><cell>0.99</cell><cell>0.96</cell><cell></cell><cell>0.97</cell></row></table><note><p>Note: Bold values shown best scalability performance. Italic values shown worst scalability performance.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>lvl 2 &gt; zstd lvl 2 &gt; zlib lvl 2 &gt; bzip2 lvl 9 &gt; xz lvl 1 &gt; xz lvl 5 &gt; zstd lvl 21 Furthermore, following was observed for the scaling of the energy efficiency</figDesc><table><row><cell></cell><cell cols="3">(A) ARM ThunderX2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(B) IBM 8335-GTB</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(C) Intel E5-2650 v3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">? xz level 5 had the largest increase on all servers</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">? zstd level 2 had the smallest increase on all servers</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">? Changes in performance were not correlated with changes in the CPU or RAM power consumption</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">TA B L E 5 Maximum measured power consumption per socket</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Intel E5-2650 v3</cell><cell></cell><cell cols="3">ARM ThunderX2</cell><cell></cell><cell cols="2">IBM 8335-GTB</cell><cell></cell><cell></cell></row><row><cell></cell><cell>TDP</cell><cell>CPU</cell><cell>RAM</cell><cell>Total</cell><cell>TDP</cell><cell>CPU</cell><cell>RAM</cell><cell>Total</cell><cell>TDP</cell><cell>CPU</cell><cell>RAM</cell><cell>Total</cell></row><row><cell>Power consumption [W]</cell><cell>105</cell><cell>97</cell><cell>8</cell><cell>105</cell><cell>180</cell><cell>140</cell><cell>-</cell><cell>-</cell><cell>190</cell><cell>189</cell><cell>43</cell><cell>232</cell></row><row><cell>Data set</cell><cell>ALICE 3</cell><cell></cell><cell></cell><cell></cell><cell>ALICE 3</cell><cell></cell><cell></cell><cell></cell><cell>LHCb 1</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>CPU and RAM power consumption for the accelerator and sw-based algorithms</figDesc><table><row><cell></cell><cell cols="4">CPU power consumption (W)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">RAM power consumption (W)</cell><cell></cell></row><row><cell></cell><cell cols="2">Compression ratio Lowest</cell><cell>Highest</cell><cell></cell><cell cols="2">zlib level 2</cell><cell>AHA</cell><cell cols="3">Throughput (Gb/s) Lowest</cell><cell cols="2">Highest</cell><cell>zlib level 2</cell><cell>AHA</cell></row><row><cell>ALICE 2</cell><cell cols="2">Lowest zstd level 21 Highest</cell><cell cols="3">zlib level 2 zstd level 2</cell><cell>zlib level 4</cell><cell>AHA</cell><cell cols="2">Lowest zlib level 2</cell><cell cols="3">Highest xz level 5 zlib level 2</cell><cell>zlib level 4</cell><cell>AHA</cell></row><row><cell>ALICE 2</cell><cell>lz4 82</cell><cell cols="2">xz level 5 101</cell><cell></cell><cell>99</cell><cell>+1%</cell><cell>58</cell><cell cols="2">zstd level 21 4</cell><cell>lz4</cell><cell>10</cell><cell></cell><cell>-29% 4</cell><cell>7</cell></row><row><cell>ATLAS 1</cell><cell cols="2">1.07 zstd level 21 1.83</cell><cell cols="2">1.42 zstd level 2</cell><cell></cell><cell>1.44</cell><cell>1.4</cell><cell>0.2</cell><cell>zlib level 2</cell><cell>45.2</cell><cell cols="2">4.5 zstd level 21</cell><cell>3.2</cell><cell>76</cell></row><row><cell>ATLAS 1</cell><cell>snappy</cell><cell cols="2">xz level 5</cell><cell></cell><cell></cell><cell>+1%</cell><cell></cell><cell cols="2">zstd level 21</cell><cell cols="3">snappy xz level 1,5</cell><cell>-21%</cell></row><row><cell></cell><cell>1.36 85</cell><cell>1.92</cell><cell>101</cell><cell>1.65</cell><cell>98</cell><cell>1.67</cell><cell>1.67 59</cell><cell>0.3</cell><cell>5</cell><cell>63.4</cell><cell>8</cell><cell>6.2</cell><cell>4.9 5</cell><cell>77 7</cell></row><row><cell>CALGARY CALGARY</cell><cell>snappy xz level 5</cell><cell cols="3">xz level 5 zstd level 2</cell><cell></cell><cell>+7%</cell><cell></cell><cell cols="2">xz level 5 zlib level 2</cell><cell cols="3">snappy xz level 5</cell><cell>-29%</cell></row><row><cell></cell><cell>1.81 88</cell><cell>185</cell><cell>103</cell><cell>2.73</cell><cell>97</cell><cell>2.93</cell><cell>2.86 56</cell><cell>0.6</cell><cell>4</cell><cell>53.5</cell><cell>11</cell><cell>9.3</cell><cell>6.6 4</cell><cell>81 6</cell></row><row><cell>ENWIK9 ENWIK9</cell><cell cols="4">snappy zstd level 21 zstd level 21 zstd level 2</cell><cell></cell><cell>+7%</cell><cell></cell><cell cols="2">zstd level 21 zlib level 2</cell><cell cols="3">snappy bzip2 level 9</cell><cell>-28%</cell></row><row><cell></cell><cell>1.79</cell><cell>3.95</cell><cell></cell><cell>2.72</cell><cell></cell><cell>2.92</cell><cell>2.87</cell><cell>0.2</cell><cell></cell><cell>50.4</cell><cell cols="2">9.6 xz level 5</cell><cell>6.9</cell><cell>79</cell></row><row><cell>LHCb 1</cell><cell>lz4</cell><cell cols="2">xz level 5</cell><cell></cell><cell></cell><cell>&lt;+1%</cell><cell></cell><cell cols="2">zstd level 21</cell><cell>lz4</cell><cell cols="2">zstd level 21</cell><cell>-13%</cell></row><row><cell></cell><cell>1.05 84</cell><cell>1.37</cell><cell>101</cell><cell>1.17</cell><cell>98</cell><cell>1.18</cell><cell>1.18</cell><cell>0.3</cell><cell>5</cell><cell>46.2</cell><cell>9</cell><cell>4.7</cell><cell>4.1 5</cell><cell>75 7</cell></row><row><cell>PROTEINS LHCb 1</cell><cell>snappy snappy</cell><cell cols="3">zstd level 21 zlib level 2</cell><cell></cell><cell>+3%</cell><cell></cell><cell cols="2">zstd level 21 zlib level 2</cell><cell>lz4</cell><cell cols="2">xz level 1,5</cell><cell>-37%</cell></row><row><cell></cell><cell>1.23</cell><cell>3.35</cell><cell></cell><cell>2.04</cell><cell></cell><cell>2.11</cell><cell>2.12</cell><cell>0.1</cell><cell></cell><cell>78.2</cell><cell cols="2">8.1 zstd level 21</cell><cell>5.1</cell><cell>78</cell></row><row><cell>SILESIA</cell><cell>snappy 72</cell><cell cols="2">xz level 5 99</cell><cell></cell><cell>99</cell><cell>+5%</cell><cell>59</cell><cell cols="2">zstd level 21 4</cell><cell cols="2">snappy 9</cell><cell></cell><cell>-26% 4</cell><cell>8</cell></row><row><cell>PROTEINS</cell><cell cols="2">1.93 zstd level 21 4.11</cell><cell cols="2">2.74 zlib level 2</cell><cell></cell><cell>2.87</cell><cell>2.94</cell><cell>0.3</cell><cell>zlib level 2</cell><cell>66.4</cell><cell cols="2">9.8 xz level 5</cell><cell>7.3</cell><cell>79</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">zstd level 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>84</cell><cell></cell><cell>101</cell><cell></cell><cell cols="2">101</cell><cell>58</cell><cell></cell><cell>4</cell><cell></cell><cell>11</cell><cell></cell><cell>4</cell><cell>7</cell></row><row><cell>SILESIA</cell><cell>snappy</cell><cell></cell><cell cols="2">zstd level 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>zlib level 2</cell><cell></cell><cell cols="2">bzip2 level 9</cell><cell></cell></row><row><cell></cell><cell cols="2">zstd level 21</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">xz level 1,5</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">zstd level 21</cell><cell></cell></row><row><cell></cell><cell cols="4">Memory bandwidth (GiB/s) 94 102</cell><cell>98</cell><cell></cell><cell>557</cell><cell></cell><cell cols="3">Percentage of reads 5 8</cell><cell></cell><cell>5</cell><cell>7</cell></row><row><cell></cell><cell>Lowest</cell><cell cols="2">Highest</cell><cell></cell><cell cols="2">zlib level 2</cell><cell>AHA</cell><cell></cell><cell>Lowest</cell><cell></cell><cell cols="2">Highest</cell><cell>zlib level 2</cell><cell>AHA</cell></row><row><cell>ALICE 2</cell><cell>zlib level 2</cell><cell cols="3">xz level 5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>lz4/snappy</cell><cell></cell><cell cols="2">xz level 1</cell><cell></cell></row><row><cell></cell><cell>1</cell><cell cols="2">33</cell><cell></cell><cell>1</cell><cell></cell><cell>30</cell><cell></cell><cell>49%</cell><cell></cell><cell cols="2">77%</cell><cell>53%</cell><cell>58%</cell></row><row><cell>ATLAS 1</cell><cell>zlib level 2</cell><cell cols="3">xz level 5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>zlib level 2</cell><cell></cell><cell cols="2">xz level 1</cell><cell></cell></row><row><cell></cell><cell>2</cell><cell cols="2">25</cell><cell></cell><cell>2</cell><cell></cell><cell>30</cell><cell></cell><cell>50%</cell><cell></cell><cell cols="2">68%</cell><cell>50%</cell><cell>59%</cell></row><row><cell>CALGARY</cell><cell>zlib level 2</cell><cell cols="3">xz level 5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">zstd level 21</cell><cell cols="2">xz level 1</cell><cell></cell></row><row><cell></cell><cell>2</cell><cell cols="2">38</cell><cell></cell><cell>2</cell><cell></cell><cell>22</cell><cell></cell><cell>50%</cell><cell></cell><cell cols="2">69%</cell><cell>64%</cell><cell>59%</cell></row><row><cell>ENWIK9</cell><cell>zlib level 2</cell><cell cols="3">zstd level 21</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>xz level 5</cell><cell></cell><cell cols="2">xz level 1</cell><cell></cell></row><row><cell></cell><cell>2</cell><cell cols="2">28</cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell>56%</cell><cell></cell><cell cols="2">69%</cell><cell>61%</cell><cell>57%</cell></row><row><cell>LHCb 1</cell><cell>zlib level 2</cell><cell cols="3">xz level 5</cell><cell></cell><cell></cell><cell>29</cell><cell></cell><cell>zlib level 2</cell><cell></cell><cell cols="2">xz level 1</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">zstd level 21</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2</cell><cell cols="2">30</cell><cell></cell><cell>2</cell><cell></cell><cell>30</cell><cell></cell><cell>45%</cell><cell></cell><cell cols="2">72%</cell><cell>45%</cell><cell>56%</cell></row><row><cell>PROTEINS</cell><cell>zlib level 2</cell><cell cols="3">xz level 5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>lz4/snappy</cell><cell></cell><cell cols="2">xz level 1</cell><cell></cell></row><row><cell></cell><cell>2</cell><cell cols="2">40</cell><cell></cell><cell>2</cell><cell></cell><cell>29</cell><cell></cell><cell>51%</cell><cell></cell><cell cols="2">83%</cell><cell>65%</cell><cell>61%</cell></row><row><cell>SILESIA</cell><cell>zlib level 2</cell><cell cols="3">xz level 5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">zstd level 21</cell><cell cols="2">xz level 1</cell><cell></cell></row><row><cell></cell><cell>2</cell><cell cols="2">22</cell><cell></cell><cell>2</cell><cell></cell><cell>28</cell><cell></cell><cell>55%</cell><cell></cell><cell cols="2">72%</cell><cell>65%</cell><cell>63%</cell></row></table><note><p><p><p><p>Note: For the sw-based algorithms only the best and worst performing algorithms are listed. Additionally, zlib level 2 and 4 are listed to be able to compare to the accelerator and to see the difference between those two levels.</p>TA B L E 7 Memory bandwidth and percentage for the accelerator and sw-based algorithms</p>Note: For the sw-based algorithms only the best and worst performing algorithms are listed. Additionally, zlib level 2 is listed to be able to compare to the accelerator.</p>TA B L E 8</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Only a subset of sw-based compression algorithms was chosen: the one with maximum ()zstd level 21, xz level 5) and minimum (snappy) compression ratio, and zlib level 2 and zstd level 2. They achieved the following ratios: snappy achieved 1.85, zlib level 2 achieved 2.65, zstd level 2 achieved 2.76, zstd level 21 achieved 3.26 and xz level 5 achieved 3.38. The accelerator achieved a geometric mean compression ratio of 3.02 significantly surpassing the cited works which achieved a compression ratio of 2.17 with 24 Gb/s 15 and 2.03</figDesc><table /></figure>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DATA AVAILABILITY STATEMENT</head><p>The data that support the findings of this study are available from the corresponding author upon reasonable request.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ORCID</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Laura Promberger</head><p>https://orcid.org/0000-0003-0127-6255</p><p>Holger Fr?ning https://orcid.org/0000-0001-9562-0680</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUPPORTING INFORMATION</head><p>Additional supporting information may be found online in the Supporting Information section at the end of this article. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How to cite this article</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Super-scalar RAM-CPU cache compression</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zukowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Heman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Boncz</surname></persName>
		</author>
		<ptr target="https://ieeexplore.ieee.org/document/1617364" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Data Engineering (ICDE&apos;06)</title>
		<meeting>the 22nd International Conference on Data Engineering (ICDE&apos;06)<address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="59" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Performance of compressed inverted list caching in search engines. Paper presented at: Proceeding of the 17th International Conference on World Wide Web 2008, WWW&apos;08</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Suel</surname></persName>
		</author>
		<idno type="DOI">10.1145/1367497</idno>
		<ptr target="https://dl.acm.org/doi/proceedings/10.1145/1367497" />
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="387" to="396" />
			<pubPlace>Beijing China</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SIMD compression and the intersection of sorted integers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lemire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Boytsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kurz</surname></persName>
		</author>
		<idno type="DOI">10.1002/spe.2326</idno>
		<ptr target="https://doi.org/10.1002/spe.2326" />
	</analytic>
	<monogr>
		<title level="j">Softw Pract Exper</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="723" to="749" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient parallel lists intersection and index compression algorithms using graphics processing units</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.14778/2002974.2002975</idno>
		<ptr target="https://doi.org/10.14778/2002974.2002975" />
	</analytic>
	<monogr>
		<title level="j">Proc VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="470" to="481" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Performance and energy consumption of lossless compression/decompression utilities on mobile computing platforms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Milenkovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dzhagaryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burtscher</surname></persName>
		</author>
		<ptr target="https://www.computer.org/csdl/proceedings/mascots/2013/12OmNvStcUq" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 IEEE 21st International Symposium on Modelling, Analysis and Simulation of Computer and Telecommunication Systems</title>
		<meeting>the 2013 IEEE 21st International Symposium on Modelling, Analysis and Simulation of Computer and Telecommunication Systems<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="volume">2013</biblScope>
			<biblScope unit="page" from="254" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A solution for transmitting and displaying UHD 3D raw videos using lossless compression</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">G</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aquino</forename><surname>J?nior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Silva</forename><surname>Neto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Souza</forename><surname>Filho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th Brazilian Symposium on Multimedia and the Web WebMedia &apos;13</title>
		<meeting>the 19th Brazilian Symposium on Multimedia and the Web WebMedia &apos;13<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A comparison of general-purpose FOSS compression techniques for efficient communication in cooperative multi-robot tasks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Portugal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Rocha</surname></persName>
		</author>
		<ptr target="https://www.ieee-ras.org/component/rseventspro/event/404-icinco-2014-11th-international-conferenceon-informatics-in-control-automation-and-robotics" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 11th International Conference on Informatics in Control, Automation and Robotics (ICINCO) INSTICC</title>
		<meeting>the 2014 11th International Conference on Informatics in Control, Automation and Robotics (ICINCO) INSTICC<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="136" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Brotli: a general-purpose data compressor</title>
		<author>
			<persName><forename type="first">J</forename><surname>Alakuijala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farruggia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ferragina</surname></persName>
		</author>
		<idno type="DOI">10.1145/3231935</idno>
		<ptr target="https://doi.org/10.1145/3231935" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans Inf Syst</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cosmological particle data compression in practice</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zeyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ahrens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Heitmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Habib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the In Situ Infrastructures on Enabling Extreme-Scale Analysis and Visualization ISAV&apos;17</title>
		<meeting>the In Situ Infrastructures on Enabling Extreme-Scale Analysis and Visualization ISAV&apos;17<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="12" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A scalable high-bandwidth architecture for lossless compression on FPGAs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hauck</surname></persName>
		</author>
		<ptr target="https://www.computer.org/csdl/proceedings/fccm/2015/12OmNwE9OtK" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE 23rd Annual International Symposium on Field-Programmable Custom Computing Machines</title>
		<meeting>the 2015 IEEE 23rd Annual International Symposium on Field-Programmable Custom Computing Machines<address><addrLine>Vancouver, BC</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page" from="52" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Database analytics acceleration using FPGAs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sukhwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thoennes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Parallel Architectures and Compilation Techniques PACT</title>
		<meeting>the 21st International Conference on Parallel Architectures and Compilation Techniques PACT<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="volume">2012</biblScope>
			<biblScope unit="page" from="411" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving performance and lifetime of solid-state drives using hardware-accelerated compression</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Arvind</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCE.2011.6131148</idno>
		<ptr target="https://doi.org/10.1109/TCE.2011.6131148" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Consumer Electron</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1732" to="1739" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Acceleration of software algorithms using hardware/software co-design techniques</title>
		<author>
			<persName><forename type="first">M</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Whelan</surname></persName>
		</author>
		<idno type="DOI">10.1016/S1383-7621(96)00071-9</idno>
		<ptr target="https://doi.org/10.1016/S1383-7621(96)00071-9" />
	</analytic>
	<monogr>
		<title level="j">J Syst Archit</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="697" to="707" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hardware acceleration in the IBM PowerEN processor: architecture and performance</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Heil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lindberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Toussi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vanderwiel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Parallel Architectures and Compilation Techniques</title>
		<meeting>the 21st International Conference on Parallel Architectures and Compilation Techniques<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="volume">2012</biblScope>
			<biblScope unit="page" from="389" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gzip on a chip: high performance lossless data compression on FPGAs using OpenCL. Paper presented at: Proceedings of the IWOCL &apos;14</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Abdelfattah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hagiescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Singh</surname></persName>
		</author>
		<ptr target="https://www.computer.org/csdl/proceedings/fccm/2018/13xI8A66zF5" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on OpenCL</title>
		<meeting>the International Workshop on OpenCL<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2013">2013. 2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">High-throughput lossless compression on tightly coupled CPU-FPGA platforms</title>
		<author>
			<persName><forename type="first">W</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 IEEE 26th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)</title>
		<meeting>the 2018 IEEE 26th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)<address><addrLine>Boulder, CO</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="37" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Energy efficient canonical Huffman encoding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Matai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kastner</surname></persName>
		</author>
		<ptr target="https://www.computer.org/csdl/proceedings/asap/2014/12OmNAndiiU" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 IEEE 25th International Conference on Application-Specific Systems, Architectures and Processors</title>
		<meeting>the 2014 IEEE 25th International Conference on Application-Specific Systems, Architectures and Processors<address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="volume">2014</biblScope>
			<biblScope unit="page" from="202" to="209" />
		</imprint>
	</monogr>
	<note>Paper presented at</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">bzip2: home</title>
		<author>
			<persName><forename type="first">J</forename><surname>Seward</surname></persName>
		</author>
		<ptr target="https://www.sourceware.org/bzip2/" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Lz4 LZ4 -extremely fast compression</title>
		<ptr target="http://www.lz4.org" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<ptr target="https://github.com/google/snappy" />
		<title level="m">google/snappy GitHub -google/snappy: a fast compressor/decompressor</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Collin</forename><forename type="middle">L</forename><surname>Utils</surname></persName>
		</author>
		<ptr target="https://tukaani.org/xz/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName><surname>Zlib Home</surname></persName>
		</author>
		<author>
			<persName><surname>Site</surname></persName>
		</author>
		<ptr target="https://www.zlib.net/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">RFC 1951 -DEFLATE compressed data format specification version 1</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Deutsch</surname></persName>
		</author>
		<ptr target="https://tools.ietf.org/html/rfc1951" />
		<imprint/>
	</monogr>
	<note>3; 2020</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Zstandard -Real-time data compression algorithm</title>
		<ptr target="https://facebook.github.io/zstd/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">About the test data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mahoney</surname></persName>
		</author>
		<ptr target="http://mattmahoney.net/dc/textdata.html" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Pizza&amp;Chili corpus -compressed indexes and their testbeds</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ferragina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Navarro</surname></persName>
		</author>
		<ptr target="http://pizzachili.dcc.uchile.cl/texts/protein/" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Dij</forename><surname>Abel</surname></persName>
		</author>
		<author>
			<persName><surname>Corpora</surname></persName>
		</author>
		<ptr target="http://www.data-compression.info/Corpora/index.html" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">GitHub -RRZE-HPC/likwid: performance monitoring and benchmarking suite</title>
		<author>
			<persName><surname>Rrze-Hpc</surname></persName>
		</author>
		<ptr target="https://github.com/RRZE-HPC/likwid" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<ptr target="https://www.intel.com/content/www/us/en/architecture-and-technology/int%el-quick-assist-technology-overview.html" />
		<title level="m">Intel Intel Quick Assist Technology (Intel QAT) improves data center</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Group AP AHA products group</title>
		<ptr target="http://www.aha.com/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
