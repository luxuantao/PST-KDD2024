<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Anomaly Detection and Localization in Crowded Scenes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2013-06-12">12 June 2013.</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Weixin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Engineering Department</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>9500 Gilman Drive, La Jolla</addrLine>
									<postCode>92093</postCode>
									<settlement>San Diego</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Vijay</forename><surname>Mahadevan</surname></persName>
							<email>vijay.mahadevan@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Engineering Department</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>9500 Gilman Drive, La Jolla</addrLine>
									<postCode>92093</postCode>
									<settlement>San Diego</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
							<email>nvasconcelos@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Engineering Department</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>9500 Gilman Drive, La Jolla</addrLine>
									<postCode>92093</postCode>
									<settlement>San Diego</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="middle">V</forename><surname>Mahadevan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Embassy Golf Links Business Park</orgName>
								<orgName type="institution">Yahoo! Labs</orgName>
								<address>
									<postCode>560071</postCode>
									<settlement>Bengaluru</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Anomaly Detection and Localization in Crowded Scenes</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2013-06-12">12 June 2013.</date>
						</imprint>
					</monogr>
					<idno type="MD5">954DB7629909DBC3C9FE02330F0C6729</idno>
					<idno type="DOI">10.1109/TPAMI.2013.111</idno>
					<note type="submission">received 15 Apr. 2012; revised 26 Feb. 2013; accepted 14 May 2013;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Video analysis</term>
					<term>surveillance</term>
					<term>anomaly detection</term>
					<term>crowded scene</term>
					<term>dynamic texture</term>
					<term>center-surround saliency</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The detection and localization of anomalous behaviors in crowded scenes is considered, and a joint detector of temporal and spatial anomalies is proposed. The proposed detector is based on a video representation that accounts for both appearance and dynamics, using a set of mixture of dynamic textures models. These models are used to implement 1) a center-surround discriminant saliency detector that produces spatial saliency scores, and 2) a model of normal behavior that is learned from training data and produces temporal saliency scores. Spatial and temporal anomaly maps are then defined at multiple spatial scales, by considering the scores of these operators at progressively larger regions of support. The multiscale scores act as potentials of a conditional random field that guarantees global consistency of the anomaly judgments. A data set of densely crowded pedestrian walkways is introduced and used to evaluate the proposed anomaly detector. Experiments on this and other data sets show that the latter achieves state-ofthe-art anomaly detection results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>S URVEILLANCE video is extremely tedious to monitor when events that require follow-up have very low probability. For crowded scenes, this difficulty is compounded by the complexity of normal crowd behaviors. This has motivated a surge of interest in anomaly detection in computer vision <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. However, this effort is hampered by general difficulties of the anomaly detection problem <ref type="bibr" target="#b9">[10]</ref>. One fundamental limitation is the lack of a universal definition of anomaly. For crowds, it is also infeasible to enumerate the set of anomalies that are possible in a given surveillance scenario. This is compounded by the sparseness, rarity, and discontinuity of anomalous events, which limit the number of examples available to train an anomaly detection system.</p><p>One common solution to these problems is to define anomalies as events of low probability with respect to a probabilistic model of normal behavior. This enables a statistical treatment of anomaly detection, which conforms with the intuition of anomalies as events that deviate from the expected <ref type="bibr" target="#b9">[10]</ref>. However, it introduces a number of challenges. First, it makes anomalies dependent on the scale at which normalcy is defined. A normal behavior at a fine visual scale may be perceived as highly anomalous when a larger scale is considered, or vice versa. Hence, normalcy models must be defined at multiple scales. Second, different tasks may require different models of normalcy. For instance, a detector of freeway speed limit violations will rely on normalcy models based on speed features. On the other hand, appearance is more important for the detection of carpool lane violators, i.e., single-passenger vehicles in carpool lanes. Third, crowded scenes require normalcy models robust to complex scene dynamics, involving many independently moving objects that occlude each other in complex ways, and can have low resolution.</p><p>In result, anomaly detection can be extremely challenging. While this has motivated a great diversity of solutions, it is usually quite difficult to objectively compare different methods. Typically, these combine different representations of motion and appearance with different graphical models of normalcy, which are usually tailored to specific scene domains. Abnormalities are themselves defined in a somewhat subjective form, sometimes according to what the algorithms can detect. In some cases, different authors even define different anomalies on common data sets. Finally, experimental results can be presented on data sets of very different characteristics (e.g., traffic intersection versus subway entrance), frequently proprietary, and with widely varying levels of crowd density.</p><p>In this work, we propose an integrated solution to all these problems. We start by introducing normalcy models that jointly account for the appearance and dynamics of complex crowd scenes. This is done by resorting to a video representation based on dynamic textures (DTs) <ref type="bibr" target="#b10">[11]</ref>. This representation is then used to design models of normalcy over both space and time. Temporal normalcy is modeled with a mixture of DTs <ref type="bibr" target="#b11">[12]</ref> (MDT) and enables the detection of behaviors that deviate from those observed in the past. Spatial normalcy is measured with a discriminant saliency detector <ref type="bibr" target="#b12">[13]</ref> based on MDTs, enabling the detection of behaviors that deviate from those of the surrounding crowd. The integration of spatial and temporal normalcy with respect to either appearance or dynamics leads to a flexible model of normalcy, applicable to the detection of anomalies of relevance to various surveillance tasks.</p><p>To address the scale problem, MDTs are learned at multiple spatial scales. This is done with an efficient hierarchical model, where layers of MDTs with successively larger regions of video support are learned recursively. The local measures of spatial and temporal abnormality are then integrated into a globally coherent anomaly map, by probabilistic inference. This is implemented with a conditional random field (CRF), whose single-node potentials are classifiers of local measures of spatial and temporal abnormality, collected over a range of spatial scales. They are complemented by a novel set of interaction potentials, which account for spatial and temporal context, and integrate anomaly information across the visual field.</p><p>Finally, to address the difficulties of empirical evaluation of anomaly detectors on crowded scenes, we introduce a data set of video from walkways in the campus of University of California, San Diego (UCSD), depicting crowds of varying densities. The data set contains 98 video sequences, and five well-defined abnormal categories. These are not "synthetic," or "staged," but abnormal events that occur naturally, for example, bicycle riders that cross pedestrian walkways. Ground truth is provided for abnormal events, as well as a protocol to evaluate detection performance.</p><p>The remainder of the paper is organized as follows: Section 2 reviews previous work on anomaly detection in computer vision. The problems of temporal and spatial anomaly detection in crowded scenes are discussed in Section 3. This is followed by the mathematical characterization of multiscale anomaly maps in Section 4, and the proposed CRF for integration of spatial and temporal anomalies across different spatial scales in Section 5. Finally, an extensive experimental evaluation is discussed in Section 6 and some conclusions are presented in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRIOR WORK</head><p>Recent advances in anomaly detection address event representation and globally consistent statistical inference. Contributions of the first type define features and models for the discrimination of normal and anomalous patterns. Models of normal and abnormal behavior are then learned from training data, and anomalies detected with a minimum probability of error decision rule. Although there are some exceptions <ref type="bibr" target="#b4">[5]</ref>, the distribution of abnormal patterns is usually assumed uniform, and abnormal events formulated as events of low probability under the model of normalcy.</p><p>One intuitive representation for event modeling is based on object trajectories. It is comprised of either explicitly or implicitly segmenting and tracking each object in the scene, and fitting models to the resulting object tracks <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. While capable of identifying abnormal behaviors of high-level semantics (e.g., unusual long-term trajectories), these procedures are both difficult and computationally expensive for crowded or cluttered scenes. A number of promising alternatives, which avoid processing individual objects, have been recently proposed. These include the modeling of motion patterns with histograms of pixel change <ref type="bibr" target="#b4">[5]</ref>, histograms of optical flow <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b19">[20]</ref>, or optical flow measures <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b0">[1]</ref>. Among these, <ref type="bibr" target="#b2">[3]</ref> models local optical flow with a mixture of probabilistic principal component analysis (PCA) models, <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b16">[17]</ref> draw inspiration from classical studies of crowd behavior <ref type="bibr" target="#b20">[21]</ref> to characterize flow with interaction features (e.g., social force model), and <ref type="bibr" target="#b0">[1]</ref> learns the representative flow of groups by clustering optical flow-based particle trajectories.</p><p>These approaches emphasize dynamics, ignoring anomalies of object appearance and, thus, anomalous behavior without outlying motion. Optical flow, pixel change histograms, or other classical background subtraction features are also difficult to extract from crowded scenes, where the background is by definition dynamic, there are lots of clutter, and occlusions. More complete representations account for both appearance and motion. For example, <ref type="bibr" target="#b1">[2]</ref> models temporal sequences of spatiotemporal gradients to detect anomalies in densely crowded scenes, <ref type="bibr" target="#b21">[22]</ref> declares as abnormal spatiotemporal patches that cannot be reconstructed from previous frames, and <ref type="bibr" target="#b22">[23]</ref> pools appearance and motion features over spatial neighborhoods, using a distance to the nearest spatially colocated feature vector among all training video clips, to quantify abnormality.</p><p>Object-based representations, based on location, blob shape, and motion <ref type="bibr" target="#b6">[7]</ref> or optical flow magnitude, gradients, location, and scale <ref type="bibr" target="#b8">[9]</ref>, have also been proposed. Other representations include a bag-of-words over a set of manually annotated event classes <ref type="bibr" target="#b23">[24]</ref>. Various methods have also been used to produce anomaly scores. While simple spatial filtering suffices for some applications <ref type="bibr" target="#b18">[19]</ref>, crowded scenes require more sophisticated graphical models and inference. For example, <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b0">[1]</ref> adopt Gaussian mixture models (GMM) to represent trajectories of normal behavior. Cong et al. <ref type="bibr" target="#b7">[8]</ref> and Zhao et al. <ref type="bibr" target="#b19">[20]</ref> learn a sparse basis and define unusual events as those that can only be reconstructed with either large error or the combination of a large number of basis vectors.</p><p>Contributions of the second type address the integration of local anomaly scores, which can be noisy, into a globally consistent anomaly map. The authors of <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b24">[25]</ref>, and <ref type="bibr" target="#b6">[7]</ref> guarantee temporally consistent inference by modeling normal temporal sequences with hidden Markov models (HMMs). While this enforces consistency along the temporal dimension, there have also been efforts to produce spatially consistent anomaly maps. For example, latent Dirichlet allocation (LDA) has been applied to force flow features, in the model of spatial crowd interactions of <ref type="bibr" target="#b3">[4]</ref>. On the other hand, <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b2">[3]</ref> rely on Markov random fields (MRF) to enforce global spatial consistency. In the realm of sparse representations, <ref type="bibr" target="#b19">[20]</ref> guarantees consistency of reconstruction coefficients over space and time by inclusion of smoothness terms in the underlying optimization problem. Finally, <ref type="bibr" target="#b8">[9]</ref> models object relationships, using Bayesian networks to implement occlusion reasoning.</p><p>It should be noted that most of these methods have not been tested on the densely crowded scenes considered in this work. It is unclear that many of them could deal with the complex motion and object interactions prevalent in such scenes. Furthermore, while most methods include some mechanism to encourage spatial and temporal consistency of anomaly judgments (MRF, LDA, etc.), the underlying decision rule tends to be either predominantly temporal (e.g., trajectories, GMMs, HMMs, or sparse representations learned over time) or spatial (e.g., interaction models) but is rarely discriminant with respect to both space and time. This makes it difficult to infer whether spatial or temporal modeling are critically important by themselves, or what benefits are gained from their joint modeling. Furthermore, the role of scale is rarely considered. These issues motivate the contributions of the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ANOMALY DETECTION</head><p>We start by proposing an anomaly detector that accounts for scene appearance and dynamics, spatial and temporal context, and multiple spatial scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Mathematical Formulation</head><p>A classical formulation of anomaly detection, which we adopt in this work, equates anomalies to outliers. A statistical model p X ðx x x xÞ is postulated for the distribution of a measurement X X X X under normal conditions. Abnormalities are defined as measurements whose probability is below a threshold under this model. This is equivalent to a statistical test of hypotheses:</p><p>. H 0 : x x x x is drawn from p X ðx x x xÞ; . H 1 : x x x x is drawn from an uninformative distribution p X ðx x x xÞ / 1. The minimum probability of error rule for this test is to reject the null hypothesis H 0 if p X ðx x x xÞ &lt; , where is the normalization constant of the uninformative distribution. As usual in the literature, we consider the problem of anomaly detection from localized video measurements x x x x, where x x x x is a spatiotemporal patch of small dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Spatial versus Temporal Anomalies</head><p>The normalcy model p X ðx x x xÞ can have both a temporal and a spatial component. Temporal normalcy reflects the intuition that normal events are recurrent over time, i.e., previous observations establish a contextual reference for normalcy judgments. Consider a highway lane where cars move with a certain orientation and speed. Bicycles or cars heading in the opposite direction are easily identified as abnormal because they give rise to observations x x x x substantially different from those collected in the past. In this sense, temporal normalcy detection is similar to background subtraction <ref type="bibr" target="#b25">[26]</ref>. A model of normal behavior is learned over time, and measurements that it cannot explain are denoted temporal anomalies.</p><p>Spatial normalcy reflects the intuition that some events that would not be abnormal per se are abnormal within a crowd. Since the crowd places physical or psychological constraints on individual behavior, behaviors feasible in isolation can have low probability in a crowd context. For example, while there is nothing abnormal about an ambulance that rides at 50 mph in a stretch of highway, the same observation within a highly congested highway is abnormal. Note that the only indication of abnormality is the difference between the crowd and the object at the time of the observation, not that the ambulance moves at 50 mph. Since the detection of such abnormalities is mostly based on spatial context, they are denoted spatial anomalies. Their detection does not depend on memory. Instead, it is based on a continuously evolving, instantaneously adaptive, definition of normalcy. In this sense, the detection of spatial anomalies can be equated to saliency detection <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Roles of Crowds and Scale</head><p>Most available background subtraction and saliency detection solutions are not applicable to crowded scenes, where backgrounds can be highly dynamic. In this case, it is not sufficient to detect variations of image intensity, or even optical flow, to detect anomalous events. Instead, normalcy models must rely on sophisticated joint representations of appearance and dynamics. In fact, even such models can be ineffective. Since crowds frequently contain distinct subentities, for example, vehicles or groups of people moving in different directions, anomaly detection requires modeling multiple video components of different appearance and dynamics. A model that has been shown successful in this context is the mixture of DTs <ref type="bibr" target="#b11">[12]</ref>. This is the representation adopted in this work.</p><p>Another challenging aspect of anomaly detection within crowds is scale. Spatial anomalies are usually detected at the scale of the smallest scene entities, typically people. However, a normal event at this scale may be anomalous at a larger scale, and vice versa. For example, while a child that rides a bicycle appears normal within a group of bicycle riding children, the group is itself anomalous in a crowded pedestrian sidewalk. Local anomaly detectors, with small regions of interest, cannot detect such anomalies. To address this, we represent crowded scenes with a hierarchy of MDTs that cover successively larger regions. This is done with a computationally efficient hierarchical model, where MDT layers are estimated recursively.</p><p>A similar challenge holds for temporal anomalies. While their detection is usually based on a small number of video frames, certain anomalies can only be detected over long time spans. For example, while it is normal for two pedestrian trajectories to converge or diverge at any point in time, a cyclical convergence and divergence is probably abnormal. Anomaly detection across time scales is, however, more complex than across spatial scales, due to constraints of instantaneous detection and implementation complexity. Since video has to be buffered before anomalies can be detected, large temporal windows imply long detection delays and storage of many video frames. Due to this, we do not consider multiple temporal scales in this work. A single scale is chosen, using acceptable values of delay and storage complexity, and used throughout our experiments. Note that, like their spatial counterparts, temporal anomaly maps are computed at multiple spatial scales. Hence, in what follows, the term "scale" refers to the spatial support of anomaly detection, for both spatial and temporal anomalies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">NORMALCY AND ANOMALY MODELING</head><p>In this section, we review the MDT model, discuss the design of temporal and spatial models of normalcy, and formulate the computation of anomaly maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Mixture of Dynamic Textures</head><p>The MDT models a sequence of video frames x x x x 1: ¼ ½x x x x 1 ; x x x x 2 ; . . . ; x x x x as a sample from one of K dynamic textures <ref type="bibr" target="#b10">[11]</ref>:</p><formula xml:id="formula_0">pðx x x x 1: Þ ¼ X K i¼1 i pðx x x x 1: jz ¼ iÞ:<label>ð1Þ</label></formula><p>The mixture components pðx x x x 1: jz ¼ iÞ are linear dynamic systems (LDS) defined by</p><formula xml:id="formula_1">s s s s tþ1 ¼ A z s s s s t þ n n n n t ; ð2aÞ x x x x t ¼ C z s s s s t þ m m m m t ;<label>ð2bÞ</label></formula><formula xml:id="formula_2">&amp;</formula><p>where Z is a multinomial random variable of parameters ð i ! 0; P i i ¼ 1Þ, which indexes the mixture component from which x x x x t is drawn. s s s s t is a hidden state variable that encodes scene dynamics, and x x x x t the vector of pixels in video frame t. A z ; C z are the transition and observation matrices of component z, whose initial condition is s s s s 1 $ N ð z ; S z Þ, and noise processes are defined by n n n n t $ N ð0; Q z Þ and m m m m t $ N ð0; R z Þ. The model parameters are learned by maximum-likelihood estimation (MLE) from a collection of video patches, with the expectation-maximization (EM) algorithm of <ref type="bibr" target="#b11">[12]</ref>, which is reviewed in Appendix A.1, which is available in the Computer Society Digital Library at http://doi.ieeecomputersociety.org/10.1109/TPAMI. 2013.111.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Temporal Anomaly Detection</head><p>Temporal anomaly detection is inspired by the popular background subtraction method of <ref type="bibr" target="#b25">[26]</ref>. This uses a GMM per image location to model the distribution of image intensities. Observations of low probability under these GMMs are declared foreground. For anomaly detection in crowds, the GMM is replaced by an MDT, and the pixel grid replaced by one of preset displacement. Grid locations define the center of video cells, from which video patches are extracted. The patches extracted from a subregion (group of cells) are used to learn an MDT, during a training phase, as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. After this phase, subregion patches of low probability under the associated MDT are considered anomalies. Given patch x x x x 1: , the distribution of the hidden state sequence s s s s 1 under the ith DT component, p SjX ðs s s s 1: jx x x x 1: ; z ¼ iÞ, is estimated with a Kalman filter and smoother <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, as discussed in Appendix A.2, available in the online supplemental material. The value of the temporal anomaly map at location l is the negative-log probability of the mostlikely state sequence for the patch at l:</p><formula xml:id="formula_3">T ðlÞ ¼ À log X K i¼1 i p À s s s s fig 1: ðlÞjz ¼ i Á " # ;<label>ð3Þ</label></formula><p>where s s s s fig 1: ðlÞ ¼ argmax s s s s1: pðs s s s 1: jx x x x ðlÞ; z ¼ iÞ. We note that this generalizes the mixture of PCA models of optical flow <ref type="bibr" target="#b2">[3]</ref>. The matrix C z of (2b) is a PCA basis for patches drawn from mixture component z, but the PCA decomposition reports to patch appearance, not optical flow. Patch dynamics are captured by the hidden state sequence s s s s 1: , which is a trajectory in PCA space. Hence, unlike mixtures of optical flow, the representation is temporally smooth. The joint representation of appearance and dynamics makes the MDT a better representation for crowd video than the mixture of PCA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Spatial Anomaly Detection</head><p>Spatial anomaly detection is inspired by previous work in saliency detection <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Saliency is defined in a centersurround manner. Given a set of features, salient locations are those of substantial feature contrast with their immediate surround. Spatial anomalies are then defined as locations whose saliency is above some threshold. In this work, we rely on the discriminant saliency criterion of <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Discriminant Saliency</head><p>Discriminant saliency formulates the saliency problem as a hypothesis test between two classes: a class of salient stimuli, and a background class of stimuli that are not salient. Two windows are defined at each scene location l: a center window W 1 l , with label CðlÞ ¼ 1, containing the location, and a surrounding annular window W 0 l , with label CðlÞ ¼ 0, containing background. A set of feature responses X are computed for each of the windows W c l , c 2 f0; 1g and SðlÞ, the saliency of location l, defined as the extent to which they discriminate between the two classes. This is quantified by the mutual information (MI) between feature responses and class label <ref type="bibr" target="#b12">[13]</ref>:</p><formula xml:id="formula_4">SðlÞ ¼ X 1 c¼0 fp CðlÞ ðcÞKL½p XjCðlÞ ðx x x xjcÞkp X ðx x x xÞg;<label>ð4Þ</label></formula><p>where p XjCðlÞ ðx x x xjcÞ are class-conditional densities and</p><formula xml:id="formula_5">KLðp q k Þ ¼ R X p X ðx x x xÞ log p X ðx x</formula><p>x xÞ q X ðx x x xÞ dx x x x the Kullback-Leibler (KL) divergence between p X ðx x x xÞ and q X ðx x x xÞ <ref type="bibr" target="#b29">[30]</ref>.</p><p>Locations of maximal saliency are those where the discrimination between center and surround can be made with highest confidence, i.e., where (4) is maximal. The discriminant saliency principle can be applied to many features <ref type="bibr" target="#b30">[31]</ref>. When X consists of optical flow, it generalizes the force flow model of <ref type="bibr" target="#b3">[4]</ref>, where saliency is defined as the difference between the optical flow at l and the average flow in its neighborhood (see <ref type="bibr">[4, (8)</ref>]). This is a simplified form of discriminant saliency, which replaces the MI of (4) by a difference to the mean background response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Center-Surround Saliency with MDTs</head><p>Optical flow methods provide a coarse representation of dynamics and ignore appearance. For background subtraction, this problem has been addressed with the combination of DTs and discriminant saliency <ref type="bibr" target="#b31">[32]</ref>. While using a more powerful representation than force flow, this method learns a single DT from both center and surround windows. This assumes a homogeneity of appearance and dynamics within the two windows that do not hold for crowds, where foregrounds and backgrounds can be quite diverse.</p><p>In this work, we adopt the MDT as the probability distribution p XjCðlÞ ðx x x x 1: jcÞ from which spatiotemporal patches x x x x 1 are drawn. We note that under assumptions of Gaussian initial conditions and noise, patches x x x x 1: drawn from a DT have a Gaussian probability distribution <ref type="bibr" target="#b32">[33]</ref>,</p><p>x x x x 1: $ N ð ; ÈÞ; ð5Þ whose parameters follow from those of the LDS (2). When the class-conditional distributions of the center and surround classes, c 2 f0; 1g, at location l are mixtures of K c DTs, it follows that</p><formula xml:id="formula_6">p XjCðlÞ ðx x x x 1: jcÞ ¼ X K c i¼1 c i N À x x x x 1: ; c i ; È c i Á ¼ X Kc i¼1 c i p i XjCðlÞ ðx x x x 1: jcÞ;<label>ð6Þ</label></formula><p>for c 2 f0; 1g. The marginal distribution is then</p><formula xml:id="formula_7">p X ðx x x x 1: Þ ¼ X 1 c¼0 ½p CðlÞ ðcÞp XjCðlÞ ðx x x x 1: jcÞ ¼ X 1 c¼0 p CðlÞ ðcÞ X Kc i¼1 c i N À x x x x 1: ; c i ; È c i Á ! ¼ X K 0 þK 1 i¼1 ! i N ðx x x x 1: ; i ; È i Þ ¼ X K0þK1 i¼1 ! i p i X ðx x x x 1: Þ;<label>ð7Þ</label></formula><p>and the saliency measure of (4) requires the KL divergence between ( <ref type="formula" target="#formula_6">6</ref>) and <ref type="bibr" target="#b6">(7)</ref>. This is problematic because there is no closed form solution for the KL divergence between two MDTs. However, because the MDT components are Gaussian, it is possible to rely on popular approximations to the KL divergence between Gaussian mixtures. We adopt the variational approximation of <ref type="bibr" target="#b33">[34]</ref>:</p><formula xml:id="formula_8">KLðp XjC kp X Þ % X i C i log P K C j C j exp À ÀKL À p i XjC p j XjC ÁÁ P K0þK1 j ! j exp À ÀKL À p i XjC p j X ÁÁ ( ) :<label>ð8Þ</label></formula><p>Each term of (8) contains a KL divergence between DTs, which can be computed in closed form <ref type="bibr" target="#b34">[35]</ref>. For example, for the terms in the denominator</p><formula xml:id="formula_9">KL À p i XjC p j X Á ¼ 1 2 log jÈ j j È C i þ Tr À È À1 j È C i Á þ C i À j 2 Èj À m " # ;<label>ð9Þ</label></formula><p>where m is the number of pixels per frame, and kz z z zk È ¼ z z z z T È À1 z z z z. Numerator terms are computed similarly. All computations can be performed recursively <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Spatial Anomaly Map</head><p>The spatial anomaly map is a map of the saliency SðlÞ at locations l. Given a location, this requires 1) learning MDTs from center and surround windows, and 2) computing a weighted average of these mixtures to obtain <ref type="bibr" target="#b6">(7)</ref>. Since learning MDTs per location is computationally prohibitive, we resort to the following approximation. A dense collection of overlapping spatiotemporal patches is first extracted from VðtÞ, a 3D video volume temporally centered at the current frame. A single MDT with K g mixture components, denoted</p><formula xml:id="formula_10">f g i ; È g i g K g i¼1</formula><p>, is learned from this patch collection. Each patch is then assigned to the mixture component of largest posterior probability. This segments the volume into superpixels, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>At location l, the MDTs of ( <ref type="formula" target="#formula_6">6</ref>) and ( <ref type="formula" target="#formula_7">7</ref>) are derived from the global mixture model. The DT components are assumed equal to those of the latter and only the mixing proportions are recomputed, using the ratio of pixels assigned to each component in the respective windows:</p><formula xml:id="formula_11">p XjCðlÞ ðx x x x 1: jcÞ ¼ X K g i¼1 P l2W c l M il P l2W c l 1 N À x x x x 1: ; g i ; È g i Á ;<label>ð10Þ</label></formula><p>for c 2 f0; 1g. M il ¼ 1 if l is assigned to mixture component i and 0 otherwise. The prior probabilities for center and surround, p C ðcÞ, are proportional to the ratio of volumes of center and surround windows. SðlÞ is computed with (4), using ( <ref type="formula" target="#formula_8">8</ref>) and <ref type="bibr" target="#b8">(9)</ref>. Note that the KL divergence terms in <ref type="bibr" target="#b7">(8)</ref> only require the computation of</p><formula xml:id="formula_12">K g 2 À Á</formula><p>KL divergences between the K g mixture components, and these are computed only once per frame because all mixture components are shared (i.e., the terms exp ðÀKLðp q k ÞÞ in (8) are fixed per frame). This procedure is repeated for every frame in the test video, as illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Multiscale Anomaly Maps</head><p>To account for anomalies at multiple spatial scales, we rely on a hierarchical mixture of dynamic textures (H-MDT). This is a model with various MDT layers, learned from regions of different spatial support. At the finest scale, a video sequence is divided into n L subregions (e.g., 5 Â 8 subregions). n L MDT models fM M M M i g n L i¼1 are then learned from patches extracted from each of the subregions. At the coarsest scale, the whole visual field is represented with a global MDT. This results in a hierarchy of MDT models ffM M M M 1 i g n 1 i¼1 ; . . . ; M M M M L 1 g, where M M M M s j , the jth model at scale s, is learned from subregion R s j . The hierarchy of support windows ffR 1 i g n 1 i¼1 ; . . . ; R L g resembles the spatial pyramid structure of <ref type="bibr" target="#b35">[36]</ref>. H-MDT models can be learned efficiently with the hierarchical expectation-maximization (H-EM) algorithm of <ref type="bibr" target="#b36">[37]</ref>. Rather than collecting patches anew from larger regions, it estimates the models at a given layer directly from the parameters of the MDT models at the layer of immediately higher resolution.</p><p>For anomaly detection, each model is applied to the corresponding window. This produces L anomaly maps, fT 1 ; . . . ; T L g, as illustrated in Fig. <ref type="figure" target="#fig_2">3</ref>. A hierarchy of spatial anomaly maps, fS 1 ; . . . ; S L g is also computed. For all s, the computation of S s relies on a global mixture model M M M M. The mixing proportions of (10) are computed using surround windows of size identical to fR s i g and center windows of constant size, as summarized in Algorithm 1 (see Appendix B for all algorithms, available in the online supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">GLOBALLY CONSISTENT ANOMALY MAPS</head><p>In this section, we introduce a layer of statistical inference to fuse anomaly information across time, space, and scale in a globally consistent manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Discriminative Model</head><p>The anomaly maps of the previous section span space, time, and spatial scale. Being derived from local measurements, they can be noisy. A principled framework is required to 1) integrate anomaly scores from the individual maps, 2) eliminate noise, and 3) guarantee spatiotemporal consistency of anomaly judgments throughout the visual field. For this, we rely on a conditional random field <ref type="bibr" target="#b37">[38]</ref> inspired by the discriminative random field (DRF) of <ref type="bibr" target="#b38">[39]</ref>. An anomaly label y i 2 fÀ1; 1g is defined at each location i in a set S of observation sites. Given a video clip x x x x, the conditional likelihood of observing a configuration of anomaly labels y y y y ¼ fy i ji 2 Sg is</p><formula xml:id="formula_13">P ðy y y yjx x x xÞ ¼ 1 Z exp ( X i2S Aðy i ; x x x xÞ þ X i2S " 1 jN i j X j2N i</formula><p>Iðy i ; y j ; x x x x; i; jÞ</p><formula xml:id="formula_14">#) ;<label>ð11Þ</label></formula><p>where Z is a partition function and N i the neighborhood of site i. The single-site and interaction potentials of (11),</p><formula xml:id="formula_15">Aðy i ; x x x xÞ ¼ log À y i w w w w T f f f f i Á ;<label>ð12Þ</label></formula><p>where ðxÞ ¼ ð1 þ e Àx Þ À1 is the sigmoid function, and Iðy i ; y j ; x x x x; i; jÞ</p><formula xml:id="formula_16">¼ y i y j Á v v v v T ðf f f f i ; f f f f j ; i; jÞ ð<label>13Þ</label></formula><p>are based on a feature vector f f f f i that concatenates the spatial and temporal anomaly scores of site i at the L spatial scales, plus a bias term (set to 1):</p><formula xml:id="formula_17">f f f f i ¼ Â 1; T 1<label>ðiÞ</label></formula><p>; . . . ; T L ðiÞ; S 1 ðiÞ; . . . ; S L ðiÞ Ã T : ð14Þ w w w w; v v v v are parameter vectors and a compound feature:</p><formula xml:id="formula_18">ðf f f f i ; f f f f j ; i; jÞ ¼ e ÀjiÀjj expðÀh h h h i;j Þ;<label>ð15Þ</label></formula><p>where ji À jj is the euclidean distance between sites i; j, and expðÀh h h h i;j Þ the entry-wise exponential of Àh h h h i;j . The vector h h h h i;j contains the diagonal entries of ðf</p><formula xml:id="formula_19">f f f i À f f f f j Þðf f f f i À f f f f j Þ T .</formula><p>The single-site potential of (12) reflects the anomaly belief at site i. Using it alone, i.e., without <ref type="bibr" target="#b12">(13)</ref>, <ref type="bibr" target="#b10">(11)</ref> is a logistic regression model. In this case, the detection of each anomaly is based on information from site i exclusively. The addition of the interaction potential of (13) enables the model to take into account information from site i's neighborhood N i . This smoothes the single-site prediction, encouraging consistency of neighboring labels. The interaction potential can be interpreted as a classifier that predicts whether two neighboring sites have the same label. Note that because f f f f contains anomaly scores at different spatial scales, h h h h i;j (or i;j ) accounts for the similarity between the two observations in anomaly spaces of different scale (i.e., under different spatial normalcy contexts). The interaction potentials adaptively modulate the intensity of intersite smoothing according to these similarity measures (and how they are weighted by v v v v). The parameters w w w w and v v v v encode the relative importance of different features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Online CRF Filter</head><p>The model of <ref type="bibr" target="#b10">(11)</ref> requires inference over the entire video sequence. This is not suitable for online applications. An online version can be implemented by conditioning the anomaly label y y y y ðÞ at time on 1) observations for t , and 2) anomaly labels for t &lt; , leading to P À y y y y ðÞ jfy y y y ðtÞ g À1 t¼1 ; fx x x x ðtÞ g t¼1 ; Â Á</p><formula xml:id="formula_20">¼ 1 Z exp ( X i2S " A À y<label>ðÞ</label></formula><formula xml:id="formula_21">i ; x x x x ðÞ Á þ 1 jN S S i j X j2N S S i I S S À y<label>ðÞ</label></formula><formula xml:id="formula_22">i ; y j ; x x x x ðÞ ; i; j Á þ 1 N T T i X k2N T T i I T T À y<label>ðÞ</label></formula><formula xml:id="formula_23">i ; y k ; x x x x; i; k Á #) ;<label>ð16Þ</label></formula><p>where S is the set of observations at time (pixels of the current frame). Two neighborhoods are defined per location i: spatial N S S i (N S S i S ) and temporal N T T i (N T T i fS t g À1 t¼1 ). The graphical model is shown at the top of Fig. <ref type="figure" target="#fig_3">4</ref>, and these neighborhoods at the bottom. The parameters Â Â ¼ fw w w w; v v v v T T ; v v v v S S ; T T ; S S g are estimated during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Learning</head><p>Both ( <ref type="formula" target="#formula_14">11</ref>) and ( <ref type="formula" target="#formula_23">16</ref>) can be learned with standard optimization techniques, such as gradient descent or the Broyden-Fletcher-Goldfarb-Shanno (BFGS) method. To improve generalization, the model is regularized with a Gaussian prior of standard deviation , for all parameters. Given N independent training samples fx x x x ðnÞ ; y y y y ðnÞ g N n¼1 , the gradients of the regularized log-likelihood with respect to w w w w, v v v v, and are @ @w w w w log ÀÈ y y y y ðnÞ É N n¼1 j</p><formula xml:id="formula_24">È x x x x ðnÞ É N n¼1 Á ¼ X N n¼1 ( X i2S À À y ðnÞ i w w w w T f f f f ðnÞ i Á y ðnÞ i f f f f ðnÞ i À IE X i2S À À y i w w w w T f f f f ðnÞ i Á y i f f f f ðnÞ i ! ) À 1 2</formula><p>w w w w w w w w;</p><formula xml:id="formula_25">@ @v v v v log p ÀÈ y y y y ðnÞ É N n¼1 j È x x x x ðnÞ É N n¼1 Á ¼ X N n¼1 ( X i2S 1 jN i j X j2N i À e ÀjiÀjj y ðnÞ i y ðnÞ j exp À Àh h h h<label>ð17Þ</label></formula><formula xml:id="formula_26">i;j ÁÁ ! À IE " X i2S 1 jN i j X j2N i À e ÀjiÀjj y i y j exp À Àh h h h<label>ðnÞ</label></formula><formula xml:id="formula_27">i;j ÁÁ !#) À 1 2 v v v v v v v v;<label>ðnÞ</label></formula><p>and @ @ log p fy y y y ðnÞ g</p><formula xml:id="formula_29">N n¼1 fx x x x ðnÞ g N n¼1 ¼ X N n¼1 X i2S 1 jN i j X j2N i À ÀI À y<label>ðnÞ</label></formula><formula xml:id="formula_30">i ; y<label>ðnÞ</label></formula><formula xml:id="formula_31">j ; x x x x ðnÞ ; i; j Á ji À jj Á 8 &lt; : þ IE X i2S 1 jN i j X j2N i À Iðy i ; y j ; x x x x ðnÞ ; i; j Á ji À jj Á 0 @ 1 A 2 4 3 5 9 = ; À 1 2 ;<label>ð19Þ</label></formula><p>where the expectation is evaluated with distribution pðYjX; Â ÂÞ. The conditional expectations of ( <ref type="formula" target="#formula_25">17</ref>)-( <ref type="formula" target="#formula_31">19</ref>) require evaluation of the partition function Z, a problem known to be NP-hard. As is common in the literature, this difficulty is avoided by estimating expectations through sampling.</p><p>Although sampling methods such as Markov chain Monte Carlo (MCMC) can converge to the true distribution, this usually requires many iterations. Since the procedure must be repeated per gradient ascent step, these methods are impractical. On the other hand, approximations such as contrastive divergence minimization (which runs MCMC a limited number of times with specific starting points) have been shown to be successful for vision applications <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>. We adopt these approximations for CRF learning. This leverages the fact that, denoting any of the parameters w w w w; v v v v T T ; v v v v S S ; T T ; S S by , the partial gradients of ( <ref type="formula" target="#formula_25">17</ref>)-( <ref type="formula" target="#formula_31">19</ref>) are @ @ log p ÀÈ y y y y ðnÞ É N n¼1 j</p><formula xml:id="formula_32">È x x x x ðnÞ É N n¼1 ; Â Á ¼ X N n¼1 È F @</formula><p>À y y y y ðnÞ ; x x x x ðnÞ Á À IE ðYjX;Â ÂÞ Â F @ ðy y y y; x x x x ðnÞ Þ ÃÉ À 1 2</p><p>;</p><formula xml:id="formula_33">ð20Þ</formula><p>where F @ ðy y y y; x x x xÞ is the sum of the terms in the summations of ( <ref type="formula" target="#formula_25">17</ref>), <ref type="bibr" target="#b17">(18)</ref>, or ( <ref type="formula" target="#formula_31">19</ref>) that depend on . Contrastive divergence approximates the intractable conditional expectation IE ðYjX;ÂÞ ½F @ ðy y y y; x x x x ðnÞ Þ by F @ ðŷ y y y; x x x x ðnÞ Þ, where ŷ y y y is the "evil twin" of the ground-truth label field y y y y ðnÞ <ref type="bibr" target="#b40">[41]</ref>. ŷ y y y is drawn by MCMC, using the inference procedure discussed in Section 5.2.2, the current parameter estimates, and the ground-truth labels y y y y ðnÞ as a starting point. Given the estimate of the partial gradients, the gradient ascent rule for parameter updates reduces to þ X N n¼1 À F @ À y y y y ðnÞ ; x x x x ðnÞ Á À F @ À ŷ y y y ðnÞ ; x x x x ðnÞ ÁÁ À 1 2</p><formula xml:id="formula_34">" # ;<label>ð21Þ</label></formula><p>where is a learning rate. In our implementation, this rule is initialized with</p><formula xml:id="formula_35">v v v v T T ¼ v v v v S S ¼ 1 and T T ¼ S S ¼ 0.</formula><p>The initial value of w w w w is learned, assuming a logistic regression model <ref type="formula" target="#formula_23">16</ref>)), with the procedure of <ref type="bibr" target="#b42">[43]</ref>.</p><formula xml:id="formula_36">(v v v v T T ¼ v v v v S S ¼ 0 in (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Inference</head><p>The inference problem is to determine the most likely anomaly prediction y y y y ? for a query frame x x x x ðÞ , given previous predictions fy y y y ðtÞ g À1 t¼1 , and observations fx x x x ðtÞ g t¼1 :</p><p>y y y y ? ¼ argmax y y y y log p À y y y yj È y y y y ðtÞ É À1 t¼1 ;</p><formula xml:id="formula_37">È x x x x ðtÞ É t¼1 ; Â Á ¼ argmax y y y y X i2S A À y i ; x x x x ðÞ Á þ 1 jN i j X j2N i</formula><p>Iðy i ; y j ; x x x x; i; jÞ</p><formula xml:id="formula_38">! :<label>ð22Þ</label></formula><p>Again, exact inference is intractable. We rely on Gibbs sampling to approximate the optimal prediction. This consists of drawing labels from the conditional distribution: </p><formula xml:id="formula_39">p À y i j È x x</formula><p>where F i ðfx x x x ðtÞ g t¼1 ; fy y y y ðtÞ g À1 t¼1 ; y y y y Ài ; y i ; ÂÞ is the sum of potential functions that depend on site i (i.e., its "Markov blanket"):</p><formula xml:id="formula_41">F i ÀÈ x x x x ðtÞ É t¼1 ;</formula><p>È y y y y ðtÞ É À1 t¼1 ; y y y y Ài ; y i ; Â</p><formula xml:id="formula_42">Á ¼ A À y i ; x x x x ðÞ Á þ 1 jN i j X j2N i I À y i ; y j ; È x x x x ðtÞ É t¼1 ; i; j Á þ X j:i2N j 1 jN j j I À y j ; y i ; È x x x x ðtÞ É t¼1 ; j; i Á ;<label>ð24Þ</label></formula><p>and Z Ài the corresponding partition function:</p><formula xml:id="formula_43">Z Ài ¼ X y 0 i exp h F i ÀÈ x x x x ðtÞ É t¼1 ;</formula><p>È y y y y ðtÞ É À1 t¼1 ; y y y y Ài ; y 0</p><formula xml:id="formula_44">i ; Â Á i :<label>ð25Þ</label></formula><p>The procedure is detailed in Algorithms 2 and 3, available in the online supplemental material, where we present the online CRF filter used to estimate the label field. During learning, the filter is initialized with the ground-truth labels (y y y y 0 ¼ y y y y ðÞ ). During testing, this initialization relies on the predictions of the single-site classifiers</p><formula xml:id="formula_45">(v v v v T T ¼ v v v v S S ¼ 0).</formula><p>In our implementation, the filter is run for N s ¼ 10 iterations. Again, the complete anomaly detection procedure is summarized in Algorithm 4, available in the online supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>In this section, we introduce a new data set and an experimental protocol for evaluation of anomaly detection in crowded environments and use it to evaluate the proposed anomaly detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">UCSD Pedestrian Anomaly Data Set</head><p>In the literature, anomaly detection has frequently been evaluated by visual inspection <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b2">[3]</ref>, or with coarse ground truth, for example, frame-level annotation of abnormal events <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b0">[1]</ref>. This does not completely address the anomaly detection problem, where it is usually desired to localize anomalies in both space and time. To enable this, we introduce a data set 1 of crowd scenes with precisely localized anomalies and metrics for the evaluation of their detection. The data set consists of video clips recorded with a stationary camera mounted at an elevation, overlooking pedestrian walkways on the UCSD campus. The crowd density in the walkways is variable, ranging from sparse to very crowded. In the normal setting, the video contains only pedestrians. Abnormal events are due to either 1) the circulation of nonpedestrian entities in the walkways, or 2) anomalous pedestrian motion patterns. Commonly occurring anomalies include bikers, skaters, small carts, and people walking across a walkway or in the surrounding grass. A few instances of wheelchairs are also recorded. All abnormalities occur naturally, i.e., they were not staged or synthesized for data set collection. The data set is organized into two subsets, corresponding to the two scenes of Fig. <ref type="figure" target="#fig_4">5</ref>. The first, denoted "Ped1," contains clips of 158 Â 238 pixels, which depict groups of people walking toward and away from the camera, and some amount of perspective distortion. The second, denoted "Ped2," has spatial resolution of 240 Â 360 pixels and depicts a scene where most pedestrians move horizontally. The video footage of each scene is sliced into clips of 120-200 frames. A number of these (34 in Ped1 and 16 in Ped2) are to be used as training set for the condition of 1. Available from http://www.svcl.ucsd.edu/projects/anomaly/dataset.html. normalcy. The test set contains clips (36 for Ped1 and 12 for Ped2) with both normal (around 5,500) and abnormal (around 3,400) frames. The abnormalities of each set are summarized in Table <ref type="table">1</ref>.</p><p>Frame-level ground-truth annotation, indicating whether anomalies occur within each frame, and manually collected pixel-level binary anomaly masks, which identify the pixels containing anomalies, are available per test clip. We note that this includes ground truth on Ped1 contributed by Anti c and Ommer <ref type="bibr" target="#b8">[9]</ref>, and supersedes the ground truth available on an earlier version of this work <ref type="bibr" target="#b42">[43]</ref>. We denote the current ground truth by "full annotation" and the previous one by "partial annotation." Unless otherwise noted, the results of the subsequent sections correspond to the full annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation Methodology</head><p>Two criteria are used to evaluate anomaly detection accuracy: a frame-level criterion and a pixel-level criterion. Both are based on true-positive rates (TPR) and falsepositive rates (FPRs), denoting "an anomalous event" as "positive" and "the absence of anomalous events" as "negative." A frame containing anomalies is denoted a positive, otherwise a negative. The true and false positives under the two criteria are:</p><p>. Frame-level criterion. An algorithm predicts which frames contain anomalous events. This is compared to the clip's frame-level ground-truth anomaly annotations to determine the number of true-and false-positive frames. . Pixel-level criterion. An algorithm predicts which pixels are related to anomalous events. This is compared to the pixel-level ground-truth anomaly annotation to determine the number of true-positive and false-positive frames. A frame is a true positive if 1) it is positive and 2) at least 40 percent of its anomalous pixels are identified; a frame is a false positive if it is negative and any of its pixels are predicated as anomalous. The two measures are combined into a receiver operating characteristic (ROC) curve of TPR versus FPR:</p><formula xml:id="formula_46">TPR ¼ # of true-positive frame # of positive frame ; FPR ¼ # of false-positive frame # of negative frame :</formula><p>Performance is also summarized by the equal error rate (EER), the ratio of misclassified frames at which FPR ¼ 1 À TPR, for the frame-level criterion, or rate of detection (RD), i.e., 1-EER, for the pixel-level criterion.</p><p>Note that, although widely used in the literature, the frame-level criterion only measures temporal localization accuracy. This enables errors due to "lucky co-occurrences" of prediction errors and true abnormalities. For example, it assigns a perfect score to an algorithm that identifies a single anomaly at a random location of a frame with anomalies. The pixel-level criterion is much stricter and more rigorous. By evaluating both the temporal and spatial accuracy of the anomaly predictions, it rules out these "lucky co-occurrences." We believe that the pixel-level criterion should be the predominant criterion for evaluation of anomaly detection algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Experimental Setup</head><p>Unless otherwise noted, observation sites are a video sublattice with spatial interval of four pixels and temporal interval of five frames. Temporal anomaly maps rely on patches of 13 Â 13 Â 15 pixels. The temporal extent of 15 frames provides a reasonable compromise between the ability to detect anomalies and the delay (1.5 s) and storage (15 video frames) required for anomaly detection. To minimize computation, patches of variance smaller than 500 are discarded. <ref type="foot" target="#foot_0">2</ref> Temporal H-MDT models are learned from fine to coarse scale. At the finer scale, there are 6 Â 10 windows R 1 i on Ped1 (8 Â 11 for Ped2), each covering a 41Â41 pixel area and overlapping by 25 percent with each of its four neighbors. An MDT of five components is learned per window. At coarser spatial scales, an MDT is estimated from the MDTs of the four regions that it covers at the immediately finer resolution. Each estimated MDT has one more component than its ancestor MDTs. Overall, there are 10 scales in Ped1 and 11 in Ped2. Spatial anomaly maps use a 31Â31 center window and surround windows of size equivalent to R s i . For segmentation, 7 Â 7 Â 10 patches are extracted from the 40 frames surrounding that under analysis. There are five DT components at all levels of the spatial hierarchy. Both temporal and spatial MDTs have an eight-dimensional state space. The sensitivity of the proposed detector to some of these parameters is discussed in Appendix C.2, available in the online supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Descriptor Comparison</head><p>The first experiment evaluated the benefits of MDT-based over optical flow descriptors. The optical flow descriptors considered were the local motion histogram (LMH) of <ref type="bibr" target="#b18">[19]</ref>, the force flow descriptor of <ref type="bibr" target="#b3">[4]</ref>, and the mixture of optical flow models (MPPCA) of <ref type="bibr" target="#b2">[3]</ref>. LMH uses statistics of local motion, and is representative of traditional background subtraction representations, force flow is a descriptor for spatial anomaly detection, and MPPCA a temporal anomaly detector. For the MDT, only the anomaly maps of finest temporal and coarsest spatial scale were considered here. Since the goal was to compare descriptors, the high-level components of the models in which they were proposed, for example, the LDA of <ref type="bibr" target="#b3">[4]</ref>, the MRF of <ref type="bibr" target="#b2">[3]</ref>, and the proposed CRF, were not used. Instead, anomaly predictions were smoothed with a simple 20 Â 20 Â 10 Gaussian filter. Anomaly predictions were generated by thresholding the filtered anomaly maps and ROC curves by varying thresholds.</p><p>The performance of the different descriptors, under both the frame-level (EER) and pixel-level (RD) criteria (using both full and partial annotation in Ped1), is summarized in Table <ref type="table" target="#tab_1">2</ref>. The corresponding ROC curves are presented in Appendix C.1 (Fig. <ref type="figure" target="#fig_2">13</ref>), available in the online supplemental material. Examples of detected anomalies are shown in Fig. <ref type="figure" target="#fig_5">6</ref>. Under the frame-level criterion, temporal MDT has the best performance in both scenes. Spatial MDT performs worse than others in Ped1 but ranks second in Ped2. However, for the more precise pixel-level criterion, spatial MDT is the top or second best performer. In this case, both MDTs significantly outperform all optical flow descriptors. The gap between corresponding competitors (e.g., temporal MDT versus MPPCA or LMH, spatial MDT versus force flow) is of at least 10 percent RD. These results show that there is a definite benefit to the joint representation of appearance and dynamics of the MDT. This is not totally surprising, given the limitations of optical flow. First, the brightness constancy assumption is easily violated in crowded scenes, where stochastic motion and occlusions prevail. Second, optical flow measures instantaneous displacement, while the DT is a smooth motion representation with extended temporal support. Finally, while optical flow is a bandpass measure, which eliminates most of the appearance information, the DT models both appearance and dynamics. The last two properties are particularly important for crowded scenes, where objects occlude and interact in complicated manners.</p><p>Overall, although optical flow can signal fast moving anomalous subjects, it leads to too many false positives in regions of complex motion, occlusion, and so on. More interesting is the lack of advantage for either spatial or temporal anomaly detection, both among MDT maps and prior techniques (no clear advantage to either force flow or MPPCA). In fact, as shown in Fig. <ref type="figure" target="#fig_5">6</ref>, temporal and spatial anomalies tend to be different objects. This suggests the combination of the two strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Scale and Globally Consistent Prediction</head><p>We next investigated the benefits of information fusion across space and scale, with the proposed CRF. We started with a single-scale description (S-MDT), using only the anomaly maps at finest temporal and coarsest spatial scales, i.e., a 3D feature per site. We next considered a multiscale description, using the whole H-MDT. In both cases, inference was performed with logistic regression, i.e., the interaction term of ( <ref type="formula" target="#formula_23">16</ref>) turned off, and the Gaussian filter of the previous section. In each trial, the logistic classifier was trained by Newton's method <ref type="bibr" target="#b41">[42]</ref>. Finally, we considered the full blown CRF, denoted CRF filter. The dimensions of the spatial and temporal CRF neighborhoods were set to jN S S j ¼ 6, jN T T j ¼ 3. ROC curves were generated by varying the threshold for prediction.</p><p>Table <ref type="table" target="#tab_2">3</ref> presents a comparison of the three approaches. The corresponding ROC curves are shown in Appendix C.1 (Fig. <ref type="figure" target="#fig_3">14</ref>), available in the online supplemental material. Under the pixel-level criterion, the multiscale maps have higher accuracy than their single-scale counterparts, demonstrating the benefits of modeling anomalies in scale space (improvement of RD by as much as 11 percent). The CRF   filter further improves performance (improvement of RD by as much as 3 percent), demonstrating the gains of globally consistent inference. As shown in Fig. <ref type="figure">7</ref>, the visual improvements are even more substantial. 3 Simple filtering does not take into account interactions between neighboring sites and smooths the anomaly maps uniformly. On the other hand, the CRF adapts the degree of smoothing to the spatiotemporal structure of the anomalies, increasing the precision of anomaly localization. Note how, in Fig. <ref type="figure">7</ref>, the CRF-filter successfully excludes occluded but normally behaving pedestrians from anomaly regions. These improvements are not always captured by the frame-level criterion. In fact, there is little EER difference between S-MDT and H-MDT. The inconsistency between frame-and pixel-level results in Tables <ref type="table" target="#tab_1">2</ref> and<ref type="table" target="#tab_2">3</ref> shows that the former is not a good measure of anomaly detection performance. Henceforth, only the pixellevel criterion is used in the remaining experiments on this data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Anomaly Detection Performance</head><p>We next evaluated the performance of the complete anomaly detector. For this, we selected two detectors from the recent literature, with state-of-the-art performance for temporal <ref type="bibr" target="#b7">[8]</ref> and combined spatial and temporal anomaly detection <ref type="bibr" target="#b8">[9]</ref>. The RD of the various methods is summarized in Table <ref type="table" target="#tab_3">4</ref>, for both partial and full annotation. The corresponding ROC curves are shown in Fig. <ref type="figure">8</ref>. Table <ref type="table" target="#tab_3">4</ref> also presents the processing time per video frame of each method. Missing entries indicate unavailable results for the particular data set and/or annotation type. A discussion of the detection errors made by the detector is given in Appendix C.3, available in the online supplemental material. On Ped1, the temporal component of the proposed detector substantially outperforms the temporal detector of <ref type="bibr" target="#b7">[8]</ref>. A multiple-scale temporal anomaly map with CRF filtering increases the 46 percent RD 4 of <ref type="bibr" target="#b7">[8]</ref> to 52 percent. A similar implementation of the spatial anomaly detector (a multiple-scale map plus CRF filtering) achieves 58 percent. Combining both maps and multiple spatial scales further improves the RD to 65 percent. Computationally, the proposed detector is also much more efficient. For implementations on similar hardware (see footnotes of Table <ref type="table" target="#tab_3">4</ref>), it requires 1.11 s/frame, as compared to the 3.8 s/frame reported for <ref type="bibr" target="#b7">[8]</ref>.</p><p>Like the proposed detector, the Bayesian video parsing (BVP) of <ref type="bibr" target="#b8">[9]</ref> combines spatial and temporal anomaly detection, using a more complex video representation, parsing of the video to extract all the objects in the scene, a support vector machine classifier for detection of temporal anomalies, a graphical model with seven nodes per site (and multiple nonparametric models for location, scale, and velocity) for detection of spatial anomalies, and occlusion reasoning. This is an elegant solution, which achieves slightly better RD than the proposed detector (2 percent for full and 3 percent for partial annotation), but at substantially higher computational cost (5 to 10 times slower). We believe that when both accuracy and computation are considered, the proposed detector is a more effective solution. However, these results suggest that gains could be achieved by expanding the proposed CRF, as <ref type="bibr" target="#b8">[9]</ref> trades a much simpler representation of video dynamics (optical flow versus MDT) for more sophisticated inference. It would be interesting to consider CRF extensions with some of the properties of the graphical model of <ref type="bibr" target="#b8">[9]</ref>, namely, explicit occlusion reasoning. This is left for subsequent research.  3. More results at http://www.svcl.ucsd.edu/projects/anomaly/ results.html.</p><p>4. These numbers refer to partial annotation, the only available for <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">Role of Context in Anomaly Judgments</head><p>We next investigated the impact of normalcy context in anomaly judgments. For temporal anomalies, context is determined by the subregion size: As the latter increases, temporal models become more global. Fig. <ref type="figure" target="#fig_7">9</ref> shows that the scale of normalcy context significantly impacts anomaly scores. For example, the two cyclists on the left-most columns of the figure are missed at small scales but detected by the more global models. On the other hand, a leftward heading pedestrian in the third column has high anomaly score at the finest scale but is not anomalous in larger contexts. In summary, no single context is effective for all scenes. Due to the stochastic arrangements of people within crowds, two crowds of the same size can require different context sizes. In general, the optimal size depends on the crowd configuration and the anomalous event.</p><p>A similar observation holds for spatial anomalies, where context is set by the size of the surround window. For example, in the fourth column of Fig. <ref type="figure" target="#fig_7">9</ref>, the subject walking on the grass is very salient when compared to her immediate neighbors, and anomaly detection benefits from a narrower context. For larger contexts, she becomes less unique than a man that walks in the direction opposite to his neighbors. On the other hand, the cart and bike of the last column only pop out when the surround window is large enough to cover some pedestrians. In summary, anomalies depend strongly on scene context, and this dependence can vary substantially from scene to scene. It is, thus, important to fuse anomaly information across spatial scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.8">Performance on Other Benchmark Data Sets</head><p>The detection of anomalous events in crowded scenes can be evaluated in a few data sets other than UCSD. These have various limitations in terms of size, saliency of the anomalies, evaluation criteria, and so on. They are discussed in this section where, for completeness, we also present the results of the proposed anomaly detector.</p><p>UMN. The UMN data set 5 contains three escape scenes. Normal events depict individuals wandering around or organized in groups. Abnormal events depict a crowd escaping in panic. Each scene contains several normal-abnormal events (e.g., seconds of normalcy followed by a short abnormal event). The main limitations of this data set are that 1. it is relatively small (scenes 1, 2, and 3 contain two, six, and three anomaly instances), 2. it has no pixel-level ground truth, 3. the anomalies are staged, and 4. it produces very salient changes in the average motion intensity of the scene. As a result, several methods achieve near perfect detection.</p><p>The proposed detector was based on 3 Â 3 subregions of size 180 Â 180 at the finest spatial scale and a 3-scale anomaly map for both the temporal and spatial components. One normal-abnormal instance of each scene was used to train the temporal normalcy model and CRF filter, and the remaining instances for testing. A comparison to previous results in the literature, under the frame-level criterion, is presented in Table <ref type="table" target="#tab_4">5</ref> and Fig. <ref type="figure" target="#fig_8">11</ref>. Due to the salient motion discontinuities, the temporal component (99.2 percent AUC) substantially outperforms the spatial component (97.9 percent). Nevertheless, the complete detector achieves the best performance (99.5 percent). This is nearly perfect, and comparable to the previous best results in the literature.</p><p>Subway. The Subway data set <ref type="bibr" target="#b18">[19]</ref> consists of two sequences recorded from the entrance (1 h and 36 min, 144,249 frames) and exit (43 min, 64,900 frames) of a  5. http://mha.cs.umn.edu/Movies/Crowd-Activity-All.avi. subway station. Normal behaviors include people entering and exiting the station; abnormal consist of people moving in the wrong direction (exiting the entrance or entering the exit) or avoiding payment. The main limitations of this data set are: 1) reduced number of anomalies, and 2) predictable spatial localization (entrance and exit regions). The original 512 Â 384 frames were down sampled to 320 Â 240, and 2 Â 3 subregions of size 90 Â 90, covering either the entrance or exit regions, were used at the finest spatial scale. A 3-scale anomaly map was computed for both spatial and temporal anomalies. Video patches were of size 15 Â 15 Â 15, and 10 min of video from each sequence was used to train the temporal normalcy model and CRF filters, while the remaining video was used for testing. Table <ref type="table" target="#tab_4">5</ref> and Fig. <ref type="figure" target="#fig_8">11</ref> present a comparison of the proposed detector against recently published results on this data set. Again, the temporal component outperforms its spatial counterpart, but the best performance is obtained by combination of both temporal and spatial anomaly maps (H-MDT CRF). This achieves the best result among all methods, outperforming the sparse reconstruction of <ref type="bibr" target="#b7">[8]</ref> and the local statistical aggregates of <ref type="bibr" target="#b22">[23]</ref>. Note that, for this data set, the gains in both AUC and EER are substantial.</p><p>U-turn. The U-turn data set <ref type="bibr" target="#b4">[5]</ref> consists of one video sequence (roughly 6,000 frames of size 360 Â 240) recorded by a static camera overlooking the traffic at a road intersection. The video is split into two clips of equal length for cross validation and anomalies consist of illegal vehicle motion at the intersection. The main limitations of this data set are: 1) the limited size, 2) absence of pixel-level ground truth, and 3) sparseness of the scenes. The enables the use of object-based operations, for example, tracking and analysis of object trajectories <ref type="bibr" target="#b4">[5]</ref>, which we do not exploit.</p><p>For temporal anomaly detection, MDTs were learned using 20 Â 20 Â 30 patches from 3 Â 4 subregions covering the intersection. This was the finest level of a 3-scale hierarchical model. For spatial anomaly detection, segmentation was computed with a 5-component MDT learned from 15 Â 15 Â 30 patches extracted from 45 consecutive frames. An observation lattice of step 15 Â 15 Â 10 was used to evaluate anomaly scores, and the neighborhood size of the CRF filter was 2. The performance of the detector is summarized in Table <ref type="table" target="#tab_4">5</ref> and Fig. <ref type="figure" target="#fig_8">11</ref>. Due to the sparsity of the scenes (not enough spatial context around cars making illegal turns to establish them as anomalous) the performance of the spatial anomaly detector is quite weak. However, the combination of the spatial and temporal anomaly maps again outperforms the temporal channel, achieving the best performance. Overall, the proposed detector has the best AUC on this data set. Examples of detected anomalies, for this and the other two data sets, are shown in Fig. <ref type="figure" target="#fig_9">10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this work, we proposed an anomaly detector that spans time, space, and spatial scale, using a joint representation of video appearance and dynamics and globally consistent inference. For this, we modeled crowded scenes with a hierarchy of MDT models, equated temporal anomalies to background subtraction, spatial anomalies to discriminant saliency, and integrated anomaly scores across time, space, and scale with a CRF. It was shown that the MDT representation substantially outperforms classical optical flow descriptors, that spatial and temporal anomaly detection are complementary processes, that there is a benefit to defining anomalies with respect to various normalcy contexts, i.e., in anomaly scale space, and that it is important to guarantee globally consistent inference across space, time and scale. We have also introduced a challenging anomaly detection data set, composed of complex scenes of pedestrian crowds, involving stochastic motion, complex occlusions, and object interactions. This data set provides both frame-level and pixel-level ground truth, and a protocol for the evaluation of anomaly detection algorithms. The proposed anomaly detector was shown effective on both this and a number of previous data sets. When compared to previous methods, it outperformed various state-of-the-art approaches, either in absolute performance or in terms of the tradeoff between anomaly detection accuracy and complexity.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Temporal anomaly detection. An MDT is learned per scene subregion, at training time. A temporal anomaly map is produced by measuring the negative log probability of each video patch under the MDT of the corresponding region.</figDesc><graphic coords="4,185.07,153.32,80.88,54.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Spatial anomaly detection using center-surround saliency with MDT models.</figDesc><graphic coords="5,449.21,149.81,78.74,52.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Computation of temporal anomaly maps with multiscale spatial supports using the H-MDT. MDTs of increasingly larger spatial support are estimated recursively, with the H-EM algorithm. Their application to a query video produces temporal anomaly maps based on supports of various spatial scales.</figDesc><graphic coords="6,117.05,158.57,82.10,54.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. CRF filter. Top: Graphical model. Bottom: Spatial and temporal neighborhoods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Exemplar normal/abnormal frames in Ped1 (top) and Ped2 (bottom). Anomalies (red boxes) include bikes, skaters, carts, and wheelchairs.</figDesc><graphic coords="8,375.29,107.69,79.46,53.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Anomaly predictions of temporal MDT, spatial MDT, MPPCA, force flow, and LMH (from left to right). Red regions are abnormal pixels. All predictions generated with thresholds such that the different approaches have similar FPR under frame-level protocol (these settings apply to all the subsequent figures unless otherwise stated).</figDesc><graphic coords="10,43.61,652.25,86.54,57.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Fig. 7. Examples of anomaly localization with Gaussian smoothing (in blue) and CRF filter (in red). The latter predicts more accurately the spatiotemporal support of anomalies in crowded regions, where occlusion is prevalent.</figDesc><graphic coords="11,40.49,52.37,97.34,64.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Impact of context on anomaly maps. First three columns: Temporal anomalies, cell coverage at different HMDT layers shown in blue. Last two columns: Spatial anomalies, example center (surround) windows shown in blue (light yellow).</figDesc><graphic coords="12,51.29,172.01,90.14,59.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 .</head><label>11</label><figDesc>Fig.11. ROC curves of frame-level criterion on the UMN (left), Subway (center), and U-turn (right) data sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Anomalies detected by H-MDT CRF on the UMN (left), Subway (center), and U-turn (right) data sets.</figDesc><graphic coords="13,36.89,105.77,72.02,54.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 Descriptor</head><label>2</label><figDesc>Performance on UCSD Anomaly Data SetÃ numbers outside/inside parentheses are results by full/partial annotation (same for the rest of the paper).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3 Filter</head><label>3</label><figDesc>Performance on the UCSD Anomaly Data Set</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4</head><label>4</label><figDesc>Performance of Various Methods (RD/Seconds per Frame) by Pixel-Level Criterion on UCSD Anomaly Data Set Implementation: } C/2.8-GHz CPU/2-GB RAM; \ C++ and Matlab (feature extraction and model inference)/2.6GHz CPU/2GB RAM; #Matlab/dualcore 2.7GHz CPU/8GB RAM.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 5 Anomaly</head><label>5</label><figDesc>Detection Performance in AUC/ERR (Percent)</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>This variance threshold is quite conservative, only eliminating regions of very little motion. For the data sets used in our experiments, this has not led to the elimination of any objects from further consideration. In other contexts, for example, scenes where objects are static for periods of time, this could happen. In this case, the threshold should be set to zero.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>a number of clips/number of anomaly instances. b some clips contain more than one type of anomaly.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weixin Li received the bachelor's degree from Tsinghua University, Beijing, China, in 2008, the MSc degree in electrical engineering from the University of California, San Diego, in 2011, and is currently working toward the PhD degree. His research interests primarily include computational vision and machine learning, with specific focus on visual analysis of human behavior, activity, and event, and models with latent variables and their applications. He is a student member of the IEEE. . For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/publications/dlib.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Chaotic Invariants of Lagrangian Particle Trajectories for Anomaly Detection in Crowded Scenes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Anomaly Detection in Extremely Crowded Scenes Using Spatio-Temporal Motion Pattern Models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nishino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Observe Locally, Infer Globally: A Space-Time MRF for Detecting Abnormal Activities with Incremental Updates</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Abnormal Crowd Behavior Detection Using Social Force Model</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Abnormal Events Detection Based on Spatio-Temporal Co-Occurences</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Benezeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jodoin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosenberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning Object Motion Patterns for Anomaly Detection and Improved Object Detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Basharat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gritai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Video Behavior Profiling for Anomaly Detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="893" to="908" />
			<date type="published" when="2008-05">May 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sparse Reconstruction Cost for Abnormal Event Detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Video Parsing for Abnormality Detection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Anti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Anomaly Detection: A Survey</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dynamic Textures</title>
		<author>
			<persName><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chiuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="109" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modeling, Clustering, and Segmenting Video with Mixtures of Dynamic Textures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="909" to="926" />
			<date type="published" when="2008-05">May 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Decision-Theoretic Saliency: Computational Principles, Biological Plausibility, and Implications for Neurophysiology and Psychophysics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="239" to="271" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning Patterns of Activity Using Real-Time Tracking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Stauffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Grimson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="747" to="757" />
			<date type="published" when="2000-08">Aug. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning Semantic Scene Models by Object Classification and Trajectory Clustering</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fusion of Multiple Tracking Algorithms for Robust People Tracking</title>
		<author>
			<persName><forename type="first">N</forename><surname>Siebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision</title>
		<meeting>European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Abnormal Detection Using Interaction Energy Potentials</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Anomalous Video Event Detection Using Spatiotemporal Context</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Tsaftaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="323" to="333" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robust Real-Time Unusual Event Detection Using Multiple Fixed-Location Monitors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reinitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="555" to="560" />
			<date type="published" when="2008-03">Mar. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Online Detection of Unusual Events in Videos via Dynamic Sparse Coding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Social Force Model for Pedestrian Dynamics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Helbing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Molna ´r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Rev. E</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="4282" to="4286" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Detecting Irregularities in Images and in Video</title>
		<author>
			<persName><forename type="first">O</forename><surname>Boiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="31" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Video Anomaly Detection Based on Local Statistical Aggregates</title>
		<author>
			<persName><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Detection and Explanation of Anomalous Activities: Representing Activities as Bags of Event N-Grams</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Batta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bobick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Isbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Coleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semi-Supervised Adapted HMMs for Unusual Event Detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gatica-Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mccowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adaptive Background Mixture Models for Real-Time Tracking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Stauffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Grimson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Model of Saliency-Based Visual Attention for Rapid Scene Analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998-11">Nov. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An Approach to Time Series Smoothing and Forecasting Using the EM Algorithm</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shumway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stoffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Time Series Analysis</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="253" to="264" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A Unifying Review of Linear Gaussian Models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="305" to="345" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Kullback</surname></persName>
		</author>
		<title level="m">Information Theory and Statistics</title>
		<imprint>
			<publisher>Dover Publications</publisher>
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On the Plausibility of the Discriminant Center-Surround Hypothesis for Visual Saliency</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vision</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Background Subtraction in Highly Dynamic Scenes</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Probabilistic Kernels for the Classification of Auto-Regressive Visual Processes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Approximating the Kullback Leibler Divergence between Gaussian Mixture Models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Olsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE Int&apos;l Conf. Acoustics, Speech, and Signal essing</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Efficient Computation of the Kl Divergence between Dynamic Textures</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<idno>SVCL- TR-2004-02</idno>
		<imprint>
			<date type="published" when="2004">2004</date>
			<pubPlace>San Diego</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Dept. of Electrical and Computer Eng., Univ. of California</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Clustering Dynamic Textures with the Hierarchical EM Algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Coviello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lanckriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th Int&apos;l Conf. Machine Learning</title>
		<meeting>18th Int&apos;l Conf. Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Discriminative Fields for Modeling Spatial Dependencies in Natural Images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multiscale Conditional Random Fields for Image Labeling</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carreira-Perpina ´n</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Training Products of Experts by Minimizing Contrastive Divergence</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">A Comparison of Numerical Optimizers for Logistic Regression</title>
		<author>
			<persName><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Microsoft Research</publisher>
		</imprint>
	</monogr>
	<note type="report_type">technical report</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Anomaly Detection in Crowded Scenes</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bhalodia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Example-Based Learning for View-Based Human Face Detection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Kah-Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yung</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="51" />
			<date type="published" when="1998-01">Jan. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
