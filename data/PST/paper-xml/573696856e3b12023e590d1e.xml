<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">935A84E9888870B79E3627EA42A389F5</idno>
					<idno type="DOI">10.1109/MSP.2015.2462851</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>dentifying a person by his or her voice is an important human trait most take for granted in natural human-to-human interaction/communication. Speaking to someone over the telephone usually begins by identifying who is speaking and, at least in cases of familiar speakers, a subjective verification by the listener that the identity is correct and the conversation can proceed. Automatic speaker-recognition systems have emerged as an important means of verifying identity in many e-commerce applications as well as in general business interactions, forensics, and law enforcement. Human experts trained in forensic speaker recognition can perform this task even better by examining a set of acoustic, prosodic, and linguistic characteristics of speech in a general approach referred to as structured listening. Techniques in forensic speaker recognition have been developed for many years by forensic speech scientists and linguists to help reduce any potential bias or preconceived understanding as to the validity of an unknown audio sample and a reference template from a potential suspect. Experienced researchers in signal processing and machine learning continue to develop automatic algorithms to effectively perform speaker recognition-with ever-improving performance-to the point where automatic systems start to perform on par with human listeners. In this article, we review the literature on speaker recognition by machines and humans, with an emphasis on prominent speaker-modeling techniques that have emerged in the last decade for automatic systems. We discuss different aspects of automatic systems, including voice-activity detection (VAD), features, speaker models, standard evaluation data sets, and performance metrics. Human speaker recognition is discussed in two parts-the first part involves forensic speaker-recognition methods, and the second illustrates how a naïve listener performs this task from a neuroscience perspective. We conclude this review with a comparative study of human versus machine speaker recognition and attempt to point out strengths and weaknesses of each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IntroductIon</head><p>Speaker recognition and verification have gained increased visibility and significance in society as speech technology, audio content, and e-commerce continue to expand. There is an ever-increasing need to search for audio materials, and searching based on speaker identity is a growing interest. With emerging technologies such as Watson, IBM's supercomputer <ref type="bibr" target="#b0">[1]</ref>, which can compete with expert human players in the game of "Jeopardy," and Siri <ref type="bibr" target="#b1">[2]</ref>, Apple's powerful speech-recognition-based personal assistant, it is not hard to imagine a future when handheld devices will be an extension of our identity-highly intelligent, sympathetic, and fully functional personal assistants, which will not only understand the meaning of what we say but also recognize and track us by our voice or other identifiable traits.</p><p>As we increasingly realize how much sensitive information our personal handheld devices can contain, it will become critical that effective biometric authentication be an integral part of access to information and files contained on the device, with a potential range of public/private access. Because speech is the most natural means of human communication, these devices will unavoidably lean toward automatic voice-based authentication in addition to other forms of biometrics. Apple's recent iPhone models have already introduced fingerprint scanners, reflecting the industry trend. The latest Intel technology on laptops employs face recognition as the password for access. Our digital personal assistant, in theory, could also replace most forms of traditional key locks as well for our home and vehicles, again making security of such a personal device more important.</p><p>Apart from personal authentication for access control, speaker recognition is an important tool in law enforcement, national security, and forensics in general. Because of widespread availability, cell phones have become the primary means of communication for the general public, and, unfortunately, also for criminals. Unlike the domain of personal authentication for personal files/ information access, these individuals usually do not want to be recognized. In such cases, many criminals may attempt to alter their voice to prevent them from being identified. This introduces additional challenges for developers of speaker-recognition technology-"Is the participant a willing individual in being assessed?" In law enforcement, any voice recorded as part of evidence may be disguised or even synthesized, to obscure recognition, adding to the difficulty of being recognized. Over a number of years, forensic speech scientists have devised different strategies to overcome these difficulties.</p><p>Interestingly, humans routinely recognize individuals by their voices with striking accuracy, especially when the degree of familiarity with the subject is high (i.e., close acquaintances or public figures). Many times, even a short nonlinguistic queue, such as a laugh, is enough for us to recognize a familiar person <ref type="bibr" target="#b2">[3]</ref>. On the other hand, it is also common knowledge that we cannot recognize a once-heard voice very easily and sometimes have difficulty in recognizing familiar voices over the phone. With these ideas in mind, a naïve person may wonder what exactly makes speaker recognition difficult and why is it a topic of such rigorous research.</p><p>From the discussion so far, it is safe to say that speaker recognition can be accomplished in three ways.</p><p>■ We can recognize familiar voices with considerable ease without any conscious training. This form of speaker recognition can be termed naïve speaker recognition. ■ In forensic investigations, speech samples from a telephone call are often compared to recordings of potential suspects (i.e., from a phone threat, emergency 911 call, or known criminal). In these cases, trained listeners are involved in systematically comparing the speech samples to provide an informed decision concerning their similarities. We would classify this as forensic speaker recognition.</p><p>■ Finally, we have automatic speaker recognition, where the complete speech analysis and decision-making process is performed using computer analysis. In both naïve and forensic speaker recognition, humans are directly involved in the process, even though some automatic or computer-assisted means may be used to supplement knowledge extraction for the purposes of comparison in the forensic scenario. However, it should be noted that both the forensic and automatic methods are highly systematic, and the procedures from both disciplines are technical in nature.</p><p>The forensic and automatic speaker-recognition research communities have developed various methods more or less independently for several decades. Conversely, naïve recognition is a natural ability of humans-which is, at times, very accurate and effective. Recent studies on brain imaging <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> have revealed many details on how we perform cognitive-based speaker recognition, which may inspire new directions for both automatic and forensic methods.</p><p>In this article, we present a tutorial review of the automatic speaker-recognition methods, especially those developed in the last decade, while providing the reader with a perspective on how humans also perform speaker recognition, especially by forensics experts and naïve listeners. The aim is to provide a discussion on the three classes of speaker recognition, highlighting the important similarities and differences among them. We emphasize how automatic techniques have evolved over time toward more current approaches. Many speech-processing techniques, such as Melscale filter-bank analysis and concepts in noise masking, are inspired by human auditory perception. Also, there are similarities between the methods used by forensic voice experts and automated systems-even though, in many cases, the research communities are separate. We believe that incorporating the perspective of speech perception by humans in this review, including highlights of both strengths and weaknesses in speaker recognition compared to machines, will help broaden the view of the reader and perhaps inspire new research directions in the area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SPEAKEr-rEcoGnItIon tASKS</head><p>First, to consider the overall research domain, it would be useful to clarify what is encompassed by the term speaker recognition, which consists of two alternative tasks: speaker identification and verification. In speaker identification, the task is to identify an unknown speaker from a set of known speakers. In other words, the goal is to find the speaker who sounds closest to the speech stemming from an unknown speaker within an audio sample. When all speakers within a given set are known, it is called a closed or in-set scenario. Alternatively, if the potential input test subject could also be from outside the predefined known speaker group, this becomes an open-set scenario, and, therefore, a world model or universal background model (UBM) <ref type="bibr" target="#b5">[6]</ref> is needed. This scenario is called open-set speaker recognition (also out-of-set speaker identification).</p><p>In speaker verification, an unknown speaker claims an identity, and the task is to verify if this claim is true. This essentially comes down to comparing two speech samples/utterances and deciding if they are spoken by the same speakers. In some methods, this is done by comparing the unknown sample against two alternative models, the claimed speaker model and a world model. In the forensic scenario, the general task is to identify the unknown speaker, who is suspected of a crime, but, in many instances, verification is also necessary.</p><p>Speaker recognition can be based on an audio stream that is text dependent or text independent. This is more relevant in authentication applications-where a claimed user says something specific, such as a password or personal identification number, to gain access to some resource/information. Throughout this article, the focus will be on text-independent speaker verification, especially in the treatment of automatic systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>cHALLEnGES In SPEAKEr rEcoGnItIon</head><p>Unlike other forms of biometrics (e.g., fingerprints, irises, facial features, gait, and hand geometry) <ref type="bibr" target="#b6">[7]</ref>, human speech is a performance biometric. Simply put, the identity information of the speaker is embedded (primarily) in how speech is spoken, not necessarily in what is being said (although in many voice forensic applications, it is also necessary to identify who said what within a multispeaker discussion). This makes speech signals prone to a large degree of variability. It is important to note that even the same person does not say the same words in exactly the same way every time (this is known as style shifting or intraspeaker variability) <ref type="bibr" target="#b7">[8]</ref>. Also, various recording devices and transmission methods commonly used exacerbate the problem. For example, we may find it difficult to recognize someone's voice through a telephone or maybe when the person is not healthy (i.e., has a cold) or is performing another task or speaking with a different level of vocal effort (i.e., whispering or shouting).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SOURCES OF VARIABILITY IN SPEAKER RECOGNITION</head><p>To consider variability, Figure <ref type="figure">1</ref> highlights a range of factors that can contribute to mismatch for speaker recognition. These can be partitioned based on three broad classes: 1) speaker based, 2) conversation based, and 3) technology based. Also, variability for speakers can be within speakers and across speakers.</p><p>■ Speaker-based variability sources: these reflect a range of changes in how a speaker produces speech and will affect system performance for speaker recognition. These can be thought of as intrinsic or within-speaker variability and include the following factors.</p><p>[FIG1] Sources of variability in speaker recognition.</p><p>• Situational task stress-the subject is performing some task while speaking, such as operating a vehicle (car, plane, truck, etc.), hands-free voice input (factory setting, emergency responders/fire fighters, etc.), which can include cognitive as well as physical task stress <ref type="bibr" target="#b8">[9]</ref>.</p><p>• Vocal effort/style-the subject alters his or her speech production from normal phonation, resulting in whispered <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, soft, loud, or shouted speech; the subject alters his or her speech production mechanism to speak effectively in the presence of noise <ref type="bibr" target="#b11">[12]</ref>, known as the Lombard effect; or the subject is singing versus speaking <ref type="bibr" target="#b12">[13]</ref>.</p><p>• Emotion-the subject is communicating his or her emotional state while speaking (e.g., anger, sadness, happiness, etc.) <ref type="bibr" target="#b13">[14]</ref>.</p><p>• Physiological-the subject has some illness or is intoxicated or under the influence of medication; this can include aging as well.</p><p>• Disguise-the subject intentionally alters his or her voice to circumvent the system. This can be by natural means (speaking in a harsh voice to avoid detection, mimicking another person's voice, etc.) or using a voice-conversion system. ■ Conversation-based/higher-level mode/language of speaking variability sources: these reflect different scenarios with respect to the voice interaction with either another person or technology system, or differences with respect to the specific language or dialect spoken, and can include</p><p>• human-to-human: speech that includes two or more individuals interacting or one person speaking and addressing an audience -language or dialect spoken -if speech is read/prompted (through visual display or through headphones), spontaneous, conversational, or disguised speech -monologue, two-way conversation, public speech in front of an audience or for TV or radio, group discussion • human-to-machine: speech produced where the subject is directing his or her speech toward a piece of technology (e.g., cell/smart/landline telephone and computer) -prompted speech: voice input to a computer -voice input for telephone/dialog system/computer input: interacting with a voice-based system. ■ Technology-or external-based variability sources: these include how and where the audio is captured and the following issues:</p><p>• electromechanical-transmission channel, handset (cell, cordless, and landline) <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref> microphone • environmental-background noise <ref type="bibr" target="#b17">[18]</ref> (stationary, impulsive, time-varying, etc.), room acoustics <ref type="bibr" target="#b18">[19]</ref>, reverberation <ref type="bibr" target="#b19">[20]</ref>, and distant microphone • data quality-duration, sampling rate, recording quality, and audio codec/compression. These multifaceted sources of variation pose the greatest challenge in accurately modeling and recognizing a speaker, whether automatic algorithms are used, or if human listening/assessment is performed. Given that speech will contain variability, the task of speaker verification is deciding if the variability is due to the same speaker (intra {within}-speaker) or different speakers (inter {across}-speaker).</p><p>In current automated speaker-recognition technology, various mathematical tools are used to mitigate the effects of these </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CHALLENGES IN SPEAKER RECOGNITION</head><p>Early efforts in speaker recognition involving technology focused more on the telecommunications domain, where telephone handset and communication channel variation was the primary concern. In the United States, when telephone systems were confined to handheld rotary phones in the home and public phone booths in public settings, technology-and telephony-based variability was an issue, but it was, to a large degree, significantly less important than it is today. With mobile cell phone/smartphone technology dominating the world's telecommunications market, the diversity of telephony scenarios has expanded considerably. Virtually all cell phones have a speaker option, which allows voice interaction at a distance from the microphone, and movement of the device introduces a wider range of channel variability.</p><p>Voice is also a time-varying entity. Research has shown that intersession variability, the inherent changes present within audio files captured at different times, results in changes in speaker-recognition performance. Analysis of the Multisession Audio Research Project corpus collected using the same audio equipment in the same location on a monthly basis over a 36-month period showed measurable differences in speaker-recognition performance <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. However, the changes in speaker-identification performance seem to be independent of the time difference between training and testing <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[23]</ref>. While no aging effects were noted for the 36-month period, other research has demonstrated long-term changes in speech physiology and production due to aging <ref type="bibr" target="#b22">[23]</ref>. More extensive research that explores the evolution of speaker structure for speaker recognition over a 20-60-year period (at least for a small subset of speakers) has shown measurable changes and suggested methods to address changes due to aging <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>.</p><p>These examples of variation point to the sensitivity of existing speaker-recognition technology. It is possible to employ such technology in a way that could lead to noncredible results. A recent example of how to wrongly use automatic speaker recognition was seen during the recent U.S. legal case involving George Zimmerman, who was accused of shooting Trayvon Martin during an argument <ref type="bibr" target="#b25">[26]</ref>. In that case, a 911 emergency call captured a scream for help heard in the background. The defense team claimed that it was Zimmerman who was yelling while supposedly being attacked by Trayvon Martin, who was killed; alternatively, the prosecutors argued that it was the unarmed victim who was shouting. Parents of both parties testified that the voice heard on the 911 call belonged to their own son. Some forensic experts did attempt to use semiautomatic methods to compare the original scream and a simulated scream obtained from Zimmerman. The issue of using automatic assessment schemes for scream analysis to assess identity was controversial, as experts from the U.S. Federal Bureau of Investigation (FBI) and U.S. National Institute of Standards and Technology (NIST) testified that these methods are unreliable. A brief probe analysis of scream and speaker-recognition technology confirmed the limitations of current technology <ref type="bibr" target="#b26">[27]</ref>.</p><p>Most forensic speaker-identification scenarios, however, are not as complicated. When there is sufficient speech material available from the offender and the suspect, systematic analysis can be performed to extract speaker idiosyncratic characteristics, also known as feature parameters, from the speech data, and a comparison between the samples can be made. Also, in automatic speakeridentification systems, features designed to differentiate among speakers are first extracted and mathematically modeled to perform a meaningful comparison. Thus, in the next section, we consider what traits help identify a person from his or her speech-in other words, what are the feature parameters that we should consider in making an assessment?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SPEAKEr cHArActErIZAtIon: FEAturE PArAMEtErS</head><p>Every speaker has some characteristic traits in his or her voice that are unique. Individual speaker characteristics may not be so easily distinguishable but are unique mainly due to speaker vocal tract physiology and learned habits of articulation. Even identical twins have differences in their voices, though studies show they have similar vocal tract shape <ref type="bibr" target="#b28">[28]</ref> and acoustic properties <ref type="bibr" target="#b29">[29]</ref>, and it is difficult to distinguish them from a perceptual/forensics perspective <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b31">[31]</ref>. Researchers in voice forensics have even participated in the National Twins Day event held in Twinsburg, Ohio, <ref type="bibr" target="#b32">[32]</ref> in an effort to capture voice and other biometrics to explore the challenges in distinguishing closely related individuals. Thus, whether recognition is performed by humans (an expert or naïve listener) or by machines, some measurable and predefined aspects of speech need to be considered to make meaningful comparisons among voices. Generally, we refer to these characterizing aspects as feature parameters.</p><p>One might expect that a unique voice must have unique features, but this is not always true. For example, two different speakers may have the same speaking rate (which is a valid feature parameter) but differ in average pitch. This is complicated by the variability and degradations discussed previously, which is why considering multiple feature parameters is critical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PROPERTIES OF IDEAL FEATURES</head><p>As outlined by Nolan <ref type="bibr" target="#b33">[33]</ref>, ideally a feature parameter should 1) show high between-speaker variability and low withinspeaker variability 2) be resistant to attempted disguise or mimicry 3) have a high frequency of occurrence in relevant materials 4) be robust in transmission 5) be relatively easy to extract and measure. These properties, though mentioned in the forensic speakeridentification context, apply in general. Interestingly, Wolf <ref type="bibr" target="#b34">[34]</ref> discussed very similar sets of properties in the context of features for automatic speaker recognition, independently, preceding Nolan <ref type="bibr" target="#b33">[33]</ref>. We refer to these properties as ideal property 1-5 throughout this article. It should be reiterated that variability in features will always exist, but the important task is to determine if the origin of the variability is the same speaker or different speakers.</p><p>We now discuss various feature parameters used in forensic speaker identification, which can be and are also useful for general speech understanding. There is no fixed set of rules for what parameters should be used in forensic speaker recognition. This is largely dependent on the circumstances or availability <ref type="bibr" target="#b35">[35]</ref>. Some forensic experts may choose parameters to compare based on the most obvious aspect of the voices under consideration. Feature parameters can be broadly classified into auditory versus acoustic, linguistic versus nonlinguistic, and short-term versus long-term features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AUDITORY VERSUS ACOUSTIC FEATURES</head><p>Some aspects of speech are better suited for auditory analysis (i.e., through listening). Auditory features are thus defined as aspects of speech that can "be heard and objectively described" by a trained listener <ref type="bibr" target="#b36">[36]</ref>. These can be specific ways of uttering individual speech sounds (e.g., the pronunciation of the vowel sounds in the word hello can be used as auditory features).</p><p>Acoustic features, on the other hand, are mathematically defined parameters derived from the speech signal using automatic algorithms. Clearly, these kinds of features are used in automatic systems, but they are also used in computer-assisted forensic speaker recognition. Fundamental frequency (F0) and formant frequency bandwidth are examples of acoustic features. Automatic systems frequently use acoustic features derived from the short-term power spectrum of speech.</p><p>Both auditory and acoustic features have their strengths and weaknesses. Two speech samples may sound very similar but have highly variant acoustic parameters <ref type="bibr" target="#b37">[37]</ref>. Alternatively, speech samples may sound very different yet have similar acoustic features <ref type="bibr" target="#b28">[28]</ref>. It is thus generally accepted that both auditory and acoustic features are indispensable for forensic investigations <ref type="bibr" target="#b35">[35]</ref>. One might argue that if reverse engineering of the human auditory system <ref type="bibr" target="#b38">[38]</ref> is fully successful, auditory features can also be extracted using automatic algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LINGUISTIC VERSUS NONLINGUISTIC FEATURES</head><p>Linguistic feature parameters can provide contrast "within the structure of a given language or across languages or dialects" <ref type="bibr" target="#b35">[35]</ref>. They can be acoustic or auditory in nature and are classified further as phonological, morphological, and syntactic <ref type="bibr" target="#b36">[36]</ref>. A simple example of a linguistic feature is whether the "r" sound at the end of a word, e.g., car, is pronounced or silent-in some dialects of English, this type of "r" sound is not pronounced (i.e., Lancashire versus Yorkshire dialects of U.K. English). This is different from an auditory analysis of how the "r" sound is pronounced, since, in this case, this speech sound will be compared across different words.</p><p>Nonlinguistic features include aspects of speech that are not related to the speech content. Typical nonlinguistic features may include: speech quality (nasalized, breathy, husky, etc.), fluency, speech pauses (frequency and type), speaking rate, average fundamental frequency, and nonspeech sounds (coughs, laughs, etc.). Again, these features can be auditory or acoustic in nature. Referring back to the Zimmerman case, the manner of screaming (i.e., loudness, pitch, and duration) could be a potential feature if it could be properly measured/parameterized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SHORT-TERM VERSUS LONG-TERM FEATURES</head><p>Depending on the time span of the feature parameters, they can be categorized as short versus long term. Most features discussed so far are short term or segmental in nature. Popular automatic systems mostly use short-term acoustic features, especially the ones extracted from the speech spectrum. The short-term features are also effective in auditory forensic analysis, for example, direct comparison of the "r" sound and consonant-vowel transition <ref type="bibr" target="#b33">[33]</ref>.</p><p>The long-term features are usually averaged short-term parameters, (e.g., fundamental frequency, short-term spectrum). These parameters have the benefit of being insensitive to fluctuations due to individual speech sounds and provide a smoother measurement from a speech segment. The long-term features also include energy, pitch, and formant contours, which are measured/averaged over long time periods. Recent automatic systems also successfully used such features <ref type="bibr" target="#b39">[39]</ref>- <ref type="bibr" target="#b41">[41]</ref>. If a feature parameter is extracted from an entire speech utterance, we refer to it as an utterance-level feature, or utterance feature for short. This concept will become very useful as we proceed with the discussion to automatic systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ForEnSIc SPEAKEr rEcoGnItIon</head><p>While the focus in this review is on automatic machine-based speaker recognition, we also briefly consider both forensic and naïve speaker recognition. The need for forensic speaker recognition/identification arises when a criminal leaves his or her voice as evidence, be it as a telephone recording or speech heard by an earwitness. The use of technology for forensic speaker recognition has been discussed as early as 1926 <ref type="bibr" target="#b42">[42]</ref> with speech waveforms. Later, the spectrographic representation of speech was developed at AT&amp;T Bell Laboratories during World War II. It was popularized much later, in the 1970s, when it came to be known as the voiceprint <ref type="bibr" target="#b43">[43]</ref>. As the name suggests, the voiceprint was presented as being analogous to fingerprints and with very high expectations. Later, the reliability of the voiceprint for voice identification, from its operating mechanisms to formal procedure, was thoroughly questioned and argued <ref type="bibr" target="#b44">[44]</ref>, <ref type="bibr" target="#b45">[45]</ref>, even called "an idea gone wrong" <ref type="bibr" target="#b45">[45]</ref>. It was simply not accurate with speech being so subject to variability. Most researchers today believe it to be controversial at best. A chronological history of voiceprints can be found in <ref type="bibr" target="#b46">[46]</ref>, and an overview discussion on forensic speaker recognition can be found in <ref type="bibr" target="#b47">[47]</ref>. Here, we present an overview with respect to current trends.</p><p>In the general domain of forensic science, the United States has recently formed the Organization of Scientific Area Committees (OSAC) (http://www.nist.gov/forensics/osac.cfm), which is overseen by NIST. The legacy structure before OSAC was Forensic Science Working Groups. The current OSAC organization was established to help formalize the process of best practices for standards as they relate to researchers, practitioners, legal and law enforcement as well as government agencies. It also allows for a more transparent process in which experts and users of the various technologies can provide feedback and help shape best practices. Currently, OSAC is establishing a number of working documents to build consensus among the various forensic subfields. A good source of current information from OSAC is the NIST Forensic Science Publications website (http://www.nist.gov/ forensics/publications.cfm).</p><p>Today, forensic speaker identification is commonly performed by expert phoneticians who generally have backgrounds in linguistics and statistics. This is a very complex procedure, and varies among practitioners. There is no standard set of procedures every practitioner agrees upon. Different aspects/features are considered when forensic experts make comparisons between utterances. The procedure is often dictated by the situation at hand-for example, if only a few seconds of screaming of the unknown speaker is available on the evidence tape, the only thing that can be done is to try to recreate a similar scream from the likely speaker (suspect) and compare, which is generally not feasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>THE LIKELIHOOD RATIO</head><p>Regardless of the varying approaches by practitioners, forensic speaker recognition essentially entails a scientific and objective method of comparing voices (there are, apparently, people who attempt to perform this task using methods unacceptable by the general forensic community <ref type="bibr" target="#b48">[48]</ref>). Forensic experts must testify in court concerning the similarity/dissimilarity of the speech samples in consideration in a meaningful way. However, they cannot make any categorical judgment about the voices (e.g., the two voices come from the same speaker). For this purpose, the likelihood ratio (LR) <ref type="bibr" target="#b49">[49]</ref> measure was introduced, which forensic experts use to express the strength of their findings <ref type="bibr" target="#b50">[50]</ref>, <ref type="bibr" target="#b51">[51]</ref>. This means that the evaluation of forensic speech samples will not yield an absolute identification or elimination of the suspect but instead provides a probabilistic confidence measure. As discussed previously, even speech samples from the same speaker will differ in realistic scenarios. The goal of the forensic voice comparison expert is thus to estimate the probability of observing the measured difference between speech samples assuming that they were spoken by 1) the same speaker and 2) different speakers <ref type="bibr" target="#b35">[35]</ref>. The procedure for measuring the LR is given next: X = Speech sample recorded during a crime (evidence recording). Y = Speech sample obtained from suspect (exemplar). H0 = The hypothesis that X and Y are spoken by the same person. H1 = The hypothesis that X and Y are spoken by different persons. E = Observed forensic evidence (e.g., average pitch from X and Y differ by 10 Hz). The LR formula is</p><formula xml:id="formula_0">( | ) ( | ) . p E H p E H LR 1 0 =</formula><p>As an example, if the average pitch difference between two utterances is considered the feature parameter, the forensic expert first computes the probability distribution of this feature parameter for speech data collected from many same-speaker (hypothesis ) H0 and different-speaker (hypothesis ) H1 pairs. In the next step, given the evidence E (average pitch from X and Y differ by 10 Hz), the conditional probabilities ( | ), p E H0 and ( | ), p E H1 can be computed. Note that the forensic expert does not try to estimate ( | ) p H E 0 (i.e., the probability that the suspect is guilty given the observed evidence). This is because this estimation is done using Bayes' theorem, which requires the prior probabilities of the hypotheses generally not provided to the expert (and are also difficult to estimate). More discussion on this can be found in <ref type="bibr" target="#b35">[35,</ref><ref type="bibr">Ch. 4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPROACHES IN FORENSIC SPEAKER IDENTIFICATION</head><p>Here, we discuss general approaches taken for forensic speaker recognition. The methods described are performed by human experts, fully or partially. While full automatic approaches are also considered for forensics, we discuss automatic speaker recognition in later sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AUDITORY APPROACH</head><p>This approach is practiced by auditory phoneticians and involves producing a detailed transcript of the evidence tape and exemplars. Drawing on their experience, experts listen to speech samples and attempt to detect any aspects of the voices that are unusual, distinctive, or noteworthy <ref type="bibr" target="#b51">[51]</ref>. The experience of the expert is obviously an important factor in deciding about rarity or typicality. The auditory features discussed previously are used in this approach.</p><p>The auditory approach is fully subjective, unless it is combined with other approaches. Although the LR can be used to express the outcome of the analysis, practitioners of the auditory approach generally do not use it. Instead, based on their comparison of auditory features, they present an evidentiary statement (a formal statement describing the basis of the evidence) in court.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AUDITORY-SPECTROGRAPHIC APPROACH</head><p>As discussed previously, the spectrographic approach, previously known as voiceprint analysis, is based on visual comparison of speech spectrograms. Generally, the same word or phrase is extracted from the known and questioned voices and their spectrograms are visually analyzed. Additional foil speakers' (background speakers) spectrograms are also included to facilitate in understanding similarity versus typicality. It is believed that visual comparison using spectrograms together with listening to the audio reinforces the voice identification procedure <ref type="bibr" target="#b44">[44]</ref>, <ref type="bibr" target="#b45">[45]</ref>, which is why the approach is termed auditory-spectrographic.</p><p>Following the controversy on voiceprints, the spectrographic method evolved in various ways. It was not evident if forensic experts could differentiate between intraspeaker (changes of speech from the same speaker) and interspeaker (changes in speech due to different speakers) variation by a general visual comparison of spectrographs. Thus, different protocols evolved that require the forensic examiner to analyze predefined aspects of the spectrographs. According to the American Board of Recorded Evidence (ABRE) protocols, the examiner is required to visually analyze and compare aspects such as general formant shaping and positioning, pitch striations, energy distribution, word length, and coupling (nasality). It also requires auditory comparisons of pitch, stress/emphasis, speaking rate, disguise, mode, etc. <ref type="bibr" target="#b51">[51]</ref>, <ref type="bibr" target="#b52">[52]</ref>.</p><p>The auditory-spectrographic, similar to the auditory approach, is also subjective and depends heavily on the experience of the examiner. Courts in some jurisdictions do not accept testimony based on this approach. The FBI seeks advice from auditory-spectrographic experts during investigations but does not allow them to testify in court <ref type="bibr" target="#b51">[51]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACOUSTIC-PHONETIC APPROACH</head><p>This approach, which is commonly taken by experts trained on acoustic-phonetics, requires quantitative acoustic measurements from speech samples, and statistical analysis of the results. Acoustic features discussed previously are ones that are considered. Generally, similar phonetic units are extracted from the known and questioned speech samples, and various acoustic parameters measured from these segments are compared. The LR can be conveniently used in this approach since it is based on numerical parameters <ref type="bibr" target="#b51">[51]</ref>.</p><p>Although the acoustic-phonetic approach is a more objective approach, it does have some subjective elements. For example, an acoustic-phonetician may identify speech sounds as being affected by stress (through listening) and then perform objective analysis. However, whether the speaker was actually under stress at that moment is a subjective quantity determined by the examiner through his or her experience. It is a matter of debate if having a human element in the forensic speaker-recognition process is advantageous <ref type="bibr" target="#b51">[51]</ref>.</p><p>Forensic speaker identification will continue to be an important research area in the coming future. As evident from the discussion, the methods are evolving toward mathematical and statistical approaches, perhaps signaling that the human element in this process may actually be a source of error. The NIST has conducted studies on human-assisted speaker recognition (HASR) comparing human experts and state-of-the-art algorithms <ref type="bibr" target="#b19">[20]</ref>. In these experiments, a set of difficult speaker pairs (i.e., same speakers that sound different in two recordings or different speakers that sound similar) were selected. The results indicated that the state-of-theart fully automatic systems outperformed the human-assisted systems. We discuss these studies further in the "Man Versus Machine in Speaker Recognition" section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>nAïvE SPEAKEr rEcoGnItIon</head><p>The ability to recognize people by their voices is an acquired human trait. Research shows that we are able to recognize our mothers' voice from as early as the fetus stage <ref type="bibr" target="#b53">[53]</ref>, <ref type="bibr" target="#b54">[54]</ref>. We analyze many different aspects of a person's voice to identify him or her, including spectral characteristics, language, prosody, and speaking style. We learn and remember these traits constantly without even putting in a conscious effort. In this section, we discuss various aspects of how a naïve listener identifies a speaker and what is currently known about the speaker-recognition process in the human brain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IDENTIFY SPEECH SEGMENTS</head><p>An important aspect of detecting speakers from audio samples is to first identify speech segments. Humans can efficiently distinguish between speech and nonspeech sounds from a very early age <ref type="bibr" target="#b55">[55]</ref>. This is observed from highly voice-selective cerebral activity measured by functional magnetic resonance imaging (fMRI) in the adult human brain <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b55">[55]</ref>, <ref type="bibr" target="#b56">[56]</ref>. Figure <ref type="figure">2</ref> shows the brain regions that demonstrate higher neural activity with vocal and nonvocal stimuli. Note that in this experiment, any sound produced by a human is considered vocal (irrespective of being voiced or unvoiced), including laughs and coughs. In later sections, we discuss a very similar process required by automatic systems as a preprocessing step before performing speaker recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SPEAKER RECOGNITION VERSUS DISCRIMINATION</head><p>It is obvious that we need to be familiar with a person's voice before identifying him or her. Familiarity is a subjective condition, but it is apparent that being familiar with a person depends on how much time the subject has spent in listening to that person. In other words, familiarity with a speaker depends on the amount of speech data observed by the listener. The familiar person can be a close acquaintance (e.g., a friend or relative) or someone famous (e.g., a celebrity or political leader).</p><p>Interestingly, familiar voice recognition and unfamiliar voice discrimination are known to be separate cognitive abilities <ref type="bibr" target="#b57">[57]</ref>. Familiar voice recognition is essentially a pattern-recognition task-humans can perform this task even if the speech signal is reversed <ref type="bibr" target="#b58">[58]</ref>. These findings suggest that unfamiliar voice discrimination involves analysis of speech features as well as the pattern-recognition ability of the brain <ref type="bibr" target="#b57">[57]</ref>. Forensic examiners heavily depend on the ability to discriminate since they are not usually familiar with the speakers in the speech samples involved. The findings in <ref type="bibr" target="#b57">[57]</ref> also imply that voice discrimination ability of the human brain is not a preprocessing step of voice recognition, since these two processes are found to be independent. For automatic systems, however, this is not usually true. The same algorithms can be used (usually with slight modification) to discriminate between speakers or identify a specific speaker. In many cases, discriminative training methods are used to learn speaker models, which can later be used to identify speakers. We discuss automatic systems further in the "Automatic Speaker Recognition" section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FAMILIARITY WITH LANGUAGE</head><p>It is observed in <ref type="bibr" target="#b59">[59]</ref> that humans are better at recognizing people who are familiar and speak a known language. Experiments reported in this study show that native English speakers with normal reading ability could identify voices speaking English significantly more accurately than voices speaking Chinese. Thus, the voice-recognition ability of humans depends on their familiarity with the phonology of the particular language. Humans can still recognize people speaking an unknown language, but with much lower accuracy <ref type="bibr" target="#b59">[59]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ABSTRACT REPRESENTATIONS OF SPEECH</head><p>The human brain forms efficient abstract representations from relevant audio features that contain both phonetic and speaker identity information. These representations aid in efficient processing and high robustness due to noise and other forms of degradations. These aspects of the brain were studied in <ref type="bibr" target="#b4">[5]</ref>, where the authors have shown that it is possible to decipher both speech content and speaker identity by observing neural activity of the human listener. The brain activities were measured by fMRI and it was found that there are certain observable patterns corresponding to speech and voice stimuli elicit in the listener's auditory cortex. This is illustrated in Figure <ref type="figure">3</ref>, where vowel (red) and speaker (blue) discriminative regions in the brain are shown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SPEAKER RECOGNITION IN THE BRAIN: FINAL REMARKS</head><p>There is still much more to discover about the human brain and how it processes information. From what we already know, the human brain performs complex spectral and temporal audio processing <ref type="bibr" target="#b60">[60]</ref>, is sensitive to vocal stimuli <ref type="bibr" target="#b3">[4]</ref>, shows familiarity to the phonology of languages <ref type="bibr" target="#b59">[59]</ref>, and builds abstract representations of speech and speaker information that are robust to noise and other degradations <ref type="bibr" target="#b4">[5]</ref>. Most of these abilities are highly desirable in automatic systems, especially the brain's ability to process noisy speech. It is thus natural to attempt to mimic the human brain in solving these problems. Research efforts are already underway to reverse engineer the processes performed by the human auditory pathway <ref type="bibr" target="#b38">[38]</ref>.</p><p>As discussed previously, the human brain processes familiar speakers differently than unfamiliar ones <ref type="bibr" target="#b55">[55]</ref>, <ref type="bibr" target="#b57">[57]</ref>. This may mean that faithfully comparing human and machine performance in a speaker-recognition task can be very difficult since it is not well understood how to quantify familiarity with a person from an automatic system's perspective-what amount of data is enough for the system to be familiar with that person? Nevertheless, it will be interesting to be able to determine exactly how the human brain stores the speaker identity information of familiar speakers. These findings may lead to breakthrough algorithmic advances in the automatic speaker-recognition area.</p><p>As we conclude this section, we want to highlight the strengths and weaknesses of humans in the speaker-recognition task. Here, humans include both forensic examiners and naïve listeners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STRENGTHS OF HUMAN LISTENERS</head><p>■ Humans (naïve listeners and experts alike) can identify familiar speakers with remarkable accuracy, even in challenging conditions (normal, disguised, and stressed) <ref type="bibr" target="#b61">[61]</ref>. ■ Humans are good at finding the idiosyncrasies of a speaker's voice. Thus, the forensic examiner may easily identify where to look. For example, a speaker may cough in a specific manner, which a human will notice very quickly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WEAKNESSES OF HUMAN LISTENERS</head><p>■ Humans are susceptible to contextual bias <ref type="bibr" target="#b62">[62]</ref>. For example, if the forensic examiner knows that a suspect already confessed to a crime, he is more likely to find a match between the exemplar and evidence recording. ■ Humans are prone to error. The reliability of voiceprints was questioned mostly due to human errors involved in the process <ref type="bibr" target="#b46">[46]</ref>. ■ Humans cannot remember a speaker's voice for a long time <ref type="bibr" target="#b63">[63]</ref>. Memory retention ability depends on the duration of speech heard by the listener <ref type="bibr" target="#b64">[64]</ref>.</p><p>■ For familiar speakers, the listener may confuse them with someone else. The subject may know that the voice is familiar but may not correctly identify exactly who the speaker is. ■ Naïve listeners cannot distinguish subtle differences between voices. However, trained experts can. For example, the difference between New York and Boston accents is distinguishable by an expert but probably not by naïve listeners <ref type="bibr" target="#b35">[35]</ref>.</p><p>■ Humans perform better while they are attentive. However, the attention level drops with time, and listeners tend to become fatigued after a certain time.</p><p>■ The outcome of voice comparison results as LRs may not be consistent across multiple experts (or the same expert at different times).</p><p>■ Human listeners (including forensic experts) may seem to identify someone from a voice recording if they are expecting to hear that person.</p><p>Concluding the discussion on speaker recognition by humans, we now move forward with the main focus of this review, which is automatic systems for speaker recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AutoMAtIc SPEAKEr rEcoGnItIon</head><p>In automatic speaker recognition, computer programs designed to operate independently with minimum human intervention identify a speaker's voice. The system user may adjust the design parameters, but to make the comparison between speech segments, all the user needs to do is provide the system with the audio recordings. In the current discussion, we focus our attention on the text-independent scenario and the speaker-verification task. Naturally, the challenges mentioned previously affect the automatic systems in the same way as they do the human listeners or forensic experts. Various speaker-verification approaches can be found in the literature that address specific challenges; see <ref type="bibr" target="#b65">[65]</ref>- <ref type="bibr" target="#b74">[74]</ref> for a comprehensive tutorial review on automatic speaker recognition. The research community is largely driven by standardized tasks set forth by NIST through the speaker-recognition evaluation (SRE) campaigns <ref type="bibr" target="#b75">[75]</ref>- <ref type="bibr" target="#b78">[78]</ref>. We discuss the NIST SRE tasks in more detail in later sections.</p><p>A simple block diagram representation of an automatic speaker-verification system is shown in Figure <ref type="figure">4</ref>. Predefined feature parameters are first extracted from the audio recordings that are designed to capture the idiosyncratic characteristics of a person's speech in mathematical parameters. These features obtained from an enrollment speaker are used to build/train mathematical models that summarize their speaker-dependent properties. For an unknown test segment, the same features are then extracted, and they are compared against the model of the enrollment/claimed speaker. The models are designed so that such a comparison provides a score (a scalar value) indicating whether the two utterances are from the same speaker. If this score is higher (or lower) than a predefined threshold then the system accepts (or rejects) the test speaker.</p><p>It should be noted that the block diagram in Figure <ref type="figure">4</ref> for speaker verification is a simplified one. As we discuss more about the standard speaker-recognition systems of today, features can be extracted from short-term segments of speech, a relatively longer duration of speech, or the entire utterance. The classification of features discussed previously also applies in this case.</p><p>In some automatic systems, the feature-extraction processes may be dependent on other speech utterances spoken by a diverse speaker population, as well as the enrollment speaker <ref type="bibr" target="#b79">[79]</ref>. In short, the recent techniques make use of the general properties of human speech by observing many different speech recordings to make effective speaker-verification decisions. This is also intuitive, since we also learn how human speech varies across conditions over time. For example, if we only heard one language in our entire life, we would have difficulty distinguishing people speaking a different language <ref type="bibr" target="#b59">[59]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FEATURE PARAMETERS IN AUTOMATIC SPEAKER-RECOGNITION SYSTEMS</head><p>As mentioned previously, feature parameters extracted from an entire utterance are referred to as utterance features in this article. This becomes more important in the automatic speaker-recognition context as many common pattern-recognition algorithms operate on fixed dimension vectors. Because of the variable length/duration property of speech, acoustic/segmental features cannot be directly used with such classifiers. However, simple methods such as averaging segmental features over time do not seem to be highly effective in this case, due to the time-varying nature and context dependency of speech <ref type="bibr" target="#b80">[80]</ref>, <ref type="bibr" target="#b81">[81]</ref>. For example, taking speaking rate as a feature, it is obvious that two people may commonly have the same speaking rate, so this feature by itself may not be very useful. Researchers noted early on that a specific speaker's idiosyncratic features will be time varying and context/speech sound dependent <ref type="bibr" target="#b34">[34]</ref>, <ref type="bibr" target="#b66">[66]</ref>. However, the high-level and long-term features such as dialect, accent, speaking style/rate, and prosody are also useful and can be beneficial when used together with low-level acoustic features <ref type="bibr" target="#b39">[39]</ref>, <ref type="bibr" target="#b82">[82]</ref>.</p><p>vAD As noted previously, humans are good at distinguishing between speech and nonspeech sounds, which is also an essential part in auditory forensic speaker recognition. Clearly, in automatic systems it is also desirable that features be extracted only from speech segments of the audio waveform, which necessitates VAD <ref type="bibr" target="#b83">[83]</ref>, <ref type="bibr" target="#b84">[84]</ref>. Detecting speech segments becomes critical when highly noisy/degraded acoustic conditions are considered. The function of VAD is illustrated in Figure <ref type="figure">5</ref>(a), where speech presence/absence is indicated by a binary signal overlaid on the speech samples. The corresponding speech spectrogram is shown in Figure <ref type="figure">5(b)</ref>. The VAD algorithm used in this plot is presented in <ref type="bibr" target="#b83">[83]</ref>, though more advanced unsupervised solutions such as Combo-Speech Activity Detection (SAD) have recently emerged as successful in diverse audio conditions for speaker recognition <ref type="bibr" target="#b85">[85]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SHORT-TERM FEATURES</head><p>These features refer to parameters extracted from short speech segments/frames of duration within 20-25 milliseconds. The most popular short-term acoustic features are the Mel-frequency cepstral coefficients (MFCCs) <ref type="bibr" target="#b86">[86]</ref> and linear predictive coding (LPC)-based <ref type="bibr" target="#b87">[87]</ref> features. For a review on different short-term acoustic features for speaker recognition, see <ref type="bibr" target="#b71">[71]</ref> and <ref type="bibr" target="#b73">[73]</ref>. We briefly discuss the MFCC features here. To obtain these coefficients from an audio recording, first the audio samples are divided into short overlapping segments. A typical 25-millisecond speech signal frame is shown in Figure <ref type="figure">6(a)</ref>. The signal obtained in these segments/frames is then multiplied by a window function (e.g., Hamming and Hanning), and the Fourier power spectrum is obtained. In the next step, the logarithm of the spectrum is computed and nonlinearly spaced Mel-space filter-bank analysis is performed. The logarithm operation expands the scale of the coefficients and also decomposes multiplicative components to additive <ref type="bibr" target="#b88">[88]</ref>. The filter-bank analysis produces the spectrum energy in each channel (also known as the filter-bank energy coefficients), representing different frequency bands.</p><p>A typical 24-channel filter bank and its outputs are shown in Figure <ref type="figure">6</ref>(c) and (d), respectively. As evident here, the filter bank is designed so that it is more sensitive to frequency variations in the lower end of the spectrum, similar to the human auditory system [86]. Finally, MFCCs are obtained by performing discrete cosine transform (DCT) on the filter-bank energy parameters and retaining a number of leading coefficients. DCT has two important properties: 1) it compresses the energy of a signal to a few coefficients and 2) its coefficients are highly decorrelated. For these reasons, removing some dimensions using DCT improves modeling efficiency and reduces some nuisance components. Also, the decorrelation property of DCT helps the models that assume feature coefficients are uncorrelated. In summary, the following sequence of operations-power spectrum, logarithm, and DCTproduces the well-known cepstral representation of a signal <ref type="bibr" target="#b88">[88]</ref>. Figure <ref type="figure">6</ref>(e) shows the static MFCC parameters, retaining the first 12 coefficients after DCT. Generally, velocity and acceleration parameters computed across multiple frames of speech are appended to the MFCCs. These parameters (known as deltas and double deltas, respectively) represent the dynamic properties of the short-term feature coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FEATURE NORMALIZATION</head><p>As stated previously, one of the desirable properties of acoustic features (and any feature parameter in a pattern-recognition problem) is robustness to degradation. This is one of the desirable characteristics of an ideal feature parameter <ref type="bibr" target="#b34">[34]</ref>. In reality, it is not possible to design a feature parameter that will be absolutely unchanged in modified acoustic conditions and also provide meaningful speakerdependent information. However, these changes can be minimized in various ways using feature-normalization techniques such as cepstral mean subtraction <ref type="bibr" target="#b89">[89]</ref>, feature warping <ref type="bibr" target="#b90">[90]</ref>, relative spectra (RASTA) processing <ref type="bibr" target="#b91">[91]</ref>, and quantile-based cepstral normalization <ref type="bibr" target="#b92">[92]</ref>. It should be noted that normalization techniques are not designed to enhance the discriminative ability of the features (ideal property 3), rather they aim to modify the features so that they are more consistent among different speech utterances (ideal property 5). Popular normalization schemes include feature warping and cepstral mean and variance normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SPEAKER MODELING</head><p>Once the audio segments are converted to feature parameters, the next task of the speaker-recognition process is modeling. In general terms, we can define modeling as a process of describing the feature properties for a given speaker. The model must also provide means of its comparison with an unknown utterance. A modeling method is robust when its characterizing process of the features is not significantly affected by unwanted distortions, even though the features are. Ideally, if features could be designed in such a way that no intraspeaker variation is present while interspeaker discrimination is maximum, the simplest methods of modeling might have sufficed. In essence, the nonideal properties of the feature extraction stage requires various compensation techniques during the modeling phase so that the effect of the nuisance variations observed in the signal are minimized during the speaker-verification process.</p><p>Most speaker-modeling techniques make various mathematical assumptions on the features (Gaussian distributed, for example). If these properties are not met by the data, we are essentially introducing imperfections during the modeling phase as well. The normalization of features can alleviate these problems to some extent, but not entirely. Consequently, mathematical models are forced to fit the features and recognition scores are derived based on these models and test data. Thus, this process introduces artifacts in the detection scores, and a family of score-normalization techniques has been proposed in the past to encounter this final-stage mismatch <ref type="bibr" target="#b16">[17]</ref>.</p><p>In summary, degradations in the acoustic signal affect features, models, and scores. Thus, improving robustness of speakerrecognition systems is important in these three domains. Recently, it has been observed that as speaker-modeling techniques are improved, score-normalization techniques become less effective <ref type="bibr" target="#b93">[93]</ref>, <ref type="bibr" target="#b94">[94]</ref>. Similarly, we can argue that if acoustic features are improved, simple modeling techniques will be sufficient. However, from the speaker-recognition research trend in the last decade, it seems that improving feature robustness beyond a certain level (for a variety of degradations) is extremely difficult-or, in other words, data-driven modeling techniques have been more successful in improving robustness compared to new features. This is especially true if large data sets are used in training strong discriminative models. In the recent approaches for speech recognition, simple filter-bank energy features are found to be more effective than MFCCs when large neural networks are used for modeling <ref type="bibr" target="#b95">[95]</ref>. Also, modeling techniques that aim at learning the behavior of the degradations from example speech utterances are at an advantage in improving robustness. For example, an automatic system that has observed several examples of speech recordings of different speakers in roadside noise will be better at distinguishing speakers in that environment.</p><p>In the following sections, we discuss how state-of-the-art systems have evolved during the last decade. We emphasize a few key advancements made during this time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GAUSSIAN-MIXTURE-MODEL-bASED METHOD</head><p>A Gaussian mixture model (GMM) is a combination of Gaussian probability density functions (PDFs) generally used to model multivariate data. The GMM clusters the data in an unsupervised way (i.e., without any labeled data), but it provides a PDF of the data. Using GMMs to model a speaker's features results in a speakerdependent PDF. Evaluating the PDF at different data points (e.g., features obtained from a test utterance) provides a probability score that can be used to compute the similarity between a speaker GMM and an unknown speaker's data. For a simple speaker-identification task, a GMM, is first obtained for each speaker. During testing, the utterance is compared against each GMM, and the most likely speaker (i.e., the highest-scoring GMM) is selected.</p><p>In text-independent speaker-recognition tasks when there is no a priori knowledge about the speech content, using GMMs to model short-term features has been found to be most effective for acoustic modeling. This is expected since the average behavior of the short-term spectral features is more speaker dependent rather than being affected by the temporal characteristics. It was first used in a speaker-recognition method in <ref type="bibr" target="#b96">[96]</ref>. Before GMMs were introduced, the vector quantization (VQ) method <ref type="bibr" target="#b81">[81]</ref>, <ref type="bibr" target="#b97">[97]</ref>, <ref type="bibr" target="#b98">[98]</ref> was used for speaker recognition. This technique models the speaker using a set of prototype vectors instead of PDFs. GMMs have been shown to be better speaker models compared to VQ because of their probabilistic nature for allowing greater variability. Therefore, even when the test utterance has a different acoustic condition, GMMs, being a probabilistic model, can relate to the data better than the more restrictive VQ model (see "GMM-Based Speaker Recognition: Summary").</p><p>A GMM is a mixture of Gaussian PDF parameterized by a number of mean vectors, covariance matrices, and weights of the individual mixture components. The model is represented by a weighted sum of the individual PDFs. If a random vector xn can be modeled by M Gaussian components with mean vectors , g n covariance matrices , g R where g = 1, 2…M indicate the component indices, the PDF of xn is given by</p><formula xml:id="formula_1">x x f N n g n g M 1 m r = = ^ĥ / , ), g g n R<label>(1)</label></formula><p>where g r indicates the weight of the gth mixture component. We denote the GMM model as</p><formula xml:id="formula_2">{ , , | }. g M 1 g g g f m r n R = =</formula><p>The likelihood of a feature vector given the GMM model can be evaluated using <ref type="bibr" target="#b0">(1)</ref>. Acoustic feature vectors are generally assumed to be independent. For a sequence of feature vectors</p><formula xml:id="formula_3">{ | }, x n T 1 X n f ! =</formula><p>the probability of observing these features given the GMM model is computed as</p><formula xml:id="formula_4">| | . x p p X n n T 1 m m = = ^ĥ h</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>%</head><p>Note that the order of the features is irrelevant in calculating the likelihood, which simplifies the computation for text-dependent speaker recognition. A GMM is usually trained using the expectation-maximization (EM) algorithm <ref type="bibr" target="#b99">[99]</ref>, which iteratively increases the likelihood of the data given the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ADAPTED GMMs: THE GMM-UbM SPEAKER-vERIFICATION SYSTEM</head><p>The GMM approach has been effective in speaker-identification tasks. For speaker verification, apart from the claimed speaker model, an alternate speaker model (representing speakers other than the target) is needed. In this way, these two models can be compared with the test data and the more likely model can be chosen, leading to an accept or reject decision. The alternate speaker model, also known as the background or world model, initiated the idea of using a UBM that represents everyone except the target speaker. It is essentially a large GMM trained to represent the speaker-independent distribution of the speech features for all speakers in general. The block diagram in Figure <ref type="figure">4</ref> becomes clear now since the background model is assumed to exist. Note that the UBM is assumed to be a "universal" model that serves as the alternate model for all enrolled speakers. Some methods have considered providing speaker-dependent unique background models <ref type="bibr" target="#b100">[100]</ref>, <ref type="bibr" target="#b101">[101]</ref>. However, using a single background model has been the most effective and meaningful strategy.</p><p>The UBM was first introduced as an alternate speaker model in <ref type="bibr" target="#b102">[102]</ref>. Later, in <ref type="bibr" target="#b5">[6]</ref>, the UBM was used as an initial model for the enrollment speaker GMMs. This concept was a significant </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed method</head><p>Model features using GMMs, compute similarity using feature likelihood Why robust?</p><p>The probabilistic nature of GMM allows more variability in the data advancement achieved by the so-called GMM-UBM method. In this approach, a speaker's GMM is adapted or derived from the UBM using Bayesian adaptation <ref type="bibr" target="#b103">[103]</ref>. In contrast to performing maximum likelihood training of the GMM for an enrollment speaker, this model is obtained by updating the well-trained UBM parameters. This relation between the speaker model and the background model provides better performance than independently trained GMMs and also lays the foundation for the speaker model adaptation techniques that were developed later. We will return to these relations as we proceed. In the following subsections, we describe the formulations of this approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The LR Test</head><p>Given an observation O and a hypothesized speaker , s the task of speaker verification can be stated as a hypothesis test between : , : .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H O s H O s</head><p>is from speaker is no from speaker t 0 1</p><p>In the GMM-UBM approach, the hypothesis H0 and H1 are represented by a speaker-dependent GMM s m and the UBM . 0 m Thus, for the set of observed feature vectors</p><formula xml:id="formula_5">{ | }, x n T 1 n f ! = X</formula><p>the LR test is performed by evaluating the following ratio:</p><formula xml:id="formula_6">( | ) ( | ) , p p H H reject accept &lt; X X s 0 0 0 $ m m x x '</formula><p>where x is the decision threshold. Usually, the LR test is performed in the logarithmic scale, providing the so-called log-LR</p><formula xml:id="formula_7">( ) ( | ) ( | ). log l og p p X X X s 0 m m K = -<label>(2)</label></formula><formula xml:id="formula_8">Maximum A Posteriori Adaptation of UBM Let { | } x n T 1 X n f ! =</formula><p>denote the set of acoustic feature vectors obtained from the enrollment speaker s. Given a UBM as in (1) and the enrollment speaker's data X, at first the probabilistic alignment of the feature vectors with respect the UBM components is calculated as These quantities are known as the zero-, first-, and second-order Baum-Welch statistics, respectively. Using these parameters, the posterior mean and covariance matrix of the features given the data vectors X can be found as</p><formula xml:id="formula_9">[ | ] ( ) ( ) , [ | ] ( ) ( ) . x F x x S E N g g E N g g X X g n s s g n n T s s = =</formula><p>The maximum a posteriori (MAP) adaptation update equations for weight, mean, and covariance, (3), (4), and (5), respectively, are proposed in <ref type="bibr" target="#b103">[103]</ref> and used in <ref type="bibr" target="#b5">[6]</ref> for speaker verification</p><formula xml:id="formula_10">[ ( )/ ( ) ] , N g T 1 g g s g g r a a r b = + - t (3) [ | ] ( ) x E 1 X g g g n g n a a = + - t , g n (4) [ | ] ( ) . x x E 1 X g g g n n T g g g g T g g T a a n n n n R R = + - + - t t t ^h<label>(5)</label></formula><p>The scaling factor b in (3) is computed from all the adapted mixture weights to ensure that they sum to unity. Thus, the new GMM parameters are a weighted summation of the UBM parameters and the sufficient statistics obtained from the observed data (see "GMM-UBM System: Summary"). The variable g a is defined as Here, r is known as the relevance factor. This parameter controls how the adapted GMM parameter will be affected by the observed speaker data. In the original study <ref type="bibr" target="#b5">[6]</ref>, this parameter was defined differently for the model weight, mean, and covariance. However, since only adaptation of the mean vectors turned out to be the most effective, we only use one relevance factor in our discussion here. Figure <ref type="figure">7</ref> shows an example of MAP adaptation for a twodimensional feature space with a four-mixture UBM case.</p><formula xml:id="formula_11">( ) ( ) . N g r N g g s s a = +<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>THE GMM SUPERvECTORS</head><p>One of the issues with speaker recognition is that the training and test speech data can be of different durations. This requires the comparison of two utterances of different lengths. Thus, one of the efforts toward effective speaker recognition has always been to obtain a fixed-dimensional representation of a single utterance <ref type="bibr" target="#b80">[80]</ref>. This is extremely useful since many different classifiers can be used on these utterance-level features from the machine-learning literature. One effective solution to obtaining a fixed-dimensional vector from a variable-duration utterance is the formation of a GMM supervector, which is essentially a large vector obtained by concatenating the parameters of a GMM model. Generally, a GMM supervector is obtained by concatenating the GMM mean vectors of a MAP-adapted speaker model, as in Figure <ref type="figure">7</ref>.</p><p>The term supervector was first used in this context for eigenvoice speaker adaptation in speech recognition applications <ref type="bibr" target="#b104">[104]</ref>. For speaker recognition, supervectors were first introduced in <ref type="bibr" target="#b105">[105]</ref>, motivating new model adaptation strategies involving eigenvoice and MAP adaptation. Researchers realized that these large dimensional vectors are a very good platform for designing channel compensation methods. Various effective modeling techniques were proposed to operate on the supervector space. The two dominating trends observed in these efforts were based on factor analysis (FA) and support vector machines (SVMs). They will be discussed next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GMM SUPERvECTOR SvMs</head><p>SVMs <ref type="bibr" target="#b106">[106]</ref> are one of the most popular supervised binary classifiers in machine learning. In <ref type="bibr" target="#b107">[107]</ref>, it was observed that GMM supervectors could be effectively used for speaker recognition/ verification using SVMs. The supervectors obtained from the training utterances were used as positive examples while a set of impostor utterances were used as negative examples. Channel compensation strategies were also developed in this domain, such as nuisance attribute projection (NAP) <ref type="bibr" target="#b108">[108]</ref> and within-class covariance normalization (WCCN) <ref type="bibr" target="#b109">[109]</ref>. Other approaches used SVM models for speaker recognition using short-and long-term features <ref type="bibr" target="#b39">[39]</ref>, <ref type="bibr" target="#b110">[110]</ref>. However, using GMM supervectors with SVM and NAP provided the most effective solution (see "GMM-SVM System: Summary").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SVMs</head><p>An SVM classifier aims at optimally separating multidimensional data points obtained from two classes using a hyperplane (a highdimensional plane). The model can then be used to predict the class of an unknown observation depending on its location with respect to the hyperplane. Given a set of training vectors and labels ( , )</p><formula xml:id="formula_12">x y n n for { }, n T 1f ! where x R n d ! and { , }, y 1 1 n ! -+</formula><p>the goal of SVM is to learn the function :</p><p>f R R d " so that the class label of an unknown vector x can be predicted as ( ( )) .</p><formula xml:id="formula_13">x x I f sign = ^h</formula><p>For a linearly separable data set <ref type="bibr" target="#b106">[106]</ref>, a hyperplane H given by , w x b 0 T + = can be obtained that separates the two classes, so that</p><formula xml:id="formula_14">( ) , . w x y b n T 1 1 n T n f $ + =</formula><p>An optimal linear separator H provides the maximum margin between the classes, i.e., the distance between H and the projections of the training data from the two different classes are maximum. The maximum margin is found to be / w 2 and data points xn for which ( ) , w x y b 1 n T n + = (i.e., points that lie on the margins, are known as support vectors). In a simple two-dimensional case, the operation of SVM is illustrated in Figure <ref type="figure">8</ref>. When training data are not linearly separable, the features can be mapped into a higher-dimensional space using Kernel functions where the classes become linearly separable. For more details on SVM training and kernels, refer to <ref type="bibr" target="#b106">[106]</ref> and <ref type="bibr" target="#b111">[111]</ref>. Compensation strategies that are developed for SVM-based speaker recognition (e.g., NAP and WCCN) are discussed in later sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FA OF THE GMM SUPERvECTORS</head><p>FA aims at describing the variability in high-dimensional observable data vectors using a lower number of unobservable/hidden Combines the effectiveness of adapted GMM as an utterance model and the discriminating ability of the SvM variables. For speaker recognition, the idea of explaining the speaker-and channel-dependent variability using FA in the GMM supervector space was first discussed in <ref type="bibr" target="#b112">[112]</ref>. Many variants of FA methods were employed since then, which finally led to the current state-of-the-art i-vector approach <ref type="bibr" target="#b79">[79]</ref>. In this section, we discuss these methods briefly to illustrate how the techniques have evolved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear Distortion Model</head><p>In the discussions to follow, a speaker-dependent GMM supervector ms is generally assumed to be a linear combination of four components. These components are as follows: </p><formula xml:id="formula_15">= + + +<label>(7)</label></formula><p>For acoustic features of dimension d and a UBM with M mixture components, these GMM supervectors are of dimension ( ). Md 1 # As an example, the speaker-and channel-independent supervector m0 is the concatenation of the UBM mean vectors. We denote the subvectors of m0 for the gth mixture as m [ ], g 0 which equals . g n In the following sections, we discuss how well-known linear Gaussian models, including FA, can be used to develop methods based on this generic decomposition of the GMM supervectors. A summary of the various linear statistical models in speaker recognition is included in Table <ref type="table">1</ref>, which highlights both formulation and specifics on matrix/model traits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classical MAP Adaptation</head><p>We revisit the MAP adaptation technique discussed previously in the GMM-UBM system. If we examine the adaptation equation ( <ref type="formula">4</ref>), which is used to update the mean vectors, it is clear that this is a linear combination of two components: one is speaker dependent and the other is independent. In a more generalized way, MAP adaptation can be represented as an operation on the GMM mean supervector as: , m m Dzs</p><formula xml:id="formula_16">s 0 = + (<label>8</label></formula><formula xml:id="formula_17">)</formula><p>where D is ( ) Md Md # a diagonal matrix and zs is a Md 1 # standard normal random vector. We dropped the subscript due to session h for simplicity. According to the linear distortion model of <ref type="bibr" target="#b6">(7)</ref>, . m Dzs spk = As discussed in <ref type="bibr" target="#b113">[113]</ref>, in the special case when we set ( / ) , D r 1 2 R = the MAP adaptation equations given in (4) <ref type="bibr" target="#b5">[6]</ref> arises from <ref type="bibr" target="#b7">(8)</ref>, where r is the relevance factor in <ref type="bibr" target="#b5">(6)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eigenvoice Adaptation</head><p>Perhaps the first FA-related model used in speaker recognition was the eigenvoice method <ref type="bibr" target="#b105">[105]</ref>. The eigenvoice method was initially proposed for speaker adaptation in speech recognition <ref type="bibr" target="#b114">[114]</ref>. In essence, this method restricts the speaker model parameters to lie in a lower dimensional subspace, which is defined by the columns of the eigenvoice matrix. In this model, a speaker-dependent GMM mean supervector ms is expressed as</p><formula xml:id="formula_18">, m m V y s s 0 = + (<label>9</label></formula><formula xml:id="formula_19">)</formula><p>where m0 is the speaker-independent supervector obtained from the UBM, the columns of the matrix V spans the speaker subspace, and ys are the standard normal hidden variables known as speaker factors. Here, we dropped the subscript h for simplicity.</p><p>In accordance with the linear distortion model in <ref type="bibr" target="#b6">(7)</ref>, the speakerdependent component is . m Vys spk = Note that this model does not have a residual noise term as in probabilistic PCA (PPCA) <ref type="bibr" target="#b115">[115]</ref> or FA. This means that the eigenvoice model is essentially equivalent to PCA. The model covariance is . VV T Since supervectors are usually of a large dimension, a full rank sample covariance matrix, i.e., the supercovariance matrix, is difficult to estimate with limited amount of data. Thus, EM algorithms <ref type="bibr" target="#b116">[116]</ref>, <ref type="bibr" target="#b117">[117]</ref> are used to estimate the eigenvoices. The speaker factors need to be estimated for an enrollment speaker. Computation [</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>tAbLE 1] a suMMAry oF tHE LInEAr StAtIStIcAL ModELS</head><p>In SPEAKEr rEcoGnItIon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ModEL ForMuLAtIon rEMArKS</head><p>ClAssiCAl MAP m m Dz</p><formula xml:id="formula_20">s s 0 = + d is diAgonAl, ( ) z 0 , I N s + EigEnvoiCE m m Vy s s 0 = + v is low rAnk, ( ) 0, I y N s + EigEnChAnnEl m m Ux Dz , s h s h 0 = + + u is low rAnk, ( , ) ( ) z x 0 I N , s h + JFA m m Ux Dz Vy , , s h h s s h 0 = + + + , U V ArE low rAnk, ( , , ) ( ) z 0I y x N , , s h h s + i-vECtor m m Tw , , s h s h 0 = + t is low rAnk, ( ) 0 I w N , , s h +</formula><p>of the likelihood is carried out as provided in [16, eq. ( <ref type="formula">19</ref>)], using the adapted supervector. This model implies that the adaptation of the GMM supervector parameters is restricted by the eigenvoice matrix. The advantage with this model is that when a small amount of data is available for adaptation, the adapted model is more robust as it is restricted to live in the speaker-dependent subspace, being less affected by nuisance directions. However, the eigenvoice model does not model the channel or intraspeaker variability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eigenchannel Adaptation</head><p>Similar to adapting the UBM toward a speaker model, a speaker model can also be adapted to a channel model <ref type="bibr" target="#b105">[105]</ref>. This can be useful when an unseen channel distortion is observed during testing, and the enrollment speaker model can be adapted to that channel. Similar to the eigenvoice model, the channel variability can also be assumed to lie in a subspace spanned by the principal eigenvectors of the channel covariance matrix. According to our distortion model <ref type="bibr" target="#b6">(7)</ref>, for a specific channel h, the term ,</p><formula xml:id="formula_21">m Uxh chn =</formula><p>where U is a low-rank matrix that spans the channel subspace, and ( , )</p><formula xml:id="formula_22">x I 0 N h !</formula><p>are the channel factors. When eigenchannel adaptation is combined with classical MAP, we obtain the model for speaker-and session-dependent GMM supervector</p><formula xml:id="formula_23">. m m Dz Ux , s h s h 0 = + +<label>(10)</label></formula><p>More details on training the hyperparameters D and U can be found in <ref type="bibr" target="#b113">[113]</ref>. Likelihood computation can be carried out in a similar way as the eigenvoice method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint FA</head><p>The joint FA (JFA) model is formulated by combining eigenvoice and eigenchannel together, which is accomplished by MAP adaptation for a single model (see "JFA: Summary"). This model assumes that both speaker and channel variability lie in lower dimensional subspaces of the GMM supervector space. These subspaces are spanned by the matrices V and U, as before. The model assumes, for a randomly chosen utterance obtained from speaker s and session , h that its GMM mean supervector can be represented by</p><formula xml:id="formula_24">. m m Ux Vy Dz , , s h h s s h 0 = + + +<label>(11)</label></formula><p>Thus, this is the only model so far that considers all four components of the linear distortion model we discussed previously.</p><p>Indeed, JFA was shown to outperform the other contemporary methods. More details on implementation of JFA can be found in <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b118">[118]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The i-Vector Approach</head><p>As discussed previously, SVM classifiers on GMM supervectors have been a very successful approach for robust speaker recognition. FA based methods (especially the JFA technique) were also among state-of-the-art systems. In an attempt to combine the strengths of these two approaches, Dehak et al. <ref type="bibr" target="#b79">[79]</ref>, <ref type="bibr" target="#b119">[119]</ref>, <ref type="bibr" target="#b120">[120]</ref> attempted to use JFA as a feature extractor for SVMs. In their initial attempt <ref type="bibr" target="#b119">[119]</ref>, the speaker factors estimated using JFA were used as features for SVM classifiers. Observing the fact that the channel factors also contain speaker-dependent information, the speaker and channel factors were combined into a single space termed the total variability space <ref type="bibr" target="#b79">[79]</ref>, <ref type="bibr" target="#b120">[120]</ref>. In this FA model, a speaker-and sessiondependent GMM supervector is represented by</p><formula xml:id="formula_25">. m m Tw , , s h s h 0 = +<label>(12)</label></formula><p>The hidden variables ( , ) w 0I N , s h + in this case are called total factors. Similar to all of the FA methods above, the hidden variables are not observable but can be estimated by their posterior expectation. The estimates of the total factors, which can be used as features to the next stage of classifiers, came to be known as the i-vectors. The term i-vector is a short form of "identity vector," regarding the speaker-identification application, and also of "intermediate vectors," referring to its intermediate dimension between those of a supervector and an acoustic feature vector <ref type="bibr" target="#b79">[79]</ref> (see "The i-Vector System: Summary").</p><p>Unlike JFA or other FA methods, the i-vector approach does not make a distinction between speaker and channel. It is simply a dimensionality reduction method of the GMM supervector. In essence, <ref type="bibr" target="#b11">(12)</ref> is very similar to a PCA model on the GMM supervectors. The T matrix is trained using the same algorithms as for the eigenvoice model, except that each utterance is assumed to be obtained from a different speaker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mismatch Compensation In i-Vector Domain</head><p>The i-vector approach itself does not perform any compensation; on the contrary, it only provides a meaningful lower-dimensional (400 , 800) representation of a GMM supervector. Thus, it has Exploits the behavior of speakers' features in variety of channel conditions learned using FA most of the advantages of the supervectors, but because of its lower dimension, many conventional compensation strategies can be applied to speaker recognition, which were previously not practical with the large-dimensional supervectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LINEAR DISCRIMINANT ANALYSIS</head><p>Linear discriminant analysis (LDA) is a commonly employed technique in statistical pattern recognition that aims at finding linear combinations of feature coefficients to facilitate discrimination of multiple classes. It finds orthogonal directions in the feature space that are more effective in discriminating the classes. Projecting the original features in these directions improve classification accuracy. Let D indicate the set of all development utterances, w , s i indicates an utterance feature (e.g., supervector or i-vector) obtained from the ith utterance of speaker , s ns denotes the total number of utterances belonging to speaker , s and S is the total number of speakers in D. The between-and within-class covariance matrices are given by S w w w w S </p><formula xml:id="formula_26">w S n 1 1 , , w s s S s i s si s T i n 1 1 s = - - = = r r / /<label>(13) ( )( ), S w w w</label></formula><p>where the speaker-dependent and speaker-independent mean vectors are given by</p><formula xml:id="formula_28">w w n 1 and , s s s i i n 1 s = = r / , w w n S 1 1 , s s s s i i n 1 1 s = = = r / /</formula><p>respectively. The LDA optimization thus aims at maximizing the between class variance while minimizing the within-class variance (due to channel variability). The projections obtained from this optimization are found by the solution of the following generalized eigenvalue problem:</p><formula xml:id="formula_29">S v S v. b w K =<label>(15)</label></formula><p>Here, K is the diagonal matrix containing the eigenvalues. If the matrix Sw is invertible, this solution can be found by finding the eigenvalues of the matrix S . S</p><formula xml:id="formula_30">w b 1 - Generally, the first k R &lt; eigenvalues are used to prepare a matrix ALDA of dimension R k # given by [ ] , A v vk 1 LDA f =</formula><p>where v vk 1 f denote the first k eigenvectors obtained by solving <ref type="bibr" target="#b14">(15)</ref>. The LDA transformation of the utterance feature w is thus obtained by</p><formula xml:id="formula_31">( ) . w A w LDA LDA T U =</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NAP</head><p>The NAP algorithm was originally proposed in <ref type="bibr" target="#b108">[108]</ref>. In this approach, the feature space is transformed using an orthogonal projection in the channel's complementary space, which depends only on the speaker (assuming that other variability in the data is insignificant). The projection is calculated using the within-class covariance matrix. Define a d d # projection matrix <ref type="bibr" target="#b108">[108]</ref> of corank k d &lt;</p><formula xml:id="formula_32">, P I u u [ ] [ ] k k T = -</formula><p>where u[ ] k is a rectangular matrix of low rank whose columns are the k principal eigenvectors of the within-class covariance matrix Sw given in <ref type="bibr" target="#b13">(14)</ref>. NAP is performed on w as ( ) . w Pw NAP U =</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WCCN</head><p>This normalization was originally proposed for improving robustness in the SVM-based speaker-recognition framework <ref type="bibr" target="#b109">[109]</ref> using a one-versus-all decision approach. The WCCN projection aims at minimizing the false-alarm and miss-error rates during SVM training. The implementation of the strategy begins with using a data set D similar to the one that was described in the previous section. The within-class covariance matrix Sw is calculated using <ref type="bibr" target="#b13">(14)</ref>, and the WCCN projection is performed as</p><formula xml:id="formula_33">( ) w w , A T WCCN WCCN U =</formula><p>where AWCCN is computed through the Cholesky factorization of</p><formula xml:id="formula_34">Sw 1 -such that . S A A T w 1 WCCN WCCN = -</formula><p>In contrast to LDA and NAP, the WCCN projection conserves the directions of the feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SPEAKER vERIFICATION USING i-vECTORS</head><p>After i-vectors were introduced, in essence, many previously available pattern-recognition methods were effectively applied in this domain. We discuss some of the popular methods of classification using i-vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SVM Classifier</head><p>As discussed previously, the i-vector representation was discovered in an attempt to utilize JFA as a feature extractor for SVMs. Thus, initially i-vectors were used with SVMs with different kernel functions <ref type="bibr" target="#b79">[79]</ref>. The idea is the same as SVM with GMM supervectors, except that the i-vectors are now used as utterance-dependent features. Because of the lower dimension of the i-vectors compared to supervectors, the application of LDA and WCCN projections together became more effective and were well suited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cosine Distance Scoring</head><p>In <ref type="bibr" target="#b79">[79]</ref>, the cosine similarity measure-based scoring was proposed for speaker verification. In this measure, the match score between a target and test i-vector wtarget and wtest is computed as their normalized dot product </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Probabilistic Linear Discriminant Analysis</head><p>Probalistic LDA (PLDA) was first used for session variability compensation for facial recognition <ref type="bibr" target="#b121">[121]</ref>. This essentially follows the same modeling assumptions as JFA, i.e., a pattern vector contains class-dependent and session-dependent variabilities, both lying in lower-dimensional subspaces. An i-vector extracted from utterance u is decomposed as</p><formula xml:id="formula_35">. w w , , s h s h s h 0 b a f U C = + + +<label>(16)</label></formula><p>Here, w R R 0 ! is the speaker-independent mean i-vector, U is the PLDA was first introduced in speaker verification in <ref type="bibr" target="#b94">[94]</ref> using a heavy-tailed distribution assumption on i-vectors instead of a Gaussian assumption. Later, it was shown that when i-vectors are length normalized (i.e., they are divided by their corresponding vector length) <ref type="bibr" target="#b122">[122]</ref>, a Gaussian PLDA model performs equivalent to its heavy-tailed version. Since the is computationally more expensive, Gaussian PLDA models are more commonly used. Also, the use of a full-covariance noise model for , h s f is feasible in this formulation that allows one to drop the eigenchannel term ) ( h a C from ( <ref type="formula" target="#formula_35">16</ref>) without loss of performance. In this case, the PLDA model would be as follows:</p><p>. w w</p><formula xml:id="formula_36">, , s h s s h 0 b f U = +</formula><p>We note that, though developed independently, the JFA model is very similar to PLDA. Looking at <ref type="bibr" target="#b10">(11)</ref> and ( <ref type="formula" target="#formula_35">16</ref>) and comparing the terms makes this clear. The obvious difference between these models is that JFA models the GMM supervectors, while PLDA models i-vectors. Since i-vectors are essentially dimensionality reduced versions of supervectors (incurring loss of information), JFA, in principle, should be better in modeling the within-and between-speaker variations. However, in reality, the amount of labeled training data is limited, and due to the large number of parameters in JFA, it cannot be trained as effectively as a PLDA model on lower dimensional i-vectors (using the same amount of labeled data). Besides, the total variability model (i-vector extractor) can be trained on unlabeled data sets, which are available in larger amounts.</p><p>Although the model equations are identical, there are significant differences in the training process of the two models. Since JFA was designed for GMM supervectors, the formulations involved processing the acoustic speech frames and their statistics in different mixtures of the UBM. Unlike i-vectors, the GMM supervectors are not extracted first before JFA training-instead, JFA operates directly on the acoustic features and can provide similarity scores between two utterances from their corresponding feature streams. This dependence on acoustic features (and the various order statistics) makes the scoring process more computationally expensive for JFA. For PLDA, the input features are i-vectors that are extracted beforehand, and, during the scoring process, only two i-vectors from the corresponding utterances are required-not the acoustic features. This makes PLDA much simpler in implementation.</p><p>It can be argued that, with a sufficiently large labeled data set, JFA can outperform an i-vector-PLDA system. However, we are not aware of such results reported at this time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PERFORMANCE EVALUATION IN STANDARDIZED DATA SETS</head><p>Evaluating the performance of a speaker-verification task using a standardized data set is a very important element of the research cycle. Over the years, new data sets and performance metrics have been introduced to match realistic scenarios. These, in turn, motivated researchers to discover new strategies to address the challenges, compare results among peers, and exchange ideas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>THE NIST SRE CHALLENGE</head><p>NIST has been organizing an SRE campaign for the past several years aiming at providing standard data sets, verification tasks, and performance metrics for the speaker ID community (Figure <ref type="figure">9</ref>). Every year's evaluation introduces new challenges for the research community. These challenges include newly introduced recording conditions (e.g., microphone, handset, and room acoustics), short test utterance duration, varying vocal effort, artificial and real-life additive noise, restrictions or allowances in data-utilization strategy, new performance metrics to be optimized, etc. It is clear that the performance metric defined for a speaker-recognition task depend on the data set and train-test pairs of speech (also known as trials) used for the evaluation. A sufficient number of such trials needs to be provided for a statistically significant evaluation measure <ref type="bibr" target="#b78">[78]</ref>. The performance measures can be based on hard verification decisions or soft scores, they may require log-LR as scores, and depend on the prior probability of encountering a target speaker. For a given data set and task, systems evaluated using a specific error/cost criteria can be compared. Before discussing the common performance measures, we introduce the type of errors encountered in speaker verification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TYPES OF ERRORS</head><p>There are mainly two types of errors in speaker verification (or any other biometric authentication) when a hard decision is made by the automatic system. From the speaker authentication point of view, we define them as ■ false accept (FA): granting access to an impostor speaker ■ false reject (FR): denying access to a legitimate speaker.</p><p>From the speaker-detection point of view (a target speaker is sought), these are called false-alarm and miss errors, respectively. According to these definitions, two error rates are defined as . This score is a scalar variable that represents the similarity between the enrolled speaker and the test speaker, with higher values indicating the speakers are more similar. To make a decision, the system needs to use a threshold ( ) x as illustrated in Figure <ref type="figure" target="#fig_10">10</ref>. If the threshold is too low, there will be a lot of FA errors, whereas if the threshold is too high, there will be too many FR/miss errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EQUAL ERROR RATE</head><p>The equal error rate (EER) is defined as the FAR and FRR values when they become equal. That is, by changing the threshold, we find a point where the FAR and FRR become equal. This is shown in Figure <ref type="figure" target="#fig_10">10</ref>. The EER is a very popular performance measure for speaker-verification systems. Only the soft scores from the automatic system are required to compute the EER. No actual hard decisions are made. It should be noted that operating a speaker-verification system on the threshold corresponding to the EER might not be desirable for practical purposes. For high-security applications, one should set the threshold higher, lowering the FA errors at the cost of miss errors. However, for high convenience, the threshold may be set lower. Let us discuss some examples. In authenticating users for bank accounts, security is of utmost importance. It is thus better to deny access to the legitimate user (and ask other forms of verification) as opposed to granting access to an impostor. On the contrary, for an automated customer service, denying a legitimate speaker will cause inconvenience and frustration to the user. In this case, accepting an illegitimate speaker is not as critical as in high-security applications.</p><p>DETECTION COST FUNCTION This is, in fact, a family of performance measures introduced by NIST over the years. As mentioned before, the EER does not differentiate between the two errors, which sometimes is not a realistic performance measure. The detection cost function (DCF), thus, introduces numerical costs/penalties for the two types of errors (FA and miss). The a priori probability of encountering a target speaker is also provided. The DCF is computed over the full range of decision threshold values as Usually, the DCF is normalized by dividing it by a constant <ref type="bibr" target="#b77">[77]</ref>. The probability values here can be computed using the distribution of true and impostor scores and computing the areas under the curve as shown in Figure <ref type="figure" target="#fig_10">10</ref>. The first three quantities above ( , , C C Miss FA and ) PTarget are predefined. Generally, the goal of the system designer is to find the optimal threshold value that minimizes the DCF.</p><p>In NIST SRE 2008, these DCF parameters were set as , 10 CMiss = , 1 CFA = and . . 0 01 PTarget =</p><p>The values of the costs indicate that the system is penalized ten times more for making a miss error rather than an FA error. As a real-world example, when detecting a known criminal's voice from evidence recordings, it may be better to have false positives (e.g., to suspect and investigate an innocent speaker) than to miss the target speaker (e.g., to be unable to detect the criminal at all). If we ignore PTarget for the moment, setting a lower threshold ( )</p><p>x would be beneficial since, in this case, the system will tolerate more FAs but will not miss too many legitimate speakers ( ) [PMiss x will be lower], yielding a lower DCF value for that threshold. Now, the value of the prior ( . ) 0 01 PTarget = indicates that a target speaker will be encountered by the system once in every 100 speaker-verification attempts. If this condition is considered independently, it is better to have a higher threshold since most of the attempts will be from impostors ( . ). 0 99 PNon arget t = However, when all three parameters are considered together, finding the optimal threshold requires sweeping through all the DCF values.</p><p>By processing the DCF, two performance measures are derived: 1) the minimum DCF (MinDCF) and 2) the actual DCF (ActDCF). The MinDCF is the minimum value of DCF that can be obtained by changing the threshold, .</p><p>x The MinDCF parameter can be computed only when the soft scores are provided by the systems.</p><p>When the system provides hard decisions, the actual DCF is used where the probability values involved (in the DCF equation) are simply computed by counting the errors. Both of these performance measures have been extensively used in the NIST evaluations. The most recent evaluation in 2012 introduced a DCF that is a dependent on two different operating points (two sets of error costs and target priors) instead of one.</p><p>It is important to note here that the MinDCF (or ActDCF) parameter is not an error rate in the general sense. Thus, its interpretation is not straightforward. Obviously, the lower MinDCF, the better the system performance. However, the exact value of the MinDCF can only be used to compare other systems evaluated using the same trials and performance measure. Generally, when the system EER improves, the DCF parameters also improve. An elaborate discussion on the relationship between EER and DCF can be found in <ref type="bibr" target="#b124">[124]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DETECTION ERROR TRADEOFF CURvE</head><p>When speaker-verification performance needs to be evaluated in a range of operating points, the detection error tradeoff (DET) curve is generally employed. The DET curve is a plot of the errors FAR versus FRR/miss. An example DET curve is shown in Figure <ref type="figure">11</ref>. As the system performance improves, the curve moves toward the origin. As illustrated in Figure <ref type="figure">11</ref>, the DET curve corresponding to System 2 is closer to the origin and thus represents a better system. The EER and minDCT points are shown on the DET curve of System 1.</p><p>During the preparation of the DET curve, the cumulative density functions (CDFs) of the true and impostor scores are transformed to normal deviates. This means that the true/impostor score CDF value for a given threshold is transformed by a standard normal inverse CDF (ICDF) and the resulting values are used to make the plot. This transform yields a linear DET curve when the two distributions are normal and have equal variances. Thus, even though the labels indicate the axis as error probabilities, they are actually plotted according to the corresponding normal deviate values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RECENT ADVANCEMENTS IN AUTOMATIC SPEAKER RECOGNITION</head><p>In recent years, considerable research progress has been made in spoofing and countermeasures <ref type="bibr" target="#b125">[125]</ref>, <ref type="bibr" target="#b126">[126]</ref>, back-end classifiers <ref type="bibr" target="#b127">[127]</ref>, <ref type="bibr" target="#b128">[128]</ref>, compensation for short utterances <ref type="bibr" target="#b129">[129]</ref>- <ref type="bibr" target="#b131">[131]</ref>, score calibration and fusion <ref type="bibr" target="#b132">[132]</ref>, <ref type="bibr" target="#b133">[133]</ref>, deep neural network (DNN) <ref type="bibr" target="#b134">[134]</ref>- <ref type="bibr" target="#b136">[136]</ref>, and alternate acoustic modeling <ref type="bibr" target="#b137">[137]</ref> techniques. In this section, we briefly discuss some of these topics and their possible implications in the speaker-recognition research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NIST i-vECTOR MACHINE-LEARNING CHALLENGE AND bACK-END PROCESSING</head><p>The most recent NIST-sponsored evaluation, the i-Vector Machine-Learning Challenge, focused on back-end classifiers. In this paradigm, instead of audio data, i-vectors from speech utterances were provided to the participants <ref type="bibr" target="#b138">[138]</ref>. In this way, the entry barrier to the evaluation was reduced as many machinelearning-focused research groups were able to participate without expertise in audio/speech processing. Significant performance improvements were observed from top-performing systems compared to the baseline system provided by NIST <ref type="bibr" target="#b138">[138]</ref>. Since only i-vectors were provided by NIST, the algorithmic improvements are all due to modeling and back-end processing of i-vectors. In addition, the i-vectors provided by NIST did not have any speaker labels, which also generated new ideas on utilizing unlabeled data in speaker recognition <ref type="bibr" target="#b139">[139]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DURATION vARIAbILITY COMPENSATION</head><p>Duration variability is one of the problems that has received considerable attention in recent years. Since the advent of GMM supervectors and i-vectors, variable-duration utterances could be mapped to a fixed-dimensional pattern. This has been a significant advancement since various machine-learning tools were being applied to these vectors, especially i-vectors due to their smaller dimensions. However, it is clear that an i-vector extracted from a short utterance will not be as representative of a speaker compared to the one extracted from a longer utterance. Duration mismatch between train and test is thus a major problem. One way to mitigate this problem is by including short utterances in the PLDA training <ref type="bibr" target="#b130">[130]</ref>, <ref type="bibr" target="#b140">[140]</ref>. Alternatively, this can be addressed in the score domain <ref type="bibr" target="#b130">[130]</ref>. In <ref type="bibr" target="#b141">[141]</ref>, <ref type="bibr">Kenny et al.</ref> propose that i-vectors extracted from short utterances are less reliable and incorporates this variability by including a noise term into the PLDA model. In <ref type="bibr" target="#b142">[142]</ref>, a DNN-based method was proposed for speaker recognition in short utterances where the content of the test utterance was searched in the enrollment data to be compared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DNN-bASED METHODS</head><p>In the last few years, DNNs have been tremendously successful at many speech-processing tasks, most prominently in speech recognition <ref type="bibr" target="#b143">[143]</ref>, <ref type="bibr" target="#b144">[144]</ref>. Naturally, DNNs have also been used in speaker recognition. Works by Kenny et al. <ref type="bibr" target="#b136">[136]</ref> have shown improvements in extracting Baum-Welch statistics for speaker recognition using DNNs. DNNs have also been incorporated for multisession speaker recognition <ref type="bibr" target="#b134">[134]</ref> as well as phonetically aware DNNs for noise-robust speaker recognition <ref type="bibr" target="#b135">[135]</ref>. DNNs have also been used to extract front-end features, also known as bottle-neck features <ref type="bibr" target="#b145">[145]</ref>. Since, there are an extensive set of literature on deep learning <ref type="bibr" target="#b143">[143]</ref>, <ref type="bibr" target="#b146">[146]</ref> and its application in speaker recognition is relatively new, we have not included a discussion on DNNs in this tutorial.</p><p>[ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAn vErSuS MAcHInE In SPEAKEr rEcoGnItIon</head><p>In this section, we attempt to compare the speaker-recognition task as performed by humans and the state-of-the-art algorithms. First we must realize that it is very difficult to do such comparisons in a statistically meaningful manner. This is because getting humans to evaluate a large number of utterances reliably is quite challenging. However, attempts have been made to make such comparisons in the past <ref type="bibr" target="#b147">[147]</ref>- <ref type="bibr" target="#b149">[149]</ref>. In the majority of these cases, especially in the recent ones, the speaker-recognition performance of humans was found to be inferior to that of automatic systems.</p><p>In <ref type="bibr" target="#b150">[150]</ref>, the authors compared the speaker-recognition performance of human listeners to a typical algorithm (automatic system is not mentioned in the paper) using a subset of NIST SRE 1998 data. of multiple human listeners were combined to form the final speaker-recognition decision. Results showed that humans are as good as the best system and outperformed standard algorithms especially when there is a mismatch in the telephone channel (a different number was used to make the phone call).</p><p>Recently, NIST presented speaker-recognition tasks for evaluating systems that combined human and machines <ref type="bibr" target="#b19">[20]</ref>. The task, known as the HASR, was designed in a way such that the most difficult test samples are selected for the evaluation (channel mismatch, noise, same/different speakers that sound highly dissimilar/similar, etc.). However, the total number of trials in these experiments was very low compared to evaluations designed for automatic systems. One of the motivations of this study was to evaluate if automatic systems have become good enough, in other words, is it beneficial to keep humans involved in the process? The HASR study was repeated during the 2012 NIST SRE where both noisy and channel degraded speech data were encountered.</p><p>Interestingly, machines consistently performed better than human-assisted approaches in the given NIST HASR tasks <ref type="bibr" target="#b151">[151]</ref>- <ref type="bibr" target="#b155">[155]</ref>. In <ref type="bibr" target="#b156">[156]</ref>, it was even claimed that by combining multiple naïve listeners' decisions, the HASR 2010 task can be performed as well as forensic experts, which somewhat undermines the role of a forensic expert. In <ref type="bibr" target="#b157">[157]</ref>, it was shown that human and machine decisions were complementary, meaning that in some cases the humans correctly identified a speaker where the automatic system failed, and vice versa. However, the HASR tasks were exceptionally difficult for human listeners because of the severe channel mismatch, unfamiliarity with the speakers, noise, and other factors. A larger and more balanced set of trials should be used for a proper evaluation of human performance. Following the HASR paradigm, further research focused on how humans can aid the decision of an automatic system, especially in the context of forensic speaker recognition <ref type="bibr" target="#b157">[157]</ref>. An i-vector system <ref type="bibr" target="#b79">[79]</ref> with a PLDA classifier was used in this particular study.</p><p>The performance of humans and machines was compared in a forensic context in <ref type="bibr" target="#b149">[149]</ref>, where 45 trials were used (nine target and 36 nontarget). The human system consisted of a panel of listeners whereas a GMM-UBM-based system <ref type="bibr" target="#b5">[6]</ref> was used for the automatic system. Here again, the automatic system outperformed the human panel of listeners. However, the results should be interpreted with caution since the number of trials here was low.</p><p>In <ref type="bibr" target="#b158">[158]</ref>, human speaker-recognition performance was compared with automatic algorithms in presence of voice mimicry. A GMM-UBM system and an i-vector-based system were used in the study. The speech database consisted of five Finnish public figures and their voices were mimicked by a professional voice actor. The results show that humans are more likely to make errors when impersonation is done. On average, the automatic algorithm performed better than the human listeners.</p><p>Although most experiments so far show human performance to be inferior to automatic algorithms, these cannot be considered as definitive proof that machines are always better than humans. In many circumstances, humans will perform better, especially when paralinguistic information becomes important. As discussed previously, humans perform exceptionally well in recognizing familiar speakers. To the best of our knowledge, a comparison of familiar speaker recognition versus automatic algorithm (with sufficient training data) has not been performed yet. Thus, for familiar speakers, humans may perform much better than state-of-the-art algorithms-and this should motivate researchers to discover how the human brain stores familiar speakers' identity information. In HASR, the goal was to have humans assist the automatic system. On the other hand, automatic systems inspired by the forensic experts' methodology have already been investigated <ref type="bibr" target="#b159">[159]</ref>, where speaker nativeness, dialect, and other demographic information were considered. A generic comparison between how humans and machines perform speaker recognition is provided in Table <ref type="table" target="#tab_7">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>concLuSIonS</head><p>A substantial amount of work still needs to be done to fully understand how the human brain makes decisions about speech content and speaker identity. However, from what we know, it can be said that automatic speaker-recognition systems should focus more on high-level features for improved performance. Humans are effective at effortlessly identifying unique traits of speakers they know very well, whereas automatic systems can only learn a specific trait if a measurable feature parameter can be properly defined. Automatic systems are better at searching over vast collections of audio and, perhaps, at being able to more effectively set aside those audio samples which are less likely to be speaker matches; whereas humans are better at comparing a smaller subset and overcoming microphone or channel mismatch more easily. It may be worthwhile to investigate what it really means to "know" a speaker from the perspective of an automatic system. The search for alternative compact representations of speakers and audio segments emphasizing the identity relevant parameters while suppressing the nuisance components will always be an ongoing challenge for system developers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AutHorS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Digital Object Identifier 10.1109/MSP.2015.2462851 Date of publication: 13 October 2015</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>I</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>An experiment on finding voice-selective regions of the human brain using fMrI. (a) the experimental paradigm: spectrograms (0-4 kHz) and amplitude waveforms of examples of auditory stimuli. vocal (voc) and nonvocal (nvoc) stimuli are presented in 20-second blocks with 10-second silence intervals. (b) voice-sensitive activation regions in the group average: regions with significantly (P &lt; 0.001) higher response to human voices than to energy-matched nonvocal stimuli are shown in color scale (t-value) on an axial slice of the group-average MrI (center) and on sagittal slices (vertical plane dividing the brain into left and right halves) of each hemisphere. (Figure adapted from [4].)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a)-(c) the regions of the human brain that contribute the most in discriminating between vowels (red) and speakers (blue). (b) and (c) Enlarged representations of the auditory cortex (region of the brain sensitive to sounds). (d) and (e) Activation patterns of sounds created from the 15 most discriminative voxels (of the fMrI) for decoding (d) vowels and (e) speakers. Each axis of the polar plot forming a pattern displays the normalized activation level in a voxel. note the similarity among the patterns of the same vowel [horizontal direction in (d)] or speaker [vertical direction in (e)]. (Figure reprinted from [5].) An overall block diagram of a basic speaker-verification system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>[</head><label></label><figDesc>FIG5] (a) A speech waveform with voice-activity decisions (1 versus 0 values indicate speech versus silence) and (b) a spectrogram plot of the corresponding speech waveform.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>[</head><label></label><figDesc>FIG6] Steps in MFcc feature extraction from a speech frame: (a) 200-sample frame representing 25 milliseconds of speech sampled at a rate of 8 kHz, (b) dFt power spectrum showing first 101 points, (c) 24-channel triangular Mel-filter bank, (d) log filter-bank energy outputs from Mel-filter, and (e) 12 static MFccs obtained by performing dct on filter-bank energy coefficients and retaining the first 12 values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>diagram of a GMM-ubM system using a four-mixture ubM. MAP adaptation procedure and supervector formation by concatenating the mean vectors are also illustrated. (a) A schematic diagram of a GMM-ubM system using a four-mixture ubM. (b)MAP adaptation procedure and supervector formation by concatenating the mean vectors are also illustrated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>A conceptual illustration of an SvM classifier: Positive (+) and negative (-) examples are correspondingly labeled, with the optimal linear separator and support vectors shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>10 [</head><label>10</label><figDesc>FIG9] A graphical representation of 79 utterances spoken by ten individuals collected from the nISt SrE 2004 corpus. the i-vector representation is used for each segment; the plot is generated using GuESS, an open-source graph exploration software<ref type="bibr" target="#b123">[123]</ref> that can visualize higher-dimensional data using distance measures between samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>False-Acceptance Rate (FAR) Number of impostor attempts Number of FA errors False-Rejection Rate (FRR) Number of legitimate attempts Number of FR errors = = Speaker-verification systems generally output a match score between the training speaker and the test utterance. This is true for most two-class recognition/binary detection problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>of a miss/FR error CFA = Cost of an FA error P arget T = Prior probability of target speaker. ( ) PM x iss = Probability of (Miss | Target, Threshold = ) x ( ) PFA x = Probability of (FA | Nontarget, Threshold = ). An illustration of target and nontarget score distributions and the decision threshold. Areas under the curves with blue and red colors represent FAr and Frr errors, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>[</head><label></label><figDesc>FIG11] dEt curves of two speaker-verification systems (System 1 and System 2). In System 1, the points on the curve corresponding to the threshold that yields the EEr and minimum dcF (as in nISt SrE 2008), and the direction of an increasing threshold are shown. being closer to the origin, System 2 shows a better performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>John H.L.Hansen  (John.Hansen@utdallas.edu) received his Ph.D. and M.S. degrees in electrical engineering from the Georgia Institute of Technology and his B.S.E.E. degree from Rutgers University, New Jersey. He is with the University of Texas at Dallas, where he is associate dean for research and professor of electrical engineering. He holds the Distinguished Chair in Telecommunications and oversees the Center for Robust Speech Systems. He is an International Science Congress Association (ISCA) fellow; a past chair of the IEEE Speech and Language Technical Committee; a coorganizer and technical chair of the IEEE International Conference on Acoustics, Speech, and Signal Processing 2010; a coorganizer of IEEE Spoken Language Technology 2014; and the general chair/organizer of ISCA Interspeech 2002. He has supervised more than 70 Ph.D./M.S. students and coauthored more than 570 papers in the field. His research interests include digital speech processing, speech and speaker analysis, robust speech/speaker/language recognition, speech enhancement for hearing loss, and robust hands-free human-interaction in car environments. He is a Fellow of the IEEE. Taufiq Hasan (taufiq.hasan@utdallas.edu) received his B.S. and M.S. degrees in electrical and electronic engineering (EEE) from Bangladesh University of Engineering and Technology, Dhaka, Bangladesh, in 2006 and 2008, respectively. He earned his Ph.D. degree from the Department of Electrical Engineering, Erik Jonsson School of Engineering and Computer Science, University of Texas at Dallas (UT Dallas), Richardson, in 2013. From 2006 to 2008, he was a lecturer in the Department of EEE, United International University, Dhaka. He served as the lead student from the Center for Robust Speech Systems at UT Dallas during the 2012 National Institute of Standards and Technology Speaker Recognition Evaluation submissions. Currently, he works as a research scientist at Robert Bosch Research and Technology Center in Palo Alto, California. His research interests include speaker recognition in mismatched conditions, speech recognition, enhancement and summarization, affective computing, and multimodal signal processing. rEFErEncES</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>/degradations, especially the extrinsic ones. Additive noise and transmission channel variability have received much attention recently. Intrinsic variability in speech is very difficult to quantify and account/address for in automatic assessment. Higherlevel knowledge may become important in these cases. For example, even if a person's voice (spectral characteristics) may change due to his or her current health (e.g., a cold) or aging, the person's accent or style of speech remains generally the same. Forensic experts pay special attention to these details when detecting a subject's voice from potential suspects' speech recordings.</figDesc><table><row><cell cols="2">Speaker Based</cell><cell>Technology Based</cell><cell>Conversation Based</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Human-to-Human Hu</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Human-to-Machine Hu</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Prompted/Read Speech Prom</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Spontaneous Speech Spo</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Monologue</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Two-Way Conversation Two-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Group Discussion Gr</cell></row><row><cell>Microphone/ Microphone/</cell><cell cols="2">Noise, Signal-to-Noise Ratio Noise, Signal-to-Noise Ratio</cell><cell>Speech Utterance Space (Two Dimensional) Speech Utterance Sp</cell></row><row><cell>Sensors Sensors</cell><cell></cell><cell></cell></row><row><cell>Vocabulary, Turn Taking Vocabulary, Turn Taking</cell><cell></cell><cell>Language/ Culture Language/ Culture</cell><cell>Variability Across Speakers</cell></row><row><cell>Speaker Speaker</cell><cell></cell><cell>Accent/ Dialect Accent/ Dialect</cell></row><row><cell>Task Stress Task Stress</cell><cell></cell><cell>Emotion Emotion</cell><cell>Variability</cell></row><row><cell>Lombard Effect</cell><cell></cell><cell>Vocal Effort Vocal Effort</cell><cell>Within Speaker</cell></row></table><note><p>variability</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>GMM-bASEd SPEAKEr rEcoGnItIon: SuMMAry</head><label></label><figDesc></figDesc><table><row><cell>First proposed</cell><cell>Reynolds et al. (1995) [96]</cell></row><row><cell cols="2">Previous methods Averaging of long-term features,</cell></row><row><cell></cell><cell>vQ-based methods [80], [97], [98]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>tHE i-vEctor SyStEM: SuMMAry</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>First proposed</cell><cell>Dehak et al. (2009) [79]</cell></row><row><cell></cell><cell cols="2">Previous methods JFA and GMM-SvM-based approaches</cell></row><row><cell></cell><cell cols="2">Proposed method Reduce supervector dimension using</cell></row><row><cell></cell><cell></cell><cell>FA before classification</cell></row><row><cell></cell><cell>Why robust?</cell><cell>i-vectors effectively summarize utter-</cell></row><row><cell></cell><cell></cell><cell>ances and allows using compensa-</cell></row><row><cell></cell><cell></cell><cell>tion methods that were not practical</cell></row><row><cell></cell><cell></cell><cell>in large dimensional supervectors</cell></row><row><cell>JFA: SuMMAry</cell><cell></cell></row><row><cell>First proposed</cell><cell>Kenny et al. (2004) [118]</cell></row><row><cell cols="2">Previous methods MAP adapted GMM, GMM-SvM</cell></row><row><cell></cell><cell>approach</cell></row><row><cell cols="2">Proposed method Model speaker and channel vari-</cell></row><row><cell></cell><cell>ability in GMM supervectors</cell></row><row><cell>Why robust?</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>tAbLE 2 ] tHE SPEAKEr-rEcoGnItIon ProcESS: MAn vErSuS MAcHInE.</head><label>2</label><figDesc></figDesc><table><row><cell>ASPEct</cell><cell>HuMAnS</cell><cell>MAcHInES</cell></row><row><cell>trAining</cell><cell>sPEAkEr rECognition is An ACquirEd</cell><cell>rEquirEs suFFiCiEnt dAtA to trAin thE rECognizErs.</cell></row><row><cell></cell><cell>huMAn trAit And rEquirEs trAining.</cell><cell></cell></row><row><cell>vAd</cell><cell>diFFErEnt PArts oF thE huMAn brAin ArE</cell><cell>sPEECh signAl ProPErtiEs And stAtistiCAl ModEls ArE</cell></row><row><cell></cell><cell>ACtivAtEd whEn sPEECh And nonsPEECh</cell><cell>usEd to dEtECt PrEsEnCE or AbsEnCE oF sPEECh.</cell></row><row><cell></cell><cell>stiMuli ArE PrEsEntEd.</cell><cell></cell></row><row><cell>Audio ProCEssing</cell><cell>thE huMAn brAin PErForMs both sPECtrAl</cell><cell>ACoustiC FEAturE PArAMEtErs dEPEnding on sPECtrAl</cell></row><row><cell></cell><cell>And tEMPorAl ProCEssing. it is not known</cell><cell>And tEMPorAl ProPErtiEs oF thE Audio signAl ArE</cell></row><row><cell></cell><cell>ExACtly how thE Audio signAl dEvEloPs thE</cell><cell>utilizEd For rECognition.</cell></row><row><cell></cell><cell>sPEAkEr-or PhonEME-dEPEndEnt AbstrACt</cell><cell></cell></row><row><cell></cell><cell>rEPrEsEntAtions/ModEls.</cell><cell></cell></row><row><cell>high-lEvEl FEAturEs</cell><cell>wE ConsidEr lExiCon, intonAtion, Prosody,</cell><cell>rECEnt AlgorithMs hAvE inCorPorAtEd Prosody,</cell></row><row><cell></cell><cell>AgE, gEndEr, diAlECt, sPEAking rAtE, And</cell><cell>PronunCiAtion, diAlECt, And othEr high-lEvEl</cell></row><row><cell></cell><cell>MAny othEr PArAlinguistiC AsPECts oF</cell><cell>FEAturEs For sPEAkEr idEntiFiCAtion.</cell></row><row><cell></cell><cell>sPEECh to rEMEMbEr A PErson's voiCE.</cell><cell></cell></row><row><cell>CoMPACt rEPrEsEntAtion</cell><cell>thE huMAn brAin ForMs sPEAkEr-dEPEndEnt,</cell><cell>high-lEvEl FEAturEs ArE ExtrACtEd thAt suMMArizE</cell></row><row><cell></cell><cell>EFFiCiEnt AbstrACt rEPrEsEntAtions. thEsE</cell><cell>thE voiCE ChArACtEristiCs oF A subJECt. thEsE ArE</cell></row><row><cell></cell><cell>invAriAnt to ChAngEs oF thE ACoustiC inPut,</cell><cell>ExtrACtEd in A wAy to MiniMizE sEssion vAriAbility</cell></row><row><cell></cell><cell>Providing robustnEss to noisE And</cell><cell>duE to noisE or distortion.</cell></row><row><cell></cell><cell>signAl distortion.</cell><cell></cell></row><row><cell>lAnguAgE dEPEndEnCE</cell><cell>sPEAkEr rECognition by huMAns is bEttEr</cell><cell>AutoMAtiC systEM's PErForMAnCE is dEgrAdEd iF</cell></row><row><cell></cell><cell>iF thEy know thE lAnguAgE bEing sPokEn.</cell><cell>thErE is A MisMAtCh in trAining And tEst lAnguAgE.</cell></row><row><cell>FAMiliAr vErsus</cell><cell>huMAns ArE ExtrEMEly good At</cell><cell>MAChinEs ProvidE ConsistEnt PErForMAnCE whEn</cell></row><row><cell>unFAMiliAr sPEAkErs</cell><cell>idEntiFying FAMiliAr voiCEs, but not</cell><cell>AdEquAtE AMount oF dAtA is ProvidEd. FAMiliArity</cell></row><row><cell></cell><cell>so For unFAMiliAr onEs.</cell><cell>CAn bE rElAtEd to thE AMount oF trAining dAtA.</cell></row><row><cell>idEntiFiCAtion vErsus</cell><cell>thE huMAn brAin ProCEssEs thEsE two</cell><cell>in Most CAsEs, thE sAME AlgorithM (with slight</cell></row><row><cell>disCriMinAtion</cell><cell>tAsks diFFErEntly.</cell><cell>ModiFiCAtion) CAn bE usEd to idEntiFy And</cell></row><row><cell></cell><cell></cell><cell>disCriMinAtE bEtwEEn sPEAkErs.</cell></row><row><cell>MEMory rEtEntion</cell><cell>huMAns' Ability to rEMEMbEr A PErson's</cell><cell>A CoMPutEr AlgorithM CAn storE thE ModEls oF</cell></row><row><cell></cell><cell>voiCE dEgrAdEs with tiME.</cell><cell>A PErson indEFinitEly iF ProvidEd suPPort.</cell></row><row><cell>FAtiguE</cell><cell>huMAn listEnErs CAnnot PErForM At</cell><cell>CoMPutErs do not hAvE issuEs with FAtiguE. long</cell></row><row><cell></cell><cell>thE sAME lEvEl For A long durAtion.</cell><cell>runtiMEs MAy CAusE ovErhEAting iF nECEssAry</cell></row><row><cell></cell><cell></cell><cell>PrECAutions ArE not tAkEn.</cell></row><row><cell>idEntiFy idiosynCrAsiEs</cell><cell>huMAns ArE vEry good At idEntiFying</cell><cell>thE MAChinE AlgorithMs hAvE to bE sPECiFiCAlly</cell></row><row><cell></cell><cell>ChArACtEristiC trAits oF A voiCE.</cell><cell>told whAt to look For And CoMPArE.</cell></row><row><cell>MisMAtChEd Conditions</cell><cell>huMAns rEly MorE on PArAlinguistiC AsPECts</cell><cell>AutoMAtiC systEMs ArE trAinEd on vArious</cell></row><row><cell></cell><cell>oF sPEECh in sEvErE MisMAtChEd Conditions.</cell><cell>ACoustiC Conditions, And usuAlly ArE MorE robust.</cell></row><row><cell>susCEPtibility to biAs</cell><cell>huMAn JudgMEnt CAn bE biAsEd by</cell><cell>AutoMAtiC AlgorithMs CAn bE biAsEd towArd thE</cell></row><row><cell></cell><cell>ContExtuAl inForMAtion.</cell><cell>trAining dAtA.</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Building Watson: An overview of the DeepQA project</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ferrucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gondek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Kalyanpur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Murdock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Mag</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="59" to="79" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">How innovative is Apple&apos;s new voice assistant, Siri?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Aron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Sci</title>
		<imprint>
			<biblScope unit="volume">212</biblScope>
			<biblScope unit="issue">2836</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2011-10-29">29 Oct. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tutorial on forensic speech science</title>
		<author>
			<persName><forename type="first">A</forename><surname>Eriksson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Speech Communication and Technology</title>
		<meeting>European Conf. Speech Communication and Technology<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="4" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Voice-selective areas in human auditory cortex</title>
		<author>
			<persName><forename type="first">P</forename><surname>Belin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Zatorre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lafaille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ahad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pike</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">403</biblScope>
			<biblScope unit="page" from="309" to="312" />
			<date type="published" when="2000-01">Jan. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">&apos;Who&apos; is saying &apos;what&apos;? Brainbased decoding of human voice and speech</title>
		<author>
			<persName><forename type="first">E</forename><surname>Formisano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">De</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bonte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">322</biblScope>
			<biblScope unit="page" from="970" to="973" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Speaker verification using adapted Gaussian mixture models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Quatieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Dunn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Process</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="19" to="41" />
			<date type="published" when="2000-01">Jan. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><surname>Online</surname></persName>
		</author>
		<ptr target="www.biometrics.gov" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Style and Sociolinguistic Variation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Eckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Rickford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Cambridge Univ. Press</publisher>
			<pubPlace>Cambridge, U.K.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Analysis and compensation of speech under stress and noise for environmental robustness in speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H L</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="173" />
			<date type="published" when="1996-11">Nov. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Speaker identification within whispered speech audio streams</title>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H L</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1408" to="1421" />
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Whisper-island detection based on unsupervised segmentation with entropy-based speech feature processing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H L</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="883" to="894" />
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Analysis and compensation of Lombard speech across noise type and levels with application to in-set/out-of-set speaker recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H L</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Varadarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="366" to="378" />
			<date type="published" when="2009-02">Feb. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Singing speaker clustering based on subspace learning in the GMM mean supervector space</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mehrabani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H L</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="653" to="666" />
			<date type="published" when="2013-06">June 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The impact of speech under &apos;stress&apos; on military speech technology</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Swail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>South</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Steeneken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Cupples</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Vloeberghs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NATO Project Rep</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The effects of telephone transmission degradations on speaker recognition performance</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Zissman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Quatieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>O'leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Carlson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, and Signal essing</meeting>
		<imprint>
			<biblScope unit="page" from="329" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Joint factor analysis versus eigenchannels in speaker recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ouellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1435" to="1447" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Score normalization for text-independent speaker verification systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Auckenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lloyd-Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Process</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="42" to="54" />
			<date type="published" when="2000-01">Jan. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Integrated models of signal and background with application to speaker identification in noise</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Hofstetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Process</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="245" to="257" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Far-field speaker recognition</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2023" to="2032" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Human assisted speaker recognition in NIST SRE10</title>
		<author>
			<persName><forename type="first">C</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Brandschain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Godfrey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Odyssey: The Speaker and Language Recognition Workshop</title>
		<meeting>Odyssey: The Speaker and Language Recognition Workshop<address><addrLine>Brno</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="180" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The multi-session audio research project (MARP) corpus: Goals, design and initial findings</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Lawson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stauffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Cupples</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wenndt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Grieco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech<address><addrLine>Brighton, U.K.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1811" to="1814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Session variability contrasts in the MARP corpus</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Godin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="298" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Effects of physiological aging on selected acoustic characteristics of voice</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Ramig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Ringel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Speech Lang. Hearing Res</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="22" to="30" />
			<date type="published" when="1983-03">Mar. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Speaker verification with long-term ageing data</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Drygajlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Harte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf</title>
		<meeting>Int. Conf</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="478" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Speaker verification in score-ageingquality classification space</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Drygajlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Harte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Speech Lang</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1068" to="1084" />
			<date type="published" when="2013-08">Aug. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Wikipedia</forename><forename type="middle">George</forename><surname>Contributors</surname></persName>
		</author>
		<author>
			<persName><surname>Zimmerman</surname></persName>
		</author>
		<ptr target="http://en.wikipedia.org/wiki/George_Zimmerman" />
		<title level="m">Entry on Wikipedia</title>
		<imprint>
			<date type="published" when="2014-11-29">2014, Nov. 29</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H L</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shokouhi</surname></persName>
		</author>
		<title level="m">Speaker identification: Screaming, stress and non-neutral speech</title>
		<imprint>
			<date type="published" when="2013-11-29">2013. Nov. 29</date>
		</imprint>
	</monogr>
	<note>is there speaker content?. [Online</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName><surname>Ieee Sltc Newsletter</surname></persName>
		</author>
		<ptr target="http://www.signalprocessingsociety.org/tech-nical-committees/list/sl-tc/spl-nl/2013-11/SpeakerIdentification/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Identical twins, different voices</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Speech Lang. Law</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="49" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Voice similarity in identical twins</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Van Gysel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vercammen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Debruyne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Otorhinolaryngol. Belg</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="55" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Genetics of vocal quality characteristics in monozygotic twins: a multiparameter approach</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Van Lierde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Vinck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">De</forename><surname>Ley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Clement</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Van Cauwenberge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Voice</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="511" to="518" />
			<date type="published" when="2005-12">Dec. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A forensic phonetic investigation into the speech patterns of identical and non-identical twins</title>
		<author>
			<persName><forename type="first">D</forename><surname>Loakes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Speech Lang. Law</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="100" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Twins</forename><surname>Day</surname></persName>
		</author>
		<ptr target="http://www.twins-days.org" />
		<imprint>
			<date type="published" when="2015">2015</date>
			<pubPlace>Twinsburg, Ohio</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The Phonetic Bases of Speaker Recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nolan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>Cambridge Univ. Press</publisher>
			<pubPlace>Cambridge, U.K.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficient acoustic parameters for speaker recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">68</biblScope>
			<biblScope unit="page">2044</biblScope>
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Forensic Speaker Identification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rose</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>CRC Press</publisher>
			<pubPlace>Boca Raton, FL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The technical comparison of forensic voice samples</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Expert Evidence, 1 ed</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Thompson Lawbook Co</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="1051" to="6102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">The limitations of auditory-phonetic speaker identification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nolan</surname></persName>
		</author>
		<editor>Texte Zur Theorie Und Praxis Forensischer Linguistik, H. Kniffka</editor>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>De Gruyter</publisher>
			<biblScope unit="page" from="457" to="479" />
			<pubPlace>Berlin, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Reverse-engineering the human auditory pathway</title>
		<author>
			<persName><forename type="first">L</forename><surname>Watts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Computational Intelligence</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="47" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Modeling prosodic feature sequences for speaker recognition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kajarekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="455" to="472" />
			<date type="published" when="2005-07">July 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Modeling prosodic dynamics for speaker recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Adami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mihaescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Godfrey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing (ICASSP&apos;03)</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, and Signal essing (ICASSP&apos;03)</meeting>
		<imprint>
			<biblScope unit="page" from="788" to="791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Modeling prosodic features with joint factor analysis for speaker verification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2095" to="2103" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A new mode of identifying criminals</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Wigmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">17 Amer Inst. Crim. L. Criminology</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="165" to="166" />
			<date type="published" when="1926-08">Aug. 1926</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Voiceprint identification</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Kersta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Police L. Q</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1973" to="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Spectrographic voice identification: A forensic survey</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Koenig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2088" to="2090" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Forensic Voice Identification</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Hollien</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Academic Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Yount</surname></persName>
		</author>
		<title level="m">Forensic Science: From Fibers to Fingerprints</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Chelsea House</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Forensic speaker recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bonastre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Matrouf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="95" to="103" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Distinguishing between science and pseudoscience in forensic acoustics</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Morrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Meetings on Acoustics</title>
		<meeting>Meetings on Acoustics</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">60001</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">On the problem of the most efficient tests of statistical hypotheses</title>
		<author>
			<persName><forename type="first">J</forename><surname>Neyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philos. Trans. R. Soc</title>
		<imprint>
			<biblScope unit="volume">231</biblScope>
			<biblScope unit="page" from="289" to="337" />
			<date type="published" when="1933-01">Jan. 1933</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Forensic voice comparison and the paradigm shift</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Morrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Justice</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="298" to="308" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Forensic voice comparison</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Morrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Expert Evidence 99</title>
		<meeting><address><addrLine>1 ed. London</addrLine></address></meeting>
		<imprint>
			<publisher>Thompson Reuters</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page">1051</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">On the Theory and Practice of Voice Identification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Bolt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979">1979</date>
			<publisher>National Academy of Sciences</publisher>
			<pubPlace>Washington, DC</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Effects of experience on fetal voice recognition</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Kisilevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Hains</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. Sci</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="220" to="224" />
			<date type="published" when="2003-05">May 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Human voice perception</title>
		<author>
			<persName><forename type="first">M</forename><surname>Latinus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Belin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Biol</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="143" to="R145" />
			<date type="published" when="2011-02">Feb. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Understanding voice perception</title>
		<author>
			<persName><forename type="first">P</forename><surname>Belin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Bestelmeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Latinus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Br. J. Psychol</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="711" to="725" />
			<date type="published" when="2011-11">Nov. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Cacioppo</surname></persName>
		</author>
		<title level="m">Foundations in Social Neuroscience</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Voice discrimination and recognition are separate abilities</title>
		<author>
			<persName><forename type="first">D</forename><surname>Van Lancker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kreiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuropsychologia</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="829" to="834" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Familiar voice recognition: Patterns and parameters. Part I: Recognition of backward voices</title>
		<author>
			<persName><forename type="first">D</forename><surname>Van Lancker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Emmorey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Phonetics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="38" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Human voice recognition depends on language ability</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Perrachione</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Del Tufo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Gabrieli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">333</biblScope>
			<biblScope unit="issue">6042</biblScope>
			<biblScope unit="page" from="595" to="595" />
			<date type="published" when="2011-07">July 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Spectral and temporal processing in human auditory cortex</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Zatorre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Belin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cereb. Cortex</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="946" to="953" />
			<date type="published" when="2001-10">Oct. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Perceptual identification of voices under normal, stress, and disguised speaking conditions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hollien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Majewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hollien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">S53</biblScope>
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">The forensic confirmation bias: Problems, perspectives, and proposed solutions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kassin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">E</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kukucka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Res. Memory Cognit</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="42" to="52" />
			<date type="published" when="2013-03">Mar. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Long-term memory for unfamiliar voices</title>
		<author>
			<persName><forename type="first">G</forename><surname>Papcun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">913</biblScope>
			<date type="published" when="1989-02">Feb. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Earwitness testimony: Never mind the variety, hear the length</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wilding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Cognit. Psychol</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="95" to="111" />
			<date type="published" when="1997-04">Apr. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Automatic speaker verification: A review</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Rosenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1976">1976</date>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="475" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Automatic recognition of speakers from their voices</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1976">1976</date>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="460" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Speaker recognition-Identifying people by their voices</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Doddington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="1651" to="1664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Speaker verification: A tutorial</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Mag</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="42" to="48" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Speaker-dependent-feature extraction, recognition and processing techniques</title>
		<author>
			<persName><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="505" to="520" />
			<date type="published" when="1991-12">Dec. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Text-independent speaker identification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="32" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Robust speaker recognition: A feature-based approach</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Mammone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Ramachandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">58</biblScope>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Recent advances in speaker recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recog. Lett</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="859" to="872" />
			<date type="published" when="1997-09">Sept. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Speaker recognition: A tutorial</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Campbell</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">A tutorial on text-independent speaker verification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bimbot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bonastre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fredouille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Magrin-Chagnolleau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Meignier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Merlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ortega-García</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP J. Appl. Signal Process</title>
		<imprint>
			<biblScope unit="page" from="430" to="451" />
			<date type="published" when="2004-04">2004. Apr. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">The NIST speaker recognition evaluations: 1996-2001</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Przybocki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Odyssey: The Speaker and Language Recognition Workshop</title>
		<meeting>Odyssey: The Speaker and Language Recognition Workshop<address><addrLine>Crete, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">NIST speaker recognition evaluations 1996-2008</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE Defense, Security, and Sensing</title>
		<meeting>SPIE Defense, Security, and Sensing</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="732411" to="732411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">The NIST 2010 speaker recognition evaluation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Greenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2726" to="2729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">The NIST speaker recognition evaluation-overview, methodology, systems, results, perspective</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Przybocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="225" to="254" />
			<date type="published" when="2000-06">June 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Front-end factor analysis for speaker verification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ouellet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="788" to="798" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Long-term feature averaging for speaker recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Markel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Oshika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gray</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="330" to="337" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">An approach to text-independent speaker recognition with short utterances</title>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wrench</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, and Signal essing</meeting>
		<imprint>
			<date type="published" when="1983">1983</date>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="555" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">The SuperSID project: Exploiting high-level information for high-accuracy speaker recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Navratil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klusacek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing (ICASSP&apos;03)</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, and Signal essing (ICASSP&apos;03)</meeting>
		<imprint>
			<biblScope unit="page" from="784" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">A statistical model-based voice activity detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">The role of voice activity detection in forensic speaker verification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Beritelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Spadaccini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Digital Signal Processing</title>
		<meeting>Digital Signal essing</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Unsupervised speech activity detection using voicing measures and perceptual spectral flux</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">O</forename><surname>Sadjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H L</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="197" to="200" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences</title>
		<author>
			<persName><forename type="first">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mermelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust. Speech Signal Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="357" to="366" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Perceptual linear predictive (PLP) analysis of speech</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1738</biblScope>
			<date type="published" when="1990-04">Apr. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">From frequency to quefrency: A history of the cepstrum</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Oppenheim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Schafer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="95" to="106" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Cepstral analysis technique for automatic speaker verification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust. Speech Signal Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="254" to="272" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Feature warping for robust speaker verification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pelecanos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Odyssey: The Speaker and Language Recognition Workshop</title>
		<meeting>Odyssey: The Speaker and Language Recognition Workshop<address><addrLine>Crete, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">RASTA processing of speech</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="578" to="589" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Unsupervised equalization of Lombard effect for speech recognition in noisy adverse environments</title>
		<author>
			<persName><forename type="first">H</forename><surname>Boril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H L</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1379" to="1393" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Full-covariance UBM and heavy-tailed PLDA in i-vector speaker verification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Matejka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Castaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Plchot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cernocky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing (ICASSP&apos;11)</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, and Signal essing (ICASSP&apos;11)</meeting>
		<imprint>
			<biblScope unit="page" from="4828" to="4831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Bayesian speaker verification with heavy tailed priors</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Odyssey: The Speaker and Language Recognition Workshop</title>
		<meeting>Odyssey: The Speaker and Language Recognition Workshop<address><addrLine>Brno, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Acoustic modeling using deep belief networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech Lang. Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="22" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Robust text-independent speaker identification using Gaussian mixture speaker models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Processing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="72" to="83" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">A vector quantization approach to speaker recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Soong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Juang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, and Signal essing</meeting>
		<imprint>
			<biblScope unit="page" from="387" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Text-dependent speaker verification using vector quantization source coding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Burton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust. Speech Signal Processing</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="143" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Royal Stat. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Speaker background models for connected digit password speaker verification</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing (ICASSP&apos;96)</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, and Signal essing (ICASSP&apos;96)</meeting>
		<imprint>
			<biblScope unit="page" from="81" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">The use of cohort normalized scores for speaker verification</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Delong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">K</forename><surname>Soong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Spoken Language Processing</title>
		<meeting>Int. Conf. Spoken Language essing</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="599" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Comparison of background normalization methods for textindependent speaker verification</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eurospeech</title>
		<meeting>Eurospeech</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="963" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Maximum a posteriori estimation for multivariate Gaussian mixture observations of Markov chains</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gauvain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="291" to="298" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">eigenvoices for speaker adaptation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Junqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Goldwasser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Niedzielski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fincke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Field</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Contolini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Spoken Language Processing</title>
		<meeting>Int. Conf. Spoken Language essing</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="1774" to="1777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">New MAP estimators for speaker recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mihoubi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="2964" to="2967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995-09">Sept. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Support vector machines using GMM supervectors for speaker verification</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Sturim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="308" to="311" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Advances in channel compensation for SVM speaker recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Solomonoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Boardman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing (ICASSP&apos;05)</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, and Signal essing (ICASSP&apos;05)</meeting>
		<imprint>
			<biblScope unit="page" from="629" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Within-class covariance normalization for SVM-based speaker recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O</forename><surname>Hatch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Kajarekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech<address><addrLine>Pittsburgh, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1471" to="1474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Generalized linear discriminant sequence kernels for speaker recognition</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, and Signal essing</meeting>
		<imprint>
			<biblScope unit="page" from="161" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Nasrabadi</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Disentangling speaker and channel effects in speaker verification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing (ICASSP&apos;04)</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, and Signal essing (ICASSP&apos;04)</meeting>
		<imprint>
			<biblScope unit="page" from="37" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">A study of interspeaker variability in speaker verification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ouellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="980" to="988" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Rapid speaker adaptation in eigenvoice space</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Junqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Niedzielski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Processing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="695" to="707" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Mixtures of probabilistic principal component analyzers</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Tipping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="443" to="482" />
			<date type="published" when="1999-02">Feb. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Eigenvoice modeling with sparse training data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="345" to="354" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">EM algorithms for PCA and SPCA</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="626" to="632" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Joint factor analysis of speaker and session variability: Theory and algorithms</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
		<idno>CRIM-06/08-13</idno>
	</analytic>
	<monogr>
		<title level="j">CRIM</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<pubPlace>Montreal, Quebec, Canada</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Support vector machines and joint factor analysis for speaker verification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hubeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Castaldo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing (ICASSP&apos;09)</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, and Signal essing (ICASSP&apos;09)</meeting>
		<imprint>
			<biblScope unit="page" from="4237" to="4240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Support vector machines versus fast scoring in the low-dimensional total variability space for speaker verification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Brümmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ouellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1559" to="1562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Probabilistic linear discriminant analysis for inferences about identity</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Prince</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision</title>
		<meeting>IEEE Int. Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Analysis of i-vector length normalization in speaker recognition systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Espy-Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="249" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">GUESS: A language and interface for graph exploration</title>
		<author>
			<persName><forename type="first">E</forename><surname>Adar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM&apos;s Special Interest Group on Computer-Human Interaction</title>
		<meeting>ACM&apos;s Special Interest Group on Computer-Human Interaction</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="791" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Measuring, refining and calibrating speaker and language information extracted from speech</title>
		<author>
			<persName><forename type="first">N</forename><surname>Brummer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ph.D. diss. Stellenbosch, Univ. Stellenbosch</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Detecting converted speech and natural speech for anti-spoofing attack in speaker recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Siong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech, Portland</title>
		<meeting>Interspeech, Portland</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1700" to="1703" />
		</imprint>
		<respStmt>
			<orgName>OR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Spoofing and countermeasures for speaker verification: a survey</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Alegre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="130" to="153" />
			<date type="published" when="2015-02">Feb. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">An investigation into back-end advancements for speaker recognition in multi-session and noisy enrollment scenarios</title>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H L</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ ACM Trans. Audio, Speech Lang. Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1978" to="1992" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">RBM-PLDA subsystem for the NIST i-Vector Challenge</title>
		<author>
			<persName><forename type="first">S</forename><surname>Novoselov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pekhovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonchik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shulipa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">Sept. 14-18, 2014</date>
			<biblScope unit="page" from="378" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Study of the effect of I-vector modeling on short and mismatch utterance duration for speaker verification</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Matrouf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bonastre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech<address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2662" to="2665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Duration mismatch compensation for i-vector based speaker recognition systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Saeidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Van Leeuwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, and Signal essing</meeting>
		<imprint>
			<biblScope unit="page" from="7663" to="7667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Modified-prior i-vector estimation for language identification of short duration utterances</title>
		<author>
			<persName><forename type="first">R</forename><surname>Travadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Segbroeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3037" to="3041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Tutorial on logistic-regression calibration and fusion: Converting a score to a likelihood ratio</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Morrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Aust. J. Foren. Sci</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="173" to="197" />
			<date type="published" when="2013-12">Dec. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Automatic regularization of cross-entropy cost for speaker recognition fusion</title>
		<author>
			<persName><forename type="first">V</forename><surname>Hautamäki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Saeidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Larcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">O</forename><surname>Sadjadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1609" to="1613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">i-Vector modeling with deep belief networks for multi-session speaker recognition</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ghahabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hernando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Odyssey:The Speaker and Language Recognition Workshop</title>
		<meeting>Odyssey:The Speaker and Language Recognition Workshop<address><addrLine>Joensuu, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">June 16-19, 2014</date>
			<biblScope unit="page" from="305" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">A novel scheme for speaker recognition using a phonetically-aware deep neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Scheffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mclaren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, and Signal essing</meeting>
		<imprint>
			<biblScope unit="page" from="1695" to="1699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Deep neural networks for extracting Baum-Welch statistics for speaker recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ouellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Alam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Odyssey: The Speaker and Language Recognition Workshop</title>
		<meeting><address><addrLine>Joensuu, Finland</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Maximum likelihood acoustic factor analysis models for robust speaker verification in noise</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H L</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech Lang. Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="381" to="391" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">The NIST 2014 speaker recognition i-vector machine learning challenge</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bansé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccree</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Odyssey: The Speaker and Language Recognition Workshop</title>
		<meeting>Odyssey: The Speaker and Language Recognition Workshop</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="224" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Investigating stateof-the-art speaker verification in the case of unlabeled development data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shokouhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Odyssey: The Speaker and Language Recognition Workshop</title>
		<meeting>Odyssey: The Speaker and Language Recognition Workshop<address><addrLine>Joensuu, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="118" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">CRSS systems for 2012 NIST speaker recognition evaluation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">O</forename><surname>Sadjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shokouhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Boril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, and Signal essing</meeting>
		<imprint>
			<biblScope unit="page" from="6783" to="6787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">PLDA for speaker verification with utterances of arbitrary duration</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ouellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, and Signal essing</meeting>
		<imprint>
			<biblScope unit="page" from="7649" to="7653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Content matching for short duration speaker recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Scheffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech, Joensuu, Finland</title>
		<meeting>Interspeech, Joensuu, Finland</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1317" to="1321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech Lang. Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="42" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Improvement of distant-talking speaker identification using bottleneck features of DNN</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech<address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3661" to="3664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Human vs. machine speaker identification with telephone speech</title>
		<author>
			<persName><forename type="first">A</forename><surname>Schmidt-Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICSLP</title>
		<meeting>ICSLP</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Shaughnessy</surname></persName>
		</author>
		<title level="m">Speech Communication: Human and Machine</title>
		<meeting><address><addrLine>India</addrLine></address></meeting>
		<imprint>
			<publisher>Universities Press</publisher>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Humans versus machine: Forensic voice comparison on a small database of swedish voice recordings</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lindh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Morrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Congress of Phonetic Sciences (ICPhS)</title>
		<meeting>Int. Congress of Phonetic Sciences (ICPhS)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Speaker verification by human listeners: Experiments comparing human and machine performance using the NIST 1998 speaker evaluation data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Schmidt-Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Process</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="249" to="266" />
			<date type="published" when="2000-01">Jan. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Approaching human listener accuracy with modern speaker verification</title>
		<author>
			<persName><forename type="first">V</forename><surname>Hautamäki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nosratighods</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech 2010</title>
		<meeting>Interspeech 2010<address><addrLine>Makuhari, Chiba, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">Sept. 26-30, 2010</date>
			<biblScope unit="page" from="1473" to="1476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">USSS-MITLL 2010 human assisted speaker recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Sturim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Granville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing (ICASSP&apos;11)</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, and Signal essing (ICASSP&apos;11)</meeting>
		<imprint>
			<biblScope unit="page" from="5904" to="5907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Calibration and weight of the evidence by human listeners. The ATVS-UAM submission to NIST human-aided speaker recognition 2010</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Franco-Pedroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzalez-Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, and Signal essing</meeting>
		<imprint>
			<biblScope unit="page" from="5908" to="5911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Speaker verification by inexperienced and experienced listeners vs. speaker verification system</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Audibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rossato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bonastre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, and Signal essing</meeting>
		<imprint>
			<biblScope unit="page" from="5912" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Including human expertise in speaker recognition systems: Report on a pilot evaluation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Godfrey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, and Signal essing</meeting>
		<imprint>
			<biblScope unit="page" from="5896" to="5899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Assessing the speaker recognition performance of naïve listeners using mechanical turk</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, and Signal essing</meeting>
		<imprint>
			<biblScope unit="page" from="5916" to="5919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Merging human and automatic system decisions to improve speaker recognition performance</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Hautamäki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hautamäki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2519" to="2523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Comparison of human listeners and speaker verification systems using voice mimicry data</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Hautamäki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hautamäki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Laukkanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Odyssey: The Speaker and Language Recognition Workshop</title>
		<meeting>Odyssey: The Speaker and Language Recognition Workshop<address><addrLine>Joensuu, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Forensically inspired approaches to automatic speaker recognition</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Omar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pelecanos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pendus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, and Signal essing</meeting>
		<imprint>
			<biblScope unit="page" from="5160" to="5163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<monogr>
		<author>
			<persName><surname>Nist Osac-</surname></persName>
		</author>
		<ptr target="http://www.nist.gov/forensics/osac.cfm" />
		<title level="m">The Organization of Scientific Area Committees</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b161">
	<monogr>
		<author>
			<persName><surname>Nist Osac</surname></persName>
		</author>
		<ptr target="http://www.nist.gov/forensics/publications.cfm" />
		<title level="m">NIST forensic science publications</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
