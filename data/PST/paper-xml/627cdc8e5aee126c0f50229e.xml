<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CachePerf: A Unified Cache Miss Classifier via Hybrid Hardware Sampling</title>
				<funder ref="#_HtQAs5Y">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-19">19 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jin</forename><surname>Zhou</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Tongping</forename><surname>Liu</surname></persName>
							<email>tongping@umass.edu</email>
						</author>
						<author>
							<persName><forename type="first">Jiaxun</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hanmei</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts Amherst</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">STEVEN (JIAXUN) TANG</orgName>
								<orgName type="institution">University of Massachusetts Amherst</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">HANMEI YANG</orgName>
								<orgName type="institution" key="instit2">University of Massachusetts Amherst</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Massachusetts Amherst</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CachePerf: A Unified Cache Miss Classifier via Hybrid Hardware Sampling</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-19">19 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3489048.3526954</idno>
					<idno type="arXiv">arXiv:2203.08943v2[cs.PF]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Cache Performance</term>
					<term>Cache Miss</term>
					<term>Conflict Miss</term>
					<term>Coherency Miss</term>
					<term>Capacity Miss</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The cache plays a key role in determining the performance of applications, no matter for sequential or concurrent programs on homogeneous and heterogeneous architecture. Fixing cache misses requires to understand the origin and the type of cache misses. However, this remains to be an unresolved issue even after decades of research. This paper proposes a unified profiling tool-CachePerf-that could correctly identify different types of cache misses, differentiate allocator-induced issues from those of applications, and exclude minor issues without much performance impact. The core idea behind CachePerf is a hybrid sampling scheme: it employs the PMU-based coarse-grained sampling to select very few susceptible instructions (with frequent cache misses) and then employs the breakpoint-based fine-grained sampling to collect the memory access pattern of these instructions. Based on our evaluation, CachePerf only imposes 14% performance overhead and 19% memory overhead (for applications with large footprints), while identifying the types of cache misses correctly. CachePerf detected 9 previous-unknown bugs. Fixing the reported bugs achieves from 3% to 3788% performance speedup. CachePerf will be an indispensable complementary to existing profilers due to its effectiveness and low overhead. CCS Concepts: ? Software and its engineering ? Multiprocessing / multiprogramming / multitasking; ? Computer systems organization ? Real-time operating systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Cache accesses are typically orders of magnitude faster (e.g., 200? <ref type="bibr" target="#b28">[30]</ref>) than memory accesses. Therefore, it is critical to reduce cache misses in order to boost the performance of applications, no matter for single-threaded or multi-threaded applications running on homogeneous or heterogeneous hardware architectures. However, it is challenging to identify cache misses statically, as they are related to access pattern <ref type="bibr" target="#b30">[32]</ref>, hardware feature (e.g., cache capacity, cache line size), or even starting addresses of objects <ref type="bibr" target="#b32">[34]</ref>.</p><p>Many tools aiming to identify cache misses have been developed in the past. Simulation-based approaches, such as different Pin tools <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b36">38]</ref>, or cachegrind (one tool inside Valgrind <ref type="bibr" target="#b38">[40]</ref>) <ref type="bibr" target="#b47">[49]</ref>, typically impose prohibitive performance overhead (e.g., 100 times) that makes them even unsuitable for development phases <ref type="bibr" target="#b47">[49,</ref><ref type="bibr" target="#b50">52]</ref>. To solve such issues, many sampling-based tools, such as perf <ref type="bibr" target="#b9">[10]</ref>, oprofile <ref type="bibr" target="#b29">[31]</ref>, are proposed to reduce the profiling overhead. They could attribute the percentage of cache misses to the specific lines of the source code based on the sampling. However, they cannot report both the type and origin of cache misses, making their reports not sufficient to guide the bug fixes.</p><p>Different types of cache misses, including compulsory misses, capacity misses, conflict misses, and coherency misses <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21]</ref>, require different fixing methods, as detailed in Section 2. A compulsory miss occurs when the related cache line is accessed for the first time, which is considered to be mandatory and unavoidable. Instead, a capacity miss may occur if the working set of a program exceeds the capacity of the cache. Capacity misses can be reduced by loop optimizations <ref type="bibr" target="#b53">[55]</ref> or array regrouping <ref type="bibr" target="#b33">[35]</ref>. In contrast, conflict misses can be introduced when more than N cache lines are mapping to the same set in N-way associative cache, and coherency misses may occur when multiple threads are accessing the same cache line simultaneously. Although some conflict and coherency misses can be reduced with padding, they require different padding strategies: fixing conflict misses should prevent the mapping to the same set, while coherency misses can be reduced by avoiding multiple threads accessing the same cache lines. Reducing cache misses also requires to know the origin: whether a problem is caused by the allocator or the application? For application bugs, which objects or which instructions are involved? Without knowing such information, it is impossible to reduce cache misses effectively.</p><p>Some tools aim to identify a specific type of cache misses, such as capacity misses <ref type="bibr" target="#b33">[35]</ref>, coherence misses <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b37">39]</ref>, and conflict misses <ref type="bibr" target="#b44">[46]</ref>. However, it is inconvenient to identify all types of cache misses using these tools, as they are designed as exclusive to each other. Further, none of them could correctly identify cache misses caused by the memory allocator. For instance, cache-thrash can be slowed down by 38? when using TCMalloc (as shown in Table <ref type="table">2</ref>). Without knowing the origin of cache misses, programmers may waste their efforts in improving the application but achieve only minor or no improvement. DProf <ref type="bibr" target="#b41">[43]</ref> is the only tool that can identify all types of cache misses for data structures of Linux kernel, which unfortunately requires significant manual effort, as further discussed in Section 6.</p><p>This paper proposes a novel tool-CachePerf-that overcomes these shortcomings: <ref type="bibr" target="#b0">(1)</ref> CachePerf is a unified profiler that could identify all fixable cache misses (except compulsory misses); (2) CachePerf only reports serious issues, saving manual effort spending on trivial issues; (3) CachePerf reports both type and origin of cache misses, providing useful information for bug fixes; (4) CachePerf only imposes reasonable overhead for its identification. Designing such a tool includes the following challenges.</p><p>The first challenge is to choose an appropriate profiling method that can classify all types of cache misses with reasonable overhead. Prior work employs different sampling events, including address sampling for capacity misses <ref type="bibr" target="#b33">[35]</ref> and coherency misses <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b31">33]</ref>, HITM events <ref type="bibr" target="#b37">[39]</ref> for coherency misses, and L1 cache misses for conflict misses <ref type="bibr" target="#b44">[46]</ref>. However, it is infeasible to combine these events together, as that will introduce prohibitive overhead and complexity. Although it is intuitive to employ the hardware-based sampling, the Performance Monitoring Units (PMU) supports up to hundreds of events (e.g., 207 events at Intel Xeon Silver 4114 <ref type="bibr" target="#b14">[15]</ref>). CachePerf's selection is driven by the requirement of differentiating the type, reporting the origin, and measuring the seriousness of cache misses, as discussed later. In summary, such an event should capture the detailed information of memory accesses, such as the memory address, the related instruction, and the hit information (indicating a cache miss or not), which is often omitted by existing work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b31">33]</ref>. Therefore, "the PMU-based precise address sampling" is chosen as the right event, and we elaborate why and how CachePerf exploits this event as follows.</p><p>The second challenge is to differentiate all types of cache misses correctly. The PMU-based sampling helps filter out cache misses, but it is impossible to correctly identify the type of each cache miss under the sampling, due to the lack of the history of cache usage and memory accesses. Instead, CachePerf proposes to identify coherency misses based on the cumulative behavior of many misses: only very few cache lines (not mapping to the same set) with extensive misses are most likely caused by coherency misses. Unfortunately, this rule cannot differentiate capacity misses from conflict misses, where the detailed access pattern is required for the differentiation, as further discussed in Section 2. Further, CachePerf classifies other types of cache misses based on a key observation: serious cache misses are typically caused by very few instructions whose access patterns are not altered during the whole execution. Based on this key observation, we propose a novel approach-hybrid hardware sampling-to classify the type of cache misses: the PMU-based coarse-grained sampling detects susceptible instructions with frequent cache misses, then the breakpoint-based fine-grained sampling is employed to identify memory access patterns of these selected instructions. This approach combines the best of both worlds, as the coarse-grained sampling could reduce the profiling overhead, while the fine-grained sampling collects a short history of memory accesses that is necessary to classify the access pattern. For instance, it is easy to determine conflict misses if multiple continuous accesses are accessing the same cache set.</p><p>The third challenge is to differentiate cache misses caused by the memory allocator from those ones caused by applications. Although allocator-induced cache misses may have a high impact on the performance, they get less attention than they deserve. This paper makes the following observations:</p><p>(1) the allocator may introduce both conflict and coherence misses (mainly false sharing, a type of coherency misses that multiple threads are accessing different words of the same cache line <ref type="bibr" target="#b30">[32]</ref>);</p><p>(2) Allocator-induced cache misses share the same attribute that multiple heap objects are involved unnecessarily, although this is not the sufficient condition. For instance, allocator-induced false sharing should have more than two objects on the same cache line. Further, these objects, accessed by different threads, must be allocated by different threads. Similarly, an allocator may introduce conflict misses, when multiple objects are mapped to the same set of cache lines. To the best of our knowledge, CachePerf is the first work that reports allocator-induced cache misses.</p><p>CachePerf further designs practical mechanisms that help reduce the detection overhead and avoid reporting minor issues: (1) CachePerf tracks a specified number of the most recent memory accesses (or a window), and then only checks cache misses inside if the miss ratio (i.e., the number of misses divided by the number of accesses) in the current buffer is larger than a threshold. This windowing mechanism also helps filter out sporadic cache misses, e.g., compulsory misses; (2) CachePerf further proposes a "ratio-based filtering" that only reports an issue if the ratio of memory accesses or cache misses is larger than a threshold;</p><p>We evaluated CachePerf on a range of well-studied benchmarks and real applications, where some have known cache misses. Based on our evaluation, CachePerf only introduces 14% performance overhead and 19% memory overhead (for large applications), while detecting all known bugs and night previously-unknown cache misses. Guided by CachePerf's report, we are able to fix most detected cache misses, achieving the performance speedup up to 38?. Overall, the paper makes the following contributions:</p><p>? It proposes a novel hybrid sampling scheme that combines coarse-grained PMU-based sampling and fine-grained breakpoint-based sampling, with a better trade-off between performance and accuracy. ? It is the first tool that can classify different types of cache misses without manual involvement.</p><p>? It proposes practical mechanisms to differentiate cache misses caused by the allocator from those from applications, and to prune insignificant issues.</p><p>? It provides the detailed implementation of a profiler with low overhead (14% on average) and high effectiveness, confirmed by our extensive evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND OVERVIEW</head><p>This section first introduces some basic background of cache misses, and then discusses the basic idea of CachePerf.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Types of Cache Misses</head><p>Cache miss can be classified into compulsory miss, capacity miss, conflict miss, and coherence miss <ref type="bibr" target="#b41">[43]</ref>. Among them, a compulsory miss occurs when the cache line is accessed for the first time, which is mandatory and unavoidable <ref type="bibr" target="#b20">[21]</ref>. In the remainder of this paper, we mainly focus on the other three types of cache misses. In the following, we will discuss their definitions, fix strategies, and possible causes.</p><p>2.1.1 Capacity Miss. Capacity misses occur when the accessed data of a program exceeds the capacity of the cache <ref type="bibr" target="#b51">[53]</ref>. When the cache cannot hold all the active data, some recently-accessed cache lines are forced to be evicted, which leads to cache misses if they are accessed again. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>(a), both for loops will suffer from cache capacity misses, as both Alpha and Beta's size is four times of the cache size (with the "CACHE_SIZE" number of integers). Capacity misses are mainly caused by applications. Not all capacity misses can be completely eliminated. However, some can be significantly reduced via array regrouping <ref type="bibr" target="#b33">[35]</ref> or loop optimizations <ref type="bibr" target="#b53">[55]</ref> (e.g., loop tiling <ref type="bibr" target="#b1">[2]</ref>). For the example shown in Fig. <ref type="figure" target="#fig_0">1</ref>(a), we could combine two loops into one loop as Fig. <ref type="figure" target="#fig_0">1</ref>(b) to reduce cache misses, also known as loop fusion <ref type="bibr" target="#b10">[11]</ref>. </p><formula xml:id="formula_0">for (int i = 0; i &lt; CACHE_SIZE; ++i) { Alpha[i] = i; } for (int i = 0; i &lt; CACHE_SIZE; ++i) { Beta[i] = Alpha[i]*2; } for (int i = 0; i &lt; CACHE_SIZE; ++i) { Alpha[i] = i; Beta[i] = Alpha[i]*2; } (a)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># of Cache Misses</head><p>Cache Set Index Fig. <ref type="figure">2</ref>. A real example of conflict misses from the Kripke application <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b44">46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Conflict</head><p>Miss. Conflict misses are introduced in direct-mapped or set-associative cache <ref type="bibr" target="#b41">[43,</ref><ref type="bibr" target="#b44">46]</ref>. For an N-way associative cache, conflict misses will occur when more than N cache lines mapping to the same set are accessed recently. Fig. <ref type="figure">2</ref>(a) shows a real example of conflict misses: Kripke accesses multiple cache lines of Set 0 and then Set 1, as shown in Figure <ref type="figure">2</ref>(c). For this example, each cache set has exactly the same number of cache misses, as shown in Figure <ref type="figure">2(b)</ref>. This indicates that conflict misses cannot be identified by the portion of misses in cache sets. Instead, the access pattern of the corresponding instruction(s) should be employed to identify such issues.</p><p>Conflict misses can not only be caused by applications, but also can be caused by the allocator when multiple allocated objects are mapped to the same cache set. raytrace, an application in PARSEC <ref type="bibr" target="#b3">[4]</ref>, introduces a 27% performance slowdown due to conflict misses of the allocator, as shown in Table <ref type="table">2</ref>. Conflict misses can be resolved or reduced by changing the starting addresses of objects, or padding the corresponding structure. Even for allocator-induced applications, we could insert some bogus memory allocations or change the size of the corresponding allocations in order to reduce cache misses.</p><p>2.1.3 Cache Coherence Misses. Multithreaded applications are prone to coherence misses when multiple threads are accessing the same cache line. When a thread writes to a cache line, the cache coherence protocol invalidates all existing copies of this cache line, introducing cache coherency misses. Coherence misses can be caused by true and false sharing. False sharing occurs when multiple threads are accessing different portions of the same cache line, while threads are accessing the same units in true sharing. When modern architectures are equipped with larger cache lines and more cores, they are more prone to coherence misses with higher performance impacts.</p><p>True sharing is typically caused by applications. Although true sharing is considered to be unavoidable <ref type="bibr" target="#b30">[32]</ref>, programmers could still refactor the code to reduce its seriousness <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b37">39]</ref>. For example, programmers may reduce the updating of shared variables by using thread-local or local variables. False sharing can be caused by both applications and allocators. For allocator-induced false sharing, multiple threads may access different objects concurrently within the same cache line that are allocated by different threads. False sharing can be reduced by padding the data structure <ref type="bibr" target="#b24">[25]</ref>, or using per-thread private pages <ref type="bibr" target="#b30">[32]</ref>. Therefore, it is important to differentiate between false and true sharing, as they need different fixing strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Basic Idea of CachePerf</head><p>CachePerf aims to identify the type and the origin of cache misses correctly so that programmers can further fix them correspondingly. More specifically, CachePerf not only differentiates capacity misses, conflict misses, and coherency misses, but also differentiates whether some misses are caused by the allocator or the application. If they are caused by the application, CachePerf further reports the lines of code with the issue, e.g., call sites and instructions. For allocator-induced cache misses, CachePerf also reports the sizes of the related objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Differentiating Different Types of Misses.</head><p>As mentioned in Section 1, it is challenging to identify the type of each miss directly. For instance, to identify a capacity miss, it is required to know the working set of the current program <ref type="bibr" target="#b41">[43]</ref>, which is infeasible under the coarse-grained sampling. CCProf <ref type="bibr" target="#b44">[46]</ref> observes that "a relatively larger portion of cache misses in a subgroup of the total cache sets over the others indicates conflicts in those cache sets". Unfortunately, this method is neither sufficient nor necessary condition of conflict misses, although it seems to be valid at the first glance. As shown in Fig. <ref type="figure">2</ref>(b), all cache sets have exactly the same number of cache misses for the Kripke application. However, this issue belongs to "conflict misses" based on the access pattern shown in Fig. <ref type="figure">2(c</ref>). Further, a for loop consecutively accessing an array (e.g., 1.5 times larger than the cache size) may cause only half of the cache sets to have significantly more cache misses than the other half, but this belongs to capacity misses instead of conflict misses.</p><p>In fact, CachePerf's identification is based on the following observations: (i) Coherence misses typically occur on few cache lines, but not for capacity and conflict misses; (ii) Extensive cache misses are typically caused by few susceptible instructions; (iii) The patterns of memory accesses are necessary to differentiate conflict misses from capacity misses: if multiple memory accesses are accessing the same set of cache lines, then it is an issue of conflict miss; If they are accessing different cache sets, the issue is more likely to be capacity miss.</p><p>Observation (i) indicates that coherence misses (e.g., false sharing and true sharing) can be identified by checking the cumulative behavior of cache lines: if few cache lines (not on the same set) have more cache misses than others, then this issue must be caused by coherence misses. Like existing work <ref type="bibr" target="#b30">[32]</ref>, false sharing can be easily differentiated from true sharing using their definitions: if multiple threads are accessing different words of the same cache line, then it is false sharing. Otherwise, it is true sharing. We will use the Performance Monitoring Unit (PMU)'s address sampling to collect accesses on a cache line, helping differentiate false sharing from true sharing.</p><p>Based on observation (i) and (ii), we propose the hybrid hardware sampling to classify cache misses: the hardware Performance Monitoring Unit (PMU) is employed to collect the coarse-grained samples in order to pinpoint susceptible instructions with extensive cache misses; After that, the breakpoints are further installed on these instructions in order to collect fine-grained memory accesses to understand their memory access patterns. After collecting memory access patterns, it is possible to differentiate conflict misses from capacity misses using observation (iii).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Differentiating Serious</head><p>Issues from Minor Ones. Minor issues, although they are not false positives, should be excluded to avoid wasting the time of programmers. Unfortunately, most existing tools <ref type="bibr">[6, 32-35, 39, 46]</ref> cannot achieve this goal, as they typically utilize the same absolute metric for different applications, e.g., the number of cache invalidations to evaluate false sharing issues, omitting the temporal effect. However, the same number of cache misses may have different performance impacts for a long-running or short-running program. Further, a program with sparse cache misses and another one with intense misses may benefit differently from the reduction of cache misses, even if they have a similar execution length and cache misses.</p><p>CachePerf further proposes two ratio-based mechanisms to exclude minor issues. First, CachePerf proposes a windowing mechanism that tracks a specified number of the most recent memory accesses, and then only checks cache misses inside if the miss ratio (i.e., the number of misses divided by the number of accesses) in the past window is larger than a threshold, as discussed in Section 3.2. This windowing mechanism excludes sparse or sporadic cache misses. Second, CachePerf only reports a potential issue if its related memory accesses and cache misses are higher than 0.01% and 1% separately. The access ratio can be utilized to predict the potential performance impact. Assuming that the memory access is 200? slower than the L1 cache access <ref type="bibr" target="#b28">[30]</ref>, and the access ratio of a potential bug is 0.01% of the total accesses (of the program). We further assume that other accesses of this program can be satisfied at L1 cache, then the total runtime of this program is 0.01% ? 200X + 99.99% ? X = 101.9%X, if the cycle of L1 cache access is X. Then this bug will introduce at most 2% slowdown, comparing to all accesses are satisfied by the L1 cache (100%X). Similarly, the ratio of cache misses helps prune insignificant instructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Differentiating</head><p>Allocator-Caused Misses from Applications. As discussed in Section 2.1, the allocator may introduce both conflict and coherence misses. When an allocator allocates multiple objects that happen to access few sets of cache lines, it introduces conflict misses. An allocator can introduce false sharing by allocating multiple objects in the same cache line to different threads <ref type="bibr" target="#b2">[3]</ref>. CachePerf tracks the allocation information (e.g., the thread, address) that could help differentiate the bugs caused by applications from those caused by the allocator. To the best of our knowledge, CachePerf is the first work that could report allocator-induced cache misses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DESIGN AND IMPLEMENTATION</head><p>This section discusses the detailed design and implementation of CachePerf. CachePerf is designed as a library that can be linked with different applications, without the need of changing and recompiling user programs. In the following, we start with the description of CachePerf's basic components, and then discuss each component separately. Fig. <ref type="figure">3</ref> shows the basic components of CachePerf. As mentioned in Section 1, CachePerf relies on the PMU-based sampling to collect the information of memory accesses and cache misses, which will be handled by its "Access Sampler" module. To exclude insignificant cache misses, CachePerf introduces a "Miss Ratio Checker" module that computes and checks the cache miss ratio (the percentage of cache misses in all memory accesses). When the cache miss ratio is larger than a predefined threshold (e.g., 0.5%), as further discussed in Section 4.5.2, all recent cache misses will be further updated to "Miss Store" and "Instruction Store". Otherwise, all cache misses will be skipped. Due to this filtering mechanism, low-frequency cache misses (such as some compulsory misses) will be excluded automatically. When continuous cache misses from the same instruction are detected or multiple misses are landing on the same cache set, indicating possible capacity or conflict misses, CachePerf further employs breakpoints to collect fine-grained memory accesses information (via "breakpoint Handler"), which enables us to differentiate conflict misses from capacity misses.</p><p>In order to attribute cache misses to data objects (called "data-centric" analysis <ref type="bibr" target="#b33">[35]</ref>), CachePerf further intercepts memory allocations and deallocations, and updates the "Object Store" correspondingly. "Object Store" tracks address ranges and callsites of heap objects. In the end, CachePerf classifies cache misses by integrating the data in "Miss Store" and "Instruction Store", and finally reports helpful information based on "Object Store", including the allocation call sites, object size, and object name (only for global objects). Different from existing tools <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b44">46]</ref>, there is no need for offline analysis, i.e., it has no hiding overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Access Sampler</head><p>For the access sampler, CachePerf employs the Performance Monitoring Units (PMU) to sample memory accesses. The PMU is the ubiquitous hardware in modern architectures (e.g., X86 or ARM) that can provide hundreds of hardware events <ref type="bibr" target="#b15">[16]</ref>. There is a trend for profilers to build on top of the PMU <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b48">50]</ref>, due to its low overhead. Currently, Linux also provides a system call -perf_event_open -that allows to configure and start the PMU easily.</p><p>CachePerf samples two types of events, including memory loads and stores. The configuration for the PMU sampling is shown in detection effect on loads and stores, we empirically set the sampling period of loads as 20,000, and the one of stores as 50,000, which has been evaluated in Section 4.5. To avoid different threads from sampling the same instructions, we introduce 10% randomized variance for each thread's sampling period. Note that it is important to include PERF_SAMPLE_DATA_SRC in the sample type so that we can know which level the corresponding instruction is hit, such as L1, L2, LLC, or memory. It is also referred to as "hit information" in the remainder of this paper. CachePerf employs the following information of the sampling: the type of access (e.g., load or store), hit information, memory address, and instruction pointer (IP). Among them, the hit information helps identify all cache misses from all sampled memory accesses, where all accesses that do not hit on the L1 cache will be treated as cache misses. IP tells the instruction performing the corresponding access, and the memory address helps pinpoint which cache line and cache set have the miss, enabling us to perform the classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Miss Ratio Checker</head><p>A miss ratio checker is introduced to filter out sparse cache misses. As mentioned above, since sparse cache misses may not incur significant performance slowdown, they should be excluded in order to avoid wasting the effort of fixing such issues. Further, the filtering reduces the memory overhead of storing such cache misses and the performance overhead of spending in classification.</p><p>In the implementation, CachePerf maintains two circular buffers to track the most recently sampled memory accesses for each thread, one buffer for memory loads and the other one for memory stores. These buffers are updated in First-In-First-Out order that the later accesses will overwrite the leastrecent memory accesses. CachePerf computes the cache miss ratio upon every access via dividing the number of cache misses by that of accesses. Only when the miss ratio of the buffer is larger than a predefined threshold (e.g., 0.5%), all cache misses in the current buffer will be handled and be updated to "Instruction Store" and "Miss Store". Otherwise, they will be skipped. The Instruction Store holds the information related to instructions, such as the number of accesses and cache misses. The Miss Store maintains the detailed information about each cache miss, e.g., object, line, and set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Breakpoint Handler</head><p>As mentioned in Section 2, CachePerf employs the breakpoints to collect fine-grained memory accesses of the selected instructions, enabling us to differentiate conflict misses from capacity misses. For the susceptible instructions, CachePerf focuses on two types of instructions: (1) instructions introducing multiple continuous cache misses, indicating that they may incur extensive cache misses.</p><p>(2) instructions introduce extensive misses on the same set in a time window, which are potential candidates for conflict misses.</p><p>After identifying these instructions, CachePerf installs hardware breakpoints via the perf_event_ open system call by specifying the type to be "PERF_TYPE_BREAKPOINT" and the bp_type to be "HW_BREAKPOINT_X". After the installation, every time a program executes such an instruction, CachePerf will be interrupted so that it could collect the fine-grained memory accesses of each instruction. However, the interrupt handler provides no information about the memory address, as the breakpoint is typically triggered before the access. CachePerf infers the memory address by analyzing the corresponding instruction. For example, if the instruction is "addl $0x1,-0x4(%rbp)", then CachePerf could infer the stored memory address via the value of register and rbp. CachePerf employs Intel's xed library <ref type="bibr" target="#b13">[14]</ref> to perform the binary analysis.</p><p>To simplify the handling, CachePerf only installs one breakpoint for all threads at a time, collecting all accesses from different threads. To reduce the overhead caused by handling endless interrupts, CachePerf only collects at most 64 accesses from one instruction. If there are 8 accesses landing on the same set, then it is identified as a bug with conflict misses. Otherwise, it is a bug with capacity misses. Based on this, if 8 continuous accesses are landing on the same cache set, which can be clearly identified as "conflict misses", CachePerf will remove the breakpoint so that it could monitor other instructions.</p><p>However, it is possible that an instruction has no or few accesses after the installation. When new instructions require to be monitored, CachePerf further introduces an expiration mechanism that a breakpoint will be expired after 100ms. In this way, CachePerf is able to install breakpoints on new instructions. The identification of cache misses is further discussed in Section 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Important Data Stores</head><p>CachePerf maintains Object Store, Miss Store, and Instruction Store, as further described below.</p><p>Object Store. Object Store tracks the information of two types of objects, heap objects and global objects, as most cache misses occur on these objects, which are handled differently.</p><p>For heap objects, CachePerf intercepts all memory management functions, such as malloc() and free(), in order to track their corresponding callsites. For each heap object, CachePerf tracks its size, callsite, and address range. As there are large amounts of heap objects, the Data Store should be carefully designed in order to support the following operations efficiently: adding and updating of an object via the starting address upon memory allocations and deallocations, and searching by an address in the range of a valid object upon each sampled access. Although the hash table can support the adding and updating operations efficiently via the starting address (as the key), it is expensive to search the memory address inside heap objects (different from the key). Instead, an ordered list/array supports the searching better via the binary search. Furthermore, we observe that heap objects are typically classified into small, medium, and large sizes, where the number of small-size objects is much larger than that of medium-size and large-size objects.</p><p>Based on these observations, CachePerf designs a three-level data store as shown in Fig. <ref type="figure">4</ref> to support efficient adding, updating, and searching. In particular, any object can be stored in one of Page Table, Chunk Table, and Sorted Huge Objects List, which are mutually exclusive. CachePerf updates these tables/lists as follows: (1) if an object exists only in a single page, it is stored in the Page Table ; (2) If the range of an object crosses two different pages but within the same megabyte, it is stored in the Chunk Table ; (3) Otherwise, it is inserted into the Sorted Huge Objects List. For each address, CachePerf always searches the Page Table at first (with the highest possibility), then the Chunk Table, and finally the Sorted Huge Objects List, and stops if the object is found already. Although three searches are required for some objects, however, we believe that such searches will Level 1: Page Table</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sorted Array</head><p>Level 2: Chunk Table</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sorted Array</head><p>Level 3: Sorted Huge Objects List Search Order Fig. <ref type="figure">4</ref>. A three-level object store that combines with the shared memory and the sorted array/list. be fewer than others. This design is based on the assumption that small objects (less than 1-page size) are typically significantly more than large objects.</p><p>For the performance reason, each entry of Page Table and Chunk Table stores a pointer pointing to a sorted array that stores all allocated objects inside the same page (4KB) and chunk (1MB). If an entry is empty (with the NULL value), then there are no objects in the corresponding page or chunk, indicating the unnecessary of searching for a higher-level store. Both tables are employing the shadow memory <ref type="bibr" target="#b46">[48,</ref><ref type="bibr" target="#b56">58]</ref> to store these pointers, where the index of each entry could be computed simply with a bit-shifting operation. Since the number of huge objects is typically small, a sorted list is used to store huge objects, which is not using the shadow memory. For the sorted arrays and lists, the search can be done efficiently via the binary search.</p><p>CachePerf also proposes a callsite-based optimization to reduce the overhead, especially on the updates of memory allocations and sampled accesses: if objects from a particular callsite have a much lower cache miss ratio, compared to the average one, then all allocations and cache misses from this callsite could be safely skipped. Based on our observation, such an optimization reduces the overhead by over 30% for a particular application (Canneal of PARSEC <ref type="bibr" target="#b3">[4]</ref>).</p><p>CachePerf handles global objects differently, as an application typically has a small number of global objects and they are not increased during the execution. All accesses of global objects (e.g., addresses) will be stored in a hash table. CachePerf obtains the name and address range by analyzing the corresponding ELF header, and then computes the miss ratio of each object, as discussed in Section 3.5.</p><p>Miss Store. The Miss Store saves the information of each cache miss, which is not filtered out as described by Miss Ratio Checker (Section 3.2). In particular, cache misses are stored in two separate data structures: an array (with the size of the number of cache sets) stores the information of each cache set, and a hash table stores the information of each cache line (using the starting address as the key). For both data structures, CachePerf stores the number of cache misses (on each cache set and each cache line). For cache lines, CachePerf further stores the thread information for accessing each word, which could help differentiate false sharing from true sharing.</p><p>Instruction Store. Instruction Store saves the information of memory accesses (e.g., loads and stores) and cache misses of the selected instructions by the miss ratio checker (as discussed in Section 3.2). The data structure of Instruction Store is a hash table that uses the instruction pointer as the hash key. For each instruction, CachePerf records the number of cache misses, the related cache set, and the detailed memory access pattern.</p><p>Since each line/statement of the code may be related with multiple instructions (at the assembly level), CachePerf further summarizes cache misses of the related statement, and only reports statements with extensive cache misses. CachePerf classifies and reports serious cache misses by combining the information from Cache Miss Store and Instruction Store together. The detailed algorithm is shown as Algorithm 1. CachePerf omits cache misses without significant performance impacts. Instead, it focuses on instructions or cache lines that have passed the "ratio-based filtering": (1) for an application, if the number of load misses is less than 3% of all load accesses and the number of store misses is less than 1% of all store accesses, then CachePerf will not report any issue; (2) for each instruction, if its memory accesses are less than 0.01% of total accesses, or its cache misses are less than 1% of total misses, this instruction will not be reported; (3) for each cache line and each cache set, it will be reported only if its misses larger than 1% of all misses. These numbers are set based on our experience, which has been evaluated as Section 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Miss</head><p>CachePerf reports potential coherence misses by checking all cache lines in Miss Store. As discussed in Section 2.2.1, few cache lines with extensive cache misses but not mapping to the same cache set can be caused by coherency misses. For each cache line, CachePerf can further determine the type, false sharing and true sharing, via word-level information of the corresponding cache lines. If multiple threads are accessing the same words of the cache line, then it is true sharing of applications. Otherwise, it is a false sharing problem. CachePerf further checks whether multiple objects on the same cache line are allocated by different threads or not. If yes, then it is allocatorinduced false sharing. Otherwise, it is the application's false sharing. If cache lines are identified as coherency misses, the corresponding instructions will be marked as checked, which will be excluded for identifying conflict misses and capacity misses afterward.</p><p>CachePerf differentiates capacity misses from conflict misses based on the memory access pattern of each instruction (with extensive cache misses) in the Instruction Store. A simple mechanism is employed to differentiate conflict misses from capacity misses: if the number of accesses mapping to the same cache set is larger than a threshold (e.g., 8), then the corresponding cache misses will be considered as conflict misses. Otherwise, they are capacity misses. For conflict misses, CachePerf further checks whether they are caused by the allocator or not: if they are involved with multiple heap objects, this belongs to allocator-induced conflict miss. Otherwise, it is an application's conflict miss.</p><p>CachePerf could further report the detailed information of cache misses, including the instruction information (from Instruction Store) and object information (from Object Store). The former one tells which instructions introduce cache misses, while the latter one helps locate the heap object with its allocation callsite. This information could guide bug fixes. For instance, if two objects mapping to the same cache set introduce excessive cache misses, such an issue can be significantly reduced by changing the address of objects (by mapping to different sets).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL EVALUATION</head><p>The experimental evaluation will answer the following research questions:</p><p>? How is the effectiveness of CachePerf? (Section 4.2)</p><p>? What is the performance overhead of CachePerf? (Section 4.3) ? What is the memory overhead of CachePerf? (Section 4.4)</p><p>? What are the impacts of different configurations? (Section 4.5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setting</head><p>Hardware Platform: Experiments are evaluated on a two-processor machine, where both processors are Intel(R) Xeon(R) Gold 6230 with 20 cores. We only enabled 16 hardware cores in one node to exclude the NUMA impact as it is outside the scope of this paper. The machine has 256GB of main memory, 64KB L1 cache, and 1MB of L2 cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Software:</head><p>The OS is Ubuntu 18.04.3 LTS, installed with Linux-5.3.0-40. The compiler is GCC-7.5.0, while we are using -O2 and -g flags for the compilation.</p><p>Evaluated Applications: Two types of applications are included in the evaluation, including general applications and applications known to have cache misses. In particular, all 13 applications from the PARSEC-2.0 benchmark are included as general applications <ref type="bibr" target="#b3">[4]</ref>, but some also have known bugs. Buggy applications with coherence misses (false sharing) include two stress tests cache-scratch and cache-thrash from Hoard <ref type="bibr" target="#b2">[3]</ref>, and two Phoenix <ref type="bibr" target="#b43">[45]</ref> applications (histogram and linear_regression). Among them, the first two applications actually have false sharing caused by the allocator. Five applications with conflict misses are collected from CCProf <ref type="bibr" target="#b44">[46]</ref>: ADI <ref type="bibr" target="#b42">[44]</ref>, HimenoBMT <ref type="bibr" target="#b12">[13]</ref>, Kripke <ref type="bibr" target="#b26">[27]</ref>, MKL-FFT <ref type="bibr" target="#b8">[9]</ref>, and NW <ref type="bibr" target="#b6">[7]</ref>. TinyDNN <ref type="bibr" target="#b52">[54]</ref> is not included, since we did not observe conflict misses and the change (based on CCProf <ref type="bibr" target="#b44">[46]</ref>) did not improve the performance. We also include irs [28] and SRAD <ref type="bibr" target="#b54">[56]</ref> applications that were employed by ArrayTool <ref type="bibr" target="#b33">[35]</ref> to evaluate capacity misses. Note: to reproduce false sharing on our machine, histogram processes a special BMP file adapted from the original one that all of the red values are set to 0 and the blue values are set to 255. For linear_regression, we also use the volatile keyword for the args variable in order to avoid the optimization of the compiler. For HimenoBMT, the grid size is medium and the number of integration is 80. NW's matrix dimension is set to be 16384 ? 16384, and its penalty is set to be 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluated Allocators:</head><p>To evaluate CachePerf's detection on issues introduced by allocators, we evaluate on two widely-used allocators, Glibc-2.28 and TCMalloc-4.5.3. Glibc-2.28 includes the default allocator in our machine, and TCMalloc is a widely-used allocated designed by Google <ref type="bibr" target="#b11">[12]</ref>.</p><p>Comparison: We compare CachePerf with two state-of-art tools in effectiveness, performance, and memory consumption. One is CCProf which detects cache conflict misses <ref type="bibr" target="#b44">[46]</ref>, and the other one is Feather for false sharing detection <ref type="bibr" target="#b5">[6]</ref>. We have difficulty running ArrayTool <ref type="bibr" target="#b33">[35]</ref> successfully, which is the reason why ArrayTool is not included for comparison. For these tools, we use their default sampling rates used for their evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Effectiveness</head><p>We list the effectiveness results of CachePerf's detection in Table <ref type="table">2</ref>. Overall, CachePerf reports all known bugs and detects 9 new bugs, while fixing the reported bugs achieves the performance improvement between 3% and 3788%. Some applications with capacity misses cannot be easily fixed, marked as "?" in the "Improve" column. This also concurs with our discussion in Section 2.1 that not all capacity misses can be fixed easily. CachePerf correctly identifies all types of bugs, except bug 7 in ADI. The type is identified by CachePerf as capacity miss, but it is actually conflict miss. Based on our investigation, the failure of the identification is caused by the skids of the PMU hardware <ref type="bibr" target="#b0">[1]</ref>. The PMU hardware fails to pinpoint the exact instruction with the sampled cache miss, with the distance of one instruction. Therefore, CachePerf actually captures the access pattern of an instruction different from the one with cache misses, which does not have the pattern of conflict misses. However, our observation that "an instruction's access pattern is not changed during the whole execution" still holds.</p><p>Note that although streamcluster has been reported by previous tools with a false sharing issue, but achieving no performance improvement after fixing the bug as suggested by previous tools <ref type="bibr" target="#b30">[32]</ref>. CachePerf successfully avoids the report of this bug, therefore, preventing programmers to spend the effort on this bug. In contrast, Feather still reports this insignificant bug, which is the reason why it is marked as "? -". Feather cannot report the origin of false sharing in both cache-scratch and cache-thrash, which are allocator-induced conflict misses. Similarly, although CCProf reports conflict misses of raytrace, but it fails to identify as an allocator-induced miss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Conflict Misses of Applications.</head><p>CachePerf could correctly report all known conflict misses, including ADI, HimenoBMT, Kripke, MKL-FFT, and NW. These bugs can be fixed by switching the order of loops (Kripke) and using the padding (others).</p><p>CachePerf further detects three unknown conflict misses, in ADI, SRAD, and swaptions. The bug of the SRAD application is shown in Fig. <ref type="figure" target="#fig_1">5</ref>, which can be detected by CCProf. CachePerf reports that line 243 of main.c introduces around 64% of load misses. As shown in Fig. <ref type="figure" target="#fig_1">5</ref>(a), SRAD uses two nested for loops to calculate the sum for every pixel in the image ROI. By simply switching these two loops, we improve the performance by 748%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Allocator-Induced Conflict Misses.</head><p>CachePerf also detects a serious conflict miss in raytrace caused by the default allocator-glibc-2.28, as shown in Fig. <ref type="figure">6</ref>. The report can be seen in Fig. <ref type="figure">6(b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category Index Application Improve CCProf Feather CachePerf New</head><p>False Sharing</p><formula xml:id="formula_1">1 cache-scratch* 1007% ? ? - ? ? 2 cache-thrash* 3788% ? ? - ? ? 3 histogram 117% ? ? ? 4 linear_regression 712% ? N/A ? 5 streamcluster 0% ? ? - ? Conflict Miss 6 ADI 246% ? ? ? 7 ADI 18% ? ? ? - ? 8 HimenoBMT 964% ? ? ? 9 Kripke 7% N/A N/A ? 10 MKL_FFT 52% ? ? ? 11 NW 245% ? ? ? 12 raytrace* 27% ? - ? ? ? 13 SRAD 748% ? ? ? ? 14 swaptions 3% ? ? ? ? Capacity Miss 15 bodytrack ? ? ? ? ? 16 canneal ? ? ? ? ? 17 IRS 33% ? ? ? 18 SRAD 12% ? ? ? 19 streamcluster ? ? ? ? ?</formula><p>Table <ref type="table">2</ref>. This table lists applications with cache misses. For applications marked with *, cache-scratch, cache-thrash have allocator-induced false sharing, and raytrace has allocator-induced conflict misses.</p><p>Column "Improve" lists the performance improvement after fixes based on information provided by CachePerf, where column "New" indicates whether it is first discovered by CachePerf. Further, "?" indicates the tool correctly detects the issue, "? -" indicates an imperfect report, "?" indicates a failed detection, and "N/A" indicates that the corresponding application crashes or deadlocks when running with the tool. Note that applications marked as "?" in "Improve" cannot be fixed easily, which confirms our discovery in Section 2.1. For this problem, the default glibc-2.28 happens to allocate many 48-byte objects mapping to the same cache set, causing conflict misses. TCMalloc does not have this issue, which runs about 27% faster on this application than glibc-2.28. We further confirm whether there exists a systematic method in TCMalloc to prevent such an issue. We find that TCMalloc always requests two pages at a time, then allocates objects (48 bytes) continuously, and skips non-used bytes in the end. This mechanism luckily avoids conflict misses of raytrace application. In summary, allocator-induced conflict misses are not easy to prevent from the design of the allocator. This also shows the importance of CachePerf that could help identify the root cause of performance slowdown. After finding out the issue, programmers may switch to a different allocator, or change the application by introducing unnecessary allocations inside or changing the alignment of the related structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Capacity Misses of Applications.</head><p>We borrowed some buggy applications from ArrayTool <ref type="bibr" target="#b33">[35]</ref>,</p><p>including IRS [28] and SRAD <ref type="bibr" target="#b54">[56]</ref>. The paper also reports serious issues in a specific version of Fig. <ref type="figure">6</ref>. CachePerf reports an allocator-caused conflict miss in raytrace LULESH <ref type="bibr" target="#b22">[23]</ref>. However, we cannot find the exact source code, which is the reason why LULESH is not included. Besides these applications, CachePerf also detects unknown capacity misses in bodytrack, canneal, and streamcluster, which has been confirmed by us manually. However, as mentioned in Section 2.1, not all capacity misses could be fixed easily.</p><p>As shown in Table <ref type="table">2</ref>, CachePerf successfully reports capacity misses hidden in both IRS and SRAD. As an example, the IRS's source code and report are shown in Fig. <ref type="figure" target="#fig_3">7</ref>. IRS's capacity misses occur in line 239 of aos3.cpp, although addr2line actually reports lines between 239 and 247. This statement accesses many objects of the same size (88824176 bytes), e.g., dbl, xdbl, dbc. Since every object has exactly the same access pattern, these accesses should be grouped together. Using the suggested fix strategy <ref type="bibr" target="#b33">[35]</ref>, the performance can be improved by 32.7%.</p><p>Application's capacity misses, accessed by: #0 instruction at:  Note that CachePerf cannot report SRAD's capacity miss in the original version when the conflict miss (as shown in Fig. <ref type="figure" target="#fig_1">5</ref>) is the dominant performance issue. We also confirmed that applying the suggested fix by ArrayTool <ref type="bibr" target="#b33">[35]</ref> achieves almost no performance improvement. In fact, this actually illustrates the effectiveness of CachePerf as its rule-based filtering mechanism avoids reporting minor issues. After fixing the conflicting miss of SRAD, then CachePerf could successfully report the capacity miss. After fixing the report bug, SRAD's performance is improved by 12.4% finally.</p><formula xml:id="formula_2">[i] = dbl[i] * xdbl[i] + dbc[i] * xdbc[i] + dbr[i] * xdbr[i] + 240: dcl[i] * xdcl[i] + dcc[i] * xdcc[i] + dcr[i] * xdcr[i] + 241: dfl[i] * xdfl[i] + dfc[i] * xdfc[i] + dfr[i] * xdfr[i] + 242: cbl[i] * xcbl[i] + cbc[i] * xcbc[i] + cbr[i] * xcbr[i] + 243: ccl[i] * xccl[i] + ccc[i] * xccc[i] + ccr[i] * xccr[i] + 244: cfl[i] * xcfl[i] + cfc[i] * xcfc[i] + cfr[i] * xcfr[i] + 245: ubl[i] * xubl[i] + ubc[i] * xubc[i] + ubr[i] * xubr[i] + 246: ucl[i] * xucl[i] + ucc[i] * xucc[i] + ucr[i] * xucr[i] + 247: ufl[i] * xufl[i] + ufc[i] * xufc[i] + ufr[i] * xufr[i]; 248: } 249: } 250: } (a) Source code (aos3.cpp) (b) CachePerf's report</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Coherency Misses (FS) of Applications.</head><p>For coherency misses of applications, we utilize three known buggy applications to evaluate CachePerf's effectiveness, including histogram, linear_regression, and streamcluster. CachePerf successfully detects these issues latent in histogram and linear_regression, similar to existing work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">32]</ref>. We show the source code and CachePerf's report of linear_regression in Fig. <ref type="figure" target="#fig_4">8</ref>. This is a known bug that the structure of args is not aligned to 64 bytes (but only 52 bytes instead). As a result, thread 1 will access the same cache line as thread 2. By simply aligning the related structure, the performance can be improved by 712%. Different from existing tools, CachePerf will not report the issue of streamcluster, although it was reported to have false sharing for the work_mem object <ref type="bibr" target="#b30">[32]</ref>. Based on existing work, we fixed the false sharing by using the padding and observed the reduction of cache misses. However, we do not observable performance impact with this change, less than 1%. That is, CachePerf successfully excludes the insignificant issue, avoiding the waste of manual effort. In contrast, Feather still reports this false sharing of streamcluster, although it only imposes little performance impact.</p><p>Application's false sharing, accessed by: #0 instruction at:</p><p>-linear_regression-pthread.c: 94  A simple solution is to change the alignment of the structure related to obj, which improves the performance by 1007%. CachePerf also reports some serious allocator-caused false sharing issues for both cache-scratch and cache-thrash with the TCMalloc allocator. There will be 3788% performance improvement using the padding. Although allocator-caused false sharing is a bug of the allocator design, it can be prevented by changing the application itself. In addition, users could switch to a new allocator to fix such issues. CachePerf provides helpful information that could help fix such bugs. Comparing with Other Tools: Overall, CachePerf shows three obvious advantages when compared with existing tools. First, CachePerf can detect multiple types of cache misses, while others could only report a specific type of cache misses. Note that the other tools are mutually exclusive, forcing programmers to use them one after the other. Second, CachePerf is the only tool that identifies the performance issues introduced by the memory allocator, preventing programmers from wasting the unnecessary effort of improving applications but achieving no performance improvement. Finally, CachePerf is the only tool excluding minor issues with little performance impact, saving users' time.</p><formula xml:id="formula_3">#</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance Overhead</head><p>We evaluated the performance overhead of CachePerf, CCProf, and Feather. Since CCProf and Feather have online and offline stages, we add their overhead of two stages together. The results (with the AVERAGE and GEOMEAN) are shown in Fig. <ref type="figure" target="#fig_6">10</ref>. On average (GEOMEAN), CachePerf introduces 14% performance overhead, while CCProf's overhead is 5.3? and Feather's overhead is 80% if considering both online profiling and offline analysis. Even only considering their online profiling, CachePerf is still faster than both CCProf and Feather. Based on our understanding, CachePerf's ratio-based mechanism helps reduce much unnecessary overhead by pruning sporadic cache misses, but without compromising its effectiveness. CachePerf's specific data structures also help reduce the overhead. CachePerf's hybrid sampling technique that combines with both coarse-grained and fine-grained sampling also balances the accuracy and the overhead. On one hand, its coarse-grained sampling works as a filter that allows it to focus on only a few susceptible instructions, avoiding installing unnecessary breakpoints. On the other hand, the breakpoints effectively ensure the precision of the tool even with a low PMU-based sampling rate.</p><p>MKL_FFT is the only application with an overhead higher than 100%. We confirmed that more than 80% of its overhead is spent in its reporting phase, which could be placed offline if necessary. This application has involved a big amount of cache sets, heap objects, and instructions. For instance, CachePerf requires to invoke the expensive addr2line to obtain the line numbers for many lines. We are planning to reduce such overhead with heuristics in the future.</p><p>CachePerf introduces 36% performance overhead for canneal. The basic reason is that canneal has a great number of allocations (about 1.3 million per second), where keeping the information of these objects adds significant overhead (and memory overhead). Similarly, CachePerf introduces high overhead for keeping and updating the information of objects for raytrace, as there are around 500 thousand memory allocations each second. CachePerf introduces high overhead for bodytrack and facesim for a similar reason. the overhead is introduced by CachePerf's initialization overhead for its pre-defined hash tables. However, CachePerf only introduces around 19% memory overhead on average for applications with large footprints (e.g., &gt; 100MB). Considering the functionalities provided by CachePerf, we believe that the memory overhead of CachePerf is reasonable and acceptable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Impact of Configurations</head><p>In this section, we investigate the performance and effectiveness impacts of different configurations using all PARSEC applications. We investigate the impact of the sampling rate, thresholds of Miss Ratio Checker, breakpoint configurations, and access/miss ratio.</p><p>4.5.1 Sampling Rate. We evaluated three sets of sampling periods as shown in Table <ref type="table">4</ref>. With its default setting (marked as bold), CachePerf's GEOMEAN performance and memory overhead are 14% and 48% separately. When the sampling frequencies are 10 times lower than the default setting, the performance overhead is 10% and the memory overhead is 47%. However, as shown in the "CP1" column of  <ref type="table">4</ref>. This table lists the performance and memory overhead under different sampling periods ("L" is the load sampling period and "S" is the store sampling period) and different thresholds of the Miss Ratio Checker ("Miss Ratio") using all PARSEC applications. The middle column (in bold) is the default configuration. 4.5.2 Threshold of Miss Ratio Checker. We further investigate the impacts of different thresholds of the Miss Ratio Checker. CachePerf will handle all cache misses inside the buffers, when the cache miss ratio is larger than the pre-defined threshold. As described in Section 3.2, the default threshold is 0.5%. In the default setting, CachePerf's performance and memory overhead are 14% and 48% separately. When the threshold is increased to 2.5%, indicating CachePerf will only handle all cache misses when there are 25 misses out of 1000 accesses, the performance overhead is 12% and the memory overhead is 45%, as shown in Table <ref type="table">4</ref>. However, as shown in "CP2" in Table <ref type="table" target="#tab_7">5</ref>, CachePerf will miss 5 issues under this configuration. Another setting is 0%, indicating CachePerf will handle all cache misses in the buffer, the performance and memory overhead is 18% and 70% correspondingly. However, this setting does not report more issues. Overall, the default threshold of Miss Ratio Checker has a good balance between overhead and effectiveness. 4.5.3 Breakpoint Configuration. We also evaluated the overhead and effectiveness impacts of different breakpoint configurations. As discussed in Section 3.3, CachePerf collects at most 64 accesses from one selected instruction, and identifies the bug as the conflict miss when more than 8 accesses are landing on the same cache set. That is, CachePerf will remove the breakpoint on this instruction if 8 continuous accesses are from the same set. Besides this default setting, we also evaluated using 4 or 16 accesses as the condition for identifying the conflict miss. We also evaluated different expiration time for the breakpoint installing on an instruction, such as 10ms and 1000ms, where the breakpoint will be installed for a new instruction. However, we do not observe a significant difference in overhead or effectiveness for different configurations.  <ref type="table" target="#tab_7">5</ref>. This table lists the effectiveness of CachePerf under different configurations. "CP" is the default setting, with the load and store sampling periods to be 20K and 50K separately. "CP1" has 10 times lower sampling frequencies ( 200K and 500K separately), but will miss 10 cases. In "CP2", its miss ratio checker uses a higher threshold (2.5%), which will miss 5 cases. "?", "? -", and "?" have the same meaning as Table <ref type="table">2</ref>.</p><p>4.5.4 Thresholds of Miss Rates. As described in Section 3.5, CachePerf will skip the report if the number of load misses is less than 3% of all load accesses and the number of store misses is less than 1% of all store accesses. The goal is to exclude minor issues. To evaluate the correctness of the two thresholds, we checked the load and store miss rates of all evaluated applications. Overall, for applications with reported issues, as listed in Table <ref type="table">2</ref>, their load or store miss rates are higher than the default thresholds. For applications where we do not observe significant issues (not listed in Table <ref type="table">2</ref>), the load and store miss rates are both lower than these thresholds. Therefore, the current thresholds of miss rates are helpful to filter out minor issues and highlight significant issues of cache misses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>This section discusses the compatibility, thresholds, and limitation of CachePerf.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Compatibility</head><p>CachePerf can be easily adapted to different hardware environments, such as cache with different cache line sizes or associativity. Currently, the cache-related parameters (e.g. cache line size, cache associativity) are listed in a configuration file. If users would like to use CachePerf for hardware with a different setting, they only need to change this configuration file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Configurable Thresholds</head><p>CachePerf introduces some thresholds to control the sampling and the reporting. Such thresholds are confirmed to balance the overhead and effectiveness on the evaluated machine. In a different environment, users may need to change these thresholds. The thresholds used by CachePerf can be easily changed via compilation flags or environmental settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Limitation</head><p>CachePerf utilizes the hardware-based sampling techniques to perform the profiling, which has the benefits that do not need to change the programs and imposes little performance overhead. However, the setting of the PMU-based sampling may require some slight changes on different machines with different implementations. Since the PMU-based sampling and the breakpoint-based sampling are generally supported by different hardware architecture, the proposed techniques should be applicable for different hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>We discuss the related work based on the type of cache misses in the following. Although some tools, such as perf <ref type="bibr" target="#b9">[10]</ref>, oprofile <ref type="bibr" target="#b29">[31]</ref>, different Pin tools <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b36">38]</ref>, or cachegrind (one tool inside Valgrind <ref type="bibr" target="#b38">[40]</ref>), could report the percentage of cache misses in the lines of code, they cannot identify the type of cache misses. Therefore, they are not the focus of this paper. In the following, we only list tools that could identify the type of cache misses.</p><p>Detecting Capacity Misses: Tao et al. propose a cache simulator that can identify cache capacity misses using the reuse distance for each memory access <ref type="bibr" target="#b49">[51]</ref>. Nikos et al. propose another cache simulation methodology <ref type="bibr" target="#b39">[41]</ref>. Both cache simulators could study cache behaviors under various cache configurations, but neither of them can be used as an online profiling tool due to their prohibitive overhead. Delorean <ref type="bibr" target="#b40">[42]</ref> improves the simulation efficiency, and identifies cache capacity misses by the number of distinct memory accesses since the last access to the observed cache line. However, it is still a simulation technique that requires the inspection of every memory access, which is slow too. ArrayTool focuses on a special type of capacity misses caused by multiple arrays <ref type="bibr" target="#b33">[35]</ref>. It utilizes the PMU-sampling to collect memory samples and determines candidate arrays by the combination of array affinities and array's access patterns.</p><p>Detecting Conflict Misses: Cache simulators detect conflict misses by simulating the cache behavior based on the memory trace <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b55">57]</ref>, but they are too slow to be used for online profiling. CCProf proposes to employ Re-Conflict Distance to filter out cache sets with low RCD <ref type="bibr" target="#b44">[46]</ref>, based on address sampling. However, CCProf may introduce high performance overhead due to the use of a low sampling rate to capture RCD. As shown in Fig. <ref type="figure" target="#fig_6">10</ref>, the overhead of CCProf can be as much as 945 times. In contrast, CachePerf imposes significantly less overhead while could identify different types of cache misses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detecting Cache Coherency Misses:</head><p>There exist multiple types of tools that could detect cache coherence issues, mostly focusing on false sharing. Some tools are relying on binary instrumentation <ref type="bibr" target="#b56">[58]</ref>, compiler-based instrumentation <ref type="bibr" target="#b32">[34]</ref>, process-based page protection <ref type="bibr" target="#b30">[32]</ref>, and the PMUbased sampling <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b37">39]</ref>. Instrumentation-based tools are generally too expensive to be employed in the production environment <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b56">58]</ref>, while Sheriff only supports C/C++ applications using standard synchronizations <ref type="bibr" target="#b30">[32]</ref>. In theory, Sheriff cannot be able to support some complicated applications (e.g., MySQL) with ad hoc synchronizations. The approaches with the PMU-based sampling is efficient, but with their own shortcomings: Cheetah utilizes a simplified method to compute the number of cache invalidations <ref type="bibr" target="#b31">[33]</ref>, instead of relying on the sampled cache misses; Jayasena et al. propose a machine learning approach based on the sampled events <ref type="bibr" target="#b19">[20]</ref>, Laser utilizes a special type of events (hit-Modified) that may not be available on all hardware <ref type="bibr" target="#b37">[39]</ref>, while Feather utilizes the combination of the PMU-sampling and watchpoints to identify false sharing <ref type="bibr" target="#b5">[6]</ref>; however, all existing tools typically report an absolute number to evaluate the seriousness of false sharing, which may report insignificant issues. They could not detect false sharing caused by external libraries.</p><p>Classifying Different Types of Cache misses. Some approaches could classify multiple types of cache misses together. Sanchez et al. propose a data locality analysis tool that can identify compulsory, conflict and capacity misses, but not coherence misses <ref type="bibr" target="#b45">[47]</ref>. Its profiling stage incurs reasonable overhead, but it requires a specialized compiler to extract reuse information beforehand and an expensive offline processing stage. These characteristics make this tool inconvenient and inefficient to use. DProf detects datatype-related cache performance issues inside the Linux kernel via the PMU-based sampling and tracing object access histories <ref type="bibr" target="#b41">[43]</ref>. DProf employs the definitions of cache misses for its classification, but with the following issues: first, it requires human effort and expertise to summarize data profile, miss classification, working set, and data flow together to identify a particular type of issue, which is not friendly to people without such expertise. Second, it may lose its precision due to its coarse-grained profiling, which is infeasible to find the last write of each miss (and then affect its report). Third, DProf requires the change of the monitored target (e.g., kernel), which may prevent people from using it. Fourth, DProf provides no mechanism of differentiating issues of applications from those of allocators. In contrast, CachePerf overcomes these issues by automatically identifying the type of cache misses, as discussed in Section 3.5. Another difference is that CachePerf requires no change of programs, as it is a library that can be linked to applications. Further, CachePerf could also identify cache misses caused by the allocator that DProf cannot do.</p><p>Other Relevant Work: DMon proposes selective profiling that could incrementally increase its monitoring scope (e.g., sampled events) based on the dynamic behavior of execution <ref type="bibr" target="#b23">[24]</ref>. In this sense, CachePerf is very similar to DMon. However, CachePerf selectively chooses the instructions to monitor (not hardware events) in order to collect fine-grained information. DMon relies on perf to collect sampled events, while CachePerf proposes a new profiler that classifies different types of cache misses based on a set of hardware events and hardware breakpoints. CachePerf could classify the type of cache misses, where DMon only reports the cache miss ratio at different lines of code, inheriting from perf, and relies on human expertise to diagnose the issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>Cache miss is a well-known performance issue. Although existing tools could report cache miss ratios at different lines of code, significant effort is still mandatory to figure out the type and the origin (e.g., object, allocator) of cache misses in order to reduce cache misses. This paper describes a unified profiling tool-CachePerf-that could correctly identify different types of cache misses while imposing reasonable overhead. This paper further proposes a new method that combines PMUbased coarse-grained sampling and breakpoint-based fine-grained sampling to balance the accuracy and performance. Overall, CachePerf only imposes 14% performance overhead, while identifying multiple known and new cache misses correctly. CachePerf is an indispensable complementary to existing profilers due to its uniqueness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An example with capacity misses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The conflict miss in SRAD, which can be fixed easily by switching the loops of line 241 and 242.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>-</head><label></label><figDesc>aos3.cpp: 239 #1 instruction at: -aos3.cpp: 240 ...... Related to multiple heap objects (8824176 bytes) allocated at: # 0 object is allocated at: -# 0: aos3.cpp: 275 # 1 object is allocated at: -# 0: aos3.cpp: 278 ...... 235: for (kk = kmin; kk &lt; kmax; kk++) { 236: for (jj = jmin; jj &lt; jmax; jj++) { 237: for (ii = imin; ii &lt; imax; ii++) { 238: i = ii + jj * jp + kk * kp; 239: b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Reported capacity miss in IRS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. CachePerf reports false sharing in linear_regression</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Allocator's falseFig. 9 .</head><label>9</label><figDesc>Fig. 9. CachePerf reports coherence misses in cache-scratch</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. The performance of CachePerf, CCProf, and Feather, where the results are normalized to the default setting (without running any tool).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 ,</head><label>1</label><figDesc>which is based on Intel's Xeon machine. To balance the</figDesc><table><row><cell cols="3">Configuration Load Sampling Store Sampling</cell></row><row><cell>type</cell><cell cols="2">PERF_TYPE_RAW</cell></row><row><cell>config</cell><cell>0x1cd</cell><cell>0x82d0</cell></row><row><cell cols="3">sample_period 20000 (?10%) 50000 (?10%)</cell></row><row><cell>freq</cell><cell>false</cell><cell></cell></row><row><cell></cell><cell cols="2">PERF_SAMPLE_IP |</cell></row><row><cell>sample_type</cell><cell cols="2">PERF_SAMPLE_ADDR |</cell></row><row><cell></cell><cell cols="2">PERF_SAMPLE_DATA_SRC</cell></row><row><cell>precise_ip</cell><cell>3</cell><cell>1</cell></row><row><cell>__reserved_1</cell><cell>3</cell><cell>0</cell></row><row><cell>config1</cell><cell>3</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Configuration of the PMU sampling</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Classifier</head><label></label><figDesc>Algorithm 1: The Algorithm of Classifying Cache Misses for cache line c in Miss Store do if multiple threads access the same words of c then Report true sharing end if multiple threads access different words of c then if c has multiple objects allocated by different threads then</figDesc><table><row><cell>Report allocator-induced false sharing</cell></row><row><cell>else</cell></row><row><cell>Report application's false sharing</cell></row><row><cell>end</cell></row><row><cell>end</cell></row><row><cell>end</cell></row><row><cell>for instruction i in Instruction Store do</cell></row><row><cell>if the issue is reported as coherency miss then</cell></row><row><cell>continue</cell></row><row><cell>end</cell></row><row><cell>if i's misses land on the same cache set then</cell></row><row><cell>if misses are landing on multiple heap objects then</cell></row><row><cell>Report allocator-induced conflict miss</cell></row><row><cell>else</cell></row><row><cell>Report application's conflict miss</cell></row><row><cell>end</cell></row><row><cell>else</cell></row><row><cell>Report application's capacity miss</cell></row><row><cell>end</cell></row><row><cell>end</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 ,</head><label>5</label><figDesc>CachePerf will miss 10 out of 19 issues. When the sampling frequencies are 10 times higher than the default setting, the performance and memory overhead are increased to 18% and 79% correspondingly, but do not report more issues. Overall, CachePerf's default sampling periods keep a good balance between performance and effectiveness.</figDesc><table><row><cell></cell><cell>L:200K, S: 500K</cell><cell>L:20K, S: 50K</cell><cell>L:20K, S: 50K</cell><cell>L:2K, S: 5K</cell><cell>L:20K, S: 50K</cell></row><row><cell></cell><cell>Miss Ratio: 0.5%</cell><cell>Miss Ratio: 2.5%</cell><cell>Miss Ratio: 0.5%</cell><cell>Miss Ratio: 0.5%</cell><cell>Miss Ratio: 0%</cell></row><row><cell cols="2">Performance 10%</cell><cell>12%</cell><cell>14%</cell><cell>18%</cell><cell>18%</cell></row><row><cell>Memory</cell><cell>47%</cell><cell>45%</cell><cell>48%</cell><cell>79%</cell><cell>70%</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We thank our <rs type="person">Shepherd Sergey Blagodurov</rs> and anonymous reviewers for their helpful comments on improving this paper. We also thank <rs type="person">Probir Roy</rs>, <rs type="person">Milind Chabbi</rs> for their help on the setup of CCProf and Feather for the comparison, and <rs type="person">Xu Liu</rs> for the initial discussions on hardware performance counters. This material is based upon work supported by the <rs type="funder">National Science Foundation</rs> under Award <rs type="grantNumber">CCF-2024253</rs>, and the UMass start-up package. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the <rs type="funder">National Science Foundation</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_HtQAs5Y">
					<idno type="grant-number">CCF-2024253</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We further checked the reason why Feather runs faster with cache-thrash and CCProf runs faster with linear_regression. Based on our investigation, Feather allocates some memory from the default memory allocator for its internal usage, which happens to alleviate the false sharing issue introduced by the allocator. Similarly, CCProf's memory usage also changes the starting address of the false sharing object, reducing the severity of false sharing. That is, they should impose higher overhead if such lucky cases are excluded. We also observe that CCProf's offline phase is very expensive, e.g., MKL_FFT, which could be as much as 945? higher. In contrast, CachePerf does not have the hidden overhead for the offline analysis, which could report cache misses immediately after the execution or when receiving the signal from users (good for long-running applications).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Memory Overhead</head><p>We also evaluated the memory overhead of CachePerf, CCProf, and Feather, as shown in Table 3. Since CCProf crashed for Kripke, and Feather encountered the deadlock for Kripke and linear_regression, these applications are marked "N/A" in the table.</p><p>In total, CachePerf adds around 11% memory consumption, although its average overhead is around 51%. When only considering the online stages of CCProf and Feather, CachePerf's memory overhead is significantly better than CCProf, but slightly worse than Feather. However, if the offline stage is also considered when using the maximum memory consumption of both stages, then CachePerf has the smallest memory consumption.</p><p>Table <ref type="table">3</ref> shows that CachePerf introduces high memory overhead for applications with small memory footprints, such as swaptions, cache-scratch, and cache-thrash. Based on our observation,</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Denis</forename><surname>Bakhvalov</surname></persName>
		</author>
		<ptr target="https://easyperf.net/blog/2018/06/08/Advanced-profiling-topics-PEBS-and-LBR" />
		<title level="m">Advanced profiling topics. pebs and lbr</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Defensive loop tiling for shared cache</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 IEEE/ACM International Symposium on Code Generation and Optimization (CGO), CGO &apos;13</title>
		<meeting>the 2013 IEEE/ACM International Symposium on Code Generation and Optimization (CGO), CGO &apos;13<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hoard: A scalable memory allocator for multithreaded applications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Emery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><forename type="middle">S</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">D</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">R</forename><surname>Blumofe</surname></persName>
		</author>
		<author>
			<persName><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS IX</title>
		<meeting>the Ninth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS IX</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The parsec benchmark suite: Characterization and architectural implications</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bienia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaswinder Pal</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th international conference on Parallel architectures and compilation techniques</title>
		<meeting>the 17th international conference on Parallel architectures and compilation techniques</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="72" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Data centric cache measurement on the Intel ltanium 2 processor</title>
		<author>
			<persName><forename type="first">Bryan</forename><forename type="middle">R</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">K</forename><surname>Hollingsworth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC &apos;04: Proc. of the 2004 ACM/IEEE Conf. on Supercomputing</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">58</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Featherlight on-the-fly false-sharing detection</title>
		<author>
			<persName><forename type="first">Milind</forename><surname>Chabbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shasha</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPoPP &apos;18</title>
		<meeting>the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPoPP &apos;18</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="152" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rodinia: A benchmark suite for heterogeneous computing</title>
		<author>
			<persName><forename type="first">Che</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayuan</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tarjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><forename type="middle">W</forename><surname>Sheaffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang-Ha</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Skadron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE International Symposium on Workload Characterization (IISWC)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="44" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<ptr target="https://www.intel.com/content/www/us/en/developer/tools/oneapi/vtune-profiler.html#gs.ddftte" />
		<title level="m">Intel vtune profiler</title>
		<imprint/>
		<respStmt>
			<orgName>Intel Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<ptr target="https://software.intel.com/content/www/us/en/develop/tools/oneapi/components/onemkl.html" />
		<title level="m">Intel math kernel library</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
		<respStmt>
			<orgName>Intel Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Stephane</forename><surname>Eranian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Gouriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tipp</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willem</forename><surname>De Bruijn</surname></persName>
		</author>
		<ptr target="https://perf.wiki.kernel.org/index.php/Tutorial" />
		<title level="m">Linux kernel profiling with perf</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Collective loop fusion for array contraction</title>
		<author>
			<persName><forename type="first">Guang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Olsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radhika</forename><surname>Thekkath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Languages and Compilers for Parallel Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="281" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Tcmalloc: Thread-caching malloc</title>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Menage</surname></persName>
		</author>
		<ptr target="http://goog-perftools.sourceforge.net/doc/tcmalloc.html" />
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Ryutaro</forename><surname>Himeno</surname></persName>
		</author>
		<ptr target="https://i.riken.jp/en/supercom/documents/himenobmt/" />
		<title level="m">Himeno benchmark</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
		<ptr target="https://intelxed.github.io/" />
	</analytic>
	<monogr>
		<title level="j">Intel. Intel xed</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<ptr target="https://software.intel.com/content/www/us/en/develop/articles/intel-sdm.html" />
		<title level="m">ntel? 64 and IA-32 Architectures Software Developer Manuals</title>
		<imprint/>
		<respStmt>
			<orgName>Intel Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<ptr target="https://software.intel.com/content/www/us/en/develop/articles/intel-sdm.html" />
		<title level="m">Intel 64 and IA-32 Architectures Software Developer&apos;s Manual</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Memory profiling using hardware counters</title>
		<author>
			<persName><forename type="first">Marty</forename><surname>Itzkowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">J N</forename><surname>Wylie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolai</forename><surname>Kosche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC &apos;03: Proc. of the 2003 ACM/IEEE Conf. on Supercomputing</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On modeling and analyzing cache hierarchies using casper</title>
		<author>
			<persName><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th IEEE/ACM International Symposium on Modeling, Analysis and Simulation of Computer Telecommunications Systems, 2003. MASCOTS 2003</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="182" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cmp$im: A pin-based on-the-fly multi-core cache simulator</title>
		<author>
			<persName><forename type="first">Aamer</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Keung</forename><surname>Robert S Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><surname>Jacob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Annual Workshop on Modeling, Benchmarking and Simulation (MoBS), co-located with ISCA</title>
		<meeting>the Fourth Annual Workshop on Modeling, Benchmarking and Simulation (MoBS), co-located with ISCA</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="28" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Detection of false sharing using machine learning</title>
		<author>
			<persName><forename type="first">Sanath</forename><surname>Jayasena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saman</forename><surname>Amarasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asanka</forename><surname>Abeyweera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gayashan</forename><surname>Amarasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Himeshi</forename><forename type="middle">De</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunimal</forename><surname>Rathnayake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanbin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC &apos;13: Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Reducing compulsory and capacity misses</title>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automated memory leak detection for production use</title>
		<author>
			<persName><forename type="first">Changhee</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Easwaran</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Software Engineering, ICSE 2014</title>
		<meeting>the 36th International Conference on Software Engineering, ICSE 2014<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="825" to="836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Lulesh programming model and performance ports overview</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Karlin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<pubPlace>Livermore, CA (United States</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Lawrence Livermore National Lab.(LLNL)</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dmon: Efficient detection and correction of data locality problems using selective profiling</title>
		<author>
			<persName><forename type="first">Tanvir</forename><surname>Ahmed Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Pokam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barzan</forename><surname>Mozafari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baris</forename><surname>Kasikci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th USENIX Symposium on Operating Systems Design and Implementation (OSDI 21)</title>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2021-07">July 2021</date>
			<biblScope unit="page" from="163" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Huron: Hybrid false sharing detection and repair</title>
		<author>
			<persName><forename type="first">Tanvir</forename><surname>Ahmed Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Pokam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barzan</forename><surname>Mozafari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baris</forename><surname>Kasikci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation</title>
		<meeting>the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="453" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Inefficient innodb row stats implementation</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kopytov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunny</forename><surname>Bains</surname></persName>
		</author>
		<ptr target="https://bugs.mysql.com/bug.php?id=79454" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Kripke-a massively parallel transport mini-app</title>
		<author>
			<persName><forename type="first">Teresa</forename><forename type="middle">S</forename><surname>Adam J Kunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">N</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><surname>Brown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<pubPlace>Livermore, CA (United States</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Lawrence Livermore National Lab.(LLNL)</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cache profiling and the spec benchmarks: A case study</title>
		<author>
			<persName><forename type="first">Alvin</forename><forename type="middle">R</forename><surname>Lebeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="15" to="26" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Performance analysis guide for intel core? i7 processor and intel xeon 5500 processors</title>
		<author>
			<persName><forename type="first">David</forename><surname>Levinthal</surname></persName>
		</author>
		<ptr target="https://software.intel.com/content/dam/develop/external/us/en/documents/performance-analysis-guide-181827.pdf" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Oprofile: A system profiler for linux</title>
		<author>
			<persName><forename type="first">John</forename><surname>Levon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Elie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sheriff: precise detection and automatic mitigation of false sharing</title>
		<author>
			<persName><forename type="first">Tongping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emery</forename><forename type="middle">D</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 ACM international conference on Object oriented programming systems languages and applications, OOPSLA &apos;11</title>
		<meeting>the 2011 ACM international conference on Object oriented programming systems languages and applications, OOPSLA &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cheetah: Detecting false sharing efficiently and effectively</title>
		<author>
			<persName><forename type="first">Tongping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 International Symposium on Code Generation and Optimization</title>
		<meeting>the 2016 International Symposium on Code Generation and Optimization<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Predator: Predictive false sharing detection</title>
		<author>
			<persName><forename type="first">Tongping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hu</forename><surname>Ziang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emery</forename><forename type="middle">D</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 19th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPOPP&apos;14</title>
		<meeting>19th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPOPP&apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Arraytool: A lightweight profiler to guide array regrouping</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mellor-Crummey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 23rd International Conference on Parallel Architecture and Compilation Techniques (PACT)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="405" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A data-centric profiler for parallel programs</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">M</forename><surname>Mellor-Crummey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2013 ACM/IEEE Conference on Supercomputing</title>
		<meeting>of the 2013 ACM/IEEE Conference on Supercomputing<address><addrLine>Denver, CO, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Arraytool: A lightweight profiler to guide array regrouping</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Mellor-Crummey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Parallel Architectures and Compilation, PACT &apos;14</title>
		<meeting>the 23rd International Conference on Parallel Architectures and Compilation, PACT &apos;14</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="405" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Multicachesim: A coherent multiprocessor cache simulator</title>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Lucia</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Laser: Light, accurate sharing detection and repair</title>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sriraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fugate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pokam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Newburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Devietti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="261" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Valgrind: A framework for heavyweight dynamic binary instrumentation</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Nethercote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Seward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI &apos;07</title>
		<meeting>the 28th ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI &apos;07</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="89" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Delorean: Virtualized Directed Profiling for Cache Modeling in Sampled Simulation</title>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Nikoleris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Hagersten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Delorean: Virtualized directed profiling for cache modeling in sampled simulation</title>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Nikoleris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Hagersten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Locating cache performance bottlenecks using data profiling</title>
		<author>
			<persName><forename type="first">Aleksey</forename><surname>Pesterev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nickolai</forename><surname>Zeldovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">T</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th European Conference on Computer Systems, EuroSys &apos;10</title>
		<meeting>the 5th European Conference on Computer Systems, EuroSys &apos;10</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="335" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">L.-N</forename><surname>Pouchet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yuki</surname></persName>
		</author>
		<ptr target="https://sourceforge.net/projects/polybench/" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Evaluating mapreduce for multi-core and multiprocessor systems</title>
		<author>
			<persName><forename type="first">Colby</forename><surname>Ranger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramanan</forename><surname>Raghuraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Penmetsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE 13th International Symposium on High Performance Computer Architecture</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Lightweight detection of cache conflicts</title>
		<author>
			<persName><forename type="first">Probir</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuaiwen</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriram</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 International Symposium on Code Generation and Optimization</title>
		<meeting>the 2018 International Symposium on Code Generation and Optimization<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="200" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Analyzing data locality in numeric applications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="58" to="66" />
			<date type="published" when="2000-08">August 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Addresssanitizer: A fast address sanity checker</title>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Serebryany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Bruening</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Vyukov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 USENIX Conference on Annual Technical Conference, USENIX ATC&apos;12</title>
		<meeting>the 2012 USENIX Conference on Annual Technical Conference, USENIX ATC&apos;12<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Cachegrind: a cache-miss profiler</title>
		<author>
			<persName><forename type="first">N</forename><surname>Seward</surname></persName>
		</author>
		<author>
			<persName><surname>Nethercote</surname></persName>
		</author>
		<author>
			<persName><surname>Fitzhardinge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Racez: A lightweight and non-invasive race detection tool for production applications</title>
		<author>
			<persName><forename type="first">Tianwei</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Vachharajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephane</forename><surname>Eranian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenguang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weimin</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Software Engineering, ICSE &apos;11</title>
		<meeting>the 33rd International Conference on Software Engineering, ICSE &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="401" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Detailed cache simulation for detecting bottleneck, miss reason and optimization potentialities</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Karl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st international conference on Performance evaluation methodolgies and tools</title>
		<meeting>the 1st international conference on Performance evaluation methodolgies and tools<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>page 62-es</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Detailed cache simulation for detecting bottleneck, miss reason and optimization potentialities</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Karl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st international conference on Performance evaluation methodolgies and tools -valuetools &apos;06</title>
		<meeting>the 1st international conference on Performance evaluation methodolgies and tools -valuetools &apos;06<address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">62</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Cache performance measurement and metric: Capacity misses</title>
		<author>
			<persName><forename type="first">Wikipedia</forename><surname>Team</surname></persName>
		</author>
		<ptr target="https://en.wikipedia.org/wiki/Cache_performance_measurement_and_metric#Capacity_misses" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Team. header only, dependency-free deep learning framework in c++14</title>
		<ptr target="https://github.com/tiny-dnn/tiny-dnn" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Loop optimization</title>
		<author>
			<persName><surname>Wikipedia</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Speckle reducing anisotropic diffusion</title>
		<author>
			<persName><forename type="first">T</forename><surname>Acton</surname></persName>
		</author>
		<author>
			<persName><surname>Yu Yj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1260" to="1270" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A mathematical cache miss analysis for pointer data structures</title>
		<author>
			<persName><forename type="first">Hongli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Martonosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM Conference on Parallel Processing for Scientific Computing</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Dynamic cache contention detection in multi-threaded applications</title>
		<author>
			<persName><forename type="first">Qin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Syed</forename><surname>Raza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Bruening</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weng-Fai</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saman</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments, VEE&apos;11</title>
		<meeting>the 7th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments, VEE&apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="27" to="38" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
