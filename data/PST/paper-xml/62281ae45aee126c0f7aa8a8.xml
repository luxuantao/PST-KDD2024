<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Incorporating Hierarchy into Text Encoder: a Contrastive Learning Approach for Hierarchical Text Classification</title>
				<funder ref="#_EpDkZad">
					<orgName type="full">PKU-Baidu Fund</orgName>
				</funder>
				<funder ref="#_rgRSaXa">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-08">8 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zihan</forename><surname>Wang</surname></persName>
							<email>wanghf@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>MOE</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peiyi</forename><surname>Wang</surname></persName>
							<email>wangpeiyi9979@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>MOE</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lianzhe</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>MOE</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>MOE</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>MOE</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Incorporating Hierarchy into Text Encoder: a Contrastive Learning Approach for Hierarchical Text Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-08">8 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2203.03825v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hierarchical text classification is a challenging subtask of multi-label classification due to its complex label hierarchy. Existing methods encode text and label hierarchy separately and mix their representations for classification, where the hierarchy remains unchanged for all input text. Instead of modeling them separately, in this work, we propose Hierarchyguided Contrastive Learning (HGCLR) to directly embed the hierarchy into a text encoder. During training, HGCLR constructs positive samples for input text under the guidance of the label hierarchy. By pulling together the input text and its positive sample, the text encoder can learn to generate the hierarchy-aware text representation independently. Therefore, after training, the HGCLR enhanced text encoder can dispense with the redundant hierarchy. Extensive experiments on three benchmark datasets verify the effectiveness of HGCLR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Hierarchical Text Classification (HTC) aims to categorize text into a set of labels that are organized in a structured hierarchy <ref type="bibr" target="#b24">(Silla and Freitas, 2011)</ref>. The taxonomic hierarchy is commonly modeled as a tree or a directed acyclic graph, in which each node is a label to be classified. As a subtask of multi-label classification, the key challenge of HTC is how to model the large-scale, imbalanced, and structured label hierarchy <ref type="bibr" target="#b18">(Mao et al., 2019)</ref>.</p><p>The existing methods of HTC have variously introduced hierarchical information. Among recent researches, the state-of-the-art models encode text and label hierarchy separately and aggregate two representations before being classified by a mixed feature <ref type="bibr" target="#b37">(Zhou et al., 2020;</ref><ref type="bibr" target="#b5">Deng et al., 2021)</ref>. As denoted in the left part of Figure <ref type="figure" target="#fig_0">1</ref>, their main goal is to sufficiently interact between text and structure to * Corresponding author.  achieve a mixed representation <ref type="bibr" target="#b3">(Chen et al., 2021)</ref>, which is highly useful for classification <ref type="bibr">(Chen et al., 2020a)</ref>. However, since the label hierarchy remains unchanged for all text inputs, the graph encoder provides exactly the same representation regardless of the input. Therefore, the text representation interacts with constant hierarchy representation and thus the interaction seems redundant and less effective. Alternatively, we attempt to inject the constant hierarchy representation into the text encoder. So that after being fully trained, a hierarchy-aware text representation can be acquired without the constant label feature. As in the right part of Figure <ref type="figure" target="#fig_0">1</ref>, instead of modeling text and labels separately, migrating label hierarchy into text encoding may benefit HTC by a proper representation learning method.</p><p>To this end, we adopt contrastive learning for the hierarchy-aware representation. Contrastive learning, which aims to concentrate positive samples and push apart negative samples, has been considered as effective in constructing meaningful representations <ref type="bibr" target="#b14">(Kim et al., 2021)</ref>. Previous work on contrastive learning illustrates that it is critical to building challenging samples <ref type="bibr" target="#b0">(Alzantot et al., 2018;</ref><ref type="bibr">Wang et al., 2021b;</ref><ref type="bibr" target="#b25">Tan et al., 2020;</ref><ref type="bibr" target="#b11">Wu et al., 2020)</ref>. For multi-label classification, we attempt to construct high-quality positive examples. Existing methods for positive example generation includes data augmentation <ref type="bibr" target="#b19">(Meng et al., 2021;</ref><ref type="bibr" target="#b11">Wu et al., 2020)</ref>, dropout <ref type="bibr" target="#b8">(Gao et al., 2021)</ref>, and adversarial attack <ref type="bibr">(Wang et al., 2021b;</ref><ref type="bibr">Pan et al., 2021)</ref>. These techniques are either unsupervised or taskunspecific: the generation of positive samples has no relation with the HTC task and thus are incompetent to acquire hierarchy-aware representations. As mentioned, we argue that both the ground-truth label as well as the taxonomic hierarchy should be considered for the HTC task.</p><p>To construct positive samples which are both label-guided and hierarchy-involved, our approach is motivated by a preliminary observation. Notice that when we classify text into a certain category, most words or tokens are not important. For instance, when a paragraph of a news report about a lately sports match is classified as "basketball", few keywords like "NBA" or "backboard" have large impacts while the game result has less influence. So, given a sequence and its labels, a shorten sequence that only keeps few keywords should maintain the labels. In fact, this idea is similar to adversarial attack, which aims to find "important tokens" which affect classification most <ref type="bibr">(Zhang et al., 2020)</ref>. The difference is that adversarial attack tries to modify "important tokens" to fool the model, whereas our approach modifies "unimportant tokens" to keep the classification result unchanged.</p><p>Under such observation, we construct positive samples as pairs of input sequences and theirs shorten counterparts, and propose Hierarchy-Guided Contrastive Learning (HGCLR) for HTC. In order to locate keywords under given labels, we directly calculate the attention weight of each token embedding on each label, and tokens with weight above a threshold are considered important to according label. We use a graph encoder to encode label hierarchy and output label features. Unlike previous studies with GCN or GAT, we modify a Graphormer <ref type="bibr" target="#b33">(Ying et al., 2021)</ref> as our graph encoder. Graphormer encodes graphs by Transformer blocks and outperforms other graph encoders on several graph-related tasks. It models the graph from multiple dimensions, which can be customized easily for HTC task.</p><p>The main contribution of our work can be summarized as follows:</p><p>? We propose Hierarchy-Guided Contrastive Learning (HGCLR) to obtain hierarchy-aware text representation for HTC. To our knowledge, this is the first work that adopts contrastive learning on HTC.</p><p>? For contrastive learning, we construct positive samples by a novel approach guided by label hierarchy. The model employs a modified Graphormer, which is a new state-of-the-art graph encoder.</p><p>? Experiments demonstrate that the proposed model achieves improvements on three datasets. Our code is available at https://github.com/wzh9969/contrastive-htc.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Hierarchical Text Classification</head><p>Existing work for HTC could be categorized into local and global approaches based on their ways of treating the label hierarchy <ref type="bibr" target="#b37">(Zhou et al., 2020)</ref> The early global approaches neglect the hierarchical structure of labels and view the problem as a flat multi-label classification <ref type="bibr" target="#b13">(Johnson and Zhang, 2015)</ref>. Later on, some work tries to coalesce the label structure by recursive regularization <ref type="bibr" target="#b10">(Gopal and Yang, 2013)</ref>, reinforcement learning <ref type="bibr" target="#b18">(Mao et al., 2019)</ref>, capsule network <ref type="bibr" target="#b21">(Peng et al., 2019)</ref>, and meta-learning <ref type="bibr" target="#b31">(Wu et al., 2019)</ref>. Although such methods can capture the hierarchical information, recent researches demonstrate that encoding the holistic label structure directly by a structure encoder can further improve performance. <ref type="bibr" target="#b37">Zhou et al. (2020)</ref> designs a structure encoder that integrates the label prior hierarchy knowledge to learn label representations. <ref type="bibr">Chen et al. (2020a)</ref> embeds word and label hierarchies jointly in the hyperbolic space. <ref type="bibr" target="#b35">Zhang et al. (2021)</ref> extracts text features according to different hierarchy levels. <ref type="bibr" target="#b5">Deng et al. (2021)</ref> introduces information maximization to constrain label representation learning. <ref type="bibr" target="#b36">Zhao et al. (2021)</ref> designs a self-adaption fusion strategy to extract features from text and label. <ref type="bibr" target="#b3">Chen et al. (2021)</ref> views the problem as semantic matching and tries BERT as text encoder. <ref type="bibr">Wang et al. (2021a)</ref> proposes a cognitive structure learning model for HTC. Similar to other work, they model text and label separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Contrastive Learning</head><p>Contrastive learning is originally proposed in Computer Vision (CV) as a weak-supervised representation learning method. Works such as MoCo <ref type="bibr" target="#b11">(He et al., 2020)</ref> and SimCLR <ref type="bibr">(Chen et al., 2020b)</ref> have bridged the gap between self-supervised learning and supervised learning on multiple CV datasets.</p><p>A key component for applying contrastive learning on NLP is how to build positive pairs <ref type="bibr">(Pan et al., 2021)</ref>. Data augmentation techniques such as back-translation <ref type="bibr" target="#b7">(Fang et al., 2020)</ref>, word or span permutation <ref type="bibr" target="#b11">(Wu et al., 2020)</ref>, and random masking <ref type="bibr" target="#b19">(Meng et al., 2021)</ref> can generate pair of data with similar meanings. <ref type="bibr" target="#b8">Gao et al. (2021)</ref> uses different dropout masks on the same data to generate positive pairs. <ref type="bibr" target="#b14">Kim et al. (2021)</ref> utilizes BERT representation by a fixed copy of BERT. These methods do not rely on downstream tasks while some researchers leverage supervised information for better performance on text classification. <ref type="bibr">Wang et al. (2021b)</ref> constructs both positive and negative pairs especially for sentimental classification by word replacement. <ref type="bibr">Pan et al. (2021)</ref> proposes to regularize Transformer-based encoders for text classification tasks by FGSM <ref type="bibr" target="#b9">(Goodfellow et al., 2014)</ref>, an adversarial attack method based on gradient. Though methods above are designed for classification, the construction of positive samples hardly relies on their categories, neglecting the connection and diversity between different labels. For HTC, the taxonomic hierarchy models the relation between labels, which we believe can help positive sample generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Definition</head><p>Given a input text x = {x 1 , x 2 , ..., x n }, Hierarchical Text Classification (HTC) aims to predict a subset y of label set Y , where n is the length of the input sequence and k is the size of set Y . The candidate labels y i ? Y are predefined and organized as a Directed Acyclic Graph (DAG) G = (Y, E), where node set Y are labels and edge set E denotes their hierarchy. For simplicity, we do not distinguish a label with its node in the hierarchy so that y i is both a label and a node. Since a non-root label of HTC has one and only one father, the taxonomic hierarchy can be converted to a tree-like hierarchy. The subset y corresponds to one or more paths in G: for any non-root label y j ? y, a father node (label) of y j is in the subset y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>In this section, we will describe the proposed HG-CLR in detail. Figure <ref type="figure">2</ref> shows the overall architecture of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Text Encoder</head><p>Our approach needs a strong text encoder for hierarchy injection, so we choose BERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> as the text encoder. Given an input token sequence:</p><formula xml:id="formula_0">x = {[CLS], x 1 , x 2 , ..., x n-2 , [SEP]} (1)</formula><p>where [CLS] and [SEP] are two special tokens indicating the beginning and the end of the sequence, the input is fed into BERT. For convenience, we denote the length of the sequence as n. The text encoder outputs hidden representation for each token:</p><formula xml:id="formula_1">H = BERT(x)<label>(2)</label></formula><p>where H ? R n?d h and d h is the hidden size. We use the hidden state of the first token ([CLS]) for representing the whole sequence</p><formula xml:id="formula_2">h x = h [CLS] .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Graph Encoder</head><p>We model the label hierarchy with a customized Graphormer <ref type="bibr" target="#b33">(Ying et al., 2021)</ref>. Graphormer models graphs on the base of Transformer layer <ref type="bibr" target="#b26">(Vaswani et al., 2017)</ref> with spatial encoding and edge encoding, so it can leverage the most powerful sequential modeling network in the graph domain.</p><p>We organize the original feature for node y i as the sum of label embedding and its name embedding:</p><formula xml:id="formula_3">f i = label_emb(y i ) + name_emb(y i ). (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>Label embedding is a learnable embedding that takes a label as input and outputs a vector with size d h . Name embedding takes the advantage of the name of the label, which we believe contains fruitful information as a summary of the entire class. We use the average of BERT token embedding of the label as its name embedding, which also has a size of d h . Unlike previous work which only adopts names on initialization, we share embedding weights across text and labels to make label features more instructive. With all node features stack as a matrix F ? R k?d h , a standard self-attention layer can then be used for feature migration.</p><p>To leverage the structural information, spatial encoding and edge encoding modify the Query-Key product matrix A G in the self-attention layer:</p><formula xml:id="formula_5">A G ij = (f i W G Q )(f j W G K ) T ? d h + c ij + b ?(y i ,y j ) (4)</formula><p>where</p><formula xml:id="formula_6">c ij = 1 D D n=1</formula><p>w en and D = ?(y i , y j ). The first term in Equation 4 is the standard scaledot attention, and query and key are projected by</p><formula xml:id="formula_7">W G Q ? R d h ?d h and W G K ? R d h ?d h . c ij</formula><p>is the edge encoding and ?(y i , y j ) denotes the distance between two nodes y i and y j . Since the graph is a tree in our problem, for node y i and y j , one and only one path (e 1 , e 2 , ..., e D ) can be found between them in the underlying graph G so that c ij denotes the edge information between two nodes and w e i ? R 1 is a learnable weight for each edge. b ?(y i ,y j ) is the spatial encoding, which measures the connectivity between two nodes. It is a learnable scalar indexed by ?(y i , y j ).</p><p>The graph-involved attention weight matrix A G is then followed by Softmax, multiplying with value matrix and residual connection &amp; layer nor-malization to calculate the self-attention,</p><formula xml:id="formula_8">L = LayerNorm(softmax(A G )V + F) (5)</formula><p>We use L as the label feature for the next step. The Graphormer we use is a variant of the selfattention layer, for more details on the full structure of Graphormer, please refer to the original paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Positive Sample Generation</head><p>As mentioned, the goal for the positive sample generation is to keep a fraction of tokens while retaining the labels. Given a token sequence as Equation <ref type="formula">1</ref>, the token embedding of BERT is defined as:</p><formula xml:id="formula_9">{e 1 , e 2 , ..., e n } = BERT_emb(x)<label>(6)</label></formula><p>The scale-dot attention weight between token embedding and label feature is first calculated to determine the importance of a token on a label,</p><formula xml:id="formula_10">q i = e i W Q , k j = l j W K , A ij = q i k T j ? d h<label>(7)</label></formula><p>The query and key are token embeddings and label features respectively, and W Q ? R d h ?d h and W K ? R d h ?d h are two weight matrices. Thus, for a certain x i , its probability of belonging to label y j can be normalized by a Softmax function.</p><p>Next, given a label y j , we can sample key tokens from that distribution and form a positive sample x. To make the sampling differentiable, we replace the Softmax function with Gumbel-Softmax <ref type="bibr" target="#b12">(Jang et al., 2016)</ref> to simulate the sampling operation:</p><formula xml:id="formula_11">P ij = gumbel_softmax(A i1 , A i2 , ..., A ik ) j (8)</formula><p>Notice that a token can impact more than label, so we do not discretize as one-hot vectors in this step. we keep tokens for positive examples if their probabilities of being sampled exceed a certain threshold ?, which can also control the fraction of tokens to be retrained. For multi-label classification, we simply add the probabilities of all ground-truth labels and obtain the probability of a token x i regarding its groundtruth label set y as:</p><formula xml:id="formula_12">P i = j?y P ij (9)</formula><p>Finally, the positive sample x is constructed as:</p><formula xml:id="formula_13">x = {x i if P i &gt; ? else 0} (<label>10</label></formula><formula xml:id="formula_14">)</formula><p>where 0 is a special token that has an embedding of all zeros so that key tokens can keep their positions. The select operation is not differentiable, so we implement it differently to make sure the whole model can be trained end-to-end. Details are illustrated in Appendix A.</p><p>The positive sample is fed to the same BERT as the original one,</p><formula xml:id="formula_15">? = BERT(x)<label>(11)</label></formula><p>and get a sequence representation ?x with the first token before being classified. We assume the positive sample should retain the labels, so we use classification loss of the positive sample as a guidance of the graph encoder and the positive sample generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Contrastive Learning Module</head><p>Intuitively, given a pair of token sequences and their positive counterpart, their encoded sentence-level representation should be as similar to each other as possible. Meanwhile, examples not from the same pair should be farther away in the representation space.</p><p>Concretely, with a batch of N hidden state of positive pairs (h i , ?i ), we add a non-linear layer on top of them:</p><formula xml:id="formula_16">c i = W 2 ReLU(W 1 h i ) ?i = W 2 ReLU(W 1 ?i )<label>(12)</label></formula><p>where </p><formula xml:id="formula_17">W 1 ? R d h ?d h , W 2 ? R d h ?d h .</formula><formula xml:id="formula_18">L con m = -log exp(sim(z m , ?(z m ))/? ) 2N i=1,i =m exp(sim(z m , z i )/? ) (<label>13</label></formula><p>) where sim is the cosine similarity function as sim(u, v) = u ? v/ u v and ? is a matching function as:</p><formula xml:id="formula_19">?(z m ) = c i , if z m = ?i ?i , if z m = c i (14)</formula><p>? is a temperature hyperparameter. The total contrastive loss is the mean loss of all examples:</p><formula xml:id="formula_20">L con = 1 2N 2N L con m (15)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Classification and Objective Function</head><p>Following previous work <ref type="bibr" target="#b37">(Zhou et al., 2020)</ref>, we flatten the hierarchy for multi-label classification. The hidden feature is fed into a linear layer, and a sigmoid function is used for calculating the probability. The probability of text i on label j is:</p><formula xml:id="formula_21">p ij = sigmoid(W c h i + b c ) j (<label>16</label></formula><formula xml:id="formula_22">)</formula><p>where W C ? R k?d h and b c ? R k are weights and bias. For multi-label classification, we use a binary cross-entropy loss function,</p><formula xml:id="formula_23">L C = - N i=1 k j=1 [y ij log(p ij )+(1-y ij ) log(1-p ij )]</formula><p>(17) where y ij is the ground truth. The classification loss of the constructed positive examples LC can be calculated similarly by Equation <ref type="formula" target="#formula_21">16</ref>and Equation 17 with ?i substituting for h i .</p><p>The final loss function is the combination of classification loss of original data, classification loss of the constructed positive samples, and the contrastive learning loss:</p><formula xml:id="formula_24">L = L C + LC + ?L con (18)</formula><p>where ? is a hyperparameter controlling the weight of contrastive loss.</p><p>During testing, we only use the text encoder for classification and the model degenerates to a BERT encoder with a classification head. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Setup</head><p>Datasets and Evaluation Metrics We experiment on Web-of-Science (WOS) <ref type="bibr" target="#b15">(Kowsari et al., 2017)</ref>, NYTimes (NYT) <ref type="bibr" target="#b22">(Sandhaus, 2008)</ref>, and RCV1-V2 <ref type="bibr" target="#b17">(Lewis et al., 2004)</ref> datasets for comparison and analysis. WOS contains abstracts of published papers from Web of Science while NYT and RCV1-V2 are both news categorization corpora. We follow the data processing of previous work <ref type="bibr" target="#b37">(Zhou et al., 2020)</ref>. WOS is for single-path HTC while NYT and RCV1-V2 include multi-path taxonomic labels. The statistic details are illustrated in Table <ref type="table" target="#tab_4">1</ref>. Similar to previous work, We measure the experimental results with Macro-F1 and Micro-F1.</p><p>Implement Details For text encoder, we use bert-base-uncased from Transformers <ref type="bibr" target="#b30">(Wolf et al., 2020)</ref> as the base architecture. Notice that we denote the attention layer in Eq. 4 and Eq. 7 as single-head attentions but they can be extended to multi-head attentions as the original Transformer block. Following BERT architecture, we set the attention head to 12 and feature size d h to 768. The batch size is set to 12. The optimizer is Adam with a learning rate of 3e -5. We implement our model in PyTorch and train end-to-end. We train the model with train set and evaluate on development set after every epoch, and stop training if the Macro-F1 does not increase for 6 epochs. The threshold ? is set to 0.02 on WOS and 0.005 on NYT and RCV1-V2. The loss weight ? is set to 0.1 on WOS and RCV1-V2 and 0.3 on NYT. We search ? and ? by grid search: ? is selected among {0.005, 0.01, 0.02, 0.05} and ? is selected among {0.03, 0.1, 0.3, 1}. The temperature of contrastive module is fixed to 1 since we have achieved promising results with this default setting in preliminary experiments.</p><p>Baselines We select a few recent work as baselines. HiAGM <ref type="bibr" target="#b37">(Zhou et al., 2020)</ref>, HTCInfoMax <ref type="bibr" target="#b5">(Deng et al., 2021)</ref>, and HiMatch <ref type="bibr" target="#b3">(Chen et al., 2021)</ref> are a branch of work that propose fusion strategies for mixed text-hierarchy representation. HiAGM applies soft attention over text feature and label feature for the mixed feature. HTCInfoMax improves HiAGM by regularizing the label representation with a prior distribution. HiMatch matches text representation with label representation in a joint embedding space and uses joint representation for classification. HiMatch is the state-of-the-art before our work. All approaches except HiMatch adopt TextRCNN <ref type="bibr" target="#b16">(Lai et al., 2015)</ref> as text encoder so that we implement them with BERT for a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Results</head><p>Main results are shown in Table <ref type="table" target="#tab_5">2</ref>. Instead of modeling text and labels separately, our model can make more use of the strong text encoder by migrating hierarchy information directly into BERT encoder. On WOS, the proposed HGCLR can achieve 1.5% and 2.1% improvement on Micro-F1 and Macro-F1 respectively comparing to BERT and is better than HiMatch even if its base model has far better performance.</p><p>BERT was trained on news corpus so that the base model already has decent performance on NYT and RCV1-V2, outperforming post-pretrain models by a large amount. On NYT, our approach observes a 2.3% boost on Macro-F1 comparing to BERT while sightly increases on Micro-F1 and outperform previous methods on both measurements.</p><p>On RCV1-V2, all baselines hardly improve Micro-F1 and only influence Macro-F1 comparing to BERT. HTCInfoMax experiences a decrease because its constraint on text representation may contradict with BERT on this dataset. HiMatch behaves extremely well on RCV1-V2 with Macro-F1 as measurement while our approach achieves state-of-the-art on Micro-F1. Besides the potential implement difference on BERT encoder, RCV1-V2 dataset provides no label name, which invalids our name embedding for label representation. Baselines like HiAGM and HiMatch only initialize labl embedding with their names so that this flaw has less impact. We will discuss more on name embedding in next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis</head><p>The main differences between our work and previous ones are the graph encoder and contrastive learning. To illustrate the effectiveness of these We remove the contrastive loss by setting ? = 0.</p><p>two parts, we test our model with them replaced or removed. We report the results on the development set of WOS for illustration. We first replace Graphormer with GCN and GAT (r.p. GCN and r.p. GAT), results are in Table <ref type="table" target="#tab_6">3</ref>. We find that Graphormer outperforms both graph encoders on this task. GAT also involves the attention mechanism but a node can only attend to its neighbors. Graphormer adopts global attention where each node can attend to all others in the graph, which is proven empirically more effective on this task. When the graph encoder is removed entirely (-r.m. graph encoder), the results drop significantly, showing the necessity of incorporating graph encoder for HTC task. The model without contrastive loss is similar to a pure data augmentation approach, where positive examples stand as augment data. As the last row of Table <ref type="table" target="#tab_6">3</ref>, on development set, both the positive pair generation strategy and the contrastive learning framework have contributions to the model. Our data generation strategy is effective even without contrastive learning, improving BERT encoder by around 1% on two measurements. Contrastive learning can further boost performance by regularizing text representation.</p><p>We further analyze the effect of incorporating label hierarchy, the Graphormer, and the positive samples generation strategy in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Effect of Hierarchy</head><p>Our approach attempts to incorporating hierarchy into the text representation, which is fed into a linear layer for probabilities as in Equation <ref type="formula" target="#formula_21">16</ref>. The weight matrix W C can be viewed as label representations and we plot theirs T-SNE projections under default configuration. Since a label and its father should be classified simultaneously, the represen- tation of a label and its father should be similar. Thus, if the hierarchy is injected into the text representation, labels with the same father should have more similar representation to each other than those with a different father. As illustrated in Figure <ref type="figure" target="#fig_1">3</ref>, label representations of BERT are scattered while label representations of our approach are clustered, which demonstrates that our text encoder can learn a hierarchy-aware representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Effect of Graphormer</head><p>As for the components of the Graphormer, we validate the utility of name embedding, spatial encoding, and edge encoding. As in  We omit a few unrelated tokens (such as a, the, or comma) for clarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Effect of Positive Example Generation</head><p>To further illustrate the effect of our data generation approach, we compare it with a few generation strategies. Dropout <ref type="bibr" target="#b8">(Gao et al., 2021)</ref> uses no positive sample generation techniques but contrasts on the randomness of the Dropout function using two identical models. Random masking <ref type="bibr" target="#b19">(Meng et al., 2021)</ref> is similar to our approach except the remained tokens are randomly selected. Adversarial attack <ref type="bibr">(Pan et al., 2021)</ref> generates positive examples by an attack on gradients. As in Table <ref type="table" target="#tab_7">5</ref>, a duplication of the model as positive examples is effective but performs poorly. Instead of dropping information at neuron level, random masking drops entire tokens and boosts Macro-F1 by over 1%, indicating the necessity of building hard enough contrastive examples. The adversarial attack can build hard-enough samples by gradient ascending and disturbance in the embedding space. But the disturbance is not regularized by hierarchy or labels so that it is less effective since there is no guarantee that the adversarial examples remain the label. Our approach guided the example construction by both the hierarchy and the labels, which accommodates with HTC most and achieves the best performance.</p><p>In Figure <ref type="figure" target="#fig_2">4</ref>, we select two cases to further illustrate the effect of labels on positive samples generation. In the first case, word machine strongly indicates this passage belongs to Machine Learning so that it is kept for positive examples. In the second case, syndrome is related to Medical and PRES occurs several times among Headache. Because of the randomness of sampling, our approach cannot construct an example with all keywords. instance, learning in case one headache in case two is omitted trial, which adds more difficulties for examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we present Hierarchy-guided Contrastive Learning (HGCLR) for hierarchy text classification. We adopt contrastive learning for migrating taxonomy hierarchy information into BERT encoding. To this end, we construct positive examples for contrastive learning under the guidance of a graph encoder, which learns label features from taxonomy hierarchy. We modify Graphormer, a state-of-the-art graph encoder, for better graph understanding. Comparing to previous approaches, our approach empirically achieves consistent improvements on two distinct datasets and comparable results on another one. All of the components we designed are proven to be effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Trick for Token Selection</head><p>To make sure P i in Equation 10 can acquire gradients, we choose to modify token embedding instead of the token itself. As in Equation <ref type="formula" target="#formula_9">6</ref>, e i is the token embedding of x i and can have gradient. The positive counterpart of e i is denoted as: ?i = e i ((P i + Detach(1 -P i )) if P i &gt; ? else 0), (19) where Detach is a function that ignores the gradient of its input. Numerically, ?i is either e i or 0 depending on the threshold ?, which serves the same purpose as Equation <ref type="formula" target="#formula_13">10</ref>. As for gradient,</p><formula xml:id="formula_25">? ?i ?P i = e i if P i &gt; ? else 0,<label>(20)</label></formula><p>which makes P i can be updated by backpropagation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Two ways of introducing hierarchy information. (a) Previous work model text and labels separately and find a mixed representation. (b) Our method incorporating hierarchy information into text encoder for a hierarchy-aware text representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3</head><label>3</label><figDesc>Figure 3: T-SNE visualization of the label representations on WOS dataset. Dots with same color are labels with a same father. (a) BERT model. (b) Our approach.</figDesc><graphic url="image-1.png" coords="7,289.60,332.58,124.35,93.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Two fragments of the generated positive examples. Tokens in red are kept for positive examples.We omit a few unrelated tokens (such as a, the, or comma) for clarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>An overview of HGCLR under a batch of 3. HGCLR adopts a contrastive learning framework to regularize BERT representations. We construct positive samples by masking unimportant tokens under the guidance of hierarchy and labels. By pulling together and pushing apart representations, the hierarchy information can be injected into the BERT encoder.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Taxonomic hierarchy</cell></row><row><cell>[News, Sports], [News, Music], [Travel]</cell><cell>[News, Sports], [News, Music], [Travel]</cell><cell></cell><cell></cell></row><row><cell cols="2">Multi-label Classifier</cell><cell>Graph Encoder</cell><cell></cell></row><row><cell>Push apart</cell><cell>Pull together</cell><cell>...</cell><cell>Label representations</cell></row><row><cell>Text representations</cell><cell></cell><cell>...</cell><cell></cell></row><row><cell></cell><cell></cell><cell>...</cell><cell></cell></row><row><cell cols="2">BERT Encoder</cell><cell>...</cell><cell></cell></row><row><cell></cell><cell></cell><cell>...</cell><cell></cell></row><row><cell></cell><cell></cell><cell>...</cell><cell></cell></row><row><cell>Input tokens</cell><cell>Positive samples</cell><cell>...</cell><cell></cell></row><row><cell cols="2">Positive sample generation</cell><cell>Select &amp; Sum</cell><cell></cell></row><row><cell>Figure 2:</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Thus, for a batch of 2N examples Z = {z ? {c i } ? { ?i }}, we compute the NT-Xent loss(Chen et al., 2020b)  for z m as:</figDesc><table /><note><p>For each example, there are 2(N -1) negative pairs, i.e., all the remaining examples in the batch are negative examples.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Data Statistics. |Y | is the number of classes. Depth is the maximum level of hierarchy. Avg(|y i |) is the average number of classes per sample.</figDesc><table><row><cell>Dataset</cell><cell cols="4">|Y | Depth Avg(|y i |) Train</cell><cell>Dev</cell><cell>Test</cell></row><row><cell>WOS</cell><cell>141</cell><cell>2</cell><cell>2.0</cell><cell></cell><cell>9,397</cell></row><row><cell>NYT</cell><cell>166</cell><cell>8</cell><cell>7.6</cell><cell>23,345</cell></row><row><cell cols="2">RCV1-V2 103</cell><cell>4</cell><cell>3.24</cell><cell cols="2">20,833 2,316 781,265</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Experimental results of our proposed model on several datasets. For a fair comparison, we implement some baseline with BERT encoder. We cannot reproduce the BERT results reported in Chen et al. (2021) so that we also report the results of our version of BERT.</figDesc><table><row><cell>Model</cell><cell></cell><cell>WOS</cell><cell></cell><cell>NYT</cell><cell></cell><cell cols="2">RCV1-V2</cell></row><row><cell></cell><cell></cell><cell cols="6">Micro-F1 Macro-F1 Micro-F1 Micro-F1 Macro-F1</cell></row><row><cell></cell><cell></cell><cell cols="3">Hierarchy-Aware Models</cell><cell></cell><cell></cell></row><row><cell cols="2">TextRCNN (Zhou et 2020)</cell><cell>83.55</cell><cell>76.99</cell><cell>70.83</cell><cell>56.18</cell><cell>81.57</cell></row><row><cell cols="2">(Zhou et al., 2020)</cell><cell>85.82</cell><cell>80.28</cell><cell></cell><cell>60.83</cell><cell>83.96</cell><cell>63.35</cell></row><row><cell cols="2">HTCInfoMax (Deng et al., 2021)</cell><cell>85.58</cell><cell>80.05</cell><cell>-</cell><cell>-</cell><cell>83.51</cell><cell>62.71</cell></row><row><cell cols="2">HiMatch (Chen et al., 2021)</cell><cell>86.20</cell><cell>80.53</cell><cell>-</cell><cell>-</cell><cell>84.73</cell><cell>64.11</cell></row><row><cell></cell><cell></cell><cell cols="3">Pretrained Language Models</cell><cell></cell><cell></cell></row><row><cell cols="2">BERT (Our implement)</cell><cell>85.63</cell><cell>79.07</cell><cell>78.24</cell><cell>65.62</cell><cell>85.65</cell><cell>67.02</cell></row><row><cell cols="2">BERT (Chen et al., 2021)</cell><cell>86.26</cell><cell>80.58</cell><cell>-</cell><cell>-</cell><cell>86.26</cell><cell>67.35</cell></row><row><cell cols="2">BERT+HiAGM (Our implement)</cell><cell>86.04</cell><cell>80.19</cell><cell>78.64</cell><cell>66.76</cell><cell>85.58</cell><cell>67.93</cell></row><row><cell cols="2">BERT+HTCInfoMax (Our implement)</cell><cell>86.30</cell><cell>79.97</cell><cell>78.75</cell><cell>67.31</cell><cell>85.53</cell><cell>67.09</cell></row><row><cell cols="2">BERT+HiMatch (Chen et al., 2021)</cell><cell>86.70</cell><cell>81.06</cell><cell>-</cell><cell>-</cell><cell>86.33</cell><cell>68.66</cell></row><row><cell>HGCLR</cell><cell></cell><cell>87.11</cell><cell>81.20</cell><cell>78.86</cell><cell>67.96</cell><cell>86.49</cell><cell>68.31</cell></row><row><cell>Ablation Models</cell><cell cols="2">Micro-F1 Macro-F1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERT</cell><cell>85.75</cell><cell>79.36</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HGCLR</cell><cell>87.46</cell><cell>81.52</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>-r.p. GCN</cell><cell>87.06</cell><cell>80.63</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>-r.p. GAT</cell><cell>87.18</cell><cell>81.45</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>-r.m. graph encoder</cell><cell>86.67</cell><cell>80.11</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>-r.m. contrastive loss</cell><cell>86.72</cell><cell>80.97</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Performance when replace or remove some components of HGCLR on the development set of WOS. r.p. stands for replace and r.m. stands for remove.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Impact of different positive example generation techniques on the development set of WOS. Hierarchy-guided is the proposed method. We control the valid tokens in positive samples roughly the same for random methods. We select FGSM as the attack algorithm followingPan et al. (2021).</figDesc><table><row><cell cols="3">Variants of Graphormer Micro-F1 Macro-F1</cell></row><row><cell>Base architecture</cell><cell>87.46</cell><cell></cell></row><row><cell>-w/o name embedding</cell><cell>86.40</cell><cell>80.40</cell></row><row><cell>-w/o spatial</cell><cell></cell><cell>80.42</cell></row><row><cell>-w/o edge encoding</cell><cell>87.25</cell><cell>80.54</cell></row><row><cell cols="3">Table Performance with variants of Graphormer on</cell></row><row><cell cols="3">development set of WOS. We remove name embed-</cell></row><row><cell cols="3">ding, spatial encoding, and edge encoding respectively.</cell></row><row><cell>"w/o" stands for "without".</cell><cell></cell><cell></cell></row><row><cell cols="3">Generation Strategy Micro-F1 Macro-F1</cell></row><row><cell>Hierarchy-guided</cell><cell>87.46</cell><cell>81.52</cell></row><row><cell>Dropout</cell><cell>86.94</cell><cell>79.91</cell></row><row><cell>Random masking</cell><cell>87.19</cell><cell>81.16</cell></row><row><cell>Adversarial attack</cell><cell>86.67</cell><cell>80.24</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>, all three</cell></row><row><cell>components contribute to embedding the graph.</cell></row><row><cell>Edge encoding is the least useful among these three</cell></row><row><cell>components. Edge encoding is supposed to model</cell></row><row><cell>the edge features provided by the graph, but the</cell></row><row><cell>hierarchy of HTC has no such information so that</cell></row><row><cell>the effect of edge encoding is not fully embodied</cell></row><row><cell>in this task. Name embedding contributes most</cell></row><row><cell>among components. Previous work only initialize</cell></row><row><cell>embedding weights with label name but we treat it</cell></row><row><cell>as a part of input features. As a result, neglecting</cell></row><row><cell>name embedding observes the largest drop, which</cell></row><row><cell>may explain the poor performance on RCV1-V2.</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank all the anonymous reviewers for their constructive feedback. The work is supported by <rs type="funder">National Natural Science Foundation of China</rs> under Grant No.<rs type="grantNumber">62036001</rs> and <rs type="funder">PKU-Baidu Fund</rs> (No. <rs type="grantNumber">2020BD021</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_rgRSaXa">
					<idno type="grant-number">62036001</idno>
				</org>
				<org type="funding" xml:id="_EpDkZad">
					<idno type="grant-number">2020BD021</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generating natural language adversarial examples</title>
		<author>
			<persName><forename type="first">Moustafa</forename><surname>Alzantot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yash</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Elgohary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo-Jhang</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mani</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1316</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2890" to="2896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hierarchical transfer learning for multi-label text classification</title>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cem</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Perez-Sorrosal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kostas</forename><surname>Tsioutsiouliklis</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1633</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6295" to="6300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hyperbolic interaction model for hierarchical multi-label classification</title>
		<author>
			<persName><forename type="first">Boli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixin</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liping</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="7496" to="7503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchy-aware label semantics matching network for hierarchical text classification</title>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianli</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenxi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangyue</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.337</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4370" to="4379" />
		</imprint>
	</monogr>
	<note>Long Papers). Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">HTCInfoMax: A global model for hierarchical text classification via information maximization</title>
		<author>
			<persName><forename type="first">Zhongfen</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxiao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.260</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3259" to="3265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Cert: Contrastive self-supervised learning for language understanding</title>
		<author>
			<persName><forename type="first">Hongchao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayuan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12766</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingcheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08821</idno>
		<title level="m">Simcse: Simple contrastive learning of sentence embeddings</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recursive regularization for large-scale classification with hierarchical and graphical dependencies</title>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1145/2487575.2487644</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="257" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<title level="m">Categorical reparameterization with gumbel-softmax</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Effective use of word order for text categorization with convolutional neural networks</title>
		<author>
			<persName><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/N15-1011</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 of the North American Chapter the for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 of the North American Chapter the for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Denver, Colorado. Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-guided contrastive learning for BERT sentence representations</title>
		<author>
			<persName><forename type="first">Taeuk</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang-Goo</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.197</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2528" to="2540" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hdltex: Hierarchical deep learning for text classification</title>
		<author>
			<persName><forename type="first">Kamran</forename><surname>Kowsari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mojtaba</forename><surname>Heidarysafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiana</forename><forename type="middle">Jafari</forename><surname>Meimandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">S</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><forename type="middle">E</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 16th IEEE international conference on machine learning and applications (ICMLA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="364" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for text classification</title>
		<author>
			<persName><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2267" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rcv1: A new benchmark collection for text categorization research</title>
		<author>
			<persName><forename type="first">Yiming</forename><surname>David D Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Russell-Rose</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="361" to="397" />
			<date type="published" when="2004-04">2004. Apr</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical text classification with reinforced label assignment</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1042</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="445" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Payal</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08473</idno>
		<title level="m">Coco-lm: Correcting and contrasting text sequences for language model pretraining</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Saloni Potdar, and Mo Yu. 2021. Improved text classification via contrastive adversarial training</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Wei</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avirup</forename><surname>Sil</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.10137</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical taxonomy-aware and attentional graph capsule rcnns for large-scale multi-label text classification</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Senzhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiran</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Evan</forename><surname>Sandhaus</surname></persName>
		</author>
		<title level="m">The new york times annotated corpus. Linguistic Data Consortium</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">26752</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">HFT-CNN: Learning hierarchical category structure for multi-label short text categorization</title>
		<author>
			<persName><forename type="first">Kazuya</forename><surname>Shimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fumiyo</forename><surname>Fukumoto</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1093</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="811" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A survey of hierarchical classification across different application domains</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">A</forename><surname>Silla</surname></persName>
		</author>
		<author>
			<persName><surname>Freitas</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10618-010-0175-9</idno>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="72" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">It&apos;s morphin&apos; time! Combating linguistic discrimination with inflectional perturbations</title>
		<author>
			<persName><forename type="first">Samson</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.263</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2920" to="2935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">2021a. Cognitive structure learning model for hierarchical multi-label text classification</title>
		<author>
			<persName><forename type="first">Boyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuegang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peipei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">218</biblScope>
			<biblScope unit="page">106876</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">CLINE: Contrastive learning with semantic negative examples for natural language understanding</title>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.181</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2332" to="2342" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hierarchical multi-label classification networks</title>
		<author>
			<persName><forename type="first">Jonatas</forename><surname>Wehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Cerri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Barros</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5075" to="5084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to learn and predict: A metalearning approach for multi-label classification</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1444</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-(EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-(EMNLP-IJCNLP)<address><addrLine>Hong China</addrLine></address></meeting>
		<imprint>
			<publisher>Madian</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4354" to="4364" />
		</imprint>
	</monogr>
	<note>Sinong Wang, Jiatao Gu</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Clear: Contrastive learning for sentence representation</title>
		<author>
			<persName><forename type="first">Sun</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15466</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Do transformers really perform badly for graph representation?</title>
		<author>
			<persName><forename type="first">Chengxuan</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adversarial attacks on deep-learning models in natural language processing: A survey</title>
		<author>
			<persName><forename type="first">Emma</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahoud</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Alhazmi</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/3374217</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="41" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">La-hcn: Label-based attention for hierarchical multi-label text classification neural network. Expert Systems with Applications</title>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlie</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihui</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">115922</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hierarchical multi-label text classification: Self-adaption semantic awareness network integrating text topic and label level information</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-82147-0_33</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Science, Engineering and Management</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="406" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hierarchy-aware global model for hierarchical text classification</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunping</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingkun</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gongshen</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.104</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1106" to="1117" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
