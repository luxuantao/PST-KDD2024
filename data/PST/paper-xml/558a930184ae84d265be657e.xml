<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Object Categories from Google&apos;s Image Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
							<email>fergus@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Engineering Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
							<email>feifeili@vision.caltech.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Electrical Engineering University of Oxford California Institute of Technology</orgName>
								<address>
									<addrLine>Parks Road</addrLine>
									<postCode>136-93, OX1 3PJ, 91125</postCode>
									<settlement>Oxford, Pasadena</settlement>
									<region>MC, CA</region>
									<country>U.K., U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">P</forename><surname>Perona</surname></persName>
							<email>perona@vision.caltech.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Electrical Engineering University of Oxford California Institute of Technology</orgName>
								<address>
									<addrLine>Parks Road</addrLine>
									<postCode>136-93, OX1 3PJ, 91125</postCode>
									<settlement>Oxford, Pasadena</settlement>
									<region>MC, CA</region>
									<country>U.K., U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Engineering Science</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Object Categories from Google&apos;s Image Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">55F2FAD137B979822D8AFFA42549245D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current approaches to object category recognition require datasets of training images to be manually prepared, with varying degrees of supervision. We present an approach that can learn an object category from just its name, by utilizing the raw output of image search engines available on the Internet. We develop a new model, TSI-pLSA, which extends pLSA (as applied to visual words) to include spatial information in a translation and scale invariant manner. Our approach can handle the high intra-class variability and large proportion of unrelated images returned by search engines. We evaluate the models on standard test sets, showing performance competitive with existing methods trained on hand prepared datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The recognition of object categories is a challenging problem within computer vision. The current paradigm <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref> consists of manually collecting a large training set of good exemplars of the desired object category; training a classifier on them and then evaluating it on novel images, possibly of a more challenging nature. The assumption is that training is a hard task that only needs to be performed once, hence the allocation of human resources to collecting a training set is justifiable. However, a constraint to current progress is the effort in obtaining large enough training sets of all the objects we wish to recognize. This effort varies with the size of the training set required, and the level of supervision required for each image. Examples range from 50 images (with segmentation) <ref type="bibr" target="#b14">[15]</ref>, through hundreds (with no segmentation) <ref type="bibr" target="#b9">[10]</ref>, to thousands of images <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>In this paper we propose a different perspective on the problem. There is a plentiful supply of images available at the typing of a single word using Internet image search engines such as Google, and we propose to learn visual models directly from this source. However, as can be seen in Fig. <ref type="figure" target="#fig_0">1</ref>, this is not a source of pure training images: as many as 85% of the returned images may be visually unrelated to the intended category, perhaps arising from polysemes (e.g. "iris" can be iris-flower, iris-eye, Iris-Murdoch). Even the 15% subset which do correspond to the category are substantially more demanding than images in typical training sets <ref type="bibr" target="#b8">[9]</ref> -the number of objects in each image is unknown and variable, and the pose (visual aspect) and scale are uncontrolled. However, if one can succeed in learning from such noisy contaminated data the reward is tremendous: it enables us to automatically learn a classifier for whatever visual category we wish. In our previous work we have considered this source of images for training <ref type="bibr" target="#b10">[11]</ref>, but only for the purpose of re-ranking the images returned by the Google search (so that the category of interest has a higher rank than the noise) since the classifier models learnt were too weak to be used in a more general setting, away from the dataset collected for a given keyword.  The problem of extracting coherent components from a large corpus of data in an unsupervised manner has many parallels with problems in the field of textual analysis. A leading approach in this field is that of probabilistic Latent Semantic Analysis (pLSA) <ref type="bibr" target="#b11">[12]</ref> and its hierarchical Bayesian form, Latent Dirichlet Allocation (LDA) <ref type="bibr" target="#b3">[4]</ref>. Recently, these two approaches have been applied to the computer vision: Fei-Fei and Perona <ref type="bibr" target="#b7">[8]</ref> applied LDA to scene classification and Sivic et al. applied pLSA to unsupervised object categorisation. In the latter work, the Caltech datasets used by Fergus et al. <ref type="bibr" target="#b9">[10]</ref> were combined into one large collection and the different objects extracted automatically using pLSA.</p><p>In this paper, we adopt and extend pLSA methods to incorporate spatial information in a translation and scaleinvariant manner and apply them to the more challenging problem of learning from search engine images. To enable comparison with existing object recognition approaches, we test the learnt models on standard datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Approach</head><p>Before outlining our approaches, we first review pLSA and its adaption to visual data, following Sivic et al.</p><p>We describe the model using the terminology of the text literature, while giving the equivalence in our application. We have a set of D documents (images), each containing regions found by interest operator(s) whose appearance has been vector quantized into W visual words <ref type="bibr" target="#b19">[20]</ref> </p><formula xml:id="formula_0">L = D d=1 W w=1 P (w, d) n(w,d) (2)</formula><p>In recognition, we lock P (w|z) and iterate with EM, to estimate the P (z|d) for the query images. Fig. <ref type="figure" target="#fig_4">4</ref>(a)-(c) shows the results of a two topic model trained on a collection of images of which 50% were airplanes from the Caltech datasets and the other 50% were background scenes from the Caltech datasets. The regions are coloured according to the most likely topic of their visual word (using P (w|z)): red for the first topic (which happens to pick out the airplane image) and green for the second (which picks out background images). P (z|d) is shown above each image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Absolute position pLSA (ABS-pLSA)</head><p>Previous work with pLSA applied to images did not use location information and we now extend the pLSA model to incorporate it. A straightforward way to do this is to quantize the location within the image into one of X bins and then to have a joint density on the appearance and location of each region. Thus P (w|z) in pLSA becomes P (w, x|z), a discrete density of size (W × X) × Z:</p><formula xml:id="formula_1">P (w, x, d) = Z z=1 P (w, x|z)P (z|d)P (d)<label>(3)</label></formula><p>The same pLSA update equations outlined above can be easily applied to this model in learning and recognition. The problem with this representation is that it is not translation or scale invariant at all, since x is an absolute coordinate frame. However, it will provide a useful comparison with our next approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Translation and Scale invariant pLSA (TSI-pLSA)</head><p>The shortcomings of the above model are addressed by introducing a second latent variable, c, which represents the position of the centroid of the object within the image, as well as its x-scale and y-scale, making it a 4-vector specifying a bounding box. As illustrated in Fig. now modeled relative to the centroid c, over a sub-window of the image. Within the sub-window, there are X fg location bins and one large background bin, x bg , giving a total of X = X fg + 1 locations a word can occur in. The word and location variables are then modeled jointly, as in section 2.1. This approach means that we confine our modeling of location to only the object itself where dependencies are likely to be present and not the background, where such correlations are unlikely. The graphical model of this approach is shown in Fig. <ref type="figure" target="#fig_3">3(d)</ref>.</p><p>We do not model an explicit P (w, x|c, z), since that would require establishing correspondence between images as c remains in an absolute coordinate frame. Rather, we marginalize out over c, meaning that we only model P (w, x|z):</p><formula xml:id="formula_2">P (w, x|z) = c P (w, x, c|z) = c P (w, x|c, z)P (c)<label>(4</label></formula><p>) P (c) here is a multinomial density over possible locations and scales, making for straightforward adaptations of the standard pLSA learning equations: P (w, x|z) in ( <ref type="formula" target="#formula_1">3</ref>) is substituted with the expression in (4). In learning we aggregate the results of moving the sub-window over the locations c.</p><p>Due to the high dimensionality of the space of c, it is not possible to marginalize exhaustively over scale and location within the image. Instead we use a small set of c, proposed in a bottom up manner for each topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Proposing object centroids within an image</head><p>We first run a standard pLSA model on the corpus and then fit a mixture of Gaussians with k = {1, 2, . . . , K} components to the location of the regions, weighted by P (w|z) for the given topic. The idea is to find clumps of regions that belong strongly to a particular topic, since these may be the object we are trying to model. The mean of the component gives the centroid location while its axis-aligned variance gives the scale of the sub-window in the x and y directions. We try different number of components, since there may be clumps of regions in the background separate from the object, requiring more than one component to fit. This process gives us a small set (of size C = K(K + 1)/2) of values of c to sum over for each topic in each frame. We use a flat density for P (c) since we have no more confidence in any one of the c being the actual object than any other. In recognition, there is no need to learn a standard pLSA model first to propose different values of c. Instead, the average word density over the sub-window ( P (w|z) = xfg P (w, x|z)) can be used to weight each region and then compute putative centroids in the manner above. Having obtained a set of centroids using P (w|z), recognition proceeds by locking P (w, x|z) and iterating to find P (z|d) for the novel images. In estimating P (z|d), all states of c are summed over, thus once convergence is reached, we find c * , the value of c within a frame which has the highest likelihood (shown in Fig. <ref type="figure" target="#fig_4">4(d</ref>)-(f) as a solid box).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Observations about TSI-pLSA</head><p>• Multiple object instances in a frame can be captured with k &gt; 1, with their information being combined by the marginalisation process. See Fig. <ref type="figure" target="#fig_4">4</ref>(d) for an example. • The model is entirely discrete, consisting of W XZ + DZ parameters, thus is able to cope with multi-modal non-Gaussian distributions. This enables the model to handle multiple aspects of the object since the different word-locations densities for each aspect will appear as different modes within the P (w, x|z) density. • Since all three approaches use histograms, unless the object occupies a reasonably large proportion of the image, it will not have a sufficient number of detections to compete with regions on the background, meaning that the image is misclassified as background.</p><p>While the sub-window approach of TSI-pLSA will help, it cannot overcome this effect entirely, so the object must still occupy a reasonable proportion of the image (1/4 to 1/5 of image area).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Implementation details</head><p>Having outlined the three approaches that we will investigate (pLSA; ABS-pLSA and TSI-pLSA), we now give specific details. All images are first converted to grayscale and resized to a moderate width (300 pixels in our experiments).</p><p>No further normalization of any kind was carried out.</p><p>In view of the large number of parameters in our models, it is vital to have a large number of data points in each frame. We therefore use four different types of circular region detector to give a complete coverage of the image: (i) Kadir &amp; Brady saliency operator <ref type="bibr" target="#b12">[13]</ref>; (ii) Multi-scale Harris detector <ref type="bibr" target="#b16">[17]</ref>; (iii) Difference of Gaussians, as used by Lowe <ref type="bibr" target="#b15">[16]</ref> and (iv) Edge based operator, detailed below.</p><p>For certain categories, edge information is important and is not adequately captured by the first three region detectors. Inspired by the approach of Berg et al. <ref type="bibr" target="#b2">[3]</ref>, we first find edgels in the image and then locate a region at points drawn at random from the edgel set. The scale of the region is chosen by drawing from a uniform distribution over a sensible scale range (a radius range of 5-30 pixels). The total number of regions sampled is capped to give a number similar to the other three types of detector. On average, around N = 700 regions per image were found, with Kadir &amp; Brady and the difference of Gaussians giving around 100 per image; the edge based detector 175, and multi-scale Harris 350.</p><p>Having found a large set of regions, we represent them by a SIFT descriptor, using 72 dimensions rather than the usual 128, resulting in larger histogram bins which are more appropriate for object categorization. The regions did not have their orientation normalised before histogramming, making them orientation variant. The descriptors are then vector quantized using a fixed codebooks of visual words, pre-computed using k-means from a large set of images drawn from the training sets of a large number of different categories. A separate codebook was formed for each feature type and then combined to give W visual words in total. In our experiments, we used W = 350. Regions could be quantized to any word, e.g. we did not restrict edge regions to only be allocated to the sub-section of the codebook formed from edge regions alone.</p><p>The two approaches with spatial densities used a grid of moderate coarseness, mindful of the need to keep the number of parameters to a reasonable level. The sub-window used in the experiments had a 6 × 6 grid, giving X = 37. Training a TSI-pLSA model with Z = 8, D ∼ 500 and the aforementioned parameters takes roughly 30 minutes using a Matlab implementation. ABS-pLSA takes approximately the same time. pLSA takes around half a minute. 100 iterations of EM were used. Assuming X = 37, W = 350, D = 500, N = 700, Z = 8, we have 109, 200 parameters in the model which are estimated from 350, 000 data points, giving a data/parameter ratio of just over 3, the minimum sensible level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Datasets</head><p>The experiments used 7 different object categories in 9 datesets. 5 of these were the Caltech datasets <ref type="bibr" target="#b8">[9]</ref>: Airplane; Car (Rear); Leopard; Face and Motorbike. Additionally, more challenging datasets for the car and motorbike classes were taken from PASCAL <ref type="bibr" target="#b5">[6]</ref>, using the test2 set of foreground/background training and test images. Finally, Guitar and Wrist watch were the two remaining categories. For each category four subsets of data were compiled: two hand gathered sets, where each image contains at least one instance of the object and two automatically gathered sets with may be contaminated with images unrelated to the category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Prepared training set (PT):</head><p>Manually gathered frames. In the case of the Caltech datasets, the training frames from <ref type="bibr" target="#b9">[10]</ref> were used. The pose of the object is quite constrained within these frames. The PASCAL datasets contained large viewpoint and pose variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Prepared test set (P):</head><p>Manually gathered frames, disjoint although statistically similar to (PT). For the Caltech datasets, the test frames from <ref type="bibr" target="#b9">[10]</ref> were used. Again, the pose is fairly constrained. In contrast, the PASCAL datasets contained large viewpoint and pose variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Raw Google set (G):</head><p>A set of images automatically downloaded from Google's Image Search<ref type="foot" target="#foot_0">1</ref> , using the category name. See Fig. <ref type="figure" target="#fig_0">1</ref> for typical images downloaded using "airplane". Duplicates images were discarded and Google's SafeSearch filter was left on, to reduce the proportion of unrelated images returned. For assessment purposes, the images returned by Google were divided into 3 distinct groups:</p><p>i Good images: these are good examples of the keyword category, lacking major occlusion, although there may be a variety of viewpoints, scalings and orientations. ii Intermediate images: these are in some way related to the keyword category, but are of lower quality than the good images. They may have extensive occlusion; substantial image noise; be a caricature or cartoon of the category; or the object is rather insignificant in the image, or some other fault. iii Junk images: these are totally unrelated to the keyword category.</p><p>The labeling was performed by an individual who was not connected with the experiments in anyway, possessing no knowledge of our algorithms. Fig. <ref type="figure" target="#fig_6">5</ref> shows the recallprecision curves of the raw Google sets for each category.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Google validation set (V):</head><p>An empirical observation (as seen in Fig. <ref type="figure" target="#fig_6">5</ref>) is that the first few pages returned by Google tend to contain more good images than those returned later on. The idea is that we assume the images from these first pages are positive examples, and hence may be used as a validation set to make model selection choices in our experiments. The catch is that the drop off in quality of Google's search is so steep that only the first few images of the first page are likely to be good examples.</p><p>Using Google's automatic translation tool <ref type="foot" target="#foot_1">2</ref> we obtain the translations of the users keyword in the following languages: German, French, Spanish, Italian, Portugese and Chinese. Since each translation returns a different set of images, albeit with the same drop off in quality, we automatically download the first few images from each different language, and combine to give a validation set of a reasonable size without a degradation in quality.</p><p>Using 7 different languages (including English), taking the first 5 images we can obtain a validation set of up to 35 images (since languages may share the same word for a category and we reject duplicate images). Note that this scheme does not require any supervision. Fig. <ref type="figure" target="#fig_7">6</ref> shows the validation set for "airplane". All datasets used are summarized in Table <ref type="table" target="#tab_0">1</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Selection of the final classifier</head><p>There are two issues to consider when training our models: (i) the optimal number of topics, Z ; (ii) which subset of these topics should be used to form a classifier for use in testing. A larger number of topics will result in more homogeneous topics at the expense of their ability to generalize. Given the varied nature of images obtained from Google, a large number of topics might seem appropriate, but this raises the issue of how to pick the topics corresponding to the good images, while ignoring topics which model the junk images within the dataset.</p><p>The number of topics to use in experiments was determined empirically: the performance of the face and airplane categories was recorded as the number of topics was varied when training from Google and a stable peak picked at Z = 8 (see Fig. <ref type="figure" target="#fig_10">8(b)</ref>). This value was then used for all experiments involving Google data. Having trained an 8 topic model, each topic is run across the validation set and single topic that performed best is picked to be the classifier used in testing. C Google experiments. Training on raw Google data (G); the best topic is then picked using the validation set (V), which is then tested on prepared data (P), measuring classification performance. All 3 methods were evaluated with 8 topics. The ability of our algorithm to train directly from Google data is evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>D Search engine improvement experiments. In the manner of <ref type="bibr" target="#b10">[11]</ref>. Training on raw Google data (G); picking the best topic using (V) and using it to re-rank the Google images (G). The idea is that the recall-precision curve of good images should be improved by the models learnt. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Caltech and PASCAL experiments</head><p>The results of experiments A, B in a classification setting are given in Table <ref type="table" target="#tab_1">2</ref>, columns 2-4. The results on the Caltech datasets show that (except for the leopard and guitar categories), the incorporation of location information gives a significant reduction in error rate. However, due to the constrained pose of instances within the images, the ABS-pLSA model often does as well if not better than the TSI-pLSA model (e.g. wrist watch and guitar). By contrast, when testing on the PASCAL datasets which contain far greater pose variability, the TSI-pLSA model shows a clear improvement over ABS-pLSA. See Fig. <ref type="figure" target="#fig_9">7</ref> for some examples of the TSI-pLSA model correctly detecting and localising cars in PASCAL test images. See Table <ref type="table">3</ref> for a comparison between TSI-pLSA and other current approaches on the PASCAL datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Google experiments</head><p>The results of experiment C in a classification setting are given in the last 3 columns of Table <ref type="table" target="#tab_1">2</ref>. As expected, training directly on Google data gives higher error rates than training on prepared data. For around half the categories, the use of location information reduces the error significantly, although only in the case of motorbikes and airplanes is TSI-pLSA better their either of the other two approaches.</p><p>Both ABS-pLSA and TSI-pLSA perform notably poorly on the guitar dataset. This may be explained by the fact that all the prepared data has the guitar in a vertical position while guitars appear at a seemingly random orientation in the Google training data. Since neither of the models using location can handle rotation they perform badly, in contrast to pLSA which still performs respectably. An example of a TSI-pLSA model learnt from Google data is shown in Fig. <ref type="figure" target="#fig_11">9</ref>. In the case of Motorbikes, the common words correspond to parts of the wheels of the bike and the exhaust/tail structure. In the case of Leopards, the textured fur of the animal is captured by the most common regions. However, their location densities are spread out, reflecting the diffuse spatial representation of the animal.</p><p>The confusion table of the seven classes is shown in Fig. <ref type="figure" target="#fig_10">8(a)</ref>. For the majority of classes the performance is respectable. Notable confusions include: airplanes being classified as cars rear (both have lots of horizontal edges); the guitar model misclassifying faces and wrist watches (due to the weak guitar model). See also Fig. <ref type="figure" target="#fig_1">2</ref> for the TSI-pLSA models used in a retrieval application. In Table <ref type="table">3</ref> we compare our performance to existing approaches to object recognition for experiments B and C, noting their degree of supervision. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Investigation of number of topics</head><p>In Fig. <ref type="figure" target="#fig_10">8</ref>(b) we vary the number of topics in a face model trained on Google data and evaluate: (a) the automatically chosen topic, and (b) the actual best topic on the prepared test set. The performance of all three methods does not seem to increase too much beyond 10 topics. This is due to the selection of a single topic -picking a combination of topics is likely to yield superior results. The difficulty is in deciding which ones to pick: the validation set picks the best topic (or close to it) reliably up to 8 topics or so. Beyond this its performance drops off significantly. For small numbers of topics, the models are unreliable, while it is difficult to pick the correct topic from very large models. The point of compromise seems to be in region of 5-10 topics (the curves are very similar for different categories), hence the use of Z = 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Improving Google's image search</head><p>As in Fergus et al. <ref type="bibr" target="#b10">[11]</ref>, the models learnt from Google data may be directly employed to improve the quality of the image search by re-ranking the images using the topic chosen from the validation set. As can be seen in Fig. <ref type="figure" target="#fig_6">5</ref>, the native performance of Google's search is quite poor. Fig. <ref type="figure" target="#fig_12">10</ref> shows the improvement in precision achieved by using the best topic chosen from an 8 topic model trained on the raw data. Figs. <ref type="figure" target="#fig_0">11</ref> and<ref type="figure" target="#fig_1">12</ref> show the top ranked images for each topic for the pLSA and TSI-pLSA approaches respectively, using the "motorbike" keyword.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Summary and Conclusions</head><p>We have proposed the idea of training using just the objects name by bootstrapping with an image search engine. The training sets are extremely noisy yet, for the most part, the results are competitive (or close to) existing methods requiring hand gathered collections of images. This was achieved by improving state-of-the-art pLSA models with spatial information. It would be interesting to compare our methods to <ref type="bibr" target="#b6">[7]</ref>, trained from the Google Validation set. However there are many open issues: the choice of features; better centroid proposals; the use of fixed background densities to assist learning; how to pick the most informative topics; the number of topics to use; the introduction of more sophisticated LDA models using priors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Images returned from Google's image search using the keyword "airplane". This is a representative sample of our training data. Note the large proportion of visually unrelated images and the wide pose variation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(a) A summary of our approach. Given the keywords: airplane, car rear, face, guitar, leopard, motorbike, wrist watch we train models from Google's image search with no supervision. We test them on a collection of 2148 images from the Caltech datasets and others, showing the top 5 images returned for each keyword in (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>. The corpus of documents is represented by a co-occurrence matrix of size W × D, with entry n(w, d) listing the number of words w in document d. Document d has N d regions in total. The model has a single latent topic variable, z, associating the occurrence of word w to document d. More formally: P (w, d) = Z z=1 P (w|z)P (z|d)P (d) (1) Thus we are decomposing a W × D matrix into a W × Z matrix and a Z × W one. Each image is modeled as a mixture of topics, with P (w|z) capturing the co-occurrence of words within a topic. There is no concept of spatial location within the model. The densities of the model, P (w|z) and P (z|d), are learnt using EM. The E-step computes the posterior over the topic, P (z|w, d) and then the M-step updates the densities. This maximizes the log-likelihood of the model over the data:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) Graphical model of pLSA. (b) Graphical model of ABS-pLSA. (c) The sub-window plus background location model. (d) Graphical model for translation and scale invariant pLSA (TSI-pLSA).</figDesc><graphic coords="3,330.81,325.64,105.29,69.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (a)-(c) Two airplane and one background image, with regions superimposed, coloured according to topic of a learnt pLSA model. Only a subset of regions are shown for clarity. (d)-(f) The same images as in (a)-(c) but showing the bounding boxes proposed by the pLSA model with dashed lines. The solid rectangle shows the centroid with highest likelihood under a TSI-pLSA model, with the colour indicating topic (the red topic appears to select airplanes). (d) shows multiple instances being handled correctly. (e) shows the object being localized correctly in the presence of background clutter.</figDesc><graphic coords="3,445.72,512.89,105.92,75.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1</head><label></label><figDesc>Recall precision of raw Google Images (Good vs. Inter+Bad)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Recall precision curves of the raw output of Google's image search for the 7 keywords. Good labels count as positive examples while Intermediate and Junk labels are negative examples. Note the precision drops rapidly as the recall increases, leveling out at 20-30% for most categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The entire validation set for "airplane" obtained automatically using Google's translation tool and Google's image search. The text by each row shows the translated keyword used to gather that particular row. The quality of the images is noticeably higher than those in Fig. 1.</figDesc><graphic coords="5,329.21,214.03,217.21,169.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Several sets of experiments were performed: A Caltech experiments. Training on a 50-50 mix of prepared data (PT) from the Caltech datasets (including watch and guitar) and data from the Caltech background dataset. Testing, in classification setting, on prepared data (P) and test data from the Caltech background. In the case of Cars Rear, the Caltech background was substituted for the Cars Rear Background for a more realistic experiment. All 3 methods (pLSA, ABS-pLSA and TSI-pLSA) were run with 2 topics (reflecting the true number of components in the training and test data). B PASCAL experiments. Training on prepared data (PT) of the two PASCAL datasets (cars, motorbikes) and their background images. Testing on prepared data (P) of PASCAL. Training was unsupervised, in the manner of [19], with the foreground and background data combined into one training set. All 3 methods (pLSA, ABS-pLSA and TSI-pLSA) were run with 6 topics and the best topic or equally weighted pair of topics chosen based on their performance on (PT). These experiments are designed to investigate the difference between ABS-pLSA and TSI-pLSA and measure localisation as well as detection performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Examples of the TSI-pLSA model, trained on the prepared PASCAL Cars data, correctly localising test instances. The ground truth bounding box is shown in magenta, while the proposed bounding box, c * , is shown in blue.</figDesc><graphic coords="6,324.11,431.81,106.10,77.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>(a) Confusion table for the 7 classes. The row is the ground truth label; the column indicates the classification. (b) "Face" keyword. Performance of models trained on Google data, tested on prepared data, with a varying number of topics. Red -pLSA; Green -ABS-pLSA; Blue -TSI-pLSA. Solid lines indicate performance of automatically chosen topic within model. Dashed lines indicate performance of best topic within model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>(a) Watches from the prepared dataset, with regions superimposed that belong to the 4 most common visual words (irrespective of location) from the automatically chosen topic of the Google-trained TSI-pLSA watch model. Each colour shows regions quantized to a different visual word. The circular bezel of the watch face is picked out. Due to the rotation sensitivity of our region presentation, different parts of the bezel are quantized to different words. (b) The location densities of the 4 most common words shown in (a). White corresponds to a high probability, black to a low one. Note their tightly constrained, multi-modal, nature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Improvement in the precision at 15% recall obtained with an 8 topic TSI-pLSA model (blue) over the raw Google ranking (yellow). This level of recall corresponds to a couple of web pages worth of images.</figDesc><graphic coords="8,334.90,57.32,198.38,136.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>topic 7 topic 7 Figure 11 :</head><label>711</label><figDesc>Figure 11: Top ranked images for each topic of an 8 topic pLSA model trained on Google data (G), using the keyword "motorbike". Topic selected by validation set (V) was topic 7. The coloured dots in the top-left corner of each image show the ground truth labels (Green = Good; Yellow = Intermediate and Red = Junk).</figDesc><graphic coords="8,76.15,205.60,198.38,136.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the datasets used in experiments.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Size of Dataset</cell><cell></cell><cell cols="3">Distrib. of Google Images (%)</cell></row><row><cell>Category</cell><cell>PT</cell><cell>P</cell><cell>V</cell><cell>G</cell><cell>Good</cell><cell>Inter.</cell><cell>Junk</cell></row><row><cell>Airplane</cell><cell>400</cell><cell>400</cell><cell>30</cell><cell>874</cell><cell>18.1</cell><cell>8.6</cell><cell>73.3</cell></row><row><cell>Cars Rear</cell><cell>400</cell><cell>400</cell><cell>30</cell><cell>596</cell><cell>32.2</cell><cell>12.9</cell><cell>54.9</cell></row><row><cell>Face</cell><cell>217</cell><cell>217</cell><cell>30</cell><cell>564</cell><cell>24.3</cell><cell>21.3</cell><cell>54.4</cell></row><row><cell>Guitar</cell><cell>450</cell><cell>450</cell><cell>25</cell><cell>511</cell><cell>25.3</cell><cell>30.5</cell><cell>44.2</cell></row><row><cell>Leopard</cell><cell>100</cell><cell>100</cell><cell>15</cell><cell>516</cell><cell>19.6</cell><cell>27.5</cell><cell>52.9</cell></row><row><cell>Motorbike</cell><cell>400</cell><cell>400</cell><cell>30</cell><cell>688</cell><cell>33.4</cell><cell>29.8</cell><cell>36.8</cell></row><row><cell>Wrist watch</cell><cell>180</cell><cell>181</cell><cell>35</cell><cell>342</cell><cell>63.4</cell><cell>13.8</cell><cell>22.8</cell></row><row><cell>PASCAL Cars</cell><cell>272</cell><cell>275</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PASCAL Cars Bg.</cell><cell>412</cell><cell>412</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PASCAL Motorbike</cell><cell>214</cell><cell>202</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PASCAL Motorbike Bg.</cell><cell>570</cell><cell>754</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Caltech Bg.</cell><cell>400</cell><cell>400</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Cars Rear Bg.</cell><cell>400</cell><cell>400</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Columns 2</cell></row><row><cell cols="8">&amp; 3: Size of the hand prepared training (PT) and test (P) datasets.</cell></row><row><cell cols="8">Column 4: The number of validation (V) images automatically</cell></row><row><cell cols="8">obtained. Column 5: The number of images automatically down-</cell></row><row><cell cols="8">loaded from Google's image search (G). The last 3 columns show</cell></row><row><cell cols="8">the breakdown (for evaluation purposes) of the raw Google im-</cell></row><row><cell cols="8">ages for each category. Note the low proportion of good examples</cell></row><row><cell cols="4">present in the majority of categories.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of different methods trained on: prepared data (first three columns) and raw Google data (rightmost three columns). All methods were tested on prepared data. The task is classification, with the figures being the error rate at point of equalerror on an ROC curve. The error margins are roughly +/-2%.</figDesc><table><row><cell></cell><cell>pLSA</cell><cell>ABS</cell><cell>TSI</cell><cell>pLSA</cell><cell>ABS</cell><cell>TSI</cell></row><row><cell>Category</cell><cell>Prep.</cell><cell>Prep.</cell><cell>Prep.</cell><cell>Google</cell><cell>Google</cell><cell>Google</cell></row><row><cell>(A)irplane</cell><cell>17.7</cell><cell>13.2</cell><cell>4.7</cell><cell>24.7</cell><cell>17.2</cell><cell>15.5</cell></row><row><cell>(C)ars Rear</cell><cell>2.0</cell><cell>0.2</cell><cell>0.7</cell><cell>21.0</cell><cell>13.2</cell><cell>16.0</cell></row><row><cell>(F)ace</cell><cell>22.1</cell><cell>11.5</cell><cell>17.0</cell><cell>20.3</cell><cell>36.4</cell><cell>20.7</cell></row><row><cell>(G)uitar</cell><cell>9.3</cell><cell>10.0</cell><cell>14.4</cell><cell>17.6</cell><cell>62.0</cell><cell>31.8</cell></row><row><cell>(L)eopard</cell><cell>12.0</cell><cell>12.0</cell><cell>11.0</cell><cell>15.0</cell><cell>16.0</cell><cell>13.0</cell></row><row><cell>(M)otorbike</cell><cell>19.0</cell><cell>6.0</cell><cell>7.0</cell><cell>15.2</cell><cell>18.5</cell><cell>6.2</cell></row><row><cell>(W)rist watch</cell><cell>21.6</cell><cell>7.7</cell><cell>15.5</cell><cell>21.0</cell><cell>20.5</cell><cell>19.9</cell></row><row><cell>PASCAL Car</cell><cell>31.7</cell><cell>33.0</cell><cell>25.8</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PASCAL Motorbike</cell><cell>33.7</cell><cell>30.2</cell><cell>25.7</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>While in this paper Google's image search was used exclusively (http://www.google.com/imghp), any other image search engine may be used provided that the images can be gathered in an automated manner</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://translate.google.com/translate_t</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Financial support was provided by: EC Project CogViSys; UK EPSRC; Caltech CNSE and the NSF. This work was supported in part by the IST Programme of the European Community, under the PASCAL Network of Excellence, IST-2002-506778. This publication only reflects the authors' views. Thanks to Rebecca Hoath and Veronica Robles for image labelling. We are indebted to Josef Sivic for his considerable help with many aspects of the paper. topic 7 topic 7 Figure 12: As per Fig. 11 but for an 8 topic TSI-pLSA model. Topic 7 was again the automatically selected topic. Note the increased consistency of each topic compared to pLSA.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The first value is the ROC EER classification rate; the second (where given) is the average precision <ref type="bibr" target="#b5">[6]</ref> in localisation. In PAS-CAL experiments (B), the classification performance is better than <ref type="bibr" target="#b14">[15]</ref>, but is less good at localisation. In Google experiments (C), the results for Leopard and Motorbike are comparable to other approaches. Airplane and Cars Rear are around 10% worse. However the supervision requirements of the other methods are greater. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to detect objects in images via a sparse, part-based representation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Awan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1475" to="1490" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Matching words and pictures</title>
		<author>
			<persName><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Duygulu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1107" to="1135" />
			<date type="published" when="2003-02">Feb 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shape matching and object recognition using low distortion correspondence</title>
		<author>
			<persName><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2005-06">June 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01">Jan 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Visual categorization with bags of keypoints</title>
		<author>
			<persName><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Statistical Learning in Computer Vision, ECCV</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc/index.html" />
		<title level="m">PASCAL visual object challenge datasets</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Bayesian approach to unsupervised one-shot learning of object categories</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2003-10">Oct 2003</date>
			<biblScope unit="page" from="1134" to="1141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Bayesian hierarchical model for learning natural scene categories</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2005-06">June 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Caltech object category datasets</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<ptr target="http://www.vision.caltech.edu/html-files/archive.html" />
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object class recognition by unsupervised scale-invariant learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2003-06">Jun 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A visual category filter for Google images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2004-05">May 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Probabilistic latent semantic indexing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scale, saliency and image description</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="83" to="105" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning methods for generic object recognition with invariance to pose and lighting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Combined object categorization and segmentation with an implicit shape model</title>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Statistical Learning in Computer Vision, ECCV</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="1999-09">Sep 1999</date>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Indexing based on scale invariant interest points</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Weak hypotheses and boosting for generic object detection and recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Opelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fussenegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Discovering object categories in image collections</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
		<idno>A. I. Memo 2005-005</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Video Google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2003-10">Oct 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sharing features: efficient boosting procedures for multiclass object detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="762" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Object recognition with informative features and linear classification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vidal-Naquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="511" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised learning of models for recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="18" to="32" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
