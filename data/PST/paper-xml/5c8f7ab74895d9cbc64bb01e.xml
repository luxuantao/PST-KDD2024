<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automated Pulmonary Nodule Detection in CT Images Using Deep Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-08-02">August 2, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hongtao</forename><surname>Xie</surname></persName>
							<email>htxie@ustc.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230026</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dongbao</forename><surname>Yang</surname></persName>
							<email>yangdongbao0903@163.com</email>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">School of Mechanical</orgName>
								<orgName type="department" key="dep2">Electrical and Information Engineering</orgName>
								<orgName type="institution">Shandong University</orgName>
								<address>
									<postCode>264209</postCode>
									<settlement>Weihai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Information Security Technologies</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100093</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nannan</forename><surname>Sun</surname></persName>
							<email>sunnannan@iie.ac.cn</email>
							<affiliation key="aff3">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Information Security Technologies</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100093</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhineng</forename><surname>Chen</surname></persName>
							<email>zhineng.chen@ia.ac.cn</email>
							<affiliation key="aff4">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230026</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Automated Pulmonary Nodule Detection in CT Images Using Deep Convolu-tional Neural Networks</orgName>
								<address>
									<settlement>Yongdong Zhang</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automated Pulmonary Nodule Detection in CT Images Using Deep Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-08-02">August 2, 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">348568E462CBDAC5E9EC84FF5C4542A4</idno>
					<idno type="DOI">10.1016/j.patcog.2018.07.031</idno>
					<note type="submission">Received date: 1 December 2017 Revised date: 3 June 2018 Accepted date: 31 July 2018 Preprint submitted to Journal of L A T E X Templates</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Pattern Recognition Nodule detection</term>
					<term>Convolutional neural network</term>
					<term>False positive reduction</term>
					<term>Computer-aided diagnosis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lung cancer is one of the leading causes of cancer-related death worldwide.</p><p>Early diagnosis can effectively reduce the mortality, and computer-aided diagnosis (CAD) as an important way to assist doctors has developed rapidly. In particular, automated pulmonary nodule detection in computed tomography (CT) images is crucial to CAD. It is a challenging task to quickly locate the exact positions of lung nodules. In this paper, a novel automated pulmonary nodule detection framework with 2D convolutional neural network (CNN) is proposed to assist the CT reading process. Firstly, we adjust the structure of Faster R-CNN with two region proposal networks and a deconvolutional layer to detect nodule candidates, and then three models are trained for three kinds of slices for later result fusion. Secondly, a boosting architecture based on 2D CNN is designed for false positive reduction, which is a classifier to distinguish true nodules from the candidates. The misclassified samples are still kept for retraining a model which boosts the sensitivity for pulmonary nodule detection.</p><p>Finally, the results of these networks are fused to vote out the final classification</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T 1. Introduction</head><p>Lung cancer is one of the widespread diseases and the leading causes of cancer-related death worldwide. As reported in global cancer statistics in 2012, nearly 1.83 million new cases of lung cancer occurred and the estimated deaths are over 1.5 million <ref type="bibr" target="#b0">[1]</ref>. In order to decrease the number of death, early detection 5 for potential patients is crucial. Especially, lung nodule detection in the initial stage is worth of attention.</p><p>With the development of computed tomography (CT) imaging, it becomes a standard modality for detecting and assessing lung cancer. Thus, millions of medical CT images are accumulated and needed to be further diagnosed by 10 radiologists, which consumes lots of time and effort. It is necessary to speed the CT scans reading process and reduce the heavy burdens of radiologists. To alleviate this problem, computer-aided diagnosis (CAD) has been a prosperous field in medical image processing, the predictions of CAD systems are used as the second diagnosis result for confirmation before making final decision. In 15 the diagnosis of lung cancer, efficiently automated pulmonary nodule detection in CT scans plays an important role in CAD to help radiologists <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b5">5]</ref>.</p><p>It is also a challenging task because there are many pulmonary nodules with various sizes, shapes, locations and types which are shown in Fig. <ref type="figure" target="#fig_1">1</ref>. Recently, the revolution of deep learning has attracted many researchers to pay their 20 attention to the applications of deep learning in CAD with the extraordinary learning power <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b9">9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Current automated pulmonary nodule detection systems mainly consist of two stages: (1) nodule candidate detection; <ref type="bibr" target="#b2">(2)</ref> false positive reduction <ref type="bibr">[10,</ref><ref type="bibr">11,</ref><ref type="bibr">12,</ref><ref type="bibr">13,</ref><ref type="bibr">14,</ref><ref type="bibr">15]</ref>. In nodule candidate detection stage, a number of candidates 25 are screened using some hand-crafted features or convolutional neural network to describe the characteristic of nodules, and then the candidate regions are obtained. The aim of this step is to detect nodule candidates at a very high sensitivity, which means that many false positives will be generated. Therefore, in false positive reduction step, a classifier is designed to reduce a large number 30 of false positive candidates.</p><p>Recently, many CAD systems based on deep learning are proposed for automatic lung cancer detection. For example, ZNET(gzuidhof) employs U-Net <ref type="bibr">[16]</ref> fully convolutional network architecture for candidate selection on axial slices.</p><p>For the subsequent false positive reduction, three orthogonal slices of each can-35 didate are fed to the same wide residual network. Resnet (QiDou) <ref type="bibr">[17]</ref> proposes a nodule detection framework based on 3D CNN, which screens the candidates with the fully convolutional network, and retrieves the high-probability locations as candidates. In false positive reduction, they employ the residual network which can ease the gradients flow within the network. JianPeiCAD <ref type="bibr">[18]</ref> 40 proposes a multi-scale rule-based screening method to obtain nodule candidates.</p><p>The false positive reduction uses 3D CNN with wide channels, which is trained using data augmentation to prevent overfitting.</p><p>Most of CAD systems achieve the satisfactory performance based on 3D CNN. Although processing using 3D structure of CT can reflect the whole in-45 formation about the nodules, it will also require more training time and storage space. In addition, the CT scans usually have different slice thicknesses (0.6-5mm), which are not recommended to be uniformly used in 3D nodule detection, and the preprocessing of 3D lung CT images is more complicated <ref type="bibr" target="#b11">[19]</ref>. On the contrary, 2D lung CT images are not influenced by the slice thickness, and 50 both training time and resources needed for processing are less than 3D CT images. Hence, using 2D image information is a more ideal way to detect the lung nodules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>In this paper, we propose a novel automated pulmonary nodule detection framework with 2D CNN to assist the CT reading process. The mainly contri-55 butions of this paper are as follows:</p><p>• A nodule candidate detection framework based on Faster Region-based Convolutional Neural Network (Faster R-CNN) <ref type="bibr" target="#b12">[20]</ref> is proposed. To adapt to the task, we adjust the structure of Faster R-CNN with two region proposal networks and a deconvolutional layer. To integrate 3D information 60 of the lung, three models are trained for three kinds of slices respectively, and then result fusion is conducted.</p><p>• We propose a boosting architecture of 2D CNN to reduce the false positive candidates, in which three models are sequentially trained, and each handles harder mimics than last model. We keep the mis-classified sam- For nodule candidate detection, the sensitivity achieves 86.42%. For the false 70 positive reduction, the sensitivity can reach 73.4% and 74.4% at 1/8 and 1/4 FPs/scan, respectively. It illustrates that the proposed method obviously achieves promising results.</p><p>The rest of this paper is organized as follows. Section 2 presents some related works on CAD systems of pulmonary nodule detection. In section 3, the 75 details of the proposed framework are elaborated. Section 4 shows the extensive experimental results on LUNA16. Finally, conclusion and discussion of the work are given in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Automated CAD for pulmonary nodule detection has been an active research 80 field, and many CAD systems have been proposed <ref type="bibr" target="#b13">[22]</ref>  <ref type="bibr" target="#b14">[23]</ref>. Most of the CAD For example, Tan et al. <ref type="bibr" target="#b17">[26]</ref> propose etrocad, in the detection stage of the 85 system, a nodule segmentation method is designed to locate the centers of nodule clusters, which is based on nodule and vessel enhancement filters and a computed divergence feature. In the subsequent classification stage, invariant features, defined on a gauge coordinates system, are used to differentiate between real nodules and some forms of blood vessels that are easily generating false positive 90 detections. Traverso et al. <ref type="bibr" target="#b18">[27]</ref> propose a WEB and Cloud-based CAD system, which is the combination of two independent CAD sub-systems: the Channeler Ant Model (lungCAM) and the Voxel-Based Neural Approach (VBNA). These two algorithms have a common starting point, which is the parenchymal volume, obtained with a 3D region growing segmentation algorithm, that also excludes 95 the trachea and separates the two lungs <ref type="bibr" target="#b20">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Recently, deep convolutional neural network has shown the power to effectively extract image features for successful classification across a variety of situations <ref type="bibr" target="#b21">[29]</ref> [30] <ref type="bibr" target="#b23">[31]</ref> [32] <ref type="bibr" target="#b25">[33]</ref>. Further more, many studies of transfer learning have found that CNN can be used as a generic image representation <ref type="bibr" target="#b27">[34]</ref> [35] <ref type="bibr" target="#b29">[36]</ref>, 100 which is capable of being applied to other tasks or datasets even without change of the network structure. Until now, many famous object detection frameworks have been proposed such as Faster R-CNN <ref type="bibr" target="#b12">[20]</ref>, SSD <ref type="bibr" target="#b31">[37]</ref>, R-FCN <ref type="bibr" target="#b32">[38]</ref>, which generate the candidate bounding boxes in the one-stage way with higher precision and less time than traditional methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>105</head><p>With the wide application of deep learning, many researchers in medical image processing have turned their attention on the exploitation of applying deep learning in their research field <ref type="bibr" target="#b33">[39]</ref>. For example, many recent proposed CAD systems for pulmonary nodule detection employ CNN to achieve accurate and fast diagnosis. According to the survey in <ref type="bibr" target="#b34">[40]</ref>, there are several CAD 110 systems proposed for complete nodule detection. ZNET uses CNN for both candidate detection and false positive reduction. The input slice is cropped to 512×512. Candidate detection is generated based on the probability map given by U-Net, which is applied on each axial slice. Then, a threshold is applied to obtain candidate masks, which is determined on the validation subset, max-115 imizing the number of detected nodules. Thereafter, a morphological erosion operation with a 4-neighborhood kernel is used to remove partial volume effects.</p><p>The candidates are then grouped by performing connected component analysis.</p><p>The center of mass of the components represent the coordinates of the candidates. For false positive reduction, ZNET uses wide residual networks <ref type="bibr" target="#b35">[41]</ref>, and [43], which firstly segments the lung internal structures by iteratively deploying ant colonies <ref type="bibr" target="#b38">[44]</ref> in voxels with intensity above a predefined threshold. The ant colony moves to a specific destination and releases pheromones based on a set of rules <ref type="bibr" target="#b39">[45]</ref>. Voxels visited by ant colonies are removed and new ant colonies 145 are deployed in not-yet-visited voxels. Iterative thresholding of the pheromone maps is applied to obtain a list of candidates. For false positive reduction stage, it computes a set of 13 features for nodule candidate analysis, including spatial, intensity, and shape features. The set of features is used to classify the candidates using a feed-forward neural network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We bring up an automated pulmonary nodule detection method, which consists of two stages: (1) nodule candidate detection, Fig. <ref type="figure" target="#fig_4">2;</ref><ref type="figure"></ref> (2) false positive reduction, Fig. <ref type="figure" target="#fig_12">5</ref>, and the whole framework is implemented based on 2D CNN.</p><p>The method is described in detail as follows. Finally, Region-of-Interest classifier is employed to get the candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Nodule candidate detection</head><p>The aim of nodule candidate detection is to detect nodule candidates at a very high sensitivity and restrict the total number of nodule candidates at The basic feature extraction network of our method is VGG16 <ref type="bibr" target="#b41">[47]</ref> with 5-170 group convolution, which is shared by the subsequently sub-networks. It is a challenging task to detect the smaller nodules in the larger whole lung images.</p><p>The minimum size of nodule is about 3mm in diameter, and the maximum size is about 30mm, which belongs to a kind of very small objects in images. Owing to this character of lung nodules, the selection of receptive field is important for The region proposal network takes an image as input and outputs a set of 185 rectangular object proposals, each with an objectness score. To implement this process, region proposal network is composed of fully convolutional network. To generate region proposals, a small network is slid over the feature map output by the last convolutional layer of the feature extraction network. This small network uses a 3×3 spatial window of the input feature map as input. Each 190 sliding window is mapped to a feature vector (512-d for VGG). This feature is fed into a box-classification layer and a box-regression layer. At each slidingwindow location, anchors are used to predict multiple region proposals. In order to integrate lower layer information to get more candidates of region proposal, a way of using two region proposal networks is applied in the framework <ref type="bibr" target="#b42">[48]</ref>. With these definitions, the multi-task loss for an image is defined as:</p><formula xml:id="formula_0">L(p i , t i , p 1 kj , t 1 kj ) = i L 1 (p i , t i ) + 2 k=1 j L 2 (p 1 kj , t 1 kj ).<label>(1)</label></formula><p>Here, L 1 and L 2 can be written as:</p><formula xml:id="formula_1">L 1 (p i , t i ) = L cls (p i , p * i ) + λp * i L reg (t i , t * i ). (<label>2</label></formula><formula xml:id="formula_2">) 210 L 2 (p 1 kj , t 1 kj ) = 1 N cls L cls (p 1 j , p * j ) + λ 1 N reg p * j L reg (t 1 j , t * j ). (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>Where i is the index of proposals produced by region proposal networks. p i is the predicted probability of proposal i being a nodule. The ground-truth label p * i is 1 if the proposal is positive, otherwise 0. t i is a vector representing the 4 parameterized coordinates of the predicted bounding box, and t * i is that of the ground-truth box associated with a positive proposal. The classification 215 loss L cls is log loss over two classes (nodule vs. not nodule). j is the index of an anchor which is chosen as a training sample in an region proposal network training mini-batch. k is the index of the two region proposal networks, p 1 kj and t 1 kj are similar to the symbols mentioned above but in the k-th region proposal network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>220</head><p>The regression loss is written as: The third model is trained with the bottom neighboring slice and its two neigh-230 boring slices. When testing, the slices are input into the three models separately, and then we merge the detection results of the three models to get the nodule candidates.</p><formula xml:id="formula_4">L reg (t i , t * i ) = R(t i -t * i ). (<label>4</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">False positive reduction</head><p>For false positive reduction stage, combined with boosting <ref type="bibr" target="#b44">[50]</ref> classifier idea, based on the extraction of different slices in a 3D cube block <ref type="bibr" target="#b45">[51]</ref>. According to statistics, the distribution of nodular diameter can cover 0.85 of nodules at 30×30 (voxels), and 0.99 of nodules can be covered at 40×40 (voxels). We choose 35×35 not only to cover almost all of the nodules, but also for small nodules, 245 the context information is rich and provides a lot of nodular information to analyze. For middle nodules, the amount of contextual information is suitable and no other noise information is included. For large nodules, the main part of the nodules can also be contained with some redundant marginal regions excluded <ref type="bibr">[52]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>250</head><p>The pixel size and coarse granularity of different scanning surfaces scanned from multifarious medical devices are different. It is not suitable for our mission, so isomorphic sampling is used to handle the problem. The pixel interval of one scanning slice may be [2.5,0.5,0.5], that is, the distance between slices is 2.5 mm. It is possible that another scan slice is [1.5,0.725,0.725]. This may 255 not be conducive to automatic analysis. The common processing method is to resample at a fixed isomorphic resolution from the full data set. In the proposed method, all the objects are sampled in 1×1×1(mm) pixels. Similar to <ref type="bibr" target="#b45">[51]</ref>,</p><p>A  nine patches on planes are also extracted. Three planes are known as sagittal, coronal, and axial planes, and the rest planes are the planes of symmetry that 260 cut two opposite faces of cubes in diagonals. The purpose is to make full use of the context around the nodules. For a nodule, we can get different patches at different orientations, and more feature information of a nodule can be obtained from the CNN. When the ratio of positive and negative samples varies widely, it is much better to get more positive sample features than simply flipping and 265 shifting. The preprocessing of getting the data for training is shown in Fig. <ref type="figure" target="#fig_11">4</ref>.</p><p>After the preprocessing, the dataset is divided into 5 subsets: 3 subsets for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>training, 1 subset for validation and 1 subset for testing. The key factor that affects the object detection is that the number of negative samples is too large, which accounts for most of the total loss, and most of these negative samples 270 are easy to classify <ref type="bibr" target="#b46">[53]</ref>. It can be inferred that hard mining is important for improving the performance of CNN, and the network should pay attention to those samples that are not easy to classify. Combining with this idea, the samples of the training dataset that are more difficult to distinguish are selected to continue to participate in training the next model, which can improve the 275 classification accuracy of single model. Every single model is based on the architecture of the AlexNet. In the task, the cross entropy classification loss function is calculated as follows:</p><formula xml:id="formula_5">L(p i ) = - 1 N i log[Sof tmax(a k )], i = 0, 1. (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>Where k is the ground-truth label of a sample, N is the batch size. There are only two categories. Softmax is defined as:</p><formula xml:id="formula_7">280 Sof tmax(a i ) = exp(a i ) j exp(a j ) , i = 0, 1.<label>(6)</label></formula><p>The training subset is divided into 3 parts, and each part is used to independently train the classification model. The first subset is employed to train a weak classification model1, and then misclassified samples from the model1 and second subset are used to independently train new model2 from scratch.</p><p>Similarly, model3 is independently trained with wrong data from model1 and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>We evaluate the performance of the framework on LUNA16 challenge. The </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>The candidate locations are computed using three existing candidate detection algorithms <ref type="bibr" target="#b15">[24]</ref> [4] <ref type="bibr" target="#b48">[55]</ref>. As lesions can be detected by multiple candidates, those that are located ≤5 mm are merged. The total candidates in LUNA16 are 754,976, and the corresponding class label (0 for non-nodule and 1 for nodule)</p><p>for each candidate is provided. Note that there can be multiple candidates per 310 nodule.</p><p>For false positive reduction stage, there is a challenge in the dataset: a serious imbalance between the false positive candidates and the true nodules (approximately 500:1). The simplest and most common method to alleviate the imbalance is data augmentation. We do image translations and horizontal 315 reflections on image data similar to <ref type="bibr" target="#b21">[29]</ref>. A pre-screening method is employed to deal with class imbalance. Downsampling is randomly applied to the negative nodule class, which make the number of negative samples similar to the positive samples. Then, they are merged to train a small CNN, in the experiment we adopt AlexNet model <ref type="bibr" target="#b21">[29]</ref>. Only the mis-judged nodules of negative samples  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluations Metrics</head><p>The performance of the proposed method is evaluated based on the results of cross validation using the Free Receiving Operating Curve (FROC) and competition performance metric (CPM). The sensitivities are measured at: 1/8, 1/4, 1/2, 1, 2, 4 and 8 FPs per patient. Sensitivities at those particular points are 330 averaged to get CPM for the system. Sensitivity also called true positive rate, which is defined as:</p><formula xml:id="formula_8">sensitivity = T P T P + F N .<label>(7)</label></formula><p>In AUC score is defined as the area under the ROC curve. The reason for this is that the ROC curve has a good characteristic: when the distribution of the 340 positive and negative samples in the test set changes, the ROC curve remains unchanged. In the actual data set, class imbalance phenomenon often occurs, that is to say, negative samples are much more than positive samples (or oppositely). What's more, the distribution of positive and negative samples in test data may change with time. In addition, the AUC value is used as an evaluation 345 criterion because the ROC curve usually does not clearly show which classifier works better, and as a numerical value, the classifier with a larger AUC score is better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head><p>The proposed method is implemented on Caffe <ref type="bibr" target="#b49">[56]</ref> framework, and employs 350 GPU to train these networks. We train the network only on a CUDA enabled graphics card: Tesla K80. For training nodule candidate detector, we perform 10-fold cross validation on LUNA16 dataset with given patient-level split. The number of iterations is 100000 in total with stochastic gradient descent optimization and momentum as 0.9. We use weight decay as 0.0005, and the base 355 learning rate is 0.001. The weights are initialized with the model pre-trained on</p><p>ImageNet <ref type="bibr" target="#b21">[29]</ref>.</p><p>The FROC performance of nodule candidate detection on LUNA16 is visualized in Fig. <ref type="figure" target="#fig_17">6</ref>. The solid line is interpolated FROC based on true prediction, and the dash lines are upper bound and lower bound for the bootstrapped 360 FROC performance. The sensitivity of the proposed nodule candidate detection method achieves 86.42%. The average number of candidates per scan is 4.67.</p><p>Table <ref type="table" target="#tab_0">1</ref> shows the sensitivities measured at: 1/8, 1/4, 1/2, 1, 2, 4 and 8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T  <ref type="table" target="#tab_1">2</ref> shows the detection rate for nodules in different sizes, which illustrates our method is better on detecting middle and large size nodules.</p><p>370 Fig. <ref type="figure">7</ref> presents some nodule candidates detected by our method. The displayed examples are some difficult samples, which include solitary nodule, vascularized nodule, juxtapleural nodule and pleural tail nodule. As shown, the proposed method can detect those nodules with high accuracy. Fig. <ref type="figure">8</ref> presents some detection results with original Faster R-CNN. As can be seen, the results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>375</head><p>of original Faster R-CNN is not comparable to our proposed architecture.</p><p>For false positive reduction, positive and negative samples can be obtained from the candidates with labels provided by the challenge. We perform cross   set to 256, the momentum <ref type="bibr" target="#b51">[58]</ref> is set to 0.9, and the dropout <ref type="bibr" target="#b52">[59]</ref> is set to 0.5 which is implemented in convolutional and fully connected layers as regularization. The results are listed in Table <ref type="table" target="#tab_2">3</ref>   In order to show the effect of our methods more visually, we list some clas-415 sification results of nodules as shown in Fig. <ref type="figure" target="#fig_22">10</ref>. The green rectangle is the true nodule, and the corresponding probability value below is predicted by the classification method proposed in this paper. We have judged the nodule with probability value greater than 0.5 as a malignant nodule, otherwise it is regarded as a benign nodule. The first row shows the nodules with larger radius.</p><formula xml:id="formula_9">A C C E P T E D M A N U S C R I P T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>420</head><p>These nodules are characterized by large radius and different shapes. For these nodules, the accuracy of our prediction results is relatively high, because these features are more obvious and representative. The second line shows us medium, even smaller nodules. The classification accuracy of these nodules is not as high as that of the first row, because their characteristic information is very little, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and discussion</head><p>In this paper, a novel and effective computer-aided automated pulmonary There is still room to improve the accuracy of nodule detection and clas-450 sification, which can be further studied. For example, fusing more context information about the nodules, such as the connections with the surrounding blood vessels, and the information about the patient such as the medical history report can also be analyzed in the automated diagnosis systems. In the next work, we will focus on more improvements in order to better serve the medical The first two rows are true nodules. The first line represents the large nodules, and the second line represents the middle and small nodules. The last line is benign nodules with high similarity to malignant nodules, however, misjudged them as malignant nodules.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>65</head><label></label><figDesc>ples and re-train a model to boost the sensitivity for pulmonary nodule detection. Finally, the results of these networks are fused and the final classification result is voted out.The performance of the proposed method is validated on LUNA16 dataset[21].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of lesions with various sizes, shapes, locations and types. Green rectangle for nodule and red rectangle for non-nodule. The images of the first line are multi-views of the same pulmonary nodule extracted 2D patches from nine symmetrical planes of a cube. The second line displays the different shapes of the diseased nodules. False positive candidates that are not related with cancers have quite similar morphological appearance to the true nodules shown in the red rectangle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>120</head><label></label><figDesc>64×64 patches from the axial, sagittal and coronal views are extracted for each candidate. Each patch is processed separately by the wide residual networks.The predicted output values of the network for these three different patches are averaged to obtain the final prediction. JianPeiCAD employs a multi-scale rule-based screening to obtain nodule candidates. The false positive reduction 125 uses 3D CNN with wide channels, which is trained using data augmentation to prevent overfitting. MOT M5Lv1 uses 3D region growing to obtain the lung A C C E P T E D M A N U S C R I P T volume, with trachea exclusion and lung separation procedures. The candidate detection algorithm is developed based on the method proposed by Messay et al. [42]. Multiple gray level thresholding and morphological processing are 130 used to segment nodules. The false positive reduction computes 15 features, among which geometrical (e.g. radius, sphericity, skewness of distance from center) and intensity features (e.g. average, standard deviation, maximum, entropy). Classification is performed using feed-forward neural networks. Resnet (QiDou) [17] proposes a nodule detection framework based on 3D CNN, which 135 screens the candidates with the fully convolutional network, and retrieves the high-probability locations as candidates. In false positive reduction, to deal with the large variations of pulmonary nodules and distinguish them from their hard mimics more robustly, they further propose to consider multilevel contextual information around pulmonary nodules by integrating a set of 3D CNNs 140 with different sizes of receptive field. M5LCAD is developed by Torres et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>150</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The framework of nodule candidate detection. The basic feature extraction network is VGG16, and a deconvolution layer is used to enlarge the feature map. Meanwhile, two region proposal networks with designed seven anchors are applied to obtain the proposals.</figDesc><graphic coords="9,108.73,307.76,355.32,103.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>the same time. Inspired by the development of object detection based on deep learning [20] [37] [38], nodule detection can also adopt one-stage object detection 160 framework based on CNN. According to the analysis and experiments [46], we design the detection structure based on Faster R-CNN, which performs better on small object detection. Fig.2 presents the proposed detection network for nodule candidate. To decrease the computation and storage cost of nodule detection, 2D axial A C C E P T E D M A N U S C R I P T slices are input into the network instead of directly using the 3D CT scans. The nodule candidate detection network is composed of three sub-networks: (1) feature extraction network; (2) region proposal network; (3) Region-of-Interest classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>175 the performance of pulmonary nodule detection when we use CNN to do the task. Due to the fact that after a series of convolution and pooling, the receptive field becomes larger. The size of the feature map in the last convolution layer of VGG16 (conv5 3) is 38×38, which leads to a limited performance in detecting 180 RoIs of nodules because the small feature map cannot clearly represent the features of nodules. To conduct region proposal in an effective receptive field, we add a deconvolution layer to obtain a 148×148 feature map after conv5 3 inspired by [10].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>195</head><label></label><figDesc>Because different region proposal networks have different perspectives, different A C C E P T E D M A N U S C R I P T useful information will be exploited. These two region proposal networks are concatenated to deconvolution layer and the middle convolution layer (conv3 3) respectively, which have approximately same size of feature maps. To fit the size of nodules, seven anchors with different sizes are designed: 12×12, 18×18, 200 27×27, 36×36, 51×51, 75×75 and 120×120 as shown in Fig.3. To obtain RoIs, a 3×3 sliding window is applied to the feature maps (deconvolution layer and conv3 3), and the designed anchors are employed to predict multiple RoIs at each location of sliding window. The region proposal network maps each 3x3 sliding window to a 512-d feature, which is fed into two sibling fully-connected 205 layers for regressing the bounding box of region and predicting score simultaneously.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Seven anchors with different sizes in the nodule detection network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>235severalAlgorithm 1 2 : 3 : 4 :</head><label>1234</label><figDesc>CNNs are used and the final result is obtained by voting. Boosting is a commonly used statistical learning method which is effective and also has a broad application. In the classification method, by changing the weight of the training sample, the classifiers are linearly combined to improve the classification A C C E P T E D M A N U S C R I P T The boosting based false positive reduction. Input: training samples, weak classifier Output: the classification results of samples 1: N training samples are randomly chose to train the first weak classifier; The mis-divided samples of the first classifier and new samples randomly chose from the remaining training set, in total N training samples, are used to train the second weak classifier; The samples that are misclassified by above two weak classifiers are merged with other new samples to train the third weak classifier; The category of each sample is decided by majority vote of above mentioned three models. performance. The general process is described in Algorithm 1: 240 For the acquisition of candidate nodules, we obtain the 2D faces (35×35)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The framework of data preprocessing. (a) subset i means a subset of the raw and unprocessed data. After data augmentation for positive samples and filtering for non-nodules, we get subset i*. (b) subset i* pass through three optimized models, and the final result is voted out.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>285Figure 5 :</head><label>5</label><figDesc>Figure 5: The architecture of the proposed boosting method. The top model1 is trained by the filtered data sets (subset i*). The error data (mis-classified(0)) of the first model in the red rectangle box and the new training data are merged to train the middle model2. The bottom models parameter is optimized by using the erroneous data from first two models and other new data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>295</head><label></label><figDesc>dataset is collected from the largest publicly available reference database for pulmonary nodules: the LIDC-IDRI<ref type="bibr" target="#b47">[54]</ref>, containing a total of 1018 CT scans.LUNA16 dataset only has the detection annotations, while LIDC-IDRI contains almost all the related information for low-dose lung CTs including several doctors annotations on nodule sizes, locations, diagnosis results, nodule texture, 300 nodule margin and other information. LUNA16 dataset removes CTs with slice thickness greater than 3mm, slice spacing inconsistent or missing slices from LIDC-IDRI dataset, and also removes the annotated nodules of size smaller than 3mm. Remaining 888 scans are divided into 10-folds with the objective to perform cross validation over them.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>320</head><label></label><figDesc>by the trained model are left for the next round training. After conducting the preliminary screening of negative samples, the ratio of negative and positive samples is 100:1. Only keeping those misclassified nodules can solve the severe class imbalance, and select the more representative and more discriminative samples for training and testing, which can enhance the robustness of the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>325</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>the binary classification problem of false positive reduction stage, we also use ROC (Receiver Operating Characteristic) curve and AUC (Area Under A C C E P T E D M A N U S C R I P T Curve) score to evaluate these methods. ROC curves and AUC scores are often 335 used to evaluate the merits of a binary classifier. ROC curve is based on a series of different classification methods regarding true positive rate (sensitivity) as vertical ordinate and false positive rate (1-specificity) as horizontal ordinate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Sensitivity with respect to false positives per scan. The CPM (average recall rate at the false positives as 0.125, 0.25, 0.5, 1, 2, 4, 8) is 77.5%. The proposed nodule candidate detection has a recall rate 86.4% for all the nodules. The dash lines are lower bound and upper bound FROC for 95% confidence interval using bootstrapping with 1,000 bootstraps [40]. The solid line is the interpolated FROC based on prediction.</figDesc><graphic coords="19,174.89,151.23,220.58,165.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>validation in 5 -</head><label>5</label><figDesc>fold across selected 888 LIDC-IDRI evaluation cases. The pixel intensity range (-1000,4000 Houndsfield Unit) is rescaled to (0,1), and we sub-380 tract the mean gray-scale value to fit the distribution of training and testing data. In the training process, stochastic gradient decent is used to learn the weights, and a batch of training samples are used to calculate every parameter update of the iteration. Weights are initialized randomly by Gaussian distribution and updated with standard backpropagation [57]. The mini-batch size is 385</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>compared with those of other LUNA16 teams using the deep CNNs. It lists the detection sensitivities of different teams at different false positive rates specified by the challenge. 390 From the results of the competition, all teams use deep CNNs, which also shows the profound impact of deep CNNs on medical image analysis community nowadays. Two of the eight teams are based on 3D CNN. Although the 3D structure of CT can encode the whole information about the nodules, it will also pay more training time and storage space. In addition, the preprocessing 395 of the 3D lung CT images makes the task more complicated [19] because of its different slice thicknesses (0.6-5mm) of the CT scans. In our study, we adopt A C C E P T E D M A N U S C R I P T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>425</head><label></label><figDesc>and there are other redundant information which is not related to nodule classification. Overall, our classification of nodules is good, with a CPM score of 0.790 for that only 2D information of nodules considered. The nodules in the red rectangle, namely, the nodules shown on the last row, are false positive nodules. Those nodules are predicted wrong by our methods with the number 430 below indicating the malignant probability. It can be seen that these nodules are similar in shape and size to true nodules, and this is an important reason for the enormity of the task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>435</head><label></label><figDesc>nodule detection method based on 2D CNN is proposed, which is composed of two stages: (1) nodule candidate detection; (2) false positive reduction. For the first stage, we introduce a detection framework based on improved Faster R-CNN to adapt to detect nodules, which integrates a deconvolution layer to enlarge the feature map and two region proposal networks to concatenate the 440 A C C E P T E D M A N U S C R I P T useful information from the lower layer. In addition, three models are trained using different slices, and then we merge the results to obtain the candidates. For the second stage, a boosting based classifier is trained to reduce the false positives produced by the first stage. Three models are sequentially trained, and the training data of each model includes the mis-classified nodules by the 445 previous model. A more accurate model is obtained by repetitively training nodules which are more difficult to classify. The results are voted out by these three models. Experiments are conducted on LUNA16, and it demonstrates that the proposed method can accurately detect the latent pulmonary nodules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Examples of the detected nodules and corresponding classification probabilities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="31,143.36,222.25,284.38,385.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results of the nodule candidate detection.</figDesc><table><row><cell>Teams</cell><cell>0.125</cell><cell>0.25</cell><cell>0.5</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>CPM</cell></row><row><cell>OUR ND</cell><cell>0.493</cell><cell>0.688</cell><cell>0.796</cell><cell>0.852</cell><cell>0.864</cell><cell>0.864</cell><cell>0.864</cell><cell>0.775</cell></row><row><cell>MOT M5Lv1</cell><cell>0.597</cell><cell>0.670</cell><cell>0.718</cell><cell>0.759</cell><cell>0.788</cell><cell>0.816</cell><cell>0.843</cell><cell>0.742</cell></row><row><cell>VisiaCTLung</cell><cell>0.577</cell><cell>0.644</cell><cell>0.697</cell><cell>0.739</cell><cell>0.769</cell><cell>0.788</cell><cell>0.793</cell><cell>0.715</cell></row><row><cell>etrocad</cell><cell>0.250</cell><cell>0.522</cell><cell>0.651</cell><cell>0.752</cell><cell>0.811</cell><cell>0.856</cell><cell>0.887</cell><cell>0.676</cell></row><row><cell>M5LCAD</cell><cell>0.306</cell><cell>0.360</cell><cell>0.540</cell><cell>0.691</cell><cell>0.762</cell><cell>0.797</cell><cell>0.798</cell><cell>0.608</cell></row><row><cell>JianPeiCAD</cell><cell>0.848</cell><cell>0.916</cell><cell>0.947</cell><cell>0.961</cell><cell>0.965</cell><cell>0.966</cell><cell>0.967</cell><cell>0.939</cell></row><row><cell>resnet(QiDou)</cell><cell>0.659</cell><cell>0.745</cell><cell>0.819</cell><cell>0.865</cell><cell>0.906</cell><cell>0.933</cell><cell>0.946</cell><cell>0.839</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The detection rate for nodules in different sizes.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results of the false positive reduction track in LUNA16 challenge.</figDesc><table><row><cell>Team</cell><cell>CNN</cell><cell>0.125</cell><cell>0.25</cell><cell>0.5</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>CPM</cell></row><row><cell>OUR</cell><cell>2D</cell><cell cols="2">0.734 0.744</cell><cell>0.763</cell><cell>0.796</cell><cell>0.824</cell><cell>0.832</cell><cell>0.834</cell><cell>0.790</cell></row><row><cell cols="2">DIAG CONVNET 2D</cell><cell>0.636</cell><cell>0.727</cell><cell>0.792</cell><cell>0.844</cell><cell>0.876</cell><cell>0.905</cell><cell>0.916</cell><cell>0.814</cell></row><row><cell>iitem03</cell><cell>2D</cell><cell>0.394</cell><cell>0.491</cell><cell>0.570</cell><cell>0.660</cell><cell>0.732</cell><cell>0.795</cell><cell>0.851</cell><cell>0.642</cell></row><row><cell>LUNA16CAD</cell><cell>2D</cell><cell>0.113</cell><cell>0.165</cell><cell>0.265</cell><cell>0.465</cell><cell>0.596</cell><cell>0.695</cell><cell>0.785</cell><cell>0.440</cell></row><row><cell>LungNess</cell><cell>2D</cell><cell>0.453</cell><cell>0.535</cell><cell>0.591</cell><cell>0.635</cell><cell>0.696</cell><cell>0.741</cell><cell>0.797</cell><cell>0.635</cell></row><row><cell>UACNN</cell><cell>2D</cell><cell>0.655</cell><cell cols="2">0.745 0.807</cell><cell>0.849</cell><cell>0.880</cell><cell cols="3">0.907 0.925 0.824</cell></row><row><cell>LUNA16CAD</cell><cell>3D</cell><cell>0.640</cell><cell>0.698</cell><cell>0.750</cell><cell>0.804</cell><cell>0.847</cell><cell>0.874</cell><cell>0.897</cell><cell>0.787</cell></row><row><cell>CUMedVis</cell><cell>3D</cell><cell>0.677</cell><cell>0.737</cell><cell cols="5">0.815 0.848 0.879 0.907 0.922</cell><cell>0.827</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Statistics on the number of nodules and non-nodules in the candidate dataset. To deal with severe class imbalance, data augmentation attached upsampling (aug+up) for nodules and screening attached downsampling (scr+dw) for non-nodules.</figDesc><table><row><cell></cell><cell>Training dataset</cell><cell>Fold 0</cell><cell>Fold 1</cell><cell>Fold 2</cell><cell>Fold 3</cell><cell>Fold 4</cell></row><row><cell></cell><cell>scans</cell><cell>178</cell><cell>178</cell><cell>178</cell><cell>178</cell><cell>176</cell></row><row><cell></cell><cell>nodule</cell><cell>308</cell><cell>339</cell><cell>297</cell><cell>274</cell><cell>339</cell></row><row><cell></cell><cell>-aug+up</cell><cell>8316</cell><cell>9207</cell><cell>8025</cell><cell>7404</cell><cell>9183</cell></row><row><cell></cell><cell>non-nodule</cell><cell>149841</cell><cell>149671</cell><cell>151979</cell><cell>151462</cell><cell>150075</cell></row><row><cell></cell><cell>-scr+dw</cell><cell>9621</cell><cell>9528</cell><cell>10115</cell><cell>10505</cell><cell>9701</cell></row><row><cell></cell><cell cols="6">the 2D CNN methods. According to the candidate nodules provided by the</cell></row><row><cell></cell><cell cols="6">False Positive phase in the LUNA16 competition, we has obtained 0.75 million</cell></row><row><cell>400</cell><cell cols="6">samples in total. After data preparation described in Section 3.2, we rebuild</cell></row><row><cell></cell><cell cols="5">the training data set which is summarized in Table 4.</cell></row><row><cell></cell><cell cols="6">The area under ROC (AUC) score is as high as 0.954 when we apply our</cell></row><row><cell></cell><cell cols="6">2D CNN to nodules/non-nodules classification task for a set of candidates. The</cell></row><row><cell></cell><cell cols="6">FROC curve of our proposed 2D CNN CAD system is presented in Fig.9. It</cell></row><row><cell>405</cell><cell cols="6">is observed that the average detection sensitivities is 0.790 at the false positive</cell></row><row><cell></cell><cell cols="6">rate as 1/8, 1/4, 1/2, 1, 2, 4 and 8. In these 2D approaches, we have achieved</cell></row><row><cell></cell><cell cols="6">excellent results. The reason why the low false positive rates( 1/8, 1/4, 1/2 false</cell></row><row><cell></cell><cell cols="6">positives per scan) are included is that this evaluation scheme can challenge the</cell></row><row><cell></cell><cell cols="6">module's acceptable percentage with very few false positives. It is worth noting</cell></row><row><cell>410</cell><cell cols="6">that, in Table 3, we have achieved the best performance compared with other</cell></row><row><cell></cell><cell cols="6">teams at 1/8 false positives per scan. For examples, sensitivity of 0.734 and</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment This work is supported by the National Key Research and Development Program of China (2017YFC0820600), the National Nature Science Foundation of China (61525206, 61771468 and 61772526), the Youth Innovation Promotion 460 Association Chinese Academy of Sciences(2017209).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>[10] J. Ding, A. Li, Z. Hu, L. <ref type="bibr">Wang</ref>     </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Global cancer statistic</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ferlay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lortet-Tieulent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ca A Cancer Journal for Clinicians</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="87" to="108" />
			<date type="published" when="2012">2012. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>A C C E P T E D M A N U S C R I P T</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Computer analysis of computed tomography scans of the lung: a survey</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sluimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schilham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Prokop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Van</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="385" to="405" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Comparing and combining algorithms for computer-aided detection of pulmonary nodules in computed tomography scans: The anode09 study</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D V S</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Duindam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niemeijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schilham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Retico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Fantacci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Camarlinghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="707" to="722" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic detection of subsolid pulmonary nodules in thoracic computed tomography images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M V</forename><surname>Rikxoort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Twellmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Scholten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A D</forename><surname>Jong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Kuhnigk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oudkerk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J D</forename><surname>Koning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Prokop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schaefer-Prokop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="374" to="384" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep learning for 480 image-based cancer detection and diagnosisa survey</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An iterative possibilistic knowledge diffusion approach for blind medical image segmentation</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">K</forename><surname>Kallel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Almouahed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Solaiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">É</forename><surname>Bossé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="182" to="197" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Comparing two classes of end-to-end machine-485 learning models in lung nodule detection and classification: Mtanns vs. cnns</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="476" to="486" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A regularized ensemble framework of deep learning for cancer detection from multi-class, imbalanced training data</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abouelenien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="160" to="172" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic segmentation of regions of interest in breast thermographic images</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Villalobos-Montiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Chacon-Murguia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Calderon-Contreras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ortega-Maynez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mexican Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><surname>A C C E P T E D M A N U S C R I P T</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An automatic detection system of lung nodule based on multi-group patch-based deep learning network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics PP</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Faster r-cnn: towards real-time object 525 detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Computeraided classification of lung nodules on computed tomography images via 530 deep learning technique</title>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Hidayati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OncoTargets and therapy</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Background extraction based on joint gaussian conditional random fields</title>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Hua</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE Transactions on Circuits and Systems for Video Technology</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gi-535 etema, M. Prokop, A large-scale evaluation of automatic pulmonary nodule detection in chest ct using local image features and k-nearest-neighbour classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">V</forename><surname>Ginneken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M R</forename><surname>Schilham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J D</forename><surname>Hoop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="757" to="770" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust common visual pattern discovery using graph matching</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Commu-540 nication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="635" to="646" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A novel computeraided lung nodule detection system for ct images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Deklerck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cornelis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical physics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5630" to="5645" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Computer-aided 545 detection systems to improve lung cancer early diagnosis: state-of-the-art and challenges</title>
		<author>
			<persName><forename type="first">A</forename><surname>Traverso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Fantacci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cerello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Physics: Conference Series</title>
		<imprint>
			<biblScope unit="volume">841</biblScope>
			<biblScope unit="page">12013</biblScope>
			<date type="published" when="2017">2017</date>
			<publisher>IOP Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><surname>A C C E P T E D M A N U S C R I P T</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatic lung seg-550 mentation in ct images with accurate handling of the hilar region</title>
		<author>
			<persName><forename type="first">G</forename><surname>De Nunzio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agrusti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cataldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>De Mitri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Favetta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maglio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Massafra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Quarta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Torsello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of digital imaging</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="27" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Supervised deep quantization for efficient image search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia &amp; Expo Workshops (ICMEW)</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="525" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Local geometric consistency constraint for image retrieval</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICIP.2011.6115596</idno>
	</analytic>
	<monogr>
		<title level="m">2011 18th IEEE International Conference on Image 560 Processing</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="101" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hierarchical clustering multitask learning for joint human action grouping and recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2016.2537337.565</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2016.2537337" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="102" to="114" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A semi-markov model for mitosis segmentation in time-lapse phase contrast microscopy image sequences of stem cell populations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2011.2169495</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="359" to="369" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/TMI.2011.2169495</idno>
		<ptr target="https://doi.org/10.1109/TMI.2011.2169495" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Supervised hash coding with deep neural network for environment perception of intelligent vehicles</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gestalt rule feature points</title>
		<author>
			<persName><forename type="first">I.-C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-H</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="526" to="537" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-grained random fields for mitosis identification in time-lapse phase contrast microscopy image sequences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1699" to="1710" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/TMI.2017.2686705</idno>
		<ptr target="https://doi.org/10.1109/TMI.2017.2686705" />
		<imprint>
			<biblScope unit="page" from="580" to="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
	<note>Ssd: Single shot multibox detector</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Object detection via region-based fully convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>-Fcn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Background extraction using random walk image fusion</title>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Lai</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: The luna16 challenge</title>
		<author>
			<persName><forename type="first">S</forename><surname>Aaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Traverso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Msn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cvd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cerello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Fantacci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Geurts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">595</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide residual networks</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A new computationally efficient cad system for pulmonary nodule detection in ct imagery</title>
		<author>
			<persName><forename type="first">T</forename><surname>Messay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Hardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Rogers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image</title>
		<imprint>
			<biblScope unit="volume">600</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="390" to="406" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Analysis</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Large scale validation of the m5l lung cad on heterogeneous ct datasets</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fiorina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pennazio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Peroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saletta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Camarlinghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Fantacci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cerello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Physics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1477" to="1489" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">3-d object segmentation using ant colonies</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cerello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Cheran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bagnasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bellotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bolanos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Catan-605 Zariti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Nunzio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Fantacci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fiorina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gargano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1476" to="1490" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">How swarms build cognitive maps</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Chialvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Millonas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The biology and technology of intelligent autonomous agents</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="page">450</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.10012</idno>
		<title level="m">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-615 scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Detecting uyghur text in complex background images with convolutional neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE international confer-620 ence on computer vision</title>
		<meeting>the IEEE international confer-620 ence on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
	<note>Fast r-cnn</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<title level="m">Sixteenth International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1401" to="1406" />
		</imprint>
	</monogr>
	<note>A brief introduction to boosting</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pulmonary 625 nodule detection in ct images: False positive reduction using multi-view convolutional networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A A</forename><surname>Setio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ciompi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gerke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J V</forename><surname>Riel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M W</forename><surname>Wille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naqibullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Snchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">V</forename><surname>Ginneken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1558" to="1567" />
			<date type="published" when="2016">2016. 2017</date>
		</imprint>
	</monogr>
	<note>IEEE Transactions on Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02002</idno>
		<title level="m">Focal loss for dense object detection</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The 635 lung image database consortium (lidc) and image database resource initiative (idri): a completed reference database of lung nodules on ct scans</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Armato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mclennan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bidaut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Mcnittgray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Reeves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Aberle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Henschke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Physics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">915</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Automatic detection of large pulmonary solid nodules in thoracic ct images</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A A</forename><surname>Setio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gelderblom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ginneken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Physics</title>
		<imprint>
			<biblScope unit="volume">640</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5642" to="5653" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">1139</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
