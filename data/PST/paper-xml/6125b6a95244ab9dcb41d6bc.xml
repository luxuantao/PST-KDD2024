<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AIRCHITECT: Learning Custom Architecture Design and Mapping Space</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Ananda</forename><surname>Samajdar</surname></persName>
							<email>anandsamajdar@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Tech Atlanta</orgName>
								<address>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jan</forename><surname>Moritz</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthew</forename><surname>Denton</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Georgia Tech Atlanta</orgName>
								<address>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AIRCHITECT: Learning Custom Architecture Design and Mapping Space</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Design space exploration is an important but costly step involved in the design/deployment of custom architectures to squeeze out maximum possible performance and energy efficiency. Conventionally, optimizations require iterative sampling of the design space using simulation or heuristic tools. In this paper we investigate the possibility of learning the optimization task using machine learning (ML) and hence using the learnt model to predict optimal parameters for the design and mapping space of custom architectures, bypassing any exploration step. We use three case studies involving the optimal array design, SRAM buffer sizing, mapping, and schedule determination for systolicarray-based custom architecture design and mapping space. We perform systematic design-aware and statistical analysis of the optimization space for our case studies and highlight the patterns in the design space. We formulate the architecture design and mapping as a ML problem that allows us to leverage existing ML models for training and inference. We design and train a custom network architecture called AIRCHITECT, which is capable of learning the architecture design space with as high as 94.3% test accuracy and predicting optimal configurations which achieve on average (GeoMean) of 99.9% the best possible performance on a test dataset with 10 5 GEMM workloads.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Custom architectures, in the form of accelerators, are arguably the de facto solution to create high performance hardware in the face of diminishing benefits from semiconductor device scaling. Among the many areas where custom architectures helped to meet the ever-increasing demand for compute performance, ML acceleration is probably the most noticeable example. As ML models start to become ubiquitous and more complex across various applications, their sheer demand for computing power and their inherent parallelism renders them too expensive and inefficient for general-purpose architectures; simultaneously creating a perfect case to reap the benefits of custom-designed accelerators <ref type="bibr" target="#b7">[8]</ref>. Unsurprisingly ML accelerators are now used across all form factors from data centers to mobile phones, providing record-breaking performance and energy efficiency <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b13">[14]</ref>.</p><p>Custom architecture design is a data-driven process. In a typical design cycle, the application characteristics are carefully studied for understanding possible optimizations, software bottlenecks, and computation/memory access patterns. Then multiple iterations of design space exploration (DSE) are performed for evaluating the costs of possible architecture choices via simulations or other cost models <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b29">[30]</ref>. Finally, based on the data obtained from DSE and applicable constraints,  an implementation choice is made to get the best performance and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Workload Dims Design Constraints</head><p>In this paper, we demonstrate that the same data-driven decision-making can be learned by a ML model for the architecture design task. To get insights into the patterns, dimensionality, feasibility of learning the architecture design and mapping space we undertake three case studies. The first case study concerns the problem of finding the optimal shape and mapping strategy of a monolithic systolic array when presented with a general matrix multiply (GEMM) workload and design constraints. The second deals with sizing of SRAM buffer dimensions in a monolithic systolic array, when queried with the information about the buffer size limits, interface bandwidth, structure and mapping of the compute array, and the workload dimensions. The third case study explores the scheduling space of optimally running GEMM workloads on systolic arrays with different configurations.</p><p>With the help of our case studies, we systematically examine the optimization space of design and mapping to understand patterns and relationships between input parameters and optimal points, which can potentially facilitate learning of the space. Our analysis demonstrates that the architecture design and mapping space is learnable, at least within the limits of our use cases. We demonstrate that by formulating the optimization prediction as a ML classification, we achieve high degrees of accuracy in determining the optimal design parameters using a learned model. We design and evaluate a custom class of neural network architectures called AIRCHITECT that is capable of arXiv:2108.08295v1 [cs.LG] 16 Aug 2021 learning the design space to get a prediction accuracy as high as 94.3% on a test dataset of 10 5 datapoints.</p><p>To the best of our knowledge, this work is one of the first that learns the design space to generalize it. Most prior works leveraging ML for accelerator DSE <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b26">[27]</ref> focus on performing the search faster. Learning the design space enables constant time prediction of the optima. Such a feature has the potential to significantly speed up the conventional architecture design cycles. For instance, custom architecture design, by its very definition is tightly coupled with an application or an algorithm, which enables extracting orders of magnitude performance and energy efficiency as compared to general-purpose hardware. However, this tight dependence means that architecture design needs to be refreshed in tandem with changes to the application. For deep learning accelerator architectures, over the years, optimization objectives shifted from supporting GEMMs, to native convolutions <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b21">[21]</ref>, and more recently to attention layers <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b18">[19]</ref>. For each such design cycle, an expensive DSE is needed to be performed repeatedly to find the architecture parameters corresponding to the new optima. A ML model similar to our proposal will significantly reduce the cost of such design iteration by condensing down the search for optimal configurations to constant time.</p><p>In summary, the following are our contributions: • We present systematic, design-aware (Section III), and statistical studies(Section IV-C) on the design and mapping space of custom architectures via three case studies, demonstrating the feasibility of learning these spaces. • We formulate traditional DSE tasks as ML classification/recommendation tasks by "quantizing" the optimization space and structuring the input and the output spaces. (Section IV) This abstraction enables different optimization tasks to be converted into learning problems, solvable by the use of existing ML classification models. • We proposes and evaluate AIRCHITECT (Figure <ref type="figure" target="#fig_0">1</ref>), a custom-designed neural recommendation network capable of learning the DSE space to a high degree of prediction accuracy (Section V). The predicted optimal configurations generated by AIRCHITECT achieve 99.99% of the best achievable performance on a test dataset of 10 5 workloads when used for our case studies.</p><p>The paper is organized as follows. Section II defines each of our case studies and illustrate the complexity of the problem for finding the optima for a single query. In Section III, we generate multiple datapoints for each of the case studies and perform a design-aware analysis to identify underlying patterns across the datapoints which help with learning the space of optimum configurations. In Section IV, we cast the design-optimization as a learning problem, generate a training dataset, and perform statistical analysis. Finally we use both off-the-shelf and our custom designed ML model AIRCHITECT (Section V) to learn the design space and perform predictions. In Section VI we examine the existing literature, before concluding in Section VII. </p><formula xml:id="formula_0">… … … … … … … … … … … …</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. CASE STUDIES</head><p>In this work, we have contrasting requirements of choosing the design goal, which is complex enough to be challenging for naive design approaches but is feasible for human experts to reason about and validate the experimental results. We choose the problem of efficient systolic arrays-based GEMM accelerator design and mapping as our design-target for a ML model to learn. The simple construction and the easy scalability of systolic arrays creates a large design-space controlled via a few input parameters. Furthermore, a large abundance of literature for these structures fosters understanding the design space and examining the outcomes of the learning algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Background</head><p>Systolic Array. Systolic arrays are highly efficient structures for dense matrix multiplication and naturally have been adapted in many commercial and academic proposals for DNN acceleration <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Figure <ref type="figure">2</ref> depicts the schematic template for the target of our design task.</p><p>Dataflow. There are three distinct strategies of mapping compute, aka dataflows, onto the systolic array named Output Stationary (OS), Weight Stationary (WS), and Input Stationary (IS) <ref type="bibr" target="#b7">[8]</ref>. The "stationarity" of a given dataflow is determined by the tensor whose element is not moved (i.e. stationary) for the maximum duration of time throughout the computation. Although many different dataflows exist for spatial arrays <ref type="bibr" target="#b20">[20]</ref>, we only consider true systolic dataflows that only use local (neighbor-to-neighbor) communication in this work. af</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Case Study Description</head><p>In this paper, we target three design-spaces / case studies: -Case Study 1: Array and Dataflow Prediction. This case study focuses on the compute array in Figure <ref type="figure">2</ref>. The array has two sets of design parameters we wish to learn and recommend for a given workload (i.e., DNN layer): (i) shape of the array, determined by choosing the rows and columns, and (ii) dataflow.</p><p>-Case Study 2: Buffer Size Prediction. Given a compute array of a fixed size and shape, this case study focuses on sizing the SRAM buffers around it. As shown in Figure <ref type="figure">2</ref>, the array is supported by three buffers. Two of the buffers prefetch and store the input operands (Input Feature map (IFMAP) and Filter in Convolution neural network terminology <ref type="bibr" target="#b7">[8]</ref>), which are fed into the array from the top or left edges as determined by the choice of dataflow. The third buffer acts as temporary storage for the generated output elements (Output feature map (OFMAP)) or partial sums until they are drained.</p><p>-Case Study 3: Multi-array Scheduling. Given several heterogeneous arrays (with their own fixed sizes and memories), shown later in Figure <ref type="figure">6</ref>, this case study focuses on scheduling multiple independent layers across them.</p><p>We motivate each case study in more detail next. We use SCALE-Sim <ref type="bibr" target="#b29">[30]</ref> and some in house simulation tools to generate the data points (see Section IV).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Compute Design Space</head><p>For our systolic-array-based accelerator, the parameters in the compute design space are the shape of the array, and the mapping strategy or dataflow. In GEMM workloads, owing to the large parallelism and ease of scaling, the more computation elements (multiply-and-accumulate or MAC units) that are allocated to a problem the better. However, just providing the extra compute resources is not enough. The mapping and the shape of the systolic array determine the fraction of these compute resources that are utilized for work. Determining the best array shape and mapping has been widely covered in the existing literature. However, for the sake of our discussion, we perform some simple experiments to depict the complexity of the problem.</p><p>First, we depict the effect of the shape of the array when running the first layer of ResNet-18 <ref type="bibr" target="#b10">[11]</ref> in Figure <ref type="figure">3</ref>. Figure <ref type="figure">3(a)</ref> shows three plots, each for the dataflow Output Stationary (OS), Weight Stationary (WS), and Input Stationary (IS), respectively. In each of these charts, we plot the runtime obtained for running the given layer for different array shapes possible when using 2 9 MAC units, with each array having at least 4 rows or columns. In Figure <ref type="figure">3</ref>(b), we depict the result for a similar analysis on this layer (first layer in ResNet-18) for the various array shapes when 2 15 MAC units are used. First, we observe from this chart is that we can have orders of magnitude in difference in the runtime between a poor configuration and the optimal one. For example, for 2 15 MACs using OS dataflow, using a shape 4 × 8192 takes 100× more cycle than when using the optimal configuration of 512 × 64. Second, we see that for a given shape of the array, the array utilization correlates with better runtimes. This is logical as, the higher the utilization is, the more parallelism can be exploited resulting in higher performance. However, utilization is not a sufficient condition for determining optimality. For example, when using OS dataflow with 2 9 MAC units, all array shapes but 1 has full utilization (8 × 64 to 128 × 4); however the runtimes among the configurations vary with the optimal configuration (16 × 32) taking almost half the time than 128 × 4. When considering the case of 2 15 units and IS dataflow, the optimal configuration (64 × 512) has a lower utilization than 100% obtained by 4 × 8192 and still achieves about a tenth in runtime.</p><p>We note that even with the same workload and dataflow, the optimal shape of the array changes with the design constraint itself. In Figure <ref type="figure">4</ref>, we plot the array shape of the optimal point when the number of MAC units used changes from 2 5 to 2 15 for the first layer and eighth layer of ResNet18 <ref type="bibr" target="#b10">[11]</ref>. The y-axis of the plot depicts the various configurations sorted by aspect ratios (the ratio of rows to columns), thus the axis with fewer rows and more columns (short-fat) are near the x-axis, the ones with comparable rows and cols (balanced) are in the center, while the ones with more rows and fewer columns (tall-skinny) are at the top. We observe that for a small number of MAC units, the balanced array configurations are preferred for both the layers and dataflows. However, as the number of MAC units is changed. the OS mappings prefer a "tall-skinny" configuration for the first layer. The WS mappings initially tend towards a "short-fat" configuration but then lean towards a "tall-skinny" configuration, whereas the IS mapping does no show any such trend in both layers.</p><p>These studies show that, as the workloads and design constraints change, the optimal architecture and mapping configurations vary. However, the optimal architecture parameters do not follow any immediate pattern that can be approximated and used to make fast predictions using simple modeling methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. SRAM Buffer Design Space</head><p>Sizing the memories properly is yet another important problem that architects need to address when building an efficient accelerator. When designing an accelerator, there is typically an upper bound placed on the real estate available for compute, buffering, other components in the chip. For the systolic-array-based accelerator depicted in Figure <ref type="figure">2</ref>, the capacities for buffers holding the three operands need to be allocated out of this budget. It is well known that the relative sizes of the buffers depend on the dimensions of the operands themselves. This task becomes challenging and nonintuitive given the fact that several extraneous factors like mapping/dataflow, other architecture parameters like the array dimensions, external bandwidth, etc also determine the relative sizes.</p><p>We illustrate this by plotting the cost of improper allocation in Figure <ref type="figure" target="#fig_2">5</ref>. In this figure, we show the stall obtained due to improper memory allocation on the second layer of GoogleNet, when running on arrays with 2 10 MAC units with various dataflows and external bandwidth. For this experiment, we only allow the buffer sizes among 128 KB, 512 KB, 1024 KB, and 2048 KB and estimate the stalls encountered. In Figure <ref type="figure" target="#fig_2">5</ref>(a) we depict the stalls encountered when the layer is run on a 32×32 array with OS dataflow, with each of the buffers backed by a 50 words/cycle interface bandwidth link. For this case, we observe that allocating the maximum amount of capacity for the buffer containing Input Feature Map (IFMAP) buffer leads to minimum stalls. The buffer containing Filter operands can be set to the lowest size as all sizes lead to zero stalls, while the rest of the capacity can be allocated for storing the outputs. In Figure <ref type="figure" target="#fig_2">5</ref>(b), we just change the dataflow from OS to IS and notice a significant change in the stall behavior. In this case, the only buffers that account for non-zero stalls are the ones responsible for holding the output buffer elements (ie the Output feature map or OFMAP). Consequently, the optimal sizing allocates minimum buffer sizes for the input matrices and the maximum to the buffer holding OFMAP elements.</p><p>Varying other architecture parameters like the interface bandwidth and the dimensions of the array also change the optima. When the interface bandwidth is changed to 100 words/cycle while keeping the same dataflow (OS) and the array dimensions (32 × 32), we observe that allocating 2048 KB to IFMAP buffer leads to zero stalls from IFMAP transfers, as shown in Figure <ref type="figure" target="#fig_2">5</ref>(c). In this case, given the stalls are then only encountered from the OFMAP buffer, the configuration with 1048 KB allocated to this buffer turns out to be optimum. When using array dimensions of 8×128, the optimal is obtained by allocating 1024 KB for the IFMAP and 2048 KB for the OFMAP lead to the best configuration (see fig Figure <ref type="figure" target="#fig_2">5(d)</ref>). The lack of any apparent pattern and the dependence of the optimal buffer sizing on the workload dimension and architecture parameters limits the alternatives of optimization to iterative search-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Multi Array Scheduling Space</head><p>In our third case study, we examine if the scheduling space of a heterogeneous accelerator can be learned. Although the scheduling problem can come in many forms, for our experiments the specific problem we address of matching a fixed number of GEMM workloads to an equal number of systolic array-based distinct compute units, such that the overall runtime is minimized. Figure <ref type="figure">6</ref> helps describe the problem qualitatively. In this particular example, we have three compute units, one a monolithic 32 × 32 systolic array, a distributed collection of 4 8 × 8 systolic arrays, and a finegrained distributed accelerator with 256 2 × 2 systolic arrays. As evident by the example, the distinct compute units can be a single or a distributed collection of arrays, working on one GEMM workload at a time, For each such unit, the dataflow is tunable at runtime. Hence, the task of the scheduler is to find the match among each workload to the corresponding array, when queried with three workload dimensions, and to chose the dataflows such that the overall runtime is the lowest. This task is equivalent to the collection of several tasks like (i) identifying the workload on the critical path, (ii) finding the best compute unit and corresponding dataflow to run the critical path workload the fastest, (iii) performing the same analysis for the second, third, etc slowest network in case of a tie among various possible scheduling combinations such that sum of time taken on all the compute arrays can be minimized.</p><p>The table in Figure <ref type="figure">7</ref>(a) depicts the number of schedules possible for this simple example.  Fig. <ref type="figure">9</ref>. The pattern of optimal aspect ratio and optimal dataflow obtained for GEMM workloads on systolic arrays with 2 8 to 2 15  MACs.</p><p>of the network on the critical path for all the possible schedules for the first layers in GoogleNet, YoloTiny, and Alexnet. The abrupt variation of runtimes for the neighboring schedules suggests that search is the practical way of determining to find the optima in this search space. Also, among the various workloads, the optimal schedule varies widely. For example, in Figure <ref type="figure">7</ref>(c) we show a similar chart for the critical path runtime of third layers in the previous networks. The optimal schedule, in this case, is the one corresponding to entry 136 of the table in Figure <ref type="figure">7</ref>(a), as compared to entry 10 in the previous figure. As with the previous case studies, predicting an optimal schedule is also challenging due to the lack of any apparent pattern, making it an interesting candidate for learning. III. DESIGN AWARE ANALYSIS In this section, we analyze the datasets for our three design targets from an architect's perspective to understand the design space and to highlight the trends and generalizations in the design space. We call this design aware analysis since we use the knowledge of the design space and the input architecture and workload parameter to understand the observed patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Array shape and dataflow</head><p>In Section II-C, we discussed the complexity of determining the optimal array shape and dataflow for a given GEMM workload and resource constraints. However, we know that human experts can generalize to trim the search space or to find heuristics for making searches quicker with the knowledge acquired from experience. To further understand the design space and identify patterns for the optimum architecture and mapping configurations, we study the optimal design points obtained and we look into the solutions obtained by a search for a handful of workloads.</p><p>First, we examine the space of optimal array dimensions (height and width) obtained in Figure <ref type="figure" target="#fig_5">8</ref> when the design constraint (i.e., the maximum number of MAC units) is fixed to 2 9 . This chart is generated by searching the optimal for 10 4 GEMM workloads, obtained by the sampling the operand dimensions (M, N, and K) from the distribution depicted in Figure <ref type="figure" target="#fig_8">11</ref>(a) (see Section IV for details on dataset generation). In Figure <ref type="figure" target="#fig_5">8</ref>(a) we show the optimal array dimensions for workloads that obtain the best runtime when using OS dataflow. The radius of the marker at each array dimension (row/col) captures the relative frequency at which the said dimension is deemed optimal. Figure <ref type="figure" target="#fig_5">8(b,c</ref>) captures similar information for the workloads for which the optimal dataflow is WS and IS dataflow, respectively.</p><p>In Figure <ref type="figure">9</ref> we capture the relative frequency of optimal array dimensions for different values of maximum MAC units varied from 2 5 to 2 15 . In each 'circle' of the figure, the optimal configurations for each dataflow (similar to the plot in Figure <ref type="figure" target="#fig_5">8</ref>) is combined and presented using a different color. We observe that first the optimal array configurations always use the maximum number of MAC units, which is expected. Another observation, however, is that the optimal array configuration does not show any dependence on the optimal dataflow. Figure <ref type="figure">9</ref> also helps in identifying the underlying pattern by combining the plots. The following observations can be made from this figure. First, we see a clear pattern that, the most frequent array configurations are the ones with are near square shape but have twice as many columns as rows, unlike the conventional approach of choosing square configurations <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b13">[14]</ref>. The square configuration of the array although optimal for a reasonable amount of workloads, does not work for most of them. Second, all of the array configurations are found to be optimal for at least one workload. Thus, none of the possible array dimensions can be summarily rejected from the search space to find the global optimum. Third, there is no apparent pattern that can be used for determining the optimal dataflow given the information about the optimal array configurations.</p><p>To understand the variation of optimal dataflow predicted, we study the design space for the shape of the operand matrices as depicted in Figure <ref type="figure" target="#fig_6">10(a-c</ref>). For example, in Figure <ref type="figure" target="#fig_6">10</ref>(a) the x-axis captures the ratio of height and weight of the first operand (M:K) capturing the shape of this matrix. The y-axis captures all the possible dimensions of the arrays using MAC units that range from 2 5 to 2 15 in powers of 2. Each data point represents the optimal array configuration and dataflow obtained by a GEMM workload, where the dataflow is represented by colors. The plots in Figure <ref type="figure" target="#fig_6">10(b, c</ref>) depict similar distributions wrt the shape of the second operand matrix (K × N) and the output matrix (M × N). One can observe from Figure <ref type="figure" target="#fig_6">10</ref>(a) that distinguishing between OS and WS is possible just from the shape of the first operand matrix. However, determining either over IS dataflow is difficult as there exists a significant overlap. Similarly, Figure <ref type="figure" target="#fig_6">10(c</ref>) depicts that the dimensions of the output operand matrix can help chose between WS and IS, while Figure <ref type="figure" target="#fig_6">10(b)</ref> shows the same for IS over OS depending on the shape of the second operand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. SRAM Buffer Sizing</head><p>Similar to the task of determining optimal array shape and dataflow for a given GEMM workload, given a constrained real estate the task of determining the optimal sizes of on-chip SRAM buffers is also challenging. In Section II-D we discussed the challenges involved in this task for a couple of example use cases. In a similar vein to our analysis in Section III-A, we try to understand if any generalizations or patterns exist in the design space of the on-chip memory buffer. For this study, we consider the optimal buffer dimensions in a systolic array, obtained by searching for 10 4 distinct GEMM workloads running on randomly chosen array dimensions between 2 4 to 2 18 , interface bandwidth between 1 to 100, and dataflow among OS, WS, and IS. For the sake of simplicity, we limit the search space to buffer sizes from 100KB to 900KB, changed in increments of 100KB. The objective of the search algorithm is to find the sizes of the three buffers simultaneously such that the overall stall cycles are minimized, and in case of multiple configurations with the same cost, the one with cumulatively minimum size.</p><p>Figure <ref type="figure" target="#fig_6">10(d-f</ref>) depicts the correlation of the optimal buffer sizes, with the input parameters, interface bandwidth, the size of the workload matrices, and the dataflow. For the sake of clarity in all the figures, we plot the points corresponding to the minimum and maximum obtainable sizes. In Figure <ref type="figure" target="#fig_6">10(d)</ref> we observe that, when using IS dataflow, small IFMAP buffers are the optimum. For WS dataflow, allocating the maximum buffer size appears to be compulsory. This observation makes sense when we consider the reuse patterns. In IS dataflow, the IFMAP matrix has the maximum reuse, whereas the elements of the filter operand matrix are streamed into the array, thus allowing a small buffer for the input to be sufficient. On the other extreme the WS dataflow requires the inputs to be streamed into the array, which requires a larger buffer size to lower the interface bandwidth pressure. Similar observation can be made in Figure <ref type="figure" target="#fig_6">10</ref>(e) where the optimal sizes for the filter operand buffers are depicted. For this buffer, the smaller buffer dimensions are predominant when WS is used since the filter matrix has the maximum reuse. Similarly, larger buffer dimensions are obtained when IS is used. However, for the OS dataflow, where both IFMAP and Filter operands are streamed, larger buffers are favored by the IFMAP operand, while small buffers are common for the Filter operand. This trend can be explained by comparing the distribution of operand sizes of the matrices. The IFMAP matrix sizes vary from 10 8 to 10 9 bytes while the Filter operands vary from 10 5 to 10 7 bytes making the input matrices about two orders of magnitude larger. When allocating from a fixed budget, it is perhaps optimal to allocate maximum memory to the larger matrix. Figure <ref type="figure" target="#fig_6">10</ref>(c) depicts the similar plot for the output buffer. In this chart, we observe that across the dataflow, small buffer sizes are optimum in general. This makes sense by considering the fact the OFMAP buffer does not contribute to extracting operand reuse and instead acts as temporary storage to hide the slow external interface. However, we notice that for large interface bandwidths and relatively small OFMAP sizes, larger OFMAP buffers are preferred. This observation can be explained by the fact that the optimal sizes are determined for all three buffers together. Furthermore, in this dataset we use the same interface bandwidth for all the buffers. When the bandwidths of the input interfaces are comparatively high, while the operand sizes are small (M and N) unless the K dimension is extraordinarily large, small input buffers are sufficient to obtain stall-free or low stall operation. Thus the stalls encountered from data draining contribute to a performance loss. At the same time, given there is a 'surplus' of allowable buffer, the size of the output can be increase to reduce the overall stalls encountered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multi-array Scheduling</head><p>This case study is a mapping problem similar to the optimal dataflow selection shown in Figure <ref type="figure" target="#fig_6">10(a-c</ref>). The patterns observed in Figure <ref type="figure" target="#fig_6">10(a-c</ref>) are almost similar to this case. For the sake of brevity, we directly show the statistical analysis for this case study in Figure <ref type="figure" target="#fig_0">13(e)</ref> IV. LEARNING ARCHITECTURE AND MAPPING SPACE Our analysis in the previous section shows that the systolicarray-based accelerator design and mapping space possesses high-level patterns that indicate that it is perhaps possible to predict design parameters given that the distribution of the data is internalized. In this section, we discuss systematically structuring the learning problem, dataset generation, and briefly perform statistical analysis on the generated datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Design Optimization as Learning Problem</head><p>The first step towards learning the observed patterns is to formulate the search-based optimization problems involved in our case study to machine learning-based regression or classification problem setting. Empirically we found classification tends to be a better fit for learning architecture and mapping optimization. A naive approach for employing classification for predicting architecture parameters is to independently predict each design parameter. However, there are a couple of problems with this approach. First, a separate model needs to be trained for each design parameter, which can get easily get out of hand when dealing with a system with significant complexity. Second, the parameters are often inter-dependent, for example, the sizes of memory buffers in the systolic array, and different models might fail to capture the interdependence if trained separately. Motivated by these factors, we formulate the problem into a recommendation setting, where a bunch of parameters is clubbed into bins, and the trained model is expected to recommend the bin corresponding to the optimal parameters. This formulation helps us leverage the existing classifier models developed by the machine learning community and also helps us reason about the problem systematically.    </p><formula xml:id="formula_1">(a) (b)<label>(c) (d)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dataset Generation</head><p>Case Study 1: Array and Dataflow prediction. For this case study, we want to predict the optimal dimensions of a systolic array and the dataflow, given then design constraints and the dimensions of the GEMM workload. The input space our for this task, therefore comprises of four integers, three for the operand dimensions (M, N, and K), and one integer capturing the maximum number of MAC units allowed. To keep the input space bounded, the MAC units are provided in exponents of 2. We limit the maximum possible MAC units at 2 18 . The workload dimensions are provided as randomly sampled integers from a uniform distribution. The limits of the workload dimensions are determined by the distribution depicted in Figure <ref type="figure" target="#fig_8">11(a)</ref>. This distribution is generated from the layer dimensions of popular convolution neural networks <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b31">[32]</ref> . This distribution dictates that the values for M dimension vary between 1 to 10 5 , N in [1, 10 4 ], and K in [1, 10 3 ]. The output space is a list of labels for this task. As shown in Figure <ref type="figure" target="#fig_9">12</ref>(b) each label serves as the index for a set of parameters, which for this case study denote the systolic array height, width, and one of the dataflows. Keeping with the conventional systolic array designs, we only consider the array dimensions that are powers of 2. In our experiments, we allow the minimum array dimensions to be 2 4 while the maximum dimensions, dictated by the maximum possible MAC units in the input space, come out to be 2 16 . With these limits in place, the output space for our problem contains 459 different array and mapping configurations. To generate the dataset, we use runtime as the cost metric. We use the SCALE-Sim <ref type="bibr" target="#b30">[31]</ref> simulator to generate runtimes for each workload for a given array dimension and workload. We modify the simulator to generate only compute runtime and ignore stalls, speeding up the search. We exhaustively search through all the valid configurations to generate the label. We generated about 5 million data points, which took about a couple of weeks of times when running over several servers using a total of 800 logical CPU cores.</p><p>Case Study 2: Buffer Size prediction. In the second case study, our goal is to predict all optimum sizes of the three operand buffers in the systolic array (see Figure <ref type="figure">2</ref>) simultaneously for given workload parameters, information about the compute, and design constraints.</p><p>The input space, as depicted in Figure <ref type="figure" target="#fig_9">12</ref>(a), captures the maximum memory capacity, workload dimensions (M, N, K), the array dimensions, dataflow, and interface bandwidth of the buffers. For simplicity, we assume the same interface bandwidth for all three buffers. The limit for the maximum memory capacity is set to 3000 KB; the workload dimensions are sampled from the same distribution as the previous case study with limits observed in Figure <ref type="figure" target="#fig_8">11(a)</ref>. The array dimensions and dataflow are sampled from the output space of the previous case study to keep our experiments consistent. The interface bandwidth expressed in bytes/cycles is taken from the space of integers in [1, 100], sampled with uniform probability. The output space, is a list of labels, where each label indexes an entry of memory sizes for each of the buffers. The minimum size of each buffer is restricted to 100 KB, and the allowable sizes increment with a step size of 100 KB as well. The maximum allowable size is 1 MB for an individual buffer. With these constraints, the output space contains 1000 distinct points defining the search space. For this case study, the optimal memory size is the one that leads to a minimum or no stall. For different configurations with the same stalls encountered, the one with the smallest cumulative capacity is considered the best. For this case study as well, we use SCALE-Sim <ref type="bibr" target="#b29">[30]</ref> to generate costs of the various memory sizes. We generate about 5 million datapoints, where a million point takes about a week when multiple runs are launched onto a server cluster totaling 800 logical CPUs.</p><p>Case Study 3: Multi-Array Scheduling. In Section II-E we describe the problem of scheduling a set of GEMM workloads on an equal number of a distinct heterogeneous collection of compute units. The input space, for this problem, is simply the GEMM workload dimensions (M, N, K) one for each compute array. The output space, is an id depicting the mapping of workload to the arrays and the corresponding dataflow to be used as shown in Figure <ref type="figure">7(a)</ref>.</p><p>It is worth noting that our case study concerns the schedules when the number and the dimensions of the arrays are fixed. This case study does not cover the cases where the number of arrays or the workloads changes. However, it is interesting to observe that when the number of arrays changes the possible number of scheduling strategies grows exponentially as well as combinatorially as depicted by the equation N = 3 x × x!, where x is the number of compute arrays.</p><p>Figure <ref type="figure" target="#fig_8">11</ref>(b) depicts the growth of the space with the number of compute units. In this case study, we chose to learn the scheduling space of four arrays, which leads to 1944 possible entries in the output space. The decision to chose this configuration is purely pragmatic, intended to keep the dataset generation times bounded. The different arrays used in the study is shown in Figure <ref type="figure" target="#fig_9">12(c)</ref>.</p><p>Run time is used as the cost function for dataset generation. The schedule which leads to the least runtime of the slowest running workload is deemed to be the winner. For the schedules which have the same critical path runtime, the one with the least cumulative runtime is chosen to be optimal. We create an in-house simulator similar to <ref type="bibr" target="#b29">[30]</ref> to compute the runtime of GEMM workloads on a distributed systolic array setting. Similar to the previous case studies we generate roughly 5 million data points for learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Statistical analysis</head><p>In Section III we observed that the optimal configurations show distinct patterns when analyzed with manually picked parameters. In this section, we perform statistical analysis on the generated datasets to gain additional insights on the ability to learn the distribution.</p><p>First, we analyze the distribution of the categories in the training dataset. In Figure <ref type="figure" target="#fig_0">13(a-c</ref>) we plot the relative frequencies of the label categories for three case studies. The dataset for our first case study (see Figure <ref type="figure" target="#fig_0">13(a)</ref>), which captures the optimal array dimensions and dataflow for given GEMM workloads, we immediately notice that the output distribution is complicated and non-uniform, but structured. We observe that several configurations are represented in the majority while there exist a reasonable amount of configurations that have minuscule representation. This is consistent with the observations we make from Figure <ref type="figure">9</ref>, where we see that highly skewed array dimensions are optimal only for a small number of use cases, leading to lower frequencies. It should be noted however that the input space of the dataset, which comprises the dimensions of the GEMM workload and the max number of MAC units is covered uniformly. We, therefore, decide to keep the dataset unprocessed and do not force explicit class balancing.</p><p>In Figure <ref type="figure" target="#fig_0">13</ref>(b) we show the distribution of categories for the second use case, pertaining to the memory sizing problem. The immediate observation that can be made on this distribution is that for this problem, a handful of configurations work for a wide variety of input configurations, evident by the few classes the dominate the output distribution. The most frequent category accounts for 38% of the output. The two most frequent capture 66%. Continuing, the ten most frequent configurations account for roughly 90% of the output. Beyond this point, other configurations have very few occurrences and are therefore much harder to predict. We expect low prediction accuracy on this dataset due to the high bias in the dataset.</p><p>A similar analysis on the scheduling dataset, depicted in Figure <ref type="figure" target="#fig_0">13</ref>(c) reveals a different pattern. In this case, not only there are a few very prominent configurations, but also a large number of categories with very low frequency. The vast majority of configurations are never optimal and do not occur. We can see that four classes, in particular, dominate the spectrum. Each accounts for roughly 14% of the output totaling 56%. There are eight additional classes that each account for roughly 3% each totaling 24%. The twelve major classes then account for 80% of the optimal configurations. We can take away two key insights from this plot. First, a relatively simple model, which can classify the larger spikes can still provide respectable prediction performance. However, employing a more sophisticated model can significantly boost the performance of the schedule predictor, if it can learn to classify the labels with smaller frequencies.</p><p>Finally, we examine the efficacy of the handcrafted features used in the datasets. In Figure <ref type="figure" target="#fig_0">13(d-e</ref>) we plot a subset of the data points along with the most prominent principal components for the first and the third case study respectively for two randomly chosen labels/categories. The visual separation of the points corresponding to different labels suggest that performing higher dimensional transformation with the chosen input parameters helps to choose a hyper-plane capable of classifying the data points. This chart provides a quality assurance that the datasets are classifiable and as a consequence can be learned, although not formal proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Learning with Existing Classifiers</head><p>The problem formulation discussed in Section IV, allows us to use off-the-shelf classifiers to capture the design space and make predictions about the optimal parameters for given workloads and design constraints. We test out various models with different degrees of complexity on the datasets generated for the three case studies. The table in Figure <ref type="figure" target="#fig_0">14</ref> show these models. To summarize we used the scikit-learn <ref type="bibr" target="#b25">[26]</ref> libraries implementation of support vector classifiers <ref type="bibr" target="#b9">[10]</ref> with linear and radial basis kernel. The state-of-the-art tree boosting method called eXtrement gradient boosting (XGBoost <ref type="bibr" target="#b5">[6]</ref>) available from the xgboost package <ref type="bibr" target="#b33">[34]</ref>. We also implemented four multi-layer perceptron (MLP) networks in TensorFlow's Keras <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b8">[9]</ref>.</p><p>Figure <ref type="figure" target="#fig_0">14</ref> shows the accuracy obtained for the three case studies when 2 × 10 6 datapoints are used for fit/training. For the MLPs, the networks are trained for 15 epochs with 90:10 training-validation split. We used a categorical cross-entropy loss function with accuracy as the optimization objective. Among the various case studies, the one for memory size prediction is learned well among all the models, with MLP-B attaining about 63% validation accuracy. Support Vector Classification, however, was able to perform the best among the models considered attaining about 50% test accuracy for case study 1, 60% and 40% for the other two respectively. V. AIRCHITECT: DESIGN AND ANALYSIS From our analysis in the previous section Section IV-D, we notice that although off-the-shelf classifiers are capable of learning the design space for our various use cases no single model appears to perform consistently across the different spaces. We design a general network structure, which we call AIRCHITECT, intending to obtain the capability to achieve reasonable learning performance across different distributions. Figure <ref type="figure" target="#fig_0">1</ref> depicts the general structure of our proposed model. The inspiration for this design comes from the structure of the modern recommendation networks like DLRM <ref type="bibr" target="#b23">[24]</ref>, which perform recommendation by mapping query inputs onto a trained embedding space, followed by MLP based classification. The trained embedding map the input data from the user-defined input space onto a latent space, which immensely improves the performance of the classifiers. This is also evident from the performance of MLP-B vs AIRCHITECT as depicted in Figure <ref type="figure" target="#fig_0">14</ref>. Among the various case studies, the number of inputs and outputs are the parameters that we changed. The number of inputs to the network is equal to the input space, while the network generates a one-hot vector of length equal to the output space of the problem indicating the optimal design or mapping parameters, as discussed in Section IV-B, Figure <ref type="figure" target="#fig_9">12</ref>. We use an embedding size of 16 and a 256-node MLP hidden layer for our models across the case studies.</p><p>For all three use cases we train the corresponding versions of AIRCHITECT on the respective datasets with 4.5M points (Section IV) using 90:10 train-validation split. We use TensorFlow's Keras library <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b8">[9]</ref> to implement the network and train using categorical-cross-entropy as the loss function, with accuracy as the optimization metric. Figure <ref type="figure" target="#fig_2">15(a-c</ref>) shows the accuracy obtained during training for the three case studies respectively. We observe that the design space of case study 1 is learned with a high validation accuracy (&gt; 94%) in about 15 epochs. The training for case study 2 finishes, in about 22 epochs achieving a validation accuracy of 74% before starting to overfit. For case study 3, the network saturates at about 15 epochs at about 76% validation accuracy. As we see in Figure <ref type="figure" target="#fig_0">14</ref>, AIRCHITECT beats the best performing off-the-shelf classifiers at least by about 10% accuracy.</p><p>To gain further insights on the quality of training, we plot the distribution of the actual labels and the predicted configuration IDs for 10 5 previously unseen test data points for each case study. Figure <ref type="figure" target="#fig_2">15(d-f</ref>) shows predicted vs actual distribution for the three case studies. The first observation is that the test datasets' actual distribution closely matches with distributions of the original dataset, shown in Figure <ref type="figure" target="#fig_0">13(a-c</ref>) corroborating that the test set does not compromise on generality. Second, visually the predicted distribution for case study 1 almost perfectly matches the actual distribution, confirming that the network learned the design space. Third, we observe that the networks for case study 2 and 3, learn to predict the configurations with significant representation on the dataset, while the configuration IDs with low statistical probability is ignored as noise. The presence of large spikes in the case of memory size prediction biases the model, shown by the high frequencies of the top two configuration IDs (Figure <ref type="figure" target="#fig_2">15</ref>(e)) leading to relatively low accuracy. Similarly, for case study 3,Figure <ref type="figure" target="#fig_2">15</ref>(f) shows that the model was successful in learning the distribution of the large spikes. However, in doing so it ignored the configurations with lower frequencies, which in turn cumulatively lowered the accuracy figure. However, it is worth noting that the model ignoring the lower frequencies as statistical noise demonstrates that the model is robust and does not overfit for any of the use cases. Further improving prediction accuracy for different design spaces requires data engineering on a case-by-case basis and therefore, is out of the scope of this paper.</p><p>To understand the cost of misprediction within the use cases, we compute the performance (reciprocal of runtime) of the workloads in the test dataset, for the predicted and label configurations. In Figure <ref type="figure" target="#fig_2">15</ref>(g) we show the performance of the predicted configurations normalized to the labels for the 10 5 datapoints. Due to the high prediction accuracy, we observe that only a few data points have catastrophic performance losses (¡20% of the optimal) leading to a 99.99% average performance (Geometric Mean) of the best possible. In Figure <ref type="figure" target="#fig_0">16</ref> we depict the performance of the network on some layers from networks like FasterRCNN <ref type="bibr" target="#b28">[29]</ref>, GoogleNet <ref type="bibr" target="#b31">[32]</ref>, Alexnet <ref type="bibr" target="#b17">[18]</ref>, MobileNet <ref type="bibr" target="#b11">[12]</ref> and ResNet-18 <ref type="bibr" target="#b10">[11]</ref>. The layers of any of these</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. General structure of the proposed class of recommendation neural networks (AIRCHITECT).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig.3. Variation of runtime and utilization (red dotted line) when using different array shapes and dataflows for (a) 29 MAC units, and (b) 215 MAC units for first layer in ResNet18</figDesc><graphic url="image-1.png" coords="3,479.47,128.70,82.38,62.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Variation of stalls encountered due to memory limitation vs memory size categorized per buffer type for GoogleNet's [32] second layer when using (a) OS dataflow, 32x32 array, and interface BW of 50 bytes per cycle; (b) IS dataflow, 32x32 array, 50 bytes/cycle BW (c) OS dataflow, 32x32 array, 100 bytes/cycle BW (d) OS dataflow, 8x128 array, 50 bytes/cycle BW</figDesc><graphic url="image-15.png" coords="4,175.45,143.84,124.40,93.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. Schematic description of the multi-array scheduling case study</figDesc><graphic url="image-22.png" coords="5,312.16,170.02,250.79,214.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure 7(b) depicts the runtime</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Relative frequency of optimal array dimensions obtained for GEMM workloads using (a) Output Stationary (b) Weight Stationary, and (c) Input Stationary dataflows</figDesc><graphic url="image-19.png" coords="5,311.98,51.77,89.50,65.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 .</head><label>10</label><figDesc>Fig.10. The correlation of optimal array dimension obtained (y-axis) and the matrix shape in terms of aspect ratio (x-axis) of (a) Input operand matrix (M×K) (IFMAP), (b) input operand matrix (K×N) (Filter), and (c) Output matrix (M×N) (OFMAP). The colors of the markers indicate the optimal dataflow obtained, highlighting the pattern in the design space1. The relation of optimal buffer sizes of (d) IFMAP operand buffer, (e) Filter operand buffer, and (f) OFMAP buffer, with operand sizes, interface bandwidth, and dataflow.</figDesc><graphic url="image-24.png" coords="7,250.04,177.50,127.78,122.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. (a) Chart showing the distribution of operand matrix dimensions for GEMM operations involved in layers of popular neural networks (b) Growth of the scheduling space</figDesc><graphic url="image-29.png" coords="8,48.96,53.35,132.76,104.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. (a) Input space size and input parameters for the three case studies; Output space of (b) Systolic Array dimension and dataflow prediction case study (c) Memory buffer size prediction case study (d) Distributed array schedule prediction case study</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 13 .Fig. 14 .</head><label>1314</label><figDesc>Fig. 13. Distribution of the configuration ids for (a) Case Study 1, (b) Case Study 2, (c) Case Study 3. Distribution of datapoints along the most relevant principal component of two configurations for (d) Case Study 1, and (e) Case Study 3.</figDesc><graphic url="image-33.png" coords="10,265.38,59.05,123.02,80.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Case Study 1 Case Study 2 Case Study 3</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="3">Input Space Dimensions</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Parameters</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">Maximum MAC Units, M, N, K</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>8</cell><cell cols="15">Buffer Size Limit (KB), M, N, K, Array Rows, Array Cols, Dataflow, Interface Bandwidth</cell></row><row><cell></cell><cell></cell><cell></cell><cell>12</cell><cell cols="2">M WL0</cell><cell>, N</cell><cell>WL0</cell><cell>, K</cell><cell>, M WL0</cell><cell>, N WL1</cell><cell>, K WL1</cell><cell cols="2">, M WL1</cell><cell>WL2</cell><cell>, N</cell><cell>WL2</cell><cell>,K</cell><cell>WL2</cell><cell>, M</cell><cell>, N WL3</cell><cell>WL3</cell><cell>, K WL3</cell></row><row><cell>Config ID</cell><cell cols="3">Array Rows Array Cols</cell><cell>Dataflow</cell><cell cols="4">Config ID</cell><cell cols="4">IFMAP Buffer KB</cell><cell cols="6">Filter Buffer KB OFMAP Buffer KB</cell></row><row><cell>0</cell><cell></cell><cell>4</cell><cell>4</cell><cell>OS</cell><cell></cell><cell></cell><cell cols="2">0</cell><cell></cell><cell>100</cell><cell></cell><cell></cell><cell></cell><cell cols="3">100</cell><cell></cell><cell>100</cell></row><row><cell>1</cell><cell></cell><cell>4</cell><cell>4</cell><cell>WS</cell><cell></cell><cell></cell><cell cols="2">1</cell><cell></cell><cell>100</cell><cell></cell><cell></cell><cell></cell><cell cols="3">100</cell><cell></cell><cell>200</cell></row><row><cell>2</cell><cell></cell><cell>4</cell><cell>4</cell><cell>IS</cell><cell></cell><cell></cell><cell cols="2">2</cell><cell></cell><cell>100</cell><cell></cell><cell></cell><cell></cell><cell cols="3">100</cell><cell></cell><cell>300</cell></row><row><cell>3</cell><cell></cell><cell>8</cell><cell>4</cell><cell>OS</cell><cell></cell><cell></cell><cell cols="2">3</cell><cell></cell><cell>100</cell><cell></cell><cell></cell><cell></cell><cell cols="3">100</cell><cell></cell><cell>400</cell></row><row><cell></cell><cell></cell><cell>…</cell><cell>…</cell><cell>…</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">…</cell><cell></cell><cell></cell><cell></cell><cell cols="2">…</cell><cell></cell><cell>…</cell></row><row><cell>458</cell><cell></cell><cell>2 18</cell><cell>4</cell><cell>IS</cell><cell></cell><cell cols="2">999</cell><cell></cell><cell></cell><cell>1000</cell><cell></cell><cell></cell><cell></cell><cell cols="3">1000</cell><cell></cell><cell>1000</cell></row><row><cell></cell><cell></cell><cell cols="2">Compute Array 0</cell><cell cols="6">Compute Array 1</cell><cell cols="6">Compute Array 2</cell><cell></cell><cell></cell><cell>Compute Array 3</cell></row><row><cell cols="2">Config ID</cell><cell cols="17">Workload ID Dataflow Workload ID Dataflow Workload ID Dataflow Workload ID Dataflow</cell></row><row><cell></cell><cell>0</cell><cell>0</cell><cell>OS</cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell cols="2">OS</cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell>OS</cell><cell></cell><cell></cell><cell></cell><cell>3</cell><cell>OS</cell></row><row><cell></cell><cell>1</cell><cell>0</cell><cell>OS</cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell cols="2">OS</cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell>OS</cell><cell></cell><cell></cell><cell></cell><cell>3</cell><cell>WS</cell></row><row><cell></cell><cell>2</cell><cell>0</cell><cell>OS</cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell cols="2">OS</cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell>OS</cell><cell></cell><cell></cell><cell></cell><cell>3</cell><cell>IS</cell></row><row><cell></cell><cell>3</cell><cell>0</cell><cell>OS</cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell cols="2">OS</cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell cols="2">WS</cell><cell></cell><cell></cell><cell>3</cell><cell>OS</cell></row><row><cell></cell><cell></cell><cell>…</cell><cell>…</cell><cell>…</cell><cell></cell><cell></cell><cell></cell><cell cols="2">…</cell><cell cols="2">…</cell><cell></cell><cell></cell><cell cols="2">…</cell><cell></cell><cell></cell><cell>…</cell><cell>…</cell></row><row><cell cols="2">1943</cell><cell>3</cell><cell>IS</cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell cols="2">IS</cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell>IS</cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>IS</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RELATED WORKS</head><p>ML for Architecture search: Apollo <ref type="bibr" target="#b34">[35]</ref> is a recent work from Google, targeting sample efficient searching through the accelerator design space using reinforcement learning. Gamma <ref type="bibr" target="#b16">[17]</ref> and ConfuciuX <ref type="bibr" target="#b15">[16]</ref> are similar ML based architecture mapping and design space configuration search methods which use genetic algorithm and reinforcement learning (RL) respectively. AutoTVM <ref type="bibr" target="#b6">[7]</ref> use ML model for cost prediction to improve fast mapping search during compile time.</p><p>ML for EDA: Recently there has been a significant push toward automating place-and-route using machine learning. Mirhoseni et al <ref type="bibr" target="#b19">[23]</ref> use RL for task placement on a heterogeneous system. Wang et al <ref type="bibr" target="#b32">[33]</ref> use GCN and RL for automatic transistor sizing. NVCell <ref type="bibr" target="#b27">[28]</ref> is a RL based proposal from Nvidia to automate standard cell placement. Nautilus <ref type="bibr" target="#b24">[25]</ref> uses genetic algorithm to improve FPGA place and route. Kwon et al <ref type="bibr" target="#b22">[22]</ref>, use online tensor-based recommender systems to aid place and route in chip design. To the best of our knowledge this work is the first work, which truly learns the architecture design and mapping space and "generalizes" to predict the optimal configuration in constant time without the requirement of searching the design space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>This paper presents AIRCHITECT, a recommendation neural network for learning the architecture design and mapping space of systolic array based accelerators. To the best of our knowledge is the first work to show that architecture design space is learneable. Our approach significantly reduces the cost of design space exploration, by replacing expensive search with constant time recommendation of optima. The paper shows a systematic study of the design and mapping space in the context of design aware and statistical analysis, demonstrates the formulation of conventional search problem in to a machine learning problem, and finally proposes a model which outperform the existing off-the-shelf ML models to capture the mapping and design space by significant margin.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>networks were not part of the training or validation dataset, but the model is able to predict the optimal array shapes and dataflow, when queried with a constraint of 2 10 MAC unit limit. Interestingly for case study 3, which had relatively low accuracy, for most cases, the performance does not lead to catastrophic losses. Figure <ref type="figure">15</ref>(h) shows that among the mispredictions, most of the points suffer about 10% to 15% loss compared to the best achievable, leading to an average of 99.1% of the best possible runtime on average (GeoMean).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="http://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf" />
		<title level="m">Nvidia tesla v100 gpu architecture</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Xilinx ML Suite</title>
		<ptr target="https://github.com/Xilinx/ml-suite" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="http://tensorflow.org/" />
		<title level="m">TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>software available from tensorflow.org. [Online</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Reinforcement Learning and Adaptive Sampling for Optimized DNN Compilation</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Ahn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12799</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Taking AI to the edge: Google&apos;s TPU now comes in a makerfriendly package</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Spectrum</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="16" to="17" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining</title>
				<meeting>the 22nd acm sigkdd international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to optimize tensor programs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3389" to="3400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Eyeriss: A spatial architecture for energy-efficient dataflow for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="367" to="379" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Keras</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Centaur: A Chiplet-based, Hybrid Sparse-Dense Accelerator for Personalized Recommendations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="968" to="981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Borchers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture</title>
				<meeting>the 44th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An Approach for Effective Design Space Exploration</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Schulte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Computer Software. Modeling, Development, and Verification of Adaptive Systems</title>
				<editor>
			<persName><forename type="first">R</forename><surname>Calinescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Jackson</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Berlin Heidelberg</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="33" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Confuciux: Autonomous hardware resource assignment for dnn accelerators using reinforcement learning</title>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="622" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">GAMMA: automating the HW mapping of DNN models on accelerators via genetic algorithm</title>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/ACM International Conference On Computer Aided Design (ICCAD)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Why systolic architectures</title>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Kung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="46" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Device placement opti-</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Understanding reuse, performance, and hardware cost of dnn dataflow: A data-centric approach</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chatarasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pellauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<meeting>the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="754" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Maeri: Enabling flexible dataflow mapping over dnn accelerators via reconfigurable interconnects</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Samajdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="461" to="475" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A learning-based recommender system for autotuning design fiows of industrial high-performance processors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Carloni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04972</idno>
	</analytic>
	<monogr>
		<title level="m">2019 56th ACM/IEEE Design Automation Conference (DAC)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2019. 2017</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>mization with reinforcement learning</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep learning recommendation model for personalization and recommendation systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Naumov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><forename type="middle">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sundaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Azzolini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00091</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Nautilus: Fast automated ip design space search using guided genetic algorithms</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Papamichael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Design Automation Conference</title>
				<meeting>the 52nd Annual Design Automation Conference</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine Learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A case for efficient accelerator design space exploration via bayesian optimization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Reagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Adolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gelbart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Whatmough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Nvcell: Standard cell layout in advanced technology nodes with reinforcement learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fojtik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Khailany</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.07044</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A Systematic Methodology for Characterizing Scalability of DNN Accelerators using SCALE-Sim</title>
		<author>
			<persName><forename type="first">A</forename><surname>Samajdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Whatmough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Symposium on Performance Analysis of Systems and Software(ISPASS)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Samajdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Whatmough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02883</idno>
		<title level="m">Scale-sim: Systolic CNN Accelerator Simulator</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Gcn-rl circuit designer: Transferable transistor sizing with graph neural networks and reinforcement learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 57th ACM/IEEE Design Automation Conference (DAC)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">XGBoost Documentation</title>
		<ptr target="https://xgboost.readthedocs.io/en/latest/index.html,version1.3.3" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Apollo: Transferable architecture exploration</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yazdanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Angermueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Akin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Narayanaswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laudon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.01723</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
