<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">In Defense of Core-set: A Density-aware Core-set Selection for Active Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-06-10">10 Jun 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yeachan</forename><surname>Kim</surname></persName>
							<email>yeachan@deargen.me</email>
						</author>
						<author>
							<persName><forename type="first">Bonggun</forename><surname>Shin</surname></persName>
							<email>bonggun.shin@deargen.me</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Deargen Inc</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>Deargen, Inc. Atlanta</settlement>
									<country>USA, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">KDD &apos;22</orgName>
								<address>
									<addrLine>August 14-18</addrLine>
									<postCode>2022</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">In Defense of Core-set: A Density-aware Core-set Selection for Active Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-06-10">10 Jun 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3534678.3539476</idno>
					<idno type="arXiv">arXiv:2206.04838v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Active Learning, Efficient Deep Learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Active learning enables the efficient construction of a labeled dataset by labeling informative samples from an unlabeled dataset. In a real-world active learning scenario, considering the diversity of the selected samples is crucial because many redundant or highly similar samples exist. Core-set approach is the promising diversitybased method selecting diverse samples based on the distance between samples. However, the approach poorly performs compared to the uncertainty-based approaches that select the most difficult samples where neural models reveal low confidence. In this work, we analyze the feature space through the lens of the density and, interestingly, observe that locally sparse regions tend to have more informative samples than dense regions. Motivated by our analysis, we empower the core-set approach with the density-awareness and propose a density-aware core-set (DACS). The strategy is to estimate the density of the unlabeled samples and select diverse samples mainly from sparse regions. To reduce the computational bottlenecks in estimating the density, we also introduce a new density approximation based on locality-sensitive hashing. Experimental results clearly demonstrate the efficacy of DACS in both classification and regression tasks and specifically show that DACS can produce state-of-the-art performance in a practical scenario. Since DACS is weakly dependent on neural architectures, we present a simple yet effective combination method to show that the existing methods can be beneficially combined with DACS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Computing methodologies ? Active learning settings; Neural networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>While deep neural networks (DNNs) have significantly advanced in recent years, collecting labeled datasets, which is the driving force of DNNs, is still laborious and expensive. This is more evident in complex tasks requiring expert knowledge for labeling. Active learning (AL) <ref type="bibr" target="#b7">[9]</ref> is a powerful technique that can economically construct a dataset. Instead of labeling arbitrary samples, AL seeks to label the specific samples that can lead to the greatest performance improvement. AL has substantially minimized the labeling costs in various fields, such as image processing <ref type="bibr" target="#b37">[39]</ref>, NLP <ref type="bibr" target="#b13">[15]</ref>, recommender systems <ref type="bibr" target="#b3">[5]</ref>, and robotics <ref type="bibr" target="#b4">[6]</ref>.</p><p>Recent AL approaches are categorized into two classes: uncertainty-based and diversity-based approaches. The former literally estimates the uncertainty of the samples through the lens of loss <ref type="bibr" target="#b41">[43]</ref>, predictive variance <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b20">22]</ref>, and information entropy <ref type="bibr" target="#b17">[19]</ref>. However, the selection of duplicate or very similar samples is a well-known weakness of this approach. The latter approach selects diverse samples that can cover the entire feature space by considering the distance between samples <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b32">34]</ref>. Although this approach can sidestep the selection of duplicate samples by pursuing diversity, it can be suboptimal due to the unawareness of the informativeness of the selected samples.</p><p>Core-set <ref type="bibr" target="#b32">[34]</ref> is one of the most promising approaches in diversitybased methods. It selects diverse samples so that a model trained on the selected samples can achieve performance gains that are competitive with that of a model trained on the remaining data points. The importance of the method can be found in a real-world scenario where there are plenty of redundant or highly similar samples. However, the core-set approach often poorly performs compared to the uncertainty-based methods. One susceptible factor is the selection area over the feature space because the core-set equally treats all samples even though each unlabeled sample has different levels of importance and influence when used to train a model <ref type="bibr" target="#b30">[32]</ref>.</p><p>In this work, we analyze the feature space of neural models through the lens of the local density and informativeness (i.e., information entropy, model loss). Interestingly, we find that samples in locally sparse regions are highly uncertain compared to samples in dense regions. Based on this analysis, we propose a densityaware core-set (DACS) which estimates the local density of the samples and selects the diverse samples mainly from the sparse regions. Unfortunately, estimating the density for all samples can lead to computational bottlenecks due to the high dimensionality of feature vectors and a large number of unlabeled samples. To circumvent these bottlenecks, we introduce a density approximation based on locality-sensitive hashing <ref type="bibr" target="#b29">[31]</ref> to the features obtained from a low-dimensional auxiliary classifier. Note that DACS is task-agnostic and weakly dependent on neural network architecture, revealing that DACS can be favorably combined with any uncertainty-based methods. We thus present a simple yet effective combination method to encourage existing methods to benefit from our work. We evaluate the effectiveness and the general applicability of DACS on both a classification task (image classification) and a regression task (drug and protein interaction). Comprehensive results and in-depth analysis demonstrate our hypothesis that sampling from the sparse regions is strongly contributed to the superior performance. Moreover, we show that DACS can consistently reach a stable and strong performance in a simulated real-world scenario where highly similar samples exist. In summary, our major contributions include followings:</p><p>? We propose a novel density-aware core-set method for AL with the analysis of the feature space, which has a novel viewpoint to the diversity-based approach. To circumvent computational bottlenecks, we also propose a new density approximation method. ? We introduce an effective method for combining DACS with other uncertainty-based methods. Once combined, DACS can work synergistically with other methods, resulting in substantially improved performance. ? The proposed method significantly improves the performance of the core-set and outperforms strong baselines in both classification and regression tasks. Surprisingly, we also find that DACS selects informative samples fairly well when compared with uncertainty-based methods, even though informativeness is not explicitly considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM SETUP FOR ACTIVE LEARNING</head><p>The objective of active learning is to obtain a certain level of prediction accuracy with the least amount of budgets for constructing a dataset. The setup consists of the unlabeled dataset (? ? , ) ? U, the labeled dataset (? ? , ? ? ) ? L, and the neural model M parameterized by ?. In the image classification case, ? ? and ? ? are the input image and its corresponding label, respectively. We define an acquisition function ? (?) that returns the most informative or diverse samples within the limited budget as follows:</p><formula xml:id="formula_0">S = {? 1 , ? 2 , ..., ? ? } = ? (U; M, ?) (1)</formula><p>where S is the selected subset with the query budget ?. After querying the subset to an oracle for its label, we continue to train the model M on the combined labeled dataset (i.e., L ? L ? S). The above process is cyclically performed until the query budget is exhausted. To denote each cycle, we add a subscript ? to both labeled and unlabeled datasets. For example, the initial labeled and unlabeled datasets are L 0 and U 0 , respectively, and the datasets after ? cycles are denoted as L ? and U ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">UNCERTAIN REGIONS ON FEATURE SPACE</head><p>The core-set approach selects diverse samples over the entire feature space (i.e., all unlabeled samples) even though each sample has a different level of influence on the training <ref type="bibr" target="#b30">[32]</ref>. Therefore, if the coreset method can be aware of the informative regions, the method could achieve both uncertainty and diversity at the same time. To this end, we characterize the feature space through the lens of the local density and analyze which density regions are closely related to the informativeness. We quantify the informativeness of unlabeled samples as prediction entropy and loss, which are the popular uncertainty measures, and the density is estimated by averaging the distance between the 20 nearest-neighbor samples' features.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> presents the correlation plots between the estimated density and the uncertainty measures and 2-d visualization of the feature vectors with their density<ref type="foot" target="#foot_0">1</ref> . As can be seen from the correlation plots, the density has a negative correlation with the uncertainty measures, and its negative correlation with information entropy is especially strong. In other words, the samples in sparse regions tend to have more information than the samples in dense regions. We also observe that samples in the highly dense regions (Figure <ref type="figure" target="#fig_2">1d</ref>) are clustered well by their labels (Figure <ref type="figure" target="#fig_2">1c</ref>) and, by contrast, the sparse regions include a number of samples that are confusing to the classifier (i.e., not clustered and mixed with other labels). A comprehensive analysis shows that the sparse regions are more informative and uncertain, suggesting that the acquisition should be focused on the locally sparse regions.  The superiority of the sparse region can be explained to some extent by the cluster assumption. Under the assumption that states the decision boundary lies in low density regions <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b31">33]</ref>, samples in sparse regions can be treated as being near the decision boundary. The near samples of the boundary have high prediction entropy and loss <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b18">20]</ref>, which is similar property to samples in sparse regions, indicating that the sparse regions are closely related to the decision boundary. Furthermore, by following the above assumption, samples in dense regions can be regarded as samples in close vicinity to a cluster where the neural models reveal low entropy. This suggests that selecting samples from sparse regions is more effective than selecting samples from dense regions when constructing the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DENSITY-AWARE CORE-SET FOR DIVERSITY-BASED ACTIVE LEARNING</head><p>This section details the proposed method, coined density-aware core-set (DACS), that enables the core-set approach to select diverse but informative samples. DACS begins by estimating the local density of the unlabeled samples (Section 4.1). Afterward, DACS selects diverse samples from the density-estimated regions such that the samples in the sparse regions are mainly selected (Section 4.2). The naive method to estimate the local density is to use ?-nearest neighbors. In this method, the density is simply calculated by the average distance to nearest k samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Density Estimation for Unlabeled Samples</head><formula xml:id="formula_1">? (? ? , ?) = 1 ? ?? ? ?? ? (? ? ,?) ? (? ? , ? ? )<label>(2)</label></formula><p>where ? ? (? ? , ?) is the function that returns ? nearest samples from the sample ? ? , ? is the distance measure (e.g., euclidean distance, angle distance) between two samples that are typically represented as intermediate features <ref type="bibr" target="#b32">[34]</ref>. However, there are two major computational bottlenecks in ?-? ? -based density estimation. The first bottleneck is the large number of unlabeled samples in active learning. To estimate the density of each sample, ? ? (?) should calculate the distance ? to all unlabeled samples. The second factor is the high dimensionality of the features of each sample in neural networks, which influences the distance calculation ? between samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Efficient density estimation.</head><p>To circumvent the above computational bottlenecks in estimating the density, we introduce the hashing-based estimation with the auxiliary training in which the low-dimensional vectors are trained to be compatible to the high-dimensional ones. Auxiliary Training. To sidestep using the high-dimensional vectors, we carefully add an auxiliary classifier to the existing architectures. The classifier has two distinct properties compared to the existing classifier. First, it consists of the low-dimensional layers to the extent that it does not hurt the accuracy. Second, the feature vectors are normalized during the training to encourage the vectors to be more separable and compact in the feature space <ref type="bibr" target="#b1">[3]</ref>.</p><p>The auxiliary classifier takes the large features of the existing network as input. Then, the input vectors are transformed to the low-dimensional normalized vectors.</p><formula xml:id="formula_2">?? = ? ? ? ? ? ||? ? ? ? ? || 2 2 (3)</formula><p>where ? ? is the large features of the existing networks, ? ? ? R ??? ? is a learnable weight matrix, ? and ? ? are the dimensionality of original and normalized vectors, respectively, and ? ? ? ?. From the viewpoint of the large feature vector ? ? , the loss function in the classification case is defined as:</p><formula xml:id="formula_3">?(? ? ; ? ? ) = - 1 |L ? | ?? (? ? ,? ? ) ? L ? ? ? ? ???? (? ? ; ? ? ) (4)</formula><p>where ? is the ground-truth label and ? is the predicted probability given feature vectors ? ? and the model parameters ? ? . The overall loss function with the auxiliary training can be represented as follows:</p><formula xml:id="formula_4">? ????? = ?(? ? ; ? ? ) + ? ? ?( ?? ; ? ? ? ?? ) (<label>5</label></formula><formula xml:id="formula_5">)</formula><p>where ? is the control factor of the normalized training, and ?? is the additional parameters of the auxiliary classifier. As the training with auxiliary classifier might hurt the performance of the main classifier, we prevent the gradient flow between the main and auxiliary classifier after specified training epochs (see Section 5.2.2 for more information). In the acquisition phase, we use the lowdimensional normalized vectors instead of large features.</p><p>Hashing-based Density Estimation Auxiliary training results in computationally efficient and well-separated vectors for unlabeled samples. However, the large number of unlabeled samples is still a potential bottleneck to find nearest neighbors (i.e., ? ? (?) in Eq. ( <ref type="formula" target="#formula_1">2</ref>)), which is the necessary process to estimate the density. Locality-sensitive hashing (LSH) has been adopted to address the computational bottlenecks of neural networks <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b21">23]</ref>. The LSH scheme assigns the nearby vectors into the same hash with high probability. To reduce the bottleneck of a large number of samples in estimating the density, the samples are hashed into ? different buckets, and finding nearest neighbors in each bucket instead of the entire dataset enables the efficient estimation for the density. To obtain ? buckets, we apply a fixed random rotation ? ? R ? ? ? ? 2 to the feature vectors and define the hashed bucket of ? ? as follows:</p><formula xml:id="formula_6">? ? = arg max([? ? ?? ; -? ? ?? ])<label>(6)</label></formula><p>where [?; ?] is the concatenation of two vectors. ? ? indicates the bucket number of the sample ? ? . The above hashing process assigns a different number of samples to each bucket, preventing batch computation. For batch processing, the samples are sorted by corresponding bucket numbers, and the sorted samples are then sequentially grouped by the same size. Formally, the bucket containing ?-th samples in the sorted order is defined as follows:</p><formula xml:id="formula_7">B (?) = { ? | ? ? ? ? -1 ? ? ? ? ? ? ? ? ? ?} (<label>7</label></formula><formula xml:id="formula_8">)</formula><p>where ? is the size of buckets (i.e., ? =</p><formula xml:id="formula_9">| U ? | ? ).</formula><p>Within each bucket, the density is estimated by calculating the weighted cosine similarity as follows:</p><formula xml:id="formula_10">? ? = ?? ? ?B (?)/? ? (???? ? ? ) ? ???? ? ? = ?? ? ?B (?)/? ? ( ?? ? ?? ) ? ?? ? ?? (? || ?? || = || ?? || = 1)<label>(8)</label></formula><p>where ? (?) is the sigmoid function, and ? ? ? is the angle between ?? and ?? . To favor near samples while reducing the effect of distant samples, sigmoid weights are applied to the similarity. Since the sizes of all buckets are the same as ???, Eq. 8 can be viewed as calculating the similarity between fixed ???-nearest neighbor samples, and the estimates are comparable across different buckets. This naturally makes the samples in the dense region have higher estimates than that of the sparse region because the samples in the dense have the more close samples in each bucket. </p><formula xml:id="formula_11">L ? +1 ? L ? ? S 14: U ? +1 ? U ? ? S ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Density-aware Core-set Selection</head><p>Based on the efficiently estimated density, we select core-set samples in which the samples in the sparse regions are more favorably selected. To this end, we first divide the unlabeled samples into dense or sparse regions by performing Jenks natural breaks optimization<ref type="foot" target="#foot_1">2</ref>  <ref type="bibr" target="#b16">[18]</ref> to the estimated densities, resulting in ? different groups which are clustered by the density, and these groups are denoted as follows:</p><formula xml:id="formula_12">C = {C 1 , C 2 , ..., C ? } where C ? = {? ? 1 , ? ? 2 , ..., ? ? | C ? | }<label>(9)</label></formula><p>where ? ? ? is the ?-th sample in the cluster C ? . Over the different density clusters, we perform k-center greedy <ref type="bibr" target="#b38">[40]</ref> to select diverse sample. As the entire feature space is divided into ? regions (i.e., from dense to sparse regions), the k-center selection in the core-set <ref type="bibr" target="#b32">[34]</ref> is also decomposed with the same number of clusters. The number of centers in decomposed greedy selection are determined by inverse proportion of the size of the cluster C ? because the groups clustered by high density tend to occupy more data than the groups with relatively low density. Such strategy enables to select more samples from the sparse regions and the selection ratio can be defined as:</p><formula xml:id="formula_13">? ? = softmax((1 - |C ? | |U ? | )/?) (<label>10</label></formula><formula xml:id="formula_14">)</formula><p>where ? is a temperature that controls the sharpness of the distribution. The detailed process of density-aware core-set is described in Algorithm 1. Note that we replace the euclidean distance with the cosine similarity since the feature are normalized in the auxiliary training. The comprehensively selected subset from the method is represented as follows:</p><formula xml:id="formula_15">S = ? 1 ? ... ? ? ? where ? ? = ? ? (C ? ; M, ?? ? ? ??)<label>(11)</label></formula><p>where ? ? (?) is the core-set-based acquisition function in cluster C ? .</p><p>After selecting the subset S, we query the subset to the oracle for its labels and perform the next cycle of the active learning on the updated dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Combination with Uncertainty-based Selection Methods</head><p>DACS can be complementarily combined with uncertainty-based methods because the proposed method naturally sidesteps the selection of duplicate samples. We take a "expand and squeeze" strategy to combine DACS with the uncertainty-based method. Specifically, DACS pre-selects ? times more samples than the query budget ? as query candidates. Then, the uncertainty-based method sorts the candidates by its uncertainty criterion and finally selects the most uncertain ? sample. Since DACS selects diverse samples as useful candidates, the uncertainty-based methods are free of selecting redundant or highly similar samples in the acquisition. Furthermore, in the case where DACS may overlook informative samples in the center selection, the uncertainty-based method can correct the missed selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we evaluate the proposed method in the settings of the active learning. We perform two different tasks, which are image classification and drug-protein interaction, to show the strength of DACS in different domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head><p>5.1.1 Baselines. We compare DACS with the four strong baselines which include two uncertainty-based methods (LearnLoss <ref type="bibr" target="#b41">[43]</ref> and NCE-Net <ref type="bibr" target="#b37">[39]</ref>) and two diversity-based methods (Core-set <ref type="bibr" target="#b32">[34]</ref> and CDAL <ref type="bibr" target="#b0">[2]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Training configuration.</head><p>For a fair comparison, we perform the comparison on the same settings of an initial labeled set (i.e., L 0 ) and the same random seeds, and we report the average performance of three trials. For a fair comparison, we not only use the same networks between baselines but also perform auxiliary training for all baselines. We have implemented the proposed method and all experiments with PyTorch <ref type="bibr" target="#b28">[30]</ref> and trained the models on a single NVIDIA Tesla V100 with 32GB of RAM.</p><p>For the hyper-parameters of DACS, the reduced dimension in the auxiliary classifier is set to 16, and we set the number of buckets (? in Eq. 6) and the number of breaks (? in Eq. 9) to 100 and 4, respectively. The temperature is set to 0.25 (? in Eq. 10). The above parameters are chosen by validation on CIFAR-10 dataset, and we found that such parameters work fairly well in different datasets in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Image Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Dataset configuration.</head><p>We use two different datasets for the image classification task. First, we evaluate each method on CIFAR-10 <ref type="bibr" target="#b22">[24]</ref> which is the standard dataset for active learning. CIFAR-10 contains 50,000 training and 10,000 test images of 32?32?3 assigned with one of 10 object categories. We also experiment on Repeated MNIST (RMNIST) <ref type="bibr" target="#b20">[22]</ref> to evaluate each method in the real-world setting where duplicate or highly similar samples exist.</p><p>RMNIST is constructed by taking the MNIST dataset and replicating each data point in the training set two times (obtaining a training set that is three times larger than the original MNIST). To be specific, after normalizing the dataset, isotropic Gaussian noise is added with a standard deviation of 0.1 to simulate slight differences between the duplicated data points in the training set. RMNIST includes 180,000 training and 10,000 test images of 28?28?1 assigned with one of 10 digit categories. As an evaluation metric, we use classification accuracy.</p><p>The active learning for CIFAR-10 starts with randomly selected 1,000 labeled samples with 49,000 unlabeled samples. In each cycle, each method selects 1,000 samples from unlabeled pool U ? and adds the selected samples to the current labeled dataset, and this process is repeatedly performed in 10 cycles. For RMNIST, we reduce the size of the initial set and the query budget to 500 samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Models.</head><p>For CIFAR-10, we use the 18-layer residual network (ResNet-18) <ref type="bibr" target="#b14">[16]</ref>. Since the original network is optimized for the large images (224?224?3), we revise the first convolution layer to The other settings are the same between CIFAR-10 and RMNIST. We train each model for 200 epochs with a mini-batch size of 128. We use the SGD optimizer with an initial learning rate of 0.1 and a momentum of 0.4. After 160 epochs, the learning rate is decreased to 0.01. As in <ref type="bibr" target="#b41">[43]</ref>, we stop the gradient from the auxiliary classifier propagated to the main classifier after 120 epochs to focus on the main objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Results. The evaluation results are shown in Table 1 (CIFAR-10) and Table 2 (RMNIST).</head><p>In CIFAR-10, the diversity-based methods underperform compared to uncertainty-based methods in early cycles (e.g., 2-5 cycles). However, it is noteworthy that even though DACS also belongs to the diversity-based approach, it outperforms other methods by a large margin during the same cycles. It means that DACS can select informative samples fairly well with the smaller amount of query budget. In later cycles as well, DACS shows competitive or better performance than other strong baselines.</p><p>We can see the strength of DACS in RMNIST which is the more practical scenario. To achieve high accuracy in this dataset, it is significant to consider both diversity and informativeness of samples because redundant samples exist. The uncertainty-based methods poorly perform in the dataset since it mainly considers informativeness and does not aware of the similarity between selected samples. In contrast, the diversity-based methods exhibit their strength over the uncertainty-based method. Particularly, DACS consistently outperforms all baselines in subsequent cycles. For example, DACS better performs on average 2.3%p and 1.1%p than uncertainty-based and diversity-based methods, respectively, in the last cycle.</p><p>It is noticeable that DACS can be beneficially combined with other methods. Without exceptions, combining DACS improves the performance of uncertainty-based methods by suggesting diverse samples as useful candidates. The improved performance is remarkable in RMNIST. For example, DACS increases the performance of LearnLoss, which shows a similar performance with Random, as much as or better than the diversity-based methods. This improvement could be attributed to preventing uncertainty-based methods from selecting redundant samples. In CIFAR-10 as well, the largest performances are achieved when combining DACS with the uncertainty-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Prediction of Drug-Protein Interaction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Dataset configuration.</head><p>For the regression task, we perform a drug-protein interaction task, which is the task to predict the affinity between the pair of drug and protein. We evaluate the performance on Davis <ref type="bibr" target="#b9">[11]</ref>, and it roughly contains 20,000 training and 5,000 test  pairs with its affinity. We follow the same pre-processing scheme as <ref type="bibr" target="#b27">[29]</ref> and evaluate each method by mean squared errors.</p><p>The active learning starts with randomly selected 1,000 labeled samples with 19,000 unlabeled samples. In each cycle, each method selects 1,000 samples from the unlabeled pool. We repeat the acquisition in ten times 5.3.2 Models. We employ DeepDTA <ref type="bibr" target="#b27">[29]</ref> as a backbone, which consists of two different CNNs for drug and protein. The concatenated vectors from each CNNs are fed to the fully-connected networks to predict the affinity. The parameters are optimized through the MSE loss. We train the networks using Adam <ref type="bibr" target="#b19">[21]</ref> optimizer with 0.001 learning rate for 50 epochs and set the mini-batch size to 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Results</head><p>. The comparison results are shown in Table <ref type="table" target="#tab_4">3</ref>. Here, we did not compare with NCE-Net and CDAL because they are optimized for the classification task. Performance trends between different methods are similar to the classification experiment. Again, DACS shows superior performance compared to other methods. The large accuracy margin between DACS and other methods in the initial cycles is remarkable and, in the last cycle as well, DACS shows approximately 11% better performance compared to Coreset and LearnLoss. In addition, the performance of LearnLoss is largely increased when combined with DACS. Comprehensive results clearly reveal the strength of the proposed method not only in classification but also in the regression task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ANALYSIS</head><p>In this analysis, we answer important questions: ?) Does AL methods still work well with the small budgets? ??) Is the sampling from sparse region indeed more effective than sampling from the dense one? ???) Why does the selected subset from DACS lead to superior performance?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Active Learning with Small Budget</head><p>As stated in earlier, we follow the experimental settings of previous works <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b41">43]</ref>. This settings typically have the size of query (i.e., ?) at least more than 1,000 samples. However, there are possible scenarios that the labeling costs are highly expensive. In this case, we are only capable to query the small number of samples to oracles.</p><p>To confirm the strength of DACS in such settings, we conduct the same experiments with the main experiments, but reduce the query size. For CIFAR10 and RMNIST, we use the same initial labeled dataset but set the query size to 100. For Davis, we query 500 samples in each cycle of active learning. The other settings are same with that of the main experiments. The results are shown in Figure <ref type="figure" target="#fig_4">3</ref>. Similar to the main experiments, DACS shows superior performance over the entire tasks. Specifically, the remarkable performance gap between DACS and others is observed in RMNIST where redundant samples exist. These results verify that DACS still works quite well in the small number of query settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Effectiveness of Dense and Sparse region</head><p>To answer the second question, we compare the performance when selecting samples only from the dense or sparse region. Here, unlabeled samples are split into three clusters (i.e., ? = 3 in Eq. 9) based on the estimated density, and we measure the performance of sampling from the most dense and sparse clusters except for the intermediate cluster. Experimental settings are the same as CIFAR-10 and the results are shown in Table <ref type="table">4</ref>. We can see that sampling from the sparse region results in better performance than sampling from the dense region. A noticeable point is that the performance of the dense region is gradually on par with the Random method, indicating that sampling from the dense region gradually fails to select informative samples compared to sampling from the sparse region. The results also present that DACS, which utilizes multiple acquisitions depending on the density, performs better than the single acquisition (i.e., sampling only from sparse or dense).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Subset Diversity and Information Analysis</head><p>Diversity-based methods consider sample diversity, and uncertaintybased methods take into account informativeness. We quantitatively analyze the selected samples from the different methods based on what each method considers to answer the last question. Similar to <ref type="bibr" target="#b37">[39]</ref>, we quantify the subset informativeness as the normalized event probability by following information theory <ref type="bibr" target="#b25">[27]</ref> and define the diversity using the average distance between selected samples.</p><p>Based on the two measures, we evaluate the subset selected from diversity-based method (Core-set), uncertainty-based method  (LearnLoss), Random, and DACS. We use the experimental settings of CIFAR-10 and the results are shown in Figure <ref type="figure" target="#fig_8">5</ref>. Understandably, the selected samples from LearnLoss show higher informativeness than Core-set as the former explicitly considers the informativeness. When it comes to the diversity, Core-set exhibits its strength over the LearnLoss. Compared to these baselines, the selected samples from DACS show superior quality in both metrics. Particularly, the informativeness result (Figure <ref type="figure" target="#fig_8">5</ref> (Left)) indicates that the DACS selects informative samples fairly well although informativeness has not been explicitly considered in the process. These results not only justify the effectiveness of the proposed method but show that DACS could take the strength from both the diversity-and uncertainty-based methods by empowering the core-set to be aware of the density of feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORKS</head><p>The uncertainty-based methods can be categorized according to the definition of uncertainty. In the beginning, the posterior probability of the predicted class is popularly used as an uncertainty measure <ref type="bibr">[1,</ref><ref type="bibr" target="#b23">25]</ref>, and these are generalized to the prediction entropy <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b33">35]</ref>. Recently, various definitions have been proposed to mainly solve classification tasks. For example, Yoo et al. <ref type="bibr" target="#b41">[43]</ref> train a loss prediction model, and the output of which serve as an uncertainty surrogate. Sinha et al. <ref type="bibr" target="#b35">[37]</ref> and Zhang et al. <ref type="bibr" target="#b42">[44]</ref> learn the feature dissimilarity between labeled and unlabeled samples by adversarial training, and they select the samples having most dissimilar to the  labeled ones. Different from these works, Wana et al. <ref type="bibr" target="#b37">[39]</ref> define the rejection or confusion on nearest neighbor samples by replacing the softmax layer with the prototypical classifier. Bayesian approaches <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b20">22]</ref> are also proposed, however, they suffer from the inefficient inference and the convergence problem.</p><p>Diversity-based methods select samples that are most representative of the unlabeled data. Among these methods, clustering-based methods are frequently used in the literature <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b26">28]</ref>. Huang et al. <ref type="bibr" target="#b15">[17]</ref> extend the strategy of clustering methods by combining the uncertainty, but it is only applicable to the binary classification. Yang et al. <ref type="bibr" target="#b40">[42]</ref> maximize the diversity by imposing a sample diversity constraint on the objective function. Similarly, Guo et al. <ref type="bibr" target="#b12">[14]</ref> performs the matrix partitioning over mutual information between labeled and unlabeled samples. However, it is infeasible to apply the above two methods to large unlabeled datasets since they requires inversion of a very large matrix (i.e., |U ? | ? |U ? |). Sener et al. <ref type="bibr" target="#b32">[34]</ref> solve sample selection problem by core-set selection and show promising results with a theoretical analysis. Agarwal et al. <ref type="bibr" target="#b0">[2]</ref> extend the idea to capture semantic diversity by estimating the difference in probability distribution between samples.</p><p>A few studies have considered the density in AL <ref type="bibr" target="#b39">[41,</ref><ref type="bibr" target="#b43">45]</ref>. However, these methods utilize the density as a secondary method for the uncertainty-based method, and they even do not use it for the diverse sampling. More importantly, these works prefer dense regions, which includes a number of highly similar samples, unlike DACS that primarily exploits sparse regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>This paper has proposed the density-aware core-set (DACS) method which significantly improves the core-set method with the power of the density-awareness. To this end, we have analyzed the feature space through the lens of the local density and, interestingly, observed that the samples in locally sparse regions are highly informative than the samples in dense regions. Motivated by this, we empower the core-set method to be aware of the local density. DACS efficiently estimate the density of the unlabeled samples and divide the all feature space by considering the density. Afterward, the samples in the sparse regions are favorably selected by decomposed selection algorithm of the core-set. The extensive experiments clearly demonstrate the strength of the proposed method and show that DACS can produce a state-of-the-art performance in the real-world setting where redundant samples exist. We believe that our research can help in environments that require expensive labeling costs such as drug discovery <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b34">36]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Correlation plots between density and information entropy (? = -0.71), density and loss (? = -0.27), and 2-d projected samples with its label and density. In the density map, a value of near one indicates dense regions (Red), and a value of near zero means sparse regions (Blue). It shows that samples in sparse regions are more informative (i.e., high entropy, high loss) than the samples in dense regions. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4. 1</head><label>1</label><figDesc>.1 Nearest-neighbor-based estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Active learning results of image classification (a, b) and predicting interaction between drug and target (c). Best viewed in colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance of different sampling strategies. This shows that sampling from the sparse region is more effective than sampling from the dense.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Subset information (Left) and diversity (Right) of selected samples in ten cycles of the active learning. This indicates that DACS can take advantages from both informativeness and diversity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 1 Density-ware Core-set Require: labeled pool L ? , unlabeled pool U ? , density group C, query budget ? 1: S ? ? 2: # Perform core-set selection in each group C 3: for each C ? ? C do ? ? ? ?? ? ? ?? and ? ? ? ? ? ? arg ??? ? ? C ? /? ? ??? ? ?( L ? ?? ? ) ???? ?? ? ? ? ? ? ? ?</figDesc><table><row><cell>4:</cell><cell># Calculate the selection ratio from Eq. 10</cell></row><row><cell>5:</cell><cell></cell></row><row><cell>6:</cell><cell># Find ? ? center points in a greedy manner</cell></row><row><cell>7:</cell><cell>repeat</cell></row><row><cell>8:</cell><cell></cell></row><row><cell>9:</cell><cell></cell></row><row><cell>10:</cell><cell>until |? ? | &lt; ? ?</cell></row><row><cell>11:</cell><cell>S ? S ? ? ?</cell></row><row><cell cols="2">12: # Update unlabeled and labeled dataset</cell></row><row><cell>13:</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Active learning results on CIFAR-10. Evaluation metric is test accuracy (%). Best and second best results are highlighted in boldface and underlined, respectively. With the full dataset, we achieve 92.1% accuracy on the test dataset.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Proportion of Labeled Samples (CIFAR-10)</cell></row><row><cell>Methods</cell><cell>2%</cell><cell>4%</cell><cell>6%</cell><cell>8%</cell><cell>10% 12% 14% 16% 18% 20%</cell></row><row><cell>Random</cell><cell cols="5">50.6 59.9 67.2 74.5 79.5 83.3 84.8 85.2 86.3 87.0</cell></row><row><cell>LearnLoss [43]</cell><cell cols="5">54.1 66.2 76.3 80.5 82.2 85.8 87.4 88.4 89.1 89.9</cell></row><row><cell>NCE-Net [39]</cell><cell cols="5">56.4 67.2 75.1 80.9 82.4 85.0 87.1 88.5 90.2 90.2</cell></row><row><cell>CDAL [2]</cell><cell cols="5">50.6 64.1 73.7 80.2 83.8 86.1 87.3 88.6 89.5 90.2</cell></row><row><cell>Core-set [34]</cell><cell cols="5">50.6 62.4 71.1 78.2 82.2 84.5 86.6 88.1 89.1 89.8</cell></row><row><cell>DACS (Ours.)</cell><cell cols="5">50.6 70.8 79.1 82.9 84.3 86.9 88.2 89.1 89.6 90.4</cell></row><row><cell cols="6">LearnLoss [43] + DACS (Ours.) 53.0 69.9 78.5 83.2 84.2 87.2 88.6 89.6 90.6 91.2</cell></row><row><cell>NCE-Net [39] + DACS (Ours.)</cell><cell cols="5">55.2 70.6 78.9 82.5 84.6 86.3 88.2 89.4 90.3 91.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Active learning results on RMNIST. Evaluation metric is Top-1 test accuracy (%). Best and second best results are highlighted in boldface and underlined, respectively. With the full dataset, we achieve 98.3% accuracy on the test dataset.</figDesc><table><row><cell></cell><cell>Proportion of Labeled Samples (RMNIST)</cell></row><row><cell>Methods</cell><cell>0.2% 0.4% 0.6% 0.8% 1.0% 1.2% 1.4% 1.6% 1.8% 2.0%</cell></row><row><cell>Random</cell><cell>85.1 88.2 90.8 92.3 93.1 93.7 94.2 94.2 94.3 94.7</cell></row><row><cell>LearnLoss [43]</cell><cell>86.1 88.5 90.1 91.6 92.4 93.0 93.5 94.4 94.5 94.5</cell></row><row><cell>NCE-Net [39]</cell><cell>87.5 88.9 90.5 91.9 93.3 93.8 94.7 95.1 95.6 95.6</cell></row><row><cell>CDAL [2]</cell><cell>85.1 90.5 93.2 94.8 95.8 95.9 96.0 96.2 96.5 96.7</cell></row><row><cell>Core-set [34]</cell><cell>85.1 88.6 91.9 93.0 94.3 94.6 95.6 96.1 96.1 96.5</cell></row><row><cell>DACS (Ours.)</cell><cell>85.1 92.1 94.3 95.1 96.0 96.4 96.8 97.0 97.4 97.6</cell></row><row><cell cols="2">LearnLoss [43] + DACS (Ours.) 85.7 90.1 91.4 93.4 93.7 94.6 95.5 96.1 96.6 97.0</cell></row><row><cell>NCE-Net [39] + DACS (Ours.)</cell><cell>87.6 91.3 93.3 94.8 95.7 95.8 96.2 96.3 96.6 96.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Active learning results on Davis. Evaluation metric is mean squared errors (i.e., lower errors indicates better models). Best and second best results are highlighted in boldface and underlined, respectively. With the full dataset, we achieve 0.271 MSE on the test dataset.</figDesc><table><row><cell>Proportion of Labeled Samples (Davis)</cell></row></table><note><p>g., horizontal flip and random crop) to the training on CIFAR-10.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Here, we train ResNet-18<ref type="bibr" target="#b14">[16]</ref> on randomly selected 3,000 samples from CIFAR-10 datasets and visualize the features of unlabeled samples using t-SNE<ref type="bibr" target="#b36">[38]</ref>. The settings are detailed in Section 5.2.2</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Jenks natural breaks optimization is analogous to the single dimensional k-means clustering. The objective is to determine which set of breaks leads to the smallest intra-cluster variance while maximizing inter-cluster variance.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contextual Diversity for Active Learning</title>
		<author>
			<persName><forename type="first">Sharat</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Himanshu</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saket</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chetan</forename><surname>Arora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="137" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Clustering and unsupervised anomaly detection with l 2 normalized deep auto-encoder representations</title>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Aytekin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyang</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Cricri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Aksu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the International Joint Conference on Neural Networks (IJCNN)</title>
		<meeting>the International Joint Conference on Neural Networks (IJCNN)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Predicting commercially available antiviral drugs that may act on the novel coronavirus (SARS-CoV-2) through a drug-target interaction deep learning model</title>
		<author>
			<persName><forename type="first">Bo Ram</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonggun</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoonjung</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungsoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keunsoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational and structural biotechnology journal</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-View Active Learning for Video Recommendation</title>
		<author>
			<persName><forename type="first">Jia-Jia</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing-Guo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng-Jun</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Transparent active learning for robots</title>
		<author>
			<persName><forename type="first">Crystal</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maya</forename><surname>Cakmak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><forename type="middle">L</forename><surname>Thomaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Human-Robot Interaction (HRI)</title>
		<meeting>of the International Conference on Human-Robot Interaction (HRI)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semi-supervised classification by low density separation</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International workshop on artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="57" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">MONGOOSE: A learnable LSH framework for efficient neural network training</title>
		<author>
			<persName><forename type="first">Beidi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binghui</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaozhuo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">Lingjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anshumali</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Re</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Active learning with statistical models</title>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>David A Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<date type="published" when="1996">1996. 1996</date>
			<publisher>JAIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hierarchical sampling for active learning</title>
		<author>
			<persName><forename type="first">Sanjoy</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Comprehensive analysis of kinase inhibitor selectivity</title>
		<author>
			<persName><forename type="first">Mindy I</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><forename type="middle">P</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanna</forename><surname>Herrgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Ciceri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">M</forename><surname>Wodicka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Pallares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Hocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">K</forename><surname>Treiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">P</forename><surname>Zarrinkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature biotechnology</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Empirical study of the topology and geometry of deep networks</title>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyed-Mohsen</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep bayesian active learning with image data</title>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riashat</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Active Instance Sampling via Matrix Partition</title>
		<author>
			<persName><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exemplar Guided Active Learning</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Hartford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Leyton-Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadas</forename><surname>Raviv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Padnos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahar</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barak</forename><surname>Lenz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Active learning by querying informative and representative examples</title>
		<author>
			<persName><forename type="first">Sheng-Jun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The data model concept in statistical mapping</title>
		<author>
			<persName><forename type="first">F</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><surname>Jenks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International yearbook of cartography</title>
		<imprint>
			<date type="published" when="1967">1967. 1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-class active learning for image classification</title>
		<author>
			<persName><forename type="first">Ajay</forename><forename type="middle">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Papanikolopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Decision boundary of deep neural networks: Challenges and opportunities</title>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the International Conference on Web Search and Data Mining</title>
		<meeting>the International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>WSDM</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kirsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joost</forename><surname>Van Amersfoort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Heterogeneous uncertainty sampling for supervised learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><surname>Catlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning proceedings</title>
		<imprint>
			<date type="published" when="1994">1994. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Latent structured active learning</title>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Information theory, inference and learning algorithms</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David Jc Mac</forename><surname>Mackay</surname></persName>
		</author>
		<author>
			<persName><surname>Kay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Active learning using pre-clustering</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnold</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">DeepDTA: deep drugtarget binding affinity prediction</title>
		<author>
			<persName><forename type="first">Hakime</forename><surname>?zt?rk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arzucan</forename><surname>?zg?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elif</forename><surname>Ozkirimli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Anand</forename><surname>Rajaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>David Ullman</surname></persName>
		</author>
		<title level="m">Mining of massive datasets</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Not all unlabeled data are equal: learning to weight data in semi-supervised learning</title>
		<author>
			<persName><forename type="first">Zhongzheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning with labeled and unlabeled data</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Seeger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Active learning for convolutional neural networks: A core-set approach</title>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An analysis of active learning strategies for sequence labeling tasks</title>
		<author>
			<persName><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Craven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Self-attention based molecule representation for predicting drug-target interaction</title>
		<author>
			<persName><forename type="first">Bonggun</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungsoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keunsoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joyce</forename><forename type="middle">C</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Healthcare Conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Variational adversarial active learning</title>
		<author>
			<persName><forename type="first">Samarth</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sayna</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">JMLR</biblScope>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mengying Fua, Xiangyang Jib, and Qingming Huanga Qixiang Yea. 2021. Nearest Neighbor Classifier Embedded Network for Active Learning</title>
		<author>
			<persName><forename type="first">Fang</forename><surname>Wana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianning</forename><surname>Yuana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association of the Advanced of Artificial Intelligence (AAAI)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Facility location: concepts, models, algorithms and case studies</title>
		<author>
			<persName><forename type="first">Gert</forename><forename type="middle">W</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Geographical Information Science</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
	<note>Series: Contributions to Management Science</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Incorporating diversity and density in active learning for relevance feedback</title>
		<author>
			<persName><forename type="first">Zuobing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ram</forename><surname>Akella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the European Conference on Information Retrieval (ECIR)</title>
		<meeting>the European Conference on Information Retrieval (ECIR)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-class active learning by uncertainty sampling with diversity maximization</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiping</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning loss for active learning</title>
		<author>
			<persName><forename type="first">Donggeun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">In</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">State-relabeling adversarial active learning</title>
		<author>
			<persName><forename type="first">Beichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Active learning with sampling by uncertainty and density for data annotations</title>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huizhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">K</forename><surname>Tsou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on audio, speech, and language processing</title>
		<imprint>
			<biblScope unit="page" from="1323" to="1331" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
