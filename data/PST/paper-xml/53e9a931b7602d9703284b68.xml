<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">News Video Classification Using SVM-based Multimodal Classifiers and Combination Strategies</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Wei-Hao</forename><surname>Lin</surname></persName>
							<email>whlin@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Avenue Pittsburgh</addrLine>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Avenue Pittsburgh</addrLine>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">News Video Classification Using SVM-based Multimodal Classifiers and Combination Strategies</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">492202CF6F1014345AFC3207043F0D72</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video classification is the first step toward multimedia content understanding.</p><p>When video is classified into conceptual categories, it is usually desirable to combine evidence from multiple modalities. However, combination strategies in previous studies were usually ad hoc. We investigate a meta-classification combination strategy using Support Vector Machine, and compare it with probability-based strategies. Text features from closedcaptions and visual features from images are combined to classify broadcast news video. The experimental results show that combining multimodal classifiers can significantly improve recall and precision, and our meta-classification strategy gives better precision than the approach of taking the product of the posterior probabilities.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Video classification is the first step toward multimedia content understanding. The video can be classified at a low level (monochrome or color films), at a middle level (cityscape or landscape), or at a very high level (comedy or tragedy). While video classification at a lower level can often be accomplished using features from single modality, higher-level classification usually needs to consider evidence from several modalities to be effective, since a single modality does not provide sufficient information for accurate classification. For example, it would be difficult to reliably distinguish action movies from detective movies if we consider only color information. Combining evidence from multiple modalities for video classification has been shown to improve classification accuracy in several studies, including combining overlay text and faces <ref type="bibr" target="#b4">[4]</ref>, color, motion and audio <ref type="bibr" target="#b5">[5]</ref>.</p><p>The method of combining multiple evidence streams, however, is not trivial, and will determine the effectiveness of the whole system, as we will show later. One straightforward approach is to concatenate features from each modality to make a long feature vector.</p><p>This approach will suffer from the 'curse of dimensionality', as more features are included and the feature vector grows exceedingly long. Another approach is to build individual classifiers for each modality, and later combine their judgments to make the final decision. The judgments can be combined using strategies like majority voting, linear combination, or winner-take-all methods <ref type="bibr" target="#b4">[4]</ref>. However, these schemes can all be considered ad-hoc, as they lack any underlying theory, and the relative importance of each classifier is either ignored or arbitrarily assigned. Recently, a meta-classification combination strategy has been successfully applied to identify unknown people in the video stream <ref type="bibr" target="#b8">[8]</ref>. In this paper, we investigate how a meta-classification strategy performs in the realm of video classification, and compare it with other strategies in a probability framework <ref type="bibr" target="#b7">[7]</ref>, which has also shown performance improvements in tasks such as biometric identity verification and handwritten digit recognition.</p><p>While video has rich content, most features exploited by video classifiers in previous studies are audio-visual features <ref type="bibr" target="#b15">[15]</ref>, such as color, motion, pitch, and chromaticity <ref type="bibr" target="#b10">[10]</ref>. Few studies in the video classification have looked at higher-level text features such as transcripts, which either generated from closed-captions or automatic speech recognition systems. While it is tempting to combine text features with other visual features, text classification poses yet another serious challenge because the dimensionality in the text feature space is usually much higher than for other lowlevel features. Instead of taking a Hidden Markov Model approach widely used in previous video classification studies <ref type="bibr">[5][9]</ref>[10], we propose building text-based classifiers and imagebased classifiers using a Support Vector Machine because it is less vulnerable to degrade due to high dimensionality, and has been shown effective in many classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MULTIMODAL CLASSIFIERS</head><p>All classifiers we implemented in this paper are based on the Support Vector Machine (SVM). We first describe the basic idea of SVM, followed by feature extraction for multimodal classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Support Vector Machine</head><p>Support Vector Machine has recently received much attention in the machine learning community. Initially proposed as a binary classification method, SVM has not only been carefully motivated by statistical learning theory <ref type="bibr" target="#b13">[13]</ref>, but also been successfully applied to numerous domains, including object detection <ref type="bibr" target="#b11">[11]</ref>, handwritten digit recognition <ref type="bibr" target="#b12">[12]</ref>, and text categorization <ref type="bibr">[6]</ref>.</p><p>The basic idea of SVM is illustrated in Figure <ref type="figure">1</ref>.</p><p>While there are many linear decision boundaries that can separate two classes (circles and squares) in Figure <ref type="figure">1</ref>, SVM will select the bold solid line over the narrow solid line because the margin of the bold line is larger. If we push the bold solid line until we reach the data points of each class, the distance between these two dash lines is the margin. Intuitively, a decision boundary with a large margin suggests a lower classification error when new data arrives. The data points (circles and squares) on the margin boundary, which are outlined in bold, are called the support vectors. The goal of SVM is to find the decision boundary with the maximal margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1 The margin and decision boundary of SVM in a twodimensional feature space</head><p>Consider a binary classification problem with linearly separable data (x i ,y i ), where y i is the label of the feature vector x i , and the value is either +1 or -1. For positive data x i with y i = +1, there exists w and b such that w•x i +b&gt;+1. Similarly, for negative data x i with for y i = -1, we have w•x i +b&gt;-1. The margin between these two supporting planes will be 2/||w|| 2 . The task of maximizing the margin can be formulated as a quadratic program with constraints w•x i +b&gt;+1 for positive data and w•x i +b&gt;-1 for negative data.</p><p>There exist many optimization methods to solve Quadratic Programming (QP). Since the QP problems are convex, we are guaranteed to find the global maximum. Moreover, the distance of the margin is decided only by support vectors and has no direct relationship to the dimensionality of the complete data. Therefore, SVM will be less susceptible to problems inherent in highdimensional data. More rigorous introductions to SVM can be found in standard texts <ref type="bibr">[3][13]</ref>. Our SVM implementation is based on libsvm <ref type="bibr" target="#b2">[2]</ref>, and the SVM we use is C-SVM <ref type="bibr" target="#b13">[13]</ref> with a radial basis function kernel. C-SVM solves the following QP,</p><formula xml:id="formula_0">i i i l i i b w b x w y C w w ξ ξ ξ - ≥ + Θ ⋅ + ⋅ = 1 ) ) ( ( 2 1 min 1 , ,</formula><p>where i 0, i = 1,…,l. (•) is the function that maps the x i into higher dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Text Features</head><p>In the field of text categorization, a document is usually assumed to be a bag of words with word order ignored. By treating each individual word as a feature, a document can be represented as a feature vector x = (x 1 ,…,x v ), where v is the size of the vocabulary. The value of each feature here is the normalized word frequency, i.e. x i = C(i,doc) / len(doc), where C(i, doc) returns the number of times that the i th word in the vocabulary appears in the document, and len(doc) returns the length of the document. Word frequency indicates the importance of a word.</p><p>Stopwords, such as 'a', 'be', 'can', etc., are filtered out because stopwords usually do not carry much information about the category of the document. Moreover, we group morphological variants using Porter's stemming algorithm <ref type="bibr" target="#b16">[16]</ref>. For example, 'clouds' and 'cloudy' are mapped to the same word stem 'cloud'. Stopword removal and stemming are common practices in the field of information retrieval to reduce dimensionality, which usually yields better performance <ref type="bibr" target="#b1">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Image Features</head><p>The video is separated into shots, defined by automatically detected editing cuts. One keyframe is extracted from each shot. A color histogram is calculated for each keyframe image. We first dissect the image into 5 5 grids, and the colors are mapped to a 5 5 5 cube in the RGB color space. Each value in the cube will be an element in the feature vector. The feature vector thus has 125 elements, and the vector from every grid cell is further concatenated to form a long feature vector with 3125 (= 125 25) elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">COMBINATION STRATEGIES</head><p>We combine multiple classifiers in the hope that the classification accuracy can be improved. The other motivation is that instead of building a highly accurate, specialized classifier with much time and effort, we build several weak classifiers quickly and combine them together. The combination strategy decides how judgments from several classifiers are combined to make the final decision. In this section we describe probability-based and metaclassification strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Probability-Based Strategies</head><p>Kittler et al. proposed a probability-based framework <ref type="bibr" target="#b7">[7]</ref> to explain various combination strategies. Assume there are p classes {w 1 ,…, w p }, and k classifiers to be combined, and the feature vector that the i th classifier observes is x i , i = 1,…,k. Without any further information, we will select the class w j with the highest posterior probability given these features, i.e. P(w j |x 1 ,…,x k ). By Bayes theorem, we can write We label (x 1 ,…,x k ) as w j if</p><formula xml:id="formula_1">∏ ∏ = = = = k i l i l p l k i j i j w x p w P w x p w P 1 1 1 ) | ( ) ( max ) | ( ) (<label>(1)</label></formula><p>In terms of the posterior probability from each classifier, i.e. P(w i |x i ), Equation 1 can be rewritten as Equation <ref type="formula" target="#formula_2">2</ref>. The decision rule is called the product rule because final decision is based on the product of probabilities of all classifiers.</p><p>We label (x 1 ,…,x k ) as w j if</p><formula xml:id="formula_2">∏ ∏ = - = = - = k i i l l k p l k i i j k j x w p w P x w p w P 1 1 1 1 1 ) | ( ) ( max ) | ( ) (<label>(2)</label></formula><p>If we further assume equal prior probabilities, the decision rule can be reduced to Equation 3.</p><p>We label (x 1 ,…,x k ) as w j if</p><formula xml:id="formula_3">∏ ∏ = = = = k i i l p l k i i j x w p x w p 1 1 1 ) | ( max ) | ( (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Meta-classification Strategies</head><p>The basic idea of meta-classification <ref type="bibr" target="#b8">[8]</ref> is that we treat the judgment from each classifier for each class as a feature, and then build another classifier, i.e. a meta-classifier, to make the final decision as illustrated in Figure <ref type="figure">2</ref>. Unimodal classifiers first classify the video from individual modalities, and then their judgments are concatenated as a feature vector.The metaclassifier makes the final decision based on the newly built feature vector.</p><p>Formally speaking, assume there are p classes and k classifiers, and the judgment from i th classifier for the j th class given an unknown pattern is o i,j . The i th classifier outputs its judgment as a feature vector o i = (o i,1 ,…o i,p ), and we combine these feature vectors into a long feature vector m=(o 1 ,…o k ). The metaclassifier takes the feature vector m and makes a final decision. The meta-classifier can be any kinds of classifier, and we chose SVM as our meta-classifier. Note that we have to build p SVM classifiers for each class because SVM is a binary classifier. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2 Meta-classification combination strategy</head><p>Compared with a probability-based combination strategy, metaclassification 'observes more information' when it makes the final decision. From meta-classification point of view, the product rule only observes a feature vector m' = (o 1,j ,o 2,j ,…,o k,j ), which is a subset of m. Second, a SVM-based meta-classifier automatically learns the weights for different classifiers, while the product rule treats all classifiers with equal weights, even though not all classifiers are equally robust across all classes. Moreover, using a binary classifier such as SVM as a meta-classifier encourages local experts. It is possible that one classifier is an expert to recognize a specific class but not all classes. The product rule and other types of meta-classifiers such as Artificial Neural Network have difficulties incorporating this idea of local experts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENT 4.1 Data processing and classifier training</head><p>Our news video was first processed by the Informedia digital video library system <ref type="bibr" target="#b14">[14]</ref>. The system breaks a continuous broadcast video into news segments, extracts their transcripts from closed captions, and selects one keyframe to represent each shot.</p><p>We manually labeled 440 news stories from 2001 CNN News broadcasts of which 40 were weather reports (positive examples) and 400 were other types of news stories (negative examples). Weather reports can be easily confused with other news stories as previous video classification studies have shown <ref type="bibr" target="#b5">[5]</ref>[9], partly because they both contain similar audio features, i.e. voices from anchorpersons or weatherwomen. The average number of words for a news story was 481. The total vocabulary size was 19895, which means the text feature vector will be very long but most of elements are zero. Each story has an average of 20.6 keyframe images. Note that the segmentation algorithm is not perfect and often only part of a segment is a weather report, followed by other news.</p><p>The goal of the classifier is to assign a label (weather, nonweather) to each segment. We built one text-based classifier for transcripts and one image-based classifier for keyframe images. Judgments from both classifiers were considered collectively using combination strategies. We randomly shuffled all stories ten times, and each time we conducted a two-fold cross- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results of Multimodal Classification</head><p>Figure <ref type="figure" target="#fig_2">3</ref> shows that if we take both recall and precision into consideration, the image-based classifier has better performance than the text-based classifiers. However, caption classifier has much higher precision. A good combination strategy should be able to take advantage of the high precision from the caption classifier and overall quality of the image classifier. The experimental results of the two combination strategies are compared in Figure <ref type="figure" target="#fig_3">4</ref>. The SVM meta-classifier does combine the strengths from the two modalities, and significantly improves the performance. However, the product rule in Equation 2 did not perform as we expected, and it is clear that the product rule unanimously favored the weather class because we observe high recall and low precision. One possible cause can be our naïve posterior probability estimation for the unimodal classifiers. Since there is no obvious way for SVM to incorporate priori information in the model, we should adopt the product rule with equal prior probabilities from Equation <ref type="formula">3</ref>. We reran the experiment and the new result is shown in Figure <ref type="figure" target="#fig_4">5</ref>. The equal prior probability version of product rule shows remarkable improvement. We conducted a paired two-sample ttest to compare the mean of recall and precision of the two combination strategies. The difference in recall is not significant, but the difference in precision is significant (p &lt; 1e -6 ). While the two combination strategies have comparable performance in recall, the SVM meta-classification gives higher precision. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>We combined transcripts from closed captions with color histogram features to classify news video into weather and nonweather categories, and applied SVM-based meta-classification to combine the results from the single modal classifiers. SVM-based multimodal classifiers behave remarkably stable even in an environment of high dimensional, noisy data and simple features.</p><p>By combining text and image features, we saw significant improvements in recall and precision. Furthermore, metaclassification yields better precision rate than a probability-based combination strategy, which can be useful for tasks requiring very high accuracy, such as TV commercial detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Assume the feature vectors for each modality are conditionally independent given the class. The decision rule is Equation 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3</head><label>3</label><figDesc>Figure 3 The performance of each unimodal classifier</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4</head><label>4</label><figDesc>Figure 4 Comparison of product rule and SVM-based metaclassification combination strategies</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5</head><label>5</label><figDesc>Figure 5 Comparison of product rule with equal priors and SVM-based meta-classification combination strategies</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>validation, using 20 segments of weather reports and 200 segments of nonweather news for training, and the same number of data for testing. After training unimodal classifiers on the training data we train the meta-classifier, by asking the single modality classifiers to classify the training data, and combining the judgments from the decision function of the classifiers as the training data for the meta-classifier.Since SVM-based text and image classifiers do not output probabilities, we scale the output value of decision functions into a range between zero and one as a posterior probability P(w weather |x j ). The P(w non-weather |x j ) is 1-P(w weather |x j ). The prior probability P(w j ) was estimated from the training data, which was fixed in our case, i.e. P(w weather )=0.09, P(w non-weather )=0.91.</figDesc><table><row><cell cols="3">Average recall and precision are reported. Consider the following</cell></row><row><cell>confusion matrix,</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Predicted Positive</cell><cell>Predicted Negative</cell></row><row><cell>Actual Positive</cell><cell>A</cell><cell>B</cell></row><row><cell>Actual Negative</cell><cell>C</cell><cell>D</cell></row><row><cell cols="3">Recall is defined as A/(A+B), precision as A/(A+C), and accuracy</cell></row><row><cell cols="3">as (A+D)/(A+B+C+D). Accuracy rate is not an informative</cell></row><row><cell cols="3">metric here because a blind classifier that always judges</cell></row><row><cell cols="3">everything as non-weather news can have 90% accuracy. On the</cell></row><row><cell cols="3">other extreme, if a classifier always classifies video into weather</cell></row><row><cell cols="3">reports, it will yield high recall and low precision. An ideal</cell></row><row><cell cols="3">classifier should have both high recall and high precision.</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Modern Information Retrieval</title>
		<author>
			<persName><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">LIBSVM: a library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/~cjlin/libsvm" />
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">An Introduction to Support Vector Machines and other Kernel-based Learning Methods</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Video Classification Based on HMM Using Text and Faces</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dimitrova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Agnihotri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Signal Processing</title>
		<meeting><address><addrLine>Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Integration of Multimodal Features for Video Scene Classification Based on HMM</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Third Workshop on Multimedia Signal Processing</title>
		<meeting><address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Text Categorization with Support Vector Machines: Learning with Many Relevant Features</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Machine Learning</title>
		<meeting>European Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On Combining Classifiers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hatef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P W</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">W.-H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><forename type="middle">R</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<title level="m">Meta-classification of Multimedia Classifiers. International Workshop on Knowledge Discovery in Multimedia and Complex Data</title>
		<meeting><address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Classification of TV Programs Based on Audio Information Using Hidden Markov Model</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Multimedia Signal Processing</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Classification of Summarized Videos using Hidden Markov Models on Compressed Chromaticity Signatures</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Au</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>ACM Multimedia</publisher>
			<pubPlace>Ottawa, Canada</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A General Framework for Object Detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Comparing Support Vector Machine with Gaussian Kernels to Radial Basis Function Classifiers</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-K</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Giroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">The Nature of Statistical Learning Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Intelligent Access to Digital Video: Informedia Project</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Wactlar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Stevens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multimedia Content Analysis Using Both Audio and Visual Clues</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An algorithm for suffix stripping</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980-07">July 1980</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
