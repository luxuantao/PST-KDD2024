<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structured Transforms for Small-Footprint Deep Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
							<email>sindhwani@google.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Google</settlement>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
							<email>tsainath@google.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Google</settlement>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
							<email>sanjivk@google.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Google</settlement>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Structured Transforms for Small-Footprint Deep Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BFA7E693F842DEE596C8AD35A1626BE7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the task of building compact deep learning pipelines suitable for deployment on storage and power constrained mobile devices. We propose a unified framework to learn a broad family of structured parameter matrices that are characterized by the notion of low displacement rank. Our structured transforms admit fast function and gradient evaluation, and span a rich range of parameter sharing configurations whose statistical modeling capacity can be explicitly tuned along a continuum from structured to unstructured. Experimental results show that these transforms can significantly accelerate inference and forward/backward passes during training, and offer superior accuracy-compactness-speed tradeoffs in comparison to a number of existing techniques. In keyword spotting applications in mobile speech recognition, our methods are much more effective than standard linear low-rank bottleneck layers and nearly retain the performance of state of the art models, while providing more than 3.5-fold compression.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Non-linear vector-valued transforms of the form, f (x, M) = s(Mx), where s is an elementwise nonlinearity, x is an input vector, and M is an m × n matrix of parameters are building blocks of complex deep learning pipelines and non-parametric function estimators arising in randomized kernel methods <ref type="bibr" target="#b19">[20]</ref>. When M is a large general dense matrix, the cost of storing mn parameters and computing matrix-vector products in O(mn) time can make it prohibitive to deploy such models on lightweight mobile devices and wearables where battery life is precious and storage is limited. This is particularly relevant for "always-on" mobile applications, such as continuously looking for specific keywords spoken by the user or processing a live video stream onboard a mobile robot. In such settings, the models may need to be hosted on specialized low-power digital signal processing components which are even more resource constrained than the device CPU.</p><p>A parsimonious structure typically imposed on parameter matrices is that of low-rankness <ref type="bibr" target="#b21">[22]</ref>. If M is a rank r matrix, with r min(m, n), then it has a (non-unique) product representation of the form M = GH T where G, H have only r columns. Clearly, this representation reduces the storage requirements to (mr + nr) parameters, and accelerates the matrix-vector multiplication time to O(mr+nr) via Mx = G(H T x). Another popular structure is that of sparsity <ref type="bibr" target="#b5">[6]</ref> typically imposed during optimization via zero-inducing l 0 or l 1 regularizers. Other techniques include freezing M to be a random matrix as motivated via approximations to kernel functions <ref type="bibr" target="#b19">[20]</ref>, storing M in low fixed-precision formats <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24]</ref>, using specific parameter sharing mechanisms <ref type="bibr" target="#b2">[3]</ref>, or training smaller models on outputs of larger models ("distillation") <ref type="bibr" target="#b10">[11]</ref>.</p><p>Structured Matrices: An m × n matrix which can be described in much fewer than mn parameters is referred to as a structured matrix. Typically, the structure should not only reduce memory requirements, but also dramatically accelerate inference and training via fast matrix-vector products and gradient computations. Below are classes of structured matrices arising pervasively in many contexts <ref type="bibr" target="#b17">[18]</ref> with different types of parameter sharing (indicated by the color).</p><p>(i) Toeplitz </p><formula xml:id="formula_0">       t0 t-</formula><formula xml:id="formula_1">       (ii) Vandermonde       1 v0 . . . v n-1 0 1 v1 . . . v n-1 1 . . . . . . . . . . . . 1 vn-1 . . . v n-1 n-1       (iii) Cauchy          1 u 0 -v 0 . . . . . . 1 u 0 -v n-1 1 u 1 -v 0 . . . . . . . . . . . . . . . . . . . . . 1 u n-1 -v 0 . . . . . . 1 u n-1 -v n-1         </formula><p>Toeplitz matrices have constant values along each of their diagonals. When the same property holds for anti-diagonals, the resulting class of matrices are called Hankel matrices. Toeplitz and Hankel matrices are intimately related to one-dimensional discrete convolutions <ref type="bibr" target="#b9">[10]</ref>, and arise naturally in time series analysis and dynamical systems. A Vandermonde matrix is determined by taking elementwise powers of its second column. A very important special case is the complex matrix associated with the Discrete Fourier transform (DFT) which has Vandermonde structure with v j = ω j n , j = 1 . . . n where ω n = exp -2πi n is the primitive n th root of unity. Similarly, the entries of n × n Cauchy matrices are completely defined by two length n vectors. Vandermonde and Cauchy matrices arise naturally in polynomial and rational interpolation problems.</p><p>"Superfast" Numerical Linear Algebra: The structure in these matrices can be exploited for faster linear algebraic operations such as matrix-vector multiplication, inversion and factorization. In particular, the matrix-vector product can be computed in time O(n log n) for Toeplitz and Hankel matrices, and in time O(n log 2 n) for Vandermonde and Cauchy matrices.</p><p>Displacement Operators: At first glance, these matrices appear to have very different kinds of parameter sharing and consequently very different algorithms to support fast linear algebra. It turns out, however, that each structured matrix class described above, can be associated with a specific displacement operator, L : R m×n → R m×n which transforms each matrix, say M, in that class into an m × n matrix L[M] that has very low-rank, i.e. rank(L[M]) min(m, n). This displacement rank approach, which can be traced back to a seminal 1979 paper <ref type="bibr" target="#b12">[13]</ref>, greatly unifies algorithm design and complexity analysis for structured matrices <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalizations of Structured Matrices:</head><p>Consider deriving a matrix by taking arbitrary linear combinations of products of structured matrices and their inverses, e.g.</p><formula xml:id="formula_2">α 1 T 1 T -1 2 + α 2 T 3 T -1 4 T 5</formula><p>where each T i is a Toeplitz matrix. The parameter sharing structure in such a derived matrix is by no means apparent anymore. Yet, it turns out that the associated displacement operator remarkably continues to expose the underlying parsimony structure, i.e. such derived matrices are still mapped to relatively low-rank matrices! The displacement rank approach allows fast linear algebra algorithms to be seamlessly extended to these broader classes of matrices. The displacement rank parameter controls the degree of structure in these generalized matrices.</p><p>Technical Preview, Contributions and Outline: We propose building deep learning pipelines where parameter matrices belong to the class of generalized structured matrices characterized by low displacement rank. In Section 2, we attempt to give a self-contained overview of the displacement rank approach <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b17">[18]</ref> drawing key results from the relevant literature on structured matrix computations (proved in our supplementary material <ref type="bibr" target="#b0">[1]</ref> for completeness). In Section 3, we show that the proposed structured transforms for deep learning admit fast matrix multiplication and gradient computations, and have rich statistical modeling capacity that can be explicitly controlled by the displacement rank hyperparameter, covering, along a continuum, an entire spectrum of configurations from highly structured to unstructured matrices. While our focus in this paper is on Toeplitz-related transforms, our proposal extends to other structured matrix generalizations. In Section 4, we study inference and training-time acceleration with structured transforms as a function of displacement rank and dimensionality. We find that our approach compares highly favorably with numerous other techniques for learning size-constrained models on several benchmark datasets. Finally, we demonstrate our approach on mobile speech recognition applications where we are able to match the performance of much bigger state of the art models with a fraction of parameters.</p><p>Notation: Let e 1 . . . e n denote the canonical basis elements of R n (viewed as column vectors). I n , 0 n denote n × n identity and zero matrices respectively. J n = [e n . . . e 1 ] is the anti-identity reflection matrix whose action on a vector is to reverse its entries. When the dimension is obvious we may drop the subscript; for rectangular matrices, we may specify both the dimensions explicitly, e.g. we use 0 1×n for a zero-valued row-vector, and 1 n for all ones column vector of length n. u • v denotes Hadamard (elementwise) product between two vectors v, u. For a complex vector u, ū will denote the vector of complex conjugate of its entries. The Discrete Fourier Transform (DFT) matrix will be denoted by Ω (or Ω n ); we will also use fft(x) to denote Ωx, and ifft(x) to denote Ω -1 x. For a vector v, diag(v) denotes a diagonal matrix given by diag(v) ii = v i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Displacement Operators associated with Structured Matrices</head><p>We begin by providing a brisk background on the displacement rank approach. Unless otherwise specified, for notational convenience we will henceforth assume squared transforms, i.e., m = n, and discuss rectangular transforms later. Proofs of various assertions can be found in our selfcontained supplementary material <ref type="bibr" target="#b0">[1]</ref> or in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>The Sylvester displacement operator, denoted as</p><formula xml:id="formula_3">L = ∇ A,B : R n×n → R n×n is defined by, ∇ A,B [M] = AM -MB<label>(1</label></formula><p>) where A ∈ R n×n , B ∈ R n×n are fixed matrices referred to as operator matrices. Closely related is the Stein displacement operator, denoted as L = A,B : R n×n → R n×n , and defined by,</p><formula xml:id="formula_4">A,B [M] = M -AMB<label>(2)</label></formula><p>By carefully choosing A and B one can instantiate Sylvester and Stein displacement operators with desirable properties. In particular, for several important classes of displacement operators, A and/or B are chosen to be an f -unit-circulant matrix defined as follows. Definition 2.1 (f -unit-Circulant Matrix). For a real-valued scalar f , the (n × n) f-circulant matrix, denoted by Z f , is defined as follows,</p><formula xml:id="formula_5">Z f = [e 2 , e 3 . . . e n , f e 1 ] =     0 0 . . . f 1 0 . . . 0 . . . . . . . . . . . . 0 . . . 1 0     = 0 1×(n-1) f I n-1 0 (n-1)×1</formula><p>The f -unit-circulant matrix is associated with a basic downward shift-and-scale transformation, i.e., the matrix-vector product Z f v shifts the elements of the column vector v "downwards", and scales and brings the last element v n to the "top", resulting in</p><formula xml:id="formula_6">[f v n , v 1 , . . . v n-1 ]</formula><p>T . It has several basic algebraic properties (see Proposition 1.1 <ref type="bibr" target="#b0">[1]</ref>) that are crucial for the results stated in this section Figure <ref type="figure">1</ref> lists the rank of the Sylvester displacement operator in Eqn 1 when applied to matrices belonging to various structured matrix classes, where the operator matrices A, B in Eqn. 1 are chosen to be diagonal and/or f -unit-circulant. It can be seen that despite the difference in their structures, all these classes are characterized by very low displacement rank. Figure <ref type="figure">2</ref> shows how this low-rank transformation happens in the case of a 4 × 4 Toeplitz matrix (also see section 1, Lemma 1.2 <ref type="bibr" target="#b0">[1]</ref>). Embedded in the 4 × 4 Toeplitz matrix T are two copies of a 3 × 3 Toeplitz matrix shown in black and red boxes. The shift and scale action of Z 1 and Z -1 aligns these sub-matrices. By taking the difference, the Sylvester displacement operator nullifies the aligned submatrix leaving a rank 2 matrix with non-zero elements only along its first row and last column. Note that the negative sign introduced by TZ -1 term prevents the complete zeroing out of the value of t (marked by red star) and is hence critical for invertibility of the displacement action. </p><formula xml:id="formula_7">1: Below r is rank(∇ A,B [M]) Structured Matrix M A B r Toeplitz T, T -1 Z 1 Z -1 ≤ 2 Hankel H, H -1 Z 1 Z T 0 ≤ 2 T + H Z 0 + Z T 0 Z 0 + Z T 0 ≤ 4 Vandermonde V (v) diag(v) Z 0 ≤ 1 V (v) -1 Z 0 diag(v) ≤ 1 V (v) T Z T 0 diag(v) ≤ 1 Cauchy C(s, t) diag(s) diag(t) ≤ 1 C(s, t) -1 diag(t) diag(s) ≤ 1 Figure 2: Displacement Action on Toeplitz Matrix t u v w x t u v y x t u z y x t             T z y x t t u v w x t u v y x t u             Z1T d o w n s h i f t u v w -t t u v -x x t u -y y x t -z             TZ-1 le f ts h if t - = * * * * 0 0 0 * 0 0 0 * 0 0 0 *             Z1T -TZ-1</formula><p>Each class of structured matrices listed in Figure <ref type="figure">1</ref> can be naturally generalized by allowing the rank of the displacement operator to be higher. Specifically, given a displacement operator L, and displacement rank parameter r, one may consider the class of matrices M that satisfies rank(L(M)) ≤ r. Clearly then, L[M] = GH T for rank r matrices G, H. We refer to rank(L(M)) as the displacement rank of M under L, and to the low-rank factors G, H ∈ R n×r as the associated low-displacement generators. For the operators listed in Table <ref type="table" target="#tab_2">1</ref>, these broader classes of structured matrices are correspondingly called Toeplitz-like, Vandermonde-like and Cauchy-like.</p><p>Fast numerical linear algebra algorithms extend to such matrices <ref type="bibr" target="#b17">[18]</ref>.</p><p>In order to express structured matrices with low-displacement rank directly as a function of its lowdisplacement generators, we need to invert L and obtain a learnable parameterization. For Stein type displacement operator, the following elegant result is known (see proof in <ref type="bibr" target="#b0">[1]</ref>):</p><formula xml:id="formula_8">Theorem 2.2 ( [19], Krylov Decomposition). If an n × n matrix M is such that A,B [M] = GH T where G = [g 1 . . . g r ], H = [h 1 . . . h r ]</formula><p>∈ R n×r and the operator matrices satisfy: A n = aI, B n = bI for some scalars a, b, then M can be expressed as:</p><formula xml:id="formula_9">M = 1 1 -ab r j=1 krylov(A, g j )krylov(B T , h j ) T<label>(3)</label></formula><p>where krylov(A, v) is defined by:</p><formula xml:id="formula_10">krylov(A, v) = [v Av A 2 v . . . A n-1 v]<label>(4)</label></formula><p>Henceforth, our focus in this paper will be on Toeplitz-like matrices for which the displacement operator of interest (see Table <ref type="table" target="#tab_2">1</ref>) is of Sylvester type: ∇ Z1,Z-1 . In order to apply Theorem 2.2, one can switch between Sylvester and Stein operators, setting A = Z 1 and B = Z -1 which both satisfy the conditions of Theorem 2.2 (see property 3, Proposition 1.1 <ref type="bibr" target="#b0">[1]</ref>). The resulting expressions involve Krylov matrices generated by f -unit-circulant matrices which are called f -circulant matrices in the literature. Definition 2.3 (f -circulant matrix). Given a vector v, the f-Circulant matrix, Z f (v), is defined as follows:</p><formula xml:id="formula_11">Z f (v) = krylov(Z f , v) =     v 0 f v n-1 . . . f v 1 v 1 v 0 . . . f v 2 . . . . . . . . . f v n-1 v n-1 . . . v 1 v 0    </formula><p>Two special cases are of interest: f = 1 corresponds to Circulant matrices, and f = -1 corresponds to skew-Circulant matrices.</p><p>Finally, one can obtain an explicit parameterization for Toeplitz-like matrices which turns out to involve taking sums of products of Circulant and skew-Circulant matrices. Theorem 2.4 ( <ref type="bibr" target="#b17">[18]</ref>).</p><formula xml:id="formula_12">If an n × n matrix M satisfies ∇ Z1,Z-1 [M] = GH T where G = [g 1 . . . g r ], H = [h 1 . . . h r ] ∈ R n×r</formula><p>, then M can be written as:</p><formula xml:id="formula_13">M = 1 2 r j=1 Z 1 (g j )Z -1 (Jh j )<label>(5)</label></formula><p>3 Learning Toeplitz-like Structured Transforms Motivated by Theorem 2.4, we propose learning parameter matrices of the form in Eqn. 5 by optimizing the displacement factors G, H. First, from the properties of displacement operators <ref type="bibr" target="#b17">[18]</ref>, it follows that this class of matrices is very rich from a statistical modeling perspective. Theorem 3.1 (Richness). The set of all n × n matrices that can be written as,</p><formula xml:id="formula_14">M(G, H) = r i=1 Z 1 (g i )Z -1 (h i )<label>(6)</label></formula><p>for some G = [g 1 . . . g r ], H = [h 1 . . . h r ] ∈ R n×r contains:</p><p>• All n × n Circulant and Skew-Circulant matrices for r ≥ 1.</p><p>• All n × n Toeplitz matrices for r ≥ 2.</p><p>• Inverses of Toeplitz matrices for r ≥ 2.</p><p>• All products of the form A 1 . . . A t for r ≥ 2t.</p><p>• All linear combinations of the form</p><formula xml:id="formula_15">p i=1 β i A (i) 1 . . . A (i)</formula><p>t where r ≥ 2tp. • All n × n matrices for r = n.</p><p>where each A i above is a Toeplitz matrix or the inverse of a Toeplitz matrix.</p><p>When we learn a parameter matrix structured as Eqn. 6 with displacement rank equal to 1 or 2, we also search over convolutional transforms. In this sense, structured transforms with higher displacement rank generalize (one-dimensional) convolutional layers. The displacement rank provides a knob on modeling capacity: low displacement matrices are highly structured and compact, while high displacement matrices start to contain increasingly unstructured dense matrices.</p><p>Next, we show that associated structured transforms of the form f (x) = M(G, H)x admit fast evaluation, and gradient computations with respect to G, H. First we recall the following wellknown result concerning the diagonalization of f -Circulant matrices. Theorem 3.2 (Diagonalization of f -circulant matrices, Theorem 2.6.4 <ref type="bibr" target="#b17">[18]</ref>). For any</p><formula xml:id="formula_16">f = 0, let f = [1, f 1 n , f 2 n , . . . f n-1 n ] T ∈ C n , and D f = diag(f ). Then, Z f (v) = D -1 f Ω -1 diag(Ω(f • v))ΩD f<label>(7)</label></formula><p>This result implies that for the special cases of f = 1 and f = -1 corresponding to Circulant and Skew-circulant matrices respectively, the matrix-vector multiplication can be computed in O(n log n) time via the Fast Fourier transform:</p><formula xml:id="formula_17">y = Z 1 (v)x = ifft (fft(v) • fft(x))<label>(8)</label></formula><formula xml:id="formula_18">y = Z 1 (v) T x = ifft fft(v) • fft(x)<label>(9)</label></formula><formula xml:id="formula_19">y = Z -1 (v)x = η • ifft (fft(η • v) • fft(η • x))<label>(10)</label></formula><formula xml:id="formula_20">y = Z -1 (v) T x = η • ifft (fft(η • v) • fft(η • x))<label>(11)</label></formula><p>where η = [1, η, η 2 . . . η n-1 ] T where η = (-1)</p><formula xml:id="formula_21">1 n = exp(i π n</formula><p>), the root of negative unity. In particular, a single matrix-vector product for Circulant and Skew-circulant matrices has the computational cost of 3 FFTs. Therefore, for matrices of the form in Eqn. 6 comprising of r products of Circulant and Skew-Circulant matrices, naively computing a matrix-vector product for a batch of b input vectors would take 6rb FFTs. However, this cost can be significantly lowered to that of 2(rb + r + b) FFTs by making the following observation:</p><formula xml:id="formula_22">Y = r i=1 Z 1 (g i )Z -1 (h i )X = Ω -1 r i=1 diag(Ωg i ) Ω diag( η) Ω -1 diag(Ω(η • h i )) X</formula><p>where X = Ω diag(η) X. Here, (1) The FFT of the parameters, Ωg i and Ω(η • h i ) is computed once and shared across multiple input vectors in the minibatch, (2) The (scaled) FFT of the input, (Ω diag(η) X) is computed once and shared across the sum in Eqn. 6, and (3) The final inverse FFT is also shared. Thus, the following result is immediate. Theorem 3.3 (Fast Multiplication). Given an n × b matrix X, the matrix-matrix product, Y = ( r i=1 Z 1 (g i )Z -1 (h i )) X, can be computed at the cost of 2(rb + b + r) FFTs, using the following algorithm.</p><p>Set η = [1, η, η 2 . . . η n-1 ] T where η = (-1)</p><formula xml:id="formula_23">1 n = exp(i π n ) Initialize Y = 0 n×b Set X = fft(diag(η)X) Set G = fft(G) = [g 1 . . . gr ] and H = fft(diag(η)H) = [ h1 . . . hr ] for i = 1 to r • U = Z -1 (h i )X = diag( η)ifft diag( hi ) X • V = diag(g i ) fft(U) • Y = Y + V Set Y = ifft (Y) Return Y</formula><p>We now show that when our structured transforms are embedded in a deep learning pipeline, the gradient computation can also be accelerated. First, we note that the Jacobian structure of f -Circulant matrices has the following pleasing form. Proposition 3.4 (Jacobian of f -circulant transforms). The Jacobian of the map f (x, v) = Z f (v)x with respect to the parameters v is Z f (x).</p><p>This leads to the following expressions for the Jacobians of the structured transforms of interest. Proposition 3.5 (Jacobians with respect to displacement generators G, H). Consider parameterized vector-valued transforms of the form,</p><formula xml:id="formula_24">f (x, G, H) = r i=1 Z 1 (g i )Z -1 (h i )x (<label>12</label></formula><formula xml:id="formula_25">)</formula><p>The Jacobians of f with respect to the j th column of G, H, i.e. g j , h j , at x, are as follows:</p><formula xml:id="formula_26">J gj f | x = Z 1 (Z -1 (h j )x) (13) J hj f | x = Z 1 (g j )Z -1 (x)<label>(14)</label></formula><p>Based on Eqns. 13, 14 the gradient over a minibatch of size b requires computing</p><formula xml:id="formula_27">, b i [J gj f | x i ] T δ i and b i=1 [J hj f | x i ] T δ i where, {x i } b</formula><p>i=1 and {δ i } b i=1 are batches of forward and backward inputs during backpropagation. These can be naively computed with 6rb FFTs. However, as before, by sharing FFT of the forward and backward inputs, and the fft of the parameters, this can be lowered to (4br + 4r + 2b) FFTs. Below we give matricized implementation. Proposition 3.6 (Fast Gradients). Let X, Z be n × b matrices whose columns are forward and backward inputs respectively of minibatch size b during backpropagation. The gradient with respect to g j , h j can be computed at the cost of (4br + 4r + 2b) FFTs as follows:</p><formula xml:id="formula_28">Compute Z = fft(Z), X = fft(diag(η)X), G = fft(G), H = fft(diag(η)H) Gradient wrt g j (2b + 1 FFTs) • return ifft fft diag(η)ifft diag( hj ) X • Z 1 b Gradient wrt h j (2b + 1 FFTs) • return diag ( η) ifft X • fft diag(η)ifft diag(g i ) Z 1 b</formula><p>Rectangular Transforms: Variants of Theorems 2.2, 2.4 exist for rectangular transforms, see <ref type="bibr" target="#b18">[19]</ref>.</p><p>Alternatively, for m &lt; n we can subsample the outputs of square n × n transforms at the cost of extra computations, while for m &gt; n, assuming m is a multiple of n, we can stack m n output vectors of square n × n transforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Empirical Studies</head><p>Acceleration with Structured Transforms: In Figure <ref type="figure" target="#fig_1">3</ref>, we analyze the speedup obtained in practice using n × n Circulant and Toeplitz-like matrices relative to a dense unstructured n × n matrix (fully connected layer) as a function of displacement rank and dimension n. Three scenarios are considered: inference speed per test instance, training speed as implicitly dictated by forward passes on a minibatch, and gradient computations on a minibatch. Factors such as differences in cache optimization, SIMD vectorization and multithreading between Level-2 BLAS (matrix-vector multiplication), Level-3 BLAS (matrix-matrix multiplication) and FFT implementations (we use FFTW: http://www.fftw.org) influence the speedup observed in practice. Speedup gains start to show for dimensions as small as 512 for Circulant matrices. The gains become dramatic with acceleration of the order of 10 to 100 times for several thousand dimensions, even for higher displacement rank Toeplitz-like transforms. Effectiveness for learning compact Neural Networks: Next, we compare the proposed structured transforms with several existing techniques for learning compact feedforward neural networks. We exactly replicate the experimental setting from the recent paper on HASHEDNETS <ref type="bibr" target="#b2">[3]</ref> which uses several image classification datasets first prepared by <ref type="bibr" target="#b14">[15]</ref>. <ref type="bibr">MNIST</ref>  where the task is to distinguish between tall and wide rectangles, and whether the "on" pixels form a convex region or not, respectively. In all datasets, input images are of size 28 × 28. Several existing techniques are benchmarked in <ref type="bibr" target="#b2">[3]</ref> for compressing a reference single hidden layer model with 1000 hidden nodes.</p><p>• Random Edge Removal (RER) <ref type="bibr" target="#b4">[5]</ref> where a fraction of weights are randomly frozen to be zero-valued.</p><p>• Low-rank Decomposition (LRD) <ref type="bibr" target="#b8">[9]</ref> • Neural Network (NN) where the hidden layer size is reduced to satisfy a parameter budget.</p><p>• Dark Knowledge (DK) <ref type="bibr" target="#b10">[11]</ref>: A small neural network is trained with respect to both the original labeled data, as well as soft targets generated by a full uncompressed neural network. • HashedNets (HN) <ref type="bibr" target="#b2">[3]</ref>: This approach uses a low-cost hash function to randomly group connection weights which share the same value. • HashedNets with Dark Knowledge (HN DK ): Trains a HashedNet with respect to both the original labeled data, as well as soft targets generated by a full uncompressed neural network.</p><p>We consider learning models of comparable size with the weights in the hidden layer structured as a Toeplitz-like matrix. We also compare with the FASTFOOD approach of <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b15">16]</ref> where the weight matrix is a product of diagonal parameter matrices and fixed permutation and Walsh-Hadamard matrices, also admitting O(n log n) multiplication and gradient computation time. The CIRCULANT Neural Network approach proposed in <ref type="bibr" target="#b3">[4]</ref> is a special case of our framework (Theorem 3.1).</p><p>Results in Table <ref type="table" target="#tab_2">1</ref> show that Toeplitz-like structured transforms outperform all competing approaches on all datasets, sometimes by a very significant margin, with similar or drastically lesser number of parameters. It should also be noted that while random weight tying in HASHEDNETS reduces the number of parameters, the lack of structure in the resulting weight matrix cannot be exploited for FFT-like O(n log n) multiplication time. We note in passing that for HASHEDNETS weight matrices whose entries assume only one of B distinct values, the Mailman algorithm <ref type="bibr" target="#b16">[17]</ref> can be used for faster matrix-vector multiplication, with complexity O(n 2 log(B)/(log n)), which still is much slower than matrix-vector multiplication time for Toeplitz-like matrices. Also note that the distillation ideas of <ref type="bibr" target="#b10">[11]</ref> are complementary to our approach and can further improve our results. Mobile Speech Recognition: We now demonstrate the techniques developed in this paper on a speech recognition application meant for mobile deployment. Specifically, we consider a keyword spotting (KWS) task, where a deep neural network is trained to detect a specific phrase, such as "Ok Google" <ref type="bibr" target="#b1">[2]</ref>. The data used for these experiments consists of 10-15K utterances of selected phrases (such as "play-music", "decline-call"), and a larger set of 396K utterances to serve as negative training examples. The utterances were randomly split into training, development and evaluation sets in the ratio of 80 : 5 : 15. We created a noisy evaluation set by artificially adding babble-type cafeteria noise at 0dB SNR to the "play-music" clean data set. We will refer to this noisy data set as CAFE0. We refer the reader to <ref type="bibr" target="#b22">[23]</ref> for more details about the datasets. We consider the task of shrinking a large model for this task whose architecture is as follows <ref type="bibr" target="#b22">[23]</ref>  Results with 11 different models are reported in Figure <ref type="figure" target="#fig_2">4</ref> (left) including the state of the art keyword spotting model developed in <ref type="bibr" target="#b22">[23]</ref>. At an operating point of 1 False Alarm per hour, the following observations can be made: With just 3348 parameters, a displacement rank=1 TOEPLITZ-LIKE structured transform outperforms a standard low-rank bottleneck model with rank=16 containing 16 times more parameters; it also lowers false reject rates from 10.2% with CIRCULANT and 14.2% with FASTFOOD transforms to about 8.2%. With displacement rank 10, the false reject rate is 6.2%, in comparison to 6.8% with the 3 times larger rank=32 standard low-rank bottleneck model. Our best Toeplitz-like model comes within 0.4% of the performance of the 80-times larger fully-connected and 3.6 times larger reference <ref type="bibr" target="#b22">[23]</ref> models. In terms of raw classification accuracy as a function of training time, Figure <ref type="figure" target="#fig_2">4</ref> (right) shows that our models (with displacement ranks 1, 2 and 10) come within 0.2% accuracy of the fully-connected and reference models, and easily provide much better accuracy-time tradeoffs in comparison to standard low-rank bottleneck models, Circulant and Fastfood baselines. The conclusions are similar for other noise conditions (see supplementary material <ref type="bibr" target="#b0">[1]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Perspective</head><p>We have introduced and shown the effectiveness of new notions of parsimony rooted in the theory of structured matrices. Our proposal can be extended to various other structured matrix classes, including Block and multi-level Toeplitz-like <ref type="bibr" target="#b11">[12]</ref> matrices related to multidimensional convolution <ref type="bibr" target="#b20">[21]</ref>.</p><p>We hope that such ideas might lead to new generalizations of Convolutional Neural Networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure</head><label></label><figDesc>Figure 1: Below r is rank(∇ A,B [M])</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Acceleration with n × n Structured Transforms (6-core 32-GB Intel(R) Xeon(R) machine; random datasets). In the plot, displacement rank = 0 corresponds to a Circulant Transform.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: "play-music" detection performance: (left) End-to-end keyword spotting performance in terms of false reject (FR) rate per false alarm (FA) rate (lower is better) (right): Classification accuracy as a function of training time. Displacement rank is in parenthesis for Toeplitz-like models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>is the original 10-class MNIST digit classification dataset with 60000 training examples and 10000 test examples. BG-IMG-ROT refers to a challenging version of MNIST where digits are randomly rotated and placed against a random black and white background. RECT (1200 training images, 50000 test images) and CONVEX (8000 training images, 50000 test images) are 2-class binary image datasets</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Error rate and number of parameters (italicized). Best results in blue.</figDesc><table><row><cell></cell><cell>RER</cell><cell>LRD</cell><cell>NN</cell><cell>DK</cell><cell>HN</cell><cell cols="6">HN DK Fastfood CIRCULANT TOEPLITZ (1) TOEPLITZ (2) TOEPLITZ (3)</cell></row><row><cell>MNIST</cell><cell>15.03</cell><cell>28.99</cell><cell>6.28</cell><cell>6.32</cell><cell>2.79</cell><cell>2.65</cell><cell>6.61</cell><cell>3.12</cell><cell>2.79</cell><cell>2.54</cell><cell>2.09</cell></row><row><cell></cell><cell cols="6">12406 12406 12406 12406 12406 12406</cell><cell>10202</cell><cell>8634</cell><cell>9418</cell><cell>10986</cell><cell>12554</cell></row><row><cell>BG-IMG-ROT</cell><cell>73.17</cell><cell>80.63</cell><cell>79.03</cell><cell>77.40</cell><cell>59.20</cell><cell>58.25</cell><cell>68.4</cell><cell>62.11</cell><cell>57.66</cell><cell>55.21</cell><cell>53.94</cell></row><row><cell></cell><cell cols="6">12406 12406 12406 12406 12406 12406</cell><cell>10202</cell><cell>8634</cell><cell>9418</cell><cell>10986</cell><cell>12554</cell></row><row><cell>CONVEX</cell><cell>37.22</cell><cell>39.93</cell><cell>34.37</cell><cell>31.85</cell><cell>31.77</cell><cell>30.43</cell><cell>33.92</cell><cell>24.76</cell><cell>17.43</cell><cell>16.18</cell><cell>20.23</cell></row><row><cell></cell><cell cols="6">12281 12281 12281 12281 12281 12281</cell><cell>3922</cell><cell>2352</cell><cell>3138</cell><cell>4706</cell><cell>6774</cell></row><row><cell>RECT</cell><cell>18.23</cell><cell>23.67</cell><cell>5.68</cell><cell>5.78</cell><cell>3.67</cell><cell>3.37</cell><cell>21.45</cell><cell>2.91</cell><cell>0.70</cell><cell>0.89</cell><cell>0.66</cell></row><row><cell></cell><cell cols="6">12281 12281 12281 12281 12281 12281</cell><cell>3922</cell><cell>2352</cell><cell>3138</cell><cell>4706</cell><cell>6774</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>: the input layer consists of 40 dimensional log-mel filterbanks, stacked with a temporal context of 32, to produce an input of 32 × 40 whose dimensions are in time and frequency respectively. This input is fed to a convolutional layer with filter size 32 × 8, frequency stride 4 and 186 filters. The output of the convolutional layer is of size 9 × 186 = 1674. The output of this layer is fed to a 1674 × 1674 fully connected layer, followed by a softmax layer for predicting 4 classes constituting the phrase "playmusic". The full training set contains about 90 million samples. We use asynchronous distributed stochastic gradient descent (SGD) in a parameter server framework<ref type="bibr" target="#b7">[8]</ref>, with 25 worker nodes for optimizing various models. The global learning rate is set to 0.002, while our structured transform layers use a layer-specific learning rate of 0.0005; both are decayed by an exponential factor of 0.1.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>play-music:cafe0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">play-music:accuracy</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">fullyconnected (2.8M)</cell><cell></cell><cell>98.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.14</cell><cell></cell><cell></cell><cell cols="2">reference (122K)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">lowrank4 (13.4K)</cell><cell></cell><cell>98.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.13</cell><cell></cell><cell></cell><cell cols="2">lowrank8 (26.8K)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">lowrank16 (53.6K)</cell><cell></cell><cell>98</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.12</cell><cell></cell><cell></cell><cell cols="2">lowrank32 (107K)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">circulant (1674)</cell><cell></cell><cell>97.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>False Rejects</cell><cell>0.09 0.1 0.11</cell><cell></cell><cell></cell><cell cols="2">fastfood (5022) toeplitz-disprank1 (3348) toeplitz-disprank2 (6696) toeplitz-disprank10 (33.5K)</cell><cell>Accuracy (%)</cell><cell>97.4 97.6 97.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">fullyconnected (2.8M) reference (122K) lowrank4 (13.4K) lowrank8 (26.8K)</cell></row><row><cell></cell><cell>0.08</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">lowrank16 (53.6K)</cell></row><row><cell></cell><cell>0.07 0.06</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>97 96.8 96.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">lowrank32 (107K) circulant (1674) fastfood (5022) toeplitz-disprank1 (3348)</cell></row><row><cell></cell><cell>0.05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">toeplitz-disprank2 (6696)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>96.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">toeplitz-disprank10 (33.5K)</cell></row><row><cell></cell><cell>0.5</cell><cell>1</cell><cell>1.5</cell><cell>2</cell><cell>2.5</cell><cell>3</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>25</cell><cell>30</cell><cell>35</cell><cell>40</cell></row><row><cell></cell><cell></cell><cell></cell><cell>False Alarms per hour</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Time (hours)</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: We thank Yu-hsin Chen, Carolina Parada, Rohit Prabhavalkar, Alex Gruenstein, Rajat Monga, Baris Sumengen, Kilian Weinberger and Wenlin Chen for their contributions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Supplementary material: Structured transforms for small footprint deep learning</title>
		<ptr target="http://vikas.sindhwani.org/st_supplementary.pdf" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Small-footprint keyword spotting using deep neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Parada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICASSP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Compressing neural networks with the hashing trick</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03436</idno>
		<title level="m">Fast neural networks with circulant projections</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">High-performance neural networks for visual object classification</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1102.0183</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Memory-bounded deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICASSP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Low-precision storage for deep learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large-scale distributed deep networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Predicting parameters in deep learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shakibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<title level="m">Toeplitz and circulant matrices: A review. Foundations and Trends in Communications and Information Theory</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generalized displacement structure for block toeplitz, toeplitz block and toeplitz-derived matrices</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kailath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Displacement ranks of matrices and linear equations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kailath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Morf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Analysis and Applications</title>
		<imprint>
			<biblScope unit="page" from="395" to="407" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Displacement structure: Theory and applications</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kailath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Sayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An empirical evaluation of deep architectures on problems with many factors of variation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fastfood -approximating kernel expansions in loglinear time</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The mailman algorithm: a note on matrix vector multiplication</title>
		<author>
			<persName><forename type="first">E</forename><surname>Liberty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Zucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Information Processing Letters</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Pan</surname></persName>
		</author>
		<title level="m">Structured Matrices and Polynomials: Unified Superfast Algorithms</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Inversion of displacement operators</title>
		<author>
			<persName><forename type="first">V</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal of Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="page" from="660" to="677" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Random features for large-scale kernel machines</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast multidimensional convolution in low-rank tensor formats via cross approximation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Rakhuba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Oseledets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Low-rank matrix factorization for deep neural network training with high-dimensional output targets</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Arisoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for small-footprint keyword spotting</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Parada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improving the speed of neural networks on cpus</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Z</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moczulski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7149</idno>
		<title level="m">Deep fried convnets</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
