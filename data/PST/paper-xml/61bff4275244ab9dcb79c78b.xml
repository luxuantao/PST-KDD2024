<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards End-to-End Image Compression and Analysis with Transformers</title>
				<funder ref="#_YfG5pQC">
					<orgName type="full">National Key Research and Development Project</orgName>
				</funder>
				<funder ref="#_Q7S54JB #_wjJ6m2n">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_Tj6sYxP">
					<orgName type="full">China Postdoctoral Science Foundation</orgName>
				</funder>
				<funder ref="#_PQN6a8C #_T5yuCtp #_PY4RFRr #_xND2F5H">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuanchao</forename><surname>Bai</surname></persName>
							<email>yuanchao.bai@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xu</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xianming</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junjun</forename><surname>Jiang</surname></persName>
							<email>jiangjunjun@hit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
							<email>wangyw@pcl.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
							<email>xyji@tsinghua.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wen</forename><surname>Gao</surname></persName>
							<email>wgao@pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Towards End-to-End Image Compression and Analysis with Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose an end-to-end image compression and analysis model with Transformers, targeting to the cloud-based image classification application. Instead of placing an existing Transformer-based image classification model directly after an image codec, we aim to redesign the Vision Transformer (ViT) model to perform image classification from the compressed features and facilitate image compression with the long-term information from the Transformer. Specifically, we first replace the patchify stem (i.e., image splitting and embedding) of the ViT model with a lightweight image encoder modelled by a convolutional neural network. The compressed features generated by the image encoder are injected convolutional inductive bias and are fed to the Transformer for image classification bypassing image reconstruction. Meanwhile, we propose a feature aggregation module to fuse the compressed features with the selected intermediate features of the Transformer, and feed the aggregated features to a deconvolutional neural network for image reconstruction. The aggregated features can obtain the long-term information from the self-attention mechanism of the Transformer and improve the compression performance. The rate-distortion-accuracy optimization problem is finally solved by a two-step training strategy. Experimental results demonstrate the effectiveness of the proposed model in both the image compression and the classification tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Vision Transformer (ViT) <ref type="bibr" target="#b12">(Dosovitskiy et al. 2021</ref>) and its variations <ref type="bibr" target="#b37">(Touvron et al. 2021;</ref><ref type="bibr" target="#b42">Wu et al. 2021;</ref><ref type="bibr" target="#b44">Yuan et al. 2021;</ref><ref type="bibr">Chen et al. 2021b;</ref><ref type="bibr" target="#b24">Liu et al. 2021)</ref>, inherited from Transformer architecture <ref type="bibr" target="#b38">(Vaswani et al. 2017</ref>) in natural language processing (NLP), have recently demonstrated outstanding performance on a board range of image analysis tasks, such as image classification <ref type="bibr" target="#b12">(Dosovitskiy et al. 2021)</ref>, segmentation <ref type="bibr" target="#b47">(Zheng et al. 2021</ref>) and object detection <ref type="bibr" target="#b14">(Fang et al. 2021)</ref>. With the self-attention mechanism, these models are capable of capturing long-range dependencies in the image data, but inevitably result in high computational cost. In practice, Transformer-based models are usually deployed in the cloud-based paradigm and executed remotely. For example, massive image data is acquired by the frontend de-vices, such as mobile phones or surveillance cameras, and transmitted to the cloud (i.e., data center) for further analysis, sharing and storage. Image compression serves as a fundamental infrastructure for data communication between the frontend and the cloud.</p><p>In the traditional paradigm of cloud-based applications, image compression is considered independent of image analysis, and adopts lossy image compression standards designed for human vision, such as JPEG <ref type="bibr" target="#b39">(Wallace 1992)</ref>. In particular, the raw images are first transformed to the frequency domain with Discrete Cosine Transform (DCT). The frequency coefficients are then quantized to discard high frequencies that are less sensitive to human eyes. The quantized coefficients are encoded to bitstreams with entropy encoding and are transmitted to the cloud. On the cloud side, the quantized coefficients are recovered from the received bitstreams, which are then inversely transformed to reconstruct images. The reconstruction distortions are minimized with respect to Peak Signal-to-Noise Ratio (PSNR). However, if the reconstructed images optimized by PSNR are fed into the downstream image analysis tasks, which are tailored to machine vision instead, the corresponding results may be inaccurate, because the principle of machine vision is different from human vision <ref type="bibr" target="#b40">(Wang et al. 2020)</ref>. Besides, the traditional image codecs are comprised of hand-crafted modules with complex dependencies. It is difficult to optimize the sophisticated compression frameworks together with subsequent machine analysis tasks.</p><p>Recently, learning-based image compression emerges as an active research area in computer vision community. A number of learning-based image codecs, such as <ref type="bibr" target="#b35">(Toderici et al. 2016;</ref><ref type="bibr" target="#b34">Theis et al. 2017;</ref><ref type="bibr" target="#b23">Li et al. 2018;</ref><ref type="bibr" target="#b3">Ball? et al. 2018;</ref><ref type="bibr" target="#b28">Minnen, Ball?, and Toderici 2018;</ref><ref type="bibr" target="#b9">Cheng et al. 2020;</ref><ref type="bibr" target="#b27">Ma et al. 2020;</ref><ref type="bibr" target="#b19">Hu et al. 2021)</ref>, have achieved comparable or even better perceptual performance than traditional image codecs for human vision. Besides, by replacing the hand-crafted modules with deep neural networks (DNNs), learning-based image compression can be integrated with high-level tasks and end-to-end optimized for machine vision <ref type="bibr" target="#b36">(Torfason et al. 2018;</ref><ref type="bibr" target="#b6">Chamain et al. 2021;</ref><ref type="bibr" target="#b22">Le et al. 2021)</ref>. However, compared with image compression for human vision, image compression for machine vision is still in its infancy, because it is challenging to achieve the best of both worlds for low-level and high-level tasks.</p><p>The Thirty-Sixth AAAI Conference on Artificial Intelligence  In this paper, we propose a novel paradigm that is friendly for both human vision and machine vision, which integrates learning-based image compression with Transformer-based image analysis. The derived end-to-end image compression and analysis model leads to the synergy effect of these two tasks. Instead of placing an existing Transformer-based image classification model directly after an image codec, we redesign the ViT model to perform image classification from the compressed features <ref type="bibr" target="#b36">(Torfason et al. 2018</ref>) and facilitate image compression with the long-term information from the Transformer. Specifically, we replace the the patchify stem (i.e., image splitting and embedding) of the ViT model with a lightweight image encoder modelled by a convolutional neural network (CNN). The compressed features generated by the image encoder are injected convolutional inductive bias and are more expressive than the features extracted by the patchify stem from the decoded images. When transmitted to the cloud, the compressed features are fed to the Transformer for image classification bypassing image reconstruction. We further propose a feature aggregation module to fuse the compressed features with the selected intermediate features of the Transformer, and feed the aggregated features to a deconvolutional neural network for image reconstruction. The aggregated features obtain the long-term information from the Transformer and effectively improve the compression performance. We interpret the corresponding rate-distortion-accuracy optimization problem based on variational auto-encoder (VAE) <ref type="bibr" target="#b34">(Theis et al. 2017;</ref><ref type="bibr" target="#b3">Ball? et al. 2018</ref>) and information bottleneck (IB) <ref type="bibr" target="#b34">(Tishby, Pereira, and Bialek 2000;</ref><ref type="bibr" target="#b0">Alemi et al. 2017)</ref>, and finally solve it with a two-step training strategy.</p><p>The main contributions are summarized as follows: ? We propose an end-to-end image compression and analysis model, which performs image classification from the compressed features. We interpret the rate-distortionaccuracy optimization problem based on VAE and IB. ? We design the network by integrating learning-based image compression with ViT-based image analysis, which leads to the synergy between the two tasks. ? In terms of rate-distortion, the proposed model achieves PSNR performance close to BPG <ref type="bibr" target="#b4">(Bellard 2014)</ref>. In terms of rate-accuracy, the proposed model outperforms ResNet50 <ref type="bibr" target="#b16">(He et al. 2016)</ref>, DeiT-S <ref type="bibr" target="#b37">(Touvron et al. 2021)</ref> and Swin-T <ref type="bibr" target="#b24">(Liu et al. 2021</ref>) classification from the decoded images, while significantly reduces the computational cost under equivalent number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Image Compression for Machine Vision. With the fast progress of artificial intelligence, an increasing amount of visual data is now not only viewed by humans but also analyzed by machines. Recently, image/video compression for machine vision has drawn significant interests in the computer vision community <ref type="bibr" target="#b12">(Duan et al. 2020)</ref>.</p><p>In order to optimize image compression with analysis, <ref type="bibr" target="#b10">(Choi and Han 2020;</ref><ref type="bibr" target="#b26">Luo et al. 2021)</ref> and <ref type="bibr" target="#b5">(Chamain, Cheung, and Ding 2019)</ref> proposed to optimize the quantization of the traditional codecs JPEG and JPEG2000 to improve the performance of the following image classification. However, since the frameworks of traditional codecs are different from fully optimizable DNN and only the quantization is involved in the optimization, the improvement is limited. In contrast, learning-based image compression is more suitable to be jointly optimized with DNN-based image analysis. The related works can be divided into two categories: 1) RGB inference, such as <ref type="bibr" target="#b6">(Chamain et al. 2021</ref>) and <ref type="bibr" target="#b22">(Le et al. 2021)</ref>, performs image analysis from RGB reconstructed images by placing image analysis methods directly after existing image codecs. 2) Compressed inference, such as <ref type="bibr" target="#b36">(Torfason et al. 2018)</ref>, performs image analysis directly from the compressed features bypassing image reconstruction.</p><p>In this paper, we propose an end-to-end image compression and analysis model with Transformers, inspired by <ref type="bibr" target="#b36">(Torfason et al. 2018)</ref>. Beyond <ref type="bibr" target="#b36">(Torfason et al. 2018)</ref>, we interpret the rate-distortion-accuracy optimization problem based on VAE and IB, and design the Transformer-based model leading to the synergy between the two tasks.</p><p>Transformers in Computer Vision. Nowadays, Transformers have shown their potential to be a viable alternative to CNNs in computer vision tasks. However, the ViT model <ref type="bibr" target="#b12">(Dosovitskiy et al. 2021)</ref> without any human-defined inductive bias suffers from over-fitting when the training data is limited, and thus needs sophisticated data augmentation schemes <ref type="bibr" target="#b37">(Touvron et al. 2021)</ref>. In order to improve the performance and the robustness of Transformers, several works <ref type="bibr" target="#b42">(Wu et al. 2021;</ref><ref type="bibr" target="#b44">Yuan et al. 2021;</ref><ref type="bibr">Chen et al. 2021b</ref>) incorporated CNNs into Transformers.</p><p>In this paper, we propose to replace the patchify stem of the ViT model with a CNN-based image encoder, which can enable image analysis from the compressed features and effectively improve the performance of image classification. The concurrent work <ref type="bibr" target="#b43">(Xiao et al. 2021</ref>) also observes that early convolutions in Transformers can increase the optimization stability and improve the Top-1 accuracy. Our experimental results are consistent with the observation of <ref type="bibr" target="#b43">(Xiao et al. 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Method Problem Formulation</head><p>We aim to perform image analysis from the compressed features. Given a raw image x and its label y, our goal is to learn a compressed representation ? that facilitates both image decoding (reconstruction) and analysis, as sketched in Fig. <ref type="figure" target="#fig_0">1</ref>. Since the compressed representation ? is extracted from the image x while not accessing the label y, we assume that x, y, ? form a Markov chain y ? x ? ?, leading to p(?|x, y) = p(?|x).</p><p>Image Compression. We first formulate the lossy image compression model without taking image analysis into consideration. Following the standard framework of variational auto-encoder based image compression <ref type="bibr" target="#b34">(Theis et al. 2017;</ref><ref type="bibr" target="#b3">Ball? et al. 2018)</ref>, the latent representation z is transformed from the raw image x by an encoder and is quantized to the discrete-valued ?. Then, ? is losslessly compressed with entropy encoding techniques <ref type="bibr" target="#b41">(Witten, Neal, and Cleary 1987</ref>; Duda 2009) to form a bitstream. On the decoder side, ? is recovered from the bitstream and inversely transformed to a reconstructed image x. To optimize the performance of the compression model, it can be approximated by the minimization of the expectation of Kullback-Leibler (KL) divergence between the intractable true posterior p(?|x) and a parametric inference model q(?|x) over the data distribution p(x) <ref type="bibr" target="#b3">(Ball? et al. 2018</ref>):</p><formula xml:id="formula_0">E p(x) D kl [q(?|x)||p(?|x)] = E p(x) E q(?|x) [ : 0 log q(?|x) -log p(x|?) -log p(?)] + const (1) where D kl [?||?] denotes KL divergence.</formula><p>Because the transform from x to z is deterministic and the quantization of z is relaxed by adding noise from uniform distribution</p><formula xml:id="formula_1">U (-1 2 , 1 2 ), we have q(?|x) = i U (z i -1 2 , z i + 1</formula><p>2 ) and thus the first term log q(?|x) = 0. The second term of ( <ref type="formula">1</ref>) is interpreted as the expected distortion between x and x, and the third term is interpreted as the cost of encoding ?, leading to the rate-distortion trade-off <ref type="bibr" target="#b32">(Shannon 1948</ref>).</p><p>Image Analysis. We then turn to consider image analysis. We propose to maximize the mutual information I(?, y) between the compressed representation ? and the label y, inspired by information bottleneck (Tishby, Pereira, and Bialek 2000; <ref type="bibr" target="#b0">Alemi et al. 2017</ref>). The mutual information I(?, y) is the reduction in the uncertainty of y due to the knowledge of ?:</p><formula xml:id="formula_2">I(?, y) = H(y) -H(y|?) = H(y) + y,? p(y, ?) log p(y|?)<label>(2)</label></formula><p>where H(?) denotes the entropy. Because the true posterior p(y|?) is also intractable, we propose a variational approximation q(y|?), which is the decoder for image analysis apart from the decoder for image reconstruction. Since D kl [p(y|?)||q(y|?)] ? 0, we have y p(y|?) log p(y|?) ? y p(y|?) log q(y|?) and thus</p><formula xml:id="formula_3">I(?, y) ? H(y) + y,? p(y, ?) log q(y|?)<label>(3)</label></formula><p>Because the entropy H(y) is independent of ?, we can maximize y,? p(y, ?) log q(y|?) as a proxy for I(?, y).</p><p>Based on the Markov chain assumption, we replace p(y, ?) with x p(x, y, ?) = x p(x, y)p(?|x), and can rewrite y,? p(y, ?) log q(y|?) as E p(x,y) E p(?|x) log q(y|?) (4) With q(y|?), we can generate the estimated label ? from ?.</p><p>Joint Optimization. With ( <ref type="formula">1</ref>) and ( <ref type="formula">4</ref>), we further formulate the joint optimization of both image compression and analysis. Since p(?|x) in ( <ref type="formula">4</ref>) is intractable, we share the inference model q(?|x) in (1) as the approximation, and minimize the approximated negative (4) together with (1)<ref type="foot" target="#foot_0">1</ref> :</p><formula xml:id="formula_4">E p(x,y) {-?E q(?|x) log q(y|?) + D kl [q(?|x)||p(?|x)]} (5)</formula><p>where ? is a trade-off parameter. Suppose that p(x|?) is given by N (x|x, (2?) -1 1), we can finally rewrite (5) to the objective function:</p><formula xml:id="formula_5">E p(x,y) E q(?|x) [-? log q(y|?) + ? x -x 2 2 -log p(?)] (6)</formula><p>The first term of (6) weighted by ? can be interpreted as the cross-entropy loss for image analysis, such as image classification or segmentation, based on the types of label y. In this work, we choose image classification as the target task. The second term weighted by ? is the mean square error (MSE) distortion loss. The third term is the rate loss.</p><p>In contrast to the image compression models <ref type="bibr" target="#b34">(Theis et al. 2017;</ref><ref type="bibr" target="#b3">Ball? et al. 2018)</ref>, the compressed representation ? in ( <ref type="formula">6</ref>) is also optimized for image analysis tasks. The complexity of ? in ( <ref type="formula">6</ref>) is controlled by minimizing the cost of encoding ?, rather than controlled by minimizing I(?, x) in the information bottleneck models (Tishby, Pereira, and Bialek 2000; <ref type="bibr" target="#b0">Alemi et al. 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer-based Network Architecture</head><p>We realize the theoretical framework in Fig. <ref type="figure" target="#fig_0">1</ref> by proposing an end-to-end image compression and analysis model with Transformers. The proposed model can promote the synergy between the two tasks.</p><p>Encoder. The network architecture of the proposed encoder is illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. Similar to the setting of <ref type="bibr" target="#b3">(Ball? et al. 2018)</ref>, we employ four stride-2 5 ? 5 convolutional layers to extract features with gradually reduced spatial resolution from the input image x ? R H?W ?3 . We use LeakyReLU as the activation function instead of using the Generalized Divisive Normalization (GDN) <ref type="bibr" target="#b2">(Ball?, Laparra, and Simoncelli 2016)</ref>, because GDN results in convergence problem when training with the Transformer blocks in the proposed model.</p><p>The resulting feature z ? R</p><formula xml:id="formula_6">H 16 ? W</formula><p>16 ?M is then quantized to the discrete-valued ?. We employ the hyper-prior module <ref type="bibr" target="#b3">(Ball? et al. 2018;</ref><ref type="bibr" target="#b28">Minnen, Ball?, and Toderici 2018)</ref> to estimate p(?|h ?) for the entropy encoding of ?, where h ? denotes the hyper-prior of ?. We do not use the serial autoregressive module <ref type="bibr" target="#b28">(Minnen, Ball?, and Toderici 2018)</ref>, because its corresponding decoding time is too long on largescale image classification datasets.</p><p>From the perspective of Transformer architecture design, the proposed encoder can be considered as the replacement of the patchify stem, i.e., a stride-16 16 ? 16 convolutional layer, applied to the input image in the ViT model <ref type="bibr" target="#b12">(Dosovitskiy et al. 2021)</ref>. Several concurrent works <ref type="bibr" target="#b42">(Wu et al. 2021;</ref><ref type="bibr" target="#b44">Yuan et al. 2021;</ref><ref type="bibr">Chen et al. 2021b;</ref><ref type="bibr" target="#b43">Xiao et al. 2021</ref>) also replace the patchify stem with a stack of convolutional layers, in order to improve the performance of image classification. In the experiments, we observe that the proposed encoder is capable of extracting compressed features suitable for both image decoding reconstruction and classification through joint optimization. Note that the proposed encoder is relatively lightweight, thus can be deployed at the frontend, such as mobile phones or surveillance cameras.</p><p>Decoder-Classifier. The decoder receives the bitstream from the encoder and adopts the shared hyper-prior module p(?|h ?) to recover ?. For image classification, we directly feed ? to an inference network, instead of the common subsequent approach-first reconstruct the decoded image</p><p>x and then conduct inference on x.</p><p>Specifically, we adopt the standard Transformer blocks in the ViT model <ref type="bibr" target="#b12">(Dosovitskiy et al. 2021</ref>) with the number of parameters equivalent to ResNet50 <ref type="bibr" target="#b16">(He et al. 2016)</ref>, as shown in Fig. <ref type="figure" target="#fig_2">3</ref>. We expand the channel dimension of ? to C with a 1 ? 1 convolutional layer, and reshape the resulting feature ?0 ? R H 16 ? W 16 ?C to a sequence ?0 ? R HW 16 2 ?C . To maintain the spatial information of the feature ?0 , we add learnable position embeddings p to ?0 leading to z0 = ?0 + p. Following <ref type="bibr" target="#b12">(Dosovitskiy et al. 2021)</ref>, we prepend a learnable class embedding c0 , and feed the sequence [c 0 ; z0 ] ? R ( HW 16 2 +1)?C to the Transformer consisting of L Transformer blocks. The architecture of each Transformer block is illustrated in Fig. <ref type="figure" target="#fig_2">3</ref>. The computation process can be formulated as</p><formula xml:id="formula_7">[c i ; z i ] = MSA(LN([c i-1 ; zi-1 ])) + [c i-1 ; zi-1 ] (7) [c i ; zi ] = FFN(LN([c i ; z i ])) + [c i ; z i ] i = 1, . . . , L</formula><p>where MSA(?) denotes the multi-head self-attention module, FFN(?) denotes the feed forward network and LN(?) denotes the layer normalization <ref type="bibr" target="#b1">(Ba, Kiros, and Hinton 2016)</ref>, respectively.</p><p>With the self-attention mechanism, the class embedding ci interacts with the image feature zi , and the final output cL is used to compute q(y|?) for image classification:</p><formula xml:id="formula_8">q(y|?) = Softmax(FFN(LN(c L )))<label>(8)</label></formula><p>where Softmax(?) denotes softmax operation. The FFN(?)</p><p>is the classifier head mapping the embedding dimension from C to the number of classes.</p><p>Decoder-Reconstructor. Reconstructing image x directly from ? (or ?0 ) ignores the global spatial correlations among the latent features. Recent image compression works <ref type="bibr" target="#b30">(Qian et al. 2021;</ref><ref type="bibr" target="#b15">Guo et al. 2021)</ref> demonstrate that leveraging global context information during entropy coding can improve the compression performance. Transformers naturally capture the global spatial information among the latent features, which can also benefits low-level tasks, such as image processing <ref type="bibr">(Chen et al. 2021a</ref>) and image generation <ref type="bibr" target="#b20">(Jiang, Chang, and Wang 2021)</ref>. Motivated by these works, we aim to extract the intermediate features zi 's of the Transformer and incorporate them into image reconstruction. Specifically, we select ?0 and [z 1 , z2 , z3 ], and propose a feature aggregation module to fuse these features, similar to <ref type="bibr" target="#b47">(Zheng et al. 2021)</ref>. Selecting [z 1 , z2 , z3 ] means that the first three Transformer blocks are also involved in the image reconstruction process. Since image reconstruction may work independently of image classification, we avoid using {z i (i &gt; 3)} that involve too many Transformer blocks in the image reconstruction, in order to reduce the computational complexity. The feature aggregation module is illustrated in Fig. <ref type="figure" target="#fig_2">3</ref>. The computational process can be formulated as</p><formula xml:id="formula_9">? 0 = Conv 0 1 (? 0 ), z 1 = Conv 1 1 (z 1 ) z 2 = Conv 2 1 (z 2 ), z 3 = Conv 3 1 (z 3 ) (9) ?f = Conv 2 ([? 0 ; z 1 ; z 2 ; z 3 ])</formula><p>where Conv i 1 (?) denotes a 1?1 convolutional layer reducing the channel dimension of the input to C 4 . Conv 2 (?) is another 1 ? 1 convolutional layer with C channels fusing the four concatenated input features. ?f is the fused feature.</p><p>Finally, we input the fused feature ?f to four stride-2 5?5 deconvolutional layers gradually increasing the spatial resolution, leading to the reconstructed RGB image x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Strategy</head><p>We observe that the one-step training strategy, i.e., minimizing (6) to train the encoder and decoder from scratch, leads to convergence problem in the experiments. Instead, we employ a two-step training strategy: 1) We pretrain the proposed model without considering the quantization of z and the hyper-prior module of ?. We remove the rate loss in (6) temporarily, and minimize the cross-entropy loss together with the MSE loss. Because the value of the cross-entropy loss is much smaller than that of the MSE loss, we set ? = 1 and ? = 0.001 in ( <ref type="formula">6</ref>) to balance the contributions of the two losses. 2) We load the pretrained parameters and minimize (6) to train the entire network including the quantization of z and the hyper-prior module of ?. The ? and ? in (6) are tuned with fixed ? ? to achieve different bit rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Experimental Settings</head><p>Datasets. We perform extensive experiments on the Ima-geNet dataset <ref type="bibr" target="#b11">(Deng et al. 2009</ref>) and iNaturalist19 (INat19) Pretraining w/o Compression. As aforementioned, we pretrain the proposed model without the quantization of z and the hyper-prior module of ?. We remove the rate loss, and minimize the cross entropy loss together with the MSE loss. We set ? = 1 and ? = 0.001, respectively. We set the input size to 224?224 and adopt the same data augmentation as DeiT <ref type="bibr" target="#b37">(Touvron et al. 2021)</ref>, except for the Exponential Moving Average (EMA) <ref type="bibr" target="#b29">(Polyak and Juditsky 1992)</ref> On the ImageNet dataset, we train the proposed network from scratch. We use AdamW optimizer <ref type="bibr" target="#b25">(Loshchilov and Hutter 2019)</ref> for 300 epochs with minibatches of size 1024. We set the initial learning rate to 0.001 and use a cosine decay learning rate scheduler with 5 epochs warm-up.</p><p>On the INat19 dataset, we initialize the network with the pretrained parameters on the ImageNet dataset. The classifier head is adjusted to the class number of INat19. We use AdamW optimizer for 100 epochs with minibatches of size 512. We set the initial learning rate to 0.0005 and use a cosine decay learning rate scheduler with 2 epochs warm-up.</p><p>Training w/ Compression. We load the pretrained parameters on the ImageNet and INat19 datasets, respectively. We recover the quantization of z and the hyper-prior module of ?. We fix ? ? = 100 and set ? ? {0.1, 0.3, 0.6}. We observe that the hyper-prior module of ? is sensitive to data augmentation, and thus we only employ RandomResizedCropAnd-Interpolation and RandomHorizontalFlip during training.</p><p>On the ImageNet dataset, we load the corresponding pretrained parameters, and use Adam optimizer <ref type="bibr" target="#b21">(Kingma and Ba 2015)</ref> with a initial learning rate of 0.0001, following <ref type="bibr" target="#b3">(Ball? et al. 2018)</ref>. We train the proposed network for 300 epochs with minibatches of size 1024, and use a cosine decay learning rate scheduler with 5 epochs warm-up.</p><p>On the INat19 dataset, we load the corresponding pretrained parameters, and also use Adam optimizer with a initial learning rate of 0.0001. We train the proposed network for 300 epochs with minibatches of size 512, and use a cosine decay learning rate scheduler with 2 epochs warm-up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>Pretrained Model. Table <ref type="table" target="#tab_1">1</ref>(a) reports the experimental results of our pretrained model without compression on Ima-geNet. We compare with the existing image classification models including CNN-based models, such as ResNet50 <ref type="bibr" target="#b16">(He et al. 2016</ref>) and RegNetY-4G <ref type="bibr" target="#b31">(Radosavovic et al. 2020)</ref>, and Transformer-based models, such as ViT-B <ref type="bibr" target="#b12">(Dosovitskiy et al. 2021)</ref>, DeiT-S <ref type="bibr" target="#b37">(Touvron et al. 2021</ref><ref type="bibr">), CvT-13 (Wu et al. 2021)</ref>, CeiT-S <ref type="bibr" target="#b44">(Yuan et al. 2021)</ref>, Visformer-S <ref type="bibr">(Chen et al. 2021b)</ref>, Swin-T <ref type="bibr" target="#b24">(Liu et al. 2021)</ref> and ViT C -4GF <ref type="bibr" target="#b43">(Xiao et al. 2021)</ref>. We select the specific settings of the models with the number of parameters closest to ResNet50. DeiT-S and Swin-T. ResNet50 is finetuned based on <ref type="bibr" target="#b16">(He et al. 2016)</ref>. DeiT-S and Swin-T are finetuned using the same setting as our pretrained model.</p><p>From Table <ref type="table" target="#tab_1">1</ref>, our pretrained model can achieve comparable or better Top-1 accuracies than the existing image classification models, which demonstrates the efficacy of the image encoder for the Transformer-based image classification. In terms of image reconstruction evaluated by PSNR, our pretrained model achieves 31.7 dB and 31.5 dB, respectively. All these results demonstrate that the pretrained model can provide a satisfactory initialization for the following training with compression.</p><p>Full Model. In Fig. <ref type="figure" target="#fig_4">4</ref>, we report the rate-distortion and rate-accuracy results of our full model on ImageNet and INat19. We compare with the existing image codecs and the image classification models applied to the decoded images.</p><p>We select the traditional image codecs including JPEG <ref type="bibr" target="#b39">(Wallace 1992</ref>), JPEG2000 <ref type="bibr" target="#b33">(Skodras, Christopoulos, and Ebrahimi 2001)</ref>, BPG <ref type="bibr" target="#b4">(Bellard 2014)</ref>, and the learningbased image codecs including bmshj <ref type="bibr" target="#b3">(Ball? et al. 2018</ref>) and mbt-m <ref type="bibr" target="#b28">(Minnen, Ball?, and Toderici 2018)</ref>. The mbt-m removes the serial autoregressive module, avoiding long decoding time on the large-scale datasets. The sophisticated learning-based image codecs with complex entropy models and network architectures, such as <ref type="bibr" target="#b18">(Hu, Yang, and Liu 2020;</ref><ref type="bibr" target="#b9">Cheng et al. 2020;</ref><ref type="bibr" target="#b30">Qian et al. 2021;</ref><ref type="bibr" target="#b15">Guo et al. 2021)</ref>, are too time-consuming to be evaluated on the large-scale datasets, despite their good compression performance.</p><p>We select the decoded images of the best performed traditional and learning-based image codecs in our experiments, i.e., BPG and mbt-m, and adopt the image classification models ResNet50, DeiT-S and Swin-T to compute the Top-1 accuracies in comparison with the proposed model. More-  In terms of the rate-distortion performance, the proposed model significantly outperforms the traditional image codecs JPEG and JPEG2000. It is comparable to BPG at relatively low bit-rates but is surpassed by BPG at high bitrates. The proposed model outperforms the learning-based image codecs bmshj and mbt-m. The rate-distortion performance of the jointly finetuned mbt-m is similar to the original mbt-m. The proposed model also outperforms the jointly finetuned mbt-m.</p><p>In terms of the rate-accuracy performance, the proposed model outperforms ResNet50, DeiT-S and Swin-T applied to the decoded images of BPG and mbt-m, because the image classification models trained on the original datasets are not robust to the decoded images at low bit-rates. Although Swin-T outperforms DeiT-S on the original datasets (Table. 1), DeiT-S outperforms Swin-T on the decoded images at low bit-rates. After jointly finetuned, Swin-T surpasses DeiT-S on the decoded images at the corresponding bit-rates. The proposed model also outperforms the jointly </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies</head><p>Rate-Distortion-Accuracy Trade-off. When the bit-rates of ? is constrained, minimizing (6) with different ? ? leads to bit allocation between image classification and reconstruction. We empirically set ? ? ? {50, 100, 200} and test the rate-distortion-accuracy trade-off as shown in Fig. <ref type="figure" target="#fig_6">6</ref>. We can observe that larger ? ? leads to better Top-1 accuracy but sacrifices PSNR. In contrast, smaller ? ? leads to better PSNR but lower Top-1 accuracy.</p><p>Feature Aggregation. In Fig. <ref type="figure" target="#fig_6">6</ref>, we compare with the proposed model without the feature aggregation module, i.e., directly feeding ?0 to the deconvolutional neural network for image reconstruction. Although the feature aggregation module is only applied to the image reconstruction process, it can benefit both the image classification and reconstruction through rate-distortion-accuracy optimization. The reduced bit-rates from the image reconstruction are potentially allocated to the image classification, leading to the improvement of both tasks. Although the improvement of Top-1 accuracy is more obvious than PSNR in Fig. <ref type="figure" target="#fig_6">6</ref>, the decrease  of the cross entropy loss -? log q(y|?) is actually similar to that of the MSE loss ? xx 2 2 in (6) in the experiments. Computational Cost. In Fig. <ref type="figure" target="#fig_7">7</ref>, we compare the computational cost of the proposed model with the concatenation of the learning-based image codec mbt-m and the image classification methods including ResNet50, DeiT-S and Swin-T. The architecture of the proposed encoder is similar to the low-rate setting of mbt-m, thus their computational costs of image encoding are similar. In terms of image reconstruction on the decoder side, our image reconstructor needs more FLOPs than mbt-m due to the feature aggregation module. In terms of image classification on the decoder side, our image classifier directly performs inference from the compressed features without the image reconstruction process, and thus needs far less computational cost compared with the inference from reconstructed RGB images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we learn an end-to-end image compression and analysis model with Transformers, targeting to the cloudbased image classification application. At the frontend, a CNN-based image encoder extracts compressed features from raw images and transmits them to the cloud. On the cloud, the compressed features injected convolutional inductive bias are directly fed to the Transformer for image classification bypassing image reconstruction. Meanwhile, the intermediate features of the Transformer capturing global information are aggregated with the compressed features for image reconstruction. Experimental results demonstrate the effectiveness of the proposed model in both rate-distortion and rate-accuracy performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The theoretical framework of the proposed end-toend image compression and analysis model.</figDesc><graphic url="image-1.png" coords="3,65.93,54.00,214.66,78.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The network architecture of the encoder. We set N = 128 and M = 192 same as the low-rate setting of (Ball? et al. 2018). The encoder replaces the patchify stem of ViT and is relatively lightweight, which can be deployed at the frontend. Q: Quantization. AE: Arithmetic Encoder.</figDesc><graphic url="image-2.png" coords="3,321.89,54.00,233.73,90.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The network architecture of the decoder. We set C = 384, L = 12 and N = 128. The number of parameters is equivalent to ResNet50 (He et al. 2016). The decoder is deployed on the cloud to classify or reconstruct images from the received bitstreams. AD: Arithmetic Decoder.</figDesc><graphic url="image-3.png" coords="5,59.04,54.00,493.94,214.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) Rate-distortion and rate-accuracy results on ImageNet. (b) Rate-distortion and rate-accuracy results on INat19.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Rate-distortion and rate-accuracy results of the proposed model, compared with the existing image codecs and the image classification methods applied to the reconstructed RGB images. JFT means joint finetune.</figDesc><graphic url="image-5.png" coords="6,321.99,224.95,243.27,146.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figure 5: (a) JPEG+DeiT-S, 29.3dB, 0.60bpp, maraca (?). (b) JPEG2000+DeiT-S, 29.17dB, 0.41bpp, can opener (?). (c) BPG+DeiT-S, 29.41dB, 0.18bpp, can opener (?). (d) mbt-m+DeiT-S, 29.79dB, 0.18bpp, can opener (?). (e) mbt-m+Swin-T(JFT), 29.75dB, 0.19bpp, reel (?). (f) Ours, 29.52dB, 0.19bpp, teddy bear ( ).</figDesc><graphic url="image-9.png" coords="7,57.52,165.20,76.33,76.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Ablation studies of rate-distortion-accuracy tradeoff and the feature aggregation (FA) module.</figDesc><graphic url="image-12.png" coords="7,320.69,54.00,236.11,118.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Ablation study of the computational cost.</figDesc><graphic url="image-13.png" coords="7,320.69,217.09,236.11,88.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, which do not enhance the performance of the proposed model. The input images are normalized with ImageNet default mean and standard deviation, and are denormalized during image reconstruction. We observe that random erasing (Zhong et al. 2020), mixup (Zhang et al. 2017) and cutmix (Yun et al. 2019) designed for the training of image classification are also compatible with the training of image reconstruction in our experiments.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 (</head><label>1</label><figDesc>b) reports the experimental results of our pretrained model on INat19 dataset, compared with ResNet50,</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Results of our pretrained model without compression on the ImageNet/INat19 datasets, compared with the existing image classification models.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>E p(x) D kl [q(?|x)||p(?|x)] = E p(x,y) D kl [q(?|x)||p(?|x)]</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported by <rs type="funder">National Key Research and Development Project</rs> under Grant <rs type="grantNumber">2019YFE0109600</rs>, <rs type="funder">National Natural Science Foundation of China</rs> under Grants <rs type="grantNumber">61922027</rs>, <rs type="grantNumber">61827804</rs>, <rs type="grantNumber">6207115</rs>, <rs type="grantNumber">61971165</rs> and <rs type="grantNumber">U20B2052</rs>, <rs type="projectName">PCNL Key Project</rs> under Grant <rs type="grantNumber">PCL2021A07</rs>, <rs type="funder">China Postdoctoral Science Foundation</rs> under Grant <rs type="grantNumber">2020M682826</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_YfG5pQC">
					<idno type="grant-number">2019YFE0109600</idno>
				</org>
				<org type="funding" xml:id="_Q7S54JB">
					<idno type="grant-number">61922027</idno>
				</org>
				<org type="funding" xml:id="_wjJ6m2n">
					<idno type="grant-number">61827804</idno>
				</org>
				<org type="funding" xml:id="_PQN6a8C">
					<idno type="grant-number">6207115</idno>
				</org>
				<org type="funding" xml:id="_T5yuCtp">
					<idno type="grant-number">61971165</idno>
				</org>
				<org type="funded-project" xml:id="_PY4RFRr">
					<idno type="grant-number">U20B2052</idno>
					<orgName type="project" subtype="full">PCNL Key Project</orgName>
				</org>
				<org type="funding" xml:id="_Tj6sYxP">
					<idno type="grant-number">PCL2021A07</idno>
				</org>
				<org type="funding" xml:id="_xND2F5H">
					<idno type="grant-number">2020M682826</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep variational information bottleneck</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer Normalization</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Density modeling of images using a generalized normalization transformation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ball?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Laparra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Variational image compression with a scale hyperprior</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ball?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Johnston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">F</forename><surname>Bellard</surname></persName>
		</author>
		<ptr target="https://bellard.org/bpg/" />
	</analytic>
	<monogr>
		<title level="j">BPG Image format</title>
		<imprint>
			<biblScope unit="page" from="2020" to="2029" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quannet: Joint Image Compression and Classification Over Channels with Limited Bandwidth</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Chamain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><forename type="middle">S</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="338" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-to-End optimized image compression for machines, a study</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Chamain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Racap?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Begaint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pushparaja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feltman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Compression Conference (DCC)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="163" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">2021a. Pre-trained image processing transformer</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.12533</idno>
		<title level="m">Visformer: The vision-friendly transformer</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learned image compression with discretized gaussian mixture likelihoods and attention modules</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Takeuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Katto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7939" to="7948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Task-Aware Quantization Network for JPEG Image Compression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="309" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Video Coding for Machines: A Paradigm of Collaborative Compression and Intelligent Analytics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2021. 2020</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="8680" to="8695" />
		</imprint>
	</monogr>
	<note>An image is worth 16x16 words: Transformers for image recognition at scale</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Duda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0902.0271</idno>
		<title level="m">Asymmetric numeral systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.00666</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Causal Contextual Prediction for Learned Image Compression</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Aodha</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/c/inaturalist-2019-fgvc6" />
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="2021" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Coarse-to-fine hyperprior modeling for learned image compression</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11013" to="11020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning end-toend lossy image compression: A benchmark</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.07074</idno>
		<title level="m">TransGAN: Two Transformers Can Make One Strong GAN</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image Coding For Machines: an End-To-End Learned Approach</title>
		<author>
			<persName><forename type="first">N</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cricri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Youvalari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1590" to="1594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning Convolutional Networks for Content-Weighted Image Compression</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3214" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Decoupled Weight Decay Regularization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The Rate-Distortion-Accuracy Tradeoff: JPEG Case Study</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Compression Conference (DCC)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="354" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">End-to-End Optimized Versatile Image Compression With Wavelet-Like Transform</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint autoregressive and hierarchical priors for learned image compression</title>
		<author>
			<persName><forename type="first">D</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ball?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="10771" to="10780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Juditsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on control and optimization</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="838" to="855" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning Accurate Entropy Model with Global Reference for Image Compression</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Designing Network Design Spaces</title>
		<author>
			<persName><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10425" to="10433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Bell system technical journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The jpeg 2000 still image compression standard</title>
		<author>
			<persName><forename type="first">A</forename><surname>Skodras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Christopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="36" to="58" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Lossy image compression with compressive autoencoders</title>
		<author>
			<persName><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bialek</surname></persName>
		</author>
		<idno>arXiv preprint physics/0004057</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2000">2017. 2000</date>
		</imprint>
	</monogr>
	<note>The information bottleneck method</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Variable Rate Image Compression with Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>O'malley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Covell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Towards Image Understanding from Deep Compression without Decoding</title>
		<author>
			<persName><forename type="first">R</forename><surname>Torfason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mentzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The JPEG still picture compression standard</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Consumer Electronics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="xviii" to="xxxiv" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Highfrequency component helps explain the generalization of convolutional neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8684" to="8694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Arithmetic coding for data compression</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Cleary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="520" to="540" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15808</idno>
		<title level="m">Cvt: Introducing convolutions to vision transformers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.14881</idno>
		<title level="m">Early Convolutions Help Transformers See Better</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11816</idno>
		<title level="m">Incorporating Convolution Designs into Visual Transformers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
		<title level="m">CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features. International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6022" to="6031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ciss?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond Empirical Risk Minimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020">2021. 2020</date>
		</imprint>
	</monogr>
	<note>AAAI Conference on Artificial Intelligence</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
