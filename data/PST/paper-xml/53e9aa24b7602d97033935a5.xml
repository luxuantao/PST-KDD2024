<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PyPy&apos;s Approach to Virtual Machine Construction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Armin</forename><surname>Rigo</surname></persName>
							<email>arigo@tunes.org</email>
							<affiliation key="aff0">
								<orgName type="department">Heinrich-Heine-Universität Düsseldorf Institut für Informatik Universitätsstraße 1</orgName>
								<address>
									<postCode>D-40225</postCode>
									<settlement>Düsseldorf</settlement>
									<country key="DE">Deutschland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Samuele</forename><surname>Pedroni</surname></persName>
							<email>pedronis@strakt.com</email>
							<affiliation key="aff1">
								<orgName type="department">AB Strakt Norra Ågatan</orgName>
								<address>
									<postCode>10A 416 64</postCode>
									<settlement>Göteborg</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PyPy&apos;s Approach to Virtual Machine Construction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2E62011DB54E9231B3DA5A4A07AF82EF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>D.3.4 [Programming Languages]: Processors-code generation</term>
					<term>interpreters</term>
					<term>run-time environments; F.3.2 [Logics and Meanings of Programs]: Semantics of Programming Languages-program analysis General Terms Languages</term>
					<term>Experimentation</term>
					<term>Performance Virtual Machine</term>
					<term>Metacircularity</term>
					<term>Type Inference</term>
					<term>Retargettable Code Generation</term>
					<term>Python</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The PyPy project seeks to prove both on a research and a practical level the feasibility of constructing a virtual machine (VM) for a dynamic language in a dynamic language -in this case, Python. The aim is to translate (i.e. compile) the VM to arbitrary target environments, ranging in level from C/Posix to Smalltalk/Squeak via Java and CLI/.NET, while still being of reasonable efficiency within these environments.</p><p>A key tool to achieve this goal is the systematic reuse of the Python language as a system programming language at various levels of our architecture and translation process. For each level, we design a corresponding type system and apply a generic type inference engine -for example, the garbage collector is written in a style that manipulates simulated pointer and address objects, and when translated to C these operations become C-level pointer and address instructions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Despite the constant trend in the programming world towards portability and reusability, there are some areas in which it is still notoriously difficult to write flexible, portable, and reasonably efficient programs. The implementation of virtual machines is one such area. Building implementations of general programming languages, in particular highly dynamic ones, using a classic direct coding approach, is typically a long-winded effort and produces a result that is tailored to a specific platform and where architectural decisions (e.g. about GC) are spread across the code in a pervasive and invasive way.</p><p>For this and other reasons, standard platforms emerge; nowadays, a language implementer could cover most general platforms in use by writing three versions of his virtual machine: for C/Posix, for Java, and for CLI/.NET. This is, at least, the current situation of the Python programming language, where independent volunteers have developed and are now maintaining Java and .NET versions of Python, which follow the evolution of the "official" C version (CPython).</p><p>However, we believe that platform standardization does not have to be a necessary component of this equation. We are basically using the standard "meta-programming" argument: if one could write the VM at a sufficiently high level of abstraction in a language supporting such level, then the VM itself could be automatically translated to any lower-level platform. Moreover by writing the VM in such a way we would gain in flexibility in architectural choices and expressiveness.</p><p>PyPy achieves this goal without giving up on the efficiency of the compiled VMs, thereby keeping it within reach of further reasonable optimization work.</p><p>The key factors enabling this result are not to be found in recent advances in any particular research area -we are not for example using constraint-based type inference. Instead, we are following a novel overall architecture: it is split into many levels of stepwise translation from the high-level source of the VM to the final target platform, each step adding support for features that were assumed primitive by the previous ones. Similar platforms can reuse many of these steps, while for very different platforms we have the option to perform very different translation steps. Each step reuses a common type inference component with a different, ad-hoc type system.</p><p>Experiments also suggest a more mundane reason why such an approach is only practical today: a typical translation takes about half an hour on a modern PC and consumes between 512MB and 1GB of RAM.</p><p>Recent projects developing meta-circular interpreters <ref type="bibr" target="#b20">[21]</ref>[3][1] incorporate a native compiler directly in their virtual machine design and high-level implementations. We believe that this is a less evolvable and maintainable encoding of the semantics of a language than a bytecode intepreter, especially with languages such as Python for which internal design simplicity was not a goal from the start or in its current evolution. Our on-going efforts (beyond the scope of this paper) aim instead at generating native compilers as a (non-trivial) step of our translation process (see sections <ref type="bibr">7 and 6)</ref>. This belief and goal inform our architecture.</p><p>In the paper we shortly describe the architecture of PyPy in section 2. In section 3 we describe our approach of varying the type systems at various levels of the translation. Section 4 gives an overview of the type inference engine we developed (and can be read independently from section 3). We present experimental results in section 5 and future work directions in section <ref type="bibr" target="#b5">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Architecture</head><p>There are two major components in PyPy:</p><p>1. the Standard Interpreter: an implementation of the Python programming language, mostly complete and compliant with the current version of the language, Python 2.4.</p><p>2. the Translation Process: a translation tool-suite whose goal is to compile subsets of Python to various environments.</p><p>In particular, we have defined a subset of the Python language called "restricted Python" or RPython. This sublanguage is not restricted syntactically, but only in the way it manipulates objects of different types. The restrictions are a compromise between the expressivity and the need to statically infer enough type information to generate efficient code. RPython still supports exceptions, inheritance but limited to single inheritance with some mix-in support, dynamic dispatch, to some extent keywords arguments and varargs, first-class function and class values, limited use of bound methods, runtime isinstance and type queries, but no runtime reflection. Bindings in class and global namespaces are assumed constant. RPython code can be run on a Python interpreter without severe semantics mismatches. Figure <ref type="figure">1</ref> shows some RPython code from the Standard Interpreter and it can be seen that it is still quite idiomatic Python code using the built-in dictionary type and first-class class values instantiation. The purpose of the translation tool-suite is to compile such RPython programs to a variety of different platforms.</p><p>Our current efforts, and the present paper, focus on this toolsuite. We will not describe the Standard Interpreter component of PyPy in the sequel, other than to mention that it is written in RPython and can thus be translated. At close to 90,000 lines of code 1 , it is the largest RPython program that we have translated so far and is the main target driving the development of the tool-chain. More information can be found in <ref type="bibr" target="#b15">[16]</ref>. 1 The core interpreter is more like 30,000 lines of code, the rest includes built-in modules and 30,000 lines of auto-generated unicode character database data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">System programming in Python</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The translation process</head><p>The translation process starts from RPython source code and eventually produces low-level code suitable for the target environment. Its architecture is shown in figure <ref type="figure">2</ref>. It can be described as a frontend followed by a series of step-wise transformations. Each transformation step is based on control flow graph transformations and rewriting, and on the ability to augment the program with further implementation code written in Python and analysed with the suitable type system.</p><p>The front-end part of the translation process analyses the input RPython program in two phases, as follows: <ref type="foot" target="#foot_0">2</ref>1. We take as input RPython function objects, <ref type="foot" target="#foot_1">3</ref> and convert them to control flow graphs -a structure amenable to analysis. These flow graphs contain polymorphic operations only: in Python, almost all operations are dynamically overloaded by type.</p><p>2. We perform type inference on the control flow graphs. At this stage, types inferred are part of the type system which is the very definition of the RPython sub-language: they are roughly a subset of Python's built-in types, with some more precision to describe e.g. the items stored in container types. Occasionally, a single input function can produce several specialized versions, i.e. several similar but differently typed graphs. This type inference process is described in more details in section 4.</p><p>At the end of the front-end analysis, the input RPython program is represented as a forest of flow graphs with typed variables. Following this analysis are a number of transformation steps. Each transformation step modifies the graphs in-place, by altering their structure and/or the operations they contain. Each step inputs graphs typed in one type system and leaves them typed in a possibly different type system, as we will describe in the sequel. Finally, a back-end turns the resulting graphs into code suitable for the target environment, e.g. C source code ready to be compiled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Transformations</head><p>The first of the transformation steps takes the RPython-typed flow graphs, still containing polymorphic operations only, and produces flow graphs with operations more familiar to the target environment. When the target is C or a C-like environment, this means monomorphic C-like operations and C-like types. In the simplest case, this is the only transformation step: these graphs are directly fed to the C emitting back-end, which turns them into ANSI C source code. Not all targets of interest are C-like, though, and we have recently developed a variant of this step for use when targeting higher-level, object-oriented (OO) environments. Its design was motivated by work on back-ends for Smalltalk/Squeak <ref type="foot" target="#foot_2">4</ref>  For an example of a transformation that is applied after this first step, consider that RPython has automatic memory management. Even with the LLTyper, the first transformation step produces flow graphs that also assume automatic memory management. Generating C code directly from there produces a fully leaking program, unless we link it with an external garbage collector (GC) like the Boehm conservative GC <ref type="bibr" target="#b6">[7]</ref> <ref type="bibr" target="#b5">[6]</ref>, which is a viable option.</p><p>We have two other alternatives, each implemented as a transformation step. The first one inserts naive reference counting throughout the whole program's graphs, which without further optimizations gives a somewhat bad performance (it should be noted that the CPython interpreter is also based on reference counting, and experience suggests that it was not a bad choice in this particular case).</p><p>The other, and better, alternative is an exact GC, coupled with a transformation, the GC transformer. It takes as input C-level-typed graphs and replaces all malloc operations with calls to a garbage collector's allocation routine. The transformation inspects all the graphs to discover the structure types in use by the program, and assigns a unique type id to each of them. These type ids are then collected in internal tables that describe the layout of the structures, e.g. their sizes and the location of the pointer fields.</p><p>We have implemented other transformations as well, e.g. performing various optimizations, or turning the whole code into a style that allows us to use coroutines (still in ANSI C: it is a variant of continuation-passing that will be the subject of another paper.) Another example is the exception transformer, which transforms graphs that still contain implicit exception handling into a form suitable for C (currently based on a global flag to signal the presence of an exception, which is set and checked around selected function calls).</p><p>More information about these transformations can be found in <ref type="bibr" target="#b16">[17]</ref>.</p><p>def ll_append(lst, newitem):</p><p># Append an item to the end of the vector. index = lst.length #get the 'length' field ll_resize(lst, index+1) #call another helper itemsarray = lst.items #get the 'items' field itemsarray[index] = item #behaves like a C array Figure <ref type="figure">3</ref>. a helper to implement list.append().</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">System code</head><p>A common pattern in all the transformation steps is to somehow lower the level at which the graphs are currently expressed. Because of this, there are operations that were atomic in the input (higher-level) graphs but that need to be decomposed into several operations in the target (lower-level) graphs. In some cases, the equivalent functionality requires more than a couple of operations: a single operation must be replaced by a call to whole new codefunctions and classes that serve as helpers. An example of this is the malloc operation for the GC transformer. Another example is the list.append() method, which is atomic for Python or RPython programs, but needs to be replaced in C-level code by a helper that possibly reallocates the array of items.</p><p>This means that in addition to transforming the existing graphs, each transformation step also needs to insert new functions into the forest. A key feature of our approach is that we can write such "system-level" code -relevant only to a particular transformation -in plain Python as well. The idea is to feed these new Python functions into the front-end, using this time the transformation's target (lower-level) type system during the type inference. In other words, we can write plain Python code that manipulates objects that conform to the lower-level type system, and turn these functions into graph that are directly typed in this lower-level type system.</p><p>For example, ll append() in figure <ref type="figure">3</ref> is a Python function that manipulates objects that behave like C structures and arrays. This function is inserted by the LLTyper, as a helper to implement the list.append() calls found in its RPython-level input graphs. By going through the front-end reconfigured to use C-level types, the above function becomes a graph with such C-level types, <ref type="foot" target="#foot_3">5</ref> which is then indistinguishable from the other graphs of the forest produced by the LLTyper.</p><p>In the example of the malloc operation, replaced by a call to GC code, this GC code can invoke a complete collection of dead objects, and can thus be arbitrarily complicated. Still, our GC code is entirely written in plain Python, and it manipulates "objects" that are still at a lower level: pointer and address objects. Even with the restriction of having to use pointer-like and address-like objects, Python remains more expressive than, say, C to write a GC (the work on the Jikes RVM's GC <ref type="bibr" target="#b4">[5]</ref> was the inspiration to try to express GCs in Python; see section 7).</p><p>In the sequel, we will call system code functions written in Python that are meant to be analysed by the front-end. For the purpose of this article we will restrict this definition to helpers introduced by transformations, as opposed to the original RPython program, although the difference is not fundamental to the translation process (and although our input RPython program, as seen in section 2, is often itself a Python virtual machine!). Note that such system code cannot typically be expressed as normal RPython functions, because it corresponds to primitive op-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RPython LL Type System</head><p>OO Type System Raw Addresses erations at that level. As an aside, let us remark that the number of primitive operations at RPython level is, comparatively speaking, quite large: all list and dictionary operations, instance and class attribute accesses, many string processing methods, a good subset of all Python built-in functions... Compared with other approaches (e.g. Squeak <ref type="bibr" target="#b10">[11]</ref>), we do not try to minimize the number of primitives -at least not at the source level. It is fine to have many primitives at any high enough level, because they can all be implemented at the next lower level in a way that makes sense to that level. The key reason why this is not burdensome is that the lower level implementations are also written in Python -with the only difference that they use (and have to be typeable in) the lower-level type system. <ref type="foot" target="#foot_4">6</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Type systems</head><p>The four levels that we considered so far are summarized in figure <ref type="figure" target="#fig_0">4</ref>.</p><p>The RPython level is a subset of Python, so the types mostly follow Python types, and the instances of these types are instances in the normal Python sense; for example where Python has only a single type list, RPython has a parametric type list(T) for every RPython type T, but instances of list(T) are just those Python lists whose items are all instances of T.</p><p>The other type systems, however, do not correspond to built-in Python types. For each of them, we implemented: 1. The types, which we use to tag the variables of the graphs at the given level (types are actually annotated self-recursive formal terms, and would have been implemented simply as such if Python supported them directly).</p><p>2. The Python objects that emulate instances of these types (more about them below).</p><p>We have defined well-typed operations between instances of these types, syntactically expressed with standard Python operators (e.g. if x is a C-level array, x[n] accesses its nth item). The emulating instances provide a concrete implementation of these operations that works in normal Python; the types involved in the operations are also known to the type inference engine when it analyses system code like the helper of figure <ref type="figure">3</ref>. Now, clearly, the purpose of types like a "C-like struct" or a "Clike array" is to be translated to a real struct or array declaration by the C back-end. What, then, is the purpose of emulating such things in Python? The answer is three-fold. Firstly, having objects that live within the Python interpreter, but faithfully emulate the behavior of their C equivalent while performing additional safety checks, is an invaluable help for testing and debugging. For example, we can check the correctness of our hash table implementation, written in Python in term of struct-and array-like objects, just by running it. The same holds for the GC.</p><p>Secondly, and anecdotally, as the type inference process (section 4) is based on abstract interpretation, we can use the following trick: the resulting type of most low-level operations is deduced simply by example. Sample C-level objects are instantiated, used as arguments to a given operation, and produce a sample result, whose C-level type must be the type of the result variable in the graph.</p><p>The third reason is fundamental: we use these emulating objects to represent pre-built objects at that level. For example, the GC transformer instantiates an object emulating a C array for the internal type id table, and fills it with the correct values. This array object is then either used directly when testing the GC, or translated by the C back-end into a static pre-initialized array.</p><p>Because our helpers can be both run on top of CPython and translated we can reuse them both as runtime implementations and to render constant objects at translation time. This is both natural and useful (especially for complex data structures). Figure <ref type="figure">5</ref> shows a function implementing insertion in a built-in dictionary for keys known to be absent, this is used at runtime after resizing and to fill the constant dictionaries at translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Type inference</head><p>The various analyses used -from type inference to lifetime analysis -are generally formulated as abstract interpretation. While this approach is known to be less efficient than more tailored algorithms like constraint-based type inference <ref type="bibr" target="#b22">[23]</ref>, we gain in freedom, controllability and simplicity. This proved essential in our overall approach: as described in section 3, we need to perform type inference with many different type systems, the details of which have evolved over time.</p><p>We mitigate the potential efficiency problem by wise choices and compromises for the domain used; the foremost example of this is that our RPython type inference performs almost no automatic specialization of functions in the presence of polymorphic arguments. Another example is that unlike some schemes which follow exact sets of concrete run-time classes, we only propagate a single superclass per variable: the most precise common parent class. This gives enough precision for our code generation purposes.</p><p>It is also simple, in the context of abstract interpretation, to perform translation-time constant propagation, computations and implement conditional code in a natural way. For example, the code in figure <ref type="figure">5</ref>, which is then specialized for the possible types of dictionary d, shows this kind of compile-time computation and introspection on low-level types -ENTRY is a C-like structure type and the hasattr-ibute checks introspect it at translation time. This kind of expressiveness increased our ability to reuse code even in low-level helpers.</p><p>In the sequel, we give a more precise description of this process and justify our claim that good performance and enough precision can be achieved -at least in some contexts -without giving up the naive but flexible approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Building control flow graphs</head><p>As described in the overview of the translation process (section 3.1), the front-end of the translation tool-chain works in two phases: it first builds control flow graphs from Python functions, and then performs whole-program type inference on these graphs.</p><p>Remember that building the control flow graphs is not done, as one might first expect, by following a function at the syntactic level. Instead, the whole program is imported in a normal Python interpreter -the full Python language is used as a kind of preprocessor with meta-programming capabilities. Once the program is imported, the object data in memory consists of Python function objects in bytecode format, as well as any other kind of objects created at import-time, like class objects, prebuilt instances of those, prebuilt tables, and so on. Note that these objects have typically no text representation any more; for example, cyclic data structures may have been built at this point. The translation tool-chain first turns these function objects into in-memory control flow graphs which contain direct references to the prebuilt data objects, and then handles and transforms these graphs.</p><p>Figure <ref type="figure">6</ref> shows the control flow graph obtained for a simple function -this is a screenshot from our graph viewer, used for debugging; basic block placement is performed by Graphviz <ref type="bibr" target="#b3">[4]</ref>.</p><p>The actual transformation from function objects -i.e. bytecode -to flow graph is performed by the Flow Object Space, a short but generic plug-in component for the Python interpreter of PyPy. The architecture of our Python interpreter is shown in figure <ref type="figure">7</ref>. Note that the left column, i.e. the bytecode interpreter and the Standard Object Space, form the full Python interpreter of PyPy. It is an RPython program, and the whole purpose of the translation process is to accept this as input, and translate it to an efficient form. The description of this particular input program is beyond the scope of the present paper; see <ref type="bibr" target="#b17">[18]</ref>.</p><p>However, the bytecode interpreter plays a double role, at two different levels. The so-called Object Spaces are domains in the abstract interpretation terminology. By design, we cleanly separated these domains from the bytecode interpreter core; the latter is only responsible for decoding the bytecodes of an application and emulating the corresponding stack machine. It treats all actual application-level objects as black boxes, and dispatches all operations on them to the Object Space. The Standard Object Space is a concrete domain, in which objects are the concrete Python objects of the various built-in types: lists, dictionaries, and so on. By opposition, the Flow Object Space is really an abstract domain. It handles objects that are placeholders.</p><p>Its lattice order (actually a join-semilattice only) is extremely simple, because most actual analysis is delayed to the next phase, the type inference engine. The objects are either Variables, which are pure placeholders for entirely unknown values, or Constants with a concrete Python object as value. The order places Variable as the top, and keeps all Constants unordered. Thus if two different constants merge during abstract interpretation, we immediately widen them to Variable.</p><p>In conjunction with the Flow Object Space, the bytecode interpreter of PyPy thus performs abstract interpretation of Python byte- codes from the application. 7 In this case, the bytecodes in question come from the RPython application that we would like to translate. The Flow Object Space records all operations that the bytecode interpreter "would like" to do between the placeholder objects. It records them into basic block objects that will eventually be part of the control flow graph of the whole function. The recorded operations take Variables and Constants as argument, and produce new Variables as results. The Constants serve two purposes: they are a way to introduce constant values into the flow graphs -these values may be arbitrarily complex objects, not just primitives -and they allow basic constant propagation. 8  In the flow graph, branching occurs when the bytecode interpreter tries to inspect the truth value of placeholder objects, as it would in response to conditional jump opcodes or other more complicated opcodes: at this point, the Flow Object Space starts two new basic blocks and -with a technique akin to continuationstricks the interpreter into following both branches, one after the other. Additionally, the bytecode interpreter sends simple positional signals that allow the Flow Object Space to detect when control paths merge, or when loops close. In this way, abstract interpretation quickly terminates and the recorded operations form a graph, which is the control flow graph of the original bytecode.</p><p>Note that we produce flow graphs in Static Single Information or SSI <ref type="bibr" target="#b1">[2]</ref> form, an extension of Static Single Assignment <ref type="bibr" target="#b8">[9]</ref>: each variable is only used in exactly one basic block. All variables that are not dead at the end of a basic block are explicitly carried over to the next block and renamed, as can be seen in figure <ref type="figure">7</ref>: each link carries a list of variables matching the formal input arguments of its target block.</p><p>While the Flow Object Space is quite a short piece of code -its core functionality takes only 300 lines -the detail of the interactions sketched above is not entirely straightforward; we refer the reader to <ref type="bibr" target="#b19">[20]</ref> for more information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Annotator</head><p>The type inference engine, which we call the annotator, is the central component of the front-end part of the translation process. Given a program considered as a family of control flow graphs, the annotator assigns to each variable of each graph a so-called annotation, which describes the possible run-time objects that this variable can contain. Following usual terminology, we will call such annotations types -not to be confused with the Python notion of the concrete type of an object. An annotation is a set of possible values, and such a set is not always the set of all objects of a specific Python type.</p><p>Here is a simplified, static model of how the annotator works. It can be considered as taking as input a finite family of functions calling each other, and working on the control flow graphs of each of these functions as built by the Flow Object Space (section 4.1). Additionally, for a particular "entry point" function, the annotator is provided with user-specified types for the function's arguments. 7 Note that this process uses the unmodified bytecode interpreter. This means that it is independent of most language details. Changes in syntax or in bytecode format or opcode semantics only need to be implemented once, in the bytecode interpreter. In effect, the Flow Object Space enables an interpreter for any language to work as a front-end for the rest of the tool-chain. 8 This is useful at this level for some constructs of the bytecode interpreter, which can temporarily wrap internal values and push them onto the regular value stack among the other application-level objects. We need to be able to unwrap them again later. Moreover, we rely on this feature to perform compile-time computations, particularly in the generic system code helpers, which ask for and compute with the concrete types of the variables they receive.</p><p>The goal of the annotator is to find the most precise of our types that can be given to each variable of all control flow graphs while respecting the constraints imposed by the operations in which these variables are involved.</p><p>More precisely, it is usually possible to deduce information about the result variable of an operation given information about its arguments. For example, we can say that the addition of two integers must be an integer. Most programming languages have this property. However, Python -like many languages not specifically designed with type inference in mind -does not possess a type system that allows much useful information to be derived about variables based on how they are used; only on how they were produced. For example, a number of very different built-in types can be involved in an addition; the meaning of the addition and the type of the result depends on the type of the input arguments. Merely knowing that a variable will be used in an addition does not give much information per se. For this reason, our annotator works by flowing types forward, operation after operation, i.e. by performing abstract interpretation of the flow graphs. In a sense, it is a more naive approach than the one taken by type systems specifically designed to enable more advanced inference algorithms. For example, Hindley-Milner type inference works in an inside-out direction, by starting from individual operations and propagating type constraints outwards <ref type="bibr" target="#b9">[10]</ref> <ref type="bibr" target="#b14">[15]</ref>.</p><p>Naturally, simply propagating types forward requires the use of a fixed point algorithm in the presence of loops in the flow graphs or in the inter-procedural call graph. Indeed, we flow types forward from the beginning of the entry point function into each basic block, operation after operation, and follow all calls recursively. During this process, each variable along the way gets a type. In various cases, e.g. when we close a loop, the previously assigned types can be found to be too restrictive. In this case, we generalise them to allow for a larger set of possible run-time values, and schedule the block where they appear for reflowing. The newly generalised types can in turn generalise the types of other result variables in the block, which in turn can generalise the types that flow into the following blocks, and so on. This process continues until a fixed point is reached.</p><p>We can consider that all variables are initially assigned the "bottom" type corresponding to the empty set of possible run-time values. Types can only ever be generalised, and the model is simple enough to show that there is no infinite chain of generalization, so that this process necessarily terminates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">RPython types</head><p>As seen in section 3, we use the annotator with more than one type systems. The most interesting and complex one is the RPython type system, which describes how the input RPython program can be annotated. The other type systems contain lower-level, C-like types that are mostly unordered, thus forming more trivial lattices than the one formed by RPython types.</p><p>The set A of RPython types is defined as the following formal terms:</p><p>• Bot, T op -the minimum and maximum elements (corresponding to "impossible value" and "most general value"); • Int, N onN egInt, Bool -integers, known-non-negative integers, booleans; • Str, Char -strings, characters (which are strings of length 1);</p><p>• Inst(class) -instance of class or a subclass thereof (there is one such term per class); • List(v) -list; v is a variable summarising the items of the list (there is one such term per variable);</p><p>• P bc(set) -where the set is a subset of the (finite) set of all prebuilt constant objects. This set includes all the callables of the input program: functions, classes, and methods. • N one -stands for the singleton None object of Python.</p><p>• N ullableStr, N ullableInst(class) -a string or None; resp.</p><p>an instance or None.</p><p>These types are ordered to form a lattice -we generally use its structure of join-semilattice only, but we will see in section 4.5 a use case for the meet. Pbcs form a classical finite set-ofsubsets lattice, class annotations are ordered by inheritance. We have left out a number of other annotations that are irrelevant for the basic description of the annotator and straightforward to handle: Dictionary, T uple, F loat, U nicodeP oint, Iterator, etc. The complete list and order is described in <ref type="bibr" target="#b16">[17]</ref>.</p><p>The type system moreover comes with a family of rules, which for every operation and every meaningful combination of input types describes the type of its result variable. Let V be the set of Variables that appear in the user program's flow graphs. Let b be a map from V to A; it is a "binding" that gives to each variable a type. The purpose of the annotator is to compute such a binding stepwise.</p><p>Let x, y and z be Variables. We introduce the rules:</p><formula xml:id="formula_0">z = add(x, y), b(x) = Int, b(y) = Int b = b with (z → Int) z = add(x, y), Bool ≤ b(x), b(y) ≤ N onN egInt b = b with (z → N onN egInt)</formula><p>The first rule specifies that if we see the addition operation applied to Variables whose current binding is Int, a new binding b can be produced: it is b except on z, where we have b (z) = Int.</p><p>The second rule specifies that in a similar case, if both arguments are known to be non-negative, so is the result. Extra rules control addition between other combinations of numbers, as well as strings, lists and tuples (for which addition is concatenation). Similar sets of rules are introduced for each operation.</p><p>The type inference engine can be seen as applying this kind of rules repeatedly. It does not apply them in random order, but follows a forward-propagation order typical of abstract interpretation.</p><p>It is outside the scope of the present paper to describe the type inference engine and the rules more formally. The difficult points involve mutable containers, e.g. initially empty list that are filled somewhere else; the discovery of instance attributes -in Python, classes do not declare upfront a fixed set of attributes for their instances, let alone their types; and the discovery of base class interfaces -across a class hierarchy, some methods with the same name may be called polymorphically by the program, while others may be unrelated methods used internally by the subclasses. These examples require reflowing techniques that invalidate existing types in already-annotated basic blocks, to account for the influence of more general types coming indirectly from a possibly distant part of the program. The reader is referred to <ref type="bibr" target="#b19">[20]</ref> for more information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Termination and complexity</head><p>The lattice model clearly contains no infinite chain. Moreover, it is designed to convince oneself that the number of reflowings required in practice is small. For example, we are not trying to do range analysis beyond detecting non-negatives -the reason is that range analysis is not an essential part of writing reasonably efficient code. Consider that a translated RPython program runs hundreds of times faster than when the same program is executed by the standard Python interpreter: in this context, range analysis appears less critical. It is a possible optimization that we can introduce in a later, optional analysis and transformation step.</p><p>The worst case behaviors that can appear in the model described above involve the lattice of Pbcs, involving variables that could contain e.g. one function object among many. An example of such behavior is code manipulating a table of function objects: when an item is read out of the table, its type is a large Pbc set: P bc({f1, f2, f3, . . .}). But in this example, the whole set is available at once, and not built incrementally by successive discoveries. This seems to be often the case in practice: it is not very common for programs to manipulate objects that belong to a large but finite family -and when they do, the whole family tends to be available early on, requiring few reflowings.</p><p>This means that in practice the complete process requires a time that is far lower than the worst case. We have experimentally confirmed this: annotating the whole PyPy interpreter (90,000 lines) takes on the order of 5 to 10 minutes, and basic blocks are typically only reflown a handful of times, providing a close-to-linear practical complexity.</p><p>We give formal termination and correctness proofs in <ref type="bibr" target="#b19">[20]</ref>, as well as worst-case bounds and common-case estimates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Precision</head><p>Of course, this would be pointless if the annotation did not give precise enough information for our needs. We must describe a detail of the abstract interpretation engine that is critical for precision: the propagation of conditional types. Consider the following source code fragment:</p><formula xml:id="formula_1">if isinstance(x, MyClass): f<label>(x) else: g(x)</label></formula><p>Although the type of x may be some parent class of MyClass, within the positive branch of the if it can be deduced to be of the more precise type Inst(M yClass).<ref type="foot" target="#foot_5">9</ref> This is implemented by introducing an extended family of types for boolean values: Bool(v1 : (t1, f1), v2 : (t2, f2), ...)</p><p>where the vn are variables and tn and fn are types. The result of a check, like isintance() above, is typically annotated with such an extended Bool. The meaning of the type is as follows: if the run-time value of the boolean is True, then we know that each variable vn has a type at most as general as tn; and if the boolean is False, then each variable vn has a type at most as general as fn. This information is propagated from the check operation to the exit of the block via such an extended Bool type, and the conditional exit logic in the type inference engine uses it to trim the types it propagates into the next blocks (this is where the meet of the lattice is used).</p><p>With the help of the above technique, we achieve a reasonable precision in small examples. For larger examples, a different, nonlocal technique is required: the specialization of type-polymorphic functions.</p><p>As described in the introduction, the most important downside of our approach is that automatic specialization is a potential performance-killer. So we decided that in the normal case, all calls to a given function contribute to a single annotation of that function. Inside the function graph, we propagate the join of the types of the actual parameters of the call sites. We do support specialization, however: we can generate several independently-annotated copies of the flow graph of certain functions. When annotating RPython programs, such specialization does not happen automatically: we rely on hints provided by the programmer in the source code, in the form of flags attached to function objects. As we had this trade-off in mind when we wrote the Python interpreter of PyPy, we only had to add about a dozen hints in the end.</p><p>This does not mean that automatic specialization policies are difficult to implement. Indeed, the simpler lower-level type systems rely quite heavily on them: this is because the system code helpers are often generic and can receive arguments of various Clevel types. In this case, because the types at this level are limited and mostly unordered, specializing all functions on their input argument's types works well.</p><p>At the level of RPython, the range of specializations that make sense is actually much wider. Although we only specialize a very small subset of the functions, we use criteria as diverse as specialization by the type of an argument, specialization by an expectedto-be-constant argument value, memoized functions that the type inference engine will actually call during annotation and replace by look-up tables, and even complete overriding of the annotator's behavior in extreme cases. In this sense, the need for manual specialization turned into an advantage, in term of simplicity and flexibility of implementing and using new specialization schemes.</p><p>This conclusion can be generalised. We experimented with a simple approach to type inference that works well in practice, and that can very flexibly accomodate small and big changes in the type system. We think that the reasons for this success are to be found on the one hand in the (reasonable) restrictions we put on ourselves when designing the RPython language and writing the Python interpreter of PyPy in RPython, and on the other hand in an ad-hoc type system that is designed to produce enough precision (but not more) for the purpose of the subsequent transformations to C-level code.</p><p>We should mention that restricting oneself to write RPython code instead of Python is still a burden, and that we are not proposing in any way that the Python language itself should evolve in this direction, nor even that RPython usage should become widespread. It is a tool designed with a specific goal in mind, which is the ability to produce reasonably efficient, stand-alone code in a large variety of environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Performance</head><p>Our tool-chain is capable of translating the Python interpreter of PyPy, written in RPython, currently producing either ANSI C code as described before, or LLVM<ref type="foot" target="#foot_6">10</ref> assembler which is then compiled to native code with LLVM tools.</p><p>The tool-chain has been tested with and can sucessfully apply transformations enabling various combinations of features. The translated interpreters are benchmarked using pystone (a Dhrystone 2.0 <ref type="bibr" target="#b23">[24]</ref> derivative traditionally used by the Python community, although it is a rather poor benchmark) and the classical Richards benchmark (ported to Python) and compared against CPython 2.4.3 <ref type="bibr" target="#b21">[22]</ref>. Results are summarized in table <ref type="table" target="#tab_2">1</ref>.</p><p>The numbers in parenthesis are slow-down factors compared to CPython. These measures reflect PyPy revision 27815 <ref type="foot" target="#foot_7">11</ref> , compiled with GCC 3.4.4. LLVM is version 1.8cvs ( <ref type="bibr">May 11, 2006)</ref>. The machine runs GNU/Linux SMP on an Intel(R) Pentium(R) 4 CPU at 3.20GHz with 2GB of RAM and 1MB of cache. The rows correspond to variants of the translation process, as follows:</p><p>pypy-c: the simplest variant; translated to C code with no explicit memory management, and linked with the Boehm conservative GC.</p><p>pypy-c-thread: the same, with OS thread support enabled (thread support is kept separate for measurement purposes because it has an impact on the GC performance).</p><p>pypy-c-stackless: the same as pypy-c, plus the "stackless transformation" step which modifies the flow graph of all functions to allow them to save and restore their local state, as a way to enable coroutines.</p><p>pypy-c-gcframework: in this variant, the "gc transformation" step inserts explicit memory management and a simple mark-andsweep GC implementation. The resulting program is not linked with Boehm. Note that it is not possible to find all roots from the C stack in portable C; instead, in this variant each function explicitly pushes and pops all roots to an alternate stack around each subcall.</p><p>pypy-c-stackless-gcframework: this variant combines the "gc transformation" step with the "stackless transformation" step. The overhead introduced by the stackless feature is theoretically balanced with the removal of the overhead of pushing and popping roots explicitly on an alternate stack: indeed, in this variant it is possible to ask the functions in the current C call chain to save their local state and return. This has the side-effect of moving all roots to the heap, where the GC can find them. (We hypothesize that the large slowdown is caused by the extreme size of the executable in this case -21MB, compared to 6MB for the basic pypy-c. Making it smaller is work in progress.) pypy-llvm-c: the same as pypy-c, but using the LLVM back-end instead of the C back-end. The LLVM assembler-compiler gives the best results when -as we do here -it optimizes its input and generates again C code, which is fed to GCC.</p><p>pypy-llvm-c-prof: the same as pypy-llvm-c, but using GCC's profile-driven optimizations.</p><p>The speed difference with CPython 2.4.3 can be explained at two levels. One is that CPython is hand-crafted C code that has been continuously optimized for a decade now, whereas the Python interpreter of PyPy first seeks flexibility and high abstraction levels. The other, probably dominant, factor is that various indices show that our approach places a very high load on the GC and on the memory caches of the machine. The Boehm GC is known to be less efficient than a more customized approach; kernel-level profiling shows that pypy-c typically spends 30% of its time in the Boehm library. Our current, naively simple mark-and-sweep GC manages to be a bit worse. The interaction with processor caches is also hard to predict and account for; in general, we tend to produce quite large amounts of code and prebuilt data. The overall performance is still reasonable: some variants are within the same order of magnitude as CPython, while others trade a slow-down for functionalities not available in CPython.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Translation times</head><p>A complete translation of the pypy-c variant takes about 39 minutes, divided as shown in table 2.</p><p>An interesting feature of this table is that type inference is not the bottleneck. Indeed, further transformation steps typically take longer than type inference alone. This is the case for the LLTyper step, although it has a linear complexity on the size of its input (most transformations do).</p><p>Other transformations like the "gc" and the "stackless" ones actually take more time, particuarly when used in combination with each other (we speculate it is because of the increase in size caused by the previous transformations). A translation of pypy-c-stackless, without counting GCC time, takes 60 minutes; the same for pypyc-stackless-gcframework takes 129 minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Future work</head><p>As described in section 5, the performance of the compiled Python interpreters is not yet up to competing with the well-established CPython. We are always working to improve matters, considering new optimizations and better GCs. Current work also includes refininments of the OOTyper and work on back-ends that use it, including those targeting CLI/.NET and Smalltalk/Squeak.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">JIT Specializer</head><p>So far, the PyPy tool-chain can only translate the Python interpreter of PyPy into a program which is again an interpreter -the same interpreter translated to C, essentially, although we have already shown that some aspects can be "weaved" in at translation time, like support for coroutines.</p><p>To achieve high performance for dynamic languages such as Python, the proven approach is to use dynamic compilation techniques, i.e. to write JITs. With direct techniques, this is a major endeavour, and increases the effort involved in further evolution of the language.</p><p>In the context of the PyPy project, we are now exploring -as we planned from the start -the possibility of producing a JIT as a graph transformation aspect from the Python interpreter. This idea is based on the theoretical possibiliy to turn interpreters into compilers by partial evaluation <ref type="bibr" target="#b11">[12]</ref>. In our approach, this is done by analysing the forest of flow graphs built from the Python interpreter, which is a well-suited input for this kind of techniques. We can currently perform binding-time analysis on these graphs, again with abstract interpretation techniques reusing the type inference engine. The next step is to transform the graphs -following the binding-time annotations -into a compiler; more precisely, in partial evalution terminology, a generating extension. We can currently do this on trivial examples.</p><p>The resulting generating extension will be essentially similar to Psyco <ref type="bibr" target="#b18">[19]</ref>, which is the only (and hand-written) JIT available for Python so far, based on run-time specialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Related work</head><p>Applying the expressiveness -or at least the syntax -of very highlevel and dynamically typed languages to their implementation has been investigated many times.</p><p>One typical approach is writing a static compiler. The viability of, and effort required for, such an approach depends usually on the binding and dispatch semantics of the language. Common Lisp native compilers, usable interactively and taking single functions or files as compilation units, are a well-known example of that approach. However, the late binding for all names and the load semantics make such an approach very hard for Python, if speed improvements are desired.</p><p>In this context, it is more relevant to consider and compare with projects using dynamic and very-high-level languages for interpreters and VM implementations, and Just-In-Time compilers.</p><p>Scheme48 was a Scheme implementation using a restricted Scheme, PreScheme <ref type="bibr" target="#b12">[13]</ref>, with static type inference based on Hindley-Milner. This is viable for Scheme as base language. Simplicity and portability across C platforms were its major goals.</p><p>Squeak <ref type="bibr" target="#b10">[11]</ref> is a Smalltalk implementation in Smalltalk. It uses SLang, a very restricted subset of Smalltalk with few types and strict conventions, which can be mostly directly translated to C. The VM, the object memory and the garbage collector support are explicitly written together in this style. Again simplicity and portability were the major goals, as opposed to sophisticated manipulation and analysis or "weaving" in of features as transformation aspects.</p><p>Jikes RVM <ref type="bibr" target="#b0">[1]</ref> is a Java VM and Just-In-Time compiler written in Java. Bootstrapping happens by self-applying the compiler on a host VM, and dumping a snapshot from memory of the resulting native code. This approach directly enables high performance, at the price of portability -as usual with pure native code emitting approaches. Modularity of features, when possible, is achieved with normal software modularity. The indirection costs are taken care of by the compiler performing inlining (which is sometimes even explicitly requested). In particular this modular approach is used for implementing a range of choices for GC support <ref type="bibr" target="#b4">[5]</ref>. This was the inspiration for PyPy's own GC framework, although much more tuning and work went into Jikes RVM. PyPy's own GC framework also exploits inlining of helpers and barriers to recover performance.</p><p>Jikes RVM's native JIT compilers <ref type="bibr" target="#b2">[3]</ref> are not meant to be retargetted to run in other environments than hardware processors, for example in a CLR/.NET runtime. Also Jikes RVM pays the complexity of writing a JIT up-front, which also means that features and semantics of the language are encoded in the JIT compiler code. Major changes of the language are likely to correspond to major surgery of the JIT.</p><p>PyPy's more indirect approach, together hopefully with our future work on generating a JIT compiler, tries to overcome these limitations, at the price of some more efforts required to achieve very good performance. It is too soon for a complete comparison of the complexity, performance and trade-offs of these approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>The PyPy project aims at showing that dynamic languages are suitable and quite useful for writing virtual machines in. We believe that we have achieved this objective. The present paper gave an overview of the architecture that enabled this result. Experiments suggest that practical virtual machines could reasonably follow in the near future, with faster-than-current virtual machines with JIT specialization techniques for the mid-term future.</p><p>Targetting platforms that are very different from C/Posix is work in progress, but given that many of the initial components are shared with the existing stack of transformations leading to C, we are confident that this work will soon give results. Moreover, we believe that these results will show reasonable efficiency, because our back-ends for VMs like Squeak and .NET can take advantage of a high-level input (as opposed to trying to translate, say, C-like code to Smalltalk).</p><p>A desirable property of our approach is to allow a given language and VM to be specified only once, in the form of an interpreter. Moreover, the interpreter can be kept simple (and thus keep its role as a specification): not only is it written in a high-level language, but it is not overloaded with low-level design choices and implementation details. This makes language evolution and experimentation easier. More generally, this property is important because many interpreters for very difference languages can be written: the simpler these interpreters can be kept, the more we win from our investment in writing the tool-chain itself -a one-time effort.</p><p>Dynamic languages enable the definition of multiple custom type systems, similar to pluggable type systems in <ref type="bibr" target="#b7">[8]</ref> but with simple type inference instead of explicit annotations. This proved a key feature in implementing our translation tool-chain, because it makes a many-levels approach convenient: each abstraction level can provide an implementation for some of the features that the higher levels considered primitive. It offsets the need to define a minimal kernel of primitives and build everything on top of it; instead, we have been able to implement, so to speak, the right feature at the right level.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. type systems; the arrows are transformations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 Figure 5 .</head><label>15</label><figDesc>Figure 5. built-in dict insertion helper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Figure 6. the control flow graph of a simple function, computed and displayed by our tool-chain.</figDesc><graphic coords="5,317.01,82.30,238.45,521.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>and CLI/.NET. When targeting a C-like environment, the first transformation step is called the LLTyper or low-level typer: it produces C-level flow graphs, where the object-oriented features of RPython</figDesc><table><row><cell>input RPython program</cell><cell></cell></row><row><cell>CPython Interpreter</cell><cell>function objects</cell></row><row><cell>forest of typed flow graphs</cell><cell>Flow Graphing Type Inference</cell></row><row><cell>Transformation 1</cell><cell></cell></row><row><cell>Transformation 2</cell><cell></cell></row><row><cell>Transformation 3</cell><cell></cell></row><row><cell>Back-end</cell><cell>compiled code</cell></row><row><cell cols="2">Figure 2. overview of the translation process.</cell></row><row><cell cols="2">(classes and instances) become manipulations of C structs with</cell></row><row><cell cols="2">explicit virtual table pointers. By contrast, for OO environments</cell></row><row><cell cols="2">this step is called the OOTyper: it targets a simple object-oriented</cell></row><row><cell cols="2">type system, and preserves the classes and instances of the origi-</cell></row><row><cell cols="2">nal RPython program. The LLTyper and OOTyper still have much</cell></row><row><cell cols="2">code in common, to convert the more Python-specific features like</cell></row><row><cell cols="2">the more complex calling conventions. In the sequel, we assume</cell></row><row><cell cols="2">the use of the LLTyper, as there are as yet no back-ends for the</cell></row><row><cell cols="2">OOTyper which are complete enough to translate the Standard In-</cell></row><row><cell cols="2">terpreter -although the CLI backend should be able to within a few</cell></row><row><cell>months.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>summary of interpreter performance.</figDesc><table><row><cell>Interpreter</cell><cell cols="3">Richards, Time/iteration Pystone, Iterations/second</cell></row><row><cell>CPython 2.4.3</cell><cell>789ms (1.0x)</cell><cell>40322 (1.0x)</cell></row><row><cell>pypy-c</cell><cell>4269ms (5.4x)</cell><cell>7587 (5.3x)</cell></row><row><cell>pypy-c-thread</cell><cell>4552ms (5.8x)</cell><cell>7122 (5.7x)</cell></row><row><cell>pypy-c-stackless</cell><cell>5121ms (6.5x)</cell><cell>6060 (6.7x)</cell></row><row><cell>pypy-c-gcframework</cell><cell>6327ms (8.0x)</cell><cell>4960 (8.1x)</cell></row><row><cell>pypy-c-stackless-gcframework</cell><cell>8743ms (11.0x)</cell><cell>3755 (10.7x)</cell></row><row><cell>pypy-llvm-c</cell><cell>3797ms (4.8x)</cell><cell>7763 (5.2x)</cell></row><row><cell>pypy-llvm-c-prof</cell><cell>2772ms (3.5x)</cell><cell>10245 (3.9x)</cell></row><row><cell>Step</cell><cell></cell><cell cols="2">Time (minutes:seconds)</cell></row><row><cell>Front-end (flow graphs and type inference)</cell><cell></cell><cell></cell><cell>9:01</cell></row><row><cell cols="2">LLTyper (from RPython-level to C-level graphs and data)</cell><cell></cell><cell>10:38</cell></row><row><cell cols="3">Various low-level optimizations (convert some heap allocations to local variables, inlining,</cell><cell>6:51</cell></row><row><cell>...)</cell><cell></cell><cell></cell></row><row><cell cols="3">Database building (this initial back-end step follows all graphs and prebuilt data structures</cell><cell>8:39</cell></row><row><cell cols="2">recursively, assigns names, and orders them suitably for code generation)</cell><cell></cell></row><row><cell>Generating C source</cell><cell></cell><cell></cell><cell>2:25</cell></row><row><cell>Compiling (gcc -O2)</cell><cell></cell><cell></cell><cell>3:23</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>dividing overall translation time by stage.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Note that the two phases are intermingled in time, because type inference proceeds from an entry point function and follows all calls, and thus only gradually discovers (the reachable parts of) the input program.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>The input to our translation chain are indeed loaded runtime function objects, not source code nor ASTs. This enables a form of staged programming in which we can use unrestricted Python for meta-programming purposes at load time: the whole of the source program -as a Python program -produces the RPython program which is sent to the tool-chain in the form of its object graph loaded in memory. This includes both the relevant functions and prebuilt</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>data.<ref type="bibr" target="#b3">4</ref> Our simple OO type system is designed for statically-typed OO environments, including Java; the presence of Smalltalk as a back-end might be misleading in that respect.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>The low-level type system specifies that the function should be specialized by the C-level type of its input arguments, so it actually turns into one graph per list type -list of integers, list of pointers, etc. This behavior gives the programmer a feeling comparable to C++ templates, without the declarations.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>This is not strictly true: the type systems are even allowed to co-exist in the same function. The operations involving higher-level type systems are turned into lower-level operations by the previous transformations in the chain, which leave the already-low-level operations untouched.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_5"><p>Remember that our graphs are in SSI form, which means that the x inside each basic block is a different Variable with a possibly different type as annotation. Given that our type inference is not flow-sensitive, SSI gives an advantage over SSA here.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_6"><p>The LLVM<ref type="bibr" target="#b13">[14]</ref> project is the realization of a portable assembler infrastructure, offering both a virtual machine with JIT capabilities and static compilation. Currently we are using the latter with its good high-level optimizations for</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_7"><p>PyPy.<ref type="bibr" target="#b10">11</ref> PyPy is an open-source project under the MIT license, PyPy source repository using subversion lives at http://codespeak.net/svn/pypy/dist/.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Jalapeno virtual machine</title>
		<author>
			<persName><forename type="first">B</forename><surname>Alpern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Attanasio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Barton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cocchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Hummel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lieber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Litvinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Mergen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Shepherd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C</forename><surname>Sreedhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Whaley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Systems Journal</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">211</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The static single information form</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Ananian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999-09">September 1999</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Architecture and policy for adaptive optimization in virtual machines</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Sweeney</surname></persName>
		</author>
		<idno>23429</idno>
		<imprint>
			<date type="published" when="2004-11">November 2004</date>
			<publisher>IBM Research</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Graphviz -graph visualization software</title>
		<author>
			<persName><surname>At&amp;t</surname></persName>
		</author>
		<ptr target="http://www.graphviz.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Oil and Water? High Performance Garbage Collection in Java with MMTk</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Blackburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICSE</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="137" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A garbage collector for C and C++</title>
		<author>
			<persName><forename type="first">H</forename><surname>Boehm</surname></persName>
		</author>
		<ptr target="http://www.hpl.hp.com/personal/HansBoehm/gc/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Garbage collection in an uncooperative environment</title>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Boehm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Weiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Softw. Pract. Exper</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="807" to="820" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pluggable type systems</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bracha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OOPSLA04 Workshop on Revival of Dynamic Languages</title>
		<imprint>
			<date type="published" when="2004-10">October 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficiently computing static single assignment form and the control dependence graph</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cytron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ferrante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Wegman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">K</forename><surname>Zadeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Programming Languages and Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="451" to="490" />
			<date type="published" when="1991-10">October 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Principal type-schemes for functional programs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Damas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Milner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">POPL &apos;82: Proceedings of the 9th ACM SIGPLAN-SIGACT symposium on Principles of programming languages</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1982">1982</date>
			<biblScope unit="page" from="207" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Back to the future: the story of Squeak, a practical Smalltalk written in itself</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ingalls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kaehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Maloney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OOPSLA &apos;97: Proceedings of the 12th ACM SIGPLAN conference on Object-oriented programming, systems, languages, and applications</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="318" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Partial Evaluation and Automatic Program Generation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gomard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sestoft</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>P-H</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Pre-scheme: A scheme dialect for systems programming</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kelsey</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">LLVM: A Compilation Framework for Lifelong Program Analysis &amp; Transformation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lattner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Adve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 International Symposium on Code Generation and Optimization (CGO&apos;04)</title>
		<meeting>the 2004 International Symposium on Code Generation and Optimization (CGO&apos;04)<address><addrLine>Palo Alto, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-03">Mar 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A theory of type polymorphism in programming</title>
		<author>
			<persName><forename type="first">R</forename><surname>Milner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Syst. Sci</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="348" to="375" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">PyPy -Architecture Overview</title>
		<author>
			<persName><forename type="first">Pypy</forename><surname>Team</surname></persName>
		</author>
		<ptr target="http://codespeak.net/pypy/dist/pypy/doc/archi-tecture.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">PyPy -Translation. Web page</title>
		<author>
			<persName><forename type="first">Pypy</forename><surname>Team</surname></persName>
		</author>
		<ptr target="http://codespeak.net/pypy/dist/pypy/doc/trans-lation.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Complete python implementation running on top of cpython</title>
		<author>
			<persName><forename type="first">Pypy</forename><surname>Team</surname></persName>
			<affiliation>
				<orgName type="collaboration">PyPy Consortium</orgName>
			</affiliation>
		</author>
		<ptr target="http://codespeak.net/svn/pypy/extradoc/eu-report/" />
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Representation-based just-in-time specialization and the psyco prototype for python</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PEPM &apos;04: Proceedings of the 2004 ACM SIGPLAN symposium on Partial evaluation and semanticsbased program manipulation</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="15" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Compiling dynamic language implementations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rigo</surname></persName>
			<affiliation>
				<orgName type="collaboration">PyPy Consortium</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hudson</surname></persName>
			<affiliation>
				<orgName type="collaboration">PyPy Consortium</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pedroni</surname></persName>
			<affiliation>
				<orgName type="collaboration">PyPy Consortium</orgName>
			</affiliation>
		</author>
		<ptr target="http://codespeak.net/svn/pypy/extradoc/eu-report/" />
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Constructing a metacircular virtual machine in an exploratory programming environment</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ungar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Spitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ausch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OOPSLA &apos;05: Companion to the 20th annual ACM SIGPLAN conference on Object-oriented programming, systems, languages, and applications</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Van Rossum</surname></persName>
		</author>
		<ptr target="http://www.python.org/download/releases/2.4.3/" />
		<imprint>
			<date type="published" when="2006-03-03">CPython 2.4.3, March 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Precise constraint-based type inference for java</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECOOP &apos;01: Proceedings of the 15th European Conference on Object-Oriented Programming</title>
		<meeting><address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="99" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dhrystone benchmark: rationale for version 2 and measurement rules</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Weicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGPLAN Not</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="49" to="62" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
