<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Wearable Assistive Devices for the Blind</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ramiro</forename><surname>Velázquez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Panamericana</orgName>
								<address>
									<settlement>Aguascalientes</settlement>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Wearable Assistive Devices for the Blind</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6307B86754EDDD8E121C08E6CC82FE8A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>assistive technology</term>
					<term>reading/mobility aids</term>
					<term>wearable devices and systems</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Assistive devices are a key aspect in wearable systems for biomedical applications, as they represent potential aids for people with physical and sensory disabilities that might lead to improvements in the quality of life. This chapter focuses on wearable assistive devices for the blind. It intends to review the most significant work done in this area, to present the latest approaches for assisting this population and to understand universal design concepts for the development of wearable assistive devices and systems for the blind.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Target Population</head><p>Globally, an estimated 40 to 45 million people are totally blind, 135 million have low vision and 314 million have some kind of visual impairment <ref type="bibr" target="#b0">[1]</ref>. The incidence and demographics of blindness vary greatly in different parts of the world. In most industrialized countries, approximately 0.4% of the population is blind while in developing countries it rises to 1%. It is estimated by the World Health Organization (WHO) that 87% of the world's blind live in developing countries.</p><p>Over the last decades, visual impairment and blindness caused by infectious diseases have been greatly reduced (an indication of the success of international public health action), but there is a visible increase in the number of people who are blind or visually impaired from conditions related to longer life expectancies. The great majority of visually impaired people are aged 65 years or older. It is estimated that there is a per-decade increase of up to 2 million persons over 65 years with visual impairments. This group is growing faster than the overall population.</p><p>In younger groups, blindness and visual impairment is mainly due to birth defects and uncorrected refractive errors. In the first case, most of the causes are in the brain rather than in the eye while in the second one, they are conditions that could have been prevented if diagnosed and corrected with glasses or refractive surgery on time.</p><p>It is estimated that by the year 2020, all blind-related numbers will double.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Dimension of the Problem</head><p>Of all sensations perceived through our senses, those received through sight have by far the greatest influence on perception. Sight combined with the other senses, mainly hearing, allow us to have a world global perception and to perform actions upon it. For the blind, the lack of sight is a major barrier in daily living: information access, mobility, way finding, interaction with the environment and with other people, among others, are challenging issues.</p><p>In fact, school and working-age blind have very high analphabet and unemployment rates. For example, in the US, the blind unemployment rate is around 75% while only 10% of the blind children receive instruction in Braille <ref type="bibr" target="#b1">[2]</ref>. Despite efforts, a true is that most schools and employers cannot accommodate blind people. In consequence, the person who is blind and his/her family face important socioeconomic constraints.</p><p>The issue of the blind becomes a very serious problem in terms of health and social security. Costly-in home expenses, nursing home care and welfare expenses on unemployment and health services have to be absorbed by the state.</p><p>A state action to enable the blind/visually impaired to live independent and productive lives has been to teach them new ways to accomplish routine daily tasks. A great variety of specialists is involved: special education teachers, Braille teachers, psychologists, orientation and mobility specialists, low-vision specialists and vision rehabilitation therapists to name a few.</p><p>Evidently, this involves a very high cost that has to be absorbed by the state. Moreover, the availability of funding and qualified personnel is insufficient to cover the actual population's demand. Other means are urgently needed to assist this population.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Assistive Technology</head><p>Advances of technology and better knowledge in human psycho-physiological 3D world perception permit the design and development of new powerful and fast interfaces assisting humans with disabilities. For the blind, research on supportive systems has traditionally focused on two main areas: information transmission and mobility assistance. More recently, computer access has been added to the list <ref type="bibr" target="#b2">[3]</ref>.</p><p>Problems related to information transmission concern reading, character recognition and rendering graphic information about 2D and 3D scenes. The most successful reading tool is the Braille dot code. Introduced by Louis Braille in the 19 th century, it has now become a standard worldwide. Inventions addressing the problems of character recognition and pictorial representation mostly consist of tactile displays. They permit character and graphic recognition by feeling a tactile version of them.</p><p>Problems related to mobility assistance are more challenging. They involve spatial information of the immediate environment, orientation and obstacle avoidance. Many electronic travel aids (ETAs) for safe and independent mobility of the blind have been proposed over the last decades. They all share the same operation principle: they all scan the environment (using different technologies) and display the information gathered to other sense (mainly hearing and touch).</p><p>With the internet revolution of the last years, problems related to computer access for the blind arose. Popular solutions are voice synthesizers, screen magnifiers and Braille output terminals. Voice synthesizers practically read the computer screen; screen magnifiers enable on-screen magnification for those with low-vision and Braille output terminals are plugged to the computer so that information on the screen is displayed in Braille.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Wearable and Portable Devices</head><p>This chapter reviews wearable assistive devices for the blind and less portable assistive devices. There is a slight difference between both.</p><p>Wearable devices are distinctive from portable devices by allowing hands-free interaction, or at least minimizing the use of hands when using the device. This is achieved by devices that are actually worn on the body such as head-mounted devices, wristbands, vests, belts, shoes, etc.</p><p>Portable devices are usually compact, lightweight, they can be easily carried (but not worn) by the user and require constant hand interaction. For example: tactile displays, electronic canes, mobile phones, laptop computers, etc.</p><p>The area of wearable devices is currently a "hot" research topic in assisting people with disabilities such as the blind. As this area is still very much young and experimental, there are not many mature commercial products with a wide user base.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Wearable Technologies for the Blind</head><p>A number of wearable assistive devices have been developed as task-specific solutions for activities such as reading and travel. Given the fact that sight is missing, they try to open new communication channels through hearing and touch. Devices are as diverse as the technology used and the location on the body. Fig. <ref type="figure" target="#fig_0">1</ref> overviews the body areas involved in wearable assistive devices: fingers, hands, wrist, abdomen, chest, feet, tongue, ears, etc. have been studied to transmit visual information to the blind. This chapter intends to review several prototypes so that their potential can be appreciated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Considerations on Hearing and Touch</head><p>After sight, hearing and touch are definitively the second and third major human senses, respectively. For the blind, they evidently become the first and second ones, respectively.</p><p>Blind people rely on hearing environmental cues for key tasks such as: awareness, orientation, mobility and safety. A representative example is when trying to cross a street intersection all alone: they stand-still listening to the environment and will not cross until the traffic light sequence is fully understood.</p><p>The ear is the sense organ that detects sound vibrations. It is responsible for transducing these vibrations into nerve impulses that are perceived by the brain. After brain processing, it is possible for humans to detect several characteristics of sound such as loudness, pitch or frequency, timbre, direction and distance to the source.</p><p>Roughly, the audible frequency range of the human ear is 20 Hz to 20 kHz with a remarkable discrimination of 0.5 to 1 Hz. Hearing a sound mainly depends on 2 parameters: sound intensity and frequency.</p><p>Fig. <ref type="figure">2</ref> shows the measured threshold of hearing curve that describes the sound intensity required to be heard according to frequency. The standard threshold of hearing at 1 kHz is nominally taken to be 0 dB, but it is actually about 4 dB. Note that there is a marked difference between low and high frequencies: while about 60 dB is required to be heard at 30 Hz, about 18 dB is required at 10 kHz. The high sensitivity region at 2 to 5 kHz is very important for the understanding of speech.</p><p>Human tolerance to sound intensity goes from the threshold of hearing at 4 dB to the threshold of pain at 120 dB, which can be produced by a jet engine.</p><p>It has been demonstrated that 8 hours of 90 dB sounds can cause damage to the ears, 1 minute of 110 dB causes hearing loss and any exposure to 140 dB sounds causes immediate and irreversible damage <ref type="bibr" target="#b3">[4]</ref>.</p><p>Additional related problems are the degradation and overload of the hearing sense. Recent studies <ref type="bibr" target="#b4">[5]</ref> have shown that a 20-30 minute listening to music/speech/sound activity causes degradation to human sensors information registration, reduces human capacity to perform usual tasks and affects the posture and equilibrium. Fig. <ref type="figure">2</ref>. Threshold of hearing: sound intensity minimal thresholds as function of frequency, after <ref type="bibr" target="#b3">[4]</ref>.</p><p>All these facts must be taken into account when designing assistive devices that exploit hearing as the substitution sense.</p><p>In healthy-sighted, touch is generally used as an additional independent sensory modality to convey information or as a redundant modality to increase information coming from vision and hearing.</p><p>For the blind, touch becomes the primary input for the receipt of non-audible physical information. Blind people can rapidly and accurately identify threedimensional objects by touch. They can also locate and orient themselves in known environments by touching objects. Braille readers access information through touch.</p><p>The skin is the sense organ that contains the essential biological sensors of touch. It encompasses 3 main groups of sensors organized by biological function: the thermoreceptors, responsible for thermal sensing, the nociceptors, responsible for pain sensing and the mechanoreceptors, sensitive to mechanical stimulus and skin deformation.</p><p>Our interest focuses on the mechanoreceptors as they are responsible for sensing and transmission of physical deformations by external forces to the nervous system. Four kinds of mechanoreceptors can be found on the human glabrous skin: Pacini corpuscles, Ruffini endings, Merkel cells and Meissner corpuscles.</p><p>According to <ref type="bibr" target="#b5">[6]</ref>, Meissner corpuscles respond to touch, Pacini corpuscles respond to vibration, Ruffini endings respond to lateral extension of the skin and articular movement and Merkel cells perceive pressure.</p><p>Our interest mainly focuses on Meissner and Pacini skin mechanoreceptors since they are involved in hand feeling during object exploration.</p><p>Similar to the relationship found for hearing, feeling a deformation on the skin depends on the relation between the amount of skin indentation and frequency. Fig. <ref type="figure">3</ref> shows this relation for Meissner and Pacini mechanoreceptors. Note that while Pacini corpuscles are sensitive to low amplitude-high frequency stimuli, Meissner ones are sensitive to high amplitude-low frequency stimuli.</p><p>The ability to discriminate stimuli on the skin varies throughout the body. The two-point discrimination threshold (TPDT) is a measure that represents how far apart two pressure points must be in order to be perceived as two distinct points on the skin <ref type="bibr" target="#b7">[8]</ref>. This measurement usually aids designers in choosing the density of a tactile display depending on the part of the body the tactile display will be Fig. <ref type="figure">3</ref>. Skin indentation minimal detection thresholds as function of frequency, after <ref type="bibr" target="#b6">[7]</ref>. Fig. <ref type="figure">4</ref>. TPDT for different areas of the body, after <ref type="bibr" target="#b8">[9]</ref>. mounted. Fig. <ref type="figure">4</ref> shows the TPDT for different areas of the body. The TPDT is 2.54 mm for the fingertips, 18 mm for the forehead, 40 mm for the forearm, 42 mm for the back, 49 mm for the calf, etc.</p><p>Figs. <ref type="figure">3</ref> and<ref type="figure">4</ref> are the basis for designing wearable touch stimulation devices such as tactile displays. For example, for correctly stimulating the fingertips, a tactile display should integrate an actuator array of 2.54 mm interspacing (Fig. <ref type="figure">4</ref>) and produce an effective skin indentation of 1 mm at 1 Hz (Meissner curve, Fig. <ref type="figure">3</ref>) or have an array of contact pins at 2.54 mm interspacing and produce 10 μm of skin indentation at 100 Hz (Pacini curve, Fig. <ref type="figure">3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Assistive Devices Worn on Fingers and Hands</head><p>Most of the assistive devices for the blind that exploit touch as the substitution sense are tactile displays for the fingertips and palms. Typical tactile displays involve arrays of vibrators or upward/downward moveable pins as skin indentation mechanisms.</p><p>Many tactile devices have been developed using a wide range of technologies. Approaches range from traditional actuation technologies such as servomotors, electromagnetic coils, piezoelectric ceramics and pneumatics <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref> to the new ones: shape memory alloys (SMAs), electroactive polymers (EAPs), electrorheological (ER) fluids and airbone ultrasound <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>.</p><p>However, most of the tactile displays found in the literature are at best good examples of portable devices. Wearable devices for the fingers and palms were not found in the literature until recently. Two examples are the band-aid-size tactile display from Sungkyunkwan University (Korea) and the Finger-Braille interface from Tokyo University (Japan). The bandage-sized tactile display is an innovative touch stimulation device based on EAP soft actuator technology (Fig. <ref type="figure" target="#fig_1">5</ref>). It is soft and flexible and can to be wrapped around the finger like a band-aid. This new wearable display could be used as a Braille display or as a multi-purpose tactile display to convey visual information to the blind.</p><p>The first prototype developed is a thin polymer sheet of 210 μm thick with 20 EAP soft actuators distributed in an active area of 14 x 11mm 2 which covers most of the fingertip's touch-sensitive area. Tactile feel is produced by actuating the 20 contact points independently. Both vibration and upward/downward patterns can be generated using an external user computer interface <ref type="bibr" target="#b17">[18]</ref>.</p><p>The Japanese Finger-Braille interface is a wearable assistive device to communicate information to the deaf-blind. In this system, the fingers are regarded as Braille dots: 6 fingers, 3 at each hand, are enough to code any 6-dot Braille character. Some examples of translation are shown in fig. <ref type="figure" target="#fig_2">6</ref>.</p><p>Using this codification, 6 small lightweight vibrating DC motors were attached to the fingers (Fig. <ref type="figure" target="#fig_3">7</ref>). They provide a 120 Hz vibration to stimulate the back of the finger. Each 3 motor-hand is controlled by a Citizen-IBM wristwatch computer and an electronic module that includes the batteries and control circuitry. The wristwatch computer is capable of communicating with external devices via Bluetooth technology. The total weight of the equipment, including battery, is approximately 170 g per hand.</p><p>The creators of the Finger-Braille interface report in <ref type="bibr" target="#b18">[19]</ref> high recognition rates from experiments conducted with deaf-blind subjects which shows the prototype's potential for providing Braille information.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Assistive Devices Worn on the Wrist and Forearm</head><p>Researchers at the University of British Columbia (Canada) have developed two prototypes of vibrotactile displays that can be worn on the forearm and wrist (Fig. <ref type="figure" target="#fig_4">8(a)</ref>).</p><p>Both displays consist of 2 vibrating DC motors spaced 60 mm apart that generate vibrations at 140 Hz <ref type="bibr" target="#b19">[20]</ref>.</p><p>These tactile devices convey information using intermittent alert-like signals and were used to alert clinicians of adverse changes in a patient's heart rate without distracting their attention with auditory alarms. Experiments conducted with these prototypes showed that there was no difference between the wrist and forearm: comfort and accuracy of information were perceived to be the same.</p><p>Similarly to this application, these devices could be used to convey simple patterns such as alert-like information to the blind for example when approaching an obstacle.</p><p>A popular wearable assistive device for the wrist is of course the Braille watch (Fig. <ref type="figure" target="#fig_4">8(b)</ref>). On the basis that a watch is a primary necessity for living a normal life, several companies are nowadays commercializing Braille watches.</p><p>These watches, which have the exact same mechanism as the regular ones, display time information as raised patterns on the dial or as Braille numbers which blind people will sense. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Assistive Devices Worn on the Tongue</head><p>In normal vision, the eyes send signals to the middle of the brain. From there, these signals are sent directly to the visual cortex. Not so for the blind.</p><p>In 1998, the University of Wisconsin (USA) introduced the TDU (Tongue Display Unit). The TDU proposed to retrain the way the brain processes visual information by first stimulating the tongue with an electrode array. The nerves in the tongue send signals through a different pathway to the brain stem in the area that deals with touch. Eventually, the blind person learns to interpret touch as sight in the virtual cortex.</p><p>The TDU first prototype translated optical images picked up by a camera into electro-tactile stimuli which were delivered to the dorsum of the tongue via a 12 x 12 flexible electrode array placed in the mouth (Fig. <ref type="figure" target="#fig_5">9(a)</ref>). Experiments report that after sufficient training (15 h), the user loses awareness of on-the-tongue sensations and perceives the stimulation as shapes and features in space <ref type="bibr" target="#b20">[21]</ref>.</p><p>Inspired by the TDU, researchers at the University of Montreal (Canada) proposed a tongue display to help blind people navigate around obstacles (Fig. <ref type="figure" target="#fig_5">9</ref>(b)) <ref type="bibr" target="#b21">[22]</ref>. This device, under the name of Brainport Vision Technology, is expected to be commercially available in the near future.</p><p>Another TDU-based display is the one developed by researchers at Joseph Fourier University (France). Their prototype consists of a matrix of 36 electrodes that transmit electrical impulses to the tongue (Fig. <ref type="figure" target="#fig_5">9(c)</ref>). This device is currently being used to detect and correct stability and posture <ref type="bibr" target="#b22">[23]</ref>.</p><p>The ultimately goal of the TDUs is to develop a compact, cosmetically acceptable, wireless system that can be worn like a dental orthodontic retainer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Head-Mounted Assistive Devices</head><p>Head-mounted devices (HMDs) such as headsets and headbands are the most popular kind of wearable assistive devices. The head is the rostral part of the human body that comprises the brain, eyes, ears, nose and mouth (all of which are responsible for sensory functions). In particular, the ears, the only sensory organ responsible for hearing throughout the body and the main substitution pathway for the blind, are located on the head. Moreover, humans use head motion to gather information from the environment. It is easy to deduce that devices worn on the head acquire that freedom of motion for environment scanning.</p><p>A number of HMDs have been developed for reading and travel assistance of the blind. For reading, the most representative example is the audio book. Entire text books are recorded in the form of speech and reproduced by wearable headset systems (earphones and player).</p><p>Since 1950, audio books have benefited the blind population by offering a simple, low-cost, non-Braille reading option. Thousands of titles are available, and with today's technology they can be downloaded for example, from the National Library Service for the Blind and Physically Handicapped (NLS) <ref type="bibr" target="#b23">[24]</ref>. Even though the undisputable advantages of audio books, two issues have to be considered:</p><p>• Availability. Even though there are thousands of titles, not all books are systematically converted to audio books and there is a significant delay with new books. Months, years (or never) could take to have a new book available in audio form. • Audio books should not be considered as the reading solution for blind people. It is true that they fit perfectly for the elderly and non-Braille readers. However as healthy-sighted do prefer to read a book instead of hearing it, why do we assume that a blind person prefers to listen to a book instead of reading it? For the blind, reading a book heightens the selfesteem and independence. Moreover, it trains their orthography which is in most cases quite bad.</p><p>For travel assistance, several HMDs have been proposed. In the early years, the most sophisticated device that first became commercially available was the Binaural Sonic Aid (SonicGuide) <ref type="bibr" target="#b24">[25]</ref>. The SonicGuide consisted of an ultrasonic wide-beam equipment mounted on spectacle lenses (Fig. <ref type="figure" target="#fig_6">10(a)</ref>). Signals reflected back from the 3D world were presented to the user as audio indicating the presence of an obstacle and its approximate distance to the user. Over the years, the SonicGuide has undergone continuous improvements and its latest version is the system called KASPA <ref type="bibr" target="#b25">[26]</ref>. KASPA is worn as a headband   Many portable ETAs like the SonicGuide and the Kaspa system have been developed in the form of hand held devices <ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref>. All these devices are similar to radar systems: a laser or ultrasonic beam is emitted in a certain direction in space and the beam is reflected back from objects that it confronts on its way. A sensor detects the reflected beam, measures the distance to the object and indicates that information to the user through audio or tactile signals.</p><p>A new generation of ETAs aims to provide a sensory substitution/ supplementation more than merely obstacle detection. The information is acquired using video cameras and its processing is more at cognitive level (brain plasticity, perception, intentionality, etc.). Two representative examples are here presented.</p><p>The vOICe system (Fig. <ref type="figure" target="#fig_7">11(a)</ref>), patented by Philips Co., converts visual depth information into an auditory representation <ref type="bibr" target="#b28">[29]</ref>. This is the first device that successfully uses a camera as an input source. An image is translated to sounds where frequency and loudness represents different scene information parameters such as position, elevation and brightness (Fig. <ref type="figure" target="#fig_7">11(b)</ref>). Simple things like finding an object may be mastered in minutes but fully mastering the vOICe's visual-toauditory language may well take years.</p><p>The vOICe is nowadays a mature commercial product <ref type="bibr" target="#b29">[30]</ref> and a large number of testimonials show that it is actually improving the quality of life of blind users.</p><p>The Intelligent Glasses (IG) is a combined HMD and tactile display system developed at Paris 6 University (France). The IG is a new generation ETA that provides tactile maps of visual spaces and allows users to deduce possible paths for navigating these spaces in order to perform independent, safe and efficient mobility tasks.</p><p>The IG system is basically composed of 3 main modules: vision module, scene analyzer and tactile display. Fig. <ref type="figure" target="#fig_8">12</ref> shows the IG first wearable prototype and its operation principle: (a) a pair of stereo-cameras mounted on the glasses frame acquire the environment's representation. (b) Vision algorithms are then applied in order to identify the obstacles in the scene and their user-related position. (c) Finally, this information is displayed on a tactile display for fast exploration by the user. The resulting tactile map is a simple edge-like representation of the obstacles' locations in the scene. All obstacles are considered and displayed in tactile domain as binary data: presence or absence of an obstacle.</p><p>Results in <ref type="bibr" target="#b30">[31]</ref> show that healthy-sighted blindfolded subjects are able to understand, interact and navigate spaces using tactile maps. Upon training, subjects become more efficient and used to the IG system and tactile maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Vests and Belts</head><p>Researchers at Carnegie Mellon University (USA) presented in <ref type="bibr" target="#b31">[32]</ref> the Kahru Tactile Outdoor Navigator (Fig. <ref type="figure" target="#fig_0">13(a)</ref>). The Kahru system is a wearable tactile harness-vest display that provides simple directional navigation instructions. A set of 6 vibrating motors generates tactile messages such as forward, back, left, right, speed up and slow down to guide the user through an environment. Communication with the vest is ensured by a belt-worn infrared receiver.</p><p>TNO Human Factors, an applied scientific research institute in the Netherlands, has developed a tactile display that consists of 128 vibrating elements attached to a vest (Fig. <ref type="figure" target="#fig_0">13(b)</ref>). Vibrations at 160 Hz present 3D spatial information to the user. This vest is currently being used to convey flight information to pilots in an intuitive way <ref type="bibr">[33]</ref>. Similarly, it could be used for the blind.</p><p>Researchers at MIT (USA) have developed a tactile display embedded in a vest that fastens around the lower torso (Fig. <ref type="figure" target="#fig_0">13(c</ref>)). This tactile display consists of a 4 x 4 array of vibrating motors which are independently controlled by an electronic unit. The electronic unit receives commands wirelessly from a remote computer.</p><p>This tactile vest display can be used as a navigation aid outdoors, as experiments in <ref type="bibr" target="#b33">[34]</ref> have proved that 8 different vibrotactile patterns can be interpreted as directional (for example: stop, look left, run, proceed faster or proceed slower) or instructional cues (for example: raise arm horizontally, raise arm vertically) with almost perfect accuracy.</p><p>The NavBelt <ref type="bibr" target="#b34">[35]</ref>, a wearable ETA proposed by the University of Michigan (USA), provides acoustical feedback from an array of ultrasonic sensors mounted on a belt around the abdomen (Fig. <ref type="figure" target="#fig_0">14(a)</ref>). These sensors provide information on very local obstacles placed in a 120° wide sector ahead of the user.</p><p>Researchers at Keio University (Japan) proposed in <ref type="bibr" target="#b35">[36]</ref> the ActiveBelt, a belttype tactile display for directional navigation. The ActiveBelt consists of a GPS, a geomagnetic sensor and 8 vibrators distributed at regular intervals around the torso (Fig. <ref type="figure" target="#fig_0">14(b)</ref>).</p><p>Vibrations within the range of 33 to 77 Hz are supplied by the ActiveBelt to indicate directions to the user. A set of experiments confirmed that subjects are able to identify the 8 directions while walking.</p><p>Another belt-type assistive device is the one developed at the University of Osnabrück (Germany). The prototype consists of an electronic compass and 13 vibrators located around the abdomen (Fig. <ref type="figure" target="#fig_0">14(c)</ref>). This belt enables its user to continuously feel his orientation in space via vibrotactile stimulation. Navigation accuracy and long-term usage of the belt are currently being evaluated <ref type="bibr" target="#b36">[37]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Assistive Devices Worn on the Feet</head><p>The human foot is a highly functional structure yet its full capabilities have not been thoroughly explored.</p><p>What we know about the human foot is that it combines mechanical complexity and structural strength. The ankle serves as foundation, shock absorber and propulsion engine. The foot can sustain enormous pressure and provides flexibility and resiliency. Sensory input from the foot, particularly from the foot sole, has long been recognized as an important source of sensory information in controlling movement and standing balance <ref type="bibr" target="#b37">[38]</ref>. As the load on the foot is transferred from heel to toe, pressure signals are automatically fed back to the brain to provide important information about the body's position with respect to the supporting surface.</p><p>Our work at Panamericana University (Mexico) has focused on evaluating the performance of the foot sole receptors for information transmission.</p><p>For this purpose, we have developed a shoe-integrated vibrotactile display to study how people understand information through their feet and to evaluate whether or not this comprehension level is sufficient to be exploited for assistance of the blind.</p><p>Based on the physiology of the plantar surface of the foot, a first prototype consisting of a 16-point array of actuators has been designed and implemented (Fig. <ref type="figure" target="#fig_11">15</ref>). All 16 vibrators have been successfully integrated in a regular foam shoe-insole with 10 mm interspacing. They provide vibrating frequencies between 10-55 Hz. Each vibrator is independently controlled with a specific vibrating frequency command.</p><p>One of the advantages of this mechatronic shoe-insole is that it can be further inserted into a shoe making it an inconspicuous and visually unnoticeable assistive device. Unlike other portable/wearable assistive devices, an on-shoe device does not heighten the handicapped image that affects the user's self-esteem. Experiments involving direction, shape, pattern recognition and navigation in space have been conducted with healthy blindfolded-sighted and blind people to gain insights into the capabilities of tactile-foot perception <ref type="bibr" target="#b38">[39]</ref>.</p><p>Results show that both healthy-sighted and blind subjects understand easily vibrations encoding simple information such as directional instructions (for example: go forward, backward, turn left, turn right and stop) and familiar patterns (for example: SMS, phone call, caution) but do not understand vibrations encoding more complex information such as shapes. Although it seems that the feet are not appropriate for precise information recognition, collected data show that it is feasible to exploit podotactile information for navigation in space.</p><p>Current work evaluates (1) whether long-term vibrating stimuli on the foot affects balance and walking and (2) user performance depending on cognitive load. The final goal is to integrate the concept of podotactile stimulation in ETAs for the blind.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Synthesis and Conclusions</head><p>The miniaturization of actuators and electronics has allowed the creation of new devices and systems that can be embedded into clothing. These wearable systems facilitate the user's ability to perform normal daily tasks without feeling encumbered by burdensome devices.</p><p>In particular, this chapter has focused on wearable assistive devices for the blind. A brief non-exhaustive survey of wearable assistive devices for this population has been presented to illustrate the most representative work done in this area. Devices worn on the finger, hands, wrist, forearm, tongue, head, chest, abdomen and feet have been proposed over the last decades to provide wearable solutions to the problems of reading and mobility.</p><p>For the blind, hearing and touch become the first and second major senses, respectively. They will never replace vision but they still gather much information from the environment for daily tasks. That is the reason why assistive devices provide acoustical and tactile feedback to compensate for visual information. In contrast, smell and taste are largely ignored as being essential to the interaction with the environment.</p><p>Several universal design concepts for acoustical/tactile based assistive devices have been presented. They provide guidelines to stimulate both hearing and touch in order to obtain the best performance from these senses. Yet, some considerations must be taken into account:</p><p>• Sensory overload. The brain simultaneously processes stimuli from several or all of the sensory modalities to interpret the surrounding environment.</p><p>Because humans have a limited capacity to receive, hold in working memory and cognitively process information taken from the environment, the use of only one sensory modality to convey information can quickly overload that modality. After a while, users may be limited in the perception of acoustical or tactile signals coming from assistive devices.</p><p>• Long learning/training time. Learning and mastering the visual-to-sound or visual-to-tactile new language is quite a challenge and requires long training time, patience and great effort from the user. • Acoustical feedback is useful only for reading applications. For mobility applications, it might interfere with the blind person's ability to pick up environmental cues. Moreover, continuous acoustic feedback (20-30 min) might affect posture and equilibrium.</p><p>• Tactile feedback is a viable choice for mobility applications. However, the information presented must be in accordance with the location of the tactile display on the body. Precise information can only be recognized with the fingers and tongue while simple information can be displayed on the rest of the body. It seems that simple directional information is the best choice for mobility of the blind. It does not require constant activity and cognitive effort that reduces walking speed and quickly fatigues the user.</p><p>Despite efforts and the great variety of wearable assistive devices available, user acceptance is quite low. Audio books and Braille displays (for those who can read Braille) and the white cane and guide dog will continue to be the most popular reading/travel assistive devices for the blind.</p><p>Acceptance of any other portable or wearable assistive device is always a challenge in blind population. Motivation, cooperation, optimism, willingness/ ability to learn or adapt new skills is not a combination that can be taken for granted.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of wearable assistive devices for the blind.</figDesc><graphic coords="4,170.71,90.49,140.33,187.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Wearable tactile display for the fingertip.</figDesc><graphic coords="7,59.22,372.56,311.64,91.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Finger-Braille code.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Finger-Braille interface with wristwatch computers.</figDesc><graphic coords="8,115.50,404.72,199.48,137.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. (a) Tactile display prototypes for the forearm and wrist and (b) Braille watch.</figDesc><graphic coords="9,82.50,305.96,126.14,106.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. TDU systems. Prototypes of: (a) the University of Wisconsin, (b) the University of Montreal and (c) Joseph Fourier University.</figDesc><graphic coords="10,84.42,365.84,105.60,97.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. (a) The SonicGuide (1974) and (b) its latest version, the KASPA system (2002).</figDesc><graphic coords="11,84.78,486.68,152.04,64.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. (a) The vOICe system and (b) an example of its image-to-sound rendering.</figDesc><graphic coords="12,83.65,202.04,119.70,170.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 .</head><label>12</label><figDesc>Fig.12. The IG wearable system and an example of its image-to-tactile rendering.</figDesc><graphic coords="12,278.54,204.82,67.63,165.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 .Fig. 14 .</head><label>1314</label><figDesc>Fig. 13. Tactile vest displays. Prototypes of: (a) Carnegie Mellon University, (b) TNO Human Factors and (c) MIT.</figDesc><graphic coords="14,163.38,216.56,104.43,102.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Shoe-integrated tactile display: back and forth. Prototype of Panamericana University.</figDesc><graphic coords="15,71.82,396.80,286.56,153.24" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">World Health Organization, Visual impairment and blindness</title>
		<ptr target="http://www.who.int/mediacentre/factsheets/fs282/en/" />
	</analytic>
	<monogr>
		<title level="j">Fact Sheet</title>
		<imprint>
			<biblScope unit="volume">282</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Breaking the chains of paternalism</title>
		<author>
			<persName><forename type="first">Blind</forename><surname>World</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Magazine</forename></persName>
		</author>
		<ptr target="http://home.earthlink.net/~blindworld/NEWS/6-06-14-02.htm" />
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Aids for people who are blind or visually impaired</title>
		<author>
			<persName><forename type="first">J</forename><surname>Brabyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Seelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Panchang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">An Introduction to Rehabilitation Engineering</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Cooper</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Ohnabe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Hobson</surname></persName>
		</editor>
		<meeting><address><addrLine>Abington</addrLine></address></meeting>
		<imprint>
			<publisher>Taylor &amp; Francis</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="287" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">An introduction to the psychology of hearing, 5th edn</title>
		<author>
			<persName><forename type="first">B</forename><surname>Moore</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Elsevier Academic Press</publisher>
			<pubPlace>Amsterdam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Postural stability and sickness symptoms after HMD use</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hakkinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vuori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paakka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Systems, Man and Cybernetics</title>
		<meeting>of IEEE International Conference on Systems, Man and Cybernetics<address><addrLine>Hammamet, Tunisia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="147" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Touch</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kandel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jessell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Principles of neural science</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Kandel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Schwartz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Jessell</surname></persName>
		</editor>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="349" to="414" />
		</imprint>
	</monogr>
	<note>3rd edn.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Sekuler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Blake</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Perception. McGraw-Hill Education</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Handbook of physiology section 1: the nervous system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Geiger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>American Physiological Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Intensive and extensive aspects of tactile sensitivity as a function of body part, sex, and laterality</title>
		<author>
			<persName><forename type="first">S</forename><surname>Weinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The skin senses, Charles C. Thomas</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Kenshalo</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1968">1968</date>
			<biblScope unit="page" from="195" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Design and performance of a tactile shape display using RC servomotors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lederman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Howe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 10th Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems</title>
		<meeting>of 10th Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems<address><addrLine>Orlando, Fl, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="354" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tactile feedback for endoscopic surgery</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Neisius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Trapp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interactive Technology and the New Paradigm for Healthcare</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Satava</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Morgan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Sieburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Mattheus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Christensen</surname></persName>
		</editor>
		<imprint>
			<publisher>Amsterdam</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="114" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A broadband tactile array on the fingertip</title>
		<author>
			<persName><forename type="first">I</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chanter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="2118" to="2126" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Thermo-pneumatic actuator for tactile displays and smart actuation circuitry</title>
		<author>
			<persName><forename type="first">F</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Madueño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Navas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SPIE International Symposium on Microtechnologies for the New Millenium</title>
		<meeting>of SPIE International Symposium on Microtechnologies for the New Millenium<address><addrLine>Sevilla, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="484" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tactile rendering with shape memory alloy pin-matrix</title>
		<author>
			<persName><forename type="first">R</forename><surname>Velazquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pissaloux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hafez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Szewczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Instrumentation and Measurement</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1051" to="1057" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Artificial tactile feel display using soft gel actuators</title>
		<author>
			<persName><forename type="first">M</forename><surname>Konyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tadokoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Takamori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Robotics and Automation</title>
		<meeting>of IEEE International Conference on Robotics and Automation<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="3416" to="3421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Advances in an electrorheological fluid based tactile array</title>
		<author>
			<persName><forename type="first">P</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pollet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Varley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Displays</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="135" to="141" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Non-contact tactile sensation synthesized by ultrasound transducers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hoshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Iwamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shinoda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 3rd Joint Eurohaptics Conference and Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems</title>
		<meeting>of 3rd Joint Eurohaptics Conference and Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems<address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="256" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Development of softactuator-based wearable tactile display</title>
		<author>
			<persName><forename type="first">I</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kwangmok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="549" to="558" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Virtual leading blocks for the deaf-blind: a real-time way-finder by verbal-nonverbal hybrid interface and highdensity RFID tag space</title>
		<author>
			<persName><forename type="first">T</forename><surname>Amemiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hirota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hirose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Virtual Reality</title>
		<meeting>of IEEE Virtual Reality<address><addrLine>Chicago, Il, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="165" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Optimizing the tactile display of physiological information: vibro-tactile vs. electro-tactile stimulation, and forearm or wrist location</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barralon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ansermino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Annual International Conference of the IEEE Engineering in Medicine and Biology Society</title>
		<meeting>of Annual International Conference of the IEEE Engineering in Medicine and Biology Society<address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="4202" to="4205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">From perception with a 49-point electrotactile stimulus array on the tongue: a technical note</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bach-Rita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kaczmarek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tyler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Garcia-Lara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Rehabilitation Research and Development</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="427" to="430" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cross-modal plasticity revealed by electrotactile stimulation of the tongue in the congenitally blind</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ptito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moesgaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gjedde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kupers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="606" to="614" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A wireless embedded tongue tactile biofeedback system for balance control</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vuillerme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pinsault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chenu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fleury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Payan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Demongeot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pervasive and Mobile Computing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="268" to="275" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title/>
		<ptr target="http://www.loc.gov/nls/" />
	</analytic>
	<monogr>
		<title level="j">National Library Service for the Blind and Physically Handicapped. Updated information</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A sonar aid to enhance spatial perception of the blind: engineering design and evaluation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radio and Electronic Engineer</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="605" to="627" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Updated information</title>
		<author>
			<orgName type="collaboration">SonicVision</orgName>
		</author>
		<ptr target="http://www.sonicvision.co.nz" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<ptr target="http://www.gdp-research.com.au" />
		<title level="m">GDP Research Australia Updated information</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Bay Advanced Technologies Ltd</title>
		<ptr target="http://www.batforblind.co.nz" />
		<imprint/>
	</monogr>
	<note>Updated information</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An experimental system for auditory image representations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Meijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="112" to="121" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Seeing with Sound -The vOICe</title>
		<ptr target="http://www.seeingwithsound.com" />
		<imprint/>
	</monogr>
	<note>Updated information</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Coding the environment in tactile maps for real-time guidance of the visually impaired</title>
		<author>
			<persName><forename type="first">R</forename><surname>Velazquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fontaine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pissaloux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Symposium on Micro-NanoMechatronics and Human Science</title>
		<meeting>of IEEE International Symposium on Micro-NanoMechatronics and Human Science<address><addrLine>Nagoya, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Design of a wearable tactile display</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gemperle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Siewiorek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of. 5th International Symposium on Wearable Computers</title>
		<meeting>of. 5th International Symposium on Wearable Computers<address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Providing directional information with tactile torso displays</title>
		<author>
			<persName><forename type="first">H</forename><surname>Van Veen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Erp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EuroHaptics</title>
		<meeting>of EuroHaptics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="page" from="471" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Tactile display and vibrotactile pattern recognition on the torso</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lockyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Piateski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advanced Robotics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1359" to="1374" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The NavBelt -A computerized multi-sensor travel aid for active guidance of the blind</title>
		<author>
			<persName><forename type="first">J</forename><surname>Borenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CSUN&apos;s 5th Annual Conference on Technology and Persons with Visual Disabilities</title>
		<meeting>of CSUN&apos;s 5th Annual Conference on Technology and Persons with Visual Disabilities<address><addrLine>Los Angeles, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="107" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">ActiveBelt: belt-type wearable tactile display for directional navigation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tsukada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yasumrua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UbiComp 2004</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Davies</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Mynatt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Siio</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3205</biblScope>
			<biblScope unit="page" from="384" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Beyond sensory substitutionlearning the sixth sense</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Carl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kringe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Märtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>König</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neural Engineering</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="13" to="R26" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Proprioceptive control of posture: a review of new concepts</title>
		<author>
			<persName><forename type="first">J</forename><surname>Allum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hulliger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hadders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gait and Posture</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="214" to="242" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A shoe-integrated tactile display for directional navigation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Velazquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Magaña</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<meeting>of IEEE/RSJ International Conference on Intelligent Robots and Systems<address><addrLine>St. Louis, MO, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1235" to="1240" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
