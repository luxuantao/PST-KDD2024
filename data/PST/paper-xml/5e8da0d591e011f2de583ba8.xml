<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ProGen: Language Modeling for Protein Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-03-08">8 Mar 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ali</forename><surname>Madani</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nikhil</forename><surname>Naik</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Namrata</forename><surname>Anand</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Raphael</forename><forename type="middle">R</forename><surname>Eguchi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Po-Ssu</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
						</author>
						<title level="a" type="main">ProGen: Language Modeling for Protein Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-03-08">8 Mar 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2004.03497v1[q-bio.BM]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative modeling for protein engineering is key to solving fundamental problems in synthetic biology, medicine, and material science. We pose protein engineering as an unsupervised sequence generation problem in order to leverage the exponentially growing set of proteins that lack costly, structural annotations. We train a 1.2B-parameter language model, ProGen, on ∼280M protein sequences conditioned on taxonomic and keyword tags such as molecular function and cellular component. This provides ProGen with an unprecedented range of evolutionary sequence diversity and allows it to generate with fine-grained control as demonstrated by metrics based on primary sequence similarity, secondary structure accuracy, and conformational energy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generating proteins with desired properties is one of the most complex yet impactful problems in biology. Protein engineering research has grown over the past 50 years and yielded remarkable outcomes including the development of new enzymes, therapies, and sensors. However, leading experimental techniques for protein engineering such as directed evolution <ref type="bibr" target="#b3">(Arnold, 1998)</ref> still rely on heuristics and random mutations to select initial sequences for rounds of evolution.</p><p>The raw amino acid sequence encodes a protein, and during synthesis, this chain of amino acids folds in ways that exhibit local (secondary) and global (tertiary) structure. These structural properties then directly determine a unique function, which is of ultimate interest to protein engineers. Unfortunately, obtaining three-dimensional structural information for proteins is expensive and time consuming. Consequently, there are three orders of magnitude more raw sequences than there are sequences with structural annotations, and protein sequence data is growing at a near exponential rate.</p><p>Recent research <ref type="bibr" target="#b1">(Alley et al., 2019;</ref><ref type="bibr" target="#b35">Rives et al., 2019;</ref><ref type="bibr" target="#b33">Rao et al., 2019)</ref> has begun to capitalize on the much larger set of raw protein sequences by adapting state-of-the-art representation learning techniques <ref type="bibr" target="#b14">(Devlin et al., 2018)</ref> from natural language processing (NLP) to classification of protein properties. However, there has been no attempt to adapt state-of-the-art methods for artificial text generation <ref type="bibr" target="#b31">(Radford et al., 2019)</ref>, and in particular the kind of controllable generation <ref type="bibr" target="#b23">(Keskar et al., 2019)</ref> that would be most useful for protein engineering.</p><p>We introduce ProGen for controllable protein generation. ProGen is a 1.2 billion parameter conditional language model trained on a dataset of 280 million protein sequences together with conditioning tags that encode a variety of annotation such as taxonomic, functional, and locational information. By conditioning on these tags, ProGen provides a new method for protein generation that can be tailored for desired properties (Figure <ref type="figure" target="#fig_0">1</ref>).</p><p>According to NLP metrics, ProGen is a powerful language model, achieving comparable performance to similarlysized models for English. This performance improves in settings with larger amino acid contexts and when ProGen is provided a larger number of conditioning tags, which highlights its potential for applications in providing viable, starting sequences for directed evolution or de novo protein design <ref type="bibr" target="#b20">(Huang et al., 2016)</ref>. ProGen also performs well when used to model unseen protein families, but it is even more effective when fine-tuned for those unseen families as an alternative to training from random initialization. These results inspire the use of ProGen to generate candidate sequences in challenging, low-homology applications.</p><p>Proteins generated by ProGen satisfy desired structural, and by extension functional, properties when evaluated with metrics for sequence similarity, secondary structure accuracy, and conformational energy-from lower level structure to higher level structure. Generation performance is judged higher quality by higher level metrics, which suggests that ProGen has learned invariances to mutations at the sequence level that conserve structure and inferred function. At the highest level, conformational energy analysis reveals that generated proteins exhibit energy levels near that of native proteins, providing our strongest evidence that these proteins satisfy the desired structure and inferred function. In our first case study, we examine completion of a VEGFR2 protein, which is held-out in training. ProGen generates candidate completions that conserve the structural elements most important for determining function and exhibit conformational energy near to native levels across a variety of generation lengths. In our second case study, we observe that ProGen can select high fitness antibody-binding GB1 proteins without supervised training or unsupervised finetuning-indicating that ProGen has learned the underlying distribution of functional proteins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Protein representation learning. Recent methods for contextualized representations <ref type="bibr" target="#b25">(McCann et al., 2017;</ref><ref type="bibr" target="#b28">Peters et al., 2018;</ref><ref type="bibr" target="#b14">Devlin et al., 2018)</ref> in natural language processing have been demonstrated to work well for contextual protein representation learning. Structural information about a protein can be extracted from such representations using linear methods, and the representations themselves can be adapted to improve performance on other tasks <ref type="bibr" target="#b35">(Rives et al., 2019)</ref>. Similarly, UniRep <ref type="bibr" target="#b1">(Alley et al., 2019)</ref> demonstrated that such representations could be used to predict stability of natural and de novo designed proteins as well as quantitative function of molecularly diverse mutants. TAPE <ref type="bibr" target="#b33">(Rao et al., 2019)</ref> is a new benchmark consisting of five tasks for assessing such protein embeddings. While this body of prior work focuses on transferable representation learning using bidirectional models, our work demonstrates controllable protein engineering with generative, unidirectional models.</p><p>Generative models for protein engineering. Recent generative modeling work such as <ref type="bibr" target="#b22">Ingraham et al. (2019)</ref> extends the transformer to condition it on a graph-structured specification of a desired target. <ref type="bibr" target="#b2">Anand &amp; Huang (2018)</ref> utilizes generative adversarial networks to produce 2D pairwise distance map for given protein structural fragments, essentially in-painting missing residues. The aforementioned work, along with O'Connell et al. ( <ref type="formula">2018</ref>), <ref type="bibr" target="#b11">Boomsma &amp;</ref><ref type="bibr" target="#b11">Frellsen (2017), and</ref><ref type="bibr" target="#b17">Greener et al. (2018)</ref>, all utilize explicit structural information for generative modeling, thereby are unable to fully capture the number and diversity of sequenceonly data available. Meanwhile sequence-only generative modeling have been attempted recently through residual causal dilated convolutional neural networks <ref type="bibr" target="#b34">(Riesselman et al., 2019)</ref> and variational autoencoders <ref type="bibr" target="#b13">(Costello &amp; Martin, 2019)</ref>. Unlike these prior works, our work on generative modeling focuses on a high-capacity language models that scale well with sequence data and can be used for controllable generation.</p><p>Language Models and Controllable Generation. Large Transformer architectures <ref type="bibr" target="#b38">(Vaswani et al., 2017)</ref> like GPT-2 <ref type="bibr" target="#b31">(Radford et al., 2019)</ref> represent the state-of-the-art in unconditional language modeling and demonstrate impressive text generation capabilities <ref type="bibr" target="#b42">(Zellers et al., 2019)</ref> after training on vast amounts of unsupervised English text. CTRL <ref type="bibr" target="#b23">(Keskar et al., 2019)</ref> trained a similarly large Transformer architecture for language generation by conditioning on properties of the text easily extracted at scale, e.g. domain, style, and even associated URL. We adapt this perspective to protein engineering by training a conditional transformer language model on amino acid sequences conditioned on a set of protein properties referred to as conditioning tags. Notably different from <ref type="bibr" target="#b23">Keskar et al. (2019)</ref>, protein engineering requires a finer-grained, much larger, and more complex set of conditioning tags. Additionally, a single protein can be paired with dozens of conditioning tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>Let a = (a 1 , . . . , a na ) be a sequence of amino acids that constitutes a protein. In the context of protein engineering, there is typically also a set of desired protein properties such as function or affiliation with a particular organism. Following recent work on controllable, conditional language modeling <ref type="bibr" target="#b23">(Keskar et al., 2019)</ref>, we refer to these properties generally as 'conditioning tags' through which we would like to control generation of amino acid sequences. Let c = (c 1 , . . . , c nc ) be a sequence of such conditioning tags, and let x = [c; a] the sequence formed by prepending a conditioning tag sequence to an amino acid sequence. p(x) is then the probability over such combined sequences of length n = n a + n c . We can factorize this distribution using the chain rule of probability <ref type="bibr" target="#b9">(Bengio et al., 2003)</ref>:</p><formula xml:id="formula_0">p(x) = n i=1 p(x i |x &lt;i )</formula><p>This decomposes the problem of conditional protein generation into next-token prediction, where a token x i can either be an amino acid or a conditioning tag. A neural network with parameters θ can then be trained to minimize the negative log-likelihood over a dataset D = {x<ref type="foot" target="#foot_2">1</ref> , . . . , x |D| }:</p><formula xml:id="formula_1">L(D) = − |D| k=1 log p θ (x k i |x k &lt;i )</formula><p>Note that p(a|c), the distribution over proteins conditioned on their corresponding conditioning tags, is just one of the many conditional distributions that can be recovered from a model that learns p(x). A new protein ã of length m a with desired properties encoded by a conditioning tag sequence c of length m c can then be generated by sequentially sampling its constituent symbols: p θ (a 0 |c), p θ (a 1 |ã 0 , c), . . . , p θ (a p |ã &lt;p , c).</p><p>We train a variant of the Transformer <ref type="bibr" target="#b38">(Vaswani et al., 2017)</ref> to learn these conditional distributions over amino acids and conditioning tags. A sequence containing n tokens is embedded as a sequence of n corresponding vectors in R d . Each vector is the sum of a learned token embedding and a sinusoidal positional embedding as in the original Transformer architecture. This sequence of vectors is stacked into a matrix X 0 ∈ R n×d so that it can be processed by l attention layers. The ith layer consists of two blocks, each of which preserves the model dimension d.</p><p>The core of the first block is multi-head attention with k heads that uses a causal mask to preclude attending to future tokens:</p><formula xml:id="formula_2">Attention(X, Y, Z) = softmax mask(XY ) √ d Z MultiHead(X, k) = [h 1 ; • • • ; h k ]W o where h j = Attention(XW 1 j , XW 2 j , XW 3 j )</formula><p>The core of the second block is a feedforward network with ReLU activation that projects inputs to an inner dimension f , with parameters U ∈ R d×f and V ∈ R f ×d :</p><formula xml:id="formula_3">F F (X) = max(0, XU )V</formula><p>Each block precedes core functionality with layer normalization <ref type="bibr" target="#b5">(Ba et al., 2016;</ref><ref type="bibr" target="#b12">Child et al., 2019)</ref> and follows it with a residual connection <ref type="bibr" target="#b18">(He et al., 2016)</ref>. Together, they yield</p><formula xml:id="formula_4">X i+1 : Block 1 Block 2 Xi = LayerNorm(X i ) Hi = LayerNorm(H i ) H i = MultiHead( Xi ) + Xi X i+1 = FF( Hi ) + Hi</formula><p>Scores are then computed from the output of the last layer:</p><formula xml:id="formula_5">Scores(X 0 ) = LayerNorm(X l )W vocab</formula><p>During training, these scores are the inputs of a crossentropy loss function. During generation, the scores corresponding to the final token are normalized with a softmax, yielding a distribution for sampling a new token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data</head><p>We utilize all protein sequences and associated tags available in Uniparc <ref type="bibr" target="#b24">(Leinonen et al., 2004)</ref>, Unipro-tKB <ref type="bibr" target="#b7">(Bairoch et al., 2005)</ref>, SWISS-PROT <ref type="bibr" target="#b6">(Bairoch et al., 2004)</ref>, TrEMBL <ref type="bibr" target="#b10">(Boeckmann et al., 2003)</ref>, Pfam <ref type="bibr" target="#b8">(Bateman et al., 2004)</ref>, and NCBI taxonomic information <ref type="bibr" target="#b16">(Federhen, 2012)</ref>. The aggregated dataset contains over 281M proteins-the most comprehensive, non-redundant, annotated database of proteins used to train a machine learning model. For the amino acid vocabulary, we use the standard 25 amino acids designations in IUPAC <ref type="bibr" target="#b29">(Pettit &amp; Powell, 2006)</ref>. The conditioning tags are divided into 2 categories:</p><p>(1) keyword tags and (2) taxonomic tags. Following the definitions laid out in the UniprotKB controlled, hierarchical vocabulary of keywords (many of which are derived from Gene Ontology (GO) terms) <ref type="bibr" target="#b4">(Ashburner et al., 2000)</ref>, the conditioning keyword tags included 1100 terms ranging from cellular component, biological process, and molecular function terms. The taxonomic tags include 100k terms from the NCBI taxonomy across the eight standard taxonomic ranks. The aggregated dataset was split into a training set of size 280M, a held-out protein family 1 test set (OOD-test) of size 100k, and a randomly sampled test set (ID-test) of size 1M. OOD-test comprises of 20 protein families, as defined in Pfam, that were excluded from the training data. Performance on OOD-test measures ability to model samples from unseen protein families, whereas performance on ID-test measures ability to model samples from a wider range of protein families that more closely match the distribution of the training set as described in section A.1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training Details</head><p>For training, we include each sequence and its reverse, as proteins are invariant to the temporal notion of sequence generation. We then prepend each sequence (and its reverse) with a corresponding subset of conditioning tags. For a given sequence, there can be multiple versions across databases, each with their own associated conditioning tags.</p><p>In training, we randomly sample which set of conditioning tags to utilize but bias toward SWISSPROT tags as they are manually verified. We apply dropout to the conditioning tags themselves at a rate of 0.4. We additionally always include a sample with the sequence alone without conditioning tags so that ProGen can be used to complete proteins using only sequence data even when no protein properties are known. We then truncate all sequences to a maximum length of 512. Sequences of length less than 512 were padded, but no loss was backpropagated through the network for padding tokens. The model has dimension d = 1028, inner dimension f = 512, 36 layers, and 8 heads per layer. Dropout with probability 0.1 follows the residual connections in each layer. Token embeddings were tied with the embeddings of the final output layer <ref type="bibr" target="#b21">(Inan et al., 2016;</ref><ref type="bibr" target="#b30">Press &amp; Wolf, 2016)</ref>.</p><p>Our model was implemented in TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref> and trained with a global batch size of 64 distributed across 256 cores of a Cloud TPU v3 Pod for 1M iterations. Training took approximately two weeks using Adagrad <ref type="bibr" target="#b15">(Duchi et al., 2011)</ref> with linear warmup from 0 to 1e −<ref type="foot" target="#foot_4">2</ref> over 40k steps. Gradient norms were clipped to 0.25. Training in early stages was improved and stabilized by initializing with the pretrained weights of <ref type="bibr" target="#b23">Keskar et al. (2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Generation Details</head><p>ProGen generates proteins one amino acid at a time. For one step of generation, ProGen takes a context sequence of amino acids as input and outputs a probability distribution over amino acids. We sample from that distribution and then update the context sequence with the sampled amino acid. This process repeats until a protein of desired length has been generated. We compare different combinations of top-k sampling <ref type="bibr" target="#b31">(Radford et al., 2019)</ref> with a repetition penalty designed for amino acid sequence generation. The repetition penalty reduces the probability of amino acids that have been generated within 4 tokens prior to the token to be predicted. Top-k sampling draws the next token from the k most probable tokens in the distribution output by ProGen. We report results for top-k values of k = 1 and k = 3 with repetition penalties of 0 and 1.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Evaluation Details</head><p>To assess how well ProGen models the training and test distributions, we rely on perplexity as the standard metric for language models, a mean hard accuracy over each token to strictly assess each amino acid error, and a mean soft accuracy defined by incorporating BLOSUM62 <ref type="bibr" target="#b19">(Henikoff &amp; Henikoff, 1992)</ref>, a standard amino acid substitution matrix.</p><p>Perplexity is the exponentiated cross-entropy loss computed over each token in a dataset. Thus, high quality language models are expected to have low perplexities. Mean pertoken hard accuracy over the tokens in a sequence judges a prediction incorrect for any amino acid that is not the ground truth. Mean per-token soft accuracy relies on BLO-SUM62, a block substitution matrix that specifies which amino acid substitutions are more or less acceptable according to their frequency in known well-formed proteins. BLOSUM62 is widely used across adopted alignment software (e.g., BLAST 2 ). Our mean per-token soft accuracy uses BLOSUM62 to penalize incorrect amino acid predictions according to the frequency of that substitution in the matrix. In this way, if the substitution is likely in nature, soft accuracy penalizes the model less.</p><p>To assess the quality of generation, we evaluate across three levels of structure: (1) primary sequence similarity, (2) secondary structure accuracy, and (3) conformational energy analysis.</p><p>Primary sequence similarity is defined by a global, pairwise sequence alignment score computed with the Biopython package<ref type="foot" target="#foot_5">3</ref> . This score is based on the Needleman-Wunsch algorithm <ref type="bibr" target="#b26">(Needleman &amp; Wunsch, 1970)</ref> informed by the BLOSUM62 substitution matrix. We use a gap open penalty of −0.5 and gap continue penalty of −0.1. The resulting score is then normalized by the length of the protein. Experiments reporting sequence similarity are limited to test samples with a form of experimental evidence of X-ray/NMR crystallography, mass spectrometry, or existence in cDNA or RT-PCR to indicate transcript existence. We refer the reader to UniprotKB existence scores with experimental evidence<ref type="foot" target="#foot_6">4</ref> for further details.</p><p>Secondary structure accuracy was computed per-residue for predicted secondary structures by PSIPRED<ref type="foot" target="#foot_7">5</ref> with greater than 0.5 confidence. PSI-BLAST was performed on each generated sample to extract the Multiple Sequence Alignments (MSAs) with respect to the UniRef90 database <ref type="bibr" target="#b37">(Suzek et al., 2015)</ref>. These MSAs were provided to PSIPRED for higher quality secondary structure prediction. Experiments reporting secondary structure accuracy were limited to test samples with high UniprotKB existence scores as described in the previous paragraph.</p><p>Conformational energy uses the Rosetta-RelaxBB protocol<ref type="foot" target="#foot_8">6</ref> . Rosetta-RelaxBB performs a Monte Carlo optimization of the Rosetta energy function over the space of amino acid types and rotamers. The Rosetta energy is based on biophysical laws and constraints. Between each design round, amino acid side-chains are replaced, while the carbon backbone torsions are kept fixed. Energy minimization/relaxation is performed after threading the amino acid sequence through the known structure. This allows the backbone to move, possibly into a lower energy state. A lower resulting Rosetta energy correlates to a more relaxed-state and viable conformation for a given protein structure. Before applying the procedure above, we relax the native template first. Experiments that report conformational energy are limited to test samples from SWISSPROT with associated 3D structures in RCSB PDB<ref type="foot" target="#foot_9">7</ref> .</p><p>To assess generative quality, we provide baselines for different levels of random mutation. For a given sequence, a proportion (25 − 100%) of amino acids in the sequence is randomly substituted within one of the 20 standard amino acids other than itself. For conformational energy, we also include an all-alanine baseline (i.e. a sequence with only the amino acid alanine), as it is a non-bulky, chemically inert amino acid that mimics the existing secondary structure well when substituted. These baselines provide a scale across each of the above metrics. A particular random mutation may or may not have constructive or destructive effects on protein structure or function. But viewed in aggregate, the performance of the 100% mutation baseline for any metric indicates failed generation. As performance approaches 0%, generation statistically indicates a closer reflection to desired structural and functional properties. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluating ProGen as a language model</head><p>In this section, we demonstrate that ProGen is a high-quality language model according to per-token metrics on the training and test sets.</p><p>ProGen generalizes to the full test set and achieves perplexities representative of a high-quality language model. Perplexities reported in Table <ref type="table" target="#tab_0">1</ref> demonstrate that ProGen dramatically improves over a Uniform Baseline, in which amino acids are sampled according to a uniform distribution, and an Empirical Baseline, in which amino acids are sampled according to the empirical frequencies in the training set. As a point of reference, state-of-the-art unidirectional language models for English Wikipedia achieve perplexities that range from 10 to 17 depending on model size (between 257M and 8.3B parameters) and whether training data was constrained to English Wikipedia <ref type="bibr" target="#b32">(Rae et al., 2019)</ref> or not <ref type="bibr" target="#b36">(Shoeybi et al., 2019)</ref>.</p><p>ProGen generalizes to unseen protein families. The second section of Table <ref type="table" target="#tab_0">1</ref> breaks this result into perplexities over the ID-test and OOD-test sets separately. Results on ID-test confirm that ProGen generalizes well to sequences that belonged to protein families randomly sampled. As expected, performance is worse on the sequences in the OOD-test set, but the model still outperforms the Empirical Baseline for those held out protein families.</p><p>Fine-tuning ProGen on unseen protein families improves over training from random initialization. We fur-Figure <ref type="figure">3</ref>. Full test set performance is better for later segments of sequences in keeping with intuition that additional context supports better predictions. We examined intervals up to 500 tokens to ensure a minimum of 30k samples per interval.</p><p>. ther split OOD-test into OOD-test-80 and OOD-test-20, fine-tuned ProGen on OOD-test-80 until convergence (5 epochs; Adam; linear learning rate warmup to 1k iterations), and retested on OOD-test-20. The third section of Table <ref type="table" target="#tab_0">1</ref> shows that fine-tuning from ProGen improves over training the same architecture with randomly initialized weights.</p><p>ProGen performance improves with increased amino acid and conditioning tag context. In Figure <ref type="figure">3</ref>, we examine the mean perplexity and per-token hard accuracy over different portions of proteins. Perplexity decreases and hard accuracy increases for later portions of a protein, in keeping with the intuition that additional amino acid context narrows down the possibilities for future tokens. The same trends hold when increasing the number of conditioning tags and taking the mean over sequence lengths with the same of tags (in Figure <ref type="figure" target="#fig_2">4</ref>). This indicates that conditioning tags also provide signal that improves model predictions.</p><p>Training curves suggest that protein generation would benefit from even larger models and longer training. With 1B parameters, ProGen is comparable in size to the largest language models that have been publicly released for any modality, and, to the best of our knowledge, it is the largest model trained on amino acid sequences. Figure <ref type="figure" target="#fig_1">2</ref> shows that despite its size and the amount of compute used to train, ProGen has yet to overfit the training data. This suggests that models for protein generation could still benefit from even larger models and additional compute.</p><p>BLOSUM62 soft accuracy reveals that ProGen prediction errors often follow natural amino acid substitutions that likely conserve higher level structure. Though Pro-Gen models proteins as pure sequences, protein function is more directly determined by the secondary and tertiary structures that these sequences encode in three-dimensional space. Model performance based on BLOSUM62 soft accuracy (Section 3.4) is more than 20% higher than using hard accuracy, which indicates that when ProGen errors may often be substitutions that are acceptable in nature because they still reflect the proper higher-level properties. This suggests that ProGen has learned how to work within function-preserving mutational invariances-we continue to validate this finding for primary, secondary, and conformational structure in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Generating with ProGen</head><p>In this section, we focus on assessing ProGen as a generative model. Generation quality is directly correlated with evolutionary viability and functional qualities, which can be inferred through protein structure. For this reason, we assess generation quality by using metrics for primary sequence similarity, secondary structure accuracy, and conformational energy (Section 3.4).</p><p>We also include several mutation baselines (Section 3.4) that allow us to compare the similarity of generated proteins to a target, reference protein across all metrics. In reference to these mutation baselines, ProGen quality improves as we move from primary sequence to full conformational structure metrics, thereby suggesting the model has learned mutational invariances in structure which present as errors in lower-level metrics.</p><p>ProGen achieves higher sequence similarity scores with an amino acid repetition penalty. Figure <ref type="figure" target="#fig_3">5</ref> depicts the results of experimenting with various combinations of topk sampling and repetition penalties (see Section 3.4 for details). Over all context lengths, ProGen performs best with k = 1 and the repetition penalty applied to recently generated amino acids. Consequently, we use these settings for all following generation experiments. With this nearly greedy sampling, ProGen manages to generate proteins with sequence similarity comparable to randomly mutating 50% of the amino acids that are not seen in the given context.</p><p>Sequence similarity suggests that ProGen merely approaches the 25% mutation baseline, but secondary structure accuracy suggests that ProGen surpasses it. In Figure <ref type="figure">6</ref>, we analyze this sequence similarity across differing numbers of conditioning tags. Sequences associated with at least 3 conditioning tags begin to exceed the 50% mutation baseline, and as amino acid context increases, sequences with at least 8 conditioning tags approach the 25% mutation baseline. Notably, even in the best case, according to sequence similarity, ProGen doesn't surpass the 25% mutation baseline. By contrast, according to secondary structure accuracy, sequences with at least 8 conditioning tags surpass the 25% mutation baseline (Figure <ref type="figure">7</ref>). This discrepancy between sequence similarity and secondary structure accuracy further corroborates our claim from Section 4: errors registered by lower-level metrics often correspond to acceptable substitutions according to higher-level metrics that more directly correspond to functional viability.</p><p>After threading and relaxation, samples generated by ProGen are likely to exhibit desired structure and function. As a measure of generation quality, we thread ProGen sequences through known structures and examine if they exhibit favorable, low energy states. Figure <ref type="figure" target="#fig_5">8</ref> shows the differences between the energy levels of native proteins, ProGen samples, the native proteins with 50% and 100% of amino acids randomly mutated, as well as the all-alanine 0.5 0.6 0.7 0.8 0.9</p><p>Proportion of Sequence as Context baseline. Proteins completed by ProGen are much closer to the energy levels of the native protein than all baselines. Generated samples exhibit energy levels near or even below their associated relaxed native templates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Case Study: Completing VEGFR2 kinase domain</head><p>VEGFR2 is responsible for fundamental cell processes such as cell proliferation, survival, migration, and differentiation. VEGFR2 was excluded from training as a subsequence belongs to a held out protein family in OOD-test. We study how well ProGen generates in the context of a protein com-  pletion task. We consider the amino acid sequence beginning at residue 806 and ending at residue 1168 of VEGFR2 (PDB ID: 2XIR). For different generation lengths, we sample from ProGen to complete the sequence up to residue 1168 with the remainder of the sequence provided as context.</p><p>Figure <ref type="figure" target="#fig_6">9</ref> shows that the conformational energy calculated after threading and relaxation of ProGen samples are lower compared to all baselines, indicating better structural conservation. Generation quality remains near the native relaxed protein independent of generation length.</p><p>The generated samples across Figure <ref type="figure" target="#fig_6">9</ref> exhibit a mean sequence identity of 73.1% with the native sequence. This correlates to a lower sequence identity than the 25% mutation baseline (74% identity) but with better Rosetta energies. This suggests meaningful deviation from the native protein while achieving the ultimate goal of preserving low energy. Figure <ref type="figure" target="#fig_7">10</ref> shows one sample from ProGen as well as one from each of the 25% and 75% mutation baselines. The ProGen sample exhibits lower energy overall, and energy is highest for amino acids that do not have secondary structure. This suggests that ProGen learned to prioritize the most structurally important segments of the protein.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Case Study: Zero-shot fitness selection for protein GB1</head><p>The ultimate goal of protein engineering is to engineer functional proteins. One promising avenue is via directed evolution, which iterates through rounds of mutation and screening to converge on a high-fitness (i.e. functioning) protein.</p><p>Machine learning has shown initial promise to aid in the subsequent rounds of directed evolution by in silico screening of proteins <ref type="bibr" target="#b41">(Wu et al., 2019)</ref>, but it still relies on random mutation in an exponentially large search space. Ideally, a generative model, such as ProGen, that has learned the distribution of evolutionarily-relevant proteins can directly generate high-fitness proteins.</p><p>We examine the empirical fitness landscape of protein G domain B1 (GB1) binding to an antibody <ref type="bibr" target="#b40">(Wu et al., 2016)</ref>. Protein G is important for the purification, immobilization, and detection of immunoglobulins (antibodies), proteins used by our immune system to neutralize pathogenic viruses and bacteria. Ideally, we would want the ability to generate GB1 proteins with high binding affinity and stability. The data includes 149,361 of a total 160,000 possible variants from NNK/NNS saturation mutagenesis at four positions known to interact epistatically. Reported fitness values correspond to a measure of both stability (i.e. the fraction of folded proteins) and function (i.e. binding affinity to IgG-Fc) by coupling mRNA display with next-generation sequencing. Protein sequences with high fitness values are desired.</p><p>Without supervised training of ProGen on the GB1 data or unsupervised fine-tuning of ProGen on a subset of similar immunoglobulin-binding proteins, we pass each variant through ProGen and select the top one hundred variants with the lowest perplexity values. In Figure <ref type="figure" target="#fig_8">11</ref>, we demonstrate ProGen is effective in zero-shot selection of high-fitness protein sequences. In comparison, random mutation, which is the main technique used by directed evolution and MLassisted directed evolution, statistically generates samples with low or zero fitness. With effective sampling techniques, ProGen can be utilized to generate a spread of samples that are statistically high fitness. These results imply that ProGen has not only learned the distribution of structurally-relevant proteins, but also functionally-relevant proteins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We introduced ProGen, a controllable protein generation language model trained on the full evolutionary diversity of one of the largest sequence databases. The model generates proteins that exhibit near native structure energies which likely implies functional viability. ProGen has the potential to play a new, complementary role alongside other stateof-the-art methods in protein engineering. For example, in directed evolution, initial sequences may be sampled from ProGen according to desired conditioning tags. In later rounds of evolution, protein completion with context for particular residue spans, or hotspots, may provide higher fitness samples. In de novo protein design, using ProGen with conditioning tags may allow for designing new proteins with existing folding motifs in new protein families or host organisms. This same strategy may be used in conjunction with threading and structure-based protein design. Because conditioning tags orient ProGen in sequence space, ProGen may even be used as a model to sample from the distribution of evolutionarily viable proteins near one particular protein.</p><p>This may provide useful augmentations around data for non-homologous domains where existing techniques, such as MSAs, fall short.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head><p>We would like to thank Alex Chu for assistance in the threading and minimization experiments along with Jesse Vig for visualizing the attention heads of ProGen.</p><p>A. Appendix</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Measuring out-of-distribution</head><p>The objective of our work is to enable high-quality protein generation. To test the effectiveness of our trained model, we had two test subsets: ID-Test and OOD-Test. ID-Test is a random split of the non-redundant sample database and can be viewed as a typical in-distribution test set of held-out samples.</p><p>In contrast, OOD-Test represents an out-of-distribution set. OOD-Test consists samples that contained a matching subsequence residing in one of twenty Pfam protein families that were held out of Train and ID-Test.  To quantify the out-of-distribution nature of OOD-Test, we computed a normalized histogram of 3-grams and 5-grams across samples in the Train, ID-Test, and OOD-Test datasets.</p><p>The sum of absolute errors (SAE) was computed for a pair of histograms as shown in Table <ref type="table" target="#tab_1">2</ref>. Two normalized histograms that align perfectly would have an SAE of 0 and two normalized histograms that are completely divergent would have an SAE of 2. The results imply that the OOD-Test is drawn from a significantly different distribution.</p><p>The held-out protein families included PF18369, PF04680, PF17988, PF12325, PF03272, PF03938, PF17724, PF10696, PF11968, PF04153, PF06173, PF12378, PF04420, PF10841, PF06917, PF03492, PF06905, PF15340, PF17055, PF05318.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Generation with only conditioning tags</head><p>We observe that ProGen can be used to generate proteins with only conditioning tags and no initial amino acid context. For the following example, we prompt ProGen to greedily generate a protein sequence with the tags Flavoprotein and FMN. As defined by the UniprotKB keyword, the FMN tag refers to "a protein involved in flavin adenine mononucleotide (FMN) synthesis or protein which contains at least one FMN as prosthetic group/cofactor (flavoproteins) or cosubstrate, such as many oxidation-reduction enzymes".</p><p>The generated sequence of length 400 is then passed to the HHblits package by <ref type="bibr" target="#b43">Zimmermann et al. (2018)</ref> to search for a multiple sequence alignment (MSA). As shown in Figure <ref type="figure" target="#fig_11">13</ref>, there are multiple sequences that align well with the ProGen sequence. Figures 14-16 demonstrate the alignments have high E-values and have related properties. The lower the E-value, the lower the probability of a random match and the higher the probability that the alignment match is related to the searched sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Model visualizations</head><p>ProGen was trained from a randomly initialized embedding layer with no prior knowledge of residue biochemical properties. Through per-token training on millions of protein sequences, ProGen seems to have inherently learned the natural clustering of amino acids that align with our understanding of biophysicochemical properties. In Figure <ref type="figure" target="#fig_10">12</ref>, the trained embedding weights for the standard amino acids tokens are reduced to three dimensions with principle component analysis (PCA).       </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. a) Protein sequence data is growing exponentially as compared to structural data. b) We utilize protein sequence data along with taxonomic and keyword tags to develop a conditional language model: ProGen.</figDesc><graphic url="image-1.png" coords="2,55.44,81.46,485.99,108.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure2. Large model capacity is warranted as ProGen has yet to overfit. BLOSUM62-informed soft accuracy shows no gap between train and test performance, suggesting hard accuracy hides the possibility that ProGen errors often correspond to amino acid substitutions found in nature. For metrics details see Section 3.4.</figDesc><graphic url="image-2.png" coords="5,307.44,81.46,234.00,175.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure4. Full test set performance also improves as the number of conditioning tags associated with proteins increases. We examined proteins with up to 14 conditioning tags to ensure a minimum of 3k samples per category.</figDesc><graphic url="image-4.png" coords="6,307.44,81.46,234.00,156.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Across all context lengths, greedily sampling with a repetition penalty provides the best results according to sequence similarity.</figDesc><graphic url="image-5.png" coords="7,55.44,81.46,234.01,167.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Figure6. A greater number of conditioning tags enables higher quality generation. With at least 8 conditioning tags, generation quality approaches the 25% mutation baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Conformational energies for ProGen generated proteins surpasses all baselines and adheres closely to the energy of the native template.</figDesc><graphic url="image-6.png" coords="8,55.44,81.46,234.00,156.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. ProGen completion quality for VEGFR2 remains steadily near native conformational energy levels across generation lengths.</figDesc><graphic url="image-7.png" coords="8,55.44,309.32,234.00,156.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. ProGen makes fewer mistakes and prioritizes conservation of secondary structure as compared to baselines. Blue is low energy (stable) and red high (unstable).</figDesc><graphic url="image-9.png" coords="8,470.27,81.46,71.18,75.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Without training on the Wu et al. (2016) dataset, Pro-Gen can identify which protein variants exhibit high fitness. The dataset reports fitness values for protein variants of GB1 binding to an antibody. Each sample corresponds to mutating one of four highlighted residues, in the above sequence, to a standard amino acid. At the left, the crystallized structure of GB1 is shown. At the right, the fitness value of samples selected through ProGen vs random selection are shown.</figDesc><graphic url="image-11.png" coords="9,55.44,81.46,234.00,101.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Principle component analysis (PCA) of the ProGen's amino acid embeddings aligns with our intuition of amino acid properties.</figDesc><graphic url="image-12.png" coords="12,307.44,340.60,234.00,156.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 .</head><label>13</label><figDesc>Figure13. There are multiple sequences that align well with the ProGen generated FMN sequence from only conditioning tags. Many of the matching alignments have properties reflective of FMN proteins (e.g. oxidoreductases). A red color corresponds to a significantly low E-value, implying a matching homolog. The MSA was directly taken using HHblits.</figDesc><graphic url="image-13.png" coords="13,55.44,223.81,486.02,303.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. First alignment (ranked by E-value) of a ProGen generated FMN protein. An E-value less than 1e −4 and identity greater than 40% is desired to consider the match as potentially homologous. The sequence labeled as Q is the ProGen protein and the sequence labeled as T is the matched sequence.</figDesc><graphic url="image-14.png" coords="14,55.44,202.68,486.01,345.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 15 .</head><label>15</label><figDesc>Figure 15. Second alignment (ranked by E-value) of a ProGen generated FMN protein. An E-value less than 1e −4 and identity greater than 40% is desired to consider the match as potentially homologous. The sequence labeled as Q is the ProGen protein and the sequence labeled as T is the matched sequence.</figDesc><graphic url="image-15.png" coords="15,91.89,219.35,413.11,312.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 16 .</head><label>16</label><figDesc>Figure 16. Third alignment (ranked by E-value) of a ProGen generated FMN protein. An E-value less than 1e −4 and identity greater than 40% is desired to consider the match as potentially homologous. The sequence labeled as Q is the ProGen protein and the sequence labeled as T is the matched sequence.</figDesc><graphic url="image-16.png" coords="16,91.89,224.05,413.10,303.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 17 .</head><label>17</label><figDesc>Figure 17. Attention patterns of ProGen for a given sequence. Layers 1-3 (rows) and attention heads 1-12 (columns) are displayed. The attention mechanism exhibits well-differentiated local and global patterns which may indicate specialization of each head on different tasks. Two corresponding attention heads from this visualization are shown in Figure 18.</figDesc><graphic url="image-17.png" coords="17,79.74,85.73,437.40,197.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 18 .</head><label>18</label><figDesc>Figure 18. Local attention pattern for two example attention heads. Lines indicate attention to previous tokens for a given predicted token.</figDesc><graphic url="image-18.png" coords="17,302.86,366.57,199.72,290.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>ProGen outperforms uniform random and empirical baselines on the full test set, which includes ID-and OOD-test. OODtest results reveal that ProGen also performs well on protein families unseen during training. Fine-tuning ProGen dramatically improves performance over training from random initialization.</figDesc><table><row><cell>MODEL</cell><cell>PPL</cell><cell>HARD ACC.</cell></row><row><cell>UNIFORM BASELINE</cell><cell>25</cell><cell>4</cell></row><row><cell>EMPIRICAL BASELINE</cell><cell>18.14</cell><cell>6</cell></row><row><cell>PROGEN</cell><cell>8.56</cell><cell>45</cell></row><row><cell>ID-TEST</cell><cell>8.17</cell><cell>45</cell></row><row><cell>OOD-TEST</cell><cell>13.34</cell><cell>22</cell></row><row><cell>OOD-TEST-20 (RAND. INIT.)</cell><cell>17.78</cell><cell>9</cell></row><row><cell>OOD-TEST-20 (FINE-TUNED)</cell><cell>7.45</cell><cell>50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The</figDesc><table /><note>training data and ID-Test data seem to be drawn from a similar distribution, but OOD-Test is markedly different from the others. SAE refers to the sum of absolute errors for normalized 3-gram and 5-gram histograms. If two histograms were entirely divergent, the SAE would yield a value of 2.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Salesforce Research, Palo Alto, CA, USA</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Department ofBioengineering, Stanford University, Stanford, CA, USA. Correspondence to: Ali Madani &lt;amadani@salesforce.com&gt;.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_2">Protein families are groups of evolutionarily-related proteins that have similar structure, function, and sequence similarity as defined by Pfam(Bateman et al.,  </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2004" xml:id="foot_3">)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_4">https://blast.ncbi.nlm.nih.gov/Blast.cgi</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_5">https://biopython.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_6">https://www.uniprot.org/help/protein existence</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_7">http://bioinf.cs.ucl.ac.uk/psipred/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_8">https://www.rosettacommons.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_9">https://www.rcsb.org/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} Symposium on Operating Systems Design and Implementation</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unified rational protein engineering with sequence-based deep representation learning</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Alley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Khimulya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alquraishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1315" to="1322" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generative modeling for protein structures</title>
		<author>
			<persName><forename type="first">N</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7494" to="7505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Design by directed evolution</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Arnold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Accounts of chemical research</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="125" to="131" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gene ontology: tool for the unification of biology</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ashburner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Botstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dolinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Dwight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Eppig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature genetics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="29" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>CoRR, abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Swiss-prot: juggling between evolution and stability</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bairoch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boeckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gasteiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in bioinformatics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="55" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The universal protein resource (uniprot)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bairoch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Apweiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boeckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gasteiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Magrane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="D154" to="D159" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>suppl</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pfam protein families database</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bateman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Coin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Durbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hollich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Griffiths-Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moxon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Sonnhammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="D138" to="D141" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02">Feb. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The swiss-prot protein knowledgebase and its supplement trembl in 2003</title>
		<author>
			<persName><forename type="first">B</forename><surname>Boeckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bairoch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Apweiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-C</forename><surname>Blatter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Estreicher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gasteiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Michoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>O'donovan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Phan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="365" to="370" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spherical convolutions and their application in molecular modelling</title>
		<author>
			<persName><forename type="first">W</forename><surname>Boomsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Frellsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3433" to="3443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">How to hallucinate functional proteins</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Costello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Martin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.00458</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07">Jul. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The ncbi taxonomy database</title>
		<author>
			<persName><forename type="first">S</forename><surname>Federhen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="D136" to="D143" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Design of metalloproteins and novel protein folds using variational autoencoders</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Greener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Amino acid substitution matrices from protein blocks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Henikoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Henikoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
				<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="10915" to="10919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The coming of age of de novo protein design</title>
		<author>
			<persName><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Boyken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">537</biblScope>
			<biblScope unit="issue">7620</biblScope>
			<biblScope unit="page" from="320" to="327" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Tying word vectors and word classifiers: A loss framework for language modeling</title>
		<author>
			<persName><forename type="first">H</forename><surname>Inan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01462</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generative models for graph-based protein design</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ingraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="15794" to="15805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><surname>Ctrl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.05858</idno>
		<title level="m">A conditional transformer language model for controllable generation</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Leinonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">G</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Binns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fleischmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Apweiler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Uniprot archive. Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="3236" to="3237" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learned in translation: Contextualized word vectors</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6294" to="6305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A general method applicable to the search for similarities in the amino acid sequence of two proteins</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Needleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Wunsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of molecular biology</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="443" to="453" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Predicting sequence profiles from protein structures using deep neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>O'connell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Heffernan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Paliwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dehzangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>Spin2</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="629" to="633" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The iupac stability constants database</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Pettit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Powell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemistry international</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Using the output embedding to improve language models</title>
		<author>
			<persName><forename type="first">O</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05859</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Compressive transformers for long-range sequence modelling</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05507</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Evaluating protein transfer learning with tape</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9686" to="9698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Accelerating protein design using autoregressive generative models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Riesselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-E</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Kollasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mcmahon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Manglik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">757252</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rives</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">622803</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<title level="m">Megatron-lm: Training multi-billion parameter language models using gpu model parallelism</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Uniref clusters: a comprehensive and scalable alternative for improving sequence similarity searches</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Suzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Mcgarvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Consortium</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="926" to="932" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><surname>Gar</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">R</forename><surname>Nett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A multiscale visualization of attention in the transformer model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05714</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Adaptation in protein fitness landscapes is facilitated by indirect paths</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Lloyd-Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Elife</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">e16965</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Machine learning-assisted directed protein evolution with combinatorial libraries</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Wittmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Arnold</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Proceedings of the National Academy of Sciences</publisher>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="8852" to="8858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12616</idno>
		<title level="m">Defending against neural fake news</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A completely reimplemented mpi bioinformatics toolkit with a new hhpred server at its core</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stephens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Z</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lozajic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gabler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Söding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Lupas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alva</forename></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of molecular biology</title>
		<imprint>
			<biblScope unit="volume">430</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="2237" to="2243" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
