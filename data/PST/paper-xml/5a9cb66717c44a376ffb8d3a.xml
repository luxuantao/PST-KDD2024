<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Lloyd</forename><forename type="middle">H</forename><surname>Hughes</surname></persName>
							<email>lloyd.hughes@tum.de</email>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Michael</forename><surname>Schmitt</surname></persName>
							<email>m.schmitt@tum.de</email>
						</author>
						<author>
							<persName><forename type="first">Lichao</forename><surname>Mou</surname></persName>
							<email>lichao.mou@dlr.de</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Yuanyuan</forename><surname>Wang</surname></persName>
							<email>yuanyuan.wang@dlr.de</email>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Xiao</forename><surname>Xiang Zhu</surname></persName>
							<email>xiao.zhu@dlr.de</email>
						</author>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Xiang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">are with Signal Processing in Earth Observation</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<postCode>80333</postCode>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Munich</orgName>
								<address>
									<postCode>80333</postCode>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Remote Sensing Technology Institute</orgName>
								<orgName type="institution">German Aerospace Center</orgName>
								<address>
									<postCode>82234</postCode>
									<settlement>Wessling</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DF2116E6BF7154832941DB66D193D039</idno>
					<idno type="DOI">10.1109/LGRS.2018.2799232</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Convolutional neural networks (CNNs)</term>
					<term>data fusion</term>
					<term>deep learning</term>
					<term>deep matching</term>
					<term>image matching</term>
					<term>optical imagery</term>
					<term>synthetic aperture radar (SAR)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this letter, we propose a pseudo-siamese convolutional neural network architecture that enables to solve the task of identifying corresponding patches in very highresolution optical and synthetic aperture radar (SAR) remote sensing imagery. Using eight convolutional layers each in two parallel network streams, a fully connected layer for the fusion of the features learned in each stream, and a loss function based on binary cross entropy, we achieve a one-hot indication if two patches correspond or not. The network is trained and tested on an automatically generated data set that is based on a deterministic alignment of SAR and optical imagery via previously reconstructed and subsequently coregistered 3-D point clouds. The satellite images, from which the patches comprising our data set are extracted, show a complex urban scene containing many elevated objects (i.e., buildings), thus providing one of the most difficult experimental environments. The achieved results show that the network is able to predict corresponding patches with high accuracy, thus indicating great potential for further development toward a generalized multisensor key-point matching procedure.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HE identification of corresponding image patches is used extensively in computer vision and remote sensing-related image analysis, especially in the framework of stereoapplications or coregistration issues. While many successful handcrafted approaches, specifically designed for the matching of optical images, exist <ref type="bibr" target="#b0">[1]</ref>, to this date, the matching of images acquired by different sensors still remains a widely unsolved challenge <ref type="bibr" target="#b1">[2]</ref>. This particularly holds for a joint exploitation of synthetic aperture radar (SAR) and optical imagery caused by two completely different sensing modalities: SAR imagery collects information about the physical properties of the scene and follows a range-based imaging geometry, while optical imagery reflects the chemical characteristics of the scene and follows a perspective imaging geometry. Hence, structures elevated above the ground level, such as buildings or trees, show strongly different appearances in both SAR and optical images (see Fig. <ref type="figure" target="#fig_0">1</ref>), in particular when dealing with very highresolution (VHR) data.</p><p>In order to deal with the problem of multisensor keypoint matching, several sophisticated approaches have been proposed, e.g., exploiting phase congruency as a generalization of gradient information <ref type="bibr" target="#b2">[3]</ref>. However, even sophisticated handcrafted descriptors reach their limitations for highly resolving data showing densely built-up urban scenes, which-in the SAR case-is often difficult to interpret even for trained experts.</p><p>Therefore, this letter aims at learning a multisensor correspondence predictor for SAR and optical image patches of the state-of-the-art VHR data. Inspired by promising results achieved in the context of stereomatching for optical imagery <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, we also make use of a convolutional neural network (CNN). The major difference of this letter to these purely optical approaches is that we focus on the aforementioned, distinctly more complicated multisensor setup and, therefore, design a specific pseudo-siamese network architecture with two separate, yet identical convolutional streams for processing SAR and optical patches in parallel instead of a weight-shared siamese network in order to deal with the heterogeneous nature of the input imagery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. NETWORK ARCHITECTURE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Pseudo-Siamese Convolutional Network</head><p>Since SAR and optical images lie on different manifolds, it is not advisable to compare them directly by descriptors designed for matching optical patches. Siamese CNN architectures are not suitable for this task either, as weights are shared between the parallel streams, thus implying the inputs share similar image features. In order to cope with the strongly different geometric and radiometric appearances of SAR and optical imagery, in <ref type="bibr" target="#b5">[6]</ref>, we proposed a pseudosiamese network architecture with two separate, yet identical convolutional streams, which process the SAR patch and the optical patch in parallel and only fuse the resulting information at a later decision stage. Using this architecture, the network is constrained to first learn meaningful representations of the input SAR patch and the optical patch separately and to combine them on a higher level. The work presented in this letter is an extension of <ref type="bibr" target="#b5">[6]</ref> by improving the fusion part of the network architecture, using a different training strategy, and resorting to nonlocally prefiltered SAR patches instead of temporal mean maps. In addition, we evaluate the network on a deterministically partitioned data set instead of a randomly partitioned one, as random partitioning will always cause positively biased results due to overlapping regions in patches.</p><p>The architecture of the proposed network is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. It is mainly inspired by the philosophy of the well-known VGG Nets <ref type="bibr" target="#b6">[7]</ref>. The SAR and optical image patches are passed through a stack of convolutional layers, where we make use of convolutional filters with a very small receptive field of 3 Ã— 3 rather than using larger ones, such as 5Ã—5 or 7Ã—7. The reason is that 3Ã—3 convolutional filters are the smallest kernels to capture patterns in different directions, such as center, up/down, and left/right, but still have an advantage: the use of small convolutional filters will increase the nonlinearities inside the network and thus make the network more discriminative.</p><p>The convolution stride in our network is fixed to 1 pixel; the spatial padding of convolutional layer input is such that the spatial resolution is preserved after convolution, i.e., the padding is 1 pixel for the all 3 Ã— 3 convolutional layers in our network. Spatial pooling is achieved by carrying out seven max-pooling layers, which follow some of the convolutional layers. They are used to reduce the dimensionality of the feature maps. Max pooling is performed over 2Ã—2 pixel windows with stride 2.</p><p>The fusion stage of our proposed network is made up of two consecutive convolutional layers, followed by two fully connected layers. The convolutional layers consist of 3 Ã— 3 filters, which operate over the concatenated feature maps of the SAR and optical streams, in order to learn a fusion rule which minimizes the final loss function. Max pooling is omitted after the first convolutional layer in the fusion stage, and a stride of 2 is used in order to downsample the feature maps while preserving the spatial information <ref type="bibr" target="#b7">[8]</ref>. The use of 3 Ã— 3 filters and the absence of max pooling after the first convolution allow the fusion layer to learn a fusion rule, which is somewhat invariant to spatial mismatches caused by the difference in imaging modalities. This is due to the fact that the fusion layer uses 3 Ã— 3 convolutions to learn relationships between the features while preserving nearby spatial information. The lack of max pooling means that these learned spatial relationships are preserved as not only the maximal response is considered, while the stride of 2 is used to reduce the feature size. The final stage of the fusion network consists of two fully connected layers: the first of which contains 512 channels; while the second, which performs one-hot binary classification, contains 2 channels.</p><p>In a nutshell, the convolutional layers in our network apart from the fusion layer generally consist of 3 Ã— 3 filters and follow two rules: 1) the layers with the same feature map size have the same number of filters and 2) the number of feature maps increases in the deeper layers, roughly doubling after each max-pooling layer (except for the last convolutional stack in each stream). All layers in the network are equipped with a rectified linear unit as an activation function, except the last fully connected layer, which is activated by a softmax function. Fig. <ref type="figure" target="#fig_1">2</ref> shows the schematic of the configuration of our network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Loss Function</head><p>We make use of the binary cross-entropy loss for training our network. Let X = (x sar 1 , x opt 1 ), (x sar 2 , x opt 2 ), . . . , (x sar n , x opt n ) be a set of SAR-optical patch pairs, where x sar i , x opt i âˆˆ R DÃ—D , âˆ€ i = 1, . . . , n and y i is the one-hot label for the pair (x sar i , x opt i ) (with [1, 0] denoting a dissimilar pair, and [0, 1] denoting a similar pair). We then seek to minimize the binary cross-entropy loss</p><formula xml:id="formula_0">E = 1 n n i=1 y i â€¢ log f x sar i , x opt i , Î¸<label>(1)</label></formula><p>where f (x sar i , x opt i , Î¸) denotes the output vector of the softmax function when comparing the input pair (x sar i , x opt i ) with the current network parameters Î¸ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Configuration Details</head><p>Fig. <ref type="figure" target="#fig_1">2</ref> shows the full configuration of our network. Apart from the previously discussed architecture, we also make use of batch normalization after the activation function of each convolutional layer. This leads to an increase in the training speed and reduces the effects of internal covariate shift. In order to reduce overfitting during training, we made use of L 2 -regularization, with Î» = 0.001, for the convolution kernels of the SAR and optical streams, and dropout with a rate of 0.7 for the first fully connected layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. AUTOMATIC PATCH POOL GENERATION</head><p>For training and testing purposes, a large pool of corresponding and noncorresponding SAR and optical image patches is needed. While the classical work on deep matching for optical imagery can usually rely on easy-to-achieve optical patch pools (see, for example, the Phototourism Patch data set <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref>), annotating corresponding patches in VHR optical and SAR imagery of complex urban scenes is a highly nontrivial task even for experienced human experts. Thus, one of the contributions of this letter is the introduction of a fully automatic procedure for SAR-optical patch pool generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. "SARptical" Framework</head><p>In order to solve the challenge of automatic data set generation, we resort to the so-called "SARptical" framework of Wang et al. <ref type="bibr" target="#b9">[10]</ref>, an object-space-based matching procedure developed for mapping textures from optical images onto 3-D point clouds derived from SAR tomography. The core of this algorithm is to match the SAR and optical images in 3-D space in order to deal with the inevitable differences caused by different geometrical distortions. Usually, this would require an accurate digital surface model of the area to link homologue image parts via a known object space. In contrast, the approach in <ref type="bibr" target="#b9">[10]</ref> creates two separate 3-D point clouds, one from SAR tomography and one from optical stereo matching, which are then registered in 3-D space to form a joint ("SARptical") point cloud, which serves as the necessary representation of the object space. The flowchart of the approach can be seen in Fig. <ref type="figure" target="#fig_2">3</ref>. In order to estimate the 3-D positions of the individual pixels in the images, the algorithm requires an interferometric stack of SAR images as well as at least a pair of optical stereoimages. The matching of the two point clouds in 3-D guarantees the matching of the SAR and the optical images. Finally, we can project the SAR image into the geometry of the optical image via the "SARptical" point cloud and vice versa. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data Preparation</head><p>For the work presented in this letter, we made use of a stack of 109 TerraSAR-X high-resolution spotlight images of the city of Berlin, acquired between 2009 and 2013 with about 1-m resolution, and of nine UltraCAM optical images of the same area with 20-cm ground spacing. After the reconstruction of the "SARptical" 3-D point cloud, 8840 pixels with high SNR (&gt;5 dB) were uniformly sampled from the nonlocally filtered master SAR amplitude image and projected into the individual optical images, yielding a total of 10 108 corresponding optical pixels. The reason for the difference in pixel numbers is that each of the nine optical multiview stereoimages is acquired from a different viewing angle, making it possible for each SAR image pixel to have up to nine corresponding optical image pixels. The actual number of corresponding optical pixels is dependent on the visibility of the SAR pixel from the respective optical point of view.</p><p>All SAR patches are centered at their corresponding SAR image pixels. Their size is fixed at 112 Ã— 112 pixels with a pixel spacing of about 1 m. In analogy, the optical patches are centered at the corresponding optical pixels. After resampling to adjust the pixel spacing, the SAR patches were rotated, so that both patches align with each other as a first approximation.</p><p>In order to reduce bias when training our network, we randomly selected a single correct optical correspondence for each SAR image patch during the final data set preparation. In addition, we randomly assign one wrong optical correspondence to each SAR patch in order to create negative examples. Thus, eventually, we end up with 17 680 SAR-optical patch pairs (see Fig. <ref type="figure" target="#fig_0">1</ref> for an example of the class of correct matches).</p><p>As final preprocessing steps, the optical patches were converted to gray scale, and all patches were normalized <ref type="bibr" target="#b10">[11]</ref> to a radiometric range of [0; 1] with subsequent subtraction of their means.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Patch Pool Partitioning</head><p>In order to provide a fair experimental design, we partition the patch pool in the following manner: 9724 (55%) of the patch pairs are used as training data set, 2652 (15%) as validation set, and 5304 (30%) as test data set. It has to be noted that we do not partition the patch pool on a purely randomized basis but rather resort to a deterministic partitioning method in order to avoid positively biased test results. The full extent SAR and optical images are first deterministically partitioned and then each partition is processed to generate positive and negative samples for training, validation, and testing, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training Details</head><p>The network was trained using the Adam <ref type="bibr" target="#b11">[12]</ref> optimization algorithm as it is computationally efficient and exhibits faster convergence than standard stochastic gradient descent methods. The optimization hyperparameters are fixed to Î² 1 = 0.9 and Î² 2 = 0.999 with a learning rate of Î± t = 0.0009. The learning rate was found via a grid search method on our training and validation data, while the Î²-parameters were kept at their recommended values. Prior to training the network, weight vectors were initialized using the truncated uniform distribution described in <ref type="bibr" target="#b12">[13]</ref>, and the bias vectors were initialized with zero values. Training was conducted with 2 Nvidia TitanX GPUs using class balanced, minibatches of 64 SAR-optical patch pairs (32 corresponding and 32 noncorresponding pairs) over 30 epochs; training took on average 25 min with a single forward pass taking around 3 ms to complete.</p><p>We trained five versions of our proposed network, each at a different patch size, in order to evaluate the effect of patch size on classification accuracy. Patch cropping was done on-the-fly with the new patch being cropped from the center of a larger patch-this was done as the center pixel is the point of correspondence between the SAR and optical patch. Furthermore, we seeded our random number generator with a fixed value of 0, at the start of training for each patch size, in order to prevent the randomization effects between networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Results</head><p>We evaluate the proposed network with different input patch sizes using our testing patch pool (described in Section III), which has further been cropped around the center pixel to produce new testing pools with different patch sizes.</p><p>The accuracy versus false positive rate curves corresponding to different patch sizes can be seen in Fig. <ref type="figure" target="#fig_3">4</ref>. Table <ref type="table" target="#tab_0">I</ref> reports  the corresponding confusion matrix values for our proposed network evaluated with each patch size; it is to be noted that the confusion matrix is the reflective of the network at the point of highest overall performance for each patch size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Key-Point Matching Results</head><p>In order to evaluate the proposed network's performance in a real-world, key-point matching scenario, we selected 100 neighboring TomoSAR key-points in the SAR image and extracted the corresponding SAR and optical patch pairs. We selected these key points from a localized area within our test set so as to reproduce the conditions found in a real-world key-point matching application. We then compared every SAR and optical patch in the selected patch set in order to determine the performance of our proposed network in the presence of large numbers of potential mismatches.</p><p>In Fig. <ref type="figure" target="#fig_4">5</ref>(a), we can see a matrix depicting the similarity scores of the various pair comparisons, where corresponding SAR and optical patches are given the same index number. It should be noted that in determining a binary value for correspondence, a threshold is applied to these similarity scores. Fig. <ref type="figure" target="#fig_4">5(b)</ref> shows the sorted scores for all nonsimilar optical patches, making it easier to see the number and strength of incorrect matches in the patch pool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION</head><p>Generally, the results summarized in Section IV-B indicate a promising discriminative power of the proposed network. However, the following major points must be considered when interpreting the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Influence of the Patch Size</head><p>As Table <ref type="table" target="#tab_0">I</ref> and Fig. <ref type="figure" target="#fig_3">4</ref> clearly indicate, the patch size strongly affects the discriminative power of the network. This result is likely due to the effects of distortions in SAR images, which are acquired in a range-based imaging geometry. Thus when patches are cropped to smaller regions, we likely crop out defining features, which are used for matching between the SAR and optical domain. This can be intuitively understood by referring to Fig. <ref type="figure" target="#fig_0">1</ref>, where we can see the effects of layover and multipath reflections of the building in the SAR image and a near top down view of the same building in the optical image. Taking away explanatory context will thus render the matching more difficult. All further discussion will be with reference to the results we obtained using the largest patch size, 112 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comments on the Discriminative Power of the Proposed Network</head><p>In summary, our approach obtains an accuracy exceeding 77% on a separate test data set when fixing the false positive rate to 5%, which falls into the same order of magnitude as what can be achieved using the powerful handcrafted HOPC descriptor in combination with an L 2 -norm cost function <ref type="bibr" target="#b2">[3]</ref>.</p><p>Furthermore, our approach produced a clear diagonal pattern in Fig. <ref type="figure" target="#fig_4">5</ref>(a), which depicts its ability to accurately determine the correct correspondence in a key-point matching scenario. Upon further investigation, it was found that the network achieved 43% top-1 matching accuracy and 74% top-3 accuracy, while 8% of points had no valid matches detected within the key-point set. This was found to be due to large amounts of layover and extreme differences in view point between the SAR and optical patches (see false negatives in Fig. <ref type="figure" target="#fig_5">6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Possible Reasons for False Predictions</head><p>From the randomly chosen prediction examples shown in Fig. <ref type="figure" target="#fig_5">6</ref>, it can be observed that many of the false positives and false negatives are erroneously matched under extreme differences in viewing angle between the SAR and optical patches. While this may partially be solvable by resorting to larger patch sizes, providing valuable context, there might be a need to exclude image parts with all too strong distortions from further processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this letter, a pseudo-siamese CNN for learning to identify corresponding patches in VHR SAR and optical images in a fully automatic manner has been presented. A first evaluation has shown promising potential with respect to multisensor key-point matching procedures. In order to ensure transferability to other applications not based on key points, e.g., dense matching, we will work on the generation of additional training patches, whose center pixel does not rely on specific key points. In addition, we will test the approach on data coming from a completely different source. In the end, we expect this letter to help paving the way for generalized SAR-optical image matching procedures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Simple detached multistory building as (Left) SAR amplitude image and (Right) optical photograph.</figDesc><graphic coords="1,334.55,167.33,205.70,102.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Proposed pseudo-siamese network architecture and layer configuration.</figDesc><graphic coords="2,62.87,57.65,215.06,167.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Flowchart of the patch-pool generation procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Comparison of different patch sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Results of key-point matching experiment. (a) Confusion matrix showing the matching scores for all SAR and optical key-point patches. (b) Spread of incorrect matches ordered by the similarity score.</figDesc><graphic coords="4,311.99,137.45,113.18,111.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Exemplary patch correspondence results.</figDesc><graphic coords="5,186.47,59.33,113.18,112.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I CONFUSION</head><label>I</label><figDesc>MATRIX VALUES FOR DIFFERENT PATCH SIZES</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank the NVIDIA Corporation for donating the Titan X Pascal GPU used in this letter.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the China Scholarship Council, in part by the European Research Council under the EU Horizon 2020 Research and Innovation Program under Grant ERC-2016-StG-714087(So2Sat), in part by the Helmholtz Association under the framework of the Young Investigators Group SiPEO under Grant VH-NG-1018, and in part by the German Research Foundation under Grant SCHM 3322/1-1.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A performance evaluation of local descriptors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1615" to="1630" />
			<date type="published" when="2005-10">Oct. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Data fusion and remote sensing: An evergrowing relationship</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Mag</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="6" to="23" />
			<date type="published" when="2016-12">Dec. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust registration of multimodal remote sensing images based on structural similarity</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2941" to="2958" />
			<date type="published" when="2017-03">Mar. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">MatchNet: Unifying feature and metric learning for patch-based matching</title>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="page" from="3279" to="3286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to compare image patches via convolutional neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4353" to="4361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A CNN for the identification of corresponding patches in SAR and optical imagery of urban scenes</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. JURSE</title>
		<meeting>JURSE<address><addrLine>Dubai, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-03">Mar. 2017</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1409.1556" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">gvnn: Neural network library for geometric computer vision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bloesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>PÈƒtrÈƒucean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mccormac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV Workshops</title>
		<meeting>ECCV Workshops<address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="67" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning local image descriptors</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A J</forename><surname>Winder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06">Jun. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fusing meter-resolution 4-D InSAR point clouds and optical images for semantic urban infrastructure monitoring</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zeisl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="26" />
			<date type="published" when="2017-01">Jan. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image registration by normalized mapping</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="181" to="189" />
			<date type="published" when="2013-02">Feb. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1412.6980" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AISTATS</title>
		<meeting>AISTATS<address><addrLine>Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
