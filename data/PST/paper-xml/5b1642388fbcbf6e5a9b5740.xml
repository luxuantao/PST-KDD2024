<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Structured Representation for Text Classification via Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tianyang</forename><surname>Zhang</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
							<email>aihuang@tsinghua.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Zhao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Tsinghua National Laboratory for Information Science and Technology Dept. of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Structured Representation for Text Classification via Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Representation learning is a fundamental problem in natural language processing. This paper studies how to learn a structured representation for text classification. Unlike most existing representation models that either use no structure or rely on pre-specified structures, we propose a reinforcement learning (RL) method to learn sentence representation by discovering optimized structures automatically. We demonstrate two attempts to build structured representation: Information Distilled LSTM (ID-LSTM) and Hierarchically Structured LSTM (HS-LSTM). ID-LSTM selects only important, task-relevant words, and HS-LSTM discovers phrase structures in a sentence. Structure discovery in the two representation models is formulated as a sequential decision problem: current decision of structure discovery affects following decisions, which can be addressed by policy gradient RL. Results show that our method can learn task-friendly representations by identifying important words or task-relevant structures without explicit structure annotations, and thus yields competitive performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Representation learning is a fundamental problem in AI, and particularly important for natural language processing (NLP) <ref type="bibr" target="#b0">(Bengio, Courville, and Vincent 2013;</ref><ref type="bibr" target="#b12">Le and Mikolov 2014)</ref>. As one of the most common tasks of NLP, text classification depends heavily on the learned representation, and is widely applied in sentiment analysis <ref type="bibr" target="#b20">(Socher et al. 2013)</ref>, question classification <ref type="bibr" target="#b9">(Kim 2014)</ref>, and language inference <ref type="bibr">(Bowman et al. 2015)</ref>.</p><p>Mainstream representation models for text classification can be roughly classified into four types. Bag-of-words representation models ignore the order of words, including deep average network <ref type="bibr" target="#b7">(Iyyer et al. 2015;</ref><ref type="bibr" target="#b8">Joulin et al. 2017</ref>) and autoencoders <ref type="bibr" target="#b15">(Liu et al. 2015)</ref>. Sequence representation models such as convolutional neural network <ref type="bibr" target="#b9">(Kim 2014;</ref><ref type="bibr" target="#b8">Kalchbrenner, Grefenstette, and Blunsom 2014;</ref><ref type="bibr" target="#b13">Lei, Barzilay, and Jaakkola 2015)</ref> and recurrent neural network <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber 1997;</ref><ref type="bibr" target="#b4">Chung et al. 2014)</ref> consider word order but do not use any structure. Structured representation models such as tree-structured LSTM <ref type="bibr" target="#b29">(Zhu, Sobihani, and Guo 2015;</ref><ref type="bibr" target="#b22">Tai, Socher, and Manning 2015)</ref> and recursive autoencoders <ref type="bibr" target="#b20">(Socher et al. 2013;</ref><ref type="bibr" target="#b19">2011;</ref><ref type="bibr" target="#b17">Qian et al. 2015)</ref> use pre-specified parsing trees to build structured representations. Attention-based methods <ref type="bibr" target="#b25">(Yang et al. 2016;</ref><ref type="bibr" target="#b27">Zhou, Wan, and Xiao 2016;</ref><ref type="bibr" target="#b14">Lin et al. 2017</ref>) use attention mechanisms to build representations by scoring input words or sentences differentially.</p><p>However, in existing structured representation models, the structures are either provided as input or predicted using supervision from explicit treebank annotations. There has been few studies on learning representations with automatically optimized structures. <ref type="bibr" target="#b26">Yogatama et al. (2017)</ref> proposed to compose binary tree structure for sentence representation with only supervision from downstream tasks, but such structure is very complex and overly deep, leading to unsatisfactory classification performance. In <ref type="bibr" target="#b3">(Chung, Ahn, and Bengio 2017)</ref>, a hierarchical representation model was proposed to capture latent structure in the sequences with latent variables. Structure is discovered in a latent, implicit manner.</p><p>In this paper, we propose a reinforcement learning (RL) method to build structured sentence representations by identifying task-relevant structures without explicit structure annotations. Structure discovery in this paper is formulated as a sequential decision problem: current decision (or action) of structure discovery affects following decisions, which can be naturally addressed by policy gradient method <ref type="bibr" target="#b21">(Sutton et al. 2000)</ref>. A delayed reward is used to guide the learning of the policy for structure discovery. The reward is computed from the text classifier's prediction based on the structured representation. The representation is available only when all sequential decisions are completed.</p><p>In our RL method, we design two structured representation models: Information Distilled LSTM (ID-LSTM) which selects important, task-relevant words to build sentence representation, and Hierarchical Structured LSTM (HS-LSTM) which discovers phrase structures and builds sentence representation with a two-level LSTM. The representation models are integrated seamlessly with a policy network and a classification network. The policy network defines a policy for structure discovery, and the classification network makes prediction on top of structured sentence representation and facilitates reward computation for the policy network.</p><p>To summarize, our contributions are as follows:</p><p>• We propose a reinforcement learning method which dis- • Even without explicit structure annotations, our method can identify task-relevant structures effectively. Moreover, the performance is better or comparable to strong baselines that use pre-specified parsing structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology Overview</head><p>The goal of this paper is to learn structured representation for text classification by discovering important, task-relevant structures. We argue that text classification can be improved with an optimized, structured representation. The overall process is shown in Figure <ref type="figure" target="#fig_0">1</ref>. The model consists of three components: Policy Network (PNet), structured representation models, and Classification Network (CNet). PNet adopts a stochastic policy and samples an action at each state. It keeps sampling until the end of a sentence, and produces an action sequence for the sentence. Then the structured representation models translate the actions into a structured representation. We design two representation models, information distilled LSTM (ID-LSTM) and hierarchically structured LSTM (HS-LSTM). CNet makes classification based on the structured representation and offers reward computation to PNet. Since the reward can be computed once the final representation is available (completely determined by the action sequence), the process can be naturally addressed by policy gradient method <ref type="bibr" target="#b21">(Sutton et al. 2000)</ref>.</p><p>Obviously the three components are interleaved together. The state representation of PNet is derived from the representation models, CNet relies on the final structured representation obtained from the representation model to make prediction, and PNet obtains rewards from CNet's prediction to guide the learning of a policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Policy Network (PNet)</head><p>The policy network adopts a stochastic policy π(a t |s t ; Θ) and uses a delayed reward to guide the policy learning. It samples an action with the probability at each state whose representation is obtained from the representation models. In order to obtain the delayed reward which is based on C-Net's prediction, we perform action sampling for the entire sentence. Once all the actions are decided, the representation models will obtain a structured representation of the sentence, and it will be used by CNet to compute P (y|X). The reward computed with P (y|X) is used for policy learning.</p><p>We briefly introduce state, action and policy, reward, and objective function as follows:</p><p>State State encodes the current input and previous contexts, and has different definitions in the two representation models. The detailed definition of state s t will be introduced in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Action and Policy</head><p>We adopt binary actions in two settings, but with different meanings. In ID-LSTM, the action space is {Retain, Delete}, where a word can be deleted from or retained in the final sentence representation. In HS-LSTM, the action space is {Inside, End}, indicating that a word is inside or at the end of a phrase<ref type="foot" target="#foot_0">1</ref> . Clearly, each action is a direct indicator of structure selection in both representation models.</p><p>We adopt a stochastic policy. Let a t denote the action at state t, the policy is defined as follows:</p><formula xml:id="formula_0">π(a t |s t ; Θ) = σ(W * s t + b),<label>(1)</label></formula><p>where π(a t |s t ; Θ) denotes the probability of choosing a t , σ denotes the sigmoid function and Θ = {W, b} denotes the parameters of PNet.</p><p>During training, the action is sampled according to the probability in Eq. 1. During test, the action with the maximal probability (i.e., a * t = argmax a π(a|s t ; Θ)) will be chosen in order to obtain superior prediction.</p><p>Reward Once all the actions are sampled by the policy network, the structured representation of a sentence is determined by our representation models, and the representation will be passed to CNet to obtain P (y|X) where y is the class label. The reward will be calculated from the predicted distribution (P (y|X)), and also has a factor considering the tendency of structure selection, which will be detailed later. This is a typical delayed reward since we cannot obtain it until the final representation is built.</p><p>Objective Function We optimize the parameters of PNet using REINFORCE algorithm <ref type="bibr" target="#b24">(Williams 1992</ref>) and policy gradient methods <ref type="bibr" target="#b21">(Sutton et al. 2000)</ref>, aiming to maximize the expected reward as shown below.</p><formula xml:id="formula_1">J(Θ) = E (st,at)∼PΘ(st,at) r(s 1 a 1 • • • s L a L ) = s1a1•••s L a L P Θ (s 1 a 1 • • • s L a L )R L = s1a1•••s L a L p(s 1 ) t π Θ (a t |s t )p(s t+1 |s t , a t )R L = s1a1•••s L a L t π Θ (a t |s t )R L .</formula><p>Note that this reward is computed over just one sample, say</p><formula xml:id="formula_2">X = x 1 x 2 • • • x L .</formula><p>Since our state at step t + 1 is fully determined by the state and action at step t, the probability p(s 1 ) and p(s t+1 |s t , a t ) are equal to 1.</p><p>By applying the likelihood ratio trick, we update the policy network with the following gradient:</p><formula xml:id="formula_3">∇ Θ J(Θ) = L t=1 R L ∇ Θ log π Θ (a t |s t ).</formula><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structured Representation Models</head><p>Information Distilled LSTM (ID-LSTM) The main idea of Information Distilled LSTM (ID-LSTM) is to build a sentence representation by distilling the most important words and removing irrelevant words in a sentence. In this way, it is expected to learn more task-relevant representations for classification. For instance, in sentiment classification, words like 'to', 'the' and 'a' may rarely contribute to the task. By distilling the most important words in a sentence, the final representation can be purified and condensed for classification. ID-LSTM translates the actions obtained from PNet to a structured representation of a sentence. Formally, given a sentence</p><formula xml:id="formula_4">X = x 1 x 2 • • • x L , there is a corresponding ac- tion sequence A = a 1 a 2 • • • a L obtained from PNet.</formula><p>In this setting, each action a i at word position x i is chosen from {Retain, Delete} where Retain indicates that the word is retained in a sentence, and Delete means that the word is deleted and it has no contribution to the final sentence representation. Formally,</p><formula xml:id="formula_5">c t , h t = c t−1 , h t−1 , a t = Delete Φ(c t−1 , h t−1 , x t ), a t = Retain (3)</formula><p>where Φ denotes the functions (including all gate functions and the update function) of a sequence LSTM, c t is the memory cell, and h t is the hidden state at position t. Note that if a word is deleted, the memory cell and hidden state of the current position are copied from the preceding position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>State:</head><p>The state for the policy network is defined as follows:</p><formula xml:id="formula_6">s t = c t−1 ⊕ h t−1 ⊕ x t ,<label>(4)</label></formula><p>where ⊕ indicates vector concatenation and x t is the current word input. To enrich the state representation, the memory state (c t−1 ) is included.</p><p>To make classification, the last hidden state of ID-LSTM is taken as input to the classification network (CNet):</p><formula xml:id="formula_7">P (y|X) = sof tmax(W s h L + b s ),<label>(5)</label></formula><p>where</p><formula xml:id="formula_8">W s ∈ R d×K , b s ∈ R K are parameters of CNet, d is the dimension of hidden state, y ∈ {c 1 , c 2 , • • • , c K }</formula><p>is the class label and K is the number of categories.</p><p>Reward: In order to compute the delayed reward R L , we use the logarithm of the output probability of CNet, i.e., P (y = c g |X) where c g is the gold label of the input X.</p><p>In addition, to encourage the model to delete more useless words, we include an additional term by computing the proportion of the number of deleted words to the sentence length:</p><formula xml:id="formula_9">R L = log P (c g |X) + γL /L,<label>(6)</label></formula><p>where L denotes the number of deleted words (where the corresponding action a t is Delete). γ is a hyper-parameter to balance the two terms.</p><p>Hierarchically Structured LSTM (HS-LSTM) Hierarchical models have been widely used in document-level classification <ref type="bibr" target="#b23">(Tang, Qin, and Liu 2015;</ref><ref type="bibr" target="#b5">Ghosh et al. 2016)</ref> and language modeling <ref type="bibr" target="#b3">(Chung, Ahn, and Bengio 2017)</ref>. Inspired by these studies, we propose a Hierarchically Structured LSTM (HS-LSTM) that can build a structured representation by discovering hierarchical structures in a sentence. We argue that a better sentence representation can be obtained by identifying sub-structures in a sentence. This process is implemented by sampling an action in {Inside, End} at each word position, where Inside indicates that a word is inside of a phrase and End means the end of a phrase. HS-LSTM translates the actions to a hierarchical structured representation of the sentence. To be precise, the word phrase in this paper, should be interpreted as substructure or segment.</p><p>In HS-LSTM, there is a two-level structure: a word-level LSTM which connects a sequence of words to form a phrase, and a phrase-level LSTM which connects the phrases to form the sentence representation. The transition of the wordlevel LSTM depends upon action a t−1 . If action a t−1 is End, the word at position t is the start of a phrase and the wordlevel LSTM starts with a zero-initialized state. Otherwise the action is Inside and the word-level LSTM continues from its previous state. The process is described formally as follows:</p><formula xml:id="formula_10">c w t , h w t = Φ w (0, 0, x t ), a t−1 = End Φ w (c w t−1 , h w t−1 , x t ), a t−1 = Inside (7)</formula><p>where Φ w denotes the transition functions of the word-level LSTM, c t is the memory cell, and h t is the hidden state at position t.</p><p>The transition of the phrase-level LSTM depends on action a t at the current position, which indicates whether a phrase is completely constructed or not (see Eq. 8). When action a t is End, a phrase ends at position t and the hidden state of the word-level LSTM will be fed into the phraselevel LSTM. Otherwise the action is Inside and the phraselevel LSTM is fixed at this step, and the variables are copied   from the preceding position. Formally,</p><formula xml:id="formula_11">c p t , h p t = Φ p (c p t−1 , h p t−1 , h w t ), a t = End c p t−1 , h p t−1 , a t = Inside (8)</formula><p>where Φ p denotes the transitions function of the phrase-level LSTM. Note that the input to the phrase-level LSTM is h w t , the hidden state of the word-level LSTM.</p><p>The behavior of HS-LSTM relies on both action a t−1 and a t , as summarized in Table <ref type="table">1</ref>. As can be seen clearly, the combination of action a t−1 and a t indicates the position of word x t in a phrase. State: The state for the policy network is defined as follows:</p><formula xml:id="formula_12">s t = c p t−1 ⊕ h p t−1 ⊕ c w t ⊕ h w t ,<label>(9)</label></formula><p>where ⊕ indicates vector concatenation. The state representation consists of both word-level and phrase-level representations.</p><p>To make classification, the last hidden state of the phraselevel LSTM (h p L ) is taken as input to the classification network (CNet):</p><formula xml:id="formula_13">P (y|X) = sof tmax(W s h p L + b s ). (<label>10</label></formula><formula xml:id="formula_14">)</formula><p>Reward: Similar to ID-LSTM, the reward R L is based on CNet's prediction and a term indicating the tendency of structure selection. Unlike ID-LSTM's reward which encourages the model to remove as many words as possible, this reward respects that a good phrase structure should contain neither too many nor too few phrases. We thus employ a unimodal function of the number of phrases to reflect the tendency of structure selection. We use the function f (x) = x + 0.1/x, which is a unimodal function with minimum at 1/ √ 10 = 0.316. Formally, we have</p><formula xml:id="formula_15">R L = log P (c g |X) − γ(L /L + 0.1L/L ),<label>(11)</label></formula><p>where L denotes the number of phrases (the number of action End). γ is a hyper-parameter. The second term encourages the number of phrases to be 0.316L, where there are about 3∼4 phrases for L = 10, which is in line with our observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification Network (CNet)</head><p>The classification network produces a probability distribution over class labels based on the structured representation obtained from ID-LSTM or HS-LSTM. CNet is parameterized by W s and b s as shown in Eq. 5 and Eq. 10 respectively, and will not be repeated here.</p><p>In order to train CNet, we adopt cross entropy as loss function:</p><formula xml:id="formula_16">L = X∈D − K y=1 p(y, X) log P (y|X),<label>(12)</label></formula><p>where p(y, X) is the gold one-hot distribution of sample X, and P (y|X) is the predicted distribution as defined in Eq. 5 and Eq. 10 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Details</head><p>Since PNet, the representation model and CNet are interleaved together, they should be trained jointly. As described in Algorithm. 1, the entire training process consists of three steps. We first pre-train the representation model and CNet, and then pre-train PNet while keeping the parameters of the Algorithm 1: The Training Process 1 Pre-train the representation model (ID-LSTM or HS-LSTM) and CNet with predefined structures by minimizing Eq. 12; 2 Fix the parameters of the strutured representation model and CNet, and Pre-train PNet by Eq. 2; 3 Train all the three components jointly until convergence;</p><p>other two models fixed. At last, we jointly train all the three components.</p><p>Since training RL from scratch would be extremely difficult and has high variance, we pretrain the RL module with some warm-start structures. For ID-LSTM, we use the original sentence without any deletion to perform pre-training. For HS-LSTM, we split a sentence into phrases shorter than the square root of sentence length, and also use some very simple heuristics. Note that the structures used for pretraining are quite different from the parsing structures used in previous work because parsing structures heavily rely on parsing tools and are error-prone, and thus more expensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Experimental Setting and Training Details</head><p>The dimension of hidden state in the representation models is 300. The word vectors are initialized using 300dimensional Glove vectors <ref type="bibr" target="#b16">(Pennington, Socher, and Manning 2014)</ref> and are updated together with other parameters. To smooth the update of policy gradient, a suppression factor is multiplied to Eq.2 and is set to 0.1. γ is set to 0.05 × K in the reward of ID-LSTM (Eq. 6) and 0.1 × K in the reward of HS-LSTM (Eq. 11), where K is the number of categories.</p><p>During the training process, Adam algorithm <ref type="bibr" target="#b10">(Kingma and Ba 2015)</ref> is used to optimize the parameters and the learning rate is 0.0005. We adopted Dropout before the classification layer in CNet, with a probability of 0.5. Mini-batch size is 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets and Baselines</head><p>Datasets We evaluated our models on various datasets for sentiment classification, subjectivity analysis, and topic classification.</p><p>• MR: This dataset contains positive/negative reviews (Pang and Lee 2005).</p><p>• SST: Stanford Sentiment Treebank, a public sentiment analysis dataset with five classes <ref type="bibr" target="#b20">(Socher et al. 2013</ref>).<ref type="foot" target="#foot_1">2</ref> </p><p>• Subj: Subjectivity dataset. The task is to classify a sentence as subjective or objective (Pang and Lee 2004).</p><p>• AG: AG's news corpus<ref type="foot" target="#foot_2">3</ref> , a large topic classification dataset constructed by <ref type="bibr" target="#b27">(Zhang, Zhao, and LeCun 2015)</ref>.</p><p>The topic includes World, Sports, Business and Sci/Tech.</p><p>Baselines We chose three types of baselines: basic neural models using no particular structure, models relying on prespecified parsing structure, and models distilling important information by attention mechanism.</p><p>• LSTM: A sequence LSTM. The version we used is proposed in <ref type="bibr" target="#b22">(Tai, Socher, and Manning 2015)</ref>.</p><p>• biLSTM: A bi-directional LSTM, commonly used in text classification.</p><p>• CNN: Convolutional Neural Network <ref type="bibr" target="#b9">(Kim 2014</ref>).</p><p>• RAE: Recursive autoencoder which is defined on predefined parsing structure <ref type="bibr" target="#b19">(Socher et al. 2011</ref>).</p><p>• Tree-LSTM: Tree-structured Long Short-Term Memory relying on predefined parsing structure <ref type="bibr" target="#b22">(Tai, Socher, and Manning 2015)</ref>.</p><p>• Self-Attentive: Structured Self-Attentive model, a selfattention mechanism and a special regularization term are used to construct sentence embedding <ref type="bibr" target="#b14">(Lin et al. 2017)</ref>.</p><p>The dimension of hidden vectors and the word vectors used in these baselines are the same to our models. Other parameter settings are consistent with the references.  <ref type="bibr" target="#b22">(Tai, Socher, and Manning 2015)</ref>, <ref type="bibr" target="#b9">(Kim 2014)</ref>, and <ref type="bibr" target="#b6">(Huang, Qian, and Zhu 2017)</ref>. The rest are obtained by our own implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification Results</head><p>Classification results as listed in Table <ref type="table" target="#tab_1">2</ref> show that our models perform competitively across different datasets and different tasks. Our models outperform basic models using no structure (LSTM, biLSTM, and CNN) , models using parsing structures (RAE and Tree-LSTM), and attention-based models (Self-Attentive). Comparing to pre-specified parsing structures, automatically discovered structures seem to be more friendly for classification. These results demonstrate the effectiveness of learning structured representations by discovering task-relevant structures.</p><p>Origin text Cho continues her exploration of the outer limits of raunch with considerable brio . ID-LSTM Cho continues her exploration of the outer limits of raunch with considerable brio . HS-LSTM Cho continues her exploration of the outer limits of raunch with considerable brio . Origin text Much smarter and more attentive than it first sets out to be .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ID-LSTM</head><p>Much smarter and more attentive than it first sets out to be . HS-LSTM Much smarter and more attentive than it first sets out to be . Origin text Offers an interesting look at the rapidly changing face of Beijing .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ID-LSTM</head><p>Offers an interesting look at the rapidly changing face of Beijing . HS-LSTM Offers an interesting look at the rapidly changing face of Beijing .</p><p>Table <ref type="table">3</ref>: Examples of the structures distilled and discovered by ID-LSTM and HS-LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structure Analysis</head><p>To investigate the discovered structures and how they influence classification performance, we presented structure analysis from both qualitative and quantitative perspectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ID-LSTM</head><p>Qualitative analysis: ID-LSTM can effectively remove the task-irrelevant words, and is still able to offer competitive performance. As shown in Table <ref type="table">3</ref>, irrelevant words such as 'and' , 'of' and 'the' are removed by ID-LSTM. Our model can even delete consecutive subsequences in a sentence, such as 'than it first sets out to be' or 'the outer limits of'. It is interesting that the classifier can classify the text correctly even without such irrelevant words. Taking sentiment classification as an example, we observed that the retained words by ID-LSTM are mostly sentiment words and negation words, indicating that the model can distill important, task-relevant words. These examples demonstrate that a purified representation can benefit classification tasks. Thus, we argue that not all words are necessary for a particular classification task. Furthermore, we analyzed what types of words are removed and retained, taking sentiment classification as an example. The most and least deleted words by ID-LSTM in the SST dataset are listed in Table <ref type="table" target="#tab_3">5</ref>, ordered by deletion percentage (Deleted/Count). On one hand, the most deleted words are non-content words (prepositions, articles, etc.), generally irrelevant to sentiment classification. This shows that ID-LSTM is able to filter irrelevant words. On the other hand, ID-LSTM is able to retain important words for the task. For instance, as we may know, sentiment and negation words are important for sentiment classification <ref type="bibr" target="#b28">(Zhu et al. 2014;</ref><ref type="bibr" target="#b18">Qian et al. 2017)</ref>. As can be seen in Table <ref type="table" target="#tab_3">5</ref>, sentiment words such as 'good' and 'interesting' are rarely removed. ID-LSTM doesn't remove 'not' or 'no'. For transitional words, 'but' appears 320 times and only 7.81% of them are removed.</p><p>To summarize, the qualitative and quantitative results demonstrate that ID-LSTM is able to remove irrelevant words and distill task-relevant ones in a sentence. ID-LSTM is effective to extract task-specific keywords, and the purified, condensed representation is classification-friendly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HS-LSTM Qualitative analysis:</head><p>Table <ref type="table">3</ref> shows some interesting structures discovered by HS-LSTM. In the given examples, phrases such as 'much smarter', 'and more attentive' and 'an interesting book', are important task-relevant phrases. We also observed that structures discovered by HS-LSTM are more flexible in length and content than traditional phrases since HS-LSTM sometimes fails to find the correct boundary between phrases. However, as we mentioned, this is extremely difficult for any Nevertheless, HS-LSTM indeed has the ability to identify common types of phrases with clear boundary. We listed more examples of different types found by HS-LSTM in Table 7. Noun and prepositional phrases found by HS-LSTM are usually long, expressive, and task-relevant. In comparison, verb phrases are shorter than noun phrases. Besides grammatical phrases, our model also finds some interesting special phrases, as shown in Table <ref type="table" target="#tab_5">7</ref>. Quantitative analysis: First of all, we compared HS-LSTM with other structured models to investigate whether classification tasks can benefit from the discovered structure. The baselines include the models that rely on parsing structure, and Com-Tree-LSTM that learns to compose deep tree structure <ref type="bibr" target="#b26">(Yogatama et al. 2017)</ref>. We also compared with Par-HLSTM which has the same structured representation model except that the phrase structure is given by Stanford parser <ref type="bibr" target="#b11">(Klein and Manning 2003)</ref> instead of RL. The results in Table <ref type="table" target="#tab_6">8</ref> show that HS-LSTM outperforms other structured models, indicating that the discovered structure may be more task-relevant and advantageous than that given by parser.</p><p>Then, we computed the statistics of the structures discov- To summarize, our HS-LSTM has the ability of discovering task-relevant structures and then building better structured sentence representations. The qualitative and quantitative results demonstrate that these discovered structures are task-friendly and suitable for text classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>This paper has presented a reinforcement learning method which learns sentence representation by discovering taskrelevant structures. In the framework of RL, we adopted two representation models: ID-LSTM that distills task-relevant words to form purified sentence representation, and HS-LSTM that discovers phrase structures to form hierarchical sentence representation. Extensive experiments show that our method has state-of-the-art performance and is able to discover interesting task-relevant structures without explicit structure annotations.</p><p>As future work, we will apply the method to other types of sequences since the idea of structure discovery (or restructuring the input) can be generalized to other tasks and domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the overall process. The policy network (PNet) samples an action at each state. The structured representation model offers state representation to PNet and outputs the final sentence representation to the classification network (CNet) when all actions are sampled. CNet performs text classification and provides reward to PNet.</figDesc><graphic url="image-1.png" coords="2,54.00,54.00,503.97,96.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2: Examples for ID-LSTM and HS-LSTM. In ID-LSTM, unimportant words are removed, and the corresponding hidden states are copied. In HS-LSTM, phrases in a sentence can be discovered and a hierarchical representation is then built by applying a word-level and phrase-level LSTM.</figDesc><graphic url="image-3.png" coords="4,115.20,138.19,381.60,72.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>a</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Classification accuracy on different datasets. Results marked with * are re-printed from</figDesc><table><row><cell></cell><cell>MR</cell><cell>SST</cell><cell>Subj</cell><cell>AG</cell></row><row><cell>LSTM</cell><cell cols="4">77.4* 46.4* 92.2 90.9</cell></row><row><cell>biLSTM</cell><cell cols="4">79.7* 49.1* 92.8 91.6</cell></row><row><cell>CNN</cell><cell cols="4">81.5* 48.0* 93.4* 91.6</cell></row><row><cell>RAE</cell><cell cols="2">76.2* 47.8</cell><cell cols="2">92.8 90.3</cell></row><row><cell>Tree-LSTM</cell><cell cols="2">80.7* 50.1</cell><cell cols="2">93.2 91.8</cell></row><row><cell cols="2">Self-Attentive 80.1</cell><cell>47.2</cell><cell cols="2">92.5 91.1</cell></row><row><cell>ID-LSTM</cell><cell>81.6</cell><cell>50.0</cell><cell cols="2">93.5 92.2</cell></row><row><cell>HS-LSTM</cell><cell>82.1</cell><cell>49.8</cell><cell cols="2">93.7 92.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>The original average length and distilled average length by ID-LSTM in the test set of each dataset.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Word</cell><cell>Count Deleted Percentage</cell></row><row><cell></cell><cell></cell><cell></cell><cell>of</cell><cell>1,074</cell><cell>947</cell><cell>88.18%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>by</cell><cell>161</cell><cell>140</cell><cell>86.96%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>the</cell><cell>1,846</cell><cell>1558</cell><cell>84.40%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>'s</cell><cell>649</cell><cell>538</cell><cell>82.90%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>but</cell><cell>320</cell><cell>25</cell><cell>7.81%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>not</cell><cell>146</cell><cell>0</cell><cell>0.00%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>no</cell><cell>73</cell><cell>0</cell><cell>0.00%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>good</cell><cell>70</cell><cell>0</cell><cell>0.00%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>interesting</cell><cell>25</cell><cell>0</cell><cell>0.00%</cell></row><row><cell cols="4">Dataset Length Distilled Length Removed</cell></row><row><cell>MR</cell><cell>21.25</cell><cell>11.57</cell><cell>9.68</cell></row><row><cell>SST</cell><cell>19.16</cell><cell>11.71</cell><cell>7.45</cell></row><row><cell>Subj</cell><cell>24.73</cell><cell>9.17</cell><cell>15.56</cell></row><row><cell>AG</cell><cell>35.12</cell><cell>13.05</cell><cell>22.07</cell></row><row><cell cols="4">Quantitative analysis: We presented further analysis on</cell></row><row><cell cols="4">what irrelevant information is removed and what importan-</cell></row><row><cell cols="4">t information is distilled by ID-LSTM. We compared the</cell></row><row><cell cols="4">original sentence length and distilled length given by ID-</cell></row><row><cell cols="4">LSTM in the test set of each dataset, as shown in Table 4. For</cell></row><row><cell cols="4">sentiment classification, we removed about 9.68/7.45/15.56</cell></row><row><cell cols="4">words from the sentences on MR/SST/Subj respectively. For</cell></row><row><cell cols="4">topic classification, we removed about 22 words (from 35.12</cell></row><row><cell cols="4">to 13.05) from the sentences on AG. Interestingly, ID-LSTM</cell></row><row><cell cols="4">removes about or more than half of the words from the sen-</cell></row><row><cell cols="4">tence. This indicates that text classification can be done with</cell></row><row><cell cols="4">highly purified, condensed information. It infers that such</cell></row><row><cell cols="4">classification tasks can be done with only important key-</cell></row><row><cell cols="4">words, while our model has an effect of distilling these im-</cell></row><row><cell cols="3">portant, task-relevant keywords for the task.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>The most/least deleted words in the test set of SST.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>one of the year 's best . Discovered by RL The film is one of the year 's best . Predefined A wonderfully warm human drama that remains vividly in memory long after viewing . Discovered by RL A wonderfully warm human drama that remains vividly in memory long after viewing . Predefined The actors are fantastic . They are what makes it worth the trip to the theater . Discovered by RL The actors are fantastic . They are what makes it worth the trip to the theater . The comparison of the predefined structures and those discovered by HS-LSTM. model without explicit structure annotations.In Table6, we listed some examples to show the difference between the predefined structures (used for pretraining) and those discovered by HS-LSTM. These examples show that our RL method learns to build quite different structures from the predefined ones. The predefined structures are built with some very simple heuristics, and consequently fragmented. In comparison, HS-LSTM tends to discover more complete and longer phrases.</figDesc><table><row><cell>Structure</cell><cell>Sentence</cell></row><row><cell>Predefined</cell><cell>The film is</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Phrase examples discovered by HS-LSTM.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Classification accuracy from structured models. The result marked with * is re-printed from<ref type="bibr" target="#b26">(Yogatama et al. 2017)</ref>.ered by HS-LSTM in Table9, including the average phrase number and words per phrase. The average number of words in a phrase is stable across different datasets: about 4 or 5 words per structure. However, this might be accordant with the term for encouraging structure selection (see Eq. 11).</figDesc><table><row><cell></cell><cell>Models</cell><cell cols="2">SST-binary AG's News</cell></row><row><cell></cell><cell>RAE</cell><cell>85.7</cell><cell>90.3</cell></row><row><cell cols="2">Tree-LSTM</cell><cell>87.0</cell><cell>91.8</cell></row><row><cell cols="2">Com-Tree-LSTM</cell><cell>86.5*</cell><cell>-</cell></row><row><cell cols="2">Par-HLSTM</cell><cell>86.5</cell><cell>91.7</cell></row><row><cell cols="2">HS-LSTM</cell><cell>87.8</cell><cell>92.5</cell></row><row><cell cols="4">Dataset Length #Phrases #Words per phrase</cell></row><row><cell>MR</cell><cell>21.25</cell><cell>4.59</cell><cell>4.63</cell></row><row><cell>SST</cell><cell>19.16</cell><cell>4.76</cell><cell>4.03</cell></row><row><cell>Subj</cell><cell>24.73</cell><cell>4.42</cell><cell>5.60</cell></row><row><cell>AG</cell><cell>35.12</cell><cell>8.58</cell><cell>4.09</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 :</head><label>9</label><figDesc>Statistics of structures discovered by HS-LSTM in the test set of each dataset.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">To be precise, phrase means a substructure or segment.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">SST provides both phrase-level and sentence-level annotations. We randomly added 5 labeled phrases for each sentence to the training data, different from<ref type="bibr" target="#b22">(Tai, Socher, and Manning 2015)</ref> which used all annotated phrases for each sentence.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">http://www.di.unipi.it/ ˜gulli/AG_corpus_ of_news_articles.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was partly supported by the National Science Foundation of China under grant No.61272227/61332007. We would like to thank Prof. Xiaoyan Zhu for her generous support to this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Hierarchical multiscale recurrent neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2014 Workshop</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Contextual lstm (clstm) models for large scale nlp tasks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Heck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD workshop Oral presentation</title>
				<imprint>
			<date type="published" when="1997">2016. 1997</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
	<note>Long short-term memory</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Encoding syntactic knowledge in neural networks for sentiment classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">26</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep unordered composition rivals syntactic methods for text classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I-Ii</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1681" to="1691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
				<imprint>
			<date type="published" when="2014">2017. 2014</date>
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Molding cnns for text: non-linear, non-consecutive convolutions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1565" to="1575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N D</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<editor>
			<persName><surname>Acl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Pang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2004">2015. 2004. 2005</date>
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning tag embeddings and tag-specific composition functions in recursive neural network</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Linguistically regularized lstm for sentiment classification</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1679" to="1689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="1642">2013. 1642</date>
			<biblScope unit="volume">1631</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1057" to="1063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Document modeling with gated recurrent neural network for sentiment classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1422" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to compose words into sentences with reinforcement learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention-based lstm network for cross-lingual sentiment classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2015">2015. 2016</date>
			<biblScope unit="page" from="247" to="256" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An empirical study on the effect of negation words on sentiment</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kiritchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="304" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Long short-term memory over recursive structures</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sobihani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1604" to="1612" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
