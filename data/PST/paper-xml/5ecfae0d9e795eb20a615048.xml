<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LRSpeech: Extremely Low-Resource Speech Synthesis and Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-08-09">9 Aug 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jin</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Interdisciplinary Information Sciences</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Institute for Interdisciplinary Information Sciences</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Ren</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
							<email>lijian83@mail.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Interdisciplinary Information Sciences</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Institute for Interdisciplinary Information Sciences</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sheng</forename><surname>Zhao</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Azure Speech</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Azure Speech</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
							<email>tyliu@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LRSpeech: Extremely Low-Resource Speech Synthesis and Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-08-09">9 Aug 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3394486.3403331</idno>
					<idno type="arXiv">arXiv:2008.03687v1[eess.AS]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Speech synthesis (text to speech, TTS) and recognition (automatic speech recognition, ASR) are important speech tasks, and require a large amount of text and speech pairs for model training. However, there are more than 6,000 languages in the world and most languages are lack of speech training data, which poses significant challenges when building TTS and ASR systems for extremely lowresource languages. In this paper, we develop LRSpeech, a TTS and ASR system under the extremely low-resource setting, which can support rare languages with low data cost. LRSpeech consists of three key techniques: 1) pre-training on rich-resource languages and fine-tuning on low-resource languages; 2) dual transformation between TTS and ASR to iteratively boost the accuracy of each other; 3) knowledge distillation to customize the TTS model on a high-quality target-speaker voice and improve the ASR model on multiple voices. We conduct experiments on an experimental language (English) and a truly low-resource language (Lithuanian) to verify the effectiveness of LRSpeech. Experimental results show that LRSpeech 1) achieves high quality for TTS in terms of both intelligibility (more than 98% intelligibility rate) and naturalness (above 3.5 mean opinion score (MOS)) of the synthesized speech, which satisfy the requirements for industrial deployment, 2) achieves promising recognition accuracy for ASR, and 3) last but not least, uses extremely low-resource training data. We also conduct comprehensive analyses on LRSpeech with different amounts of data resources, and provide valuable insights and guidances for industrial deployment. We are currently deploying LRSpeech into a commercialized cloud speech service to support TTS on more rare languages.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Speech synthesis (text to speech, TTS) <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41]</ref> and speech recognition (automatic speech recognition, ASR) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> are two key tasks in speech domain, and attract a lot of attention in both the research and industry community. However, popular commercialized speech services (e.g., Microsoft Azure, Google Cloud, Nuance, etc.) only support dozens of languages for TTS and ASR, while there are more than 6,000 languages in the world <ref type="bibr" target="#b20">[21]</ref>. Most languages are lack of speech training data, which makes it difficult to support TTS and ASR for these rare languages, as large-amount and high-cost speech training data are required to ensure good accuracy for industrial deployment.</p><p>We describe the typical training data to build TTS and ASR systems as follows:</p><p>• TTS aims to synthesize intelligible and natural speech from text sequences, and usually needs single-speaker high-quality recordings that are collected in professional recording studio. To improve the pronunciation accuracy, TTS also requires a pronunciation lexicon to convert the character sequence into phoneme sequence as the model input (e.g., "speech" is converted into "s p iy ch"), which is called as grapheme-to-phoneme conversion <ref type="bibr" target="#b35">[36]</ref>. Additionally, TTS models use text normalization rules to convert the irregular word into the normalized type that is easier to pronounce (e.g., "Sep 7th" is converted into "September seventh"). • ASR aims to generate correct transcripts (text) from speech sequences, and usually requires speech data from multiple speakers in order to generalize to unseen speakers during inference. The multi-speaker speech data in ASR do not need to be as highquality as that in TTS, but the data amount is usually an order of magnitude bigger. We call the speech data for ASR as multispeaker low-quality data <ref type="foot" target="#foot_0">1</ref> <ref type="foot" target="#foot_1">1</ref> . Optionally, ASR can first recognize the speech into phoneme sequence, and further convert it into character sequence with the pronunciation lexicon as in TTS. • Besides paired speech and text data, TTS and ASR models can also leverage unpaired speech and text data to further improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>According to the data resource used, previous works on TTS and ASR can be categorized into rich-resource, low-resource and unsupervised settings.</p><p>As shown in Table <ref type="table">1</ref>, we list the data resources and the corresponding related works in each setting : Table <ref type="table">1</ref>: The data resource to build TTS and ASR systems and the corresponding related works in rich-resource, low-resource, extremely low-resource and unsupervised settings.</p><p>• In the rich-resource setting, both TTS <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35]</ref> and ASR <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> require a large amount of paired speech and text data to achieve high accuracy: TTS usually needs dozens of hours of single-speaker high-quality recordings, while ASR requires at least hundreds of hours multiple-speaker low-quality data. Besides, TTS in the rich-resource setting also leverages pronunciation lexicon for accurate pronunciation. Optionally, unpaired speech and text data can be leveraged. • In the low-resource setting, the single-speaker high-quality paired data are reduced to dozens of minutes in TTS <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31]</ref> while the multi-speaker low-quality paired data is reduced to dozens of hours in ASR <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b38">39]</ref>, compared to that in the richresource setting. Additionally, they leverage unpaired speech and text data to ensure the performance. • In the unsupervised setting, only unpaired speech and text data are leverage to build ASR models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>As can be seen, a large amount of data resources are leveraged in the rich-resource setting to ensure the accuracy for industrial deployment. Considering nearly all low-resource languages are lack of training data and there are more than 6,000 languages in the world, it will be a huge cost for training data collection. Although data resource can be reduced in the low-resource setting, it still requires 1) a certain amount of paired speech and text (dozens of minutes for TTS and dozens of hours for ASR), 2) a pronunciation lexicon, and 3) a large amount of single-speaker high-quality unpaired speech data that still incur high data collection cost <ref type="foot" target="#foot_2">2</ref> . What is more, the accuracy of the TTS and ASR models in the low-resource setting is not high enough. The purely unsupervised methods for ASR suffer from low accuracy and cannot meet the requirement of industrial deployment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Our Method</head><p>In this paper, we develop LRSpeech, a TTS and ASR system under the extremely low-resource setting, which supports rare languages with low data collection cost. LRSpeech aims for industrial deployment under two constraints: 1) extremely low data collection cost, and 2) high accuracy to satisfy the deployment requirement. For the first constraint, as the extremely low-resource setting shown in Table <ref type="table">1</ref>, LRSpeech explores the limits of data requirements by 1) using single-speaker high-quality paired data as few as possible (several minutes), 2) using a few multi-speaker low-quality paired data (several hours), 3) using slightly more multi-speaker low-quality unpaired speech data (dozens of hours), 4) not using single-speaker high-quality unpaired data, and 5) not using the pronunciation lexicon but directly taking character as the input of TTS and the output of ASR.</p><p>For the second constraint, LRSpeech leverages several key techniques including transfer learning from rich-resource languages, iterative accuracy boosting between TTS and ASR through dual transformation, and knowledge distillation to further refine TTS and ASR models for better accuracy. Specifically, LRSpeech consists of a three-stage pipeline:</p><p>• We first pre-train both TTS and ASR models on rich-resource languages with plenty of paired data, which can learn the alignment capability between speech and text and benefit the alignment learning on low-resource languages. • We further leverage dual transformation between TTS and ASR to iteratively boost the accuracy of each other with unpaired speech and text data. • Furthermore, we leverage knowledge distillation with unpaired speech and text data to customize the TTS model on a highquality target-speaker voice and improve the ASR model on multiple voices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Data Cost and Accuracy</head><p>Next, we introduce the extremely low data cost while promising accuracy achieved by LRSpeech.</p><p>According to <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b42">43]</ref>, the pronunciation lexicon, singlespeaker high-quality paired data and single-speaker high-quality unpaired speech data require much higher collection cost than other data such as multi-speaker low-quality unpaired speech data and unpaired text, since they can be crawled from the web. Accordingly, compared to the low-resource setting in Table <ref type="table">1</ref>, LRSpeech 1) removes the pronunciation lexicon, 2) reduces the single-speaker high-quality paired data by an order of magnitude, 3) removes single-speaker high-quality unpaired speech data, 4) also reduces multi-speaker low-quality paired data by an order of magnitude, 5) similarly leverages multi-speaker low-quality unpaired speech, and 6) additionally leverage paired data from rich-resource languages which incur no additional cost since they are already available in the commercialized speech service. Therefore, LRSpeech can greatly reduce the data collection cost for TTS and ASR.  To verify the effectiveness of LRSpeech under the extremely low-resource setting, we first conduct comprehensive experimental studies on English and then verify on the truly low-resource language: Lithuanian, which is for product deployment. For TTS, LRSpeech achieves 98.08% intelligibility rate, 3.57 MOS score, with 0.48 gap to the ground-truth recordings, satisfying the online deployment requirements <ref type="foot" target="#foot_3">3</ref> . For ASR, LRSpeech achieves 28.82% WER and 14.65% CER, demonstrating great potential under the extremely low-resource setting. Furthermore, we also conduct ablation studies to verify the effectiveness of each component in LRSpeech, and analyze the accuracy of LRSpeech under different data settings, which provide valuable insights for industrial deployment. Finally, we apply LRSpeech to Lithuanian and also meets the online requirement for TTS and achieves promising results on ASR. We are currently deploying LRSpeech to a commercialized speech service to support TTS for rare languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">LRSPEECH</head><p>In this section, we introduce the details of LRSpeech for extremely low-resource speech synthesis and recognition. We first give an overview of LRSpeech, and then introduce the formulation of TTS and ASR. We further introduce each component of LRSpeech respectively, and finally describe the model structure of LRSpeech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pipeline Overview</head><p>To ensure the accuracy of TTS and ASR models under extremely low-resource scenarios, we design a three-stage pipeline for LR-Speech as shown in Figure <ref type="figure" target="#fig_1">1:</ref> • Pre-training and fine-tuning. We pre-train both TTS and ASR models on rich-resource languages and then fine-tune them on low-resource languages. Leveraging rich-resource languages in LRSpeech are based on two considerations: 1) a large amount of paired data on rich-resource languages are already available in the commercialized speech service, and 2) the alignment capability between speech and text in rich-resource languages can benefit the alignment learning in low-resource languages, due to the pronunciation similarity between human languages <ref type="bibr" target="#b41">[42]</ref>. • Dual transformation. Considering the dual nature between TTS and ASR, we further leverage dual transformation <ref type="bibr" target="#b30">[31]</ref> to boost the accuracy of each other with unpaired speech and text data. • Knowledge distillation. To further improve the accuracy of TTS and ASR and facilitate online deployment, we leverage knowledge distillation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b36">37]</ref> to synthesize paired data to train better TTS and ASR models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Formulation of TTS and ASR</head><p>TTS and ASR are usually formulated as sequence to sequence problems <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b40">41]</ref>. Denote the text and speech sequence pair (x, y) ∈ D, where D is the paired text and speech corpus. Each element in the text sequence x represents a phoneme or character, while each element in the speech sequence y represents a frame of speech. To learn the TTS model θ , a mean square error loss is used:</p><formula xml:id="formula_0">L(θ ; D) = −Σ (x,y)∈D (y − f (x; θ )) 2 . (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>To learn the ASR model ϕ, a negative log likelihood loss is used:</p><formula xml:id="formula_2">L(ϕ; D) = −Σ (y,x )∈D log P(x |y; ϕ).<label>(2)</label></formula><p>TTS and ASR models can be developed based on an encoder-attentiondecoder framework <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b39">40]</ref>, where the encoder transforms the source sequence into a set of hidden representations, and the decoder generates the target sequence autoregressively based on the source hidden representations obtained through an attention mechanism <ref type="bibr" target="#b2">[3]</ref>. We make some notations for the data used in LRSpeech. Denote D rich_tts as the high-quality TTS paired data in rich-resource languages, D rich_asr as the low-quality ASR paired data in rich-resource languages, D h as the single-speaker high-quality paired data for target speaker, and D l as the multi-speaker low-quality paired data. Denote X u as unpaired text data while Y u as multi-speaker lowquality unpaired speech data.</p><p>Next, we introduce each component of the LRSpeech pipeline in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Pre-Training and Fine-Tuning</head><p>The key to the conversion between text and speech is to learn the alignment between the character/phoneme representations (text) and the acoustic features (speech). Since people coming from different nations and speaking different languages share similar vocal organs and thus similar pronunciations, the ability of alignment learning in one language can help the alignment in another language <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b41">42]</ref>. This motivates us to transfer the TTS and ASR models that trained in rich-resource languages into low-resource languages, considering there are plenty of paired speech and text data for both TTS and ASR in rich-resource languages.</p><p>Pre-Training. We pre-train the TTS model θ with data corpus D rich_tts following Equation 1 and pre-train the ASR model ϕ with D rich_asr following Equation <ref type="formula" target="#formula_2">2</ref>.</p><p>Fine-Tuning. Considering the rich-resource and low-resource languages have different phoneme/character vocabularies and speakers, we initialize the TTS and ASR models on low-resource language with all the pre-trained parameters except the phoneme/character and speaker embeddings in TTS and the phoneme/character embeddings in ASR <ref type="foot" target="#foot_4">4</ref> respectively. We then fine-tune the TTS model θ and ASR model ϕ both with the concatenation corpus of D h and D l following Equation 1 and Equation 2 respectively. During finetuning, we first fine-tune the character embeddings and speaker embeddings following the practice in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>, and then fine-tune all parameters. It can help prevent the TTS and ASR models from overfitting on the limited paired data in a low-resource language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Dual Transformation between TTS and ASR</head><p>TTS and ASR are two dual tasks and their dual nature can be explored to boost the accuracy of each other, especially in the lowresource scenarios. Therefore, we leverage dual transformation <ref type="bibr" target="#b30">[31]</ref> between TTS and ASR to improve the ability to transform between text and speech. Dual transformation shares similar ideas with backtranslation <ref type="bibr" target="#b33">[34]</ref> in machine translation and cycle-consistency <ref type="bibr" target="#b45">[46]</ref> in image translation, which are effective ways to leverage unlabeled data in speech, text and image domains respectively. Dual transformation works as follows:</p><p>• For each unpaired text sequence x ∈ X u , we transform it into speech sequence using the TTS model θ , and construct a pseudo corpus D(X u ) to train the ASR model ϕ following Equation 2. • For each unpaired speech sequence y ∈ Y u , we transform it into text sequence using the ASR model ϕ, and construct a pseudo corpus D(Y u ) to train the TTS model θ following Equation <ref type="formula" target="#formula_0">1</ref>.</p><p>During training, we run the dual transformation process on the fly, which means the pseudo corpus are updated in each iteration and the model can benefit from the newest data generated by each other. Next, we introduce some specific designs in dual transformation to support multi-speaker TTS and ASR.</p><p>Multi-Speaker TTS Synthesis. Different from <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b30">31]</ref> that only support a single speaker in both TTS and ASR model, we support multi-speaker TTS and ASR in the dual transformation stage. Specifically, we randomly choose a speaker ID and synthesize speech of this speaker given a text sequence, which can benefit the training of the multi-speaker ASR model. Furthermore, the ASR model transforms multi-speaker speech into text, which can help the training of the multi-speaker TTS model.</p><p>Levering Unpaired Speech of Unseen Speakers. Since multiplespeaker low-quality unpaired speech data are much easier to obtain than high-quality single-speaker unpaired speech data, enabling the TTS and ASR models to utilize unseen speakers' unpaired speech in dual transformation can make our system more robust and scalable. Compared to ASR, it is more challenging for TTS to synthesize voice on unseen speakers. To this end, we split dual transformation into two phases: 1) In the first phase, we only use the unpaired speech whose speakers are seen before in the training data. 2) In</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Customization on TTS and ASR through Knowledge Distillation</head><p>The TTS and ASR models we currently have are far from ready for online deployment after dual transformation. There are several issues we need to address: 1) While the TTS model can support multiple speakers, the speech quality of our target speaker is not good enough and needs further improvement; 2) The synthesized speech by the TTS models still have word skipping and repeating issues; 3) The accuracy of the ASR model needs to be further improved. Therefore, we further leverage knowledge distillation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b36">37]</ref>, which generates target sequences given source sequences as input to construct a pseudo corpus, to customize the TTS and ASR models for better accuracy.</p><p>2.5.1 Knowledge Distillation for TTS. The knowledge distillation process for TTS consists of three steps:</p><p>• For each unpaired text sequence x ∈ X u , we synthesize the corresponding speech of the target speaker using the TTS model θ , and construct a single-speaker pseudo corpus D(X u ). • Filter the pseudo corpus D(X u ) whose synthesized speech has word skipping and repeating issues. • Use the filtered corpus D(X u ) to train a new TTS model dedicated to the target speaker following Equation 1.</p><p>In the first step, the speech in the pseudo corpus D(X u ) are single-speaker, which is different from the multi-speaker pseudo corpus D(X u ) in Section 2.4. The TTS model (obtained by dual transformation) in the first step has word skipping and repeating issues. Therefore, in the second step, we filter the synthesized speech which has word skipping and repeating issues, and thus the distilled model can be trained on accurate text and speech pairs. In this way, the word skipping and repeating problem can be largely reduced. We filter the synthesized speech based on two metrics: word coverage ratio (WCR) and attention diagonal ratio (ADR).</p><p>Word Coverage Ratio. We observe that word skipping happens when a word has small or no attention weights from the target melspectrograms. Therefore, we propose word coverage ratio (WCR):</p><formula xml:id="formula_3">W CR = min i ∈[1, N ] { max t ∈[1,T i ] max s ∈[1,S ] A t,s }, (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where N is the number of words in a sentence, T i is the number of characters in the i-th word, S is the number of frames of the target mel-spectrograms, and A t,s denotes the element in the t-th row and s-th column of the attention weight matrix A. We get the attention weight matrix A from the encoder-decoder attention weights in the TTS model and calculate the mean over different layers and attention heads. A high WCR indicates all words in a sentence have high attention weights from target speech frames, and thus is less likely to cause word skipping. Attention Diagonal Ratio. As demonstrated by previous works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b40">41]</ref>, the attention alignments between text and speech are monotonic and diagonal. When the synthesized speech has word skipping and repeating issues, or is totally crashed, the attention alignments will deviate from the diagonal. We define the attention diagonal ratio (ADR) as:</p><formula xml:id="formula_5">ADR = T t =1 kt +b s=kt −b A t,s T t =1 S s=1 A t,s ,<label>(4)</label></formula><p>where T and S are the number of characters and speech frames in a text and speech pair, k = S T , and b is a hyperparameter to determine the width of diagonal. ADR measures how much attention lies in the diagonal area with a width of b. A higher ADR indicates that the synthesized speech has good attention alignment with text and thus has less word skipping, repeating or crashing issues.</p><p>2.5.2 Knowledge Distillation for ASR. Since the unpaired text and low-quality multi-speaker unpaired speech are both available for ASR , we leverage both the ASR and TTS models to synthesize data during the knowledge distillation for ASR:</p><p>• For each unpaired speech y ∈ Y u , we generate the corresponding text using the ASR model ϕ, and construct a pseudo corpus D(Y u ). • For each unpaired text x ∈ X u , we synthesize the corresponding speech of multiple speakers using the TTS model θ , and construct a pseudo corpus D(X u ). • We combine the above pseudo corpus D(Y u ) and D(X u ), as well as the single-speaker high-quality paired data D h and multispeaker low-quality paired data D l to train a new ASR model following Equation <ref type="formula" target="#formula_2">2</ref>.</p><p>Similar to the knowledge distillation for TTS, we also leverage a large amount of unpaired text to synthesize speech. To further improve the ASR accuracy, we use SpecAugment <ref type="bibr" target="#b26">[27]</ref> to add noise in the input speech which acts like data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Model Structure of LRSpeech</head><p>In this section, we introduce the model structure of LRSpeech, as shown in Figure <ref type="figure" target="#fig_2">2</ref>.</p><p>Transformer Model. Both the TTS and ASR models adopt the Transformer based encoder-attention-decoder structure <ref type="bibr" target="#b39">[40]</ref>. One difference from the original Transformer model is that we replace the feed-forward network with a one-dimensional convolution network following <ref type="bibr" target="#b30">[31]</ref>, in order to better capture the dependencies in a long speech sequence.</p><p>Input/Output Module. To enable the Transformer model to support ASR and TTS, we need different input and output modules for speech and text <ref type="bibr" target="#b30">[31]</ref>. For the TTS model: 1) The input module of the encoder is a character/phoneme embedding lookup table, which converts character/phoneme ID into embedding; 2) The input module of the decoder is a speech pre-net, which consists of multiple dense layers to transform each speech frame non-linearly;</p><p>3) The output module of the decoder consists of a linear layer to convert hidden representations into mel-spectrograms, and a stop linear layer with a sigmoid function to predict whether current step should stop or not. For the ASR model: 1) The input module of the encoder consists of multiple convolutional layers, which reduce the length of the speech sequence; 2) The input module of the decoder is a character/phoneme embedding lookup table <ref type="table">;</ref> 3) The output module of the decoder consists of a linear layer and a softmax function, where the linear layer shares the same weights with the character/phoneme embedding lookup table in the decoder input module.</p><p>Speaker Module. The multi-speaker TTS model relies on a speaker embedding module to differentiate multiple speakers. We add a speaker embedding vector both in the encoder output and decoder input (after the decoder input module). As shown in Figure <ref type="figure" target="#fig_2">2 (c)</ref>, we convert the speaker ID into a speaker embedding vector using an embedding lookup table, and then add a linear transformation with a softsign function x = x/(1 + |x |). We further concatenate the obtained vector with the encoder output or decoder input, and use another linear layer to reduce the hidden dimension to the original hidden of the encoder output or decoder input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS AND RESULTS</head><p>In this section, we conduct experiments to evaluate LRSpeech for extremely low-resource TTS and ASR. We first describe the experiment settings, show the results of our method, and conduct some analyses of LRSpeech. 3.1.1 Datasets. We describe the datasets used in rich-resource and low-resource languages respectively:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>• We select Mandarin Chinese as the rich-resource language. The TTS corpus D rich_tts contains 10000 paired speech and text data (12 hours) of a single speaker from Data Baker<ref type="foot" target="#foot_5">5</ref> . The ASR corpus D rich_asr is from AIShell <ref type="bibr" target="#b4">[5]</ref>, which contains about 120000 paired speech and text data (178 hours) from 400 Mandarin Chinese speakers. • We select English as a low-resource language for experimental development. The details of the data resources used are shown in Table <ref type="table" target="#tab_1">2</ref>. More information about these datasets are shown in Section A.1 and Table <ref type="table">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Training and Evaluation.</head><p>We use a 6-layer encoder and a 6-layer decoder for both the TTS and ASR models. The hidden size, character embedding size, and speaker embedding size are all set to 384, and the number of attention heads is set to 4. During dual transformation, we up-sample the paired data to make its size roughly the same with the unpaired data. During knowledge distillation, we filter the synthesized speech with WCR less than 0.7 and ADR less than 0.7. The width of diagonal (b) in ADR is 10. More model training details are introduced in Section A.2. The TTS model uses Parallel WaveGAN <ref type="bibr" target="#b43">[44]</ref> as the vocoder to synthesize speech. To train Parallel WaveGAN, we combine the speech data in the Mandarin Chinese TTS corpus D rich_tts with the speech data in the English target-speaker high-quality corpus D h . We up-sample the speech data in D h to make it roughly the same with the speech data in D rich_tts .</p><p>For evaluation, we use MOS (mean opinion score) and IR (intelligibility rate) for TTS, and WER (word error rate) and CER (character error rate) for ASR. For TTS, we select English text sentences from the news-crawl<ref type="foot" target="#foot_6">6</ref> dataset to synthesize speech for evaluation. We randomly select 200 sentences for IR test and 20 sentences for MOS test, following the practice in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b40">41]</ref>  <ref type="foot" target="#foot_7">7</ref> . Each speech is listened by at least 5 testers for IR test and 20 testers for MOS test, who are all native English speakers. For ASR, we measure the WER and CER score on the LibriSpeech "test-clean" set. The test sentences and speech for TTS and ASR do not appear in the training corpus. The results are shown in Table <ref type="table" target="#tab_2">3</ref>. We have several observations:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setting</head><p>• Both baselines cannot synthesize reasonable speech and the corresponding IR and MOS are marked as "/". The WER and CER on ASR are also larger than 100% <ref type="foot" target="#foot_8">8</ref> , which demonstrates the poor quality when only using the limited paired data D h and D l for TTS and ASR training. • Based on Baseline #2, pre-training and fine-tuning (PF) can achieve an IR score of 93.09% and a MOS score of 2.84 for TTS, and reduce the WER to 103.70% and CER to 69.53%, which demonstrates the effectiveness of cross-lingual pre-training for TTS and ASR. • However, the paired data in both rich-resource and low-resource languages cannot guarantee high accuracy, and thus we further leverage the unpaired speech corpus Y u seen and Y u unseen , and unpaired text corpus X u through dual transformation (DT). DT can greatly improve IR to 96.70% and MOS to 3.28 on TTS, as well as WER to 38.94% and CER to 19.99%. The unpaired text and speech samples can cover more words and pronunciations, as well as more speech prosody, which help the synthesized speech in TTS achieves higher intelligibility (IR) and naturalness (MOS), and also help ASR achieves better WER and CER. Table <ref type="table">4</ref>: The word coverage ratio (WCR) and attention diagonal ratio (ADR) scores in TTS model under different settings.</p><p>• Furthermore, adding knowledge distillation (KD) brings 1.38% IR, 0.29 MOS, 10.12% WER and 5.34% CER improvements. We also list the speech quality in terms of MOS for the ground-truth recordings (GT) and the synthesized speech from the ground-truth mel-spectrogram by Parallel WaveGAN vocoder (GT (Parallel WaveGAN)) in Table <ref type="table" target="#tab_2">3</ref> as the upper bounds for references. It can be seen that LRSpeech achieves a MOS score of 3.57, with a gap to the ground-truth recordings less than 0.5, demonstrating the high quality of the synthesized speech. • There are also some related works focusing on low-resource TTS and ASR, such as Speech Chain <ref type="bibr" target="#b38">[39]</ref>, Almost Unsup <ref type="bibr" target="#b30">[31]</ref>, and SeqRQ-AE <ref type="bibr" target="#b22">[23]</ref>. However, these methods require much data resource to build systems and thus cannot achieve reasonable accuracy in the extremely low-resource setting. For example, <ref type="bibr" target="#b30">[31]</ref> requires a pronunciation lexicon to convert the character sequence into phoneme sequence, and dozens of hours of single-speaker high-quality unpaired speech data to improve the accuracy, which are costly and not available in the extremely low-resource setting. As a result, <ref type="bibr" target="#b30">[31]</ref> cannot synthesize reasonable speech in TTS and achieves high WER according to our preliminary experiments.</p><p>As a summary, LRSpeech achieves an IR score of 98.08% and a MOS score of 3.57 for TTS with extremely low data cost, which meets the online requirements for deploying the TTS system. Besides, it also achieves a WER score of 28.82% and a CER score of 14.65%, which is highly competitive considering the data resource used, and shows great potential for further online deployment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.2</head><p>Analyses on the Alignment Quality of TTS. Since the quality of the attention alignments between the encoder (text) and decoder (speech) are good indicators of the performance of TTS model, we analyze the word coverage ratio (WCR) and attention diagonal ratio (ADR) as described in Section 2.5.1 and show their changes among different settings in Table <ref type="table">4</ref>. We also show the attention alignments of a sample case from each setting in Figure <ref type="figure" target="#fig_3">3</ref>. We have several observations:</p><p>• As can be seen from Figure <ref type="figure" target="#fig_3">3</ref>  • Since there still exist some word skipping and repeating issues after DT, we filter the synthesized speech according to WCR and ADR during knowledge distillation (KD). The final WCR is further improved to 0.72 and ADR is improved to 98.81% as shown in Table <ref type="table">4</ref>, and the attention alignments in Figure <ref type="figure" target="#fig_3">3</ref> (e) are much more clear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Further Analyses of LRSpeech</head><p>There are some questions to further investigate in LRSpeech:</p><p>• Low-quality speech data may bring noise to the TTS model. How can the accuracy change if using different scales of low-quality paired data D l ? • As described in Section 2.4, supporting the LRSpeech training with unpaired speech data from seen and especially unseen speakers (Y u seen and Y u unseen ) is critical for a robust and scalable system. Can the accuracy be improved if using Y u seen and Y u unseen ? • How can the accuracy change if using different scales of unpaired text data X u to synthesize speech during knowledge distillation?  We conduct experimental analyses to answer these questions. For the first two questions, we simply analyze LRSpeech without knowledge distillation, and for the third question, we analyze in the knowledge distillation stage. The results are shown in Figure <ref type="figure" target="#fig_6">4</ref> <ref type="foot" target="#foot_9">9</ref> . We have several observations:</p><p>• As shown in Figure <ref type="figure" target="#fig_6">4</ref> (a), we vary the size of D l with 1/5×, 1/2× and 5× of the default setting (1000 paired data, 3.5 hours) used in LRSpeech, and find that more low-quality paired data result in the better accuracy for TTS. • As shown in Figure <ref type="figure" target="#fig_6">4</ref> (b), we add Y u seen and Y u unseen respectively, and find that both of them can boost the accuracy of TTS and ASR, which demonstrates the ability of LRSpeech to utilize unpaired speech from seen and especially unseen speakers.</p><p>• As shown in Figure <ref type="figure" target="#fig_6">4</ref> (c), we vary the number of synthesized speech for TTS during knowledge distillation with 1/20×, 1/7× and 1/4× of the default setting (20000 synthesized speech data), and find more synthesized speech data result in better accuracy. • During knowledge distillation for ASR, we use two kinds of data:</p><p>1) the realistic speech data (8050 data in total), which contains D h , D l and the pseudo paired data distilled from Y u seen and Y u unseen by the ASR model, 2) the synthesized speech data, which are the pseudo paired data distilled from X u by the TTS model. We vary the number of synthesized speech data from X u (the second type) with 0×, 1/3×, 1/2×, 1×, 2×, 3× of the realistic speech data (the first type) in Figure <ref type="figure" target="#fig_6">4</ref> (d). It can be seen that increasing the ratio of synthesized speech data can achieve better results.</p><p>All the observations above demonstrate the effectiveness and scalability of LRSpeech by leveraging more low-cost data resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Apply to Truly Low-Resource Language: Lithuanian</head><p>Data Setting. The data setting in Lithuanian is similar to that in English. We select a subset of Liepa corpus <ref type="bibr" target="#b19">[20]</ref> and only use the characters as the raw texts. The D h contains 50 paired text and speech data (3.7 minutes), D l contains 1000 paired text and speech data (1.29 hours), Y u seen contains 4000 unpaired speech data (5.1 hours), Y u unseen contains 5000 unpaired speech data (6.7 hours), and X u contains 20000 unpaired texts.</p><p>We select Lithuanian text sentences from the news-crawl dataset as the test set for TTS. We randomly select 200 sentences for IR test and 20 sentences for MOS test, following the same test configuration in English. Each audio is listened by at least 5 testers for IR test and Results. As shown in Table <ref type="table" target="#tab_3">5</ref>, the TTS model on Lithuanian achieves an IR score of 98.60% and a MOS score of 3.65, with a MOS gap to the ground-truth recording less than 0.5, which also meets the online deployment requirement 10  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSION</head><p>In this paper, we developed LRSpeech, a speech synthesis and recognition system under the extremely low-resource setting, which supports rare languages with low data costs. We proposed pre-training and fine-tuning, dual transformation and knowledge distillation in LRSpeech to leverage few paired speech and text data, and slightly more multi-speaker low-quality unpaired speech data to improve the accuracy of TTS and ASR models. Experiments on English and Lithuanian show that LRSpeech can meet the requirements of online deployment for TTS and achieve very promising results for ASR under the extremely low-resource setting, demonstrating the effectiveness of LRSpeech for rare languages.</p><p>Currently we are deploying LRSpeech to a large commercialized cloud TTS service. In the future, we will further improve the accuracy of ASR in LRSpeech and also deploy it to this commercialized cloud service.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A REPRODUCIBILITY A.1 Datasets</head><p>We list the detailed information of all of the datasets used in this paper in Table <ref type="table">6</ref>. Next, we first describe the details of the data preprocessing for speech and text data, and then describe what is the "high-quality" and "low-quality" speech mentioned in this paper <ref type="foot" target="#foot_10">11</ref> .</p><p>Data Proprocessing. For the speech data, we re-sample it to 16kHZ and convert the raw waveform into mel-spectrograms following Shen et al. <ref type="bibr" target="#b34">[35]</ref> with 50ms frame size, 12.5ms hop size. For the text, we use text normalization rules to convert the irregular word into the normalized type which is easier to pronounce, e.g., "Sep 7th" will be converted into "September seventh".</p><p>High-Quality Speech. We use high-quality speech to refer the speech data from TTS corpus (e.g., LJSpeech, Data Baker as shown in Table <ref type="table">6</ref>), which are usually recorded in a professional recording studio with consistent characteristics such as speaking rate.</p><p>Collecting high-quality speech data for TTS is typically costly <ref type="bibr" target="#b12">[13]</ref>.</p><p>Low-Quality Speech. We use low-quality speech to refer the speech data from ASR corpus (e.g., LibriSpeech, AIShell, Liepa as shown in Table <ref type="table">6</ref>). Compared to high-quality speech, low-quality speech usually contains noise due to the recording devices (e.g., smartphones, laptops) or the recording environment (e.g., room reverberation, traffic noise). However, low-quality speech cannot be too noisy for model training. We just use the term "low-quality" to differ from high-quality speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Model Configurations and Training</head><p>Both the TTS and ASR models use the 6-layer encoder and 6-layer decoder. For both the TTS and ASR models, the hidden size and speaker ID embedding size is 384 and the number of attention heads is 4. The kernel sizes of 1D convolution in the 2-layer convolution network are set to 9 and 1 respectively, with input/output size of 384/1536 for the first layer and 1536/384 in the second layer. For the TTS model, the input module of the decoder consists of 3 fully-connected layers. The first two fully-connected layers have 64 neurons each and the third one has 384 neurons. The ReLU nonlinearity is applied to the output of every fully-connected layer. We also insert 2 dropout layers in between the 3 fully-connected layers, with dropout probability 0.5. The output module of the decoder is a fully-connected layer with 80 neurons. For the ASR model, the encoder contains 3 convolution layers. The first two are 3 × 3 convolution layers with stride 2 and filter size 256, and the third one with stride 1 and filter size 256. The ReLU non-linearity is applied to the output of every convolution layer except the last one.</p><p>We implement LRSpeech based on the tensor2tensor codebase <ref type="foot" target="#foot_11">12</ref> . We use the Adam optimizer with β 1 = 0.9, β 2 = 0.98, ε = 10 −9 and follow the same learning rate schedule in Vaswani et al. <ref type="bibr" target="#b39">[40]</ref>. We train both the TTS and ASR models in LRSpeech on 4 NVIDIA V100 GPUs. Each batch contains 20,000 speech frames in total. The pre-training and fine-tuning, dual transformation and knowledge distillation take nearly 1, 7, 1 days respectively. We measure the TTS inference speed on a server with 12 Intel Xeon CPU, 256GB memory, 1 NVIDIA V100 GPU. The TTS model takes about 0.21s to generate 1.0s of speech, which satisfies online deployment requirements for inference speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Evaluation Details</head><p>Mean Opinion Score (MOS). The MOS test is a speech quality test for naturalness where listeners (testers) were asked to give their opinions on the speech quality in a five-point scale MOS: 5=excellent, 4=good, 3=fair, 2=poor, 1=bad. We randomly select 20 sentences to synthesize speech for MOS test and each audio is listened by 20 testers, who are all native speakers. We present a part of the MOS test results in Figure <ref type="figure" target="#fig_8">5</ref>. The complete test report can be downloaded here <ref type="foot" target="#foot_12">13</ref> . Intelligibility Rate (IR). The IR test is a speech quality test for Intelligibility. During the test, the listeners (testers) are requested to mark every unintelligible word in the text sentence. IR is calculated by the proportion of the words that are intelligible over the total test words. We randomly select 200 sentences to synthesize speech for IR test and each audio is listened by 5 testers, who are all native speakers. A part of the IR test results is shown in Figure <ref type="figure">6</ref>. You can find more test reports from the demo link.</p><p>WER and CER. Given the reference text and predicted text, the WER calculates the edit distance between them and then normalizes the distance by dividing the number of words in the reference sentence. The WER is defined as W ER = S +D+I N , where the N is the number of words in the reference sentence, S is the number of substitutions, D is the number of deletions and I is the number of insertions. The WER can be larger than 100%. For example, given the reference text "an apple" and predicted text "what is history", the predicted text needs two substitution operations and one insertion operation. For this case, the WER is 2+1  2 =150%. The CER is similar to WER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Some Explorations in Experiments</head><p>We briefly describe some other explorations in training LRSpeech in this paper:</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The three-stage pipeline of LRSpeech.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The Transformer based TTS and ASR models in LRSpeech.</figDesc><graphic url="image-1.png" coords="5,53.80,83.69,504.41,113.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The TTS attention alignments (where the column and row represent the source text and target speech respectively) of an example chosen from the test set. The source text is "the paper's author is alistair evans of monash university in australia".</figDesc><graphic url="image-2.png" coords="7,241.76,71.64,146.18,117.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) and (b), both Baseline #1 and #2 achieve poor attention alignments and their synthesized speech samples are crashed (ADR is smaller than 0.5). The attention weights of Baseline #2 are almost randomly assigned and the synthesized speech is crashed, which demonstrates that simply adding a few low-quality multi-speaker data (D l ) on D h cannot help the TTS model but make it worse. Due to the poor alignment quality of Baseline #1 and #2, we do not analyze their corresponding WCR. • After adding pre-training and fine-tuning (PF), the attention alignments in Figure 3 (c) become diagonal, which demonstrates the TTS model pre-training in rich-resource languages can help build reasonable alignments between text and speech in lowresource languages. Although the synthesized speech can be roughly understood by humans, it still has many issues such as word skipping and repeating. For example, the word "in" in the red box of Figure 3 (c) has low attention weight (WCR), and thus the speech skips the word "in". • Further adding dual transformation (DT) improves WCR and ADR, and also alleviates the words skipping and repeating issues. Accordingly, the attention alignments in Figure 3 (d) are better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) Varying the data scale of D l (b) Results using Y u seen and Y u unseen (c) Varying the data X u for (d) Varying the data X u for TTS knowledge distillation ASR knowledge distillation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Analyses of LRSpeech with different training data.</figDesc><graphic url="image-7.png" coords="8,53.80,78.02,504.39,82.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>20 testers for MOS test, who are all native Lithuanian speakers. For ASR evaluation, we randomly select 1000 speech data (1.3 hours) with 197 speakers from Liepa corpus to measure the WER and CER scores. The test sentences and speech for TTS and ASR do not appear in the training corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: A part of the English MOS test report.</figDesc><graphic url="image-8.png" coords="10,324.59,258.72,226.98,135.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The data used in the low-resource language: English.</figDesc><table><row><cell cols="3">Notation Quality Type</cell><cell>Dataset</cell><cell>#Samples</cell></row><row><cell>D h</cell><cell>High</cell><cell>Paired</cell><cell>LJSpeech [17]</cell><cell>50 (5 minutes)</cell></row><row><cell>D l</cell><cell>Low</cell><cell>Paired</cell><cell cols="2">LibriSpeech [26] 1000 (3.5 hours)</cell></row><row><cell>Y u seen Y u unseen X u</cell><cell>Low Low /</cell><cell cols="2">Unpaired LibriSpeech Unpaired LibriSpeech Unpaired news-crawl</cell><cell>2000 (7 hours) 5000 (14 hours) 20000</cell></row><row><cell cols="5">D h represents target-speaker high-quality paired data. D l</cell></row><row><cell cols="5">represents multi-speaker low-quality paired data (50 speak-</cell></row><row><cell cols="5">ers). Y u seen represents multi-speaker low-quality unpaired</cell></row><row><cell cols="5">speech data (50 speakers), where speakers are seen in the</cell></row><row><cell cols="5">paired training data. Y u unseen represents multi-speaker low-</cell></row><row><cell cols="5">quality unpaired speech data (50 speakers), where speakers</cell></row><row><cell cols="5">are unseen in the paired training data. X u represents un-</cell></row><row><cell cols="2">paired text data.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The accuracy comparisons for TTS and ASR. PF, DT and KD are the three components of LRSpeech, where PF represents pre-training and fine-tuning, DT represents dual transformation, KD represents knowledge distillation. GT is the ground-truth and GT (Parallel WaveGAN) is the audio generated with Parallel WaveGAN from the ground-truth mel-spectrogram. Baseline #1 and #2 are two baseline methods with limited paired data.</figDesc><table><row><cell></cell><cell></cell><cell>TTS</cell><cell></cell><cell>ASR</cell></row><row><cell></cell><cell cols="4">IR (%) MOS WER (%) CER (%)</cell></row><row><cell>Baseline #1</cell><cell>/</cell><cell>/</cell><cell>148.29</cell><cell>100.16</cell></row><row><cell>Baseline #2</cell><cell>/</cell><cell>/</cell><cell>122.09</cell><cell>97.91</cell></row><row><cell>+PF</cell><cell cols="2">93.09 2.84</cell><cell>103.70</cell><cell>69.53</cell></row><row><cell>+PF+DT</cell><cell cols="2">96.70 3.28</cell><cell>38.94</cell><cell>19.99</cell></row><row><cell cols="4">+PF+DT+KD (LRSpeech) 98.08 3.57 28.82</cell><cell>14.65</cell></row><row><cell>GT (Parallel WaveGAN)</cell><cell>-</cell><cell>3.88</cell><cell>-</cell><cell>-</cell></row><row><cell>GT</cell><cell>-</cell><cell>4.05</cell><cell>-</cell><cell>-</cell></row></table><note>3.2.1 Main Results. We compare LRSpeech with the baselines that purely leverage the limited paired data for training, including 1) Baseline #1, which trains TTS and ASR model only with corpus D h , and 2) Baseline #2, which adds additional corpus D l on Baseline #1 for TTS and ASR model training. We also conduct experiments to analyze the effectiveness of each component (pre-training and finetuning, dual transformation, knowledge distillation) in LRSpeech.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>. The ASR model achieves a CER score of 10.30% and a WER score of 17.04%, which shows great potential under this low-resource setting. The results of LRSpeech on TTS and ASR with regard to Lithuanian.</figDesc><table><row><cell>Setting</cell><cell cols="4">IR (%) MOS WER (%) CER (%)</cell></row><row><cell>Lithuanian</cell><cell cols="2">98.60 3.65</cell><cell>17.04</cell><cell>10.30</cell></row><row><cell cols="2">GT (Parallel WaveGAN) -</cell><cell>3.89</cell><cell>-</cell><cell>-</cell></row><row><cell>GT</cell><cell>-</cell><cell>4.01</cell><cell>-</cell><cell>-</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">This work was conducted at Microsoft. Correspondence to: Tao Qin &lt;taoqin@microsoft.com&gt;.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">The low quality here does not mean the quality of ASR data is very bad, but is just relatively low compared to the high-quality TTS recordings.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">Although we can crawl the multi-speaker low-quality unpaired speech data from the web, it is hard to crawl the single-speaker high-quality unpaired speech data. Therefore, it has the same collection cost (recorded by human) with the single-speaker high-quality paired data.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">According to the requirements of a commercialized cloud speech service, the intelligibility rate should be higher than 98% and the MOS score should be higher than 3.5 while the MOS gap to the ground-truth recordings should be less than 0.5.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4">ASR model does not need speaker embeddings, and the target embeddings and the softmax matrix are usually shared in many sequence generation tasks for better accuracy<ref type="bibr" target="#b28">[29]</ref>.the second phase, we also add the unpaired speech whose speakers are unseen in the training data. As the ASR model can naturally support unseen speakers, the pseudo paired data can be used to train and enable the TTS model with the capability to synthesize speech of new speakers.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5">https://www.data-baker.com/open_source.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6">http://data.statmt.org/news-crawl</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_7">The sentences for IR and MOS test, audio samples and test reports can be founded in https://speechresearch.github.io/lrspeech.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_8">The WER and CER can be larger than 100%, and the detailed reasons can be founded in Section A.3.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_9">The audio samples and complete experiments results on IR and MOS for TTS, and WER and CER for ASR can be founded in https://speechresearch.github.io/lrspeech.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10">We show some high-quality speech (target speaker) and low-quality speech (other speakers) from the training set in the demo page: https://speechresearch.github.io/lrspeech.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_11">https://github.com/tensorflow/tensor2tensor</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_12">https://speechresearch.github.io/lrspeech</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ACKNOWLEDGEMENTS</head><p>Jin Xu and Jian Li are supported in part by the National Natural Science Foundation of China Grant 61822203, 61772297, 61632016, 61761146003, and the Zhongguancun Haihua Institute for Frontier <ref type="bibr" target="#b9">10</ref> The audio samples can be founded in https://speechresearch.github.io/lrspeech Information Technology, Turing AI Institute of Nanjing and Xi'an Institute for Interdisciplinary Information Core Technology.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setting Result</head><p>Reference some mysterious force seemed to have brought about a convulsion of the elements Baseline #1 in no characters is the contrast between the ugly and vulgar illegibility of the modern type Baseline #2 the queen replied in a careless tone for instance now she went on + PF some of these ceriase for seen to him to have both of a down of the old lomests + PF + DT some misterious force seemed to have brought about a convulsion of the elements + PF + DT + KD (LRSpeech) some mysterious force seemed to have brought about a convulsion of the elements  • Pre-training and Finetune We also try different methods such as unifying the character spaces between rich-resource and lowresource languages, or learning the mapping between the character embeddings of rich-and low-resource languages as used in <ref type="bibr" target="#b8">[9]</ref>. However, we find these methods result in similar accuracy for both TTS and ASR. • Speaker Module To design the speaker module, we explore several ways including replacing softsign with ReLU, etc. Experimental results show that the design as </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Case Analyses for ASR</head><p>We also conduct a case analysis on ASR as shown in Table <ref type="table">7</ref>. Please refer to Section 3.2 for the descriptions of each setting in this table.</p><p>The generated text by Baseline #1 is completely irrelevant to the reference. Besides, we find from the test set that the generated text is usually the same for many completely different speech, due to the lack of paired data (only 50 paired data) for training. Baseline #2 can generate different text sentences for different speech, but still cannot generate reasonable results. After pre-training and finetuning (PF), the model can recognize some words like "have". Similar to the effect of pre-training on TTS, pre-training ASR on richresource language can also help to learn the alignment between speech and text. By further leveraging unpaired speech and text, with dual transformation (DT), the generated sentence is more accurate. However, for some hard words like "mysterious", the model cannot recognize it correctly. TTS and ASR can also help each other not only in dual transformation but also in knowledge distillation (KD). During KD, a large amount of pseudo paired data generated from the TTS model can help the ASR model recognize most words and give correct results as shown in Table <ref type="table">7</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11856</idno>
		<title level="m">On the cross-lingual transferability of monolingual representations</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Effectiveness of self-supervised pre-training for speech recognition</title>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03912</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dictionary Augmented Sequence-to-Sequence Neural Network for Grapheme to Phoneme Prediction</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bruguier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dravyansh</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Interspeech</title>
		<imprint>
			<biblScope unit="page" from="3733" to="3737" />
			<date type="published" when="2018">2018. 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Aishell-1: An open-source mandarin speech corpus and a speech recognition baseline</title>
		<author>
			<persName><forename type="first">Hui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayu</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bengu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the International Coordinating Committee on Speech Databases and Speech I/O Systems and Assessment</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
	<note>20th Conference of the Oriental Chapter</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="4960" to="4964" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Extensible cross-modal hashing</title>
		<author>
			<persName><forename type="first">Tian-Yi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi-Cong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi-Long</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bai-Chuan</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
				<meeting>the 28th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2109" to="2115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Yi-Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chia-Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung-Feng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10952</idno>
		<title level="m">Towards Unsupervised Automatic Speech Recognition Trained by Unaligned Speech and Text only</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">End-to-end text-to-speech for low-resource languages by cross-lingual transfer learning</title>
		<author>
			<persName><forename type="first">Yuan-Jui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Chieh</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019. 2019</date>
			<biblScope unit="page" from="2075" to="2079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">State-of-the-art speech recognition with sequence-to-sequence models</title>
		<author>
			<persName><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjuli</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanishka</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekaterina</forename><surname>Gonina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4774" to="4778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">End-to-end continuous speech recognition using attention-based recurrent nn: First results</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2014 Workshop on Deep Learning</title>
				<imprint>
			<date type="published" when="2014-12">2014. December 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised training for improving data efficiency in end-to-end speech synthesis</title>
		<author>
			<persName><forename type="first">Yu-An</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Skerry-Ryan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6940" to="6944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Characteristics of Text-to-Speech and Other Corpora</title>
		<author>
			<persName><forename type="first">Erica</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Hirschberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Speech Prosody</title>
				<meeting>Speech Prosody</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Text-to-speech synthesis using found data for lowresource languages</title>
		<author>
			<persName><forename type="first">Erica</forename><surname>Lindsay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cooper</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>Ph.D. Dissertation. Columbia University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Joel</forename><surname>Harband</surname></persName>
		</author>
		<ptr target="http://elearningtech.blogspot.com/2010/11/text-to-speech-costs-licensing-and.html" />
		<title level="m">Text-to-Speech Costs âĂŤ Licensing and Pricing</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cycle-consistency training for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramon</forename><surname>Astudillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6271" to="6275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Keith</forename><surname>Ito</surname></persName>
		</author>
		<ptr target="https://keithito.com/LJ-Speech-Dataset/" />
		<title level="m">The LJ Speech Dataset</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sequence-Level Knowledge Distillation</title>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1317" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Phonetic learning as a pathway to language: new data and native language magnet theory expanded (NLM-e)</title>
		<author>
			<persName><forename type="first">Barbara</forename><forename type="middle">T</forename><surname>Patricia K Kuhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharon</forename><surname>Conboy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denise</forename><surname>Coffey-Corina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maritza</forename><surname>Padden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobey</forename><surname>Rivera-Gaxiola</surname></persName>
		</author>
		<author>
			<persName><surname>Nelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">363</biblScope>
			<biblScope unit="page" from="979" to="1000" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lithuanian Speech Corpus Liepa for development of human-computer interfaces working in voice recognition and synthesis mode</title>
		<author>
			<persName><forename type="first">Sigita</forename><surname>Laurinčiukaitė</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laimutis</forename><surname>Telksnys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pijus</forename><surname>Kasparaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Kliukienė</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vilma</forename><surname>Paukštytė</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Informatica</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="487" to="498" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewis</forename></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gary</surname></persName>
		</author>
		<ptr target="http://www.ethnologue.com" />
		<title level="m">Ethnologue: Languages of the World</title>
				<editor>
			<persName><forename type="first">Charles</forename><forename type="middle">D</forename><surname>Simons</surname></persName>
		</editor>
		<editor>
			<persName><surname>Fennig</surname></persName>
		</editor>
		<meeting><address><addrLine>Dallas, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>SIL International</publisher>
			<date type="published" when="2013">2013. 2015. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Naihan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
		<title level="m">Neural Speech Synthesis with Transformer Network</title>
				<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Tao</forename><surname>Alexander H Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung-Yi</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin-Shan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.12729</idno>
		<title level="m">Towards Unsupervised Speech Recognition and Synthesis with Quantized Speech Representation Learning</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Completely Unsupervised Phoneme Recognition by Adversarially Learning Mapping Relationships from Audio Embeddings</title>
		<author>
			<persName><forename type="first">Da-Rong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin-Shan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Interspeech</title>
		<imprint>
			<biblScope unit="page" from="3748" to="3752" />
			<date type="published" when="2018">2018. 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Effective Approaches to Attention-based Neural Machine Translation</title>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Librispeech: an ASR corpus based on public domain audio books</title>
		<author>
			<persName><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition</title>
		<author>
			<persName><forename type="first">William</forename><surname>Daniel S Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Cheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekin</forename><forename type="middle">D</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019. 2019</date>
			<biblScope unit="page" from="2613" to="2617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep Voice 3: 2000-Speaker Neural Text-to-Speech</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kainan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sercan</forename><forename type="middle">O</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Using the Output Embedding to Improve Language Models</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<title level="s">Short Papers</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="157" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fastspeech: Fast, robust and controllable text to speech</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangjun</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3165" to="3174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Almost Unsupervised Text to Speech and Automatic Speech Recognition</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5410" to="5419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhuvana</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zelin</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11699</idno>
		<title level="m">Speech Recognition with Augmented Synthesized Speech</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">wav2vec: Unsupervised Pre-Training for Speech Recognition</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019. 2019</date>
			<biblScope unit="page" from="3465" to="3469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improving Neural Machine Translation Models with Monolingual Data</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Natural tts synthesis by conditioning wavenet on mel spectrogram predictions</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rj</forename><surname>Skerrv-Ryan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4779" to="4783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Token-Level Ensemble Distillation for Grapheme-to-Phoneme Conversion</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Wei</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019. 2019</date>
			<biblScope unit="page" from="2115" to="2119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multilingual Neural Machine Translation with Knowledge Distillation</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=S1gUsoR9YX" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Comparison of grapheme-to-phoneme conversion methods on a myanmar pronunciation dictionary</title>
		<author>
			<persName><forename type="first">Ye</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thu</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Win</forename><forename type="middle">Pa</forename><surname>Pa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshinori</forename><surname>Sagisaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoto</forename><surname>Iwahashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on South and Southeast Asian Natural Language Processing</title>
				<meeting>the 6th Workshop on South and Southeast Asian Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016. WSSANLP2016</date>
			<biblScope unit="page" from="11" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Listening while speaking: Speech chain by deep learning</title>
		<author>
			<persName><forename type="first">Andros</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sakriani</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="301" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Tacotron: Towards End-to-End Speech Synthesis</title>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daisy</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongheng</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017. 2017</date>
			<biblScope unit="page" from="4006" to="4010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The evolutionary history of the human speech organs</title>
	</analytic>
	<monogr>
		<title level="j">Studies in language origins</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="173" to="197" />
			<date type="published" when="1989-01">Jan Wind. 1989. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Thousands of voices for HMM-based speech synthesis-Analysis and application of TTS systems built on various ASR corpora</title>
		<author>
			<persName><forename type="first">Junichi</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bela</forename><surname>Usabaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Dines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rile</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keiichiro</forename><surname>Oura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Jian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="984" to="1004" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Parallel WaveGAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram</title>
		<author>
			<persName><forename type="first">Ryuichi</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunwoo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jae-Min</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11480</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised Speech Recognition via Segmental Empirical Output Distribution Matching</title>
		<author>
			<persName><forename type="first">Chih-Kuan</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengzhu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
