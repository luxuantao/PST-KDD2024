<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Information Bottleneck</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-10-24">24 Oct 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tailin</forename><surname>Wu</surname></persName>
							<email>tailin@cs.stanford.edu</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Equal contribution 34th Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2020)</postCode>
									<settlement>Vancouver</settlement>
									<region>NeurIPS</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
							<email>hyren@cs.stanford.edu</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Equal contribution 34th Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2020)</postCode>
									<settlement>Vancouver</settlement>
									<region>NeurIPS</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
							<email>panli0@cs.stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Information Bottleneck</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-10-24">24 Oct 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2010.12811v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Representation learning of graph-structured data is challenging because both graph structure and node features carry important information. Graph Neural Networks (GNNs) provide an expressive way to fuse information from network structure and node features. However, GNNs are prone to adversarial attacks. Here we introduce Graph Information Bottleneck (GIB), an information-theoretic principle that optimally balances expressiveness and robustness of the learned representation of graph-structured data. Inheriting from the general Information Bottleneck (IB), GIB aims to learn the minimal sufficient representation for a given task by maximizing the mutual information between the representation and the target, and simultaneously constraining the mutual information between the representation and the input data. Different from the general IB, GIB regularizes the structural as well as the feature information. We design two sampling algorithms for structural regularization and instantiate the GIB principle with two new models: GIB-Cat and GIB-Bern, and demonstrate the benefits by evaluating the resilience to adversarial attacks. We show that our proposed models are more robust than state-of-theart graph defense models. GIB-based models empirically achieve up to 31% improvement with adversarial perturbation of the graph structure as well as node features.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Representation learning on graphs aims to learn representations of graph-structured data for downstream tasks such as node classification and link prediction <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Graph representation learning is a challenging task since both node features as well as graph structure carry important information <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. Graph Neural Networks (GNNs) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref> have demonstrated impressive performance, by learning to fuse information from both the node features and the graph structure <ref type="bibr" target="#b7">[8]</ref>.</p><p>Recently, many works have been focusing on developing more powerful GNNs <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>, in a sense that they can fit more complex graph-structured data. However, at present GNNs still suffer from a few problems. For example, the features of a neighborhood node can contain non-useful information that may negatively impact the prediction of the current node <ref type="bibr" target="#b13">[14]</ref>. Also, GNN's reliance on message passing over the edges of the graph also makes it prone to noise and adversarial attacks that target at the graph structure <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>Here we address the above problems and rethink what is a "good" representation for graph-structured data. In particular, the Information Bottleneck (IB) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> provides a critical principle for representation learning: an optimal representation should contain the minimal sufficient information for the downstream prediction task. IB encourages the representation to be maximally informative about the target to make the prediction accurate (sufficient). On the other hand, IB also discourages the representation from acquiring additional information from the data that is irrelevant for predicting the Figure <ref type="figure">1</ref>: Graph Information Bottleneck is to optimize the representation Z to capture the minimal sufficient information within the input data D = (A, X) to predict the target Y . D includes information from both the graph structure A and node features X. When Z contains irrelevant information from either of these two sides, it overfits the data and is prone to adversarial attacks and model hyperparameter change. â„¦ defines the search space of the optimal model P(Z|D). I(â€¢; â€¢) denotes the mutual information <ref type="bibr" target="#b16">[17]</ref>.</p><p>target (minimal). Based on this learning paradigm, the learned model naturally avoids overfitting and becomes more robust to adversarial attacks.</p><p>However, extending the IB principle to representation learning on graph-structured data presents two unique challenges. First, previous models that leverage IB assume that the training examples in the dataset are independent and identically distributed (i.i.d.). For graph-structured data, this assumption no longer holds and makes model training in the IB principle hard. Moreover, the structural information is indispensable to represent graph-structured data, but such information is discrete and thus hard to optimize over. How to properly model and extract minimal sufficient information from the graph structure introduces another challenge that has not been yet investigated when designing IB-based models.</p><p>We introduce Graph Information Bottleneck (GIB), an information-theoretic principle inherited from IB, adapted for representation learning on graph-structured data. GIB extracts information from both the graph structure and node features and further encourages the information in learned representation to be both minimal and sufficient (Fig. <ref type="figure">1</ref>). To overcome the challenge induced by non-i.i.d. data, we further leverage local-dependence assumption of graph-structure data to define a more tractable search space â„¦ of the optimal P(Z|D) that follows a Markov chain to hierarchically extract information from both features and structure. To our knowledge, our work provides the first information-theoretic principle for supervised representation learning on graph-structured data.</p><p>We also derive variational bounds for GIB, making GIB tractable and amenable for the design and optimization of GNNs. Specifically, we propose a variational upper bound for constraining the information from the node features and graph structure, and a variational lower bound for maximizing the information in the representation to predict the target.</p><p>We demonstrate the GIB principle by applying it to the Graph Attention Networks (GAT) <ref type="bibr" target="#b4">[5]</ref>, where we leverage the attention weights of GAT to sample the graph structure in order to alleviate the difficulty of optimizing and modeling the discrete graph structure. We also design two sampling algorithms based on the categorical distribution and Bernoulli distribution, and propose two models GIB-Cat and GIB-Bern. We show that both models consistently improve robustness w.r.t. standard baseline models, and outperform other state-of-the-art defense models. GIB-Cat and GIB-Bern improve the classification accuracy by up to 31.3% and 34.0% under adversarial perturbation, respectively. Project website and code can be found at http://snap.stanford.edu/gib/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries and Notation</head><p>Graph Representation Learning. Consider an undirected attributed graph G = (V, E, X) with n nodes, where V = [n] = {1, 2, ...n} is the node set, E âŠ† V Ã— V is the edge set and X âˆˆ R nÃ—f includes the node attributes. Let A âˆˆ R nÃ—n denote the adjacency matrix of G, i.e., A uv = 1 if (u, v) âˆˆ E or 0 otherwise. Also, let d(u, v) denote the shortest path distance between two nodes u, v (âˆˆ V ) over A. Hence our input data can be overall represented as D = (A, X).</p><p>In this work, we focus on node-level tasks where nodes are associated with some labels Y âˆˆ [K] n . Our task is to extract node-level representations Z X âˆˆ R nÃ—f from D such that Z X can be further</p><formula xml:id="formula_0">ğ‘Œ ğ‘ # (%) ğ´ ğ‘ ( ()) ğ‘ ( (*) ğ‘ # ()) ğ‘ # (*) â€¦ ğ‘ # (+)</formula><p>, ğ‘Œ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compression</head><p>Prediction structural information flow feature information flow</p><formula xml:id="formula_1">= ğ‘‹ v u 1 u 2 u 3 ğ‘ #,0 (12)) ğ‘ #,3! (12)) ğ‘ #,3" (12)) ğ‘ #,3# (12)) v u 1 u 2 u 3 The original structure ğ´ Local extraction of structural info. : ğ‘ ( (1) v u 1 u 2 u 3</formula><p>Local extraction of feature info.:</p><formula xml:id="formula_2">ğ‘ #,0 (12)) â†’ ğ‘ #,0<label>(1)</label></formula><p>ğ‘ " X of the blue nodes and A that conveys the structural information that the blue nodes lie within 2-hops of the black node, the representations Z (l+1) X are independent between the black node and the white nodes. However, the correlation between them may be established in</p><formula xml:id="formula_3">($%&amp;) ğ‘ " ($%() ğ‘ " ($)</formula><formula xml:id="formula_4">Z (l+2) X .</formula><p>used to predict Y . We also use the subscript with a certain node v âˆˆ V to denote the affiliation with node v. For example, the node representation of v is denoted by Z X,v and its label is denoted by Y v .</p><p>Notation. We do not distinguish the notation of random variables and of their particular realizations if there is no risk of confusion. For any set of random variables H, we use P(H), Q(H), ... to denote joint probabilistic distribution functions (PDFs) of the random variables in H under different models. P(â€¢) corresponds to the induced PDF of the proposed model while Q(H) and Q i (H), i âˆˆ N correspond to some other distributions, typically variational distributions. For discrete random variables, we use generalized PDFs that may contain the Dirac delta functions <ref type="bibr" target="#b19">[20]</ref>. In this work, if not specified, E[H] means the expectation over all the random variables in H w.r.t. P(H). Otherwise, we use E Q(H) <ref type="bibr">[H]</ref> to specify the expectation w.r.t. other distributions denoted by Q(H). We also use X 1 âŠ¥ X 2 |X 3 to denote that X 1 and X 2 are conditionally independent given X 3 . Let Cat(Ï†), Bernoulli(Ï†) denote the categorical distribution and Bernoulli distribution respectively with parameter Ï† (âˆˆ R 1Ã—C â‰¥0 ). For the categorical distribution, Ï† corresponds to the probabilities over different categories and thus Ï† 1 = 1. For the Bernoulli distribution, we generalize it to high dimensions and assume we have C independent components and each element of Ï† is between 0 and 1. Let Gaussian(Âµ, Ïƒ 2 ) denote the Gaussian distribution with mean Âµ and variance Ïƒ 2 . Âµ and Ïƒ 2 could be vectors with the same dimension, in which case the Gaussian distribution is with the mean vector Âµ and covariance matrix Î£ = diag(Ïƒ 2 ). Let Î¦(â€¢ : Âµ, Ïƒ 2 ) denote its PDF. We use [i 1 : i 2 ] to slice a tensor w.r.t. indices from i 1 to i 2 âˆ’ 1 of its last dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Graph Information Bottleneck</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Deriving the Graph Information Bottleneck Principle</head><p>In general, the graph information bottleneck (GIB) principle, inheriting from the principle of information bottleneck (IB), requires the node representation Z X to minimize the information from the graph-structured data D (compression) and maximize the information to Y (prediction). However, optimization for the most general GIB is challenging because of the correlation between data points. The i.i.d. assumption of data points is typically used to derive variational bounds and make accurate estimation of those bounds to learn IB-based models <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. However, for the graph-structured data D, this is impossible as node features, i.e., different rows of X, may be correlated due to the underlying graph structure A. To fully capture such correlation, we are not allowed to split the whole graph-structured data D w.r.t. each node. In practice, we typically have only a large network, which indicates that only one single realization of P(D) is available. Hence, approximating the optimal Z X in the general formulation GIB seems impossible without making additional assumptions.</p><p>Here, we rely on a widely accepted local-dependence assumption for graph-structured data: Given the data related to the neighbors within a certain number of hops of a node v, the data in the rest of the graph will be independent of v. We use this assumption to constrain the space â„¦ of optimal representations, which leads to a more tractable GIB principle. That is, we assume that the optimal representation follows the Markovian dependence shown in Fig. <ref type="figure" target="#fig_1">2</ref>. Specifically, P(Z X |D) iterates node representations to hierarchically model the correlation. In each iteration l, the local-dependence assumption is used: The representation of each node will be refined by incorporating its neighbors </p><p>A } 1â‰¤lâ‰¤L is obtained by locally adjusting the original graph structure A and essentially controlling the information flow from A. Finally, we will make predictions based on Z (L) X . Based on this formulation, the objective reduces to the following optimization:</p><formula xml:id="formula_6">min P(Z (L) X |D)âˆˆâ„¦ GIB Î² (D, Y ; Z (L) X ) âˆ’I(Y ; Z (L) X ) + Î²I(D; Z (L) X )<label>(1)</label></formula><p>where â„¦ characterizes the space of the conditional distribution of Z (L)</p><p>X given the data D by following the probabilistic dependence shown in Fig. <ref type="figure" target="#fig_1">2</ref>. In this formulation, we just need to optimize two series of distributions P(Z</p><formula xml:id="formula_7">(l) X |Z (lâˆ’1) X , Z<label>(l)</label></formula><p>A ) and P(Z</p><formula xml:id="formula_8">(l) A |Z (lâˆ’1) X , A), l âˆˆ [L],</formula><p>which have local dependence between nodes and thus are much easier to be parameterized and optimized.</p><p>Variational Bounds. Even using the reduced GIB principle and some proper parameterization of P(Z</p><formula xml:id="formula_9">(l) X |Z (lâˆ’1) X , Z<label>(l)</label></formula><p>A ) and P(Z</p><formula xml:id="formula_10">(l) A |Z (lâˆ’1) X , A), l âˆˆ [L], exact computation of I(Y ; Z (L) X ) and I(D; Z (L)</formula><p>X ) is still intractable. Hence, we need to introduce variational bounds on these two terms, which leads to the final objective to optimize. Note that variational methods are frequently used in model optimization under the traditional IB principle <ref type="bibr" target="#b20">[21]</ref>. However, we should be careful to derive these bounds as the data points now are correlated. We introduce a lower bound of I(Y ; Z (L) X ), which is reproduced from <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>, and an upper bound of I(D; Z </p><formula xml:id="formula_11">(L) X )). For any distributions Q 1 (Y v |Z (L) X,v ) for v âˆˆ V and Q 2 (Y ), we have I(Y ; Z (L) X ) â‰¥ 1 + E log vâˆˆV Q 1 (Y v |Z (L) X,v ) Q 2 (Y ) + E P(Y )P(Z (L) X ) vâˆˆV Q 1 (Y v |Z (L) X,v ) Q 2 (Y ) (2) Proposition 3.2 (The upper bound of I(D; Z (L) X )). We choose two groups of indices S X , S A âŠ‚ [L] such that D âŠ¥ Z (L) X |{Z (l) X } lâˆˆS X âˆª {Z (l)</formula><p>A } lâˆˆS A based on the Markovian dependence in Fig. <ref type="figure" target="#fig_1">2</ref>, and then for any distributions Q(Z (l) X ), l âˆˆ S X , and Q(Z</p><formula xml:id="formula_12">(l) A ), l âˆˆ S A , I(D; Z (L) X ) â‰¤ I(D; {Z (l) X } lâˆˆS X âˆª {Z (l) A } lâˆˆS A ) â‰¤ lâˆˆS A AIB (l) + lâˆˆS X XIB (l) , where<label>(3)</label></formula><formula xml:id="formula_13">AIB (l) = E log P(Z (l) A |A, Z (lâˆ’1) X ) Q(Z (l) A ) , XIB (l) = E log P(Z (l) X |Z (lâˆ’1) X , Z (l) A ) Q(Z (l) X ) ,<label>(4)</label></formula><p>The proofs are given in Appendix B and C. Proposition 3.2 indicates that we need to select a group of random variables with index sets S X and S A to guarantee the conditional independence between D and Z (L)</p><p>X . Note that S X and S A that satisfy this condition have the following properties: (1) S X = âˆ…, and (2) suppose the greatest index in S X is l and then S A should contain all integers in [l + 1, L].</p><p>To use GIB, we need to model P(Z</p><formula xml:id="formula_14">(l) A |Z (lâˆ’1) X , A) and P(Z (l) X |Z (lâˆ’1) X , Z (l) A ). Then, we choose some variational distributions Q(Z (l) X ) and Q(Z (l)</formula><p>A ) to estimate the corresponding AIB (l) and XIB (l) for regularization, and some</p><formula xml:id="formula_15">Q 1 (Y v |Z (L) X,v ) and Q 2 (Y )</formula><p>to specify the lower bound in Eq. ( <ref type="formula">2</ref>). Then, plugging Eq. ( <ref type="formula">2</ref>) and Eq. ( <ref type="formula" target="#formula_12">3</ref>) into the GIB principle (Eq. ( <ref type="formula" target="#formula_2">1</ref>)), one obtains an upper bound on the objective to optimize. Note that any model that parameterizes P(Z</p><formula xml:id="formula_16">(l) A |Z (lâˆ’1) X , A) and P(Z (l) X |Z (lâˆ’1) X , Z<label>(l)</label></formula><p>A ) can use GIB as the objective in training. In the next subsection, we will introduce two instantiations of GIB, which is inspired by GAT <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Instantiating the GIB Principle</head><p>The GIB principle can be applied to many GNN models. As an example, we apply it to the Graph Attention Network model <ref type="bibr" target="#b4">[5]</ref> and present GIB-Cat and GIB-Bern. Algorithm 1 illustrates the base framework of both models with different neighbor sampling methods shown in Algorithm 2 and 3. In each layer, GIB-Cat and GIB-Bern need to first refine the graph structure using the attention weights to obtain Z (Steps 4-7). Concretely, we design two algorithms for neighbor sampling, which respectively use the categorical distribution and the Bernoulli distribution. For the categorical version, we view the attention weights as the parameters of categorical distributions to sample the refined graph structure to extract structural information. We sample k neighbors with replacement from the pool of nodes V vt for each node v, where V vt includes the nodes whose shortest-path-distance to v over A is t. We use T as an upper limitation of t to encode the local-dependence assumption of the GIB principle, which also benefits the scalability of the model. For the Bernoulli version, we model each pair of node v and its neighbors independently with a Bernoulli distribution parameterized by the attention weights. Note that here we did not normalize it with the softmax function as in the categorical version, however, we use the sigmoid function to squash it between 0 and 1. Here we do not need to specify the number of neighbors one node sample (k in the categorical version). Step 4 is sum-pooling of the neighbors, and the output will be used to compute the parameters for a Gaussian distribution where the refined node representations will be sampled. Note that we may also use a mechanism similar to multi-head attention <ref type="bibr" target="#b4">[5]</ref>: We split Z(lâˆ’1)</p><p>X into different channels w.r.t. its last dimension, perform Steps 2-7 independently for each channel and then concatenate the output of different channels to obtain new Z (l) X . Moreover, when training the model, we adopt reparameterization trick for Steps 3 and 7: Step 3 uses Gumbel-softmax <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> while Step 7 uses</p><formula xml:id="formula_17">áº(l) X,v = Âµ (l) v + Ïƒ (l) v</formula><p>z where z âˆ¼ Gaussian(0, I), z âˆˆ R 1Ã—f and is element-wise product.</p><p>Algorithm 1: Framework of GIB-Cat and GIB-Bern Input: The dataset D = (X, A); T : An integral limitation to impose local dependence; k: The number of neighbors to be sampled. Ï„ : An element-wise nonlinear rectifier. Initialize:</p><formula xml:id="formula_18">Z (0) X â† X; For all v âˆˆ V, t âˆˆ [T ], construct sets V vt â† {u âˆˆ V |d(u, v) = t}; Weights: a âˆˆ R T Ã—4f , W (1) âˆˆ R f Ã—2f , W (l) âˆˆ R f Ã—2f , for l âˆˆ [2, L], W out âˆˆ R f Ã—K . Output: Z (L) X , Å¶v = softmax(Z (L) X,v W out ) 1. For layers l = 1, ..., L and For v âˆˆ V , do: 2. Z(lâˆ’1) X,v â† Ï„ (Z (lâˆ’1) X,v )W (l) 3. Z (l) A,v â† NeighborSample(Z lâˆ’1 X , T , V vt , a) 4. Z(l) X,v â† uâˆˆZ (l) A,v Z(lâˆ’1) X,v 5. Âµ (l) v â† Z(l) X,v [0 : f ] 6. Ïƒ 2(l) v â† softplus( Z(l) X,v [f : 2f ]) 7. Z (l) X,v âˆ¼ Gaussian(Âµ (l) v , Ïƒ 2(l) v )</formula><p>Properties. Different from traditional GNNs, GIB-Cat and GIB-Bern depend loosely on the graph structure since A is only used to decide the potential neighbors for each node, and we perform message passing based on Z A . This property renders our models extremely robust to structural perturbations/attacks where traditional GNNs are sensitive <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. Both our models also keep robustness to the feature perturbation that is similar to other IB-based DNN models <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26]</ref>. Moreover, the proposed models are invariant to node permutations as we may show that for any permutation matrix Î  âˆˆ R nÃ—n , with permuting</p><formula xml:id="formula_19">A â†’ A Î  = Î AÎ  T , X â†’ X Î  = Î X, the obtained new node rep- resentations Z (L)</formula><p>X,Î  and Î Z (L)</p><p>X share the same distribution (proof in Appendix E). Permutation invariance is known to be important for structural representation learning <ref type="bibr" target="#b12">[13]</ref>. X ) as in Eq. ( <ref type="formula">2</ref>), and further compute the bound of the GIB objective in Eq. (1). To characterize AIB (l) in Eq. (3), we assume Q(Z (l)</p><formula xml:id="formula_20">Algorithm 2: NeighborSample (categorical) Input: Z l X , T , V vt , a, as defined in Alg. 1; Output: Z (l+1) A,v 1.For t âˆˆ [T ], do: 2. Ï† (l) vt â† softmax({( Z(lâˆ’1) X,v âŠ• Z(lâˆ’1) X,u )a T } uâˆˆVvt ) 3. Z (l+1) A,v â† âˆª T t=1 {u âˆˆ V vt |u iid âˆ¼ Cat(Ï† (l) vt ), k times} Algorithm 3: NeighborSample (Bernoulli) Input: Z l X , T , V vt , a, as defined in Alg. 1; Output: Z (l+1) A,v 1.For t âˆˆ [T ], do: 2. Ï† (l) vt â† sigmoid({( Z(lâˆ’1) X,v âŠ• Z(lâˆ’1) X,u )a T } uâˆˆVvt ) 3. Z (l+1) A,v â† âˆª T t=1 {u âˆˆ V vt |u iid âˆ¼ Bernoulli(Ï†<label>(</label></formula><p>A ) is a non-informative distribution <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. Specifically, we use the uniform distribution for the categorical version:</p><formula xml:id="formula_21">Z A âˆ¼ Q(Z A ), Z A,v = âˆª T t=1 {u âˆˆ V vt |u iid âˆ¼ Cat( 1 |Vvt| )} and Z A,v âŠ¥ Z A,u if v = u;</formula><p>and we also adopt a non-informative prior for the Bernoulli version:</p><formula xml:id="formula_22">Z A,v = âˆª T t=1 {u âˆˆ V vt |u iid âˆ¼ Bernoulli(Î±)}</formula><p>, where Î± âˆˆ (0, 1) is a hyperparameter. The difference is that, unlike the categorical distribution, we have an additional degree of freedom provided by Î±. After the model computes Ï† </p><formula xml:id="formula_23">(l) = E P(Z (l) A |A,Z (lâˆ’1) X ) log P(Z (l) A |A, Z (lâˆ’1) X ) Q(Z (l) A )</formula><p>, which is instantiated as follows for the two versions,</p><formula xml:id="formula_24">AIB C (l) = vâˆˆV,tâˆˆ[T ] KL(Cat(Ï† (l) vt )||Cat( 1 |V vt | )) AIB B (l) = vâˆˆV,tâˆˆ[T ] KL(Bernoulli(Ï† (l) vt )||Bernoulli(Î±))</formula><p>To estimate XIB (l) , we set Q(Z</p><p>X ) as a mixture of Gaussians with learnable parameters <ref type="bibr" target="#b26">[27]</ref>. Specifically, for any node v, Z X âˆ¼ Q(Z X ), we set Z X,v âˆ¼ m i=1 w i Gaussian(Âµ 0,i , Ïƒ 2 0,i ) where w i , Âµ 0,i , Ïƒ 0,i are learnable parameters shared by all nodes and Z X,v âŠ¥ Z X,u if v = u. We estimate XIB (l) by using the sampled Z (l)</p><formula xml:id="formula_26">X : XIB (l) = log P(Z (l) X |Z (lâˆ’1) X , Z<label>(l)</label></formula><formula xml:id="formula_27">A ) Q(Z (l) X ) = vâˆˆV log Î¦(Z (l) X,v ; Âµ v , Ïƒ 2 v ) âˆ’ log( m i=1 w i Î¦(Z (l) X,v ; Âµ 0,i , Ïƒ 2 0,i )) .</formula><p>Therefore, in practice, we may select proper sets of indices S X , S A that satisfy the condition in Proposition 3.2 and use substitution</p><formula xml:id="formula_28">I(D; Z (L) X ) â†’ lâˆˆS A AIB (l) + lâˆˆS X XIB (l)<label>(5)</label></formula><p>To characterize Eq. ( <ref type="formula">2</ref>), we may simply set</p><formula xml:id="formula_29">Q 2 (Y ) = P(Y ) and Q 1 (Y v |Z (L) X,v ) = Cat(Z (L)</formula><p>X,v W out ). Then, the RHS of Eq. ( <ref type="formula">2</ref>) reduces to the cross-entropy loss by ignoring constants, i.e.,</p><formula xml:id="formula_30">I(Y ; Z (L) X ) â†’ âˆ’ vâˆˆV Cross-Entropy(Z (L) X,v W out ; Y v )<label>(6)</label></formula><p>Other choices of Q 2 (Y ) may also be adopted and yield the contrastive loss <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28]</ref> (Appendix D). However, in our case, we use the simplest setting to illustrate the benefit of the GIB principle. Plugging Eq. ( <ref type="formula" target="#formula_28">5</ref>) and Eq. ( <ref type="formula" target="#formula_30">6</ref>) into Eq. ( <ref type="formula" target="#formula_2">1</ref>), we obtain the objective to train our models.</p><p>Other Formalizations of the GIB Principle. There are also other alternative formalizations of the GIB principle, especially when modeling P(Z</p><p>A |Z (lâˆ’1) X , A). Generally speaking, any node-pair representations, such as messages over edges in MPNN <ref type="bibr" target="#b28">[29]</ref>, can be leveraged to sample structures. Applying the GIB principle to other architectures is a promising direction for future investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>GNNs learn node-level representations through message passing and aggregation from neighbors <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref>. Several previous works further incorporate the attention mechanism to adaptively learn the correlation between a node and its neighbor <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">32]</ref>. Recent literature shows that representations learned by GNNs are far from robust and can be easily attacked by malicious manipulation on either features or structure <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. Accordingly, several defense models are proposed to increase the robustness by injecting random noise in the representations <ref type="bibr" target="#b32">[33]</ref>, removing suspicious and uninformative edges <ref type="bibr" target="#b33">[34]</ref>, low-rank approximation of the adjacency matrix <ref type="bibr" target="#b34">[35]</ref>, additional hinge loss for certified robustness <ref type="bibr" target="#b35">[36]</ref>. In contrast, even though not specifically designed against adversarial attacks, our model learns robust representations via the GIB principle that naturally defend against attacks. Moreover, none of those defense models has theoretical foundations except <ref type="bibr" target="#b35">[36]</ref> that uses tools of robust optimization instead of information theory.</p><p>Recently several works have applied contrastive loss <ref type="bibr" target="#b27">[28]</ref> as a regularizer for GNNs. The idea is to increase the score for positive samples while decrease the score for negative samples. This can be further formulated as a mutual information maximization term that aims to maximize the mutual information between representations of nodes and their neighbor patches <ref type="bibr" target="#b36">[37]</ref>, between representations of sub-structures and the hidden feature vectors <ref type="bibr" target="#b37">[38]</ref>, between representations of graphs and their sub-structures <ref type="bibr" target="#b38">[39]</ref>. In contrast, our model focuses on the compression of node features and graph structure while at the same time improves prediction, which is orthogonal to these previous works on unsupervised representation learning with information maximization.</p><p>Another line of related work is representation learning with the IB principle. DVIB <ref type="bibr" target="#b20">[21]</ref> first applies IB <ref type="bibr" target="#b17">[18]</ref> to deep neural networks, and shows increased robustness of learned representations. Other methods apply IB to various domains <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>. The difference is that we develop information-theoretic modeling of feature, structure and their fusion on graph-structured data. Furthermore, several works on GNNs <ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref> leverage information maximization <ref type="bibr" target="#b41">[42]</ref> for unsupervised learning. However, we focus on learning robust representations by controlling the information in supervised learning setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>The goal of our experiments is to test whether GNNs trained with the GIB objective are more robust and reliable. Specifically, we consider the following two questions: (1) Boosted by GIB, does GIB-Cat and GIB-Bern learn more robust representations than GAT to defend against attacks? (2) How does each component of GIB contribute to such robustness, especially, to controlling the information from one of the two sides -the structure and node features?</p><p>We compare GIB-Cat and GIB-Bern with baselines including GCN <ref type="bibr" target="#b2">[3]</ref> and GAT <ref type="bibr" target="#b4">[5]</ref>, the most relevant baseline as GIB-Cat and GIB-Bern are to impose the GIB principle over GAT. In addition, we consider two state-of-the-art graph defense models specifically designed against adversarial attacks: GCNJaccard <ref type="bibr" target="#b33">[34]</ref> that pre-processes the graph by deleting the edges between nodes with low feature similarity, and Robust GCN (RGCN) <ref type="bibr" target="#b32">[33]</ref> that uses Gaussian reparameterization for node features and variance-based attention. Note that RGCN essentially includes the term XIB (Eq. ( <ref type="formula" target="#formula_12">3</ref>)) to control the information of node features while it does not have the term AIB (Eq. ( <ref type="formula" target="#formula_12">3</ref>)) to control the structural information. For GCNJaccard and RGCN, we perform extensive hyperparameter search as detailed in Appendix G.3. For GIB-Cat and GIB-Bern, we keep the same architectural component as GAT, and for the additional hyperparameters k and T (Algorithm 1, 2 and 3), we search k âˆˆ {2, 3} and T âˆˆ {1, 2} for each experimental setting and report the better performance. Please see Appendix G for more details.</p><p>We use three citation benchmark datasets: Cora, Pubmed and Citeseer <ref type="bibr" target="#b42">[43]</ref>, in our evaluation. In all experiments, we follow the standard transductive node classification setting and standard trainvalidation-test split as GAT <ref type="bibr" target="#b4">[5]</ref>. The summary statistics of the datasets and their splitting are shown in Table <ref type="table">4</ref> in Appendix F. For all experiments, we perform the experiments over 5 random initializations and report average performance. We always use F1-micro as the validating metric to train our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Robustness Against Adversarial Attacks</head><p>In this experiment, we compare the robustness of different models against adversarial attacks. We use Nettack <ref type="bibr" target="#b14">[15]</ref>, a strong targeted attack technique on graphs that attacks a target node by flipping the edge or node features. We evaluate the models on both evasive and poisoning settings, i.e. the attack happens after or before the model is trained, respectively. We follow the setting of Nettack <ref type="bibr" target="#b14">[15]</ref>: for each dataset, select (i) 10 nodes with highest margin of classification, i.e. they are clearly correctly classified, (ii) 10 nodes with lowest margin but still correctly classified and (iii) 20 more nodes randomly, where for each target node, we train a different model for evaluation. We report the classification accuracy of these 40 targeted nodes. We enumerate the number of perturbations from 1 to 4, where each perturbation denotes a flipping of a node feature or an addition or deletion of an edge. Since Nettack can only operate on Boolean features, we binarize the node features before training.</p><p>Table <ref type="table" target="#tab_0">1</ref> shows the results. We see that compared with GAT, GIB-Cat improves the classification accuracy by an average of 8.9% and 14.4% in Cora and Pubmed, respectively, and GIB-Bern improves the classification accuracy by an average of 8.4% and 14.6% in Cora and Pubmed, respectively, which demonstrates the effectiveness of the GIB principle to improve the robustness of GNNs. Remarkably, when the number of perturbation is 1, GIB-Cat and GIB-Bern boost accuracy over GAT (as well as other models) by 31.3% and 34.0% in Pubmed, respectively. GIB-Cat also outperforms GCNJaccard and RGCN by an average of 10.3% and 12.3% on Cora (For GIB-Bern, it is 9.8% and 11.7%), and by an average of 15.0% and 14.6% on Pubmed (For GIB-Bern, it is 15.2% and 14.8%), although GIB-Cat and GIB-Bern are not intentionally designed to defend attacks. For Citeseer, GIB-Cat and GIB-Bern's performance are worse than GCNJaccard in the poisoning setting. This is because Citeseer has much more nodes with very few degrees, even fewer than the number of specified perturbations, as shown in Table <ref type="table" target="#tab_2">13</ref> in Appendix J. In this case, the most effective attack is to connect the target node to a node from a different class with very different features, which exactly matches the assumption used by GCNJaccard <ref type="bibr" target="#b33">[34]</ref>. GCNJaccard proceeds to delete edges with dissimilar node features, resulting in the best performance in Citeseer. However, GIB does not depend on such a restrictive assumption. More detailed analysis is at Appendix J.</p><p>Ablation study. To see how different components of GIB contribute to the performance, we perform ablation study on Cora, as shown in Table <ref type="table" target="#tab_1">2</ref>. Here, we use AIB-Cat and AIB-Bern to denote the models that only sample structures with AIB (Eq. ( <ref type="formula" target="#formula_28">5</ref>)) in the objective (whose NeighborSample() function is identical to that of GIB-Cat and GIB-Bern, respectively), and use XIB to denote the model that only samples node representations with XIB (Eq. ( <ref type="formula" target="#formula_28">5</ref>)) in the objective. We see that the AIB (structure) contributes significantly to the improvement of GIB-Cat and GIB-Bern, and on average, AIB-Cat (AIB-Bern) only underperforms GIB-Cat (GIB-Bern) by 0.9% (0.4%). The performance gain is due to the attacking style of Nettack, as the most effective attack is typically via structural perturbation <ref type="bibr" target="#b14">[15]</ref>, as is also confirmed in Appendix J. Therefore, next we further investigate the case that only perturbation on node features is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Only Feature Attacks</head><p>To further check the effectiveness of IB for node features, we inject random perturbation into the node features. Specifically, after the models are trained, we add independent Gaussian noise to each dimension of the node features for all nodes with increasing amplitude. Specifically, we use the mean of the maximum value of each node's feature as the reference amplitude r, and for each feature dimension of each node we add Gaussian noise Î» â€¢ r â€¢ , where âˆ¼ N (0, 1), and Î» is the feature noise ratio. We test the models' performance with Î» âˆˆ {0.5, 1, 1.5}. Table <ref type="table" target="#tab_2">3</ref> shows the results. We see across different feature noise ratios, both GIB-Cat and GIB-Bern consistently outperforms other models without IB, especially when the feature noise ratio is large (Î» = 1.5), and the AIB models with only structure IB performs slightly worse or equivalent to the GIB models. This shows that GIB makes the model more robust when the feature attack becomes the main source of perturbation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Discussion</head><p>In this work, we have introduced Graph Information Bottleneck (GIB), an information-theoretic principle for learning representations that capture minimal sufficient information from graph-structured data. We have also demonstrated the efficacy of GIB by evaluating the robustness of the GAT model trained under the GIB principle on adversarial attacks. Our general framework leaves many interesting questions for future investigation. For example, are there any other better instantiations of GIB, especially in capturing discrete structural information? If incorporated with a node for global aggregation, can GIB break the limitation of the local-dependence assumption? May GIB be applied to other graph-related tasks including link prediction and graph classification?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>Who may benefit from this research: Graphs have been used to represent a vast amount of realworld data from social science <ref type="bibr" target="#b43">[44]</ref>, biology <ref type="bibr" target="#b44">[45]</ref>, geographical mapping <ref type="bibr" target="#b45">[46]</ref>, finances <ref type="bibr" target="#b46">[47]</ref> and recommender systems <ref type="bibr" target="#b47">[48]</ref>, because of their flexibility in modeling both the relation among the data (structures) and the content of the data (features). Graph neural networks (GNN), naturally entangle both aspects of the data in the most expressive way, have attracted unprecedented attention from both academia and industry across a wide range of disciplines. However, GNNs share a common issue with other techniques based on neural networks. They are very sensitive to noise of data and are fragile to model attacks. This drawback yields the potential safety problems to deploy GNNs in the practical systems or use them to process data in those disciplines that heavily emphasize unbiased analysis. The Graph Information Bottleneck (GIB) principle proposed in this work paves a principled way to alleviate the above problem by increasing the robustness of GNN models. Our work further releases the worries about the usage of GNN techniques in practical systems, such as recommender systems, social media, or to analyze data for other disciplines, including physics, biology, social science. Ultimately, our work increases the interaction between AI, machine learning techniques and other aspects of our society, and could achieve far-reaching impact. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3 Implementation Details for RGCN and GCNJaccard</head><p>We used the implementation in this repository: https://github.com/DSE-MSU/DeepRobust. We perform hyperparameter tuning for both baselines for the adversarial attack experiment in Section 5.1. We first tune the latent dimension, learning rate, weight decay for both models. Specifically, we search within {16, 32, 64, 128} for latent dimension, {10 âˆ’3 , 10 âˆ’2 , 10 âˆ’1 } for learning rate, and {10 âˆ’4 , 5 Ã— 10 âˆ’4 , 10 âˆ’3 } for weight decay. For GCNJaccard, we additionally fine-tune the threshold hyperparameter which is used to decide whether two neighbor nodes are still connected. We search threshold within 0.05}. For RGCN, we additionally fine-tune the Î² 1 within {10 âˆ’4 , 5 Ã— 10 âˆ’4 , 10 âˆ’3 } and Î³ within {0.1, 0.3, 0.5, 0.9}. Please find the best set of hyperparameters for both models in Table <ref type="table" target="#tab_7">9</ref>, 10 and 11. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.4 Additional Details for Adversarial Attack Experiment</head><p>We use the implementation of Nettack <ref type="bibr" target="#b14">[15]</ref> in the repository https://github.com/DSE-MSU/ DeepRobust with default settings. As stated in the main text, for each dataset we select 40 nodes in the test set to attack with 10 having the highest margin of classification, 10 having the lowest margin of classification (but still correctly classified), and 20 random nodes. For each target node, we independently train a different model and evaluate its performance on the target node in both evasive and poisoning setting. Different from <ref type="bibr" target="#b14">[15]</ref> that only keeps the largest connected component of the graph and uses random split, to keep consistent settings across experiments, we still use the full graph and standard split, which makes the defense even harder than that in <ref type="bibr" target="#b14">[15]</ref>. For each dataset and each number of perturbations (1, 2, 3, 4), we repeat the above experiment 5 times with random seeds 0, 1, 2, 3, 4, and report the average accuracy on the targeted nodes (therefore, each cell in From the above summary, we see that the target nodes in Citeseer dataset in general have fewest degrees, which are most prone to added-edge structural attacks by connecting nodes with different classes. This exactly satisfies the assumption of GCNJaccard <ref type="bibr" target="#b33">[34]</ref>. GCNJaccard proceeds by deleting edges with low feature similarity, so those added edges are not likely to enter into the model training during poisoning attacks. This is probably the reason why in Nettack poisoning mode in Citeseer, GCNJaccard has the best performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Y</head><label></label><figDesc>: The target, D: The input data (= (A, X)) A: The graph structure, X: The node features Z: The representation Graph Information Bottleneck: min P(Z|D)âˆˆâ„¦ GIB Î² (D, Y ; Z) [âˆ’I(Y ; Z) + Î²I(D; Z)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Our GIB principle leverages local-dependence assumption. (a) The Markov chain defines the search space â„¦ of our GIB principle, of which each step uses a local-dependence assumption to extract information from the structure and node features. The correlation between node representations are established in a hierarchical way: Suppose local dependence appears within 2-hops given the structure A. (b) In the graph, given the representations Z (l)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>X</head><label></label><figDesc>), as shown in Propositions 3.1 and 3.2. Proposition 3.1 (The lower bound of I(Y ; Z</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(l) A (Step 3) and then refines node representations Z (l) X by propagating Z (lâˆ’1) X over Z (l) A</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>l) vt )} Objective for training. To optimize the parameters of the model, we need to specify the bounds for I(D; Z (L) X ) as in Eq. (3) and I(Y ; Z (L)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(l) vt according to Step 4, we get an empirical estimation of AIB (l) : AIB</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Average classification accuracy (%) for the targeted nodes under direct attack. Each number is the average accuracy for the 40 targeted nodes for 5 random initialization of the experiments. Bold font denotes top two models.</figDesc><table><row><cell></cell><cell>Model</cell><cell>Clean (%)</cell><cell>1</cell><cell>Evasive (%) 2 3</cell><cell>4</cell><cell>1</cell><cell>Poisoning (%) 2 3</cell><cell>4</cell></row><row><cell></cell><cell>GCN</cell><cell>80.0Â±7.87</cell><cell cols="6">51.5Â±4.87 38.0Â±6.22 31.0Â±2.24 26.0Â±3.79 47.5Â±7.07 39.5Â±2.74 30.0Â±5.00 26.5Â±3.79</cell></row><row><cell>Cora</cell><cell>GCNJaccard RGCN GAT</cell><cell>75.0Â±5.00 80.0Â±4.67 77.8Â±3.97</cell><cell cols="6">48.5Â±6.75 36.0Â±6.51 32.0Â±3.25 30.0Â±3.95 47.0Â±7.37 38.0Â±6.22 33.5Â±3.79 28.5Â±3.79 49.5Â±6.47 36.0Â±5.18 30.5Â±3.25 25.5Â±2.09 46.5Â±5.75 35.5Â±3.70 29.0Â±3.79 25.5Â±2.73 48.0Â±8.73 39.5Â±5.70 36.5Â±5.48 32.5Â±5.30 50.5Â±5.70 38.0Â±5.97 33.5Â±2.85 26.0Â±3.79</cell></row><row><cell></cell><cell>GIB-Cat</cell><cell>77.6Â±2.84</cell><cell cols="6">63.0Â±4.81 52.5Â±3.54 44.5Â±5.70 36.5Â±6.75 60.0Â±6.37 50.0Â±2.50 39.5Â±5.42 30.0Â±3.95</cell></row><row><cell></cell><cell>GIB-Bern</cell><cell>78.4Â±4.07</cell><cell cols="6">64.0Â±5.18 51.5Â±4.54 43.0Â±3.26 37.5Â±3.95 61.5Â±4.18 46.0Â±4.18 36.5Â±4.18 31.5Â±2.85</cell></row><row><cell>Pubmed</cell><cell>GCN GCNJaccard RGCN GAT GIB-Cat</cell><cell>82.6Â±6.98 82.0Â±7.15 79.0Â±5.18 78.6Â±6.70 85.1Â±6.90</cell><cell cols="6">39.5Â±4.81 32.0Â±4.81 31.0Â±5.76 31.0Â±5.76 36.0Â±4.18 32.5Â±6.37 31.0Â±5.76 28.5Â±5.18 37.5Â±5.30 31.5Â±5.18 30.0Â±3.95 30.0Â±3.95 36.0Â±3.79 32.5Â±4.67 31.0Â±4.87 28.5Â±4.18 39.5Â±5.70 33.0Â±4.80 31.5Â±4.18 30.0Â±5.00 38.5Â±4.18 31.5Â±2.85 29.5Â±3.70 27.0Â±3.70 41.0Â±8.40 33.5Â±4.18 30.5Â±4.47 31.0Â±4.18 39.5Â±3.26 31.0Â±4.18 30.0Â±3.06 25.5Â±5.97 72.0Â±3.26 51.0Â±5.18 37.5Â±5.30 31.5Â±4.18 71.0Â±4.87 48.0Â±3.26 37.5Â±1.77 28.5Â±2.24</cell></row><row><cell></cell><cell>GIB-Bern</cell><cell>86.2Â±6.54</cell><cell cols="6">76.0Â±3.79 50.5Â±4.11 37.5Â±3.06 31.5Â±1.37 72.5Â±4.68 48.0Â±2.74 36.0Â±2.85 26.5Â±2.85</cell></row><row><cell></cell><cell>GCN</cell><cell>71.8Â±6.94</cell><cell cols="6">42.5Â±7.07 27.5Â±6.37 18.0Â±3.26 15.0Â±2.50 29.0Â±7.20 20.5Â±1.12 17.5Â±1.77 13.0Â±2.09</cell></row><row><cell>Citeseer</cell><cell>GCNJaccard RGCN GAT GIB-Cat</cell><cell>72.5Â±9.35 73.5Â±8.40 72.3Â±8.38 68.6Â±4.90</cell><cell cols="6">41.0Â±6.75 32.5Â±3.95 20.5Â±3.70 13.0Â±1.11 42.5Â±5.86 30.5Â±5.12 17.5Â±1.76 14.0Â±1.36 41.5Â±7.42 24.5Â±6.47 18.5Â±6.52 13.0Â±1.11 31.0Â±5.48 19.5Â±2.09 13.5Â±2.85 5.00Â±1.77 49.0Â±9.12 33.0Â±5.97 22.0Â±4.81 18.0Â±3.26 38.0Â±5.12 23.5Â±4.87 16.5Â±4.54 12.0Â±2.09 51.0Â±4.54 39.0Â±4.18 32.0Â±4.81 26.5Â±4.54 30.0Â±9.19 14.0Â±5.76 9.50Â±3.26 6.50Â±2.24</cell></row><row><cell></cell><cell>GIB-Bern</cell><cell>71.8Â±5.03</cell><cell cols="6">49.0Â±7.42 37.5Â±7.71 32.5Â±4.68 23.5Â±7.42 35.0Â±6.37 19.5Â±4.81 11.5Â±3.79 6.00Â±2.85</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Average classification accuracy (%) for the ablations of GIB-Cat and GIB-Bern on Cora dataset.</figDesc><table><row><cell>Model</cell><cell>Clean (%)</cell><cell>1</cell><cell>Evasive (%) 2 3</cell><cell>4</cell><cell>1</cell><cell>Poisoning (%) 2 3</cell><cell>4</cell></row><row><cell>XIB</cell><cell>76.3Â±2.90</cell><cell cols="6">57.0Â±5.42 47.5Â±7.50 39.5Â±6.94 33.0Â±3.71 54.5Â±2.09 41.0Â±3.79 36.0Â±5.18 31.0Â±4.54</cell></row><row><cell>AIB-Cat</cell><cell>78.7Â±4.95</cell><cell cols="6">62.5Â±5.86 51.5Â±5.18 43.0Â±3.26 36.0Â±3.35 60.5Â±3.26 47.5Â±5.00 36.0Â±3.35 31.5Â±6.27</cell></row><row><cell>AIB-Bern</cell><cell>79.9Â±3.78</cell><cell cols="6">64.0Â±4.50 51.5Â±6.50 42.0Â±5.40 37.0Â±5.70 58.5Â±3.80 46.0Â±4.50 39.0Â±4.20 30.0Â±3.10</cell></row><row><cell>GIB-Cat</cell><cell>77.6Â±2.84</cell><cell cols="6">63.0Â±4.81 52.5Â±3.54 44.5Â±5.70 36.5Â±6.75 60.0Â±6.37 50.0Â±2.50 39.5Â±5.42 30.0Â±3.95</cell></row><row><cell>GIB-Bern</cell><cell>78.4Â±4.07</cell><cell cols="6">64.0Â±5.18 51.5Â±4.54 43.0Â±3.26 37.5Â±3.95 61.5Â±4.18 46.0Â±4.18 36.5Â±4.18 31.5Â±2.85</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Classification F1-micro (%) for the trained models with increasing additive feature noise. Bold font denotes top 2 models.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>Feature noise ratio (Î») 0.5 1 1.5</cell></row><row><cell></cell><cell>GCN</cell><cell>64.0Â±2.05 41.3Â±2.05 31.4Â±2.81</cell></row><row><cell></cell><cell cols="2">GCNJaccard 61.1Â±2.18 41.2Â±2.28 31.8Â±2.63</cell></row><row><cell>Cora</cell><cell>RGCN GAT</cell><cell>57.7Â±2.27 39.1Â±1.58 29.6Â±2.47 62.5Â±1.97 41.7Â±2.32 29.8Â±2.98</cell></row><row><cell></cell><cell>AIB-Cat</cell><cell>67.9Â±2.65 49.6Â±5.35 38.4Â±5.06</cell></row><row><cell></cell><cell>AIB-Bern</cell><cell>68.8Â±1.85 49.0Â±2.87 37.1Â±4.47</cell></row><row><cell></cell><cell>GIB-Cat</cell><cell>67.1Â±2.21 49.1Â±3.67 37.5Â±4.76</cell></row><row><cell></cell><cell>GIB-Bern</cell><cell>69.0Â±1.91 51.3Â±2.62 38.9Â±3.38</cell></row><row><cell></cell><cell>GCN</cell><cell>61.3Â±1.52 50.2Â±2.08 44.3Â±1.43</cell></row><row><cell></cell><cell cols="2">GCNJaccard 62.7Â±1.25 51.9Â±1.53 45.1Â±2.04</cell></row><row><cell>Pubmed</cell><cell>RGCN GAT</cell><cell>58.4Â±1.74 49.0Â±1.65 43.9Â±1.29 62.7Â±1.68 50.2Â±2.35 43.7Â±2.43</cell></row><row><cell></cell><cell>AIB-Cat</cell><cell>64.5Â±2.13 50.9Â±3.83 43.0Â±3.73</cell></row><row><cell></cell><cell>AIB-Bern</cell><cell>61.1Â±2.70 47.8Â±3.65 42.0Â±4.21</cell></row><row><cell></cell><cell>GIB-Cat</cell><cell>67.1Â±4.33 57.2Â±5.27 51.5Â±4.84</cell></row><row><cell></cell><cell>GIB-Bern</cell><cell>64.9Â±2.52 54.7Â±1.83 48.2Â±2.10</cell></row><row><cell></cell><cell>GCN</cell><cell>55.9Â±1.33 40.6Â±1.83 32.8Â±2.19</cell></row><row><cell></cell><cell cols="2">GCNJaccard 56.8Â±1.49 41.3Â±1.81 33.1Â±2.27</cell></row><row><cell>Citeseer</cell><cell>RGCN GAT</cell><cell>51.4Â±2.00 36.5Â±2.38 29.5Â±2.17 55.8Â±1.43 40.8Â±1.77 33.8Â±1.93</cell></row><row><cell></cell><cell>AIB-Cat</cell><cell>55.1Â±1.26 43.1Â±2.46 35.6Â±3.19</cell></row><row><cell></cell><cell>AIB-Bern</cell><cell>55.8Â±2.01 43.3Â±1.67 36.3Â±2.47</cell></row><row><cell></cell><cell>GIB-Cat</cell><cell>54.9Â±1.39 42.0Â±1.92 34.8Â±1.75</cell></row><row><cell></cell><cell>GIB-Bern</cell><cell>54.4Â±5.98 50.3Â±4.33 46.1Â±2.47</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Hyperparameter scope for Section 5.1 and 5.2 for GIB-Cat and GIB-Bern.</figDesc><table><row><cell>Hyperparameters</cell><cell>Value/Search space</cell><cell>Type</cell></row><row><cell>S A</cell><cell>[L]</cell><cell>Fixed  *</cell></row><row><cell>S X</cell><cell>{L âˆ’ 1}</cell><cell>Fixed</cell></row><row><cell>Number m of mixture components for Q(Z X )</cell><cell>100</cell><cell>Fixed</cell></row><row><cell>Î² 1</cell><cell>{0.1, 0.01, 0.001}</cell><cell>Choice  â€ </cell></row><row><cell>Î² 2</cell><cell>{0.1, 0.01}</cell><cell>Choice</cell></row><row><cell>Ï„</cell><cell>{0.05,0.1,1}</cell><cell>Choice</cell></row><row><cell>k</cell><cell>{2, 3}</cell><cell>Choice</cell></row><row><cell>T</cell><cell>{1, 2}</cell><cell>Choice</cell></row><row><cell>*  Fixed: a constant value</cell><cell></cell><cell></cell></row><row><cell>â€  Choice: choose from a set of discrete values</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Hyperparameter for adversarial attack experiment for GIB-Cat and GIB-Bern.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>Î² 1</cell><cell>Î² 2</cell><cell>Ï„</cell><cell>k T</cell></row><row><cell>Cora</cell><cell cols="4">GIB-Cat GIB-Bern 0.001 0.01 0.1 0.001 0.01 1</cell><cell>3 2 -2</cell></row><row><cell>Pubmed</cell><cell cols="4">GIB-Cat GIB-Bern 0.001 0.01 0.1 0.001 0.01 1</cell><cell>3 2 -2</cell></row><row><cell>Citeseer</cell><cell cols="5">GIB-Cat GIB-Bern 0.001 0.01 0.05 -2 0.001 0.01 0.1 2 2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Hyperparameter for adversarial attack experiment for the ablations of GIB-Cat and GIB-Bern.</figDesc><table><row><cell>Model</cell><cell>Î² 1</cell><cell>Î² 2</cell><cell>Ï„</cell><cell>k T</cell></row><row><cell>AIB-Cat</cell><cell>-</cell><cell>0.01</cell><cell>1</cell><cell>3 2</cell></row><row><cell>AIB-Bern</cell><cell>-</cell><cell cols="3">0.01 0.1 -2</cell></row><row><cell>XIB</cell><cell>0.001</cell><cell>-</cell><cell>-</cell><cell>-2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Hyperparameter for feature attack experiment (Section 5.2) for GIB-Cat and GIB-Bern.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>Î² 1</cell><cell>Î² 2</cell><cell>Ï„</cell><cell>k T</cell></row><row><cell>Cora</cell><cell>GIB-Cat AIB-Cat</cell><cell cols="4">0.01 0.01 0.1 2 2 -0.01 0.1 2 2</cell></row><row><cell></cell><cell cols="5">GIB-Bern 0.001 0.01 0.05 -2</cell></row><row><cell></cell><cell>AIB-Bern</cell><cell>-</cell><cell cols="3">0.01 0.05 -2</cell></row><row><cell>Pubmed</cell><cell>GIB-Cat AIB-Cat</cell><cell cols="2">0.001 0.01 -0.01</cell><cell>1 1</cell><cell>3 2 3 2</cell></row><row><cell></cell><cell cols="5">GIB-Bern 0.01 0.01 0.05 -1</cell></row><row><cell></cell><cell>AIB-Bern</cell><cell>-</cell><cell cols="3">0.01 0.05 -1</cell></row><row><cell>Citeseer</cell><cell>GIB-Cat AIB-Cat</cell><cell cols="4">0.001 0.01 0.1 2 2 -0.01 0.1 2 2</cell></row><row><cell></cell><cell>GIB-Bern</cell><cell>0.1</cell><cell cols="3">0.01 0.05 -2</cell></row><row><cell></cell><cell>AIB-Bern</cell><cell>-</cell><cell cols="3">0.01 0.05 -2</cell></row><row><cell cols="3">G.2 Implementation Details for GCN and GAT</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">We follow the default setting of GCN [3] and GAT [5], as implemented in https://github.com/</cell></row></table><note>rusty1s/pytorch_geometric/blob/master/examples/gcn.py and https://github.com/ rusty1s/pytorch_geometric/blob/master/examples/gat.py, respectively. Importantly, we keep the dropout on the attention weights as the original GAT. Whenever possible, we keep the same architecture choice between GAT and GIB-Cat (and GIB-Bern) as detailed in Section G.1, for a fair comparison.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 :</head><label>9</label><figDesc>Hyperparameter of baselines used on Citeseer dataset.</figDesc><table><row><cell></cell><cell>RGCN</cell><cell>GCNJaccard</cell></row><row><cell>latent dim</cell><cell>64</cell><cell>16</cell></row><row><cell>learning rate</cell><cell>10 âˆ’2</cell><cell>10 âˆ’2</cell></row><row><cell cols="2">weight dacay 5 Ã— 10 âˆ’4</cell><cell>5 Ã— 10 âˆ’4</cell></row><row><cell>threshold</cell><cell>-</cell><cell>5 Ã— 10 âˆ’2</cell></row><row><cell>Î² 1</cell><cell>5 Ã— 10 âˆ’4</cell><cell>-</cell></row><row><cell>Î³</cell><cell>0.3</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10 :</head><label>10</label><figDesc>Hyperparameter of baselines used on Cora dataset.</figDesc><table><row><cell></cell><cell>RGCN</cell><cell>GCNJaccard</cell></row><row><cell>latent dim</cell><cell>64</cell><cell>16</cell></row><row><cell>learning rate</cell><cell>10 âˆ’2</cell><cell>10 âˆ’2</cell></row><row><cell cols="2">weight dacay 5 Ã— 10 âˆ’4</cell><cell>5 Ã— 10 âˆ’4</cell></row><row><cell>threshold</cell><cell>-</cell><cell>5 Ã— 10 âˆ’2</cell></row><row><cell>Î² 1</cell><cell>5 Ã— 10 âˆ’4</cell><cell>-</cell></row><row><cell>Î³</cell><cell>0.3</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 11 :</head><label>11</label><figDesc>Hyperparameter of baselines used on Pubmed dataset.</figDesc><table><row><cell></cell><cell>RGCN</cell><cell>GCNJaccard</cell></row><row><cell>latent dim</cell><cell>16</cell><cell>16</cell></row><row><cell>learning rate</cell><cell>10 âˆ’2</cell><cell>10 âˆ’2</cell></row><row><cell cols="2">weight dacay 5 Ã— 10 âˆ’4</cell><cell>5 Ã— 10 âˆ’4</cell></row><row><cell>threshold</cell><cell>-</cell><cell>5 Ã— 10 âˆ’2</cell></row><row><cell>Î² 1</cell><cell>5 Ã— 10 âˆ’4</cell><cell>-</cell></row><row><cell>Î³</cell><cell>0.1</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Table 1 is the mean and std. of the performance of 200 model instances (5 seeds Ã— 40 targeted nodes, each training one model instance). Across the 5 runs of the experiment, the 20 nodes with highest and lowest margin of classification are kept the same, and the 20 random nodes are sampled randomly â€¢ Compared to Cora and Pubmed, Citeseer has much more nodes with degrees less than 1, 2, 3, 4. This explains why in general the 5 models has worse performance in Citeseer than in Cora and Pubmed. â€¢ Almost all attacks (â‰¥ 99.1%) are structural attacks. â€¢ Within structural attacks, most of them (â‰¥ 83.4%) are via adding edges, with Citeseer having the largest fraction. â€¢ For the added edges, almost all of them (â‰¥ 98.5%) have different classes for the end nodes.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>We thank the anonymous reviewers for providing feedback on our manuscript. Hongyu Ren is supported by the Masason Foundation Fellowship. Jure Leskovec is a Chan Zuckerberg Biohub investigator. We also gratefully acknowledge the support of DARPA under Nos. FA865018C7880 (ASED), N660011924033 (MCS); ARO under Nos. W911NF-16-1-0342 (MURI), W911NF-16-1-0171 (DURIP); NSF under Nos. OAC-1835598 (CINES), OAC-1934578 (HDR), CCF-1918940 (Expeditions), IIS-2030477 (RAPID); Stanford Data Science Initiative, Wu Tsai Neurosciences Institute, Chan Zuckerberg Biohub, Amazon, Boeing, JPMorgan Chase, Docomo, Hitachi, JD.com, KDDI, NVIDIA, Dell.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Who may be put at disadvantage from this research: Not applicable.</p><p>What are the consequences of failure of the system: Not applicable. Does the task/method leverage biases in the data: The proposed GIB principle and the GIB-GAT model as an instantiation of GIB leverage the node features and structural information which in general are not believed to include undesirable biases. The datasets to evaluate our approaches are among the most widely-used benchmarks, which in general are not believed to include undesirable biases as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Preliminaries for Information Bottleneck</head><p>Here we briefly review the Information Bottleneck (IB) principle and its application to representation learning.</p><p>Given the input data D and target Y , and an stochastic encoding Z of D by P(Z|D) that satisfies the Markov chain Z âˆ’ D âˆ’ Y , IB has the following objective: min </p><p>It also has an equivalent form: max P(Z|D):I(D;Z)â‰¤Ic</p><p>Intuitively, Eq. ( <ref type="formula">7</ref>) or <ref type="bibr" target="#b7">(8)</ref> encourages the representation Z to maximally capture the information in Y , while controlling the complexity of the representation in terms of I(D; Z). When increasing Î² from 0 to some large value, we are essentially using a straight line with slope Î² to sweep out the Pareto frontier of I(Y ; Z) vs. I(X; Z) as given by Eq. <ref type="bibr" target="#b7">(8)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ğ’Ÿ ğ‘Œ</head><p>minimal sufficient info.</p><p>irrelevant info.</p><p>optimal ğ‘ + overfitting Using the information diagram (Fig. <ref type="figure">3</ref>), where we represent the information of D, Y as circles and their shared part as the overlapping region of the circles, then IB encourages Z to cover as much of the I(D; Y ) as possible, and cover as little of H(D|Y ) (the irrelevant information part) as possible.</p><p>An optimal representation is defined as the minimal sufficient representation <ref type="bibr" target="#b48">[49]</ref> that only covers I(D; Y ). In practice, due to the expressiveness of the models and different choices of Î² in Eq. ( <ref type="formula">7</ref>), this optimal information can hardly be reached, and may only be approached. It is an interesting future direction to study that when sweeping Î², how near it is to the optimal representation on the diagram of I(Y ; Z) vs. I(X; Z).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proof for Proposition 3.1</head><p>We restate Proposition 3.2: For any PDFs</p><p>Proof. We use the Nguyen, Wainright &amp; Jordan's bound I NWJ <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>: Lemma B.1. <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> For any two random variables X 1 , X 2 and any function g : g</p><p>We use the above lemma to (Y, Z</p><p>X ) and plug in g(Y, Z (L)</p><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Proof for Proposition 3.2</head><p>We restate Proposition 3.2: For any groups of indices S X , S</p><p>A } lâˆˆS A , and for any probabilistic distributions Q(Z (l) X ), l âˆˆ S X , and Q(Z</p><p>Proof. The first inequality I(D; Z</p><p>A } lâˆˆS A ) directly results from the data processing inequality <ref type="bibr" target="#b16">[17]</ref> and the Markov property</p><p>To prove the second inequality, we define an order "â‰º" of random variables in {Z (l)</p><p>X . Based on the order, define a sequence of sets</p><p>We may decompose I(D; {Z</p><p>A } lâˆˆS A ) with respect to this order</p><p>Next, we bound each term in the RHS</p><p>where 1), 2) use the basic properties of mutual information, 3) uses X âŠ¥ Z (l)</p><p>A } and 5) uses the definitions of AIB (l) and XIB (l) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D The Contrastive Loss Derived from the Variational Bound Eq. (2)</head><p>To characterize Eq. ( <ref type="formula">2</ref>), We may also use a contrastive loss <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28]</ref> which empirically may sometimes improve the robustness of the model. Concretely, we keep</p><p>X,v ) as the same as that to derive Eq. ( <ref type="formula">6</ref>), i.e., Q</p><p>Here, P(Z (L) X ) refers to the distribution of the last-layer node representation after we replace A with a random graph structure A âˆˆ R nÃ—n where A is uniformly sampled with the constraint that A has the same number of edges as A. When using this contrastive loss, we simply use the estimation of Q 2 (Y ) based on the sampled Z (L) X,v and Z (L) X,v . Moreover, the last term of Eq. ( <ref type="formula">2</ref>) is empirically closed to 1 and thus we ignore it and other constants in Eq. ( <ref type="formula">2</ref>). Overall, we have the substitution for the contrastive loss,</p><p>where</p><p>) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Permutation Invariance of GIB-Cat and GIB-Bern</head><p>Let Î  âˆˆ R nÃ—n denote a permutation matrix where each row and each column contains exactly one single 1 and the rest components are all 0's. For any variable in GIB-Cat or GIB-Bern, we use subscript Î  to denote the corresponding obtained variable after we permutate the node indices of the input data, i.e., D = (X, A) â†’ Î (D) = (Î X, Î AÎ  T ). For example, Z</p><p>X,Î  denotes the node representations after l layers of GIB-Cat or GIB-Bern based on the input data Î (D). Moreover, the matrix Î  also defines a bijective mapping Ï€ : V â†’ V , where Ï€(v) = u iff Î  uv = 1. We also use " d =" to denote that two random variables share the same distribution. Now, we formally restate the permutation invariant property of GIB-Cat and GIB-Bern: Suppose Î  âˆˆ R nÃ—n is any permutation matrix, if the input graph-structured data becomes Î (D) = (Î X, Î AÎ  T ), the corresponding node representations output by GIB-Cat or GIB-Bern satisfy</p><p>X is the output node representations based on the original input data D = (X, A).</p><p>Proof. We use induction to prove this result. Specifically, we only need to show that for a certain l âˆˆ [L], if node representations Z Ï€(v)t . Here, we use A â†’ Î AÎ  T and thus V vt â†’ V Ï€(v)t , and assume that Ï† (l) vt,Î  , Ï† (l) Ï€(v)t are represented as vectors in R nÃ—1 where their uth components, Ï†</p><p>X and concludes the proof. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Summary of the Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Implementation Details for the GIB-Cat, GIB-Bern and Other Compared Models</head><p>For all experiments and all models, the best models are selected according to the classification accuracy on the validation set. All models are trained with a total of 2000 epochs. For all experiments, we run it with 5 random seeds: 0, 1, 2, 3, 4 and report the average performance and standard deviation.</p><p>The models are all trained on NVIDIA GeForce RTX 2080 GPUs, together with Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GH CPUs. We use PyTorch <ref type="bibr" target="#b49">[50]</ref> and PyTorch Geometric <ref type="bibr" target="#b50">[51]</ref> for constructing the GNNs and evaluation. Project website and code can be found at http://snap.stanford.edu/ gib/. In Section G.1, G.2 and G.3, we detail the hyperparameter setting for Section 5.1, and in Section G.4 and G.5, we provide additional details for the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1 Implementation Details for the GIB-Cat and GIB-Bern</head><p>The architecture of GIB-Cat and GIB-Bern follows Alg. 1 (and Alg. 2 and 3 for the respective neighbor-sampling). We follow GAT <ref type="bibr" target="#b4">[5]</ref>'s default architecture, in which we use 8 attention heads, nonlinear activation Ï„ as LeakyReLU, and feature dropout rate of 0.6 between layers. We follow GAT's default learning rate, i.e. 0.01 for Cora and Citeseer, and 5Ã—10 âˆ’3 for Pubmed. As stated in the main text, the training objective is Eq. ( <ref type="formula">1</ref>), substituting in Eq. ( <ref type="formula">5</ref>) and <ref type="bibr" target="#b5">(6)</ref>. To allow more flexibility (in similar spirit as Î²-VAE <ref type="bibr" target="#b40">[41]</ref>), we allow the coefficient before AIB and XIB to be different, and denote them as Î² 1 and Î² 2 . In summary, the objective is written as:</p><p>In this work, we set the index set S A = [L] = {1, 2, ...L} and S X = {L âˆ’ 1}, which satisfies Proposition 3.2. For XIB, we use mixture of Gaussians as the variational marginal distribution Q(Z X ). For the mixture of Gaussians, we use m = 100 components with learnable weights, where each component is a diagonal Gaussian with learnable mean and standard deviation. This flexible variational marginal allows it to flexibly approximate the true marginal distribution P(Z X ). For the reparameterization in AIB, we use Gumbel-softmax <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> with temperature Ï„ . For GIB-Cat, the number of neighbors k to be sampled is a hyperparameter. For GIB-Bern, we use Bernoulli(Î±) as the non-informative prior, where we fix Î± = 0.5. To facilitate learning at the beginning, for the first 25% of the epochs we do not impose AIB or XIB, and gradually anneal up both Î² 1 and Î² 2 during the 25% -50% epochs of training, and keep them both at their final value afterwards. For the experiment in Section 5.1 and section 5.2, we perform hyperparameter search of Î² 1 âˆˆ {0.1, 0.01, 0.001}, Î² 2 âˆˆ {0.01, 0.1}, T âˆˆ {1, 2}, Ï„ âˆˆ {0.05, 0.1, 1}, k âˆˆ {2, 3} for each dataset, and report the one with higher validation F1-micro. A summary of the hyperparameter scope is in Table <ref type="table">5</ref>. In Table <ref type="table">6</ref> and 7, we provide the hyperparameters that produce the results in Section 5.1, and in Table <ref type="table">8</ref>, we provide hyperparameters that produce the results in Section 5.2. and then fixed. We also make sure that for the same seed, different models are evaluated against the same 40 target nodes, to eliminate fluctuation between models due to random sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.5 Additional Details for Feature Attack Experiment</head><p>As before, for each model to compare, we train 5 instances with seeds 0, 1, 2, 3, 4. After training, for each seed and each specified feature noise ratio Î», we perform 5 random node feature attacks, by adding independent Gaussian noise Î» â€¢ r â€¢ to each dimension of the node feature, where r is the mean of the maximum value of each node's feature, and âˆ¼ N (0, 1). Therefore, each number in Table <ref type="table">3</ref> is the mean and std. of 25 instances (5 seeds Ã— 5 attacks per seed).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Training time for GIB-Cat and GIB-Bern</head><p>The training time of GIB-Cat and GIB-Bern is on the same order as GAT with the same underlying architecture. For example, with 2 layers, GIB-Cat takes 98s (GIB-Bern takes 84s) to train 2000 epochs on a NVIDIA GeForce RTX 2080 GPU, while GAT takes 51s to train on the same device. The similar order of training time is due to that they have similar number of parameters and complexity. Compared to GAT, GIB-Cat and GIB-Bern introduce minimal more parameters. In this work, on the structural side, we use the attention weights of GAT as parameters to encode structural representation, which keeps the same number of parameters as GAT. On the feature side, we set S X = {L âˆ’ 1}, which only requires to predict the diagonal variance of the Gaussian in addition to the mean, which introduce small number of parameters. Therefore, in total, GIB-Cat and GIB-Bern have similar complexity. The added training time is due to the sampling of edges and node features during training. We expect that when GIB is applied to other GNNs, the augmented model has similar complexity and training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Additional experiments for Deep Graph Infomax (DGI)</head><p>Here we perform additional experiment for adversarial attacks on Cora using Nettack. The result is in Table <ref type="table">12</ref>. We see that both GIB-Cat and GIB-Bern outperform DGI by a large margin.</p><p>J More Detailed Analysis of Adversarial Attack in Section 5.1 </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Optimizing generalized pagerank methods for seedexpansion community detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Milenkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>VeliÄkoviÄ‡</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>LiÃ²</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">FastGCN: Fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>GÃ¼nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Position-aware graph neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Geom-gcn: Geometric graph convolutional networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Provably powerful graph networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Relational pooling for graph representations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Riberio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the equivalence between graph isomorphism testing and function approximation with gnns</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Measuring and improving the use of graph information in graph neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adversarial attacks on neural networks for graph data</title>
		<author>
			<persName><forename type="first">D</forename><surname>ZÃ¼gner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akbarnejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>GÃ¼nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adversarial attack on graph structured data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02371</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Elements of information theory</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bialek</surname></persName>
		</author>
		<idno>arXiv preprint physics/0004057</idno>
		<title level="m">The information bottleneck method</title>
				<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep learning and the information bottleneck principle</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zaslavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Information Theory Workshop (ITW)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The principles of quantum mechanics</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A M</forename><surname>Dirac</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
			<publisher>Oxford university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep variational information bottleneck</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00410</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On variational bounds of mutual information</title>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Estimating divergence functionals and the likelihood ratio by convex risk minimization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Ceb improves model robustness</title>
		<author>
			<persName><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05380</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deep unsupervised clustering with gaussian mixture variational autoencoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dilokthanakul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Mediano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Salimbeni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Arulkumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shanahan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02648</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adaptive graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>-I. Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03536</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Gaan: Gated attention networks for learning on large and spatiotemporal graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07294</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Robust graph convolutional networks against adversarial attacks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adversarial examples for graph data: Deep insights into attack and defense</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tyshetskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Docherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence, IJCAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">All you need is low (rank) defending against adversarial attacks on graphs</title>
		<author>
			<persName><forename type="first">N</forename><surname>Entezari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Al-Sayouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Darvishzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Papalexakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Web Search and Data Mining</title>
				<meeting>the 13th International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Certifiable robustness and robust training for graph convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>ZÃ¼gner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>GÃ¼nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deep graph infomax</title>
		<author>
			<persName><forename type="first">P</forename><surname>VeliÄkoviÄ‡</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>LiÃ²</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10341</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Graph representation learning via graphical mutual information maximization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
				<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName><forename type="first">F.-Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01000</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Variational discriminator bottleneck: Improving imitation learning, inverse rl, and gans by constraining information flow</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Toyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00821</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>AI magazine</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Friendship and mobility: user movement in locationbased social networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graph theory and networks in biology</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Verwoerd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET systems biology</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Spatial networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>BarthÃ©lemy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics Reports</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Designing a neural network for forecasting financial</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kaastra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">The conditional entropy bottleneck</title>
		<author>
			<persName><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05379</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>AlchÃ©-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
