<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gait Energy Volumes and Frontal Gait Recognition using Depth Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sabesan</forename><surname>Sivapalan</surname></persName>
							<email>sivapalen.sabesan@qut.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Chen</surname></persName>
							<email>daniel.chen@qut.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Simon</forename><surname>Denman</surname></persName>
							<email>s.denman@qut.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Sridha</forename><surname>Sridharan</surname></persName>
							<email>s.sridharan@qut.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Clinton</forename><surname>Fookes</surname></persName>
							<email>c.jookes@qut.edu.au</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Image and Video Research Laboratory</orgName>
								<orgName type="institution">Queensland University of Technology GPO</orgName>
								<address>
									<postBox>Box 2434</postBox>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">George St. Brisbane</orgName>
								<address>
									<postCode>4001</postCode>
									<region>Queensland</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Gait Energy Volumes and Frontal Gait Recognition using Depth Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">63E888E912490CBC043F4FE2CF23EBCB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Gait energy images (GEls) and its variants form the ba sis of many recent appearance-based gait recognition sys tems. The GEl combines good recognition performance with a simple implementation, though it suffers problems inherent to appearance-based approaches, such as being highly view dependent. In this paper, we extend the concept of the GEl to 3D, to create what we call the gait energy vol ume, or GEV. A basic GEV implementation is tested on the eMU MoBo database, showing improvements over both the GEl baseline and afused multi-view GEl approach. We also demonstrate the efficacy of this approach on partial volume reconstructions created from frontal depth images, which can be more practically acquired, for example, in biometric portals implemented with stereo cameras, or other depth acquisition systems. Experiments on frontal depth images are evaluated on an in-house developed database captured using the Microsoft Kinect, and demonstrate the validity of the proposed approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The recognition of people is a challenging task in the computer vision field. A number of biometrics such as gait, fingerprint, iris and face are used for this purpose. Gait has a unique advantage over other biometrics in that it can be recognised from a distance without alerting the subject. There are many successful gait recognition techniques that have been developed, however, they still struggle to perform well under certain factors, such as changes in viewing an gles.</p><p>There are two major approaches to gait recogni tion; appearance-based (model-free) and model-based <ref type="bibr" target="#b11">[ 12]</ref>. Model-based techniques gather gait dynamics directly by modelling the underlying kinematics of human motion. The trade off though is that these algorithms are generally more 978-1-4577-1359-0/11/$26.00 ©2011 IEEE complex and computationally expensive. Examples of this approach include the work of Cunado et al. <ref type="bibr" target="#b1">[2]</ref>, where a motion-based model is fitted to an extracted binary silhou ette to analyse the angular motion of the hip and thigh by means of a Fourier series. Wagg and Nixon <ref type="bibr" target="#b14">[ 15]</ref> proposed bulk motion and shape estimation guided by bio-mechanical analysis and used mean gait data to create the motion mod els.</p><p>In contrast to these model-based approaches, appearance-based techniques are less complex to im plement and establish correspondence between successive frames based upon the implicit notion of what is being ob served. Earlier appearance-based approaches are based on gait features that can be derived from the spatio-temporal pattern of a walking person <ref type="bibr" target="#b13">[ 14]</ref>. Recently, Han and Bhanu <ref type="bibr" target="#b5">[6]</ref> proposed an energy based feature extraction technique and introduced a new concept, the gait energy image (GEl). GEl represents the temporal motion pattern within a gait cycle in a single image, that holds several key features of human gait such as motion frequency, temporal and spatial changes of the human body, as well as a global body shape statistic pattern <ref type="bibr" target="#b8">[9]</ref>. However, the GEl, like other appearance-based algorithms, is view dependent and performs best when a side view is used <ref type="bibr" target="#b16">[ 17]</ref>.</p><p>There are number of approaches proposed to resolve the view dependency issues in appearance-based gait recogni tion. View transformation models (V TM) based on GEl proposed by Worapan et al. <ref type="bibr" target="#b8">[9]</ref> adopts singular value de composition (SV D) to transform models to different view points. However, the performance of the algorithm is good only for a limited range of view angle deviation. Another way to address the issue of view dependency is to use multi ple cameras with overlapping field of views. With sufficient coverage, one could have a view which is similar enough to match with the existing closest view <ref type="bibr" target="#b9">[ 10]</ref>. The performance of this algorithm also depends on how close the probe view is to an existing gallery view.</p><p>Another possible approach is performing recognition on a reconstructed 3D model. This requires the use of mul tiple calibrated cameras from widely different viewpoints. The majority of 3D techniques are model-based approaches. Yamuachi et al. <ref type="bibr" target="#b15">[ 16]</ref> used skeleton models to extract the joint angles and static parameters in 3D reconstructed data. Gu et al. <ref type="bibr" target="#b4">[5]</ref> extracted the complete pose of a person in 3D using grid-based segmentation and adaptive particle fil ters. Working in 3D bypasses the issue of view dependency, though, the performance of these model-based methods, as well as 2D variants, are reported to be lower than recent appearance-based techniques, such as <ref type="bibr" target="#b5">[6]</ref>, using only the side view. This suggests that significant identifying infor mation is present in the shape and the lack of appearance modelling is detrimental to the performance.</p><p>In this paper, we present an appearance-based technique, specifically, one derived from gait energy images <ref type="bibr" target="#b5">[6]</ref>. We propose a 3D analogue of the GEl, where, instead of tem porally averaging segmented silhouettes, we perform this on reconstructed voxel volumes. We term the resulting av eraged volume, the Gait Energy Volume, or GEV. The recognition based on GEV features is tested on the CMU MoBo <ref type="bibr" target="#b3">[4]</ref> database.</p><p>Having a multi-view camera setup, however, can be im practical under many applications, even in controlled con ditions such as gait-based biometric authentication. An al ternative to acquiring this 3D data would be to use some form of depth sensing device. We apply the GEV to partial volume reconstructions from depth images captured from a frontal viewpoint. Frontal based depth has the advan tage of being able to capture essentially all characteristics of gait from a single viewpoint without the issue of self occlusion. These depth images can be easily acquired using devices such as the Microsoft Kinect. A frontal viewpoint also makes it possible to easily integrate into biometric por tals such as the one used in the multiple biometric grand challenge (MBGC) <ref type="bibr" target="#b12">[ 13]</ref>.</p><p>The applicability of the frontal depth based GEV is anal ysed using an in-house database. The CMU MoBo database is also used to simulate frontal depth GEV by synthesising appropriate volume reconstructions.</p><p>The remainder of this paper is organised as follows. Sec tion 2 outlines the GEl algorithm, as well as our proposed GEV implementation, followed by its application on depth images in Section 3. Section 4 outlines the feature extrac tion and classification algorithms used in the experiments in this paper, the results of which is shown in Section 5. Section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Gait Energy Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Gait Energy Images</head><p>Inspired by motion history images (MHI) and motion en ergy images (MEl) <ref type="bibr" target="#b2">[3]</ref> proposed for action recognition, gait energy images <ref type="bibr" target="#b5">[6]</ref> were developed for use in gait recogni tion. GEl represents the gait features in multiple silhou ettes of a person over a gait cycle in a single image frame. The silhouettes are normalised, aligned, and temporally av eraged. This forms a compact representation of a person's spatial occupancy over a gait cycle, encoding information about their gait characteristics as well as appearance, allow ing identification to be performed.</p><p>Many extensions to the initial GEl concept have been proposed recently, including the enhanced gait energy im age (EGEI) <ref type="bibr" target="#b10">[ 11]</ref> and the shifted energy image (SEI) <ref type="bibr" target="#b6">[7]</ref>. However, these algorithms are aimed to address issues such as wearing clothes <ref type="bibr" target="#b6">[7]</ref>, and do not significantly improve the performance of the underlying algorithm in situations such as view variation. Extracted GEls from different viewpoints are shown in Figure <ref type="figure">1</ref>.</p><p>In this paper, we use the GEl algorithm proposed by <ref type="bibr" target="#b5">[6]</ref> to benchmark the performance of the proposed algorithms. The pixel values in the GEl are assembled into a feature vector, to which principal component analysis (PCA) and multiple discriminant analysis (MDA) are applied before classification (Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Gait Energy Volumes</head><p>The proposed gait energy volumes have many advan tages over gait energy images, simply by virtue of working in 3 dimensional space. It circumvents the issue of view dependency, as well as having no pose ambiguity (e.g. left right limbs), no self occlusion, and allowing easier segmen tation of unwanted regions (e.g. hand movements). How ever, it is not without its disadvantages, most notably the more complex hardware setup, making it impractical for many applications.</p><p>Derived from GEls, the construction of the GEV follows a similar process to its 2D counterpart. Binary voxel vol umes, analogous to silhouettes, are spatially aligned and av eraged over a gait cycle as follows,</p><formula xml:id="formula_0">1 n GEV(k) = -L V(t), n t = l (1)</formula><p>where n is the number of frames in the k t h and V is the aligned voxel volumes. An example of binary volume and corresponding GEV is shown in Figure <ref type="figure" target="#fig_1">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Multi-View GEl</head><p>The proposed GEV requires multiple camera views of a subject in order to achieve a complete volume reconstruc tion. This gives it an advantage over the GEl in benchmarks as it has access to information not available to the GEL In this paper, we implement a simple multi-view GEl based al gorithm in order to demonstrate the advantages of extracting gait energy features in 3D.</p><p>In our algorithm, we apply the GEl to the same camera views used to construct the voxel volume used for the GEY. To combine the individual views, the feature vectors are ex tracted from each view's GEl and simply concatenated into a single super vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">GEVs and Frontal Depth Images</head><p>Gait recognition using depth images has been attempted in the past <ref type="bibr" target="#b7">[8]</ref>. Frontally captured data has many advantages over a lateral view for gait based biometric authentication, including easy integration into biometric portals and sim ilar devices, as well as not having field of view issues in confined spaces such as a narrow corridor. The addition of depth also enables more data to be captured than from the side, as there is no issue of self-occlusion. In fact, all gait-based information (kinematics) can be acquired from a frontal depth sequence.</p><p>We propose applying our proposed GEV algorithm to perform gait recognition using these depth images. As only the front surface of the subject is visible, only a partial vol ume reconstruction is possible. The voxel model is created by taking the frontal surface reconstruction and filling to the back of the defined voxel space along the 'depth' axis. The GEV is then computed as normal using these voxel vol umes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Synthesised Depth Volumes</head><p>No gait data using depth images exists publicly. In order to test our approach, we simulate a depth reconstruction us ing multi-view camera data. This allows us to benchmark our results against existing gait algorithms on a common database, as well as allowing us to quantify what effects, if any, are produced by removing the back-face of the subject model.</p><p>To generate the data used in this experiment, the front surface of the full voxel model is found (as described in Sec tion 2.2) and filled to the back of the volume boundary. The GEV is then computed using these frontal volumes. Fig ure <ref type="figure">3</ref> shows an example of a synthesised depth volume and corresponding GEY.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Frontal Depth Gait Database</head><p>To test the application of our algorithm on real-world data, we created a small gait dataset of frontally acquired depth images l . The database consists of 15 subjects walk ing towards the camera at two different speeds, 'normal' and 'fast'. Five sequences were recorded for each sub ject and class, with each sequence covering an average of 2 -3 gait cycles, though only about 2 cycles is useful due to limitations in depth resolution. The dataset is captured at approximately 30 fps using the Microsoft Kinect. Colour video was also recorded but was not used. An example frame from our database is shown in Figure <ref type="figure" target="#fig_4">4</ref>.</p><p>In order to create the GEV, we first project each pixel l Contact the author at s.sridharan@qut.edu.au for a copy of the database. in the depth image into world coordinates, where segmen tation is performed to remove the floor, ceiling and walls. From this, a surface mesh is constructed depicting the front of the subject, and alignment is performed using the centre of the torso surface. The GEV is generated as described in Section 2.2. Figure <ref type="figure" target="#fig_3">5</ref> shows an example of a constructed volume, as well as computed GEV Walking gait is mostly represented by the lower part of body (legs). In addition the lower part is not affected by appearance changes due to carrying goods and hand move ments. Therefore, we only consider the features of the lower part of human, by considering the section below to the cen troid. Extracted gait energy based features using the pro posed methods are of a high dimensionality as they use im age pixels or voxels. We use PCA to reduce the dimension ality, and MDA to extract the effective discriminant features as in <ref type="bibr" target="#b5">[6]</ref>.</p><p>Given n, d-dimensional (here d represents the number of image pixels for GEl based features and number of voxels for GEV based features) feature templates Xl , X2, ... , X n , PCA attempts to minimise the error function, where dp is the number of principal components required; m is the mean of the template features; and el, e2, ... , ed p is a set of orthogonal unit vectors. Xk is projected into this space to form a. The error, Jd p , is minimised when el, e2) ... ) ed p are the eigenvectors with the largest eigen val ues. For our proposed methods, the required dimension, dp, is defined as number of eigen vectors required to reduce the error rate less than 1 %. The original d-dimensional feature vectors are then projected to the new dp-dimensional fea ture space, Yk&gt; using the PCA transfonnation matrix, Tp ca , that contains the major eigenvector coefficients.</p><p>MDA is applied to these projected features (Y) to ex tract the most discriminant features to recognise the indi viduals. Suppose Y corresponds to c classes. MDA finds the transfonnation matrix, W, that maximises the ratio of between-class (inter-class) to within class (intra-class) vari ance. This will happen when the columns of W are the gen eralised eigenvectors and correspond to the largest eigenval ues of the within-class and between-class scatter matrices. The MDA transfonnation, Tmda, contains the largest coef ficients of the generalised eigenvectors and projects Y to dm-dimensional Z. The equation, shows the total transformation from X to Z with the fused method of PCA and MDA.</p><p>Selection of dm depends on the number of subjects. It is obvious dm cannot be greater than the number of classes. Therefore, dm is set as one less than the number of classes (i.e, c -1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Classification</head><p>Since the gait energy based features represent the gait cy cle, the distance, di j between ith probe cycle and lh gallery cycle is determined by computing the Euclidean distance between the gait cycles' feature vectors (F),</p><p>To determine the distance between a particular probe and the gallery subject, each of which is composed of multi ple gait cycles, the algorithm proposed by Boulgouris et al. <ref type="bibr" target="#b0">[ 1]</ref> is used. From this, the distance to the closest gallery cycle from each probe cycle (dmin f), and the distance to the closest probe cycle from each gallery cycle ( dmin r) are found, <ref type="bibr" target="#b4">(5)</ref> The final distance, D, between the probe and gallery se quence is, D = � (median (dmin P ) + median (dmin G)) . ( <ref type="formula">6</ref>)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and Results</head><p>Two different databases are used to evaluate the pro posed algorithms, CMU MoBo and our in-house captured frontal depth-based gait database (see Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">CMU MoBo Database</head><p>The CMU MoBo database <ref type="bibr" target="#b3">[4]</ref> is used to evaluate all the systems presented in this paper. The database consists of 25  subjects under four test classes captured from 6 cameras si multaneously as they walk on an indoor treadmill. The four classes are slow walk, fast walk, walking while carrying a ball, and walking on an inclined surface, though the last is not used in this evaluation as no background image with the inclined treadmill was provided to enable clean silhou ette segmentation. Examples of multi-view images from the CMU MoBo database are shown in Figure <ref type="figure" target="#fig_6">6</ref>.</p><p>The experiments evaluate the GEV s generated from the visual hull of the multi-view silhouettes, as well as the frontal GEV s extracted through synthesised 'depth' recon struction as described in Section 3. 1. As a baseline, the GEl of the front and side views is also tested, as well as the multi-view GEl (Section 2.3).</p><p>Both intra and inter-class cases are considered and Re ceiver Operating Curves (ROC) are used to compare the re sults. In the intra-class cases, the video sequence is split in half, with the gait cycles in the first half used as the gallery and the second half used as the probe. Intra-class recog nition performance of all algorithms is 100%, without any false alarms. For inter-class tests, the full sequence is used as either the gallery or probe, in the combination shown in Table <ref type="table" target="#tab_0">1</ref>.</p><p>ROC curves for the inter-class experiments are shown in Figure <ref type="figure" target="#fig_8">7</ref>. From the results, it can be seen that our GEV approaches outperform the GEL This includes the multi view GEl, showing that it is worth while performing the simple volume reconstruction and working in a 3D space given multi-view data. The GEV applied to the synthesised depth reconstruction outperforms the full volume GEV This is likely due to essentially all gait characteristics being ac quired from a frontal perspective, while the back filling re duces the impact of noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Frontal Depth Gait Database</head><p>Experiments evaluating the GEV on real world depth im ages is done using the in-house frontal depth gait database (Section 3.2), as no such data is available in the public do- False Alarm Rate (%)</p><p>-GEV -Frontal GEV -Multi View GEl -Side View GEl -FrontalView GEl main. We perform tests for both inter and intra-class cases, with an inter-class experiment done with fast walk as the probe and normal as the gallery. The GEl was also tested with silhouettes extracted directly from the depth images (Figure <ref type="figure" target="#fig_4">4</ref>), as these silhouettes were of higher quality than those produced from the colour images. Due to the small database size, only a single cycle was used for both probe and gallery in order to artificially lower the performance.</p><p>During intra-class tests, GEl and GEV both record 100% without any false alarms. However, the inter-class recogni tion rate of 100% for GEV and only 75% for GEl at false alarm rates of less than 1 % (Figure <ref type="figure" target="#fig_9">8</ref>), shows the applica bility of our GEV algorithm to real world depth images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose an extension of the GEl to op erate in the 3D domain, using binary voxel volumes instead of 2D silhouettes. This proposed GEV algorithm shows an improvement over its 2D variant in performing gait recog nition, given multi-view data. We also demonstrated the applicability of our algorithm to frontally captured depth images, such as would be acquired using a biometric por tal, through the use of synthesised data as well as recorded sequences.</p><p>We plan to expand our depth-based gait dataset to in clude more test subjects, as well as cover a variety of test conditions, particularly those more likely to be experienced biometric gait application, such as the carrying of luggage, changes in clothing and changes in session times. Our proposed algorithm will be re-evaluated on the expanded database.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure l .</head><label>l</label><figDesc>Figure l. GEls from multiple view angles. Higher pixel values represent greater spatial occupancy within the gait cycle.</figDesc><graphic coords="2,309.13,71.03,236.16,67.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. An example of a voxel model reconstruction (left) and GEV (middle). A cross-section representation of the GEV (right) is shown to better illustrate the internal densities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .Figure 4 .</head><label>34</label><figDesc>Figure 3. Synthesised depth image reconstruction and GEY.</figDesc><graphic coords="3,309.13,216.00,235.20,105.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Volume reconstruction from depth image and GEY.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4 .</head><label>4</label><figDesc>Feature Modelling and Classification 4.1. Feature Modelling</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Examples of multi-view images from the CMU MoBo database.</figDesc><graphic coords="5,49.92,71.03,236.16,53.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>__ -L __ � __ �� __ � __ � __ � __ L-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. ROC curves for inter-class tests on the CMU Mobo database.</figDesc><graphic coords="5,320.65,439.67,209.28,159.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. ROC curves for inter-class test on frontal depth gait database.</figDesc><graphic coords="6,82.56,81.59,189.12,157.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Inter-class test cases.</figDesc><table><row><cell>Gallery Class</cell><cell>Probe Class</cell></row><row><cell>Slow Walk</cell><cell>Ball</cell></row><row><cell>Slow Walk</cell><cell>Fast Walk</cell></row><row><cell>Ball</cell><cell>Fast Walk</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gait recognition using linear time normalization</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Boulgouris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Plataniotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hatzinakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recog nition</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="969" to="979" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic ex traction and description of human gait models for recognition purposes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cunado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Nixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Carter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hierarchical motion history images for recog nizing human motion</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop on Detection and Recognition of Events in Video</title>
		<meeting>IEEE Workshop on Detection and Recognition of Events in Video</meeting>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="39" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The CMU motion of body (MoBo) database</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><surname>Shi</surname></persName>
		</author>
		<idno>CMU-RI-TR-0 1-18</idno>
		<imprint>
			<date type="published" when="2001-06">June 200 1. 2,4</date>
			<pubPlace>Robotics Institute, Pittsburgh, PA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Action and gait recog nition from recovered 3-D human joints. Systems, Man, and Cybernetics, Part B: Cybernetics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="102" to="103" />
			<date type="published" when="2002">20 10. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Individual recognition using gait en ergy image</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bhanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="316" to="322" />
			<date type="published" when="2004">2006. 1,2,4</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gait recognition using lin ear discriminant analysis with artificial walking conditions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Boulgouris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Con! on Image Processing</title>
		<meeting>IEEE Int. Con! on Image essing</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">246</biblScope>
			<biblScope unit="page" from="1" to="2464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gait recognition using compact feature extraction transforms and depth information</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tzovaras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">G</forename><surname>Damousis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Argyropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Moustakas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="623" to="630" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi ple views gait recognition using view transformation model based on optimized gait energy image</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kusakunniran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Con! on Com puter Vision Workshops</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1058" to="1064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards scalable view-invariant gait recognition: Multilinear analysis for gait</title>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Audio and Video-Based Biometric Person Authenti cation</title>
		<meeting>Int. Conf. on Audio and Video-Based Biometric Person Authenti cation</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="395" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A behavior classification based on en hanced gait energy image</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Con! on Networking and Digital Society</title>
		<meeting>Int. Con! on Networking and Digital Society</meeting>
		<imprint>
			<date type="published" when="2002">20 10. 2</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="589" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automated per son recognition by walking and running via model-based ap proaches</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Nixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1057" to="1072" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Overview of the multiple biometrics grand chal lenge</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Scruggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>O'toole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Bolme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Givens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sahibzada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Scallan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Weimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Con! on Biometrics</title>
		<meeting>Int. Con! on Biometrics</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="705" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The humanid gait challenge problem: data sets, performance, and analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">R</forename><surname>Vega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="162" to="177" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On automated model-based extraction and analysis of gait</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Wagg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Nixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Con! on Automatic Face and Gesture Recognition</title>
		<meeting>IEEE Int. Con! on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="11" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recognition of walk ing humans in 3D: Initial results</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yamauchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bhanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="45" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modelling the effect of view an gle variation on appearance-based gait recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asian Con! on Computer Vision</title>
		<meeting>Asian Con! on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="807" to="816" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
