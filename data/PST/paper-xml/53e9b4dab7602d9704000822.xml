<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Genetic algorithm-based feature set partitioning for classification problems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Lior</forename><surname>Rokach</surname></persName>
							<email>liorrk@bgu.ac.il.</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information System Engineering</orgName>
								<orgName type="institution">Ben-Gurion University of the Negev</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Genetic algorithm-based feature set partitioning for classification problems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3D7596BF48EAD83E2823BC1324F9D587</idno>
					<idno type="DOI">10.1016/j.patcog.2007.10.013</idno>
					<note type="submission">Received 23 June 2006; received in revised form 14 September 2007; accepted 14 October 2007</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Feature set-partitioning</term>
					<term>Feature selection</term>
					<term>Genetic algorithm</term>
					<term>Ensemble learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Feature set partitioning generalizes the task of feature selection by partitioning the feature set into subsets of features that are collectively useful, rather than by finding a single useful subset of features. This paper presents a novel feature set partitioning approach that is based on a genetic algorithm. As part of this new approach a new encoding schema is also proposed and its properties are discussed. We examine the effectiveness of using a Vapnik-Chervonenkis dimension bound for evaluating the fitness function of multiple, oblivious tree classifiers. The new algorithm was tested on various datasets and the results indicate the superiority of the proposed algorithm to other methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction and motivation</head><p>An inducer aims to build a classifier (also known as a classification model) by learning from a set of pre-classified instances. The classifier can then be used for classifying unlabeled instances. It is well known that the required number of labeled instances for supervised learning increases as a function of dimensionality <ref type="bibr" target="#b0">[1]</ref>. Fukunaga <ref type="bibr" target="#b1">[2]</ref> showed that the required number of training instances for a linear classifier is linearly related to the dimensionality and for a quadratic classifier to the square of the dimensionality. In terms of nonparametric classifiers such as decision trees, the situation is even more severe. It has been estimated that, as the number of dimensions increases, the training set size needs to increase exponentially in order to obtain an effective estimate of multivariate densities <ref type="bibr" target="#b2">[3]</ref>.</p><p>Bellman <ref type="bibr" target="#b3">[4]</ref>, while working on complicated signal processing problems, was the first to define this phenomenon as the "curse of dimensionality." Techniques that are efficient in low dimensions, such as decision trees inducers, fail to provide meaningful results when the number of dimensions increases beyond a "modest" size. Furthermore, humans are better able to comprehend smaller classifiers involving fewer features (probably less than 10). Smaller classifiers are also more appropriate for user-driven data mining techniques such as visualization.</p><p>In this paper we propose a way to avoid the curse of dimensionality by decomposing the original feature set into several mutually exclusive subsets. This is known as feature set partitioning and may be regarded as a generalization of the feature selection task. Moreover, feature set partitioning is regarded as a specific case of ensemble methodology in which members use disjoint feature subsets, i.e., every classifier in the ensemble is trained on a different projection of the original training set.</p><p>As an example of some of the aspects involved in feature set partitioning, consider a training set containing data about health insurance policyholders. Each policyholder is characterized by four features: Asset Ownership, Education (years), Car Engine Volume (in cubic centimeters) and Employment Status. The target feature (i.e., the label) describes whether a specific policyholder was willing to purchase complementary insurance and what type of complementary insurance she was willing to buy. A possible feature set partitioning ensemble for resolving the question includes two decision trees. The first decision tree uses the features Asset Ownership and Volume, while the second uses the features Employment Status and Education.</p><p>The aim of this work is to examine whether genetic algorithm (GA)-based feature set partitioning can improve classification performance. We propose a new encoding schema. Theoretical results are used to explain why this new encoding is more suitable than more straightforward encoding schemas. In order to avoid long training time, a Vapnik-Chervonenkis dimension bound for multiple oblivious trees evaluates the fitness function. A caching mechanism is suggested in order to reduce further the computational cost of the GA. The superiority of the suggested algorithm to other methods is illustrated on various datasets.</p><p>The rest of this paper is organized as follows: Section 2 reviews related works in the field of feature selection, feature set partitioning, and the usage of ensemble of feature selectors. Section 3 formulates the problem. Section 4 presents a new algorithm for solving the problem discussed here. Section 5 reports the experiments carried out to examine the new algorithm. Finally, Section 6 concludes the work and presents suggestions for further research in the field. Proofs for the theoretical claims presented in this paper appear in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>In this section we briefly review some of the central issues that have been addressed, and their treatment in the literature. The related work described in this section falls into three categories:</p><p>• First, we discuss three feature oriented tasks (namely feature selection, feature set partitioning, and feature subset-based ensemble) in pattern recognition and the relations among them.</p><p>• Then, we survey the usage of GAs for solving the abovementioned tasks. • The oblivious decision tree (ODT) and its usage for solving feature selection problems.</p><p>Finally, in the light of previous work, we summarize the original contribution of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Feature selection</head><p>Most methods of dealing with high dimensionality focus on feature selection techniques, i.e., selecting a single subset of features upon which the inducer will run, while ignoring the rest. The selection of the subset can be done manually using prior knowledge to identify irrelevant variables or feature selection algorithms. In the last decade, many researchers have shown increased interest in feature selection, and consequently many algorithms have been proposed, with some demonstrating remarkable improvements in accuracy. Since the subject is too wide to survey here, the reader is referred to Ref. <ref type="bibr" target="#b4">[5]</ref> for further reading.</p><p>Despite their popularity, there are several drawbacks to using feature selection methodologies in order to overcome the dimensionality curse:</p><p>• The assumption that a large set of input features can be reduced to a small subset of relevant features is not always true; in some cases the target feature is actually affected by most of the input features and removing features will cause a significant loss of important information.</p><p>• The outcome (i.e., the subset) of many algorithms for feature selection (for example, almost any of the algorithms that are based on the wrapper methodology) is strongly dependent on the training set size. That is, if the training set is small, the size of the reduced subset will also be small. Consequently, relevant features might be lost. Accordingly, the induced classifiers might achieve a lower degree of accuracy compared to classifiers that have access to all relevant features. • In some cases, even after eliminating a set of irrelevant features, the researcher is left with a relatively large number of relevant features. • The backward elimination strategy that some methods implement is extremely inefficient for working with large-scale databases, where the number of original features is greater than 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Feature subset-based ensemble methods</head><p>Ensemble methodology, which builds a predictive classifier by integrating multiple classifiers, can be used to improve prediction performance. During the past few years, experimental studies have shown that combining the outputs of multiple classifiers reduces the generalization error <ref type="bibr" target="#b5">[6]</ref>. Ensemble methods are very effective, mainly due to the phenomenon that various types of classifiers have different "inductive biases" <ref type="bibr" target="#b6">[7]</ref>. Indeed, ensemble methods can effectively make use of such diversity to reduce the variance-error <ref type="bibr" target="#b7">[8]</ref> without increasing the bias-error.</p><p>Bagging <ref type="bibr" target="#b8">[9]</ref> and AdaBoost <ref type="bibr" target="#b9">[10]</ref> are popular implementations of the ensemble methodology. Bagging employs bootstrap sampling to generate several training sets and then trains a classifier from each generated training set. Note that, since sampling with replacement is used, some of the original instances may appear more than once in the same generated training set and some may not be included at all. The classifier predictions are often combined via majority voting. AdaBoost sequentially constructs a series of classifiers, where the training instances that are wrongly classified by a certain classifier will get a higher weight in the training of its subsequent classifier. The classifiers' predictions are combined via weighted voting where the weights are determined by the algorithm itself based on the training error of each classifier.</p><p>Feature subset-based ensemble methods are those that manipulate the input feature set for creating the ensemble members. The idea is simply to give each classifier a different projection of the training set. Tumer and Oza <ref type="bibr" target="#b10">[11]</ref> claim that feature subset-based ensembles potentially facilitate the creation of a classifier for high dimensionality datasets without the feature selection drawbacks mentioned above. Moreover, these methods can be used to improve the classification performance due to the reduced correlation among the classifiers. Bryll et al. <ref type="bibr" target="#b11">[12]</ref> also indicate that the reduced size of the dataset implies faster induction of classifiers. Feature subset avoids the class under-representation which may occur in instance subsets methods such as bagging. Three popular strategies for creating feature subset-based ensembles exist: random-based, reduct-based, and performance-based.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Random-based strategy</head><p>The most straightforward techniques for creating a feature subset-based ensemble are based on random selection. Ho <ref type="bibr" target="#b12">[13]</ref> creates a forest of decision trees. The ensemble is constructed systematically by pseudo-randomly selecting subsets of features. The training instances are projected to each subset and a decision tree is constructed using the projected training samples. The process is repeated several times to create the forest. The classifications of the individual trees are combined by averaging the conditional probability of each class at the leaves (distribution summation). Ho shows that simple random selection of feature subsets may be an effective technique because the diversity of the ensemble members compensates for their lack of accuracy.</p><p>Bay <ref type="bibr" target="#b13">[14]</ref> proposed using simple voting in order to combine outputs from multiple K-nearest neighbor (KNN) classifiers, each having access only to a random subset of the original features. Each classifier employs the same number of features. Bryll et al. <ref type="bibr" target="#b11">[12]</ref> introduce attribute bagging (AB) which combines random subsets of features. AB first finds an appropriate subset size by a random search in the feature subset dimensionality. It then randomly selects subsets of features, creating projections of the training set on which the classifiers are trained. A technique for building ensembles of simple Bayesian classifiers in random feature subsets was also examined <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Reduct-based strategy</head><p>A reduct is defined as the smallest feature subset which has the same predictive power as the whole feature set. By definition, the size of the ensembles that were created using reducts is limited to the number of features. There have been several attempts to create classifier ensembles by combining several reducts. Wu et al. <ref type="bibr" target="#b15">[16]</ref> introduce the worst-attribute-drop-first algorithm to find a set of significant reducts and then combine them using naïve Bayes. Bao and Ishii <ref type="bibr" target="#b16">[17]</ref> examine the idea of combining multiple KNN classifiers for text classification by reducts. Hu et al. <ref type="bibr" target="#b17">[18]</ref> propose several techniques to construct decision forests, in which every tree is built on a different reduct. The classifications of the various trees are combined using a voting mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3.">Performance-based strategy</head><p>Cunningham and Carney <ref type="bibr" target="#b18">[19]</ref> introduced an ensemble feature selection strategy that randomly constructs the initial ensemble. Then, an iterative refinement is performed based on a hill-climbing search in order to improve the accuracy and diversity of the base classifiers. For all the feature subsets, an attempt is made to switch (include or delete) each feature. If the resulting feature subset produces a better performance on the validation set, that change is retained. This process is continued until no further improvements are obtained. Similarly, Zenobi and Cunningham <ref type="bibr" target="#b19">[20]</ref> suggest that the search for the different feature subsets will not be guided solely by the associated error but also by the disagreement or ambiguity among the ensemble members. Tumer and Oza <ref type="bibr" target="#b10">[11]</ref> present a new method called input decimation (ID), which selects feature subsets based on the correlations between individual features and class labels. This experimental study shows that ID can outperform simple random selection of feature subsets.</p><p>Tsymbal et al. <ref type="bibr" target="#b20">[21]</ref> compare several feature selection methods that incorporate diversity as a component of the fitness function in the search for the best collection of feature subsets. This study shows that there are some datasets in which the ensemble feature selection method can be sensitive to the choice of the diversity measure. Moreover, no particular measure is superior in all cases.</p><p>Gunter and Bunke <ref type="bibr" target="#b21">[22]</ref> suggest employing a feature subset search algorithm in order to find different subsets of the given features. The feature subset search algorithm not only takes the performance of the ensemble into account, but also directly supports diversity of subsets of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Feature set partitioning</head><p>Feature set partitioning decomposes the original set of features into several subsets and builds a classifier for each subset. Thus, a set of classifiers is trained such that each classifier employs a different subset of the original feature set. Subsequently, an unlabeled instance is classified by combining the classifications of all classifiers.</p><p>Feature set partitioning is a particular case of feature subsetbased ensembles in which the subsets are pairwise disjoint subsets. At the same time, it generalizes the task of feature selection which aims to provide a single representative set of features from which a classifier is constructed.</p><p>Several researchers have shown that the partitioning methodology can be appropriate for classification tasks with a large number of features <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. Fig. <ref type="figure" target="#fig_0">1</ref> presents the Venn diagram of the search space of the feature-oriented tasks. As can be seen, the search space of a feature subset-based ensemble contains the search space of feature set partitioning, and the latter contains the search space of feature selection.</p><p>While mutually exclusive partitioning restricts the search space, it has some important and helpful properties:</p><p>1. Compared to non-exclusive approaches, this approach offers a greater possibility of achieving reduced execution time.</p><p>Since most learning algorithms have computational complexity that is greater than linear in the number of features or tuples, partitioning the problem dimensionality in a mutually exclusive manner results in a decrease in computational complexity <ref type="bibr" target="#b24">[25]</ref>. 2. Since mutual exclusiveness entails using smaller datasets, the classifiers obtained for each sub-problem are smaller in size.</p><p>Without the mutually exclusive restriction, each classifier can be as complicated as the classifier obtained for the original problem. Smaller classifiers contribute to comprehensibility and ease in maintaining the solution. 3. According to Ref. <ref type="bibr" target="#b13">[14]</ref>, mutually exclusive partitioning may help avoid some error correlation problems that characterize feature subset-based ensembles. However, Sharkey <ref type="bibr" target="#b25">[26]</ref> argues that mutually exclusive training sets do not necessarily result in low error correlation. 4. In feature subset-based ensembles, different classifiers might generate contradictive classifications using the same features. This inconsistency in the way a certain feature can affect the final classification may increase mistrust among end-users. Accordingly, Rokach <ref type="bibr" target="#b22">[23]</ref> claims that end-users can grasp mutually exclusive partitioning much more easily. 5. The mutually exclusive approach encourages smaller datasets which are generally more practicable. Some data mining tools can process only limited dataset sizes (for instance, when the program requires that the entire dataset be stored in the main memory). The mutually exclusive approach can ensure that data mining tools can be scaled fairly easily to large datasets <ref type="bibr" target="#b26">[27]</ref>.</p><p>The literature includes several works that deal with feature set partitioning. In one research study, the features are grouped according to the feature type: nominal value, numeric value, and text value <ref type="bibr" target="#b23">[24]</ref>. A similar approach was also used for developing the linear Bayes classifier <ref type="bibr" target="#b27">[28]</ref>. The basic idea consists of aggregating the features into two subsets, the first containing only the nominal and the second only the continuous features.</p><p>In another research study, the feature set was decomposed according to the target class <ref type="bibr" target="#b28">[29]</ref>. For each class, the features with low correlation relating to that class were removed. This method was applied on a feature set of 25 sonar signals where the target was to identify the meaning of the sound (whale, cracking ice, etc.). Feature set partitioning has also been used for radar-based volcano recognition <ref type="bibr" target="#b29">[30]</ref>. The researcher manually decomposed a feature set of 119 into 8 subsets, grouping features that were based on different image processing operations together. As a consequence, for each subset, four neural networks of different sizes were built. A new combining framework for feature set partitioning has been used for textindependent speaker identification <ref type="bibr" target="#b30">[31]</ref>.</p><p>The feature set partitioning can be achieved by grouping features based on pairwise mutual information with statistically  similar features assigned to the same group <ref type="bibr" target="#b31">[32]</ref>. For this purpose, one can use an existing hierarchical clustering algorithm. As a consequence, several feature subsets are constructed by selecting one feature from each group. A neural network is subsequently constructed for each subset. All networks are then combined.</p><p>As part of our previous work <ref type="bibr" target="#b32">[33]</ref>, a simple hill-climbing algorithm, decomposed-oblivious-gain (DOG), was proposed. This algorithm searches for the optimal partitioning using incremental ODTs. One limitation of the DOG algorithm is that it has no backtracking capabilities (for instance, removing a single feature from a subset or removing an entire subset). Furthermore, DOG begins the search from an empty partitioning structure, which may lead to subsets with a relatively small number of features. The limitations of DOG led us to consider a more profound exploration of the search space. This in turn led us to employ a GA, since an exhaustive search for large problems is impractical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">GA and their applicability in feature oriented tasks</head><p>GAs are a popular type of evolutionary algorithm (EA) that have been successfully used for feature selection. Inspired by the Darwinian process of evolution, EAs are stochastic search algorithms. The motivation for applying EAs to data mining tasks is that they offer robust, adaptive search techniques that search the solution space globally <ref type="bibr" target="#b33">[34]</ref>. When an EA is well designed, it continually considers new solutions. Thus, it can be viewed as an "anytime" learning algorithm <ref type="bibr" target="#b34">[35]</ref>. Such a learning algorithm should produce a good-enough solution quite quickly. It then continues to search the solution space, reporting the new "best" solution whenever one is found. Fig. <ref type="figure" target="#fig_1">2</ref> presents a high level pseudocode of GA adapted from Ref. <ref type="bibr" target="#b33">[34]</ref>.</p><p>GAs begin by randomly generating a population of L candidate solutions. Given such a population, a GA generates a new candidate solution (population element) by selecting two of the candidate solutions as the parent solutions. This process is termed "reproduction." Generally, parents are selected randomly from the population with a bias toward the better candidate solutions. Given two parents, one or more new solutions are generated by taking some characteristics of the solution from the first parent (the "father") and some from the second parent (the "mother"). This process is termed "crossover." For example, in GAs that use binary encoding of n bits to represent each possible solution, we might randomly select a crossover bit lo-cation denoted as o. Two descendant solutions could then be generated. The first descendant would inherit the first o string characteristics from the father and the remaining no characteristics from the mother. The second descendant would inherit the first o string characteristics from the mother and the remaining no characteristics from the father. This type of crossover is the most common and it is termed a "one-point crossover." Crossover is not necessarily applied to all pairs of individuals selected for mating: a P crossover probability is used in order to decide whether crossover will be applied. If crossover is not applied, the offspring are simply duplications of the parents.</p><p>Once descendant solutions are generated, GAs allow characteristics of the solutions to be changed randomly, that is, to mutate. In the binary encoding representation, according to a certain probability (P mut ) each bit is changed from its current value to the opposite value. Once a new population has been generated, it is decoded and evaluated. The process continues until some termination criterion is satisfied. A GA converges when most of the population is identical, or in other words, the diversity is minimal. Louis and Rawlins <ref type="bibr" target="#b35">[36]</ref> analyzed the convergence of binary strings using the Hamming distance and showed that traditional crossover operators (such as one-point crossover operators) do not change the average Hamming distance of a given population. In fact, selection is responsible for the Hamming distance convergence. When the GA solves a partitioning problem, then the Rand index <ref type="bibr" target="#b36">[37]</ref> is more appropriate than the Hamming distance.</p><p>Empirical comparisons between GAs and other kinds of feature selection methods can be found in Ref. <ref type="bibr" target="#b37">[38]</ref> as well as in Ref. <ref type="bibr" target="#b38">[39]</ref>. In general, these empirical comparisons show that GAs, with their associated global search in the solution space, usually (though not always) obtain better results than local search-based feature selection methods. In particular, Kudo and Sklansky <ref type="bibr" target="#b38">[39]</ref> compared a GA with 14 non-evolutionary feature selection methods (some of them variants of each other) across eight different datasets. The authors concluded that the advantage of the global search associated with GAs over the local search associated with other algorithms is particularly important in datasets with a large number of features, where "large" was defined as including more than 50 features. Hsu <ref type="bibr" target="#b39">[40]</ref> developed the idea of using GAs for feature selection. Specifically he developed two GA wrappers, one for the variable selection problem for decision tree inducers and the other for the variable ordering problem for Bayesian network structure learning.</p><p>Opitz and Shavlik <ref type="bibr" target="#b40">[41]</ref> applied GAs to ensembles. However, in the algorithm which they developed, the genetic operators were designed explicitly for hidden nodes in knowledge-based neural networks and the algorithm does not work well with problems lacking prior knowledge. In a later study, Opitz <ref type="bibr" target="#b34">[35]</ref> used genetic search for ensemble feature selection. This genetic ensemble feature selection (GEFS) strategy begins by creating an initial population of classifiers where each classifier is generated by randomly selecting a different subset of features. Then, new candidate classifiers are continually produced by using the genetic operators of crossover and mutation on the feature subsets. The final ensemble is composed of the most fitted classifiers. Similarly, the GA that Hu et al. <ref type="bibr" target="#b17">[18]</ref> use for selecting the reducts to be included in the final ensemble first creates N reducts, and then it trains N decision trees using these reducts. It finally uses a GA for selecting which of the N decision trees are included in the final forest.</p><p>Given the positive evidence of the benefits of using GAs for feature selection tasks <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref>, on the one hand, and for creating an ensemble of classifiers <ref type="bibr" target="#b34">[35]</ref> on the other, the rationale for implementing a GA for feature set partitioning is self-evident. In fact, Hsu et al. <ref type="bibr" target="#b41">[42]</ref> presented this idea as part of a position paper. However, there has been no report about whether the idea was implemented and whether it can improve classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Alternatives for the fitness function</head><p>The wrapper approach for evaluating the fitness function has been used in all reported works which utilize either GAs for feature selection per se or feature selection for creating an ensemble of classifiers. In this approach, a certain solution is evaluated by repeatedly sampling the training set and measuring the accuracy of the inducers obtained for feature subsets over a holdout validation dataset. The main advantages of this approach are that it generates reliable evolutions and can be used for any induction algorithm. A major drawback, however, is that the wrapper procedure repeatedly executes the inducer. For this reason, wrappers may not scale well to large datasets containing many features.</p><p>An alternative approach to evaluating performance is to use the generalization error bound in terms of the training error and concept size. In his book "Mathematics of Generalization," Wolpert <ref type="bibr" target="#b42">[43]</ref> discusses four theoretical frameworks for estimating the generalization error, namely: probably approximately correct (PAC), Vapnik-Chervonenkis (VC), Bayesian, and statistical physics. All these frameworks combine the training error (which can be easily calculated) with some penalty function expressing the capacity of the inducers. In this paper we use the VC theory for evaluating the generalization error bound. This choice follows from the use of VC theory in previous works to evaluate decision trees <ref type="bibr" target="#b43">[44]</ref> and ODTs <ref type="bibr" target="#b32">[33]</ref>. Fröhlich et al. <ref type="bibr" target="#b44">[45]</ref> have used a VC dimension bound for guiding a GA while solving the feature selection problem in support vector machines. In the same spirit we opt for using VC dimension theory in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Oblivious decision trees (ODTs)</head><p>When dealing with classification problems, decision tree induction is one of the most widely used approaches (see, for instance, Ref. <ref type="bibr" target="#b45">[46]</ref>). Decision trees are considered to be comprehensible classifiers and easy to follow when they include a few nodes. This paper focuses on feature set partitioning designed for decision trees which are combined using the naïve Bayes combination <ref type="bibr" target="#b46">[47]</ref>. For this purpose, each decision tree should provide a probability estimate. Using the class frequency in the tree leaves as-is will typically overestimate the probability. In order to avoid this phenomenon, it is useful to perform the Laplace correction. According to Laplace's law of succession, the probability of the event y = c i is</p><formula xml:id="formula_0">(m i + kp a priori )/(m + k)</formula><p>where y is a random variable; c i is a possible outcome of y which has been observed m i times out of m observations; p a priori is an a priori probability estimation of the event; and k is the equivalent sample size that determines the weight of the a priori estimation relative to the observed data. This paper concentrates on a specific type of decision tree, the ODT in which all nodes at the same level test the same feature. ODTs are found to be effective for feature selection which is a simplified case of the problem solved here.</p><p>Fig. <ref type="figure" target="#fig_2">3</ref> demonstrates a typical ODT with three input features: the slicing machine model used in the manufacturing process; the rotation speed of the slicing machine and the shift (i.e., when the item was manufactured); and the Boolean target feature representing whether that item passed the quality assurance test. The arcs that connect the hidden terminal nodes and the nodes of the target layer are labeled with the number of records that fit this path. For instance, the twelve items in the training set, which were produced using the old slicing machine that was set up to rotate at a speed greater than 1000 rpm, were classified as "good" items (i.e., passed the quality assurance test).</p><p>The principal difference between the ODT and a regular decision tree structure is the constant ordering of input features at every terminal node of ODT, the property which is necessary for minimizing the overall subset of input features (resulting in dimensionality reduction). Therefore, despite its restriction, an ODT is found to be effective as a feature selection procedure. Almuallim and Dietterichm <ref type="bibr" target="#b47">[48]</ref>, as well as Schlimmer <ref type="bibr" target="#b48">[49]</ref>, have proposed a forward feature selection procedure using construction of ODTs, while Langley and Sage <ref type="bibr" target="#b49">[50]</ref> suggested backward selection using the same means. Recently, Last and Maimon <ref type="bibr" target="#b50">[51]</ref> have suggested a new algorithm for constructing ODTs, called an info-fuzzy network (IFN) based on information theory.</p><p>Since the degree of accuracy of an ODT is usually lower than that of a regular decision tree <ref type="bibr" target="#b50">[51]</ref>, and since the amount of instances that are ascribed to a node exponentially fades as we draw away from the root, an ODT might require more leaves than a regular DT to represent the same classifier. Thus, its leaves are based on a smaller amount of instances, which also leads to less reliable classifications than those of regular deci-sion tree. Nevertheless, it has been shown that the effect of this drawback is diminished for small sets of attributes <ref type="bibr" target="#b50">[51]</ref>. Additionally, previous studies have shown that an ensemble is useful for small classifiers (see for instance Ref. <ref type="bibr" target="#b51">[52]</ref>). Specifically, it has been shown that feature set partitioning is particularly effective with small subsets <ref type="bibr" target="#b12">[13]</ref>.</p><p>Because we are interested in mutually exclusive feature set partitioning, each feature subset is represented by a single ODT and each feature is located on a different layer. As a result, adding a new feature to a subset is performed by adding a new layer and connecting it to the nodes of the last layer. The nodes of a new layer are defined as the Cartesian product combinations of the previous layer's nodes with the values of the new added feature. In order to avoid unnecessary splitting, the algorithm splits a node only if it is useful. In the study reported in this paper, we split a node if the information gain of the new feature in this node was strictly positive.</p><p>The unique structure of the ODT is very convenient for our GA approach. First, because the search space of an ODT is smaller than the search space of a regular DT, it is possible to develop a tighter VC dimension bound, which makes it more practical to use VC dimension bound as a fitness function. Furthermore, using ODTs, moving from one generation to the other usually requires small changes to the subset structures; because each feature is located on a different layer, it is relatively easy to add or remove features incrementally. This approach stands in contrast to regular decision tree inducers, in which every iteration of the search may require generating the decision tree from scratch. Thus, we assume that ODTs are suitable for the problem discussed in this paper. This hypothesis will be put to the test in the experimental study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7.">Originality and contribution</head><p>The novel contributions of this paper include:</p><p>• A new encoding schema specifically designed for feature set partitioning. The new encoding eliminates the redundancy of existing encodings. Together with the new encoding, we also suggest a new crossover operator called "group-wise crossover" (GWC). The new encoding ensures the convergence of the GA. • The use of a structural risk measure to compute the fitness function. The new measure is much faster than the wrapper approach, which is frequently used in studies reported in the literature.</p><p>• A new caching mechanism to speed up the execution and avoid recreation of the same classifier. • An examination of the hypothesis that ODTs are suitable for feature set partitioning. • A detailed experimental study encompassing benchmark data and synthetic data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem formulation</head><p>In a typical classification problem, a training set of labeled examples is given. The training set can be described in a variety of languages, most frequently, as a collection of records that may contain duplicates. A vector of feature values describes each record. The notation A denotes the set of input features containing n features: A = {a 1 , . . . , a i , . . . , a n } and y represents the class variable or the target feature. Features (sometimes referred to as attributes) are typically one of two types: categorical (values are members of a given set), or numeric (values are real numbers). When the feature a i is categorical, it is useful to denote its domain values by dom(a i ). In a similar way, dom(y) = {c 1 , . . . , c |dom(y)| } represents the domain of the target feature. Numeric features have infinite cardinalities.</p><p>The instance space (the set of all possible examples) is defined as a Cartesian product of all the input feature domains:</p><formula xml:id="formula_1">X=dom(a 1 )×dom(a 2 )×• • •×dom(a n ).</formula><p>The universal instance space (or the labeled instance space) U is defined as a Cartesian product of all input feature domains and the target feature domain, i.e., U = X × dom(y). The training set consists of a set of m records and is denoted as S = ( x 1 , y 1 , . . . , x m , y m ) where x q ∈ X and y q ∈ dom(y).</p><p>Usually, it is assumed that the training set records are generated randomly and independently according to some fixed and unknown joint probability distribution D over U. Note that this is a generalization of the deterministic case when a supervisor classifies a record using a function y = f (x).</p><p>The notation I represents a probabilistic inducer (i.e., an algorithm that generates classifiers that also provide estimates of the conditional probability of the target feature given the input features), and I (S) represents a probabilistic classifier which was induced by activating the induction method I onto dataset S. In this case it is possible to estimate the conditional probability PI(S) (y = c j |x q ) of an observation x q . Note the addition of the "hat" -∧ -to the conditional probability estimation is used to distinguish it from the actual conditional probability. We denote the projection of an instance x q onto a subset of features G as G x q . Similarly the projection of a training set S onto G is denoted as G S.</p><p>The problem of partitioning an input feature set is to find the best partition such that, if a specific inducer is trained on each feature subset data, then the combination of the generated classifiers will have the highest possible degree of accuracy. Consequently the problem can be formally phrased as follows:</p><p>Given an inducer I, a combination method C, and a training set S with input feature set A = {a 1 , a 2 , . . . , a n } and target feature y from a distribution D over the labeled instance space, the goal is to find an optimal partitioning Z opt = {G 1 , . . . , G k , . . . , G } of the input feature set A into mutually exclusive subsets G k ⊆ A that are not necessarily exhaustive. Optimality is defined in terms of minimization of the generalization error of the induced classifiers I ( G k ∪y S); k = 1, . . . , combined using method C over the distribution D.</p><p>In this paper we assume that I is any decision tree inducer and C is the naïve Bayes combination. In the naïve Bayes combination, a classification of a new instance is based on the product of the conditional probability of the target feature, given the values of the input features in each subset. Mathematically it can be formulated as follows:</p><formula xml:id="formula_2">v MAP (x q ) = arg max c j ∈dom(y) PI(S) (y = c j ) × k=1 PI( G k ∪y S) (y = c j | G k x q ) PI(S) (y = c j )<label>(1)</label></formula><p>or</p><formula xml:id="formula_3">v MAP (x q ) = arg max c j ∈dom(y) k=1 PI( G k ∪y S) (y = c j | G k x q ) PI(S) (y = c j ) -1 . (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>In the case of decision trees, PI( G k ∪y S) (y =c j | G k x q ) can be estimated by using the appropriate frequencies in the relevant leaf. It should be noted that the optimal partitioning structure is not necessarily unique. Furthermore it is not obligatory that all input features actually belong to one of the subsets. Consequently, the problem can be treated as an extension of the feature selection problem, i.e., finding the optimal partitioning of the form Z opt = {G 1 }, as the non-relevant features are in fact NR = A -G 1 . Moreover, when using a naïve Bayes for combining the classifiers as in this case, the naïve Bayes method can be treated as specific partitioning:</p><formula xml:id="formula_5">Z = {G 1 , G 2 , . . . , G n }, where G i = {a i }.</formula><p>Definition 1 (Classification-preservation partitioning). The partitioning Z = {G 1 , . . . , G k , . . . , G } is said to be classification-preservation if, for each instance in the support of P (x q ), the following is satisfied:</p><formula xml:id="formula_6">∀x q ∈ X, arg max c j ∈dom(y) k=1 P (y = c j | G k x q ) P (y = c j ) -1 = arg max c j ∈dom(y) P (y = c j |x q ).</formula><p>(</p><p>Since the right term of the equation is optimal, it follows that classification-preservation partitioning is also optimal. The importance of finding classification-preservation partitioning is derived from the fact that in real problems with limited training sets it is easier to approximate probabilities with fewer dimensions.</p><p>The following four lemmas are presented in order to shed light on the suggested problem. This set of lemmas defines classification-preservation and demonstrates that conditional independence is not a necessary precondition. More specifically, these lemmas show that the naïve Bayes combination can be useful in various cases of separable functions even when the naïve assumption of conditional independence is not necessarily fulfilled. Furthermore because these lemmas provide the optimal partitioning structures, they can be used for evaluating the performance of the algorithms proposed in Section 4. The proofs of these lemmas are straightforward and appear in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 1 (Sufficient condition). Let Z be a partitioning that satisfies the following conditions:</head><p>1. the subsets G k , k = 1, . . . , and the NR = A -k=1 G k are conditionally independent given the target feature; 2. the NR set and the target feature are independent; then Z is classification-preservation.</p><p>Lemma 1 represents a sufficient condition for classificationpreservation. It is important to note that it does not represent a necessary condition, as illustrated in the following lemma:</p><p>Lemma 2 (The read-once DNF case). Let A={a 1 , . . . , a l , . . . , a n } denote a group of n independent input binary features and let Z={G 1 , . . . , G } denote a partitioning. If the target feature follows the function:</p><formula xml:id="formula_8">y = f 1 (a i , i ∈ R 1 ) ∨ f 2 (a i , i ∈ R 2 ) ∨ • • • ∨ f (a i , i ∈ R ) or y = f 1 (a i , i ∈ R 1 ) ∧ f 2 (a i , i ∈ R 2 ) ∧ • • • ∧ f (a i , i ∈ R ),</formula><p>where f 1 , . . . , f are Boolean functions and R 1 , . . . , R are mutually exclusive, then Z is classification-preservation.</p><p>Lemma 3 (The additive case). Let A = {a 1 , . . . , a l , . . . , a n } be a group of n independent input binary features and let Z = {G 1 , . . . , G } be a partitioning. If the target feature follows the function:</p><formula xml:id="formula_9">y = 2 0 • f 1 (a i , i ∈ R 1 ) + 2 1 • f 2 (a i , i ∈ R 2 ) + • • • + 2 -1 f (a i , i ∈ R ),</formula><p>where f 1 , . . . , f are Boolean functions and R 1 , . . . , R are mutually exclusive, then Z is classification-preservation.</p><p>Lemmas 2 and 3 illustrate that although the conditionally independence requirement is not fulfilled, it is still possible to find a classification-preservation partitioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 4 (The XOR case). Let</head><formula xml:id="formula_10">A = {a 1 , . . . , a i , . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. , a n } be a group of n input binary features distributed uniformly. If the target feature behaves as y</head><formula xml:id="formula_11">= a 1 ⊕ a 2 ⊕ • • • ⊕ a n , then there is no partitioning beside Z = {A}, which is classification- preservation.</formula><p>Lemma 4 shows that there are problems such that no classification-preservation partitioning can be found, besides the obvious one.</p><p>The number of combinations into which n * input features may be decomposed exactly relevant subsets is</p><formula xml:id="formula_12">Q(n * , ) = 1 ! j =0 j (-1) j ( -j) n * . (<label>4</label></formula><formula xml:id="formula_13">)</formula><p>Evidently the number combinations into which n * input features may be decomposed up to n * subsets is</p><formula xml:id="formula_14">C(n * ) = n * =1 Q(n * , ) = n * =1 1 ! j =0 j (-1) j ( -j) n * . (<label>5</label></formula><formula xml:id="formula_15">)</formula><p>In the feature set partitioning problem defined above, it is possible that part of the input feature will not be used by the inducers (the irrelevant set). Thus, the total search space is then</p><formula xml:id="formula_16">T (n) = n n * =0 n n * C(n * ) = n n * =0 n n * n * =1 1 ! j =0 j (-1) j ( -j) n * . (<label>6</label></formula><formula xml:id="formula_17">)</formula><p>Eq. ( <ref type="formula" target="#formula_16">6</ref>) implies that an exhaustive search is intractable for large problems. Thus, a heuristic search algorithm is required. The next section presents a GA for solving this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">A GA method for feature set partitioning</head><p>In order to solve the problem defined in Section 3, we suggest using a GA search procedure. Fig. <ref type="figure" target="#fig_3">4</ref> presents the proposed process schematically. The left side in Fig. <ref type="figure" target="#fig_3">4</ref> specifies the creation of the ODTs ensemble based on feature set partitioning. Searching for the best partitioning is governed by a GA search. Each partitioning candidate is evaluated using a VC dimensionbased evaluator. For this purpose, an ODT is generated for each feature partition. The ODT generator utilizes a caching mechanism in order to reduce the generation time. The output of this process is an ODT ensemble that is then used to classify unlabeled instances (the right side of Fig. <ref type="figure" target="#fig_3">4</ref>). Note that in the suggested procedure, the ensemble's creation is embedded in the partitioning process. One could also consider a slightly different procedure in which the output of the partitioning phase is the partitioning itself and not the ensemble of classifiers. The creation of the ensemble is then performed in a subsequent phase using an inducer that is not limited to ODT. The following sections specify in-depth each of the above-mentioned components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">GA search</head><p>To implement a GA, a schema for encoding, manipulating, and evaluating the solution must be provided. A candidate solution consists mainly of values of variables-in essence, data. In particular, GA individuals are usually represented by a fixedlength linear genome.</p><p>A straightforward individual representation for feature set partitioning consists simply of a string of n integers. Recall that n is the number of features. The ith integer, i=1, . . . , n, can take the value 0, . . . , n, indicating to which subset (if any) the ith feature belongs. A value of 0 indicates that the corresponding feature is not selected and is filtered out. For instance, in a 10feature dataset, the individual '1 0 2 0 1 3 3 2 0 1' represents a candidate solution where the 1st, 5th and 10th features are located in the first subset. The 3rd and 8th are located in the second subset. The 6th and the 7th are located in the third group. All other features are filtered out. This individual representation is simple, and a traditional one-point crossover operator can easily be applied. As for the mutation operator, according to a certain probability (P mut ), each integer is changed from its current value to a different valid value.</p><p>The last representation has redundancy, i.e., the same solution can be represented in several ways. For instance, the illustrated solution '1 0 2 0 1 3 3 2 0 1' can be also represented as '3 0 1 0 3 2 2 1 0 3'. Moreover, similar solutions can be represented in quite different ways. This property can lead to situations in which the offspring are dissimilar to their parents. For example, if we perform the one-point crossover operator on the two equal solutions above-'1 0 2 0 1 3 3 2 0 1' and '3 0 1 0 3 2 2 1 0 3'-we may obtain the following descendant solution '1 0 2 0 3 5 5 1 0 3'. Because the two parents are equal, we expect that the descendant (before mutation) should also be equal. However, this is not the case here and the descendant represents quite a different solution. Although the above case is rare, it still illustrates the problematic character of the above representation. Besides not being compact, the above encoding may result in a slow convergence of the GA. We begin by defining a measure called partitioning structural distance. This measure can be used to determine the distance of two partitioning structures as follows:</p><p>Definition 2 (Partitioning structural distance (revised rand index)).</p><formula xml:id="formula_18">(Z 1 , Z 2 ) = n-1 i=1 n j =i+1 2 • (a i , a j , Z 1 , Z 2 ) n • (n -1) ,<label>(7)</label></formula><p>where (a i , a j , Z 1 , Z 2 ) is a binary function that returns the value "0" if the features a i , a j belong to the same subset in both partitioning structures Z 1 , Z 2 or if a i , a j belong to different subsets in both partitioning structures. In all other cases the function returns the value "1". </p><formula xml:id="formula_19">(a i , a j , Z 1 , Z 2 ) = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ 0, i / ∈ 1 k 1 =1 R 1 k 1 ; i / ∈ 2 k 2 =1 R 2 k 2 and j / ∈ 1 k 1 =1 R 1 k 1 ; j / ∈ 2 k 2 =1 R 2 k 2 , 0, i / ∈ 1 k 1 =1 R 1 k 1 ; i / ∈ 2 k 2 =1 R 2 k 2 and j ∈ 1 k 1 =1 R 1 k 1 ; j ∈ 2 k 2 =1 R 2 k 2 , 0, i ∈ 1 k 1 =1 R 1 k 1 ; i ∈ 2 k 2 =1 R 2 k 2 and j / ∈ 1 k 1 =1 R 1 k 1 ; j / ∈ 2 k 2 =1 R 2 k 2 , 0, ∃ k 1 , k 2 ; i, j ∈ R 1 k 1 , i,j ∈ R 2 k 2 , 0, ∃ k 1,1 = k 1,2 , k 2,1 = k 2,2 ; i ∈ R 1 k 1,1 , j ∈ R 1 k 1,2 ; i ∈ R 2 k 2,1 , j ∈ R 2 k 2,2 , 1 otherwise. (8)</formula><formula xml:id="formula_20">-1 0 0 0 0 -1 a 2 0 1 0 1 0 0 a 3 0 0 1 0 1 0 a 4 0 1 0 1 0 0 a 5 0 0 1 0 1 0 a 6 -1 0 0 0 0 -1</formula><p>For example, given that A = {a 1 , a 2 , a 3 , a 4 , a 5 , a 6 }, Z 1 = {{a 4 , a 2 }; {a 5 , a 3 }} and Z 2 = {{a 1 , a 3 , a 5 }; {a 2 , a 4 }} then</p><formula xml:id="formula_21">(Z 1 , Z 2 ) = n-1 i=1 n j =i+1 2 • (a i , a j , Z 1 , Z 2 ) n • (n -1) = 2 5 • 6 ( (a 1 , a 2 , Z 1 , Z 2 ) + (a 1 , a 3 , Z 1 , Z 2 ) + (a 1 , a 4 , Z 1 , Z 2 ) + (a 1 , a 5 , Z 1 , Z 2 ) + (a 1 , a 6 , Z 1 , Z 2 ) + (a 2 , a 3 , Z 1 , Z 2 ) + (a 2 , a 4 , Z 1 , Z 2 ) + (a 2 , a 5 , Z 1 , Z 2 ) + (a 2 , a 6 , Z 1 , Z 2 ) + (a 3 , a 4 , Z 1 , Z 2 ) + (a 3 , a 5 , Z 1 , Z 2 ) + (a 3 , a 6 , Z 1 , Z 2 ) + (a 4 , a 5 , Z 1 , Z 2 ) + (a 4 , a 6 , Z 1 , Z 2 ) + (a 5 , a 6 , Z 1 , Z 2 )) = 2 30 (1 + 1 + 1 + 1 + 1 + 0 + 0 + 0 + 0 + 0 + 0 + 0 + 0 + 0 + 0) =<label>5 15 .</label></formula><p>By using an adjacency matrix-like encoding, one can represent any partitioning structure as n × n matrix B in which B i,j = 1 if features a i and a j are located in the same group. Additionally B i,j = 1 if features a i and a j are both filtered out. In any other case B i,j = 0. The values on the diagonal indicate whether each feature is included in one of the subsets (1) or not (-1). For example, Table <ref type="table" target="#tab_1">1</ref> illustrates the representation of Z 1 = {{a 4 , a 2 }; {a 5 , a 3 }} given that A = {a 1 , a 2 , a 3 , a 4 , a 5 , a 6 }. Note that because the above matrix is always symmetric, we can specify only the upper triangle.</p><p>Definition 3 (Encoding matrix B is said to be well-defined if). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Commutative</head><formula xml:id="formula_22">: ∀i = j ; B i,j = B j,i 2. Transitive: ∀i = j = k; if B i,j = 0 and B i,k = 0 then B j,k = 0 3. Sign Property: ∀i = j ; if B i,j = 0 then B i,j = B i,i .</formula><formula xml:id="formula_23">Z 1 a 1 -1 0 0 0 0 -1 a 2 0 1 0 1 0 0 a 3 0 0 1 0 1 0 a 4 0 1 0 1 0 0 a 5 0 0 1 0 1 0 a 6 -1 0 0 0 0 -1 Z 2 a 1 1 0 1 1 0 0 a 2 0 1 0 0 0 1 a 3 1 0 1 1 0 0 a 4 1 0 1 1 0 0 a 5 0 0 0 0 1 0 a 6 0 1 0 0 0 1 Z 3 a 1 1 0 1 0 0 0 a 2 0 1 0 1 0 0 a 3 1 0 1 0 0 0 a 4 0 1 0 1 0 0 a 5 0 0 0 0 1 0 a 6 0 0 0 0 0 1 Z 4 a 1 1 0 1 1 0 0 a 2 0 1 0 0 0 0 a 3 1 0 1 1 0 0 a 4 1 0 1 1 0 0 a 5 0 0 0 0 1 0 a 6 0 0 0 0 0 -1</formula><p>We now suggest a new crossover operator called "groupwise crossover" (GWC). In this operator, we select one anchor subset from the subsets that define the first parent partitioning and one anchor subset from the subsets that define the second parent partitioning (the selected subset can also be the filteredout subset). The anchor subsets are used as is, without any addition or diminution of features.</p><p>The first offspring is created by copying the columns and rows of the features that belong to the first selected anchor subset from the first parent. All remaining elements in B are filled in with the corresponding values that are obtained from the second parent. The second offspring is similarly created, using the second anchor subset by copying the appropriate columns and the rows from the second parent. The remaining elements are filled in with the corresponding values from the first parent.</p><p>Example. Assume that two partitioning structures Z 1 = {{a 4 , a 2 }; {a 5 , a 3 }} and Z 2 = {{a 2 , a 6 }; {a 1 , a 4 , a 3 }{a 5 }} are given over the feature set A = {a 1 , a 2 , a 3 , a 4 , a 5 , a 6 }. In order to use a GWC operator, two anchor subsets are selected, one from each partitioning, {a 2 , a 4 } from Z 1 and {a 1 , a 4 , a 3 } from Z 2 . Table <ref type="table" target="#tab_2">2</ref> illustrates representations of the Z 1 and Z 2 and their offspring Z 3 and Z 4 . Z 3 is obtained by keeping the group {a 2 , a 4 } while the remaining elements are copied from Z 2 . Z 4 is obtained by keeping the group {a 1 , a 4 , a 3 } while the remaining elements are copied from Z 1 . Thus, Z 3 = {{a 2 , a 4 }; {a 1 , a 3 }; {a 5 }; {a 6 }} and Z 4 = {{a 1 , a 4 , a 3 }; {a 5 }; {a 2 }}. The highlighted elements indicate the selected group that was copied into the offspring.</p><p>The following set of lemmas shows that the well-defined property of an adjacency matrix is preserved under a GWC operator.</p><p>Lemma 5 (Structural distance measure properties). The structural distance measure has the following properties:</p><formula xml:id="formula_24">1. Symmetry: (Z 1 , Z 2 ) = (Z 2 , Z 1 ) , 2. Positivity: (Z 1 , Z 2 ) = 0 Iff Z 1 = Z 2 , 3. Triangular inequality: (Z 1 , Z 2 ) (Z 1 , Z 3 ) + (Z 2 , Z 3 ). Lemma 6.</formula><p>A projection of a well-defined encoding matrix is a well-defined encoding matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 7. Using a GWC operator on two well-defined encoding matrices generates a new well-defined encoding matrix.</head><p>Lemma 8. Operator GWC creates two offspring with an interdistance that is not greater than the inter-distance of their parents.</p><p>Lemma 8 indicates that the GWC operator together with the proposed encoding does not slow down the convergence of the GA. Together with the selection process that prefers solutions with higher fitness values, one can ensure that the algorithm converges.</p><p>As to the mutation operator, according to a certain probability (P mut ) each feature can be cut off from its original group to join another randomly selected group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Fitness function</head><p>In each iteration, we have to create a new population from the current generation. The selection operation determines which parent chromosomes participate in producing offspring for the next generation. Usually, members are selected for mating with a selection probability proportional to their fitness values. The most common way to implement this method is to set the selection probability p i equal to</p><formula xml:id="formula_25">p i = f i j f j . (<label>9</label></formula><formula xml:id="formula_26">)</formula><p>For a classification problem, the fitness value f i of the ith member can be the generalized accuracy. Note that using training accuracy as is does not suffice to evaluate classifiers due to the over-fitting phenomena.</p><p>The most straightforward way to estimate generalization error is to use the wrapper procedure. In this approach the partitioning structure is evaluated by repeatedly sampling the training set and measuring the accuracy of the inducers obtained for this partitioning on an unused portion of the training set. This is the most common approach for evaluating the fitness function in feature selection problems. However, as stated in Section 2, the fact that the wrapper procedure repeatedly executes the inducer is considered a major drawback. According to the VC theory, the bound on the generalization error of hypothesis space H with finite VC dimension d is given by </p><formula xml:id="formula_27">| (h, D)-ˆ (h, S)| d• ln 2m d +1 -ln 4 m ∀h∈H, ∀ &gt;0<label>(10</label></formula><formula xml:id="formula_28">f i = 1 - (h i , D).</formula><p>In order to use the bound (Eq. ( <ref type="formula" target="#formula_27">10</ref>)), one needs to measure the VC dimension. The VC dimension for a set of indicator functions is defined as the maximum number of data points that can be shattered by the set of admissible functions. By definition, a set of m points is shattered by a concept class if there are concepts (functions) in the class that split the points into two classes in all of the 2 m possible ways. The VC dimension, which might be difficult to compute accurately, depends on the induction algorithm.</p><p>As stated before, using an ODT may be attractive in this case since it adds features to a classifier in an incremental manner. Due to the fact that ODTs can be considered as restricted decision trees, any generalization error bound that has been developed for decision trees in studies reported in the literature can be used in this case as well. However, there are several reasons for developing a specific bound. First, by utilizing the fact that the oblivious structure is more restricted, it might be possible to develop a tighter bound. Second, it is necessary to extend the bound for several oblivious trees combined using the naïve Bayes combination.</p><p>The following theorem introduces an upper and lower bound of the VC dimension that was recently used by the DOG algorithm. The hypothesis class of multiple mutually exclusive ODTs can be characterized by two vectors and one scalar: L = (l 1 , . . . , l ) , T = (t 1 , . . . , t ) and n, where l k is the numbers of layers (not including the root and target layers) in the tree k, t k is the number of terminal nodes in the tree k, and n is the number of input features.</p><p>For the sake of simplicity, the bound described in this section is developed on the assumption that the input features and the target feature are both binary. This bound can be extended for other cases in a straightforward manner. Note that each ODT with non-binary input features can be converted to a corresponding binary ODT by using appropriate artificial features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 1 (Upper and lower bound for VC dimension of multiple ODTs combined with naïve Bayes). The VC dimension of</head><p>mutually exclusive ODTs on n binary input features that are combined using the naïve Bayes combination and that have L = (l 1 , . . . , l ) layers and T = (t 1 , . . . , t ) terminal nodes is not greater than:</p><formula xml:id="formula_29">F + log U, = 1 2(F + 1) log(2e) + 2 log U, &gt; 1</formula><p>and at least: F -+ 1, where</p><formula xml:id="formula_30">F = i=1 t i , U= n! !•(n-i=1 l i )! • i=1 (2t i -4)! (t i -2)!•(t i -2)! .</formula><p>The proof of this theorem is provided in Appendix A.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Caching mechanism</head><p>The Achilles heel of using GAs in feature set partitioning problems is the requirement to create a classifier for each subset in each solution candidate. Assuming that there are G generations, that the population size is L, and that each solution has on average D subsets, then G • L • D classifiers are created. Recall that by using ODTs we might not need to create each classifier from scratch but rather be able to reuse classifiers that have already been created. Since it is well known that one can trade computational complexity with storage complexity, we suggest using the caching mechanism presented here.</p><p>First, when moving from one generation to the consequent generation, we can exploit all subsets that have remained unchanged. By means of the GWC operator and ignoring the mutation, each member in the new population has at least one subset (the anchor subset) that has not been changed at all. Moreover, all other subsets have some common members. However, in that case, we cannot use the ODT as is because the original ODT might have unused features in the inherited subset. For this purpose we eliminate features from the original ODT, layer by layer, until we obtain an ODT, which can be used in the inherited subset.</p><p>Example. Assume that two partitioning structures Z 1 = {{a 2 , a 4 }; {a 5 , a 3 }} and Z 2 = {{a 2 , a 6 }; {a 1 , a 4 , a 3 }{a 5 }} are given over the feature set A = {a 1 , a 2 , a 3 , a 4 , a 5 , a 6 }. We also assume that in the previous generation the following feature order has been used in the created ODT: a 4 → a 2 ; a 5 → a 3 ; a 2 → a 6 ; a 1 → a 3 → a 4 ; a 5 Recall that by using the GWC operator (and ignoring the mutation operator), the following subsets may be obtained: Z 3 = {{a 2 , a 4 }; {a 1 , a 3 }; {a 5 }; {a 6 }} and Z 4 = {{a 1 , a 4 , a 3 }; {a 5 }; {a 2 }}. Thus, in order to create the ODTs for Z 3 and Z 4 , we can use the following ODTs as is: a 4 → a 2 ; a 1 → a 3 → a 4 ; a 5 . The ODT for {a 6 } will be created from scratch. The remaining subsets can be (partially or completely) obtained by removing features from the existing ODTs. The ODT for {a 1 , a 3 } can be obtained by removing feature a 4 from a 1 → a 3 → a 4 . This removal is possible since a 4 is located last. The ODT for {a 2 } can be obtained by removing feature a 6 from a 2 → a 6 .</p><p>In addition to the ODTs of the previous generations, we can use the existing ODTs in a different subset of the current generation. While generating an ODT, we check at the end of each iteration (i.e., after adding a new feature to the ODT) whether there is another solution in the current generation that also groups these features together in the same subset. If this is the case, we store the current ODT in the cache for future use. Later, when the time has come to generate the ODT for the solution with the common subset, instead of creating the tree from scratch we make use of the tree that was stored in the caching mechanism. For example, we are given in the first generation the following members: Assuming that we are evaluating the members one by one according to the above order, and that while creating the tree for the first subset in the first solution we get an ODT with the following order a 5 → a 1 → a 6 , then we might want to store this ODT in the caching mechanism, and use it also for members 2 and 3.</p><formula xml:id="formula_31">Z 1 = {{a 1 , a 4 ,</formula><p>It should be noted that utilizing this caching mechanism reduces the search space, because it dictates the order in which the features are located in the ODT. For instance, in the last example, the first tree of solution 2 could have the following structure: a 8 → a 1 → a 5 → a 6 . However, by using the ODT a 5 → a 1 → a 6 that was stored in the cache, we a priori ignore this structure. In order to solve this dilemma, we decide not to store small ODTs (in this paper fewer than three features). In such cases the saving in computational cost is not worth the loss in generalization capability.</p><p>Obviously, it is desirable to store the longest common subset in the cache. Thus, in each iteration we check if the current ODT can still be used by the same number of solutions. If this is the case, the current ODT will replace the older one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Classification of an unlabeled instance</head><p>After multiple ODTs have been created, the following steps may be performed to classify an unlabeled instance:</p><p>(A) For each tree:</p><p>1. Locate the appropriate leaf for the unseen instance.</p><p>For every instance there is exactly one path from the root to the relevant leaf. The relevant leaf is chosen by navigating from the root of the tree down to a leaf, according to the outcome of the decision tests along the path. 2. Extract the frequency vector. The frequency vector has an entry for every possible class value. The value in a certain entry is calculated according to the number of training instances that have been navigated</p><p>to the selected leaf and have been labeled with that class. 3. Transform the frequency vector to a probability vector according to Laplace's law of succession, as described in Section 2. (B) Combine the probability vectors using the naïve Bayes combination. (C) Select the class that maximizes the naïve Bayes combination. In the case of a tie, we select the class with the highest a priori probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental study</head><p>In order to illustrate the potential of the feature set partitioning approach in classification problems and to evaluate the performance of the proposed GA, a comparative experiment was conducted on benchmark datasets. The following subsections describe the experimental setup and the results obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Algorithms used</head><p>This study examines an implementation of a GA in feature set partitioning using the suggested adjacency matrix-encoding, GWC operator, and fitness function based on the VC dimension of multiple ODTs combined with naïve Bayes. This algorithm is called GOV (genetic algorithm for ODTs using VC dimension upper bound). It uses a population of 50 chromosomes and has been executed for no more than 50 generations.</p><p>The GOV algorithm is compared to DOG, our previous hillclimbing algorithm for feature set partitioning, as well as to the following single-classifier algorithms: IFN (a greedy ODT inducer that uses gain ratio as the splitting criteria), naïve Bayes and C4.5. The first two algorithms were chosen because they represent specific points in the search space of the GOV algorithm. The C4.5 algorithm was selected because it is considered a state-of-the-art decision tree algorithm which has been used widely in many other comparative studies.</p><p>In the second part of the experiment, the new algorithm is also compared to GEFS, AdaBoost, AB all of which are nonmutually exclusive ensemble algorithms, i.e., algorithms that may use the same feature in several classifiers of the ensemble. All these ensemble methods use the C4.5 as the base classifier. The GEFS employs a wrapper evaluator, which was set to perform five folds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Datasets</head><p>The selected algorithms were examined on 26 datasets, 23 of which were selected manually from the UCI Machine Learning Repository <ref type="bibr" target="#b52">[53]</ref> and are widely used by the pattern recognition community for evaluating learning algorithms. The remaining datasets were chosen from the NIPS2003 feature selection challenge (see http://clopinet.com/isabelle/Projects/NIPS2003/). The datasets vary across such dimensions as the number of target classes, of instances, of input features and their type (nominal, numeric).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Metrics measured</head><p>In this experiment the following metrics were measured:</p><p>(A) Generalized accuracy: This represents the probability that an instance was classified correctly. In order to estimate the generalized accuracy, a 10-fold cross-validation procedure was repeated five times. For each 10-fold cross-validation, the training set was randomly partitioned into 10 disjoint instance subsets. Each subset was utilized once in a test set and nine times in a training set. The same cross-validation folds were implemented for all algorithms. Since the average accuracy is a random variable, the confidence interval was estimated by using the normal approximation of the binomial distribution. Furthermore, the one-tailed paired t-test with a confidence level of 95% verified whether the differences in accuracy between the GOV algorithm and the other algorithms were statistically significant. In order to conclude which algorithm performs best over multiple datasets, we followed the procedure proposed in Ref. <ref type="bibr" target="#b53">[54]</ref>.</p><p>In the case of multiple classifiers we first used the adjusted Friedman test in order to reject the null hypothesis and then the Bonferroni-Dunn test to examine whether the new algorithm performs significantly better than existing algorithms. In the case of only two classifiers, we use the Wilcoxon test. (B) Classifier complexity: Since this paper focuses on decision trees, classifier complexity was measured as the total number of nodes, including the leaves. For multiple decision trees classifiers, the complexity was measured as the total number of nodes in all trees. (C) Computational cost: The running time required for producing the composite classifier.</p><p>The following additional metrics were measured in order to characterize the partitioning structures obtained by the GOV algorithm:</p><p>(A) Number of subsets. (B) Average number of features in a single subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparing single-classifier algorithms</head><p>Table <ref type="table" target="#tab_4">3</ref> presents the results obtained by averaging five standard 10-fold cross-validation experiments. The results indicate that there is no significant case where either naïve Bayes or IFN was more accurate than GOV. On the other hand, GOV was significantly more accurate than naïve Bayes and IFN in 16 databases and 14 databases, respectively. Moreover, GOV was significantly more accurate than C4.5 in 13 databases, and less accurate in only two databases. GOV's classifier complexity (total number of nodes) was comparable to the complexity of the C4.5 algorithm obtained in most of the cases.</p><p>The results of the experimental study are encouraging. On the datasets obtained from the UCI repository, the GOV outperformed naïve Bayes mostly when the data were large in size or had a small number of features. For moderate dimensionality The superscript "+" indicates that the degree of accuracy of GOV was significantly higher than the corresponding algorithm at a confidence level of 95%. The "-" superscript indicates the accuracy was significantly lower.</p><p>(from 50 features up to 500), the performance of naïve Bayes was not necessarily inferior. More specifically, regarding the datasets OPTIC, SONAR, SPI, AUDIOLOGY, LUNG-CANCER, the superiority of GOV over naïve Bayes was statistically significant only in three features (SPI, AUDIOLOGY, LUNG-CANCER). However, for high dimensionality datasets (having at least 500 features), GOV significantly outperforms naïve Bayes in all cases. The null-hypothesis, that all classifiers perform the same and the observed differences are merely random, was rejected using the adjusted Friedman test. We proceeded with the Bonferroni-Dunn test and found that GOV statistically outperforms naïve Bayes and IFN with a 95% confidence level. Using Hochberg's step-up procedure, we found that GOV statistically outperforms C4.5 with a confidence level of 90%.</p><p>Analysis of the number of features in each subset shows that the GOV algorithm tends to build small subsets. Moreover, there are two cases (OPTIC and MONKS3) in which the GOV algorithm used only one feature in each tree. In these cases the classifiers that were built are equivalent to naïve Bayes. This suggests that in some cases GOV acts as a feature selection procedure for naïve Bayes.</p><p>A comparison of the accuracy of GOV and DOG indicated that in most cases GOV obtained better results. This observation is not surprising, considering the fact that GOV performs a more intensive search than DOG. A comparison of the mean number of subsets obtained by DOG <ref type="bibr">(11.58)</ref> and that obtained by GOV (6.7) indicates that DOG tends to have more subsets. Moreover, in 16 datasets out of 26 DOG incorporated more features than GOV. However, for high dimensionality datasets (having at least 500 features), GOV significantly used more features than DOG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Comparing to ensemble algorithms</head><p>Since the accuracy and the classifier complexity are affected by the ensemble size (number of classifiers), we examined various ensemble sizes. Following the empirical results for asymptotic convergence of ensembles <ref type="bibr" target="#b5">[6]</ref>, the ensemble sizes created using the GEFS algorithm included up to 15 classifiers. Similarly, the ensemble size created with the AdaBoost included up to 25 classifiers. Table <ref type="table" target="#tab_5">4</ref> presents the results obtained based on a 10-fold cross-validation procedure which was repeated five times.</p><p>As can be seen from Table <ref type="table" target="#tab_5">4</ref>, the predictive accuracy of GOV algorithm tends to be only slightly worse than that of AdaBoost. There are datasets in which the GOV algorithm obtained a degree of accuracy similar to that of GEFS and AdaBoost (with the AUST dataset). There are cases in which GEFS or AdaBoost achieved much higher degrees of accuracy (AUDIOLOGY and HEPATITIS) and there are cases in which GOV achieved the most accurate results (with the BCAN or MADELON datasets).</p><p>A statistical analysis of the results of the entire dataset collection indicates that in nine datasets AdaBoost achieved significantly higher accuracies (note that the compared value is the best degree of accuracy achieved by enumerating the ensemble size from 1 to 25). On the other hand, GOV was significantly more accurate than AdaBoost in only four datasets including the high dimensional datasets, MADELON and DEXTER. GOV was significantly more accurate than GEFS in nine datasets while GEFS was significantly more accurate than GOV in only four datasets. GOV was significantly more accurate than AB in eight datasets, while AB was significantly more accurate in four datasets.</p><p>The null-hypothesis that all classifiers perform the same was rejected using the adjusted Friedman test with a confidence level of 95%. However, when we used the Bonferroni-Dunn test, we could not reject the null-hypothesis that GOV and Ad-aBoost perform the same at confidence levels of 95% and 90%, respectively. Moreover we could not reject the null-hypothesis that GOV and GEFS perform the same at confidence levels of 95% and 90%, respectively. However, using the same test, we found that GOV significantly outperforms AB with a confidence level of 95%.</p><p>The above results disregard the classifier complexity. Generally, in the UCI datasets, a small loss in accuracy (the mean difference is about 2%) is compensated for by a considerable reduction in the number of nodes (on average, the algorithm uses about 1% of the nodes that are used by AdaBoost). In the NIPS datasets, which are articulated by many input features, GOV gained an improvement of about 4% in the degree of accuracy, but still kept the lowest number of nodes in the forest (on average, the algorithm uses about 40% of the nodes that are used by AdaBoost). GEFS does not show any advantages at all since it has the lowest average accuracy while using more nodes than GOV.</p><p>By taking into consideration the classifier's complexity, we compared the accuracy obtained by the AdaBoost algorithm with that of the GOV algorithm using the same complexity of the GOV classifier. Because it is impossible to tune the Ad-aBoost classifier's complexity to a certain value, we interpolate the two closest points in the AdaBoost's accuracy-complexity graph that bounds this value, on condition that these points are "dominant," i.e., there are no less complicated points in the AdaBoost's graph that have a higher degree of accuracy. Geometrically this means that we examined the datasets in which the GOV point is significantly above or below the AdaBoost's trend line. If no such pair of points could be found, we used the highest degree of accuracy whose complexity was less than or equal to the GOV's classifier complexity. If no such point could be found, we used the first point (ensemble of size 1). Fig. <ref type="figure" target="#fig_5">5</ref> illustrates the complexity-accuracy tradeoff for the Audiology dataset. The X-axis refers to the classifier complexity (the total number of nodes) and the Y-axis refers to the classification accuracy. The series of quadrangle points AdaBoost 1 to AdaBoost 4 refer to an AdaBoost ensemble with 1-4 classifiers, respectively. The circle point refers to the result obtained by GOV. Because the complexity of GOV is greater than that of AdaBoost 2 but less than that of AdaBoost 3, we interpolate these two points (the full line). The triangle point indicates the interpolated value with the same complexity as GOV (the dashed line). Because GOV has a higher degree of accuracy, it is considered to be the winner in the Audiology dataset. The superscript "+" indicates that the degree of accuracy of GOV was significantly higher than the corresponding algorithm at a confidence level of 95%. The "-" superscript indicates the accuracy was significantly lower. The accuracy-complexity tradeoff analysis indicates that GOV significantly outperformed AdaBoost in 13 datasets while AdaBoost significantly outperformed GOV in only three datasets (TTT, VOTE, LABOR). With two of these datasets, the complexity of AdaBoost was much higher than the GOV complexity (because the single C4.5 decision tree already contained more nodes than the GOV classifiers). In other words, the AdaBoost is not necessarily better in these cases because GOV introduces new points in the complexity-accuracy tradeoff. Furthermore, in two of these three datasets (TTT, VOTE), a single C4.5 has already significantly outperformed the GOV algorithm. This observation seems to imply that the limited structure of ODTs used in the GOV algorithm compared to the C4.5 decision tree implemented in AdaBoost might be the reason for the poor results in these cases. In addition, GOV obtained better accuracy-complexity tradeoff than AdaBoost for all datasets with moderate dimensionality (number of features between 50 and 100) and with high dimensionality (number of features greater than 100). The accuracy-complexity tradeoff analysis indicates that GOV significantly outperformed GEFS in 16 datasets, while there is no significant case where GEFS outperformed GOV.</p><p>The null-hypothesis that all classifiers perform the same for the same complexity level was rejected using the Friedman test with a confidence level of 95%. The Bonferroni-Dunn test indicates that the hypothesis that GOV and AdaBoost perform the same at confidence levels of 95% and 90%, respectively, cannot be rejected. However, the same test indicates that GOV significantly outperforms GEFS at a confidence level of 95%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Analysis of computational cost</head><p>The aim of this section is to compare the computational cost of the various methods by measuring the running time. Table <ref type="table" target="#tab_6">5</ref> presents the actual time (in seconds) required for producing the composite classifier. We conducted all of our experiments on the following hardware configuration: a desktop computer implementing a Windows XP operating system with Intel Pentium 4-2.8 GHz, and 1 GB of physical memory.</p><p>GOV is consistently faster than GEFS, with the savings in time becoming more significant when the data dimensionality increases. These results might be due to three different properties of the GOV algorithm. First, instead of using the wrapper approach, which requires several repetitions of the decision tree training, we used the VC-based evaluation approach. Second, since GOV uses a caching mechanism together with the ODT representation, most of the training is not performed from the very beginning. Third, due to the feature set partitioning, the classifiers members are simpler than the GEFS (fewer nodes), and thus require less time to be trained.</p><p>AdaBoost and DOG have a similar running time, DOG being slightly faster. Both AdaBoost and DOG are faster than GOV. This may be due to the fact that GOV, like any other GA-based algorithm, performs a much more extensive search. However, it is encouraging that the running time of GOV is not intrinsically longer than that of AdaBoost and DOG. Naturally the single classifiers took the shortest running time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Evaluation of the new contributions</head><p>In this section we compare five different variations of the proposed algorithm. First, we evaluate the contribution of the new fitness function by comparing it to the wrapper approach, which is frequently used by other GA-based algorithms. The wrapper approach usually provides a better approximation to the generalization error than do theoretical methods. However, it adds considerable overhead to an already expensive search process. Moreover, we evaluate the contribution of the new encoding schema by comparing it to the straightforward representation of integers presented in Section 4.1. Finally we show which of the VC bounds (lower or upper) is more suitable as a fitness function.</p><p>Table <ref type="table" target="#tab_7">6</ref> presents the results of the five different variants. Each variant is based on a different fitness function (wrapper, upper VC, lower VC) and on a different encoding type (simple, new). All variants have been executed with the same population size and the same number of generations. The wrapper variants have used the IFN algorithm for creating the ODT. The last row in the table presents the corresponding average ranks. The null-hypothesis that all classifiers perform the same for the same complexity level was rejected using the Friedman test at a confidence level of 95%. Implementation of the Nemenyi test to compare all classifiers with each other indicates that there are no significance differences between "upper VC-new" (GOV) and "wrapper-new." The same conclusion is obtained when comparing "upper VC-simple" and "wrapper-simple." However, the Nemenyi test indicates that "upper VC-new" significantly outperforms "lower VC-new" at confidence levels of 95%. Moreover "upper-VC-new" significantly outperforms "upper-VC-simple". Thus, we can conclude that 1. The new encoding is better than the simple encoding. 2. The upper-VC-based fitness function and the wrapper-based fitness function provide equivalent accuracies. Since the upper-VC is much faster, it is preferable. 3. The VC lower bound is too "rough" to be used by the GA's fitness function. It is well known that VC dimension theory does not accurately evaluate generalization capabilities (see, for instance, Ref. <ref type="bibr" target="#b54">[55]</ref>). However, the last result indicates that in our case using the upper VC dimension bound is sufficient. This is due to the fact that we are not interested in the accuracy itself, but use the bound only to compare solutions. Thus, the imprecision is less crucial, especially if in most of the cases the pairwise dominance is retained, namely: if the generalized error of solution A is lower than that of solution B, then the VC bound of A is also lower than that of solution B. Moreover, because we take specific account of the restricted structure of decision tree (ODT), the obtained VC bound is tighter than those provided for a general decision tree. This makes this bound more applicable in practice than previous existing VC bounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8.">The suitability of ODTs to feature set partitioning</head><p>As Table <ref type="table" target="#tab_4">3</ref> shows, a single, regular DT usually outperforms a single ODT. In this section we examine the suitability of ODTs for feature set partitioning. We compare the performance of the new encoding first with the ODT (with IFN algorithm) and then with a regular DT (with C4.5 algorithm). In both cases the wrapper approach is used to calculate the fitness functions, Table <ref type="table" target="#tab_8">7</ref> presents the results obtained for each method.</p><p>It can be seen that in most of the datasets these two methods obtained similar results. There are two datasets (TTT and Vote) in which the superiority of C4.5 is statistically significant. On the other hand, there are two datasets (SPI and Zoo) in which ODT was superior. The last row in the table presents the corresponding average ranks. This measure indicates that the regular DT slightly outperforms ODT. However, the null-hypothesis that the two classifiers perform the same cannot be rejected using the Wilcoxon test with a confidence level of 95%. Thus, we conclude that there is no reason to prefer regular DT to ODT in feature set partitioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.9.">The Performance of the GOV algorithm in artificial cases</head><p>This section examines the capability of the GOV algorithm to converge into the classification-preservation partitioning structure. Recall that in certain artificial cases Lemmas 1 and 2 define efficient partitioning structures that are classificationpreservation. Thus, having synthetically created datasets according to the conditions of Lemmas 1 and 2, we now examine the convergence of the GOV algorithm as a function of the training set size.</p><p>The first group of synthetic datasets is based on read-once DNF functions (each variable appears at most once). This experiment examined 16 datasets. Each dataset is denoted by DNF(m, l), where m indicates the number of disjunctions and l the number of features in each disjunction. The input feature values were drawn from a uniform distribution. Note that the read-once DNF problem was investigated in the past and there are several polynomial time induction algorithms that are PAClearnable under uniform distribution (see, for example, Ref. <ref type="bibr" target="#b55">[56]</ref>). It should be noted that, although these algorithms are very efficient in learning specific Boolean function structures, they are limited in their capability to learn general domain problems as required in practice.</p><p>The second synthetic dataset group examined the ability of the proposed algorithms to converge to the optimal partitioning structure as presented in Lemma 1. All datasets in this group contained several binary input features and a binary class. The synthetic data were generated in such a manner that all features were relevant for modeling the class and the feature set could be divided into m conditionally independent groups of l features each. In order to obtain this synthetic dataset, the following procedure was performed for each class:</p><p>1. All input features were randomly allocated into m equally sized groups of l features. 2. For each value combination (i) of each group (j) and for each value of the target feature, a value 0 p i,j,k 1 is randomly selected such that 2 l i=1 p i,j,k =1; ∀j, k, where p i,j,k denotes the probability of the features in group j to get the value combination i when the target feature obtains the value k. Note that, because in each group there are exactly l binary features, then there are 2 l value combinations.</p><p>In order to fabricate one instance, the value of the target feature was sampled first (assuming uniform distribution). The values of all input features were then sampled according to the appropriate distribution.</p><p>Table <ref type="table">8</ref> presents the results obtained by executing the GOV on each problem on different training set sizes. It can be seen that the partitioning structural distance (PSD) of GOV from the classification-preservation partitioning decreases with the size of the training set. Moreover, in simple cases having only three disjunctions, the distance algorithm converges to 0 with a training set of 400 instances. A similar observation can be identified in the INDEP datasets. The GOV algorithm converges to the classification-preservation partitioning as the training set size increases. When the problem is simpler (i.e., there are fewer features), then the distance is shorter for the same training set. This is not surprising because in larger problems the search space increases in an exponential manner. Evidently the GOV algorithm is capable of identifying the desired structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.10.">Discussions</head><p>The advantages of the new GOV algorithm, as made clear from the experimental study, can be summarized as following:</p><p>1. When compared to the state-of-the-art ensemble methods, GOV provides classifiers which are of an equivalent or The GOV algorithm has also several drawbacks:</p><p>1. It is slower than non-GA feature set partitioning methods.</p><p>2. The fact that it is specifically designed for an ODT is considered to be its Achilles' heel. Potentially, there might be domains in which using the ODT as the base classifier will dramatically reduce accuracy. A partial solution in such cases would be to use ODTs internally as an agile inducer only for the feature set partitioning phase. Subsequently, when a good partition is obtained, we can employ more sophisticated inducers on each subset. Similarly, as stated in Section 2, a single ODT has been used for the preprocess phase of feature selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we have presented a novel genetic algorithm for finding the best mutually exclusive feature set partitioning. The basic idea is to decompose the original set of features into several subsets, build a decision tree for each projection, and then combine them. This paper examines whether genetic algorithms can be useful for discovering the appropriate partitioning structure.</p><p>For this purpose we suggested a new encoding schema and fitness function that were specially designed for feature set partitioning with oblivious decision trees. Additionally a caching mechanism was implemented in order to reduce computational cost.</p><p>The algorithm was evaluated on a wide range of standard datasets containing continuous, categorical, and binaryvalued attributes. The results show that this algorithm outperforms other state-of-the-art ensemble methods in the accuracy-complexity tradeoff. This observation leads us to conclude that the proposed algorithm can be used for creating compact ensemble structures.</p><p>Additional issues to be further studied include: how the feature set partitioning concept can be implemented with other inducers such as neural networks and other techniques for combining the generated classifiers (such as voting).</p><p>According to the complete probability theorem:</p><formula xml:id="formula_32">P (y = 1) = 1 - i=1 P (f i = 0) and P (y = 1|f i = 0) = 1 - k =i P (f k = 0). What is left to prove is: arg max y∈{0,1} i=1 k =i P (f k = 0) ( i=1 P (f i = 0)) -1 , i=1 (1 -k =i P (f k = 0)) (1 -i=1 P (f i = 0)) -1 = 0.</formula><p>As the first argument of the argmax function equals 1, it is required to show that</p><formula xml:id="formula_33">i=1 (1 -k =i P (f k = 0)) (1 -i=1 P (f i = 0)) -1 &lt; 1.</formula><p>The last inequality can be validated by multiplying the numerator and denominator by (1 -i=1 P (f i = 0)) with the assumption that 1 &gt; (1 -i=1 P (f i = 0)) &gt; 0.</p><p>(Note: If the term is equal to 0, then P (y = 1) = 0 and if the term is equal to 1 then P (y = 1) = 1. In both cases the partitioning Z is classification-preservation.)</p><formula xml:id="formula_34">(1 -i=1 P (f i = 0)) • i=1 (1 -k =i P (f k = 0)) (1 -i=1 P (f i = 0)) • (1 -i=1 P (f i = 0)) -1 = 1 - i=1 P (f i = 0) • i=1 (1 -k =i P (f k = 0)) (1 -j =1 P (f j = 0)) . Because k =i P (f k = 0) j =1 P (f j = 0): i=1 (1 -k =i P (f k = 0)) (1 -j =1 P (f j = 0)) 1 or 1 - i=1 P (f i = 0) • i=1 (1 -k =i P (f k = 0)) (1 -j =1 P (f j = 0)) &lt; 1.</formula><p>To complete the proof, it is required to show that it is true also for the case of</p><formula xml:id="formula_35">y = f 1 (G 1 ) ∧ f 2 (G 2 ) ∧ • • • ∧ f (G ).</formula><p>For this purpose it is sufficient to show that it is true for the opposite target feature y. According to Morgan's law:</p><formula xml:id="formula_36">y = f 1 (G 1 ) ∨ f 2 (G 2 ) ∨ • • • ∨ f (G ), y = f 1 (G 1 ) ∨ f 2 (G 2 ) ∨ • • • ∨ f (G ) = f * 1 (G 1 ) ∨ f * 2 (G 2 ) ∨ • • • ∨ f * (G ).</formula><p>Because Z is classification-preservation for y it is classificationpreservation for y as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Proof of Lemma 3</head><p>In order to prove this lemma it is useful to define the following functions: bit(i, x) = The th bit of x = (x -2 i • x/2 i )/2 i-1 XNOR(x, y) = x • y + (1x) • (1y) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Proof of Lemma 4</head><p>Obviously if Z = {A} then Z contains at least one subset. If there are an odd number of input features with the value "1" then the target feature should get the value "1" as well. For that reason the posteriori probability for the target feature to get "1" given only subset of the input feature set is 1  2 .</p><p>P (y = 1|S ⊂ A) = 1 2 . That is to say k=1 P (y = 1| G k x) P (y = 1) -1 = k=1 P (y = 0| G k x) P (y = 0) -1 = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Proof of Lemma 5</head><p>The proof of the first property of Lemma 5 results explicitly from definition. So does the proof of the first direction of property 2 of Lemma 5, namely, if Z 1 =Z 2 then (Z 1 , Z 2 )=0.</p><p>The opposite direction, namely if (Z 1 , Z 2 )=0 then Z 1 =Z 2 , is proved by contradiction. We assume that there are cases where (Z 1 , Z 2 ) = 0 but Z 1 = Z 2 . If Z 1 = Z 2 then without loss of generality ∃G 1 i ∈ Z 1 such that there is no G 2 j ∈ Z 2 which fulfill G 1 i = G 2 j . Consequently ∃a i , a j such that (a i , a j , Z 1 , Z 2 ) = 1, which contradict the assumption and therefore our original assumption that (Z 1 , Z 2 ) = 0 but Z 1 = Z 2 must be false.</p><p>In order to prove property 3 of Lemma 5, note that</p><formula xml:id="formula_37">(Z 1 , Z 3 ) + (Z 2 , Z 3 ) = n-1 i=1 n j =i+1</formula><p>2 • (a i , a j , Z 1 , Z 3 ) + (a i , a j , Z 2 , Z 3 ) n • (n -1) .</p><p>Because the following arguments hold:</p><p>1. If (a i , a j , Z 1 , Z 3 ) + (a i , a j , Z 2 , Z 3 ) = 0 then (a i , a j , Z 1 , Z 2 ) = 0, 2. If (a i , a j , Z 1 , Z 3 ) + (a i , a j , Z 2 , Z 3 ) = 2 then (a i , a j , Z 1 , Z 2 ) = 0, 3. If (a i , a j , Z 1 , Z 3 ) + (a i , a j , Z 2 , Z 3 ) = 1 then (a i , a j , Z 1 , Z 2 ) = 1.</p><p>Then also the triangular inequality is true.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6. Proof of Lemma 6</head><p>A projection of matrix is obtained by removing certain features (i.e., removing their corresponding rows and columns). Without the loss of generality, we assume that the removed features are the last t features. Let us assume by contradiction that the projected matrix is not well defined but that the original matrix is well defined. Because the projected matrix is not well-defined then ∃i, j, k nt. This violates one of the constraints specified in Definition 3. However, because the original matrix is well-defined then for ∀i, j, k n or more specifically for ∀i, j, k nt the above constraints hold. We have reached a contradiction and therefore our original assumption according to which the projected matrix is not well defined, is not true.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7. Proof of Lemma 7</head><p>If the GWC operator is used then the new offspring are obtained by diagonally concatenating the projections of the anchor subset from one parent and the remaining features from the second parent. Based on Lemma 6, because the parents were well-defined so are their projections. It remains to show that the elements that are not obtained from the projection do not violate Definition 3.</p><p>We denote by R the original feature index of the anchor subset in the set A. Because the rows and the columns of the anchor subset R are copied as is, then B i,j =B j,i =0 for ∀i ∈ R; j / ∈ R. Therefore constraint 1 in Definition 3 is always true and constraints 2 and 3 are not relevant in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8. Proof of Lemma 8</head><p>We denote by Z 1 and Z 2 the parent solutions and by Z 3 and Z 4 the offspring. Because each element of the offspring is obtained from one of the parent then,</p><formula xml:id="formula_38">(Z 3 , Z 1 ) + (Z 3 , Z 2 ) = (Z 1 , Z 2 ), (Z 4 , Z 1 ) + (Z 4 , Z 2 ) = (Z 1 , Z 2 ).</formula><p>The last equation is true because in Eq. ( <ref type="formula" target="#formula_18">7</ref>), the term (a i , a j , Z 1 , Z 2 ) = 0 if B i,j in both matrices are equal.</p><p>Using the triangular inequality we obtain that (Z 3 , Z 4 ) (Z 3 , Z 1 ) + (Z 4 , Z 1 ), (Z 3 , Z 4 ) (Z 3 , Z 2 ) + (Z 4 , Z 2 ).</p><p>Thus 2 (Z 3 , Z 4 ) (Z 3 , Z 1 )+ (Z 4 , Z 1 )+ (Z 3 , Z 2 )+ (Z 4 , Z 2 ) or (Z 3 , Z 3 ) (Z 1 , Z 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.9. Proof of Theorem 1</head><p>To prove Theorem 1, it is useful to consider Lemmas 9 and 10 first.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 9. The VC dimension of an ODT on n binary input features with l layers and t terminal nodes is not greater than t + log 2 n! (nl)!</head><p>• (2t -4)! (t -2)! • (t -2)! .</p><p>Proof. Any ODT can be converted to a suitable classification tree with leaves labeled {0, 1} according to the highest weight of each of the terminal nodes in the original tree. Because the probabilistic oblivious tree and its corresponding classification tree shatter the same subsets, their VC dimensions are identical. The hypothesis space size of a classification oblivious tree with l layers, t terminal nodes and n input features to choose from is not greater than</p><formula xml:id="formula_39">n! (n -l)! • 2 t • (2t -4)! (t -2)! • (t -2)! .</formula><p>The first multiplier indicates the number of combinations for selecting with order l features from n. The second multiplier corresponds to the different classification options of the terminal nodes. The third multiplier represents the number of different binary tree structures that contain t leaves. The last multiplier is calculated using the Wallace <ref type="bibr" target="#b56">[57]</ref> tree structure. Note that in the case of the binary tree there is exactly one more leaf than inner nodes. Furthermore, the tree string always begins with an inner node (when l 1) and end with at least two leaf nodes. Based on the familiar relation VC(H ) log 2 (|H |) for finite H, the lemma has been proved. Lemma 10. Consider mutually exclusive ODTs that are combined with the naïve Bayes and that have a fixed structure containing T = (t 1 , . . . , t ) terminal nodes. The number of dichotomies it induces on a set of cardinality m is at most:</p><formula xml:id="formula_40">2 em 1 + i=1 t i 1+ i=1 t i .</formula><p>Proof. The proof of this lemma, uses a similar lemma introduced by Schmitt <ref type="bibr" target="#b57">[58]</ref>: the number of dichotomies that a higher order threshold neuron with k monomials induces on a set of cardinality m is at most</p><formula xml:id="formula_41">2 k i=0 m -1 i &lt; 2 em k k for m &gt; k 1.</formula><p>A definition of a higher order threshold neuron has the form</p><formula xml:id="formula_42">w 1 M 1 + w 2 M 2 + • • • + w k M k -t r ,</formula><p>where M 1 , M 2 , . . . , M k are monomials. ODTs which are combined with naïve Bayes can be converted to a higher order threshold neuron, where the set of terminal nodes constitutes the neuron's monomials and the log-odds in favor of y = 1 in each terminal node is the corresponding neuron's weight. Furthermore, in order to use the sign activation function, the threshold has been set to the sum of all other monomials. Now it is possible to prove Theorem 1. The proof of the upper bound is discussed first. If = 1, then Lemma 9 can be used directly. For the case &gt; 1, the bound of the number of dichotomies induced by mutually exclusive ODTs on an arbitrary set of cardinality m is first introduced. Because the biggest shattered set follows this bound as well, the statement of the theorem is derived.</p><p>There are at most</p><formula xml:id="formula_43">n! ! • (n -i=1 l i )! • i=1 (2t i -4)! (t i -2)! • (t i -2)!</formula><p>different structures for mutually exclusive oblivious trees on n binary input features with L = (l 1 , . . . , l ) layers and T = (t 1 , . . . , t ) terminal nodes. Notice that the division by ! is required as there is no relevance to the order of the trees.</p><p>According to Lemma 10, a fixed structure and variable weights can induce at most However, the last inequality will not be true if m 2 • (F + 1) log(2e) + 2 log U where F = i=1 t i and</p><formula xml:id="formula_44">U = n! ! • (n -i=1 l i )! • i=1 (2t i -4)! (t i -2)! • (t i -2)! .</formula><p>The lower bound is true due to the fact that any set of trees with a fixed structure has the above VC dimension. The result can be achieved by setting in each tree (besides one) a neutralized terminal node (i.e., a terminal node with posteriori probabilities that are equal to the a priori probabilities). This concludes the proof.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Venn diagram for the search space of the feature-oriented tasks.</figDesc><graphic coords="3,308.07,71.41,237.60,165.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. A pseudocode for GA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Oblivious decision tree for quality assurance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Overall diagram of the GA-based proposed method.</figDesc><graphic coords="9,126.07,71.25,332.73,249.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>) with probability of 1 -</head><label>1</label><figDesc>where ˆ (h, S) represents the training error of classifier h measured on training set S of cardinality m, and (h, D) represents the generalization error of the classifier h over the distribution D. Note that in this case</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The complexity-accuracy tradeoff for the audiology dataset.</figDesc><graphic coords="17,39.08,71.25,237.60,132.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>arg max c j ∈dom(y) k=1 P (y = c j |G k ) P (y = c j ) -1 = arg max c j ∈dom(y)k=1 P (y = c j |f k ) P (y = c j ) -1 = arg max c j ∈dom(y) k=1 {XNOR(f k , bit(k, c j ))•P (f j =bit(j, c j ) ∀j =k)} P (y=c j ) -1 .As the input features are independent= arg max c j ∈dom(y) k=1 {XNOR(f k ,bit(k, c j ))• j =k P(f j =bit(j, c j ))} P (y=c j ) -1 = arg max c j ∈dom(y) k=1 P (f j =bit(j, c j )) -1 • k=1 XNOR(f k , bit(k, c j )) P (y=c j ) -1 = arg max c j ∈dom(y) P (y = c j ) -1 • k=1 XNOR(f k , bit(k, c j )) P (y = c j ) -1= arg max c j ∈dom(y) k=1 XNOR(f k , bit(k, c j )) = arg max c j ∈dom(y) P (y = c j |x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell cols="3">Illustration of adjacency matrix like encoding</cell><cell></cell><cell></cell><cell></cell></row><row><cell>a 1</cell><cell>a 2</cell><cell>a 3</cell><cell>a 4</cell><cell>a 5</cell><cell>a 6</cell></row><row><cell>a 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="2">Illustration of GWC operator</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>a 1</cell><cell>a 2</cell><cell>a 3</cell><cell>a 4</cell><cell>a 5</cell><cell>a 6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>a 5 , a 6 }; {a 2 , a 3 , a 8 , a 10 }; {a 7 , a 9 }}, Z 2 = {{a 1 , a 5 , a 6 , a 8 }; {a 2 , a 3 , a 4 , a 10 }; {a 7 , a 9 }}, Z 3 = {{a 1 , a 3 , a 4 , a 5 , a 6 }; {a 2 , a 8 , a 10 }; {a 7 , a 9 }}.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell>GOV</cell><cell>Accuracy # Nodes # Subsets Average</cell><cell>subset size</cell><cell>85.35 ± 4.6 56 3 3.33</cell><cell>81.5 ± 4.29 124 7 2.12</cell><cell>97.13 ± 1.6 76 5 1.12</cell><cell>81.29 ± 5.46 7 1 3</cell><cell>96 ± 3.33 11 1 4</cell><cell>99.44 ± 0.35 140 3 7.5</cell><cell>95.17 ± 3.5 20 4 2</cell><cell>72.36 ± 3.7 47 4 3.33</cell><cell>75.02 ± 1.7 313 10 1.67</cell><cell>53.55 ± 10.05 27 5 2</cell><cell>98.51 ± 1.3 12 3 2</cell><cell>61.56 ± 7.6 24 1 5</cell><cell>93.44 ± 5.34 6 4 1</cell><cell>100 ± 0 3 7 1 5</cell><cell>96.82 ± 1.16 339 2 4</cell><cell>91.84 ± 1.1 981 60 1</cell><cell>76.42 ± 3.23 125 5 2.2</cell><cell>94.95 ± 0.4 134 2 5</cell><cell>96.3 ± 0.7 420 15 3</cell><cell>80.24 ± 2.7 95 2 4.5</cell><cell>93.79 ± 2.8 23 1 7</cell><cell>95.92 ± 4.41 65 5 1.8</cell><cell>97.21 ± 3.42 18 3 2.5</cell><cell>87.06 134.78 6.39 3.22</cell><cell>77 ± 7.2 119 8 7.2</cell><cell>90.28 ± 1.9 789 16 52.41</cell><cell>71.2 ± 2.9 990 3 97.81</cell><cell>79.49 632.67 9.00 52.51</cell></row><row><cell>Comparing single-classifier algorithms: summary of experimental results</cell><cell>Dataset # Instances # Features Naïve Bayes C4.5 IFN DOG</cell><cell>Accuracy Accuracy # Nodes Accuracy # Nodes Accuracy # Nodes # Subsets Average</cell><cell>subset size</cell><cell>Aust 690 15 84.93 ± 2.7 85.36 ± 5.1 30 84.49 ± 5.1 27 86.52 ± 2.5 84 11 1.27</cell><cell>Audiology 200 70 + 65.5 ± 7.39 + 75 ± 6.95 52 + 74 ± 7.95 100 + 78.5 ± 6.54 64 3 4.67</cell><cell>Bcan 699 10 97.4249 ± 1.17 + 92.99 ± 2.87 61 + 94.39 ± 3.5 55 97.42 ± 1.17 99 9 1</cell><cell>Hepatitis 155 20 82.58 ± 7.56 81.29 ± 5.46 7 78.97 ± 8.99 68 80 ± 6.89 38 2 2</cell><cell>Iris 150 5 95.33 ± 5.05 96 ± 3.33 11 96 ± 3.33 90 95.33 ± 5.05 40 4 1</cell><cell>Kr-vs-kp 3197 37 + 87.86 ± 1.41 99.44 ± 0.55 87 98.06 ± 0.42 220 98.47 ± 0.63 330 2 7</cell><cell>Labor 57 17 92.98 ± 4.52 + 73.72 ± 12.72 12 + 84.63 ± 8.14 32 96.49 ± 5.5 67 16 1</cell><cell>LED17 220 25 + 63.18 ± 8.7 + 59.09 ± 6.9 69 + 55.55 ± 6.3 73 73.64 ± 5.5 370 7 3.28</cell><cell>LETTER 15 000 17 73.29 ± 1 74.96 ± 0.8 11169 + 69.56 ± 0.7 5321 73.46 ± 0.64 272 16 1</cell><cell>Lung 31 56 + 41.94 ± 19.96 + 38.71 ± 17.82 16 + 38.71 ± 17.82 16 53.55 ± 10.05 27 4 2</cell><cell>Monks1 124 6 + 73.39 ± 6.7 + 75.81 ± 8.2 18 + 75.00 ± 10.7 40 98.39 ± 2.3 28 5 1.2</cell><cell>Monks2 169 6 + 56.21 ± 6.1 61.54 ± 8.6 31 62.72 ± 10.4 194 60.36 ± 7.55 30 4 1.5</cell><cell>Monks3 122 6 93.44 ± 3.7 93.44 ± 3.7 12 92.38 ± 3.3 12 93.442 ± 3.3 19 5 1.2</cell><cell>MUSH 8124 22 + 95.48 ± 0.9 100 ± 0 28 100 ± 0 30 100 ± 0 28 1.2 7.67</cell><cell>Nurse 12 960 8 + 65.39 ± 24 97.45 ± 0.4 527 92.47 ± 0.5 135 + 91.65 ± 0.6 38 6 1.33</cell><cell>OPTIC 5628 64 91.73 ± 1.3 + 62.42 ± 2 4059 + 48.90 ± 2.5 1257 91.73 ± 1.4 981 64 1</cell><cell>Sonar 208 60 75.48 ± 7.3 + 69.71 ± 5.4 51 76.48 ± 6.8 97 77.12 ± 8.7 98 35 1.657</cell><cell>Soybean 683 35 + 91.95 ± 1.99 + 92.83 ± 1.52 85 92.24 ± 2.46 72 92.9 ± 2.56 122 3 4</cell><cell>Splice 1000 60 + 94.1 ± 0.4 + 91.2 ± 1.9 117 + 87.00 ± 2.6 523 95.8 ± 0.9 300 50 1.2</cell><cell>TTT 958 9 + 69.27 ± 3.2 -85.7 ± 1.65 142 73.19 ± 3.9 540 + 73.33 ± 4 51 6 2.5</cell><cell>Vote 290 16 + 90.34 ± 3.44 -96.21 ± 2.45 16 93.79 ± 2.8 23 + 90.52 ± 1.23 18 6 1.333</cell><cell>Wine 178 13 96.63 ± 3.9 + 85.96 ± 6.9 41 + 91.45 ± 5 41 96.63 ± 3.9 143 13 1</cell><cell>Zoo 101 8 + 89.11 ± 7 + 93.07 ± 5.8 21 + 90.89 ± 9.7 21 98.02 ± 3.02 50 4 4</cell><cell>UCI Av. 2214.96 25.43 81.20 81.82 724.43 80.47 390.74 86.13 143.35 12.01 2.34</cell><cell>Arcene 100 10 000 + 70 ± 12.3 75 ± 9.2 9 + 54 ± 8.3 46 76 ± 8.1 97 12 3.2</cell><cell>Dexter 300 20 000 + 86.33 ± 3.9 + 78.33 ± 3.6 53 + 76.13 ± 2.1 47 89.33 ± 2.7 562 11 52.72</cell><cell>Madelon 2000 500 + 58.3 ± 1.5 69.8 ± 4.7 259 + 62 ± 3.4 127 71.4 ± 2.6 660 2 117.8</cell><cell>NIPS Av. 800 10 166.67 71.54 74.38 107.00 64.04 73.33 78.91 439.67 8.33 57.91</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell>AB GOV</cell><cell># Nodes Ensemble size Accuracy # Nodes Ensemble size Accuracy # Nodes # Subsets Average subset size</cell><cell>± 3.6 30 1 86.81 ± 2.36 9 2 85.35 ± 4.6 56 3 3.33</cell><cell>± 4.25 471.2 8 + 76 ± 6.9 525 10 81.5 ± 4.29 124 7 2.12</cell><cell>± 2.7 1793 19 + 93.4 ± 2.8 117 3 97.13 ± 1.6 76 5 1.12</cell><cell>± 5.46 7 1 81.3 ± 5.8 7 1 81.29 ± 5.46 7 1 3</cell><cell>± 3.33 11 1 95.3 ± 5.9 92 11 96 ± 3.33 11 1 4</cell><cell>± 0.59 421 5 99.4 ± 0.4 592 23 99.44 ± 0.35 140 3 7.5</cell><cell>± 0 59 5 + 89.7 ± 12.7 67 9 95.17 ± 3.5 20 4 2</cell><cell>± 4.2 365.8 5 + 60.4 ± 3.7 716 10 72.36 ± 3.7 47 4 3.33</cell><cell>± 2.3 24031 20 -92.13 ± 1.7 1923 19 75.02 ± 1.7 313 10 1.67</cell><cell>± 12.8 32.8 3 + 46.9 ± 14.1 142 10 53.55 ± 10.0 27 5 2</cell><cell>± 7.4 307.1 18 + 92.74 ± 15 3 98.51 ± 1.3 12 3 2</cell><cell>± 6.4 371.5 13 62.13 ± 2 8 4 61.56 ± 7.6 24 1 5</cell><cell>± 2.3 297.1 14 93.4 ± 5.3 24 2 93.44 ± 5.34 6 4 1</cell><cell>± 0 30 1 100 ± 0 328 10 100 ± 0 37 1 5</cell><cell>± 1.5 6069 19 -97.4 ± 0.31 55 9 96.82 ± 1.16 339 2 4</cell><cell>± 2.1 73838 20 + 60.53 ± 1.2 40702 11 91.84 ± 1.1 981 60 1</cell><cell>± 6.7 994 16 71.1 ± 8.1 107 2 76.42 ± 3.23 125 5 2.2</cell><cell>± 2.51 1271 15 91.8 ± 1.7 967 10 94.95 ± 0.4 134 2 5</cell><cell>± 4.6 2331 19 + 94.5 ± 1.7 1170 19 96.3 ± 0.7 420 15 3</cell><cell>± 3.9 1906 15 -88 ± 1.67 1721 12 80.24 ± 2.7 95 2 4.5</cell><cell>± 2.3 16 1 -95.86 ± 2.5 76 10 93.79 ± 2.8 23 1 7</cell><cell>± 6.1 513 11 90.44 ± 3.2 391 14 95.92 ± 4.41 65 5 1.8</cell><cell>± 0 110 7 + 92.0 ± 4.52 127 8 97.21 ± 3.42 18 3 2.5</cell><cell>14415 10.30 84.83 2168 9.2 87.06 134.78 6.39 3.22</cell><cell>± 5.2 467 10 75 ± 9.08 149 11 77 ± 7.2 119 8 7.2</cell><cell>± 3.1 391 7 87 ± 2.4 1720 25 90.28 ± 1.9 789 16 52.41</cell><cell>± 4.14 3693 14 70.5 ± 3.9 3090 15 71.2 ± 2.9 990 3 97.81</cell><cell>1517 10.33 77.5 1653 17 79.49 632.67 9.00 52.51</cell></row><row><cell>Comparing ensemble algorithms: summary of experimental results</cell><cell>Dataset GEFS Adaboost</cell><cell>Accuracy # Nodes Ensemble size Accuracy</cell><cell>Aust 86.96 ± 2.1 517.2 10 85.36</cell><cell>Audiology 81.1 ± 7.29 562.7 12 83.5</cell><cell>Bcan + 94.66 ± 2.17 822 14 96.71</cell><cell>Hepatitis 83.92 ± 5.41 91.4 6 81.29</cell><cell>Iris 97.11 ± 2.27 77.1 8 96</cell><cell>Kr-vs-kp + 98.31 ± 0.79 567.2 13 99.69</cell><cell>Labor + 91.22 ± 10.12 67.2 8 -100</cell><cell>LED17 66.73 ± 5.2 611.5 11 65.91</cell><cell>LETTER -81.69 ± 1.4 1065.2 15 -87.72</cell><cell>Lung + 48.22 ± 10.82 99.9 10 -57.5</cell><cell>Monks1 + 81.36 ± 8.2 51.6 2 97.56</cell><cell>Monks2 61.22 ± 9.1 474.8 14 62.76</cell><cell>Monks3 + 89.1 ± 2.6 44.7 3 93.73</cell><cell>MUSH 100 ± 0 90.4 3 100</cell><cell>Nurse 96.64 ± 1.2 5495.9 12 -98.2</cell><cell>OPTIC + 78.22 ± 1.5 45111 5 + 87.24</cell><cell>Sonar 74.95 ± 1.6 502 3 -79.24</cell><cell>Soybean 94.44 ± 2.51 1257.6 13 93.47</cell><cell>Splice + 92.1 ± 2.1 1042.6 9 + 93.7</cell><cell>TTT -94.58 ± 0.59 1959.2 15 -97.29</cell><cell>Vote -96.55 ± 3.21 156.2 12 -96.21</cell><cell>Wine -89.87 ± 4.1 256 5 95.56</cell><cell>Zoo 94.09 ± 2.4 141.6 9 -100</cell><cell>UCI Av. 85.78 2655.00 9.22 89.07</cell><cell>Arcene 76 ± 8.4 161 16 78</cell><cell>Dexter + 80.12 ± 1.9 478 9 + 81.13</cell><cell>Madelon 70.9 ± 5.1 2725 10 + 67.77</cell><cell>NIPS Av. 75.67 1121.33 11.67 75.63</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="2">Comparing the execution time</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>Naïve Bayes</cell><cell>C4.5</cell><cell>IFN</cell><cell>DOG</cell><cell>GOV</cell><cell>GEFS</cell><cell>AdaBoost</cell></row><row><cell>Aust</cell><cell>0.01</cell><cell>0.02</cell><cell>0.032</cell><cell>0.234</cell><cell>0.16</cell><cell>23.52</cell><cell>0.282</cell></row><row><cell>Audiology</cell><cell>0.016</cell><cell>0.06</cell><cell>0.063</cell><cell>0.25</cell><cell>0.93</cell><cell>111.31</cell><cell>0.375</cell></row><row><cell>Bcan</cell><cell>0.01</cell><cell>0.02</cell><cell>0.01</cell><cell>0.047</cell><cell>0.16</cell><cell>5.53</cell><cell>0.078</cell></row><row><cell>Hepatitis</cell><cell>0.01</cell><cell>0.01</cell><cell>0.016</cell><cell>0.031</cell><cell>0.16</cell><cell>5.67</cell><cell>0.02</cell></row><row><cell>Iris</cell><cell>0.016</cell><cell>0.02</cell><cell>0.015</cell><cell>0.016</cell><cell>0.15</cell><cell>0.58</cell><cell>0.047</cell></row><row><cell>Kr-vs-kp</cell><cell>0.016</cell><cell>0.125</cell><cell>0.125</cell><cell>1.797</cell><cell>3.28</cell><cell>407.25</cell><cell>3.61</cell></row><row><cell>Labor</cell><cell>0.01</cell><cell>0.02</cell><cell>0.01</cell><cell>0.016</cell><cell>0.1</cell><cell>6.39</cell><cell>0.04</cell></row><row><cell>LED17</cell><cell>0.01</cell><cell>0.03</cell><cell>0.015</cell><cell>0.125</cell><cell>0.16</cell><cell>39.42</cell><cell>0.156</cell></row><row><cell>LETTER</cell><cell>0.032</cell><cell>1.19</cell><cell>1.469</cell><cell>9.766</cell><cell>17.5</cell><cell>1351.23</cell><cell>14.734</cell></row><row><cell>Lung</cell><cell>0.01</cell><cell>0.01</cell><cell>0.01</cell><cell>0.031</cell><cell>0.16</cell><cell>7.56</cell><cell>0.016</cell></row><row><cell>Monks1</cell><cell>0.01</cell><cell>0.01</cell><cell>0.01</cell><cell>0.016</cell><cell>0.1</cell><cell>0.703</cell><cell>0.04</cell></row><row><cell>Monks2</cell><cell>0.01</cell><cell>0.01</cell><cell>0.01</cell><cell>0.015</cell><cell>0.15</cell><cell>0.547</cell><cell>0.031</cell></row><row><cell>Monks3</cell><cell>0.01</cell><cell>0.01</cell><cell>0.01</cell><cell>0.023</cell><cell>0.1</cell><cell>0.797</cell><cell>0.016</cell></row><row><cell>MUSH</cell><cell>0.031</cell><cell>0.13</cell><cell>0.125</cell><cell>0.625</cell><cell>4.69</cell><cell>247.031</cell><cell>0.094</cell></row><row><cell>Nurse</cell><cell>0.016</cell><cell>0.41</cell><cell>0.14</cell><cell>1.781</cell><cell>1.56</cell><cell>152.27</cell><cell>2.672</cell></row><row><cell>OPTIC</cell><cell>0.047</cell><cell>0.765</cell><cell>1.156</cell><cell>5.828</cell><cell>26.41</cell><cell>1247.562</cell><cell>7.516</cell></row><row><cell>Sonar</cell><cell>0.01</cell><cell>0.11</cell><cell>0.016</cell><cell>0.109</cell><cell>0.94</cell><cell>70.13</cell><cell>0.125</cell></row><row><cell>Soybean</cell><cell>0.01</cell><cell>0.13</cell><cell>0.047</cell><cell>0.484</cell><cell>1.25</cell><cell>176.09</cell><cell>0.672</cell></row><row><cell>SPI</cell><cell>0.015</cell><cell>0.047</cell><cell>0.079</cell><cell>0.532</cell><cell>2.81</cell><cell>2638.86</cell><cell>0.657</cell></row><row><cell>TTT</cell><cell>0.016</cell><cell>0.03</cell><cell>0.015</cell><cell>0.156</cell><cell>0.16</cell><cell>17.313</cell><cell>0.172</cell></row><row><cell>Vote</cell><cell>0.01</cell><cell>0.01</cell><cell>0.016</cell><cell>0.219</cell><cell>0.16</cell><cell>2.437</cell><cell>0.047</cell></row><row><cell>Wine</cell><cell>0.01</cell><cell>0.02</cell><cell>0.01</cell><cell>0.031</cell><cell>0.15</cell><cell>4.984</cell><cell>0.016</cell></row><row><cell>Zoo</cell><cell>0.01</cell><cell>0.05</cell><cell>0.015</cell><cell>0.015</cell><cell>0.1</cell><cell>3.766</cell><cell>0.015</cell></row><row><cell>UCI Av.</cell><cell>0.015</cell><cell>0.140739</cell><cell>0.148435</cell><cell>0.962913</cell><cell>2.666957</cell><cell>283.5195</cell><cell>1.366391</cell></row><row><cell>Arcene</cell><cell>0.656</cell><cell>4.453</cell><cell>2.343</cell><cell>37.469</cell><cell>7366.8</cell><cell>23 207</cell><cell>53.641</cell></row><row><cell>Dexter</cell><cell>1.469</cell><cell>11.953</cell><cell>5.125</cell><cell>116.328</cell><cell>9233.75</cell><cell>54 336</cell><cell>141.172</cell></row><row><cell>Madelon</cell><cell>0.812</cell><cell>10.281</cell><cell>4.671</cell><cell>165.859</cell><cell>253.205</cell><cell>56 718</cell><cell>249.531</cell></row><row><cell>NIPS Av.</cell><cell>0.979</cell><cell>8.895667</cell><cell>4.046333</cell><cell>106.552</cell><cell>5617.918</cell><cell>44 753.67</cell><cell>148.1147</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 A</head><label>6</label><figDesc>comparison of five variants of the proposed algorithm</figDesc><table><row><cell></cell><cell>Fitness function</cell><cell>Wrapper</cell><cell>VC upper bound</cell><cell>Wrapper</cell><cell>VC lower bound</cell><cell>VC upper bound</cell></row><row><cell></cell><cell>Encoding</cell><cell>Simple</cell><cell>Simple</cell><cell>New</cell><cell>New</cell><cell>New</cell></row><row><cell>Dataset</cell><cell>Aust</cell><cell>82.36 ± 3.79</cell><cell>83.2 ± 4.63</cell><cell>86.52 ± 2.60</cell><cell>83.83 ± 4.20</cell><cell>85.35 ± 4.6</cell></row><row><cell></cell><cell>Audiology</cell><cell>78.95 ± 2</cell><cell>78.1 ± 3.2</cell><cell>81.68 ± 3.89</cell><cell>77.25 ± 4.09</cell><cell>81.5 ± 4.29</cell></row><row><cell></cell><cell>Bcan</cell><cell>96.24 ± 1.16</cell><cell>96.6 ± 1.37</cell><cell>96.82 ± 1.18</cell><cell>96.67 ± 1.00</cell><cell>97.13 ± 1.6</cell></row><row><cell></cell><cell>Hepatitis</cell><cell>79.1 ± 2.1</cell><cell>79.9 ± 2.12</cell><cell>83.67 ± 5.41</cell><cell>76.15 ± 5.09</cell><cell>81.29 ± 5.46</cell></row><row><cell></cell><cell>Iris</cell><cell>92.86 ± 3.66</cell><cell>94.09 ± 3.04</cell><cell>94.87 ± 3.34</cell><cell>94.47 ± 3.38</cell><cell>96 ± 3.33</cell></row><row><cell></cell><cell>Kr-vs-kp</cell><cell>98.37 ± 0.68</cell><cell>98.45 ± 0.49</cell><cell>99.35 ± 0.31</cell><cell>99.27 ± 0.63</cell><cell>99.44 ± 0.35</cell></row><row><cell></cell><cell>Labor</cell><cell>94.13 ± 3.67</cell><cell>94.2 ± 3.61</cell><cell>95.64 ± 3.66</cell><cell>94.54 ± 3.66</cell><cell>95.17 ± 3.5</cell></row><row><cell></cell><cell>LED17</cell><cell>70.68 ± 3.49</cell><cell>69.35 ± 4</cell><cell>73.67 ± 3.20</cell><cell>70.97 ± 3.69</cell><cell>72.36 ± 3.7</cell></row><row><cell></cell><cell>LETTER</cell><cell>73.87 ± 1.41</cell><cell>73.96 ± 0.73</cell><cell>77.34 ± 1.11</cell><cell>74.72 ± 1.01</cell><cell>75.02 ± 1.7</cell></row><row><cell></cell><cell>Lung</cell><cell>47.65 ± 6.4</cell><cell>49.04 ± 9.17</cell><cell>50.03 ± 10.02</cell><cell>45.74 ± 10.32</cell><cell>53.55 ± 10.05</cell></row><row><cell></cell><cell>Monks1</cell><cell>97.5 ± 1.9</cell><cell>98.33 ± 1.12</cell><cell>98.67 ± 0.76</cell><cell>98.41 ± 1.55</cell><cell>98.51 ± 1.3</cell></row><row><cell></cell><cell>Monks2</cell><cell>58.4 ± 7.23</cell><cell>57.37 ± 7.31</cell><cell>63.48 ± 7.42</cell><cell>58.91 ± 7.62</cell><cell>61.56 ± 7.6</cell></row><row><cell></cell><cell>Monks3</cell><cell>92 ± 5.47</cell><cell>92.77 ± 5.65</cell><cell>94.44 ± 5.59</cell><cell>92.96 ± 5.19</cell><cell>93.44 ± 5.34</cell></row><row><cell></cell><cell>MUSH</cell><cell>99.17 ± 1.2</cell><cell>99.17 ± 1.2</cell><cell>100.00 ± 0</cell><cell>100.00 ± 0</cell><cell>100 ± 0</cell></row><row><cell></cell><cell>Nurse</cell><cell>93.01 ± 1.12</cell><cell>92.59 ± 1.01</cell><cell>96.85 ± 1.15</cell><cell>93.35 ± 0.95</cell><cell>96.82 ± 1.16</cell></row><row><cell></cell><cell>OPTIC</cell><cell>92.89 ± 0.84</cell><cell>92.71 ± 1.2</cell><cell>91.91 ± 0.88</cell><cell>91.11 ± 0.79</cell><cell>91.84 ± 1.1</cell></row><row><cell></cell><cell>Sonar</cell><cell>74.84 ± 2.61</cell><cell>74.8 ± 2.57</cell><cell>75.60 ± 2.68</cell><cell>75.20 ± 2.71</cell><cell>76.42 ± 3.23</cell></row><row><cell></cell><cell>Soybean</cell><cell>94.48 ± 0.14</cell><cell>94.32 ± 0.15</cell><cell>95.04 ± 0.53</cell><cell>94.57 ± 0.27</cell><cell>94.95 ± 0.4</cell></row><row><cell></cell><cell>SPI</cell><cell>95.3 ± 0.12</cell><cell>95.7 ± 0.96</cell><cell>96.31 ± 0.1</cell><cell>96.00 ± 0.53</cell><cell>96.3 ± 0.7</cell></row><row><cell></cell><cell>TTT</cell><cell>78.04 ± 2.39</cell><cell>77.82 ± 2.17</cell><cell>80.04 ± 2.30</cell><cell>78.76 ± 2.28</cell><cell>80.24 ± 2.7</cell></row><row><cell></cell><cell>Vote</cell><cell>90.77 ± 2.94</cell><cell>89.63 ± 2.69</cell><cell>94.16 ± 2.56</cell><cell>90.99 ± 2.92</cell><cell>93.79 ± 2.8</cell></row><row><cell></cell><cell>Wine</cell><cell>93.05 ± 4.28</cell><cell>93.02 ± 4.13</cell><cell>94.27 ± 3.90</cell><cell>93.98 ± 4.25</cell><cell>95.92 ± 4.41</cell></row><row><cell></cell><cell>Zoo</cell><cell>96.22 ± 2.42</cell><cell>96.22 ± 2.45</cell><cell>98.69 ± 2.72</cell><cell>97.02 ± 2.79</cell><cell>97.21 ± 3.42</cell></row><row><cell></cell><cell>Arcene</cell><cell>67.69 ± 6.36</cell><cell>71.21 ± 6.33</cell><cell>73.43 ± 6.60</cell><cell>71.87 ± 6.73</cell><cell>77 ± 7.2</cell></row><row><cell></cell><cell>Dexter</cell><cell>89.73 ± 2.02</cell><cell>89.31 ± 1.76</cell><cell>91.09 ± 1.51</cell><cell>90.23 ± 2.20</cell><cell>90.28 ± 1.9</cell></row><row><cell></cell><cell>Madelon</cell><cell>68.89 ± 5.12</cell><cell>68 ± 4.34</cell><cell>71.87 ± 2.38</cell><cell>67.59 ± 3.14</cell><cell>71.2 ± 2.9</cell></row><row><cell>Average rank</cell><cell></cell><cell>4.2</cell><cell>4.22</cell><cell>1.42</cell><cell>3.37</cell><cell>1.77</cell></row></table><note><p>Each variant is defined based on a different fitness function (wrapper, upper VC, lower VC) and on a different encoding type (simple, new).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell cols="3">Comparing ODT and regular DT in feature set partitioning</cell></row><row><cell>Dataset</cell><cell>ODT (with IFN)</cell><cell>Regular DT (with C4.5)</cell></row><row><cell>Aust</cell><cell>86.52 ± 2.60</cell><cell>86.72 ± 3.36</cell></row><row><cell>Audiology</cell><cell>81.68 ± 3.89</cell><cell>81.38 ± 5.3</cell></row><row><cell>Bcan</cell><cell>96.82 ± 1.18</cell><cell>96.48 ± 1.62</cell></row><row><cell>Hepatitis</cell><cell>83.67 ± 5.41</cell><cell>83.64 ± 6.4</cell></row><row><cell>Iris</cell><cell>94.87 ± 3.34</cell><cell>95.2 ± 4.27</cell></row><row><cell>Kr-vs-kp</cell><cell>99.35 ± 0.31</cell><cell>99.41 ± 0.39</cell></row><row><cell>Labor</cell><cell>95.64 ± 3.66</cell><cell>95.95 ± 4.99</cell></row><row><cell>LED17</cell><cell>73.67 ± 3.20</cell><cell>73.77 ± 3.8</cell></row><row><cell>LETTER</cell><cell>77.34 ± 1.11</cell><cell>77.45 ± 1.3</cell></row><row><cell>Lung</cell><cell>50.03 ± 10.02</cell><cell>50.31 ± 13.23</cell></row><row><cell>Monks1</cell><cell>98.67 ± 0.76</cell><cell>98.29 ± 0.91</cell></row><row><cell>Monks2</cell><cell>63.48 ± 7.42</cell><cell>63.74 ± 10.02</cell></row><row><cell>Monks3</cell><cell>94.44 ± 5.59</cell><cell>94.59 ± 6.42</cell></row><row><cell>MUSH</cell><cell>100.00 ± 0</cell><cell>100.00 ± 0</cell></row><row><cell>Nurse</cell><cell>96.85 ± 1.15</cell><cell>97.49 ± 1.52</cell></row><row><cell>OPTIC</cell><cell>91.91 ± 0.88</cell><cell>92.93 ± 1.18</cell></row><row><cell>Sonar</cell><cell>75.60 ± 2.68</cell><cell>76.07 ± 3.25</cell></row><row><cell>Soybean</cell><cell>95.04 ± 0.53</cell><cell>95.47 ± 0.72</cell></row><row><cell>SPI</cell><cell>96.31 ± 0.16</cell><cell>+ 94.73 ± 0.29</cell></row><row><cell>TTT</cell><cell>80.04 ± 2.30</cell><cell>-86.2 ± 1.63</cell></row><row><cell>Vote</cell><cell>94.16 ± 2.56</cell><cell>-96.51 ± 1.2</cell></row><row><cell>Wine</cell><cell>94.27 ± 3.90</cell><cell>96.9 ± 2.6</cell></row><row><cell>Zoo</cell><cell>98.69 ± 2.72</cell><cell>+ 95.14 ± 1.9</cell></row><row><cell>Arcene</cell><cell>73.43 ± 6.60</cell><cell>73.2 ± 8.35</cell></row><row><cell>Dexter</cell><cell>91.09 ± 1.51</cell><cell>93.01 ± 2.04</cell></row><row><cell>Madelon</cell><cell>71.87 ± 2.38</cell><cell>72.32 ± 2.71</cell></row><row><cell>Average rank</cell><cell>1.67</cell><cell>1.33</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>dichotomies on a given set of cardinality m that are induced by the class considered. If the above class shatters the given</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>set, then</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2 m</cell><cell>n! ! • (n -i=1 l i )!</cell><cell>•</cell><cell>i=1</cell><cell>(2t i -4)! (t i -2)! • (t i -2)!</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>• 2</cell><cell>em 1 + i=1 t i</cell><cell>1+ i=1 t i</cell><cell>.</cell></row><row><cell>2</cell><cell cols="2">1 + i=1 t i em</cell><cell cols="3">1+ i=1 t i</cell></row><row><cell cols="6">dichotomies on a given set of cardinality m. Enumerating over</cell></row><row><cell cols="6">all structures, it is concluded that there are at most</cell></row><row><cell></cell><cell cols="3">n! ! • (n -i=1 l i )!</cell><cell>•</cell><cell>i=1</cell><cell>(2t i -4)! (t i -2)! • (t i -2)!</cell></row><row><cell></cell><cell>• 2</cell><cell cols="2">1 + i=1 t i em</cell><cell></cell><cell>i=1 t i</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The author gratefully thank the action editor and the anonymous reviewers whose constructive comments helped in improving the quality and accuracy of this paper.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>slightly lower degree of accuracy, but which have much fewer nodes. Users generally regard smaller decision trees as more comprehensible. Though the choice of the best model (either the most accurate or the simplest) depends on a specific application, we believe that, in many cases, a small degree of accuracy can be sacrificed for the sake of obtaining a much more compact and interpretable model, such as the one produced by GOV. There are, however, certain cases in which the differences in the degree of accuracy are not negligible. For instance, AdaBoost obtained an accuracy of 87.72% for the LETTER dataset (compared to an accuracy of 75.02% obtained by the GOV algorithm). Nevertheless, the average complexity of the AdaBoost classifier in this case was 240 319 nodes (compared to only 313 nodes of GOV in this case). 2. The mutually exclusive property of GOV makes the classifiers more interpretable. Consider a classifier which is designated to improve the quality of a certain manufacturing line. In this case the target feature stands for the quality of a certain product (high/low) and the input features represent the values of various manufacturing parameters (such as speed, temperature, etc.). In a mutually exclusive forest, the user can easily find the best parameter values by selecting the path in every decision tree that most favors the "high" label (i.e., with the highest probability). If the mutually exclusive property is not retained (such as in the case of GEFS or AdaBoost), finding the best parameter values becomes a complicated task since paths from differ- ent trees might incorporate the same features but not necessarily the same values. The user is compelled to resolve these conflicts, if she wants to best tune the manufacturing process. 3. In GOV, the decision trees are based on the original distribution of the training set. The class distribution at the tree's leafs is supported by the training set. In AdaBoost (starting from the second decision tree) and in GEFS, the class distribution at the leaf level does not necessarily fit the original distribution. This makes it difficult to justify the results to a non-professional user. 4. The new algorithm is faster than existing GA-based ensemble methods for the following two reasons:</p><p>(a) The fitness function uses a VC dimension bound, which is faster than the wrapper estimation. (b) A new caching mechanism reduces the need to build ODT from scratch.</p><p>5. The new encoding schema is more efficient than straightforward encoding, because it provides better results for the same population size and number of generations. 6. The use of ODT as the base classifier provides reasonable results. 7. In artificial cases, we have shown that the GOV algorithm usually almost converges to the optimal partitioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Proofs</head><p>A.1. Proof of Lemma 1</p><p>According to Bayes' theorem:</p><p>Using the independence assumption: = P ( NR x q |y = c j ) • k=1 P ( G k x q |y = c j )P (y = c j ) P (x q ) .</p><p>Using Bayes' theorem again, the last term becomes</p><p>Due to the fact that the NR set and the target feature are independent</p><p>As the value of the expression</p><p>is constant given specific values of the input features arg max c j ∈dom(y)</p><p>P (y = c j |x q ) = arg max c j ∈dom(y)</p><p>i.e., Z is classification-preservation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Proof of Lemma 2</head><p>It is obvious that all input features which do not belong to any of the sets G 1 , . . . , G can be ignored. The proof begins by showing that if y fulfills y =f 1 (G 1 )∨• • •∨f (G ) and that the values of the functions are independent, then the partitioning Z = {G 1 , . . . , G } is classification-preservation.</p><p>For the sake of simplicity we will denote f k ( G k x) as f k . Case 1: At least one of the functions of the instance to be classified gets the value 1. Because such a function also fulfillsP (y = 0|f k = 1) = 0:  In this case P (y = 0) = P (f 1 = 0 ∩ • • • ∩ f = 0). Due to the fact that the input features are independent: P (y = 0) = i=1 P (f i = 0). Furthermore P (y = 0|f i = 0) = k =i P (f k = 0).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Supervised classification in highdimensional space: geometrical, statistical, and asymptotical properties of multivariate data</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Landgrebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern. Part C Appl. Rev</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="39" to="54" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
		<title level="m">Introduction to Statistical Pattern Recognition</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Nonparametric multivariate density estimation: a comparative study</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lippman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="2795" to="2810" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Bellman</surname></persName>
		</author>
		<title level="m">Adaptive Control Processes: A Guided Tour</title>
		<meeting><address><addrLine>Princeton, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Princeton University Press</publisher>
			<date type="published" when="1961">1961</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Feature Extraction, Foundations and Applications</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nikravesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Series Studies in Fuzziness and Soft Computing</title>
		<meeting><address><addrLine>Wurzburg, Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Popular ensemble methods: an empirical study</title>
		<author>
			<persName><forename type="first">D</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Maclin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="169" to="198" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<author>
			<persName><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bienenstock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Doursat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks and the bias/variance dilemma</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Linear and order statistics combiners for pattern classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Combining Artificial Neural Nets</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Sharkey</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="127" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Experiments with a new boosting algorithm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference</title>
		<meeting>the 13th International Conference<address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
	<note>Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Input decimated ensembles</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Oza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="65" to="77" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attribute bagging: improving accuracy of classifier ensembles by using random feature subsets</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bryll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gutierrez-Osuna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Quek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1291" to="1302" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The random subspace method for constructing decision forests</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="832" to="844" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Nearest neighbor classification from multiple feature subsets, Intelligent Data Anal</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="191" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ensemble feature selection with the simple Bayesian classification in medical diagnostics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tsymbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Puuronen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th IEEE Symposium on Computer-Based Medical Systems CBMS&apos;2002</title>
		<meeting>the 15th IEEE Symposium on Computer-Based Medical Systems CBMS&apos;2002<address><addrLine>Maribor, Slovenia; Silver Spring, MD</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Soc. Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="225" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-knowledge for decision making</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mcginnity</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="246" to="266" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Combining multiple K-nearest neighbor classifiers for text classification by reducts</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Discovery Science</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting>the 5th International Conference on Discovery Science<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2534</biblScope>
			<biblScope unit="page" from="340" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Constructing rough decision forests</title>
		<title level="s">Lecture Notes in Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Slezak</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3642</biblScope>
			<biblScope unit="page" from="147" to="156" />
		</imprint>
	</monogr>
	<note>RSFDGrC 2005</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Diversity versus quality in classification ensembles based on feature selection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ECML 2000</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>De Mántaras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Plaza</surname></persName>
		</editor>
		<meeting>the ECML 2000<address><addrLine>Barcelona, Spain; Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">1810</biblScope>
			<biblScope unit="page" from="109" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Using diversity in preparing ensembles of classifiers based on different feature subsets to minimize generalization error</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zenobi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ECML</title>
		<title level="s">Lecture Notes in Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">L</forename><surname>De Readt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Flach</surname></persName>
		</editor>
		<meeting>the ECML<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="volume">2167</biblScope>
			<biblScope unit="page" from="576" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Diversity in search strategies for ensemble feature selection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tsymbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pechenizkiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Feature selection algorithms for the generation of multiple classifier systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Lett</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1323" to="1336" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Decomposition methodology for classification tasks-a meta decomposer framework</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rokach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="257" to="271" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Decomposition in data mining: an industrial case study</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kusiak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Electron. Packag. Manuf</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="345" to="353" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A survey of methods for scaling up inductive learning algorithms</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kolluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 3rd International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On combining artificial neural nets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sharkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Connection Sci</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="299" to="313" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On the accuracy of meta-learning for scalable data mining</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Stolfo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Intelligent Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="5" to="28" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A linear-Bayes classifier</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances on Artificial Intelligence, Proceedings of the SBIA 2000</title>
		<title level="s">Lecture Notes in Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Monard</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">1952</biblScope>
			<biblScope unit="page" from="269" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Error correlation and error reduction in ensemble classifiers, connection science</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combining Artificial Neural Networks: Ensemble Approaches</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="385" to="404" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
	<note>special issue</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Human expert-level performance on a scientific image analysis task by a system using combined artificial neural networks</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Cherkauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Notes, Integrating Multiple Learned Models for Improving and Scaling Machine Learning Algorithms Workshop, 13th National Conference on Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Chan</surname></persName>
		</editor>
		<meeting><address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="15" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Methods of combining multiple classifiers with different features and their applications to text-independent speaker identification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Pattern Recognition Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="417" to="445" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Constructing heterogeneous committees via input feature grouping</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Moody</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Leen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K.-R</forename><surname>Muller</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Feature set decomposition for decision trees</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rokach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Maimon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Intelligent Data Anal</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="131" to="158" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Evolutionary algorithms for data mining</title>
		<author>
			<persName><forename type="first">A</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Data Mining and Knowledge Discovery Handbook</title>
		<editor>
			<persName><forename type="first">O</forename><surname>Maimon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Rokach</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="435" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Feature selection for ensembles</title>
		<author>
			<persName><forename type="first">D</forename><surname>Opitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th National Conference on Artificial Intelligence, AAAI</title>
		<meeting>the 16th National Conference on Artificial Intelligence, AAAI</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="379" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Predicting convergence time for genetic algorithms</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J E</forename><surname>Rawlins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Genetic Algorithms</title>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Whitley</surname></persName>
		</editor>
		<meeting><address><addrLine>Los Altos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="141" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Objective criteria for the evaluation of clustering methods</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Rand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="846" to="850" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficient GA based techniques for classification</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Sharpe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Glover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Intell</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="277" to="284" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Comparison of algorithms that select features for pattern classifiers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sklansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="25" to="41" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Genetic wrappers for feature selection in decision tree induction and variable ordering in Bayesian network structure learning</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="103" to="122" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Actively searching for an effective neural-network ensemble</title>
		<author>
			<persName><forename type="first">D</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shavlik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Connection Sci</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="337" to="353" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Genetic algorithms for selection and partitioning of features in large-scale data mining problems</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint AAAI-GECCO Workshop on Data Mining with Evolutionary Algorithms</title>
		<meeting>the Joint AAAI-GECCO Workshop on Data Mining with Evolutionary Algorithms<address><addrLine>Orlando, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-07">July 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The relationship between PAC, the statistical physics framework, the Bayesian framework, and the VC framework</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Mathematics of Generalization, The SFI Studies in the Sciences of Complexity</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Wolpert</surname></persName>
		</editor>
		<meeting><address><addrLine>Reading, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="117" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Generalization bounds for decision trees</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Annual Conference on Computer Learning Theory</title>
		<meeting>the 13th Annual Conference on Computer Learning Theory<address><addrLine>San Francisco; Los Altos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="69" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Feature selection for support vector machines using genetic algorithms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fröhlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Artif. Intell. Tools</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="791" to="800" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<title level="m">C4.5: Programs for Machine Learning</title>
		<meeting><address><addrLine>Los Altos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
		<title level="m">Pattern Classification and Scene Analysis</title>
		<meeting><address><addrLine>New-York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning Boolean concepts in the presence of many irrelevant features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Almuallim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterichm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="279" to="306" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Efficiently inducing determinations: a complete and systematic search algorithm that uses optimal pruning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Schlimmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1993 International Conference on Machine Learning</title>
		<meeting>the 1993 International Conference on Machine Learning<address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="284" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Induction of selective Bayesian classifiers</title>
		<author>
			<persName><forename type="first">P</forename><surname>Langley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the 10th Conference on Uncertainty in Artificial Intelligence<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="399" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A compact and accurate model for classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Last</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maimon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="203" to="215" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning ensembles from bites: a scalable and accurate approach</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="421" to="451" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">UCI repository of Machine Learning Databases</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Merz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<pubPlace>Irvine, CA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Information and Computer Science, University of California</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Statistical comparisons of classifiers over multiple data sets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Demsar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">On the practical applicability of VC dimension bounds</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niranjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1265" to="1288" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning Boolean formulas</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1298" to="1328" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">MML inference of predictive trees, graphs and nets</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Learning and Probabilistic Reasoning</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Gammerman</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="43" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">About the Author-LIOR ROKACH is an assistant professor in the Department of Information System Engineering and the Program of Software Engineering of Ben-Gurion University, Israel. His research interests include artificial intelligence, pattern recognition, data mining, control of production processes and medical informatics. Dr. Rokach is the co-author of the book &quot;Decomposition Methodology for Knowledge Discovery and Data Mining: Theory and Applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schmitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Data Mining and Knowledge Discovery Handbook</title>
		<imprint>
			<publisher>Springer. Dr. Rokach holds B.Sc., M.Sc. and Ph</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="241" to="301" />
		</imprint>
		<respStmt>
			<orgName>D. in Industrial Engineering from Tel Aviv University</orgName>
		</respStmt>
	</monogr>
	<note>On the complexity of computing and learning with multiplicative neural networks</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
