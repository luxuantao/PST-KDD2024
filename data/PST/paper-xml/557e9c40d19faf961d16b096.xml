<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Asymptotic Properties of Data Compression and Suffix Trees</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Wojciech</forename><surname>Szpankowski</surname></persName>
						</author>
						<title level="a" type="main">Asymptotic Properties of Data Compression and Suffix Trees</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4E76BE0FA37567612923EBCD93E85A27</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Universal data compression</term>
					<term>Lempel-Ziv parsing algorithm</term>
					<term>suffix trees</term>
					<term>repeated subwords</term>
					<term>asymptotic analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, Wyner and Ziv have proved that the typical length of a repeated subword found within the first n positions of a stationary ergodic sequence is (l/h) log n in probability where h is the entropy of the alphabet. This finding was used to obtain several insights into certain universal data compression schemes, most notably the Lempel-Ziv data compression algorithm. Wyner and Ziv have also coqjectured that their result can be extended to a stronger almost sure convergence. In this paper, we settle this coqjecture in the negative in the so called right domain asymptotic, that is, during a dynamic phase of expanding the data base. We proveunder an additional assumption involving mixing conditionsthat the length of a typical repeated subword oscillates almostsurely (as.) between ( l / h l ) logn and (l/h2) log n where 0 &lt; ha &lt; h 5 hl &lt; CO. We also show that the length of the nth block in the Lempel-Ziv parsing algorithm reveals a similar behavior. We relate our findings to some problems on digital trees, namely the asymptotic behavior of a (noncompact) suffix tree built from suffixes of a random sequence. We prove that the height and the shortest feasible path in a suffix tree are typically (l/hz) log n (8s.) and ( l / h l ) log n (a.s.) respectively. These results were inspired by a seminal paper of Pittel who analyzed typical behavior of digital trees built from independent words (i.e., the so called independent tries).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>EPEATED patterns and related phenomena in words R (sequences, strings) are known to play a central role in many facets of telecommunications and theoretical computer science, notably in coding theory and data compression, in the theory of formal languages, and in the design and analysis of algorithms. Several efficient algorithms have been designed to detect and to exploit the presence of repeated substrings and other kinds of regularities in words. In data compression, such a repeated subsequence can be used to reduce the size of the original sequence (e.g., universal data compression schemes <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b42">[43]</ref>). In exact string matching algorithms the longest suffix that matches a substring of the pattern string is used for "fast" shift of the pattern over a text string (cf. Knuth-Morris-Pratt and Boyer-Moore <ref type="bibr" target="#b1">[2]</ref>; see also <ref type="bibr">[ll]</ref>), and so forth.</p><p>The problem of repeated patterns is studied here in a probabilistic framework. We assume that a stationary and Manuscript received <ref type="bibr">February 7, 1991</ref>; revised <ref type="bibr">March 28, 1993</ref>. This paper was presented in part at the Data Compression Conference, Snowbird, 1991. Wojciech Szpankowski Department of Computer Science Purdue University W. Lafayette, IN 47907 U.S.A. This work was supported in part by NSF Grants CCR-8900305, its extension CCR-9201078, and NCR-9206315, AFOSR Grant 90-0107, NATO Grant 0057/89, and Grant R01 LM05118 from the National Library of Medicine. ergodic source of information generates an infinite sequence {Xk}r=-=_oo over a finite alphabet C of size V. This probabilistic model contains other simpler probabilistic schemes such as the Berrloulli model (i.e., symbols from the alphabet are generated independently) and the Markovian model (i.e., the next generated symbol depends in a probabilistic sense only on the previous one).</p><p>The relevance of our problem is illustrated in the following examples taken from data compression and algorithms on words. The data compression example is also used to motivate our further study. In particular, we define below some parameters of interest that are analyzed in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 1.1: Data Compression</head><p>The following idea is behind most data compression schemes. Consider a "database" sequence of length n which is known to both sender and receiver. Instead of transmitting the next L, symbols to the other side of a communication channel, the sender can "look backward" into the data base and verify whether these L, symbols have already occurred in the data base. If this is the case, then instead of .sending L, symbols the sender transmits only the location of these symbols in the data base and the length of L,. After identifying L,, we either append the data base with these new L, symbols orif the length of the data base is fixed (e.g., sliding window implementation in <ref type="bibr" target="#b6">[7]</ref>) -we move the data base to the new position. This idea can be modeled mathematically in two different fashions that are discussed next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Static Model -Left Domain Asymptotic</head><p>This is the model of Wyner and Ziv <ref type="bibr" target="#b40">[41]</ref>.It is assumed that the subsequence. to be compressed { X k } z O is always the same (by definition fixed at position IC = 0), and the data base {Xk}L:-, expands only to the left. Therefore, we coin the term left domain asymptotic for such a model with n tending to infinity. In practice, such a static situation occurs rather rarely since usually a new word is to be transmitted and compressed. Nevertheless, the model has some mathematical appeal and can be used to estimate the entropy. Following Wyner and Ziv <ref type="bibr" target="#b40">[41]</ref>, we define two parameters relevant to the perfoFance of some data compression schemes. For every n, let L, be the smallest integer L &gt; 0 such that xf-1 # XI,"+for all 1 5 m 5 n, (l.la) -(i.e., L, -1 is the length of the longest substring that is repeated and can be recopied from a data base of size n). In the above, we use the standard notation for subsequences, that is, X : = ( X i , . . . , X j ) .</p><p>In a practical implementatip, the encoder @serves X:"-l, determines mo such that Xtn-' = X I z : + L n -2 , and transmits mo, L, and X Z , -~. To encode mo we need log, n symbols, and it is known that z, may be represented by log E, bits (cf. <ref type="bibr" target="#b32">[33]</ref>). As noted by Wyner and Ziv <ref type="bibr" target="#b40">[41]</ref>, the number of encoded symbols per source symbol is asymptotically --lo&amp; l0gV</p><formula xml:id="formula_0">+ v + v .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ln Ln Ln</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I</head><p>Hence, the ratio log, n / L , determines an asymptotic efficiency of the compression scheme.</p><p>Another parameter of interest can be defined as follows.</p><p>For every integer C, let Ne be the smallest nonnegative integer</p><formula xml:id="formula_1">N &gt; 0 such that x:-l = ~-N + t -l -N 7 (l.lb)</formula><p>that is, a wortof length C is repeated for the first time in a data base of size Ne. <ref type="bibr">Wyner and Ziv [41]</ref> suggested the following compression scheme. The encoder sends the first n source symbols, say X I : with no compression, but the next C symbols Xi-' are encoded as follows: if Xi-' is a substring of X!i2 (i.e.,fie 5 n), then Xi-' is compressed by specifying only Ne; otherwise Xi-' is not compressed. As noted in <ref type="bibr" target="#b40">[41]</ref>, in this scheme the average number of ymbols required to encode Xi-' is Pr{Ne 5 n } log, n+Pr{Ne &gt; n}C+l, where log, n is the number of symbols needed to transmit fie.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dynamic Model -Right Domain Asymptotic</head><p>We introduce here a new model in which the next word to be compressed is not fixed, and each time after the compression the word is added to the (expanding) data base. In the analysis of such a model, it is more convenient to deal with a one-sided stationary and ergodic sequence {Xk}&amp;. Then, the data base of length n is represented by { X k } i = l and the word to be compressed starts at k = n + 1. Asymptotic analysis of such a model is carried out in the so called right domain asymptotic (since the data base is expanded to the right). This model seems to fit better to real implementation (e.g., sliding window <ref type="bibr" target="#b6">[7]</ref>) of data compression schemes, and most of our analyses deal with this model. We tan definctwo parameters L, and Ne which correspond to L, and Ne in the static model. More specifically, L, is defined as the largest value of L such that for some mo E {I,. , . ,n}. (1.2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X E : + ~ = X ; : , " "</head><p>In a similar fashion, Ne is defined as the smallest N such that Xf = X:$t. It is easy to see that the compression schemes discussed above can be naturally expressed in terms of Ne and L,.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 1.2: Lempel-Ziv Parsing Algorithm</head><p>The heart of the Lempel-Ziv compression scheme is a method of parsing a string { X k } i Z l into blocks of different words. The precise scheme of parsing the first n symbols of a sequence {Xk}T=' is complicated and can be found in <ref type="bibr" target="#b27">[27]</ref>. Rvo important features of the parsing are: (i) the blocks are pairwise distinct; (ii) each block that occurs in the parsing has already been seen somewhere to the left. For example, for { X k } = 110101001111~~~ the parsing looks like (l)(lO)(lOlOO)(ll)(l1~ s-); that is, the first block has length one, the second block length has two, the next one is of length five, and so on.' Observe that the third block is the longest prefix of X F and X r . <ref type="bibr">Grassberger [15]</ref> has shown how to construct such a parsing by using a special data structure called suffix tree (cf. <ref type="bibr">[l]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b15">[15]</ref>). Naturally, one is interested in the length of a block in the parsing algorithm. Let 1, be the length of the nth block in the Lempel-Ziv parsing algorithm. There is a relationship between 1, and the parameter L, defined above, namely 1, = Lcn-1 (cf. Section 2 for details). In view of this, the asymptotic behavior of L, can be used to obtain the asymptotic bounds for the length 1, in the Lempel-Ziv parsing algorithm.</p><formula xml:id="formula_2">k = l</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EX4MPLE 1.3: String Matching Algorithms</head><p>Repeated substrings also arise in many algorithms on strings, notably string matching algorithms (cf. <ref type="bibr">[l]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b39">[40]</ref>). A string matching algorithm searches for all (exact or approximate) occurrences of the pattern string P in the text string T. Consider either the Knuth-Morris-Pratt algorithm or the Boyer-Moore algorithm (cf. <ref type="bibr" target="#b1">[2]</ref>). Both algorithms rely on an observation that in the case of a mismatch between T and P , say at position n + 1 of P , the next attempt to match depends on the internal structure (i.e., repeated substrings) of the first n symbols of the pattern P. It turns out that this problem can be efficiently solved by means of a suffix tree From the above discussion, one concludes that suffix trees can be used to unify analyses of repeated patterns, and in particular to analyze L,. Therefore, a short description of a suffix tree follows. An interested reader may find more on such trees and their applications in Ah0 et al. <ref type="bibr">[l]</ref> and Apostolic0 <ref type="bibr" target="#b3">[4]</ref> (cf. Grassberger <ref type="bibr" target="#b15">[15]</ref>). A suffix tree is a digital tree built from suffixes of a string X . In general, a digital treealso called a triestores a set of words (strings, sequences, keys) X = {X(l), . . . , X ( n ) } , each key being a sequence from a finite alphabet E, that is, for 1 5 n we have {Xk(C)}&amp; with Xk(C) E E. A trie consists of branching nodes, called also internal nodes, and external nodes that store the keys. Every external node is able to store only one key. The branching policy at any level, say IC, is based on the Icth symbol of a string (key, word). For example, for a binary alphabet C = (0, l}, if the Icth symbol in a key is "O", then we branch-out left in the trie, otherwise we go to the right. This process terminates when 'There is another way of parsing a sequence in which phrases do not overlap. For example, our sequence is parsed according to this algorithm as (l)(lO)(lOl)(OO)(ll)(ll . . .). In this paper, we do not consider such a parsing scheme. s,= 1 1 0 1 1 1 0 sa= 1 0 1 1 1 0 s* = 0 1 0 1 1 0 1 1 1 0 s2 = 1 0 1 1 0 1 1 1 0 ss = 0 1 1 0 1 1 1 0 n Fig. <ref type="figure">1</ref>. Suffix tree built from the first five suffixes of x = 0 1 0 1 1 0 1 1 1 0 ~~~ .</p><p>for the first time we encounter a different symbol between a key that is currently inserted into the trie and all other keys already in the trie. Then, this new key is stored in a newly generated external node. If X(1), . . . , X(n) are statistically independent sequences, then the constructed trie is called an independent trie. If, however, X = {SI, 5'2, ..., S,} where Si is the ith suffix of a one-sided single sequence {Xk}&amp;, then the trie built from X is called a suffix tree. Certainly, in suffix trees the keys X(l) = SI,. . . , X(n) = S, are statistically dependent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EXAMPLE 1.4: S u m Tree</head><p>Let X = 0 1 0 1 1 0 1 1 1 0 .... . Then the first five suffixes are S1 = 0 1 0 1 1 0 1 1 1 0 .... , 5'2 = 1 0 1 1 0 1 1 1 0 .... , S, =</p><p>The suffix tree built from these five suffixes of X is shown in An important parameter of a suffix tree that plays a crucial role in the analysis and design of algorithms on strings and data compression schemes is the depth ofa s u m . Let S, be a suffix tree constructed from the first n suffixes of a sequence {Xk}r=l. Then, the depth of the ith suffix Ln(i) in S, is the length of the path from the root to this suffix. We shall write ,+1(n + l ) , that is, L, is the depth of insertion of the Sn+l-st suffix into the tree S,. Naturally, we call this parameter the depth of insertion. In the_next section, we will show that this L, coincides with the L, and L, introduced in Example 1.1 (cf. (l.la) and (1.2)).</p><p>From the previous discussion, it should be clear that the behavior of L, is of considerable importance to combinatorial problems on words, in particular to data compression and string algorithms. The probabilistic behavior of repeated patterns for stationary and ergodic sequences was recently studied by Wyner and Ziv <ref type="bibr" target="#b40">[41]</ref>. In fact, the authors of <ref type="bibr" target="#b40">[41]</ref> studied the 0 1 1 0 1 1 1 0 ...., S q = 1 1 0 1 1 1 0 .... and S5 = 1 0 1 1 1 0 ..... TheoremO: <ref type="bibr">(Wyner and Ziv [41]</ref>) Let {Xk}T=-, be a stationary and ergodic sequence built over a finite alphabet E.</p><p>Then, in the left domain asymptotic as n 4 00 -1 logn h 4 -in probability (pr.) and -logNe 4 h in probability (pr.)</p><formula xml:id="formula_3">e (1.3a) (1.3b)</formula><p>where h is the entropy of X. 0</p><p>This result concerns the convergence in probability (pr.) of z,. In fact, a similar results also holds for L, in the right domain asymptotics (cf. <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b38">[39]</ref>). Wyner and Ziv <ref type="bibr" target="#b40">[41]</ref> asked whether it can be extended to a stronger almost sure (as.) convergence. In the right domain asymptotic, we shall settle this question in the negative for the Markovian case, and show that L, oscillates with probability one between ( l / h l ) log n and (1lhz)logn where h2 &lt; h 5 hl. From this, it should be clear that the Wyner-Ziv conjecture cannot be also true in a more general than Markovian framework. In the course of the proof of our main results,-we also indicate that the Wyner-Ziv conjecture concerning L, can be directly proved from their convergence in probability result in the left domain asympotic for a Markovian source. This is due to the fact that L, is a nondecreasing sequence as opposed to L,. In the Eon-Markovian case, the proof for the (as.) convergence of L, is more intricate and due to Omstein and Weiss <ref type="bibr" target="#b30">[30]</ref>.</p><p>In this paper, we mainly deal with the more interesting right domain asymptotic which has also several applications in the analysis and design of algorithms on words. In particular, during the course of the proof we establish some new results regarding a typical (probabilistic) behavior of the height H, and the shortest feasible path s, in a suffix tree. The height H, is the longest path in S,, while the shortest feasible path is the shortest path from the root to an available (feasible) node. A node is called available if it does not belong to the tree S, but its predecessor node (either an internal or an external one) is in S,. Then, under some additional assumptions involving mixing conditions, we show that H, N (l/hz) log n (as.) and s, -( l l h l ) logn (as.), where hl and hz will be given explicitly. This result implies that a typical suffix tree is fairly balanced. As a consequence of this, brute force (i.e., straightforward) algorithms for problems on words (e.g., construction of a suffix tree) could be a challenging competitor of more sophisticated algorithms designed to optimize the worst-case behavior (cf. Apostolico and Szpankowski <ref type="bibr" target="#b4">[5]</ref>).</p><p>Asymptotic analyses of suffix trees and universal data compressions are rather scanty in the literature. To our best knowledge, asymptotic analysis of universal data compressions was pursued by Ziv and Lempel (cf. <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b27">[27]</ref>; see also <ref type="bibr" target="#b6">[7]</ref>), Wyner and Ziv <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, and Kieffer <ref type="bibr" target="#b22">[22]</ref>. The average case analysis of suffix trees was initialized by Grassberger <ref type="bibr" target="#b15">[15]</ref>, and Apostolico and Szpankowski <ref type="bibr" target="#b4">[5]</ref>. For the Bernoulli model, the asymptotic behavior of the height was recently obtained by Devroye, Szpankowski and Rais <ref type="bibr" target="#b14">[14]</ref>, and the limiting k xr, 0 1 0 distribution of the depth in a suffix tree is reported in Jacquet and Szpankowski <ref type="bibr" target="#b18">[18]</ref>. Arratia and Waterman <ref type="bibr" target="#b5">[6]</ref> investigated a related problem, namely the longest contiguous matching within a single sequence, and obtained several interesting results in this direction. Their findings are related to the height of a suffix tree. Finally, heuristic arguments were used by Blumer et al. <ref type="bibr" target="#b9">[9]</ref> to show that the average number of internal nodesin a slightly different model of suffix treesis a linear function of n (more precisely, the coefficient of n contains an oscillating term). Jacquet and Szpankowski [ 181 established rigorously the latter result regarding the average size of a sufflx tree. Some related topics were discussed by Guibas and Odlyzko in <ref type="bibr" target="#b16">[16]</ref>.</p><p>Our findings were inspired by the seminal paper of Pittel <ref type="bibr" target="#b31">[31]</ref> who considered a typical behavior of a trie constructed from independent words (i.e., independent tries). Pittel was the first who noticed that the depth of insertion in an independent trie does not converge almost surely but rather oscillates between the typical height and the typical shortest feasible path in a trie. Therefore, one can also consider this paper as a direct extension of Pittel's results to dependent tries such as suffix trees.</p><p>This paper is organized as follows. In the next section we formulate our main results, discuss some consequences of our findings, and suggest some further studies. Most proofs are delayed till Section I11 which is of its own interest.</p><p>-4 -3 -2 -1 0 1 2 3 4 5 1 1 0 1 1 1 0</p><formula xml:id="formula_4">11. MAI N RESULTS</formula><p>Let {Xk}r=-m be a stationary ergodic sequence of symbols generated from a finite alphabet C of size V. Define a partial sequence X : as X : = (Xm, ..., X,) for m &lt; n, and the nth order probability distribution as follows:</p><p>(2.1)</p><formula xml:id="formula_5">P(X,") = Pr{Xk = z k , 1 5 k 5 n,zk E E}. The entropy of { X k } is 9 (2.</formula><p>2)</p><formula xml:id="formula_6">E log P-l(X,") h = lim n+m n</formula><p>The existence of the above limit is guaranteed by Shannon's Theorem (cf. [SI). It is also known that h 5 logV. Hereafter, all logarithmsunless stated explicitly otherwiseare natural logarithms.</p><p>It is well known (cf. <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b13">[13]</ref>, <ref type="bibr" target="#b22">[22]</ref>) that the entropy of a stationary ergodic information source is intimately related to coding and certain data compression schemes, most notably the universal compression scheme of Lempel and Ziv <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b42">[43]</ref>. Following Wyner and Ziv <ref type="bibr" target="#b40">[41]</ref>, we defined in the Introduction two parameters to capture certain properties of repeated subsequences, namely L, and Ne (cf. (1.2)). Hereafter, we shall mainly deal with L,, and we recall that L, is the smallest</p><formula xml:id="formula_7">integer L &gt; 0 such that X:+~-' # X ~~, L f o r a l l l 5 m 5 n.</formula><p>(2.3)</p><p>We shall analyze L, in the right domain asymptotic, as discussed in the Introduction. However, we also show that the left domain asymptotic falls into our framework, and we provide some results in this domain (see Remark 2(ii)). To illustrate our definition (2.3), we present one example below. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EXAMPLE 2.1: Illustration of Definitions</head><p>We first discuss the left domain (cf. (1.lb)). Let {Xk} be given in Fig. <ref type="figure" target="#fig_3">2</ref> which is identical to the sequence discussed by Wyner and Ziv <ref type="bibr" target="#b40">[41]</ref>, and used in our Examplcl. <ref type="bibr" target="#b3">4</ref> </p><p>[SI). In some statements of our results, we need a stronger form of the mixing condition, namely the strong a-mixing condition which reads as follows The existence of hl and h2 was established by Pittel <ref type="bibr" target="#b31">[31]</ref> who also noticed that 0 5 ha 5 h 5 hl. It should be noted that h2 is the second-order R h y i entropy, while hl could be interpreted as Rtnyi entropy of order -00 (cf. <ref type="bibr" target="#b13">[13]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 1:</head><p>(i) Bernoulli Model: In this model, symbols from the alphabet C are generated independently, that is, P(XT) = P"(X:). In particular, we assume that the ith symbol from the alphabet C is generated according to the probability p i , where 1 5 i _&lt; V and CL1pi = 1. Thus, h = C L 1 p i l o g p i l ([SI), hl = log(l/pmin) and ha = 2log(l/P) where pmin = minll;lv{p,} and P = CL1p:. The probability P can be interpreted as the probability of a match between any two symbols.</p><p>(ii) Markovian Model: In this model, the sequence { X k } forms a stationary Markov chain, that is, the (IC + 1)st symbol in {X,} depends on the previously selected symbol. Hence, the transition probability becomes p2,3 = Pr{Xle+l = j E C(Xk = i E E}, and the transition matrix is P = { p a , , } ~3 = 1 .</p><p>It is well known <ref type="bibr" target="#b8">[8]</ref> that the entropy h can be computed as h = -Cr2=, logp,,3 where rs is the stationary distribution of the Markov chain. The other quantities, that is, hl and h2, are a little harder to evaluate. Pittel <ref type="bibr" target="#b31">[31]</ref> and Szpankowski <ref type="bibr" target="#b37">[38]</ref>  </p><formula xml:id="formula_9">a(d) = O(d0pd) (2.9)</formula><p>for some constants 0 &lt; p &lt; 1 and ,B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 2:</head><p>(i) How restrictive is condition (2.9)? First of all, we note that we really need (2.9) only for establishing the lower bound in the liminf case (see <ref type="bibr">(3.22)</ref> in Section 111-B). Nevertheless, even with (2.9) we can cover many interesting cases including the Bernoulli model and the Markovian model. Naturally, in the Bernoulli model (2.9) holds since in this case a(d) = 0. In the Markovian model, it is known (cf. <ref type="bibr" target="#b8">[8]</ref>) that for a finite state Markov chain the coefficient a(d) decays exponentially fast; that is, for some c &gt; 0 and p &lt; 1 we have a(d) = cpd, as needed for (2.9). However, for general stationary ergodic sequences our result possibly does not hold. This is due to P. Shields who constructedusing his approach from <ref type="bibr" target="#b34">[35]</ref> an ergodic mixing, stationary sequence that does not satisfy the lim sup part of (2.8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(ii) Asymptotic behavior of E, in the left domain asymptotic:</head><p>We now show that in the left domain asymptotic the (as.) This is true, for example, for the Bernoulli model, the Markovian model, the hidden Markov model and m-dependent model (cf. <ref type="bibr" target="#b13">[13]</ref>). In order to apply the Borel-Cantelli Lemma, we use the trick suggested by Kingman <ref type="bibr" target="#b23">[23]</ref>; that is, we construct a subsequence n, of n for which O ( l / J e ) is summable. and similarly for the liminf case. Taking in the last display s + 00, we finally prove our assertion (for more details see 0 Theorem 1 will be proved below as a simple consequence of some new results concerning a typical behavior of a suffix tree S , built over the first n suffixes of {Xk}&amp;, as discussed in the Introduction. The clue to the proof of Theorem 1 is to reformulate the definition of L, in terms of some parameters of the associated suffix tree S,.</p><p>Define for S, the mth depth L,(m), the height H, and the shortest feasible path s , as in the Introduction. That is, the depth of the mth external node containing the mth suffix (e.g., Figs. <ref type="figure">1</ref> and<ref type="figure" target="#fig_9">3</ref>) is equal to one plus the number of internal nodes in the path from the root to the mth external node. Then, also the end of Section 111-1C).</p><p>H, = max {L,(m)}. l l m s n <ref type="bibr">(2.11)</ref> The shortest feasible path is defined as follows. Consider a suffix tree S,, and append it with available nodes, that is, nodes that are not in the tree S , but whose predecessors (either internal or external nodes) are in S,. Then, the shortest feasible path is the shortest path to an available node. Furthermore, we define the average depth D, and the depth of insertion L,.</p><p>The depth of insertion L, is the depth of the (n+ 1)st external node after insertion of the ( n + 1)st suffix Sn+l into the suffix tree S,, that is, L, = L,+l(n + l ) . ' Finally, D, is defined as the depth of a randomly selected external node, that is, For our purpose, another characterization of the above parameters is more useful. For a suffix tree S, built from n suffixes Sl,S,, ..., S, of a stationary ergodic sequence {Xk}r=l, define the self-alignment ci,j between si and sj as the length of the longest common prefix of S i and Si. Then, the following is easy to establish (cf. <ref type="bibr" target="#b37">[38]</ref>) and finally</p><formula xml:id="formula_10">Ln(m) = max {Ck,m} + 1 l &lt; k &lt; n , k # m (2.13a) (2.13b) (2.13~)</formula><p>From the last display it is clear that L, defined in terms of the suffix tree and L, defined in (1.2) are the same. Therefore, we can further reason only in terms of L, as defined in (2.13~).</p><p>In passing, we note that the second parameter discussed in Example 1.1, namely Ne, can also be re-defined in terms of the associated suffix tree. Indeed, Ne is the size of a suffix tree in which the depth of the first suffix in S, is equal to C, that is,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L N ~</head><p>(1) = C. We should also point out that in the left domain asymptotic, the analysis of fit (cf. Ll.lb)) is much easier due to the following relationship with L, (cf. <ref type="bibr" target="#b40">[41]</ref>)</p><p>This relationship does not hold for Ne and L, (e.g., consider X = 011010101010101 for which N3 &gt; 7 and L7 &gt; 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 3:</head><p>Asymptotic Behavior of Ne.We can predict the (as.) behavior of Ne in the right domain asymptotic since we have already observed that the quantity Ne can be alternatively defined as L N , ( ~) = C. But, for the Markovian model it is known (cf. Szpankowski <ref type="bibr" target="#b38">[39]</ref>) that L,(l)/logn -+ l / h (as.). Hence, e/ log Ne -+ l / h (as.). Interestingly enough, the asymptotic behavior of Ne is the same in the left domain asymptotic, as proved recently by Qrnstein and Weiss <ref type="bibr" target="#b30">[30]</ref> for a general probabilistic model. This is in surprising contrast to the asymptotic behavior of Ln in these two domains of</p><p>The quantity z, defined by Wyner and Ziv <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>(see Example 1.1) can be easily obtained from our suffix tree model, too. Indeed, in this case one has to conspxt the suffix tree from the first n suffixes of {Xk}T=-,, and L, is the depth of insertion of the first suffix S1 into this suffix tree. Note that in the Wyner-Ziv model, we always insert the same suffix, namely {xk}r=o, and we expand the sequence {Xk}r?-, to the left, that is, the next suffix inserted into the tree 5, is X?n-l. Finally, in Remark 2(ii) we already noted that L, is a nondecreasing sequence in the sense that L, 5 Ln+l. This is illustrated below. Note that almost surely L, = H, whenever Hn+l &gt; H,, which happens infinitely often (io.) since H, -+ CO (as.), and { x k } is an ergodic sequence. Therefore, Pr{L, = H, i.0.) = 1 implies that almost surely there exists a subsequence, say n k -+ CO, such that L,, = H,, . So, A,, / log n k = limnk+, H,, / log n k (as..), and this finally implies that (a.s.);</p><p>(2.16b) lim sup-</p><formula xml:id="formula_11">&gt; lim - n -m logn -n+m logn</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ln</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hn</head><p>that is, limn--ta sup Ln/ log n = limn+m H,/ log n (as.), and by (2.15a) this proves the lim sup part of (2.8) in Theorem 1. In a similar manner we can prove the lim inf part by using s , and (2.14) from Theorem 2 since sn is also a nondecreasing and unbounded (as.) sequence.</p><p>Finally, we apply Theorem 1 to estimate the length of the nth block in the Lempel-Ziv parsing algorithm as discussed in Example 1.2 (cf. <ref type="bibr" target="#b27">[27]</ref>). We prove the following result.</p><p>Corollary3: Let I , be the length of the nth block in the Lempel-Ziv parsing algorithm. In addition, we assume that the source is Markovian. Then, in the right domain asymptotic</p><formula xml:id="formula_12">1 1, 1, 1 -&lt; lim inf -5 lim sup -5 - hi -n + w n logn n+a logn h2 (a.s.).</formula><p>(2.17)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>~I</head><p>Proofi In Example 1.2 we noted that I, = LC;_l This is a direct consequence of the Lempel-Ziv parsing algorithm (see also <ref type="bibr">Grassberger [15]</ref>). Then,</p><p>The second term of the above can be further estimated as follows where the right-hand side (RHS) of the above is a direct consequence of the fact that for Markovian models E&amp;, Lk -(n/h) log n (as.) (cf. Shields <ref type="bibr" target="#b34">[35]</ref>, and Szpankowski <ref type="bibr" target="#b38">[39]</ref>). Hence, (2.17) follows from Theorem 1 and the above. 0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 4:</head><p>(i) Lempel-Ziv Parsing Algorithm for Finite Strings. In Corollary 3, we assumed an infinite length sequence {xk}p=1, and I, denoted the nth block length in such a sequence.</p><p>The original parsing algorithm of Lempel and Ziv, however, postulates that the underlying sequence is finite, say of length n, and the number of blocks M, is such that E z l l k = n.</p><p>Nevertheless, Corollary 3 is valid for finite sequences too. Indeed, this follows directly from the fact M, = O(n/ log n) <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b27">[27]</ref>. (A simple argument for M, = O(n/logn) works as follows: It suffices to note that n = ~2~ lk 5 ~2~ Lk -( l/h)Mn log M,, where the last asymptotic is already known from Shields <ref type="bibr" target="#b34">[35]</ref>).</p><p>(ii) Behavior of 1, Revisited. Corollary 3 does not exclude the possibility that In/ log n converges (as.) to a constant, however, this seems to be very unlikely. In fact, we expect that l,/logn resembles the behavior of L,. We have three reasons to believe this. First of all, 1, coincides with L, approximately every O(1ogn) symbols, so a formal proof would require to show that 1, hits H, and s , infinitely often.</p><p>Second, the consecutive blocks are only weakly dependent in the Markovian model. Thus, one can build a suffix tree from n weakly dependent sequences (e.g., in the Bernoulli model these sequences are practically independent), and in the view of M, = O(n/ log n), and the results of Pittel <ref type="bibr" target="#b31">[31]</ref> for independent tries, we obtain the desired result. Finally, we can easily prove our conjecture for a modified version of the Lempel-Ziv parsing algorithm that we already discussed in Example 1.2 (cf. <ref type="bibr" target="#b2">[3]</ref>) in which the phrases do not overlap. Such a parsing scheme can be conveniently modeled by another digital tree, namely the so called digital search tree (cf. <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr">[l]</ref>, <ref type="bibr" target="#b26">[26]</ref>). Then the length E, of the nth block is exactly equal to the depth of the nth node in such a digital search tree. Applying Pittel's result for independent digital search trees <ref type="bibr" target="#b31">[31]</ref>, we can prove under weak mixing condition that (cf. <ref type="bibr" target="#b19">[19]</ref>)</p><formula xml:id="formula_13">1, 1 (8.5.) lim sup -= - n + a logn hl logn h3 1, -1 lim inf --- (2.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>18) where h3 is defined as</head><p>In the Bernoulli model, we have h3 = log(l/pmar) where p, , , = maxllilvpi. Again, h3 is the Rtnyi entropy of order +CO.</p><p>(iii) Second Order Properties of L,. From our previous discussion, we know that L,/logn -+ l / h (pr.). We also know that the typical depth D, and the depth of insertion L, have the same limiting distribution, however, their almost sure behaviors are different. Recently, Jacquet and Szpankowski <ref type="bibr" target="#b18">[18]</ref> showed that for the Bernoulli asymmetric model the normalized depth (D, -ED,)/varD, converges in distribution to the standard normal distrbution N ( 0 , l ) with mean and variance as below <ref type="bibr">(2.19a)</ref> V for some E &gt; 0 where H2 = p: logp;, PI($) and Pz(x)</p><p>are fluctuating periodic functions with small amplitudes (an explicit formula for the constant C can be found in <ref type="bibr" target="#b36">[37]</ref>). We conjecture that the same type of limiting distributions can be obtained for the Markovian model. This is due to two facts: 1) the limiting behavior of independent tries do not differ too much from asymptotics of suffix trees-(cf. <ref type="bibr" target="#b18">[18]</ref>); 2) Jacquet and Szpankowski [ 171 established the limiting distribution of the depth for independent tries in a Markovian framework.</p><p>(iv) Second Order Behavior for the Lempel-Ziv Parsing Scheme. The limiting distribution of a randomly selected phrase in the Lempel-Ziv parsing algorithm is related to the typical depth in a sequence of dynamically built suffix trees. This is due to the relationship 1, = L as already noted by us. More interestingly, the number of phrases M, defined as cEl l k = n, is related-but not in a simple manner-to the external path length E, as defined as E, = L,(m).</p><p>Even in the Bernoulli model major difficulties arise in the evaluation of the limiting distribution of E, in a suffix tree. For independent tries, Kirschenhofer, Prodinger, and Szpankowski <ref type="bibr" target="#b24">[24]</ref> recently obtained for the symmetric alphabet the variance E, which is varE, = (a + P3(logn))n + O(log2n) where (Y M 4.35.. . (explicit formula for a can be found in <ref type="bibr">[a]</ref>)</p><p>and PS(1ogn) is a fluctuating function. We conjecture that the limiting distribution of the external path length in a suffix tree converges to a normal distribution, but this might not be too easy to prove. For the modified Lempel-Ziv parsing algorithm as discussed in Example 1.2 and Remark 4(ii) (without phrase overlapping), the situation is similar. This time, however, one needs to analyze an independent digital search tree, and this simplifies the problem. The symmetric Bernoulli model was already analyzed in Aldous and Shields <ref type="bibr" target="#b2">[3]</ref> who obtained the limiting distribution for the number of phrases M, and the internal path lenghth of the associated digital tree, however, without explicit formula for the variance. This was recently rectified by Jacquet and Szpankowski <ref type="bibr" target="#b19">[19]</ref> who used a result of Kirschenhofer, Prodinger, and <ref type="bibr">Szpankowski [25]</ref> to show that var M, (p + P4(logn))n/log3n where p M 0.26600.. .</p><p>(explicit but complicated formula on / 3 can be found in [25]), and P4(logn) is a fluctuating function. The extension to the asymmetric model seems to be a challenging problem. Nevertheless, it was recently shown (cf. <ref type="bibr" target="#b19">[19]</ref>) that the internal path length E, and the number of phrases M, are related as follows: Pr{M, &gt; m} = Pr{E, 5 n } , so one only needs the limiting distribution for E,. Nothing is known about the limiting distribution of E, in the asymmetric model, however, we conjecture that (E, -(n/h)logn)/var E, + N(0,l) 0</p><formula xml:id="formula_14">c k = l "'</formula><p>where var E, = O(n1ogn).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="111.">ANALYSIS</head><p>In this section we prove our main results presented in Theorem 2, that is, we establish almost sure convergence of the height H, and the shortest feasible path s , in a suffix tree. We prove these results separately for the height and for the shortest feasible path. In each case, we consider an upper bound and a lower bound. In both proofs we use quite often a technique that Jacquet and Szpankowski <ref type="bibr" target="#b18">[18]</ref> named stringruler approach, and was already used by Pittel <ref type="bibr" target="#b31">[31]</ref>. We also shall use some ideas from Szpankowski <ref type="bibr" target="#b37">[38]</ref>, and Devroye et al. <ref type="bibr" target="#b14">[14]</ref>.</p><p>In the string-ruler approach, a correlation between different (sub)strings is measured by means of another string, say w , that does not necessarily have to be random. We call w a string-ruler. Its "randomness" comes from the fact that the underlying sequence {xk} is random. To illustrate the technique, consider estimating the length of the longest common prefix of two independent strings, say { Xk (1)}' &amp; and { x k (2)}&amp;.</p><p>Let C1,z be the length of such a longest prefix (the reader should recognize in C1,z the alignment between X(l) and X ( <ref type="formula">2</ref>) ) . The clue is to note that C1,2 2 k implies the existence of a string w of length k such that Xf (1) = w and Xf (2) = w .</p><p>In fact, the reverse holds too, that is, the existence of w of length k such that Xf(1) = w and X f ( 2 ) = w is enough for C1,2 2 k. We shall use this observation to estimate the self-alignment between suffixes of a single sequence {xk}.</p><p>We adopt the following notation. Let Wk be the set of all strings w of length k, that is, WF, = { w E C k : JwI = k}, where IwI is the length of w . An element of wk will be denoted as W k , i.e., W k E wk, and by w :</p><p>we mean a concatenation of strings W k from W k . If a subsequence x"+' is equal to a string ruler wk, then we write</p><formula xml:id="formula_15">P ( W k ) = P ( x ~+ ~)</formula><p>for the probability that x,"+~ = W k .</p><p>Finally, for a function f ( W k ) of W k we write cwk f ( W k ) = CWkEWh f ( W k ) for the sum over all strings W k of length k.</p><p>Below, we shall reason only in terms of suffix trees leaving other interpretations of our results (e.g., data compression) for the reader. A more detailed discussion of (3.2) can be found in <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">The Height in a S u f i Tree</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Upper Bound</head><p>We use (3.2) to prove an upper bound for the height H,. We start with Boole's inequality applied to the event {maxi,dCi,i+d} (we set below i = 1 for simplicity of notation) which leads to (cf. also The first sum in (3.3) is more difficult to assess, but we shall also use only the weak mixing condition (2.4). We proceed as follows for d 5 k w d where the inequality (A) is due to the mixing condition (2.4), inequality (B) is a consequence of the inequality on means (cf. for some constant c. This proves the upper bound for the convergence in probability. The almost sure convergence will follow from the above after some algebra, and we shall discuss it after deriving the lower bound for the height.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Lower Bound</head><p>The lower bound is more intricate, although the main idea behind the proof is quite simple. We need a sharp bound on Pr{Hn 2 k} for k = ( 1 -E ) &amp; log n. We use two techniques to achieve it: first, we reduce the problem to a simpler one on tries with weaker dependency among keys, and second we apply the second moment method to evaluate the appropriate probability.</p><p>Note that Hn is stochastically nondecreasing, that is, if .. , Y ( m ) are not independent. However, using our strong a-mixing condition we can partition the original sequence X ; such that different parts are weakly dependent. This is illustrated in Fig. (e.g., using the second moment method). We note that Tm is built from weakly dependent sequences Y(1), ..., Y(m), hence one can expect that techniques used for independent tries (cf.</p><formula xml:id="formula_16">m 5 n,</formula><p>[14], <ref type="bibr" target="#b37">[38]</ref>) should work in this case too.</p><p>Let Ai,j = {Ci,j 2 k} be the event that the alignment3 Ci,j between Y (i) and Y ( j ) keys is greater than k = O(1og n). By the second moment method (cf.   <ref type="bibr" target="#b20">[20]</ref> and Szpankowski [38],</p><p>where d, is the length of the gap between the keys Y(i) and Y ( i + 1) (See Fig. <ref type="figure">4</ref>). The second Sum in the denominator Of <ref type="bibr">(3.8)</ref> is estimated as follows. Let W k g W k be a concatenation of a string-ruler wk, a gap-string g , and again the string-ruler W k . Note that Pr{Aij n At,} = P(wkgiWk n W ~g 2 ' &amp; ) . (3.9) wk wk Finally, the above implies the following estimate for the second sum in the denominator of (3.8)  In order to estimate the RHS of (3.9), we consider two cases:</p><p>Case A. The gapsgl and g2 do not overlap. In this case the events Aij and ASt are separated by a gap of length at least Pr{Aij n A s } ( i , j ) # ( t , s )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">m4(i + a(d,))3E2P(wk) + m3C(EP(Wk))3/2</head><p>We now refer to Cc,J as an alignment instead of the self-alignment since the trie 7 , is built from weakly dependent ("almost independent") sequences</p><formula xml:id="formula_17">Y ( l ) , ... , Y ( m ) .</formula><p>where c is a constant. Pr{l --logn hz I--nE for some constants c1 and ca. This proves Hn/ log n -+ l/h2 (pr.). To establish Theorem 2(i) we must extend the above to the almost sure convergence, which is discussed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Almost Sure Convergence</head><p>The estimate in <ref type="bibr">(3.12)</ref> does not yet allow us to use the Borel-Cantelli Lemma to prove the almost sure convergence.</p><p>But, the fact that H , is nondecreasing and the fact that logn is slowly varying function will allow to show the almost sure convergence (see also Remark 2(ii)). To do so we apply the trick suggested by Kesten and reported by Kingman in <ref type="bibr" target="#b23">[23]</ref> (cf. also <ref type="bibr" target="#b31">[31]</ref>). The idea is to replace n by s2' for some integers s and r . Note then for n = 92' the estimate (3.12) implies that M provided for s -+ 00. In a similar manner, we prove the liminf, and after some algebra we get This leads to </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The Shortest Feasible Path in a S u f i Tree</head><p>Now we concentrate on establishing the almost sure convergence for the shortest feasible path length sn in a suffix tree S,. Our proof resembles the derivation used by Pittel <ref type="bibr" target="#b31">[31]</ref> for independent tries (cf. also <ref type="bibr" target="#b18">[18]</ref>). We need some more notation.</p><p>We write (xr, wk) to denote the set of positions on which x; and Wk agree, that is, i E ( X ; , W k ) if the ith suffix X r agrees entirely with wk (i.e., on k positions). Moreover, let c(x, w) be the alignment between X and w, that is, the length of the longest common prefix of X and w. Clearly, C(X, wk) 5 k.</p><p>Finally, we define <ref type="bibr">(3.16)</ref> Note that according to our definition (2.6) we have pmin(k) N e-hlk for large k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Upper bound</head><p>Define Wmin as P(wmin) = pmin(k), SO w,in E wk. k t {s, &gt; k } . Then, up to the level k, the suffix tree S, can be modeled as a complete tree, that is, all nodes of the depth not higher than k have the maximum degree equal to V. This implies that for every word Wk E w k there must exist at least one suffix of X r whose prefix of length k agrees with Wk. Therefore, the set (Xr, W k ) is nonempty, i.e., [ ( X ? , W k ) ( 2 1. This is particularly true for the word w,in. Hence which holds for example for a(.) = O(n-1/2-6) for some 6 &gt; 0.</p><p>To finish the proof we need to translate (3.13) for every n. Fix s. Naturally, for every n we find such r that 92" 5 n 5 (s + 1)2' By the above and (3.13), together with the fact that logarithm is a slowly varying function, we have   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Lower Bound</head><p>The lower bound is more intricate. Fortunately, we can use the same trick as in the case of the lower bound for the height.</p><p>We partition Xr into m keys Y(1), ... , Y(m) separated by m gaps of size d (cf. Fig. <ref type="figure">4</ref>). We construct a trie 7 , from those m weakly dependent keys (for detailed construction see previous subsection). Let s , be the shortest feasible path in  For the almost sure convergence, we apply the same arguments as in the case of the height since s , is also a nondecreasing sequence. Note, however, that in this case we need only to re-consider the upper bound since the Borel-Cantelli Lemma can be directly applied to <ref type="bibr">(3.23)</ref>. That is, we set n = ~2~ and due to the the monotonicity property of s , , we finally prove hl  It is my pleasure to acknowledge numerous enlightning discussions that I have had with Professor Boris Pittel and Professor Luc Devroye on the subject of this paper and related topics. The author is particularly obliged to Professor Paul Shields for bringing to his attention the existence of what is called in this paper the right and the left domain asymptotics. Finally, comments of two conscientious referees led to the improved presentation and elimination of some slips in the first draft of this paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>0018-9448/93$03.00 0 1993 IEEE _ _ IEEE TRANSACTIONS ON I N F O R M O N THEORY, VOL. 39, NO. 5, SEPTEMBER 1993</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc>cf. PI, PI, [51, PI, [14l, [IS], [281, [40]). In particular, recently Chang and Lawler [ l l ] used suffix trees to design an algorithm that on average needs O((ITI/IPl)log [PI) steps to find all occurrences of the pattern P of length [PI in the text T of length 12' 1. 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1. 0 L</head><label>0</label><figDesc>Fig. 1. 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. A sample of data used in Example 2.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(1-a(d))Pr{d}Pr{B} 5 Pr{dB} 5 (1-a(d))Pr{d}Pr{B} where d E 3-", and B E 3E'd and a( .) is a function of d such that a(d) + 0 as d --t 00.Following Pittel<ref type="bibr" target="#b31">[31]</ref>, we define two new parameters of (2.5) {&amp;I, namely max{logP-l(XF), P ( X r ) &gt; 0} h l = lim n+w n n+m n = lim log(I/ min{P(Xy), P(XT) &gt; 0}) 7 (2.6)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1651</head><label></label><figDesc>behavior of E, is the one predicted by Wyner and Ziv. From their proof of the convergence in p_robability (cf.<ref type="bibr" target="#b40">[41]</ref> pp. 1253-1255), one concludes that Pr{ ILn/ lognl/hl &gt; E } = O ( l / m ) + P ( B , ) , where P(Bn) is the probability of "bad states" in the McMillan-Shannon theorem (cf. [SI). Assume now that P(B,) is summable, that is, CrZ1 P(Bn) &lt; m.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>IC} =n Pr{L,(m) &lt; IC}. m=l This typical depth depth D, is used in the analysis of the average complexity of the exact matching algorithms (cf. Example 1.3)). *It would be more natural to denote this depth of insertion as &amp; + I , but we try to keep our notation consistent with the one introduced in [41].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 3 .EXAMPLE 2 . 2 :0</head><label>322</label><figDesc>Fig. 3. Suffix tree built from the first four suffixes of x = 0 1 0 1 1 0 1 1 1 o...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>( 2 .</head><label>2</label><figDesc>13b), which is repeated below In the analysis of the height H,, we use the definition where C,,, is the self-alignment between subsequences X,and X y . It is easy to notice that the self-alignment really depends only on the difference d = ( j -i ( since {xk} is stationary. From (3.1) one concludes that the distribution of the height depends on the distribution of the self-alignments, and we express the latter by an appropriate probability on the string-ruler set wk. Let k A d = min{k, d} for given d 2 0 andk 2 0.Thenforany 15 i 5 nand 15 d 5 n-i we have 1$1+1-Pr{CZ,Z+d 2 k } = P(wkAd wd) (3.2) W k h d where ?iid is a prefix of W d such that l w ~~~' + + l ~d l = k + min{k,d}, and Lk/dJ is the integer part of k/d. Note that for d 2 k the right-hand side (RHS) of (3.2) becomes cwk P ( w i ) . Identity (3.2) is a simple consequence of the following fact on combinatorics of words: if 2 is the longest common prefix of two suffixes S, and S, of a single sequence { X k } such that 1 2 1 = k, then the string 2 can be represented as Z = w$i7d where d = 1 ji J .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>3 )</head><label>3</label><figDesc>We consider the above two terms separately. We first deal with the second sum, which in view of (3.2), becomes n d=k n where the inequality above is implied by the (weak) mixing condition (2.4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>[ 29 ], then c log n 1 Pr{Hn 2</head><label>2912</label><figDesc>), and the last inequality (C) follows from Wd c Wk. In the above, the constant c1 may change from line to line. Finally, putting everything together we have Let now k = (1 + E ) &amp; logn. Then, by (2.7) we have EP(wk)exp( -2 log = n-2(1+E)(1 + E ) -h2 logn} 5 -nE + 0 (3.5)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>4 .Fig. 4 .</head><label>44</label><figDesc>Fig. 4. Illustration for the construction of the suffix tree 7,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>(3.7)) we have where D = { ( i , j ) : 1 5 i 5 m , l 5 j 5 m and i # j } . We evaluate each term in (3.8) separately. Using the strong amixing condition, and arguing as in the case of the upper bound (cf. the first inequality after (3.3)) we immediately obtain for k = O(1ogn) d, (cf. Fig. 4), hence Case B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>Putting everything together, inequality (3.8)k = (1 -~) k logn with E P ( W k ) N n-2(1-E) n2(l-')(1 + a(d,)) ( m2(1a ( d n ) ) 2becomes for Now, setting nl-/m -+ 0 (e.g., m = Ln/ log nJ and d, = @(log n)), we finally obtain Pr{H, 5 (1 -E ) -log n) lower bound for H,, and hence by (3.6) also for H, in our original suffix tree S,. In summary, (3.5) and (3.11) lead to the following log n &gt; E } &lt; ~1 -+cza2(1ogn) -+ 0 (3.12) Hn 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>I (Xr, wmin) 12 1 implies that at least one depth is greater than k which further implies that there exists an index, say 1 I i 5 n, such that C(X,~,w,i,) = k. Then, by Boole's inequality Pr{s, &gt; k} 5 nPr{C(Xy,wmin) = k } = npmin(k).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>(3. 18 )</head><label>18</label><figDesc>The rest is easy. Let k = (1 + E ) &amp; log n. Then, using the definition of hl and (3.18) we finally obtain which proves the desired upper bound.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>7 ,.</head><label>7</label><figDesc>Then, due to the fact that s , sst s , , we have Pr{s, &lt; k} I Pr{s, &lt; k}.(3.20)    Now, we need only to investigate the reduced tree 7,. But the event {s,, &lt; k} implies that there exists a word Wk E Wr, such that all (longest) common prefixes between Wk and the keys Y (1), . . . , Y (m) are of lengths smaller than k. That is, This is the same as in Pittel<ref type="bibr" target="#b31">[31]</ref> since the condition (3.21) is naturally also true for independent tries.By the strong a-mixing condition (2.5), and the above we have Let now k = (1 -E ) $ logn and m = [n/ IognJ while d, = O(1ogn). Then, Pr{s, &lt; (1 -E ) -log n} 5 (l+a(log n ) ) , exp(-nE/'/log n).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>( 3 . 22 )</head><label>322</label><figDesc>To complete our derivation we recall the condition (2.9) which implies that (1 + a(log n ) ) , I cnp for some constants c and p. Then, (3.22) becomes 1 hl 1 Pr{s, &lt; (1 -E ) -logn} 5 cnPexp(-nE/'/logn).(3.23)    which completes the proof of the convergence in probability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>hl as needed for Theorem 2(ii).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>ACKNOWLEDGMENT</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>evaluated the height of regular tries with Markovian dependency, and they showed that the parameter h2 is a function of the largest eigenvalue 8 of the matrix Pp] = P o P which represents the Schur product of P (Le., studied in the graph theory and it is called the minimum cycle mean. Clearly, hl = minc{l(C)/(C(}, and Karp<ref type="bibr" target="#b21">[21]</ref> showed 0 Now, we are ready to present our results. Our main finding of this paper is given in the following theorem which is proved after the statement of Theorem 2.</figDesc><table><row><cell cols="4">elementwise product). More precisely, hz = (1/2) log 8-l.</cell></row><row><cell cols="4">With respect to hl we need a result from digraphs (cf.</cell></row><row><cell cols="4">Romanovski [34], Karp [21]). Consider a digraph on C with</cell></row><row><cell>how to compute it efficiently. weights equal to n-00 n logn hl Ln 1</cell><cell>n+w n logn Ln</cell><cell>h2 1</cell><cell>(2.8)</cell></row><row><cell cols="4">for all stationary ergodic sequences { X k } F -, provided that</cell></row><row><cell>for d + 00</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>-logpij where w,, wj E E. Define a cycle C = { w l , w2, ..., w,,w1} for some U 5 V such that w, E C, and let l ( C ) = -log(pwt,w,+l) (with w,+1 = w1) be the total weight of the cycle C. The quantity minc{.!(C)/w} is well Theorem I: Let the mixing condition (2.5) hold together with hl &lt; 00 and h2 &gt; 0. Then, lim inf ----(a.s.) lim sup -= -</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Fix s, and define nr = 2s222'. Note that En?/ logn, -+ l / h (a.s.) provided P(Bn) is summable. Ttprove that Ln/ log n converges (as.), we use two facts: (i) L, is a nondecreasing sequence (cf. Example 2.1 below); (ii) for every n we can choose such T that 2s222T 5 n 5 2(s+1)222p.Then,    </figDesc><table><row><cell>-Ln logn -r+w lim sup-&lt; lim sup-n-00 logn, -Lr</cell><cell></cell></row><row><cell>10g2("+1)222p + -~ 1 (8 + 1)2 log 2s222T h s2</cell><cell>(as.), (2.10)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>[31] and Szpankowski [38] who proved that Pr{H, 2 (1 -E ) &amp; logn} = 1 -O ( F ) . This together with (3.6) would</head><label></label><figDesc>then H , sst Hn, where Sst means stochastically In order to fulfill this plan we must solve several problems. First of all, we introduce a new trie with height H , such that (3.6) holds. We illustrate the idea in the case of the Bernoulli model. We partition the string XT into m complete the proof of the lower bound for the Bernoulli model. The general model is more intricate since the keys Y ( 1 ) ,</figDesc><table><row><cell>smaller, and hence (cf. [36])</cell><cell></cell></row><row><cell>Pr{Hn 2 k} 2 Pr{H, 2 k} for m 5 n,</cell><cell>(3.6)</cell></row><row><cell cols="2">with k = O(1ogn). We select m in such a way that the</cell></row><row><cell cols="2">probability of the RHS of the above will be easier to evaluate</cell></row><row><cell cols="2">than the original probability. In order to estimate Pr{H, 2 k}</cell></row><row><cell cols="2">we use the second moment method (cf. Chung and Erdos [lo]),</cell></row><row><cell>which states that for events A;</cell><cell></cell></row><row><cell cols="2">In our case, we set A ; j = { C;,j 2 k}, and hence Pr{ H , 2</cell></row><row><cell cols="2">Naturally, the first k symbols of these m substrings (keys), say Y ( 1) , Y (2), . . . , Y (m), are independent in the Bernoulli</cell></row><row><cell cols="2">model, and we can construct a trie from these m keys. Denote</cell></row><row><cell cols="2">such a tree as 7,. By the sample path comparison [36], we</cell></row><row><cell cols="2">can construct such a realization of 7, that its height H, is</cell></row><row><cell cols="2">smaller (in the sample path sense) than in our original suffix</cell></row><row><cell cols="2">tree S,, hence in particular (3.6) holds. The evaluation of</cell></row><row><cell cols="2">Pr{ H , 2 k} in 7 , is easy since independent tries 7 , were</cell></row><row><cell cols="2">studied very extensively in the last decade. In particular, the</cell></row><row><cell>reader is referred to Pittel</cell><cell></cell></row></table><note><p>k} = Pr{uTj=, Ai,j}. Our aim is to prove that for k = ( 1 -E ) &amp; log n the probability Pr{H, 2 k } tends to 1, hence also by (3.6) we have Pr{Hn 2 (1 -E ) &amp; logn} + 1. = [n/k] consecutive substrings of length k = O(1ogn).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>The gaps g1 and 92 overlap.</head><label></label><figDesc>Let g be the overlapping string of g1 and 92, that is,</figDesc><table><row><cell cols="2">wi), and (B) is a consequence of the following inequal-</cell><cell></cell></row><row><cell>ity, which can be found in ~~~l and Ost</cell><cell>i</cell><cell>~</cell></row></table><note><p><p>g1 = g i g and g2 = gg;. We consider two subcases:</p>Neither g i nor g i is null. Then, it is easy to see that we can reduce this case to the previous case A with g1 and g2 replaced by g i and g i . One of the strings g i and g; is empty, that is, two keys out of the following four strings Y(i), Y(j), Y(s) and Y(t) are the same, say the ith one and the tth one. Then, where (A) follows directly from (3.9) (by setting W k =</p></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The Design and Analysis of Computer Algorithms</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Aho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Algorithms for &quot;finding patterns in strings</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Aho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. van Leeuwen</title>
		<imprint>
			<biblScope unit="page" from="255" to="300" />
			<date type="published" when="1990">1990</date>
			<publisher>The MIT Press</publisher>
			<pubPlace>Ed. Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
	<note>A: Algorithms and Complexity</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A diffusion limit for a class of randomgrowing binary trees</title>
		<author>
			<persName><forename type="first">D</forename><surname>Aldous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shields</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Probab. Th. Rel. Fields</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="509" to="542" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The myriad virtues of suffix trees</title>
		<author>
			<persName><forename type="first">A</forename><surname>Apostolico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Combinatorial Algorithms on Words</title>
		<imprint>
			<publisher>New York Springer-Verlag</publisher>
			<date type="published" when="1985">1985</date>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Self-alignments in Words and Their Applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Apostolico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Szpankowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ofdlgorithms</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">446467</biblScope>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Erdos-Rknyi strong law for pattern matching with given proportion of mismatches</title>
		<author>
			<persName><forename type="first">R</forename><surname>Arratia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Waterman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Probab</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">New asymptotic bounds and improvements on the Lemoel-Ziv data comoression algorithm</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform</title>
		<imprint>
			<biblScope unit="page" from="1152" to="1169" />
			<date type="published" when="1974">1974. 1989</date>
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Reading, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Theory</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="721" to="734" />
			<date type="published" when="1965">1991. 1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Billingsley</surname></persName>
		</author>
		<title level="m">Ergodic Theory and Information. New YorkWiley</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Average size of suffix trees and DAWGS</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ehrenfeucht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Appl. Mathemat</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="37" to="45" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the application of the Borel-Cantelli lemma</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-P.</forename><surname>Erdos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Amer. Math. Soc</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="179" to="186" />
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Approximate String Matching in Sublinear Expected Time</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lawler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 1990 FOCS</title>
		<meeting>of 1990 FOCS</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="116" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Elements of Information Theory</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<pubPlace>New York Wiley</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Information Theory: Coding Theorem for Discrete Memoryless Systems</title>
		<author>
			<persName><forename type="first">I</forename><surname>Csiszir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Korner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
			<publisher>New YorkAcademic</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A note of the height of sui% trees</title>
		<author>
			<persName><forename type="first">L</forename><surname>Devroye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Szpankowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SlAM J. Comput</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="48" to="53" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Estimating the information content of symbol sequences and efficient codes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Grassberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="669" to="675" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">String overlaps, pattem matching, and nontransitive games</title>
		<author>
			<persName><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Odlyzko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Combinatorial Theory, Series A</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="183" to="208" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Analysis of digital tries with markovian dependency</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jacquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Szpankowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1470" to="1475" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Autocorrelation on words and its applications. analysis of suffix tree by String-Ruler approach</title>
	</analytic>
	<monogr>
		<title level="j">J. Combinatorial Theory. Ser. A</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the Lempel-Ziv parsing algorithm and its digital tree representation</title>
	</analytic>
	<monogr>
		<title level="j">INRIA Rapport de Recherche</title>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Counts of Long Aligned Word Matches Among Random Letter Sequences</title>
		<author>
			<persName><forename type="first">S</forename><surname>Karlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Appl. Prob</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="293" to="351" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A characterization of the minimum cycle mean in a digraph</title>
		<author>
			<persName><forename type="first">R</forename><surname>Karp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Mathemat</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="309" to="311" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sample converses in source coding theory</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Kieffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="263" to="268" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F C</forename><surname>Kingman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ecole d&apos;Etk de Probabilitks de Saint-Flour V-1975</title>
		<title level="s">Lecture Notes in Mathematics</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1976">1976</date>
			<biblScope unit="volume">539</biblScope>
		</imprint>
	</monogr>
	<note>Subadditive Processes</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On the variance of the external path in a symmetric digital trie</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kirschenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Prodinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Szpankowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Discrete Appl. Mathemat.</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="129" to="143" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Digital search trees again revisited: The internal path length perspective</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kirschenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Prodinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Szpankowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Comput</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">D</forename><surname>Knuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">The Art of Computer Programming. Sorting and Searching</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">On the complexity of finite sequences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lempel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ziv</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A space economical suflix tree construction algo</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Mccreight</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Hardy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Littlewood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
		<imprint>
			<publisher>P6lya, Inequalities. Cambridge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Entropy and data compression schemes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ornstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Weiss</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Asymptotic growth of a class of random trees</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pittel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Paths in a random digital tree: Limiting distributions</title>
		<imprint>
			<publisher>Adv</publisher>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Linear algorithm for data compression</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rodeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Even</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Optimization of stationary control of a discrete</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Romwovski</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Entropy and prefixes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Shields</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Inform Theory</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="52" to="62" />
			<date type="published" when="1952">1973. 1976. 1976. 1952. 1985. 1986. 1981. 1967. 1992</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Reading, MA</pubPlace>
		</imprint>
	</monogr>
	<note>Cybernet.</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Comparison MethodF for Queues and Other Stochastic Models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Stoyan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>New YorkWiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Some results on V-ary asymmetric tries</title>
		<author>
			<persName><forename type="first">W</forename><surname>Szpankowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Algor i t h</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="224" to="244" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On the height of digital trees and related problems</title>
		<author>
			<persName><forename type="first">W</forename><surname>Szpankowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithmica</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="256" to="277" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A generalized suffix tree and its (un)expected asymptotic behaviors</title>
	</analytic>
	<monogr>
		<title level="j">SL4M J. Comput</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Linear pattern matching algorithms</title>
		<author>
			<persName><forename type="first">P</forename><surname>Weiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th Annu. Symp. Switching and Automata Theory</title>
		<meeting>14th Annu. Symp. Switching and Automata Theory</meeting>
		<imprint>
			<date type="published" when="1973">1973</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Some asymptotic properties of the entropy of a stationary ergodic data source with applications to data compression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wyner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ziv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1250" to="1258" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fixed data base version of the lempel-ziv data compression algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wyner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ziv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="878" to="880" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A universal algorithm for sequential data compression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ziv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lempel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="337" to="343" />
			<date type="published" when="1977-05">May 1977</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
