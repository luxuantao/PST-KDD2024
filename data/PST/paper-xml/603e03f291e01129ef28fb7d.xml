<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transformer in Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kai</forename><surname>Han</surname></persName>
							<email>hankai@ios.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">State Key Lab of Computer Science</orgName>
								<orgName type="institution">ISCAS &amp; UCAS</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">An</forename><surname>Xiao</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Enhua</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">State Key Lab of Computer Science</orgName>
								<orgName type="institution">ISCAS &amp; UCAS</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Macau</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
							<email>yunhe.wang@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Transformer in Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformer is a new kind of neural architecture which encodes the input data as powerful features via the attention mechanism. Basically, the visual transformers first divide the input images into several local patches and then calculate both representations and their relationship. Since natural images are of high complexity with abundant detail and color information, the granularity of the patch dividing is not fine enough for excavating features of objects in different scales and locations. In this paper, we point out that the attention inside these local patches are also essential for building visual transformers with high performance and we explore a new architecture, namely, Transformer iN Transformer (TNT). Specifically, we regard the local patches (e.g., 16×16) as "visual sentences" and present to further divide them into smaller patches (e.g., 4×4) as "visual words". The attention of each word will be calculated with other words in the given visual sentence with negligible computational costs. Features of both words and sentences will be aggregated to enhance the representation ability. Experiments on several benchmarks demonstrate the effectiveness of the proposed TNT architecture, e.g., we achieve an 81.5% top-1 accuracy on the ImageNet, which is about 1.7% higher than that of the state-of-the-art visual transformer with similar computational cost.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the past decade, the mainstream deep neural architectures used in the computer vision (CV) are mainly established on convolutional neural networks (CNNs) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b10">11]</ref>. Differently, transformer is a type of neural network mainly based on self-attention mechanism <ref type="bibr" target="#b34">[35]</ref>, which can provide the relationships between different features. Transformer is widely used in the field of natural language processing (NLP), e.g., the famous BERT <ref type="bibr" target="#b7">[8]</ref> and GPT-3 <ref type="bibr" target="#b1">[2]</ref> models. The power of these transformer models inspires the whole community to investigate the use of transformer for visual tasks.</p><p>To utilize the transformer architectures for conducting visual tasks, a number of researchers have explored for representing the sequence information from different data. For example, Wang et al. explore self-attention mechanism in non-local networks <ref type="bibr" target="#b36">[37]</ref> for capturing long-range dependencies in video and image recognition. Carion et al. present DETR <ref type="bibr" target="#b2">[3]</ref>, which treats object detection as a direct set prediction problem and solve it using a transformer encoder-decoder architecture. Chen et al. propose the iGPT <ref type="bibr" target="#b4">[5]</ref>, which is the pioneering work applying pure transformer model (i.e., without convolution) on image recognition by self-supervised pre-training. Different from the data in NLP tasks, there exists a semantic gap between input images and the ground-truth labels in CV tasks. To this end, Dosovitskiy et al. develop the ViT <ref type="bibr" target="#b8">[9]</ref>, which paves the way for transferring the success of transformer based NLP models. Concretely, ViT divides the given image into several local patches as a visual sequence. Then, the attention can be naturally calculated between any two image patches for generating effective feature representations for the recognition task. Subsequently, Touvron et al. explore the data-efficient training and distillation to enhance the performance of ViT on the ImageNet benchmark and obtain an about 81.8% ImageNet top-1 accuracy, which is comparable to that of the state-of-the-art convolutional networks. Chen et al. further treat the image processing tasks (e.g., denosing and super-resolution) as a series of translations and develop the IPT model for handling multiple low-level computer vision problems <ref type="bibr" target="#b3">[4]</ref>. Nowadays, transformer architectures have been used in a growing number of computer vision tasks <ref type="bibr" target="#b9">[10]</ref> such as image recognition <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b28">29]</ref>, object detection <ref type="bibr" target="#b45">[46]</ref>, and segmentation <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>Although the aforementioned visual transformers have made great efforts to boost the models' performances, most of existing works follow the conventional representation scheme used in ViT, i.e., dividing the input images into patches. Such a exquisite paradigm can effectively capture the visual sequential information and estimate the attention between different image patches. However, the diversity of natural images in modern benchmarks is very high, e.g., there are over 120 M images with 1000 different categories in the ImageNet dataset <ref type="bibr" target="#b25">[26]</ref>. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, representing the given image into local patches can help us to find the relationship and similarity between them. However, there are also some sub-patches inside them with high similarity. Therefore, we are motivated to explore a more exquisite visual image dividing method for generating visual sequences and improve the performance.</p><p>In this paper, we propose a novel Transformer-iN-Transformer (TNT) architecture for visual recognition as shown in Figure <ref type="figure" target="#fig_0">1</ref>. To enhance the feature representation ability of visual transformers, we first divide the input images into several patches as "visual sentences" and then further divide them into sub-patches as "visual words". Besides the conventional transformer blocks for extracting features and attentions of visual sentences, we further embed a sub-transformer into the architecture for excavating the features and details of smaller visual words. Specifically, features and attentions between visual words in each visual sentence are calculated independently using a shared network so that the increased amount of parameters and FLOPs (floating-point operations) is negligible. Then, features of words will be aggregated into the corresponding visual sentence. The class token is also used for the subsequent visual recognition task via a fully-connected head. Through the proposed TNT model, we can extract visual information with fine granularity and provide features with more details. We then conduct a series of experiments on the ImageNet benchmark and downstream tasks to demonstrate its superiority and thoroughly analyze the impact of the size for dividing visual words. The results show that our TNT can achieve better accuracy and FLOPs trade-off over the state-of-the-art transformer networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>In this section, we describe the proposed transformer-in-transformer architecture and analyze the computation and parameter complexity in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminaries</head><p>We first briefly describe the basic components in transformer <ref type="bibr" target="#b34">[35]</ref>, including MSA (Multi-head Self-Attention), MLP (Multi-Layer Perceptron) and LN (Layer Normalization).</p><p>MSA. In the self-attention module, the inputs X ∈ R n×d are linearly transformed to three parts, i.e., queries Q ∈ R n×d k , keys K ∈ R n×d k and values V ∈ R n×dv where n is the sequence length, d, d k , d v are the dimensions of inputs, queries (keys) and values, respectively. The scaled dot-product attention is applied on Q, K, V :</p><formula xml:id="formula_0">Attention(Q, K, V ) = softmax( QK T √ d k )V.<label>(1)</label></formula><p>Finally, a linear layer is used to produce the output. Multi-head self-attention splits the queries, keys and values to h parts and perform the attention function in parallel, and then the output values of each head are concatenated and linearly projected to form the final output.</p><p>MLP. The MLP is applied between self-attention layers for feature transformation and non-linearity:</p><formula xml:id="formula_1">MLP(X) = FC(σ(FC(X))), FC(X) = XW + b,<label>(2)</label></formula><p>where W and b are the weight and bias term of fully-connected layer respectively, and σ(•) is the activation function such as GELU <ref type="bibr" target="#b12">[13]</ref>.</p><p>LN. Layer normalization <ref type="bibr" target="#b0">[1]</ref> is a key part in transformer for stable training and faster convergence. LN is applied over each sample x ∈ R d as follows:</p><formula xml:id="formula_2">LN(x) = x − µ δ • γ + β<label>(3)</label></formula><p>where µ ∈ R, δ ∈ R are the mean and standard deviation of the feature respectively, • is the element-wise dot, and γ ∈ R d , β ∈ R d are learnable affine transform parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Transformer in Transformer</head><p>Given a 2D image, we uniformly split it into n patches</p><formula xml:id="formula_3">X = [X 1 , X 2 , • • • , X n ] ∈ R n×p×p×3</formula><p>, where (p, p) is the resolution of each image patch. ViT <ref type="bibr" target="#b8">[9]</ref> just utilizes a standard transformer to process the sequence of patches which corrupts the local structure of a patch, as shown in Fig. <ref type="figure" target="#fig_2">1(a)</ref>. Instead, we propose Transformer-iN-Transformer (TNT) architecture to learn both global and local information in an image. In TNT, we view the patches as visual sentences that represent the image. Each patch is further divided into m sub-patches, i.e., a visual sentence is composed of a sequence of visual words:</p><formula xml:id="formula_4">X i → [x i,1 , x i,2 , • • • , x i,m ],<label>(4)</label></formula><p>where x i,j ∈ R s×s×3 is the j-th visual word of the i-th visual sentence, (s, s) is the spatial size of sub-patches, and j = 1, 2, • • • , m. With a linear projection, we transform the visual words into a sequence of word embeddings:</p><formula xml:id="formula_5">Y i = [y i,1 , y i,2 , • • • , y i,m ], y i,j = FC(Vec(x i,j )),<label>(5)</label></formula><p>where y i,j ∈ R c is the j-th word embedding, c is the dimension of word embedding, and Vec(•) is the vectorization operation.</p><p>In TNT, we have two data flows in which one flow operates across the visual sentences and the other processes the visual words inside each sentence. For the word embeddings, we utilize a transformer block to explore the relation between visual words:</p><formula xml:id="formula_6">Y i l = Y i l−1 + MSA(LN(Y i l−1 )),<label>(6)</label></formula><formula xml:id="formula_7">Y i l = Y i l + MLP(LN(Y i l )).<label>(7)</label></formula><p>where l = 1, 2, • • • , L is the index of the l-th block, and L is the total number of stacked blocks. The input of the first block Y i 0 is just Y i in Eq. 5. All word embeddings in the image after transformation are</p><formula xml:id="formula_8">Y l = [Y 1 l , Y 2 l , • • • , Y n l ]</formula><p>. This can be viewed as an inner transformer block, denoted as T in . This process builds the relationships among visual words by computing interactions between any two visual words. For example, in a patch of human face, a word corresponding to the eye is more related to other words of eyes while interacts less with forehead part.</p><p>For the sentence level, we create the sentence embedding memories to store the sequence of sentencelevel representations:</p><formula xml:id="formula_9">Z 0 = [Z class , Z 1 0 , Z 2 0 , • • • , Z n 0 ] ∈ R (n+1)×d</formula><p>where Z class is the class token similar to ViT <ref type="bibr" target="#b8">[9]</ref>, and all of them are initialized as zero. In each layer, the sequence of word embeddings are transformed into the domain of sentence embedding by linear projection and added into the sentence embedding:</p><formula xml:id="formula_10">Z i l−1 = Z i l−1 + FC(Vec(Y i l )),<label>(8)</label></formula><p>where Z i l−1 ∈ R d and the fully-connected layer FC makes the dimension match for addition. With the above addition operation, the representation of sentence embedding is augmented by the word-level features. We use the standard transformer block for transforming the sentence embeddings:</p><formula xml:id="formula_11">Z l = Z l−1 + MSA(LN(Z l−1 )),<label>(9)</label></formula><formula xml:id="formula_12">Z l = Z l + MLP(LN(Z l )).<label>(10)</label></formula><p>This outer transformer block T out is used for modeling relationships among sentence embeddings.</p><p>In summary, the inputs and outputs of the TNT block include the visual word embeddings and sentence embeddings as shown in Fig. <ref type="figure" target="#fig_0">1</ref>(b), so the TNT can be formulated as</p><formula xml:id="formula_13">Y l , Z l = TNT(Y l−1 , Z l−1 ).<label>(11)</label></formula><p>In our TNT block, the inner transformer block is used to model the relationship between visual words for local feature extraction, and the outer transformer block captures the intrinsic information from the sequence of sentences. By stacking the TNT blocks for L times, we build the transformerin-transformer network. Finally, the classification token serves as the image representation and a fully-connected layer is applied for classification.</p><p>Position encoding. Spatial information is an important factor in image recognition. For sentence embeddings and word embeddings, we both add the corresponding position encodings to retain spatial information as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. The standard learnable 1D position encodings are utilized here. Specifically, each sentence is assigned with a position encodings:</p><formula xml:id="formula_14">Z 0 ← Z 0 + E sentence ,<label>(12)</label></formula><p>where E sentence ∈ R (n+1)×d are the sentence position encodings. As for the visual words in a sentence, a word position encoding is added to each word embedding:</p><formula xml:id="formula_15">Y i 0 ← Y i 0 + E word , i = 1, 2, • • • , n<label>(13)</label></formula><p>where E word ∈ R m×c are the word position encodings which are shared across sentences. In this way, sentence position encoding can maintain the global spatial information, while word position encoding is used for preserving the local relative position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Complexity Analysis</head><p>A standard transformer block includes two parts, i.e., the multi-head self-attention and multi-layer perceptron. The FLOPs of MSA are 2nd(</p><formula xml:id="formula_16">d k + d v ) + n 2 (d k + d v )</formula><p>, and the FLOPs of MLP are 2nd v rd v where r is the dimension expansion ratio of hidden layer in MLP. Overall, the FLOPs of a standard transformer block are</p><formula xml:id="formula_17">FLOPs T = 2nd(d k + d v ) + n 2 (d k + d v ) + 2nddr. (<label>14</label></formula><formula xml:id="formula_18">)</formula><p>Since r is usually set as 4, and the dimensions of input, key (query) and value are usually set as the same, the FLOPs calculation can be simplified as</p><formula xml:id="formula_19">FLOPs T = 2nd(6d + n).<label>(15)</label></formula><p>The number of parameters can be obtained as</p><formula xml:id="formula_20">Params T = 12dd.<label>(16)</label></formula><p>Our TNT block consists of three parts: an inner transformer block T in , an outer transformer block T out and a linear layer. The computation complexity of T in and T out are 2nmc(6c + m) and 2nd(6d + n) respectively. The linear layer has FLOPs of nmcd. In total, the FLOPs of TNT block are</p><formula xml:id="formula_21">FLOPs T N T = 2nmc(6c + m) + nmcd + 2nd(6d + n).<label>(17)</label></formula><p>Similarly, the parameter complexity of TNT block is calculated as</p><formula xml:id="formula_22">Params T N T = 12cc + mcd + 12dd. (<label>18</label></formula><formula xml:id="formula_23">)</formula><p>Although we add two more components in our TNT block, the increase of FLOPs is small since c d and O(m) ≈ O(n) in practice. For example, in the DeiT-S configuration, we have d = 384 and n = 196. We set c = 24 and m = 16 in our structure of TNT-S correspondingly. From Eq. 15 and Eq. 17, we can obtain that FLOPs T = 376M and FLOPs T N T = 429M . The FLOPs ratio of TNT block over standard transformer block is about 1.14×. Similarly, the parameters ratio is about 1.08×. With a small increase of computation and memory cost, our TNT block can efficiently model the local structure information and achieve a much better trade-off between accuracy and complexity as demonstrated in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Network Architecture</head><p>We build our TNT architectures by following the basic configuration of ViT <ref type="bibr" target="#b8">[9]</ref> and DeiT <ref type="bibr" target="#b30">[31]</ref>. The patch size is set as 16×16. The number of sub-patches is set as m = 4 • 4 = 16 by default. Other size values are evaluated in the ablation studies. As shown in Table <ref type="table" target="#tab_0">1</ref>, there are three variants of TNT networks with different model sizes, namely, TNT-Ti, TNT-S and TNT-B. They consist of 6.1M, 23.8M and 65.6M parameters respectively. The corresponding FLOPs for processing a 224×224 image are 1.4B, 5.2B and 14.1B respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we conduct extensive experiments on visual benchmarks to evaluate the effectiveness of the proposed TNT architecture. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and Experimental Settings</head><p>Datasets. ImageNet ILSVRC 2012 <ref type="bibr" target="#b25">[26]</ref> is an image classification benchmark consisting of 1.2M training images belonging to 1000 classes, and 50K validation images with 50 images per class. We adopt the same data augmentation strategy as that in DeiT <ref type="bibr" target="#b30">[31]</ref> including random crop, random clip, Rand-Augment <ref type="bibr" target="#b6">[7]</ref>, Random Erasing <ref type="bibr" target="#b43">[44]</ref>, Mixup <ref type="bibr" target="#b41">[42]</ref> and CutMix <ref type="bibr" target="#b40">[41]</ref>. For the license of ImageNet dataset, please refer to http://www.image-net.org/download.</p><p>In addition to ImageNet, we also test on the downstream tasks with transfer learning to evaluate the generalization ability of TNT. The details of used visual datasets are listed in Table <ref type="table" target="#tab_1">2</ref>. The data augmentation strategy of image classification datasets are the same as that of ImageNet. For COCO and ADE20K, the data augmentation strategy follows that in PVT <ref type="bibr" target="#b35">[36]</ref>. For the licenses of these datasets, please refer to the original papers.</p><p>Implementation Details. We utilize the training strategy provided in DeiT <ref type="bibr" target="#b30">[31]</ref>. The main advanced technologies apart from common settings <ref type="bibr" target="#b11">[12]</ref> include AdamW <ref type="bibr" target="#b19">[20]</ref>, label smoothing <ref type="bibr" target="#b26">[27]</ref>, DropPath <ref type="bibr" target="#b17">[18]</ref>, and repeated augmentation <ref type="bibr" target="#b13">[14]</ref>. We list the hyper-parameters in Table <ref type="table" target="#tab_2">3</ref> for better understanding. All the models are implemented with PyTorch <ref type="bibr" target="#b23">[24]</ref> and MindSpore <ref type="bibr" target="#b14">[15]</ref> and trained on NVIDIA Tesla V100 GPUs. The potential negative societal impacts may include energy consumption and carbon dioxide emissions of GPU computation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">TNT on ImageNet</head><p>We train our TNT models with the same training settings as that of DeiT <ref type="bibr" target="#b30">[31]</ref>. The recent transformerbased models like ViT <ref type="bibr" target="#b8">[9]</ref> and DeiT <ref type="bibr" target="#b30">[31]</ref> are compared. To have a better understanding of current progress of visual transformers, we also include the representative CNN-based models such as ResNet <ref type="bibr" target="#b11">[12]</ref>, RegNet <ref type="bibr" target="#b24">[25]</ref> and EfficientNet <ref type="bibr" target="#b27">[28]</ref>. The results are shown in Table <ref type="table" target="#tab_3">4</ref>. We can see that our transformer-based model, i.e., TNT outperforms all other visual transformer models. In particular, TNT-S achieves 81.5% top-1 accuracy which is 1.7% higher than the baseline model DeiT-S, indicating the benefit of the introduced TNT framework to preserve local structure information inside the patch. Compared to CNNs, TNT can outperform the widely-used ResNet and RegNet. Note that all the transformer-based models are still inferior to EfficientNet which utilizes special depth-wise convolutions, so it is yet a challenge of how to beat EfficientNet using pure transformer.  We also plot the accuracy-parameters and accuracy-FLOPs line charts in Fig. <ref type="figure">2</ref> to have an intuitive comparison of these models. Our TNT models consistently outperform other transformer-based models by a significant margin.</p><p>Inference speed. Deployment of transformer models on devices is important for practical applications, so we test the inference speed of our TNT model. Following <ref type="bibr" target="#b30">[31]</ref>, the throughput is measured on an NVIDIA V100 GPU and PyTorch, with 224×224 input size. Since the resolution and content inside the patch is smaller than that of the whole image, we may need fewer blocks to learn its representation. Thus, we can reduce the used TNT blocks and replace some with vanilla transformer blocks. From the results in Table <ref type="table" target="#tab_4">5</ref>, we can see that our TNT is more efficient than DeiT and PVT by achieving higher accuracy with similar inference speed.  Effect of position encodings. Position information is important for image recognition. In TNT structure, sentence position encoding is for maintaining global spatial information, and word position encoding is used to preserve locally relative position. We verify their effect by removing them separately. As shown in Table <ref type="table" target="#tab_5">6</ref>, we can see that TNT-S with both patch position encoding and word position encoding performs the best by achieving 81.5% top-1 accuracy. Removing sentence/word position encoding results in a 0.8%/0.7% accuracy drop respectively, and removing all position encodings heavily decrease the accuracy by 1.0%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of heads.</head><p>The effect of #heads in standard transformer has been investigated in multiple works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b34">35]</ref> and a head width of 64 is recommended for visual tasks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b30">31]</ref>. We adopt the head width of 64 in outer transformer block in our model. The number of heads in inner transformer block is another hyper-parameter for investigation. We evaluate the effect of #heads in inner transformer block (Table <ref type="table" target="#tab_6">7</ref>). We can see that a proper number of heads (e.g., 2 or 4) achieve the best performance. Number of visual words. In TNT, the input image is split into a number of 16×16 patches and each patch is further split into m sub-patches (visual words) of size (s, s) for computational efficiency. Here we test the effect of hyper-parameter m on TNT-S architecture. When we change m, the embedding dimension c also changes correspondingly to control the FLOPs. As shown in Table <ref type="table" target="#tab_7">8</ref>, we can see that the value of m has slight influence on the performance, and we use m = 16 by default for its efficiency, unless stated otherwise.</p><p>TNT DeiT  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Visualization</head><p>Visualization of Feature Maps. We visualize the learned features of DeiT and TNT to further understand the effect of the proposed method. For better visualization, the input image is resized to 1024×1024. The feature maps are formed by reshaping the patch embeddings according to their spatial positions. The feature maps in the 1-st, 6-th and 12-th blocks are shown in Fig. <ref type="figure" target="#fig_3">3</ref>(a) where 12 feature maps are randomly sampled for these blocks each. In TNT, the local information are better preserved compared to DeiT. We also visualize all the 384 feature maps in the 12-th block using t-SNE <ref type="bibr" target="#b32">[33]</ref> (Fig. <ref type="figure" target="#fig_3">3(b)</ref>). We can see that the features of TNT are more diverse and contain richer information than those of DeiT. These benefits owe to the introduction of inner transformer block for modeling local features.</p><p>In addition to the patch-level features, we also visualize the pixel-level embeddings of TNT in Fig. <ref type="figure" target="#fig_4">4</ref>.</p><p>For each patch, we reshape the word embeddings according to their spatial positions to form the feature maps and then average these feature maps by the channel dimension. The averaged feature maps corresponding to the 14×14 patches are shown in Fig. <ref type="figure" target="#fig_4">4</ref>. We can see that the local information is well preserved in the shallow layers, and the representations become more abstract gradually as the network goes deeper.</p><p>Visualization of Attention Maps. There are two self-attention layers in our TNT block, i.e., an inner self-attention and an outer self-attention for modeling relationship among visual words and sentences respectively. We show the attention maps of different queries in the inner transformer in Figure <ref type="figure" target="#fig_5">5</ref>. For a given query visual word, the attention values of visual words with similar appearance are higher, indicating their features will be interacted more relevantly with the query. These interactions are missed in ViT and DeiT, etc. The attention maps in the outer transformer can be found in the supplemental material.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Transfer Learning</head><p>To demonstrate the strong generalization ability of TNT, we transfer TNT-S, TNT-B models trained on ImageNet to the downstream tasks.</p><p>Pure Transformer Image Classification. Following DeiT <ref type="bibr" target="#b30">[31]</ref>, we evaluate our models on 4 image classification datasets with training set size ranging from 2,040 to 50,000 images. These datasets include superordinate-level object classification (CIFAR-10 [16], CIFAR-100 <ref type="bibr" target="#b15">[16]</ref>) and fine-grained object classification (Oxford-IIIT Pets <ref type="bibr" target="#b22">[23]</ref>, Oxford 102 Flowers <ref type="bibr" target="#b21">[22]</ref> and iNaturalist 2019 <ref type="bibr" target="#b33">[34]</ref>), shown in Table <ref type="table" target="#tab_1">2</ref>. All models are fine-tuned with an image resolution of 384×384. We adopt the same training settings as those at the pre-training stage by preserving all data augmentation strategies. In order to fine-tune in a different resolution, we also interpolate the position embeddings of new patches. For CIFAR-10 and CIFAR-100, we fine-tune the models for 64 epochs, and for fine-grained datasets, we fine-tune the models for 300 epochs. Table <ref type="table" target="#tab_8">9</ref> compares the transfer learning results of TNT to those of ViT, DeiT and other convolutional networks. We find that TNT outperforms DeiT in most datasets with less parameters, which shows the superiority of modeling pixel-level relations to get better feature representation. Pure Transformer Object Detection. We construct a pure transformer object detection pipeline by combining our TNT and DETR <ref type="bibr" target="#b2">[3]</ref>. For fair comparison, we adopt the training and testing settings in PVT <ref type="bibr" target="#b35">[36]</ref> and add a 2×2 average pooling to make the output size of TNT backbone the same as that of PVT and ResNet. All the compared models are trained using AdamW <ref type="bibr" target="#b19">[20]</ref> with batch size of 16 for 50 epochs. The training images are randomly resized to have a shorter side in the range of [640,800] and a longer side within 1333 pixels. For testing, the shorter side is set as 800 pixels.</p><p>The results on COCO val2017 are shown in Table <ref type="table" target="#tab_9">10</ref>. Under the same setting, DETR with TNT-S backbone outperforms the representative pure transformer detector DETR+PVT-Small by 3.5 AP with similar parameters. Pure Transformer Semantic Segmentation.</p><p>We adopt the segmentation framework of Trans2Seg <ref type="bibr" target="#b37">[38]</ref> to build the pure transformer semantic segmentation based on TNT backbone. We follow the training and testing configuration in PVT <ref type="bibr" target="#b35">[36]</ref> for fair comparison. All the compared models are trained by AdamW optimizer with initial learning rate of 1e-4 and polynomial decay schedule. We apply random resize and crop of 512×512 during training. The ADE20K results with single scale testing are shown in Table <ref type="table" target="#tab_10">11</ref>. With similar parameters, Trans2Seg with TNT-S backbone achieves 43.6% mIoU, which is 1.0% higher than that of PVT-small backbone and 2.8% higher than that of DeiT-S backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose a novel Transformer-iN-Transformer (TNT) network architecture for visual recognition. In particular, we uniformly split the image into a sequence of patches (visual sentences) and view each patch as a sequence of sub-patches (visual words). We introduce a TNT block in which an outer transformer block is utilized for processing the sentence embeddings and an inner transformer block is used to model the relation among word embeddings. The information of visual word embeddings is added to the visual sentence embedding after the projection of a linear layer. We build our TNT architecture by stacking the TNT blocks. Compared to the conventional vision transformers (ViT) which corrupts the local structure of the patch, our TNT can better preserve and model the local information for visual recognition. Extensive experiments on ImageNet and downstream tasks have demonstrate the effectiveness of the proposed TNT architecture.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the proposed Transformer-iN-Transformer (TNT) framework. The inner transformer block is shared in the same layer. The word position encodings are shared across visual sentences.</figDesc><graphic url="image-7.png" coords="2,109.71,187.84,60.82,60.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( a )Figure 2 :</head><label>a2</label><figDesc>Figure 2: Performance comparison of the representative visual backbone networks on ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( a )</head><label>a</label><figDesc>Feature maps in Block-1/6/12. (b) T-SNE of Block-12.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visualization of the features of DeiT-S and TNT-S.</figDesc><graphic url="image-8.png" coords="8,109.99,266.70,273.24,132.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Visualization of the averaged word embeddings of TNT-S.</figDesc><graphic url="image-9.png" coords="9,136.45,67.02,336.61,106.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Attention maps of different queries in the inner transformer. Red cross symbol denotes the query location.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Variants of our TNT architecture. 'Ti' means tiny, 'S' means small, and 'B' means base. The FLOPs are calculated for images at resolution 224×224.</figDesc><table><row><cell>Model</cell><cell>Depth</cell><cell cols="3">Inner transformer dim c #heads MLP r</cell><cell cols="3">Outer transformer dim d #heads MLP r</cell><cell cols="2">Params FLOPs (M) (B)</cell></row><row><cell>TNT-Ti</cell><cell>12</cell><cell>12</cell><cell>2</cell><cell>4</cell><cell>192</cell><cell>3</cell><cell>4</cell><cell>6.1</cell><cell>1.4</cell></row><row><cell>TNT-S</cell><cell>12</cell><cell>24</cell><cell>4</cell><cell>4</cell><cell>384</cell><cell>6</cell><cell>4</cell><cell>23.8</cell><cell>5.2</cell></row><row><cell>TNT-B</cell><cell>12</cell><cell>40</cell><cell>4</cell><cell>4</cell><cell>640</cell><cell>10</cell><cell>4</cell><cell>65.6</cell><cell>14.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Details of used visual datasets.</figDesc><table><row><cell>Dataset</cell><cell>Type</cell><cell cols="3">Train size Val size #Classes</cell></row><row><cell>ImageNet [26]</cell><cell>Pretrain</cell><cell>1,281,167</cell><cell>50,000</cell><cell>1,000</cell></row><row><cell>Oxford 102 Flowers [22]</cell><cell></cell><cell>2,040</cell><cell>6,149</cell><cell>102</cell></row><row><cell>Oxford-IIIT Pets [23]</cell><cell></cell><cell>3,680</cell><cell>3,669</cell><cell>37</cell></row><row><cell>iNaturalist 2019 [34]</cell><cell>Classification</cell><cell>265,240</cell><cell>3,003</cell><cell>1,010</cell></row><row><cell>CIFAR-10 [16]</cell><cell></cell><cell>50,000</cell><cell>10,000</cell><cell>10</cell></row><row><cell>CIFAR-100 [16]</cell><cell></cell><cell>50,000</cell><cell>10,000</cell><cell>100</cell></row><row><cell>COCO2017 [19]</cell><cell>Detection</cell><cell>118,287</cell><cell>5,000</cell><cell>80</cell></row><row><cell>ADE20K [45]</cell><cell>Segmentation</cell><cell>20,210</cell><cell>2,000</cell><cell>150</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Default training hyper-parameters used in our method, unless stated otherwise.</figDesc><table><row><cell cols="2">Epochs Optimizer</cell><cell cols="2">Batch Learning size rate</cell><cell>LR decay</cell><cell cols="2">Weight Warmup decay epochs</cell><cell cols="3">Label smooth path Drop Repeated Aug</cell></row><row><cell>300</cell><cell>AdamW</cell><cell>1024</cell><cell>1e-3</cell><cell>cosine</cell><cell>0.05</cell><cell>5</cell><cell>0.1</cell><cell>0.1</cell><cell>√</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results of TNT and other networks on ImageNet.</figDesc><table><row><cell>Model</cell><cell>Resolution</cell><cell>Params (M)</cell><cell>FLOPs (B)</cell><cell>Top-1</cell><cell>Top-5</cell></row><row><cell>CNN-based</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet-50 [12]</cell><cell>224×224</cell><cell>25.6</cell><cell>4.1</cell><cell>76.2</cell><cell>92.9</cell></row><row><cell>ResNet-152 [12]</cell><cell>224×224</cell><cell>60.2</cell><cell>11.5</cell><cell>78.3</cell><cell>94.1</cell></row><row><cell>RegNetY-8GF [25]</cell><cell>224×224</cell><cell>39.2</cell><cell>8.0</cell><cell>79.9</cell><cell>-</cell></row><row><cell>RegNetY-16GF [25]</cell><cell>224×224</cell><cell>83.6</cell><cell>15.9</cell><cell>80.4</cell><cell>-</cell></row><row><cell>EfficientNet-B3 [28]</cell><cell>300×300</cell><cell>12.0</cell><cell>1.8</cell><cell>81.6</cell><cell>94.9</cell></row><row><cell>EfficientNet-B4 [28]</cell><cell>380×380</cell><cell>19.0</cell><cell>4.2</cell><cell>82.9</cell><cell>96.4</cell></row><row><cell>Transformer-based</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DeiT-Ti [31]</cell><cell>224×224</cell><cell>5.7</cell><cell>1.3</cell><cell>72.2</cell><cell>-</cell></row><row><cell>TNT-Ti</cell><cell>224×224</cell><cell>6.1</cell><cell>1.4</cell><cell>73.9</cell><cell>91.9</cell></row><row><cell>DeiT-S [31]</cell><cell>224×224</cell><cell>22.1</cell><cell>4.6</cell><cell>79.8</cell><cell>-</cell></row><row><cell>PVT-Small [36]</cell><cell>224×224</cell><cell>24.5</cell><cell>3.8</cell><cell>79.8</cell><cell>-</cell></row><row><cell>T2T-ViT_t-14 [40]</cell><cell>224×224</cell><cell>21.5</cell><cell>5.2</cell><cell>80.7</cell><cell>-</cell></row><row><cell>TNT-S</cell><cell>224×224</cell><cell>23.8</cell><cell>5.2</cell><cell>81.5</cell><cell>95.7</cell></row><row><cell>ViT-B/16 [9]</cell><cell>384×384</cell><cell>86.4</cell><cell>55.5</cell><cell>77.9</cell><cell>-</cell></row><row><cell>DeiT-B [31]</cell><cell>224×224</cell><cell>86.4</cell><cell>17.6</cell><cell>81.8</cell><cell>-</cell></row><row><cell>T2T-ViT_t-24 [40]</cell><cell>224×224</cell><cell>63.9</cell><cell>13.2</cell><cell>82.2</cell><cell>-</cell></row><row><cell>TNT-B</cell><cell>224×224</cell><cell>65.6</cell><cell>14.1</cell><cell>82.9</cell><cell>96.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>GPU throughput comparison of vision transformer models.</figDesc><table><row><cell>Model</cell><cell>Indices of TNT blocks</cell><cell cols="3">FLOPs (B) Throughput (images/s) Top-1</cell></row><row><cell>DeiT-S [31]</cell><cell>-</cell><cell>4.6</cell><cell>907</cell><cell>79.8</cell></row><row><cell>DeiT-B [31]</cell><cell>-</cell><cell>17.6</cell><cell>292</cell><cell>81.8</cell></row><row><cell>PVT-Small [36]</cell><cell>-</cell><cell>3.8</cell><cell>820</cell><cell>79.8</cell></row><row><cell>PVT-Medium [36]</cell><cell>-</cell><cell>6.7</cell><cell>526</cell><cell>81.2</cell></row><row><cell>TNT-S</cell><cell>[1, 2,</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Effect of position encoding.</figDesc><table><row><cell>2</cell><cell>428</cell><cell>81.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Effect of #heads in inner transformer block in TNT-S.</figDesc><table><row><cell>#heads</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell></row><row><cell>Top-1</cell><cell>81.0</cell><cell>81.4</cell><cell>81.5</cell><cell>81.3</cell><cell>81.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Effect of #words m.</figDesc><table><row><cell>m</cell><cell>c</cell><cell cols="3">Params FLOPs Top-1</cell></row><row><cell>64</cell><cell>6</cell><cell>23.8M</cell><cell>5.1B</cell><cell>81.0</cell></row><row><cell cols="2">16 24</cell><cell>23.8M</cell><cell>5.2B</cell><cell>81.5</cell></row><row><cell>4</cell><cell>96</cell><cell>25.1M</cell><cell>6.0B</cell><cell>81.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Results on downstream image classification tasks with ImageNet pre-training. ↑ 384 denotes fine-tuning with 384×384 resolution.</figDesc><table><row><cell>Model</cell><cell cols="7">Params (M) ImageNet CIFAR10 CIFAR100 Flowers Pets iNat-19</cell></row><row><cell>CNN-based</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Grafit ResNet-50 [32]</cell><cell>25.6</cell><cell>79.6</cell><cell>-</cell><cell>-</cell><cell>98.2</cell><cell>-</cell><cell>75.9</cell></row><row><cell>Grafit RegNetY-8GF [32]</cell><cell>39.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>99.1</cell><cell>-</cell><cell>80.0</cell></row><row><cell>EfficientNet-B5 [28]</cell><cell>30</cell><cell>83.6</cell><cell>98.7</cell><cell>91.1</cell><cell>98.5</cell><cell>-</cell><cell>-</cell></row><row><cell>Transformer-based</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ViT-B/16 ↑384 [9]</cell><cell>86.4</cell><cell>77.9</cell><cell>98.1</cell><cell>87.1</cell><cell>89.5</cell><cell>93.8</cell><cell>-</cell></row><row><cell>DeiT-B ↑384 [31]</cell><cell>86.4</cell><cell>83.1</cell><cell>99.1</cell><cell>90.8</cell><cell>98.4</cell><cell>-</cell><cell>-</cell></row><row><cell>TNT-S ↑384</cell><cell>23.8</cell><cell>83.1</cell><cell>98.7</cell><cell>90.1</cell><cell>98.8</cell><cell>94.7</cell><cell>81.4</cell></row><row><cell>TNT-B ↑384</cell><cell>65.6</cell><cell>83.9</cell><cell>99.1</cell><cell>91.1</cell><cell>99.0</cell><cell>95.0</cell><cell>83.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Results of object detection on COCO2017 val set with ImageNet pre-training. † Results from our implementation.</figDesc><table><row><cell>Backbone</cell><cell cols="2">Params Epochs</cell><cell>AP</cell><cell cols="4">AP50 AP75 APS APM</cell><cell>APL</cell></row><row><cell>ResNet-50 [36]</cell><cell>41M</cell><cell>50</cell><cell>32.3</cell><cell>53.9</cell><cell>32.3</cell><cell>10.7</cell><cell>33.8</cell><cell>53.0</cell></row><row><cell>DeiT-S  † [31]</cell><cell>38M</cell><cell>50</cell><cell>33.9</cell><cell>54.7</cell><cell>34.3</cell><cell>11.0</cell><cell>35.4</cell><cell>56.6</cell></row><row><cell>PVT-Small [36]</cell><cell>40M</cell><cell>50</cell><cell>34.7</cell><cell>55.7</cell><cell>35.4</cell><cell>12.0</cell><cell>36.4</cell><cell>56.7</cell></row><row><cell>PVT-Medium [36]</cell><cell>57M</cell><cell>50</cell><cell>36.4</cell><cell>57.9</cell><cell>37.2</cell><cell>13.0</cell><cell>38.7</cell><cell>59.1</cell></row><row><cell>TNT-S</cell><cell>39M</cell><cell>50</cell><cell>38.2</cell><cell>58.9</cell><cell>39.4</cell><cell>15.5</cell><cell>41.1</cell><cell>58.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Results of semantic segmentation on ADE20K val set with ImageNet pre-training.† Results from our implementation.</figDesc><table><row><cell>Backbone</cell><cell cols="3">Params FLOPs Steps mIoU</cell></row><row><cell cols="2">ResNet-50 [38] 56.1M 79.3G</cell><cell>40k</cell><cell>39.7</cell></row><row><cell>DeiT-S  † [31]</cell><cell>30.3M 27.2G</cell><cell>40k</cell><cell>40.5</cell></row><row><cell cols="2">PVT-Small [43] 32.1M 31.6G</cell><cell>40k</cell><cell>42.6</cell></row><row><cell>TNT-S</cell><cell>32.1M 30.4G</cell><cell>40k</cell><cell>43.6</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported by NSFC (62072449, 61632003), Guangdong-Hongkong-Macao Joint Research Grant (2020B1515130004) and Macao FDCT (0018/2019/AKP, 0015/2019/AKP).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pre-trained image processing transformer</title>
		<author>
			<persName><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiping</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Do we really need explicit position encodings for vision transformers?</title>
		<author>
			<persName><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10882</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Ekin D Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT (1)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A survey on vision transformer</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixing</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12556</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ghostnet: More features from cheap operations</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Augment your batch: Improving generalization through instance repetition</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niv</forename><surname>Giladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><surname>Huawei</surname></persName>
		</author>
		<author>
			<persName><surname>Mindspore</surname></persName>
		</author>
		<ptr target="https://www.mindspore.cn/" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maire</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName><surname>Fractalnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07648</idno>
		<title level="m">Ultra-deep neural networks without residuals</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Are sixteen heads really better than one?</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Omkar M Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3498" to="3505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Designing network design spaces</title>
		<author>
			<persName><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Augmented shortcuts for vision transformers</title>
		<author>
			<persName><forename type="first">Yehui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiping</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.15941</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Manifold regularized dynamic network pruning</title>
		<author>
			<persName><forename type="first">Yehui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiping</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5018" to="5028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Hugo Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Grafit: Learning fine-grained image representations with coarse labels</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Hugo Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.12982</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8769" to="8778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Segmenting transparent object in the wild with transformer</title>
		<author>
			<persName><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Renas: Relativistic evaluation of neural architecture search</title>
		<author>
			<persName><forename type="first">Yixing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangling</forename><surname>Jui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4411" to="4420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><forename type="middle">Eh</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong</forename><surname>Joon Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjoon</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><surname>Philip Hs Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
