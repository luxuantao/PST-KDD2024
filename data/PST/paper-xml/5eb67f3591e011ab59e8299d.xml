<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MAZE: Data-Free Model Stealing Attack Using Zeroth-Order Gradient Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-05-06">6 May 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Sanjay</forename><surname>Kariyappa</surname></persName>
							<email>sanjaykariyappa@gatech.edu</email>
						</author>
						<author>
							<persName><forename type="first">Moinuddin</forename><surname>Qureshi</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology Atlanta</orgName>
								<address>
									<settlement>Atul Prakash</settlement>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Georgia Institute of Technology Atlanta</orgName>
								<address>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MAZE: Data-Free Model Stealing Attack Using Zeroth-Order Gradient Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-05-06">6 May 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2005.03161v1[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Model Stealing (MS) attacks allow an adversary with black-box access to a Machine Learning model to replicate its functionality, compromising the confidentiality of the model. Such attacks train a clone model by using the predictions of the target model for different inputs. The effectiveness of such attacks relies heavily on the availability of data necessary to query the target model. Existing attacks either assume partial access to the dataset of the target model or availability of an alternate dataset with semantic similarities. Unfortunately, these attacks fail to produce a high-accuracy clone without access to the target dataset or a representative dataset.</p><p>This paper proposes MAZE -a data-free model stealing attack using zeroth-order gradient estimation. In contrast to prior works, MAZE does not require any data and instead creates synthetic data using a generative model. Inspired by recent works in data-free Knowledge Distillation (KD), we train the generative model using a disagreement objective to produce inputs that maximize disagreement between the clone and the target model. However, unlike the white-box setting of KD, where the gradient information is available, training a generator for model stealing requires performing black-box optimization, as it involves accessing the target model under attack. MAZE relies on zeroth-order gradient estimation to perform this optimization and enables a highly accurate MS attack.</p><p>Our evaluation with four datasets shows that MAZE provides a normalized clone accuracy in the range of 0.91? to 0.99?, and outperforms even the recent attacks that rely on partial data (JBDA, clone accuracy 0.13? to 0.69?) and surrogate data (KnockoffNets, clone accuracy 0.52? to 0.97?). We also study an extension of MAZE in the partial-data setting, and develop MAZE-PD, which generates synthetic data closer to the target distribution. MAZE-PD further improves the clone accuracy (0.97? to 1.0?) and reduces the query required for the attack by 2?-24?.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Advances in Machine Learning (ML) algorithms such as Deep Neural Networks (DNNs) have enabled the automation of a wide variety of challenging tasks in multiple fields, including computer vision and speech recognition. This, in turn, has enabled companies to train and deploy DNN models to create new products and services like intelligent cameras, voice assistants, self-driving cars, and even improve existing products such as recommendation and predictive text. Furthermore, several companies currently also offer ML as a service where a user can query a remotely hosted ML model with an input to obtain the output predictions. The users of such services typically only have black-box access to the predictions of the model without knowing the model parameters or architecture.</p><p>As the performance of ML models scales with the training data <ref type="bibr" target="#b13">[14]</ref>, companies spend a significant amount of money and engineering resources to collect vast amounts of data to train high-performance ML models. Protecting the confidentiality of these models is vital both to maintain a competitive advantage as well as preventing the stolen model from being misused by an adversary to compromise security and privacy. For example, an adversary can use the stolen model to craft adversarial examples that can fool the model using model evasion attacks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37]</ref>, establish membership of an individual's record in the training data through membership inference attacks <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40]</ref>, and even leak sensitive user data used to train the model through model inversion <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b44">45]</ref> attacks. Thus, ML models are considered valuable intellectual properties and are closely guarded against theft and data leaks. Model stealing attacks compromise the confidentiality of ML models by allowing an adversary to train a clone model that closely mimics the predictions of the target model, effectively copying its functionality. These attacks only require black-box access to the target model where the adversary can access the predictions of the model for any given input. We explain MS attacks using Fig. <ref type="figure" target="#fig_1">1</ref>. The adversary first queries the target model T with various inputs {x i } n i=1 and uses the predictions of the target model y i = T (x i ) to construct a labeled dataset D = {x i , y i }. This dataset is used to train a clone model C to match the predictions of T . With an appropriate set of inputs {x i } n i=1 , this process can be used to train a clone model that achieves high accuracy on the classification task, effectively replicating the functionality of the target model.</p><p>In existing MS attacks, the availability of data necessary to query the target model plays a key role in the ability of the attacker to train high accuracy clone models. For example, if the inputs from the entire training dataset of T is available to the adversary, then the attacker can simply use the predictions of T on these inputs to construct a labeled training dataset, which can be used to train a highly accurate clone model. However, in most real-world scenarios, the training data is not readily available to the attacker as companies typically train their models using proprietary datasets. Recently, several MS attacks have been proposed for the setting where only surrogate data or a subset of the training data is available to the adversary. For example, Jacobian-Based Dataset Augmentation (JBDA) <ref type="bibr" target="#b29">[30]</ref> is an attack that uses a subset of the training data to create additional synthetic data and uses that to query the target model. Similarly, KnockoffNets <ref type="bibr" target="#b27">[28]</ref> is another MS attack that uses a surrogate dataset to query the target model. The efficacy of this attack depends on the availability of a surrogate dataset that is representative of the training data (e.g., using CIFAR-100 to attack a CIFAR-10 model). Thus, existing attacks either assume partial access to the dataset of the target mode or availability of a suitable surrogate dataset. Unfortunately, these attacks become ineffective without access to the target dataset or representative dataset. The goal of our paper is to develop a highly accurate MS attack without relying on any access to the target dataset or a surrogate dataset.</p><p>We propose MAZE, a data-free model stealing attack using zerothorder (ZO) gradient estimation. Unlike existing attacks, MAZE does not require access to the target or a surrogate dataset and instead uses a generative model to produce the synthetic inputs for launching the attack. The generator is trained on a disagreement objective, wherein the generator is trained to produce inputs that maximize the disagreement between the predictions of the teacher (target model) and the student (clone model). Similar objectives have been used by recent work on data-free Knowledge Distillation (KD) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b40">41]</ref>. But, unlike the work on data-free KD, which assumes white-box access to the teacher model, MAZE operates in a blackbox setting where no such access is available. Our key insight in MAZE is to use the zeroth-order gradient estimate <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26]</ref> to approximate the gradient of the black-box target model and use this to train the generator. Unfortunately, the dimensionality of the generator's parameters can be in the order of millions, making a direct application of ZO gradient estimation challenging. So, instead we show how to estimate the gradient with respect to the lower dimensional synthetic input and use that to train the generator in a query-efficient manner.</p><p>We evaluate our proposal on image classification DNNs across multiple datasets, including FashionMNIST, SVHN, GTSRB, and CIFAR-10. Our evaluation shows that MAZE provides a normalized clone accuracy 1 of 0.91? to 0.99?. In fact, MAZE, even though it does not use any data inputs, outperforms the recent attacks that rely on partial data (JBDA, clone accuracy of 0.13? to 0.69?) or surrogate data (KnockoffNets, clone accuracy of 0.52? to 0.97?). Thus, MAZE not only allows the attacker to launch the attack in a data-free setting but also have higher accuracy for the attack. 1 Normalized clone accuracy expresses the accuracy of the clone model as a fraction of the target-model accuracy  <ref type="bibr" target="#b27">[28]</ref> and JBDA <ref type="bibr" target="#b29">[30]</ref> that also rely on partial datasets.</p><p>Table <ref type="table" target="#tab_0">1</ref> contrasts MAZE against KD and other MS attacks. KD is typically used to train a student model from a teacher model when the training dataset is available. Recent works on data-free KD <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref> have shown that it is possible to perform KD without knowledge of the training data, however, they require white-box access to the teacher model. Model stealing attacks, on the other hand, aim to copy a target model in the black-box setting, but assume the availability of a representative dataset. MAZE requires neither white-box access to a model nor any dataset. It only requires black-box access to the probabilistic predictions of the target model. While prior works <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b35">36]</ref> have investigated model stealing with noise-like inputs for simple models, such attacks become ineffective for complex models and datasets (e.g., our evaluations show a clone accuracy of only 0.10? for CIFAR-10 and 0.19? for GTSRB). To the best of our knowledge, MAZE is the first data-free model stealing attack that provides high accuracy across multiple image classification datasets and complex DNN models, such as Resnet <ref type="bibr" target="#b15">[16]</ref>. MAZE-PD extends MAZE to the partial-data setting to further improve accuracy and reduce the query budget.</p><p>Overall our paper makes the following key contributions:</p><p>(1) We propose MAZE, a highly accurate model stealing attack that does not require any data. MAZE uses a generative model trained with a disagreement objective using zerothorder gradient estimation to generate the inputs. Even though MAZE does not use any data, it still outperforms recent attacks that rely on partial data or surrogate data. <ref type="bibr" target="#b1">(2)</ref> We propose MAZE-PD for the partial-data setting, where a small subset of training data (e.g. 100 examples) is available. MAZE-PD trains the generator to produce inputs that are closer to the target data distribution. This improves the quality of synthetic data and results in even higher accuracy and a reduced number of queries for the attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND MOTIVATION</head><p>The objective of Model Stealing (MS) attacks is to train a clone model that replicates the functionality of a target model and achieves high accuracy on the task of the target model. As the attacker typically does not have access to the dataset used to train the target model, attacks need alternate forms of data to query the target model and perform model stealing. Depending on the availability of data, MS attacks can broadly be classified into three categories: (1) partialdata, (2) surrogate-data, and (3) data-free. We provide background on these attack settings and provide examples of recent attacks in each case. We also discuss the limitations of the existing attacks and motivate the need for our attack. Lastly, we formally state the goal and threat model for our attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model Stealing with Partial Data</head><p>In the partial-data setting, the attacker has access to a subset of the data used to train the target model. While this in itself may be insufficient to carry out model stealing, it allows the attacker to craft synthetic examples using the available data. Jacobian Based Dataset Augmentation (JBDA) <ref type="bibr" target="#b29">[30]</ref> is an example of one such attack that assumes that the adversary has access to a small set of seed examples from the target data distribution. The attack works by first training a clone model C using the seed examples and then progressively adding synthetic examples to the training dataset. JBDA uses a perturbation based heuristic to generate new synthetic inputs from existing labeled inputs. E.g., from an input-label pair (x, y), a synthetic input x ? is generated by using the jacobian of the clone model's loss function ? x L (C (x; ? c ) , y) as shown in Eqn. 1.</p><formula xml:id="formula_0">x ? = x + ?si?n (? x L (C (x; ? c ) , y))<label>(1)</label></formula><p>The dataset of synthetic examples {x ? i } generated this way are labeled by using the predictions of the target model y ? i = T (x ? i ) and the labeled examples {x ? i , y ? i } are added to the pool of labeled examples that can be used to train the clone model C. In addition to requiring a set of seed examples from the target distribution, a key limitation of JBDA is that, while it works well for simpler datasets like MNIST, it tends to produce clone models with lower classification accuracy for more complex datasets. For example, our evaluations show that JBDA provides a relative clone accuracy of only 0.13? (GTSRB dataset) and 0.18? (SVHN dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model Stealing with Surrogate Data</head><p>In the surrogate data setting, the attacker has access to alternate datasets that can be used to query the target model. For example, consider an attacker who wants to steal a DNN model trained with the CIFAR-10 dataset. Given a lack of access to a large collection of examples from CIFAR-10, the attacker can potentially use images from an alternate dataset, such as CIFAR-100 to query the target model to perform model stealing. KnockoffNets <ref type="bibr" target="#b27">[28]</ref> is an example of a MS attack that is designed to operate in such a setting. With a suitable surrogate dataset, KnockoffNets can produce clone models with up to 0.97? the accuracy of the target model. However, the efficacy of such attacks is dictated by the availability of a suitable surrogate dataset. For instance, if we use the MNIST dataset to perform Model stealing on a FashionMNIST model, it only produces a clone model with 0.41? the accuracy of the target model. This is because the surrogate dataset is not representative of the target dataset, which reduces the effectiveness of the attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Data-Free Model Stealing</head><p>In the data-free setting, the adversary does not have access to any data. This represents the hardest setting to carry out MS as the attacker has no knowledge of the data distribution used to train the target model. A recent work by Roberts et al. <ref type="bibr" target="#b31">[32]</ref> studies the use of inputs derived from various noise distributions to carry out model stealing attack in the data-free setting. The authors show that it is possible to carry out MS attacks with relatively good accuracy for simple datasets like MNIST just using inputs sampled from noise distributions. However, our evaluations show that such attacks do not scale to more complex datasets such as CIFAR-10 (we obtained relative clone accuracy of only 0.11?), limiting their applicability. Another attack by Tramer et al. <ref type="bibr" target="#b35">[36]</ref> attempts to extract the exact model parameters of a black-box target model. The authors apply equation-solving, using the response of the model for various random queries, to deduce the exact values of the model parameters. This work assumes partial knowledge of the target model like the number of neurons and their connectivity. Unfortunately, this technique was only shown to work on simple single-layer neural networks with 20 neurons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Our Goal</head><p>Existing attacks in the partial-data and data-free settings only seem to be effective for small models trained on simple datasets. Our goal is to develop an attack that can be used to train a highly accurate clone model in the data-free setting only using black-box access to the target model. We formally state the objective and constraints of our proposed model stealing attack.</p><p>Attack Objective: Consider a target model T (x; ? T ) that performs a classification task with high accuracy on inputs x sampled from a target distribution P T . Our goal is to train a clone model C(x; ? C ) that replicates the functionality of T and achieves high accuracy on the classification task as shown by Eqn. 2 max</p><formula xml:id="formula_1">? C E x ?P T [Acc(C(x; ? C ))]<label>(2)</label></formula><p>Threat Model: In this work, we assume that the adversary does not know any details about the Target model's architecture or the model parameters ? T . The adversary is only allowed black-box access to the target model. We assume the soft-label setting where the adversary can query the target model with any input x and observe its output probabilities ? y = T (x; ? T ). We consider model stealing attacks under two settings based on the availability of data:   </p><formula xml:id="formula_2">Generator (G) x L G L G L C x = G(z) y T = T (x) y C = C(x) L G = -D KL (y T , y C ) L C = D KL (y T , y C )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MAZE: DATA-FREE MODEL STEALING</head><p>We propose MAZE, a data-free model stealing attack using zeroth order gradient estimation. Unlike existing attacks, MAZE does not require access to the target or a surrogate dataset and instead uses a generative model to produce the synthetic queries for launching the attack. Figure <ref type="figure" target="#fig_3">2</ref> shows an overview of MAZE. MAZE trains a clone model C using the predictions of T for various input queries {x }. MAZE uses a generative model G to produce the inputs necessary to query the target model. In this section, we first describe the training objectives of the clone and the generator model. We then motivate the need for gradient estimation to update G in the black-box setting of MS attack and show how zeroth-order gradient estimation can be used to optimize the parameters of G and enable our attack. Finally, we discuss our algorithm to carry out model stealing with MAZE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training the Clone Model</head><p>The clone model is trained on the input queries produced by the generator. The generator G takes in a low dimensional latent vector z, sampled from a random normal distribution, and produces an input query x ? R d that matches the input dimension of the target classifier (Eqn. 3). We use x to query T and obtain the output probabilities of the target model ? y T (Eqn. 4). Similarly, we obtain the predictions of the clone model ? y C on x as shown by Eqn. 5.</p><formula xml:id="formula_3">x = G(z; ? G ); z ? N (0, I )<label>(3)</label></formula><formula xml:id="formula_4">? y T = T (x; ? T )<label>(4)</label></formula><formula xml:id="formula_5">? y C = C(x; ? C )<label>(5)</label></formula><p>Where ? T , ? C and ? G represent the parameters of the target, clone, and generator models, respectively. The parameters of the clone model ? C are updated using the loss function in Eqn. 6 to minimize the KL divergence between ? y C and ? y T .</p><formula xml:id="formula_6">L C = D K L ( ? y T ? ? y C )<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training the Generator Model</head><p>The generator model G plays the role of synthesizing the queries necessary to perform model stealing. Similar to recent works in data-free KD <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b40">41]</ref>, MAZE trains the generator to produce queries that maximize the disagreement between the predictions of the teacher and the student by maximizing the KL-divergence between ? y T and ? y C . The loss function used to update the model parameters of the generator ? G is described by Eqn. 7, which we refer to as the disagreement objective.</p><formula xml:id="formula_7">L G = -D K L ( ? y T ? ? y C )<label>(7)</label></formula><p>Training G on this loss function maximizes the disagreement between the predictions of the target and the clone model. Since C and G have opposing objectives, training both models together results in a two-player game, similar to Generative Adversarial Networks <ref type="bibr" target="#b10">[11]</ref>, resulting in the generation of inputs that maximize the learning of the clone model. By training C to match the predictions of T on the queries generated by G, we can perform knowledge distillation and obtain a highly accurate clone model. Training G using the loss function in Eqn. 7 requires backpropagating through the predictions of the target model T , as shown by the dashed lines in Figure <ref type="figure" target="#fig_3">2</ref> and Eqn. 4, 5. Unfortunately, as we only have black-box access to T , we cannot perform back-propagation directly, preventing us from training G and carrying out the attack. To solve this problem, our insight is to use zeroth-order gradient estimation to approximate the gradient of the loss function L G . The number of black-box queries necessary for ZO gradient estimation scales with the dimensionality of the parameters being optimized. Estimating the gradients of L G with respect to the generator parameters ? G directly is expensive as the generator has on the order of millions of parameters. Instead, we choose to estimate the gradients with respect to the synthetic input x produced by the generator, which has a much lower dimensionality (3072 for CIFAR-10), and use this estimate to back propagate through G (Eqn. 9). This modification allows us to compute a gradient estimates in a query efficient manner to update the generator model. The following section describes how we efficiently apply zeroth-order gradient estimation to train the generator model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Train via Zeroth-Order Gradient Estimate</head><p>Zeroth-order gradient estimation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26]</ref> is a popular technique to perform optimization in the black-box setting. We use this technique to train our generator model G. Recall that our objective is to update the generator model parameters ? G using gradient descent to minimize the loss function L G as shown in Eqn. 8.</p><formula xml:id="formula_8">? t +1 G = ? t G -?? ? G L G<label>(8)</label></formula><p>Updating ? G in this way requires us to compute the derivative of the loss function ? ? G L G . By the use of chain-rule, ? ? G L G can be decomposed into two components as shown in Eqn. 9.</p><formula xml:id="formula_9">? ? G L G = ?L G ?? G = ?L G ?x ? ?x ?? G<label>(9)</label></formula><p>We can compute the second term ?x ?? G in Eqn. 9 by performing backpropagation through G. Computing the first term ? L G ?x however requires access to the model parameters of the target model (? T ). Since T is a black-box model from the perspective of the attacker, we do not have access to ? T , which prevents us from computing ? L G ?x through backpropagation. Instead, we propose to use an approximation of the gradient by leveraging zeroth-order gradient estimation. To explain how the gradient estimate is computed, consider an input vector x ? R d generated by G that is used to query T . We can estimate ? L G ?x by using the method of forward differences <ref type="bibr" target="#b30">[31]</ref> as shown in Eqn. <ref type="bibr" target="#b9">10</ref>.</p><formula xml:id="formula_10">?x L G (x; u i ) = d ? (L G (x + ?u i ) -L G (x)) ? u i<label>(10)</label></formula><p>Where u i is a random variable drawn from a d dimensional unit sphere with uniform probability and ? is a small positive constant called the smoothing factor. The random gradient estimate, shown in Eqn. 10, tends to have a high variance. To reduce the variance, we use an averaged version of the random gradient estimate <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22]</ref> by computing the forward difference using m random directions {u 1 , u 2 , ..u m }, as shown in Eqn. 11.</p><formula xml:id="formula_11">?x L G (x) = 1 m m i=1 ?x L G (x; u i )<label>(11)</label></formula><p>Where ?x L G is an estimate of the true gradient ? x L G . By substituting ?x L G into Eqn. 9, we can compute an approximation for the gradient of the loss function of the generator: ?? G L G . The gradient estimate ?? G L G computed this way can be used to perform gradient descent by updating the parameters of the generator model ? G according to Eqn. 8. By updating ? G , we can train G to produce the synthetic examples required to perform model stealing.</p><p>One problem that arises with numerical approximation of gradients as described above is that we need to ensure that the inputs used to query the target model should have values that lie within the valid range, in our case [-1, 1]. However, the perturbation (x + ?u i ) might cause the input to take values outside this range, making the query invalid. To avoid this problem, we apply the gradient estimation to the activations x p in the penultimate layer of G, which does not have a constraint on the values it can take. Note that the last layer of G consists of a tanh function that ensures that the output of the generator x = tanh(x p ) is always in the range [-1, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">MAZE Algorithm for Model Stealing Attack</head><p>We outline the algorithm of MAZE in Algorithm 1 by putting together the individual training algorithms of the generator and clone models. We start by fixing a query budget Q, which dictates the maximum number of queries we are allowed to make to the target model T . ? is the smoothing parameter and m is the number of random directions used to estimate the gradient. We set the value of ? to 0.001 in our experiments. N G , N C represent the number of training iterations and ? G , ? C represent the learning rates of the generator and clone model, respectively. N R denotes the number of iterations for experience replay.</p><p>Our attack starts by initializing the generator and the clone model G(?; ? G ), C(?; ? C ). q is a variable that holds the number of queries already performed and D is a dataset used to collect the input, label pairs: (x,T (x)) generated by querying the target model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: MAZE Algorithm for Model Stealing Attack</head><p>Input:</p><formula xml:id="formula_12">T , Q, ?, m, N G , N C , N G , ? G , ? C Output: Clone model C(?; ? C ) Initialize G(?; ? G ), C(?; ? C ), q ? 0, D ? {} while q &lt; Q do // Generator Training for i ? 0 to N G do x = G(z) : z ? N (0, I ) L G = -D K L (T (x)?C(x)) ?? G L G ? ZO_?rad_est(G,T , C, x, ?, m) ? G ? ? G -? G ?? G L G end // Clone Training for i ? 0 to N C do x = G(z) : z ? N (0, I ) L C = D K L (T (x)?C(x)) ? C ? ? C -? C ? ? C L C D ? D ? {(x,T (x))} end // Experience Replay for i ? 0 to N T do (x, y T ) ? D L C = D K L (y T ?C(x)) ? C ? ? C -? C ? ? C L C end q ? update(q) end</formula><p>The outermost loop of the attack repeats till we exhaust our query budget Q. The attack algorithm involves three phases: 1. Generator Training 2. Clone Training and 3. Experience Replay. In the Generator Training phase, we perform N G rounds of gradient descent for the generator model G, which is trained to produce inputs x that maximize the KL-divergence between the predictions of the target and clone model. The parameters of the generator ? G are updated by using zeroth-order gradient estimates as described in Section 3.3. This is followed by the Clone Training phase where we perform N C rounds of gradient descent for C. In each round, we generate a batch of inputs x = G(z) and use these inputs to query the target model. The clone model is trained to match the predictions of the target model by minimizing D K L (T (x)?C(x)). The input, prediction pair: (x,T (x)) generated in each round is stored in dataset D. Finally, we perform Experience Replay, where we train the clone on previously seen inputs that are stored in D. Retraining on previously seen queries reduces catastrophic forgetting <ref type="bibr" target="#b22">[23]</ref> and ensures that the clone model continues to classify old examples seen during the earlier part of the training process correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Computing the Query Cost</head><p>The target model needs to be queried in order to update both the generator and the clone models. Considering a batch size of 1, one training iteration of G requires m + 1 queries to T for the zerothorder gradient estimation and each training loop of C requires 1 query. Experience replay, on the other hand, does not require any additional queries to T . Thus, with a batch size of B, the query cost of each iteration is described by Eqn. 12 Query cost per iteration = B(N G (m + 1)</p><formula xml:id="formula_13">+ N C )<label>(12)</label></formula><p>We use B = 128, N G = 1, N C = 5, N R = 10 and m = 10 in our experiments, unless stated otherwise. Thus, each iteration of the attack requires 2048 queries. Unless specified otherwise, MAZE is evaluated with a default query budget of 30 million queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL EVALUATION</head><p>We validate our attack by performing model stealing attacks on various target models and provide experimental evidence to show that our attack can produce high accuracy clone models without using any data. We first describe our experimental setup followed by the results of our proposed model stealing attack on various target datasets. We compare our results against two prior works-KnockoffNets and Jacobian Based Dataset Augmentation (JBDA)and show that the clone models produced by our attack have comparable or better accuracy than the ones produced by these attacks, despite not using any data. Additionally, we perform sensitivity studies to understand how the efficacy of our attack depends on the query budget and the number of gradient estimation directions (m) used in our attack. Finally, to understand the limitation of gradient estimation, we repeat our attack under a setting where we assume we have access to perfect gradient information through the target model. This provides an upper bound to the clone model accuracy that can be obtained when we have perfect gradient information instead of using a zeroth-order estimate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup: Dataset and Architecture</head><p>We evaluate our attack by performing model stealing using MAZE to replicate the functionality of Convolutional Neural Networks (CNNs) trained to perform classification tasks on various image datasets. Table <ref type="table" target="#tab_1">2</ref> lists the model architectures and datasets that we use as the target model T . FashionMNIST <ref type="bibr" target="#b38">[39]</ref> is a classification problem involving 28 ? 28 grayscale images while the remaining datasets use 32 ? 32 RGB images. FashionMNIST, SVHN <ref type="bibr" target="#b26">[27]</ref>, and CIFAR-10 [20] have 10 classes while GTSRB <ref type="bibr" target="#b33">[34]</ref> has 43. With the exception of CIFAR-10, all the target models are trained for 50 epochs with Adam optimizer with a learning rate of 0.001. We train the CIFAR-10 target model for 200 epochs using SGD optimizer and cosine annealing with an initial learning rate of 0.1. We assume no knowledge of the target model and use a randomly initialized 22-layer Wide Residual Network <ref type="bibr" target="#b43">[44]</ref> model as the clone model for all the datasets. In general, any sufficiently complex DNN can be used as the clone model. We use Stochastic Gradient Descent (SGD) to train our clone model with an initial learning rate of 0.1. For G, we use a generative model with 3 convolutional layers. Each convolutional layer in G is followed by a batchnorm layer and the activations are upsampled to ensure that the outputs generated by G are of the correct dimensionality corresponding to the dataset being attacked. We use an SGD optimizer with an initial learning rate of 0.0001 to train G. The learning rates for both the clone and generator models are decayed using cosine annealing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Configuration of Existing Attacks</head><p>Existing MS attacks either use surrogate data or synthetic datasets derived from partial access to the target dataset. We compare MAZE with the following attacks:</p><p>1. KnockoffNets <ref type="bibr" target="#b27">[28]</ref>: This attack uses a surrogate dataset to query the target model to construct a labeled dataset using the predictions of the target model. This labeled dataset is used to train the clone model. We evaluate this attack for all the target models listed in Table <ref type="table" target="#tab_1">2</ref>. We use MNIST, CIFAR10, CIFAR100, and CIFAR10 as the surrogate datasets for FashionMNIST, SVHN, CIFAR10, and GTSRB models, respectively. In each case, we query the target model with the training examples of the surrogate dataset. We then use the dataset constructed from these queries to train the clone model for 100 epochs using an SGD optimizer with a learning rate of 0.1 with cosine annealing scheduler.</p><p>2. JBDA <ref type="bibr" target="#b29">[30]</ref>: The JBDA attack uses synthetic datasets to query the target model to perform model stealing. These synthetic examples are generated by adding perturbations to a set of seed examples, which are obtained from the data distribution of the target model. The perturbations are computed by using the Jacobian of the loss function of the clone model, as shown in Eqn. 1. We start with an initial dataset of 100 seed examples. We perform 6 rounds<ref type="foot" target="#foot_0">2</ref> of synthetic data augmentation and train the clone model for 10 epochs between each round. ? in Eqn. 1 dictates the magnitude of the perturbation. We set this to a value of 0.1. We use Adam optimizer with a learning rate of 0.001 to train the clone model for JBDA.</p><p>3. Noise: To test if inputs sampled from noise can be used to carry out MS attack, we design a Noise attack. Here, the inputs that are used to query the target model are derived from a uniform distribution i.e., x ? U d [-1, 1]. This is similar, in spirit, to the proposal by Roberts et al. <ref type="bibr" target="#b31">[32]</ref> and serves as a baseline data-free MS attack to compare with our proposal. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Key Result: Normalized Clone Accuracy</head><p>Table <ref type="table" target="#tab_2">3</ref> shows the accuracy of the clones obtained by attacking various target models using MAZE. The numbers in brackets express the clone accuracy normalized to the accuracy of the target model being attacked. We also compare our results with three other model stealing attacks-KnockoffNets, JBDA, and Noise. The best clone accuracy we obtain from each dataset is highlighted in bold.</p><p>Our results show that MAZE produces high accuracy clone models with a normalized accuracy greater than 0.90? for all the target models under attack. In contrast, the baseline Noise attack fails to produce high accuracy clone models for most of the datasets. Furthermore, the results from our attack also compare favorably against KnockoffNets and JBDA, both of which require access to some data. We find that the effectiveness of KnockoffNets is highly dependent on the surrogate data being used to query the target model. For example, using MNIST to attack FashionMNIST dataset results in a low accuracy clone model (0.52? target accuracy) as these datasets as visually dissimilar. However, using CIFAR-100 to query CIFAR-10 results in a high accuracy clone model (0.89? target accuracy) due to the similarities in the two datasets. JBDA seems to be effective for attacking simpler datasets like FashionMN IST , but the accuracy reduces when attacking more complex datasets. This is in part because JBDA produces queries that are highly correlated to the initial set of "seed" examples, which sometimes results in worse performance even compared to noise (e.g. SVHN). By using the disagreement objective to train the generator, MAZE can generate queries that are more useful in training the clone model and result in higher accuracy of clones (0.91?-0.99?) compared to other attacks like JBDA (0.13?-0.69?) that use synthetic data.</p><p>We study the sensitivity of MAZE to two attack parameters: 1. Query budget (Q) and 2. Number of gradient estimation directions (m). We perform experiments by varying these parameters and study the dependence of the clone model accuracy obtained from our attack to these parameter values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Sensitivity to Query Budget</head><p>A larger query budget allows the attacker to carry out more attack iterations and train a better clone model. To understand the dependence of query budget Q to clone accuracy, we perform sensitivity studies by carrying out our attack for seven different values of Q for each dataset. We set Q ? {0.625M, 1.25M, 2.5M, 5M, 10M, 20M, 30M } and report the normalized clone accuracy for each dataset in Fig 3 . 
As expected, we find that the clone accuracy increases with an increase in Q. Additionally, we note that the number of queries necessary to reach a given clone accuracy seems to scale with the dimensionality of the input and the difficulty of the target classification task. While we use a large query budget of Q = 30M to report the clone accuracy in Table <ref type="table" target="#tab_2">3</ref>, not all datasets require such a high query budget to reach the desired level of accuracy. For example, we only require around 5M queries to reach a normalized accuracy of 0.95? for SVHN, whereas for CIFAR-10, which is a harder dataset to classify, we require around 30M queries to reach the same accuracy. for various target models under MAZE attack. Clone accuracy improves with a higher query budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Sensitivity to Number of Directions for Estimating the Gradient</head><p>Our attack uses a generative model to synthesize the queries necessary to perform model stealing. The loss function of the generator L G involves the evaluation of a black-box model T . As gradient information is unavailable, our attack uses zeroth-order gradient estimation instead to approximate the gradient of the loss function ?? G L G required to update the generator parameters ? G . The estimation error of this zeroth-order approximation is inversely related to the number of gradient estimation directions (m) used in our numerical approximation of the gradient (Eqn. 8). By using a larger value of m, we can get a more accurate estimate of the gradient. Unfortunately, increasing m also increases the queries that need to be made to the target model as described by Eqn. 12. This means that given a fixed query budget Q, increasing m results in more queries being consumed to update G, leaving fewer queries to train the clone model. To understand this trade-off, we fix the query budget and perform our attack by setting m to four different values (m ? {1, 5, 10, 20}). With a large query budget of 30M, changing m has limited impact as most clone models achieve high accuracy regardless of the value used for m. Hence, for this analysis, we use smaller query budgets instead to magnify the impact of m on the clone accuracy. We set the query budget Q to 1.25M for Fashion-MNIST and SVHN, 10M for GTSRB, and 20M for CIFAR-10. The normalized accuracy of clones obtained for these different values of m on various datasets are shown in Fig. <ref type="figure" target="#fig_5">4</ref>. Our results show that for FashionMNIST and SVHN, lowering the value of m results in an improvement in clone accuracy. Setting m to a smaller value allows more queries to be used to train the clone model, while sacrificing the accuracy of the gradient update of the generator. This seems to be a favorable trade-off for simpler datasets. Similarly, for GTSRB we find that reducing the number of directions from m = 20 to m = 5 improves clone accuracy. However, reducing it further to m = 1 leads to a degradation in the clone accuracy. We observe a similar trend for CIFAR-10 as well. While reducing m from 20 to 10 improves accuracy, reducing it further causes a degradation. This degradation in accuracy by reducing m for GTSRB and CIFAR-10 can be attributed to the increased error in gradient estimation with fewer gradient estimation directions. Thus, in a query limited setting, varying m provides a trade-off between the number of queries used to update the generator G and clone model C. The optimal value of m to perform model stealing depends on the complexity of the target dataset being attacked.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Importance of Experience Replay</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Impact of Error in Gradient Estimation</head><p>MAZE uses numerical methods to approximate the gradient through the black-box target model T to update the parameters of the generator model. To understand how the gradient estimation error impacts the accuracy of the clone model, we repeat MAZE by assuming that we have access to perfect gradient information. For the purpose of analysis in this section only, we obtained perfect gradient information by treating the target T as a white-box model that allows back-propagation in the Generator Training phase shown in Algorithm 1. By comparing the results of our attack with the accuracy of the clone model trained with perfect gradient information, we can understand how the gradient estimation error impacts the accuracy of the clone model obtained from our attack.</p><p>Figure <ref type="figure" target="#fig_8">6</ref> shows the clone accuracy of MAZE with zeroth-order gradient estimation and MAZE with perfect gradient information. We use a reduced query budget of 1.25M for FashionMNIST and SVHN, 10M for GTSRB, and 20M for CIFAR-10. We observe that there is a slight improvement in clone accuracy (on average, 3.8%) when perfect gradient information is available. This shows that our approximation of gradient is reasonably accurate and therefore MAZE is able to train clone model with high accuracy.</p><p>When the query budget is increased, the error in estimating the gradient is tolerated by the training algorithm, and we observe that at the default budget of 30 million queries the difference in clone accuracy with estimated gradient and perfect gradient is negligible. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">MAZE-PD: MAZE WITH PARTIAL-DATA</head><p>Thus far, we have described our model stealing attack in a datafree setting, where the adversary does not have access to any dataset. The accuracy and speed of our attack can be further increased if there are a few examples available from the training-data distribution of the target model. In this section, we consider such a partial-data setting and develop MAZE-PD, an extension of MAZE to the partial-data setting.</p><p>MAZE-PD uses an organization similar to MAZE, where a generator model is trained to create synthetic data. However, MAZE-PD uses Waserstein Generative Adversarial Networks (WGANs) <ref type="bibr" target="#b1">[2]</ref> to train the generator in order to improve the quality of images generated. In the data-free setting of MAZE, G is trained on a disagreement objective to produce inputs that maximize the disagreement between the target and the clone model. In the presence of a limited amount of data, we can additionally train the generator to produce inputs that are closer to the target distribution by using the WGAN training objective.</p><p>We observe that even a small amount of data from the target distribution (100 examples) can enable the generator to produce synthetic inputs that are closer to the target distribution. By improving the quality of the generated queries, MAZE-PD not only improves the effectiveness of the attack but also allows the attack to succeed with far fewer queries compared to MAZE.</p><p>In this section, we first provide background on WGANs, which can be used to generate synthetic examples closer to the target distribution. Following this, we describe how WGANs can be incorporated into the training of the generator model of our attack to develop MAZE-PD. Finally, we provide empirical evidence to show that MAZE-PD improves clone accuracy and reduces query cost significantly compared to MAZE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Background on WGAN</head><p>WGANs can be used to train a generative model to produce synthetic examples from a target distribution P T using a small set of examples {x i } n i=1 sampled from this distribution. To explain the training process, we consider a generative model G parameterized by ? G , which produces samples x = G(z; ? G ) where z ? N (0, I ). Let P G be the probability distribution of the examples x generated by G. WGAN aims to minimize the Wasserstein Distance between the generator distribution P G and the target distribution P T by optimizing over the generator parameter ? G . The expression for the Wasserstein Distance between P T and P G is given by Eqn 13.</p><formula xml:id="formula_14">W P t , P ? = inf ? ??(P t ,P ? ) E (x,y)?? [?x -y ?]<label>(13)</label></formula><p>Here, ? P t , P ? denotes the set of all joint distributions ? for which P t and P ? are marginals. Unfortunately, computing the infimum in Eqn 13 is intractable as it involves searching through the space of all possible joint distributions ? . Instead, Arjovsky et al. <ref type="bibr" target="#b1">[2]</ref> derive an alternate formulation to measure the Wasserstein distance using Kantorovich-Rubinstein duality as shown in Eqn. 14.</p><formula xml:id="formula_15">W (P T , P G ) = sup ?f ? L ?1 E x ?P T [f (x)] -E x ?P G [f (x)]<label>(14)</label></formula><p>Here the supremum is taken over all 1-Lipschitz continuous functions f . To find a function f that satisfies Eqn 14, the authors consider a parameterizable critic function D w : X ? R. The parameters w of the critic function are chosen by solving the optimization problem shown in Eqn 15.</p><formula xml:id="formula_16">W (P T , P G ) = max w E x ?P T [D w (x)] -E z?N(0, I ) [D w (G(z))] (15)</formula><p>Thus, in order to generate realistic synthetic examples that are close to the target distribution, G is trained to minimize the estimate of Wasserstein Distance in Eqn 15. This can be achieved by maximizing the value of the critic function for the generator's examples using the loss function shown in Eqn 16. D is trained to solve the optimization problem in Eqn 15 by using the loss function in Eqn 17.</p><formula xml:id="formula_17">z ?N (0, I ); x ? {x i } n i=1 L G = -D w (G(z))<label>(16)</label></formula><formula xml:id="formula_18">L D = D w (G(z)) -D w (x)<label>(17)</label></formula><p>To ensure that D is K-Lipschitz continuous, the original WGAN paper proposed weight clipping. A later work by Gulrajani et al. <ref type="bibr" target="#b12">[13]</ref> proposed using gradient penalty to improve the stability of training, which we adopt in our proposal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Incorporating WGAN in MAZE</head><p>We describe the modifications to the training algorithm of MAZE (Algorithm. 1) that are necessary to incorporate WGAN training in the partial-data setting. In addition to the generator (G) and clone (C) models, we define a critic model D to estimate the Wasserstein distance. We modify the original loss function of G (Eqn 7) with an additional term as shown in Eqn 18.</p><formula xml:id="formula_19">x = G(z) : z ? N (0, I ) L G = -D K L (T (x)?C(x)) -?D(x)<label>(18)</label></formula><p>The first term in Eqn 18 represents the disagreement loss from MAZE and the second term is the WGAN loss. The hyper-parameter ? balances the relative importance between these two losses. To train the critic model D, we add an extra training phase (described by Algorithm 2) to our original training algorithm. This involves training the critic model for N d iterations to update its weights to maximize the objective function described in Eqn 15. We also include a gradient penalty term GP</p><formula xml:id="formula_20">= (?? x D(x)? 2 -1) 2 in L D to ensure that D is 1-Lipschitz continuous. Algorithm 2: Critic Training // Critic Training for i ? 0 to N d do z ? N (0, I ); x ? {x i } n i=1 L D = D(G(z)) -D(x) + GP ? D ? ? D -? D ? ? D L D end</formula><p>The training loops for the clone training and experience replay in Algorithm 1 remain unchanged. Using these modifications we can Normalized Clone Acc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-10</head><p>Figure <ref type="figure">7</ref>: Normalized clone accuracy of MAZE (data-free), MAZE-PD (partial-data), and JBDA (partial-data) as the query budget is varied. Our results show that for a given query budget, MAZE-PD can train a clone model with higher accuracy than MAZE.</p><p>The accuracy of MAZE-PD is also significantly than the JBDA attack.</p><p>train a generator model that produces inputs closer to the target distribution P T . These inputs are more effective in carrying out the task of model stealing and help improve the query efficiency of our attack. Thus, the availability of a few samples from the training data allows MAZE-PD to not only obtain higher accuracy but also to orchestrate a faster attack. We provide experimental results for MAZE-PD in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results: Clone Accuracy with Partial Data</head><p>We repeat the model stealing attack using MAZE-PD in the partial data setting. We assume that the attacker now has access to Comparison with JBDA: Additionally, we compare MAZE-PD with JBDA, which also operates in the partial-data setting. We report results using 100 seed examples derived from the training dataset for the JBDA attack. We find that the clone models obtained from JBDA have low accuracy for most datasets (less than 0.30? for SVHN, GTSRB, and CIFAR-10). In contrast, MAZE-PD obtains highly accurate clone models (0.97?-1.0?) across all four datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Impact on Query Budget</head><p>To understand the reduction in query budget with MAZE-PD, we compare the query budget necessary to reach a minimum normalized clone accuracy of 0.90? between MAZE-PD and MAZE in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Comparing Synthetic Images Generated by MAZE and MAZE-PD</head><p>Both MAZE and MAZE-PD use a generator to create synthetic images that are used to query the target model. MAZE produces these images without relying on any input data from the target model. On the other hand, MAZE-PD aims to use the available data to encourage the distribution of images produces by the generator P G to be closer to the target distribution P T using the WGAN loss to train the generator. Thus, MAZE-PD is expected to produce visually closer images to the distribution of the target model, and this is the reason for the increased accuracy and speed of MAZE-PD.</p><p>To better understand the quality of the images created by MAZE-PD we compare a small number of representative images that are produced by MAZE-PD with MAZE. Fig <ref type="figure" target="#fig_11">8</ref> shows four images produced by MAZE and MAZE-PD. We also show four representative images from the corresponding dataset. It can be seen that the images produced by MAZE-PD are visually similar to the images from the target distribution. For example, for FashionMNIST, the synthetic image produced by MAZE-PD resembles a garment. For GTSRB, it resembles a traffic sign. For SVHN, it resembles the number "8". And, for CIFAR-10, it resembles an automobile.</p><p>Furthermore, we can see a clear distinction between the foreground and the background for the images produced by MAZE-PD. However, such separation of foreground and background is absent in the synthetic images produced by MAZE. Thus, using the WGAN objective in the training of the GAN encourages the generator to produce synthetic images that are closer to the target distribution, likely resulting in MAZE-PD generating clones with higher accuracy using fewer queries.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>We summarize the related work in model stealing and contrast our proposed attack with prior research in this field. Additionally, as our work is closely related to Data-Free Knowledge Distillation, we describe two recent works and explain why they cannot be used to perform model stealing. Finally, we describe the related works in adversarial machine learning that also leverage zeroth-order optimization techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Model Stealing Attacks</head><p>Prior works on MS attacks typically rely on the availability of data in some form to query the target model. KnockoffNets <ref type="bibr" target="#b27">[28]</ref> assumes that the adversary has access to a suitable surrogate dataset that is similar to the data distribution of the target model. JBDA <ref type="bibr" target="#b29">[30]</ref> is another attack that assumes partial access to the data used to train the target model. This attack crafts synthetic examples from the available data that can be used to query the target model as explained in Section 2.1.</p><p>In the data-free setting, prior works have only investigated model stealing in a restricted setup where the attacker has some prior knowledge of the model architecture or the model complexity is limited. Roberts et al. <ref type="bibr" target="#b31">[32]</ref> investigate using inputs sampled from noise distributions to query the target model to perform model stealing on simple CNNs trained on MNIST and KMNIST. Tramer et al. <ref type="bibr" target="#b35">[36]</ref> propose using equation solving to determine the exact model parameters when the model architecture is known. However, these methods do not scale to more complex models and datasets.</p><p>To the best of our knowledge, MAZE is the first attack that can be used to train high accuracy clone models (0.91 -0.99? normalized clone accuracy) on complex DNN models trained for multiple image classification tasks. Furthermore, we show that if a small number of examples are available from the distribution of the target model, then our attack can become even more accurate and faster in such a partial-data setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Data-Free Knowledge Distillation</head><p>We discuss two recent works on data-free KD that are closely related to our proposal and also explain why these works cannot be used directly to perform model stealing as they require white-box access to the target model.</p><p>Adversarial Belief Matching (ABM) <ref type="bibr" target="#b23">[24]</ref>: ABM performs knowledge distillation by using images generated from a generative model G(z; ?). This generative model is trained to produce inputs x such that the predictions of the teacher model T (X ; ? T ) and the predictions of the student model S(x; ? S ) are dissimilar. Specifically, G is trained to maximize the KL-divergence between the predictions of the teacher and student model using the loss function shown in Eqn. 20. The student model, on the other hand, is trained to match the predictions of the teacher by minimizing the KL-divergence between the predictions of T and S using the loss function in Eqn. <ref type="bibr" target="#b20">21</ref>. By iteratively updating the generator and student model, ABM performs knowledge distillation between T and S.</p><formula xml:id="formula_21">x = G(z)<label>(19)</label></formula><formula xml:id="formula_22">L G = -D K L (T (x) ? S (x))<label>(20)</label></formula><formula xml:id="formula_23">L S = D K L (T (x) ? S (x))<label>(21)</label></formula><p>In addition to the basic idea presented above, ABM also uses an additional Attention Transfer <ref type="bibr" target="#b42">[43]</ref> term in the loss function of the student. AT tries to match the attention maps of the intermediate activations between the student and teacher networks.</p><p>The training process of the generator model in ABM assumes white-box access to the target model as the loss function of G (Eqn. 20) requires backpropagating through the target model T . Moreover, ABM also uses AT, which requires access to the intermediate activations of T . Due to these requirements, ABM cannot be directly used in the black-box setting of model stealing attacks.</p><p>Dreaming To Distill (DTD) <ref type="bibr" target="#b40">[41]</ref>: Given a DNN model T and a target class y, DTD proposes to use the loss function of T along with various image-prior terms to generate realistic images that resemble the images from the target class in the training distribution of T by framing it as an optimization problem as shown in Eqn. <ref type="bibr" target="#b21">22</ref>.</p><formula xml:id="formula_24">min x L(T (x), y)) + R p (x)<label>(22)</label></formula><p>In this equation, R p is the image prior regularization term which steers the synthetic image x away from unnatural images. The key insight of DTD is that the statistics of the batch norm layers (channel-wise mean and variance information of the activation maps for the training data) can be used to construct an image prior term that helps craft realistic inputs by performing the optimization in Eqn. 22 . In the knowledge distillation setting, the inputs generated using this technique can be used to train a student model S from the predictions of the teacher model T .</p><p>Note that, much like ABM, DTD also requires white-box access to the target model since the optimization problem in Eqn. 22 requires backpropagating through the teacher model. Furthermore, this work requires us to know the running mean and variance information stored in the batch norm layer, which is unavailable in the black-box setting of the model stealing attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">ZO Gradient Estimation in Adversarial ML</head><p>Zeroth-order gradient estimation is a commonly used technique to solve black-box optimization problems where the gradient information is unavailable. In adversarial machine learning, this technique has been used to craft adversarial examples in the black-box setting <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b37">38]</ref>. The goal of adversarial attacks is to cause misclassification by adding targeted perturbations to an input. E.g. given an input x, that belongs to class y and a target model T , we want to find a perturbation ? such that the perturbed input x ? = x + ? causes a misclassification in the target model such that T (x ? ) y. In blackbox settings, several attacks use zeroth-order gradient estimates to iteratively perturb x in order to find an adversarial example x ? that results in a misclassification.</p><p>Note that these attacks require an input from the target distribution that produces correct output and apply zeroth-order gradient estimation to transform this valid image to a malicious image that produces incorrect output, whereas, MAZE uses zeroth-order gradient estimation to train a generator without any data so that this generator can produce synthetic inputs to facilitate cloning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DEFENDING AGAINST MAZE</head><p>MAZE is the first data-free model stealing attack that can effectively produce high accuracy clone models for multiple vision-based DNN models. Several recent works have been proposed to defend against model stealing attacks. We discuss the applicability of these defenses against our attack and explain their limitations.</p><p>MAZE requires access to the prediction probabilities of T in order to estimate gradient information. Thus, a natural way to defend against it would be to limit access to the predictions of the model to the end-user by restricting the output of the model only to provide hard-labels. Such a defense would make it harder, although not necessarily impossible, for an adversary to estimate the gradient by using numerical methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b16">17]</ref>. Unfortunately, such a defense may limit benign users from using the prediction probabilities from the service for downstream processing tasks.</p><p>Another potential method to defend against MAZE is to prevent an adversary from accessing the true predictions of the model by perturbing the output probabilities with some noise. Several defenses have been proposed along these lines <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29]</ref>. Unfortunately, a key shortcoming of such a perturbation-based defense is that it can destroy information contained in the class probabilities that can be important for a benign user of the service for downstream processing tasks. Furthermore, such a defense can reduce classification accuracy for benign users, which is undesirable.</p><p>Yet another way to reduce the effectiveness of our attack may be to limit the number of queries that each user can make to the service. However, an adversary could circumvent such a defense by launching a distributed attack where the task of attacking the model is split across multiple users. Furthermore, limiting the number of queries may also constrain some of the legitimate users of the service from making benign queries, which may also be undesirable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>Commercially available machine learning models are often trained with significant resources and proprietary data, and are considered valuable intellectual property. Model Stealing (MS) attacks can allow the users of such proprietary models to copy the functionality of these models to a clone model. The efficacy of current MS attacks is heavily reliant on the availability of data from the target distribution or a representative distribution.</p><p>In this paper, we develop MAZE, a highly accurate MS attack that can be launched without any input data. To the best of our knowledge, MAZE is the first data-free MS attack that works effectively for complex DNN models trained across multiple image-classification tasks. MAZE uses a generator trained with zeroth-order optimization to craft synthetic inputs, which are then used to copy the functionality of the target model to the clone model. Our evaluations, with four datasets and complex models, show that MAZE produces clone models with high classification accuracy (0.91? to 0.99?). Despite not using any data, MAZE provides even higher accuracy than the recent attacks that rely on partial-data or surrogate-data. Thus, MAZE not only allows the attacker to launch the attack without data but also have higher accuracy for the attack.</p><p>In addition, we propose MAZE-PD to extend MAZE to the partialdata setting, where the adversary has access to a small number of examples from the target distribution. MAZE-PD uses generative adversarial training to produce inputs that are closer to the target distribution. This further improves accuracy (0.97? to 1.0?) and yields a significant reduction in the number of queries (2? to 24?) necessary to carry out the attack compared to MAZE.</p><p>Our work presents an important step towards developing highly accurate data-free model-stealing attack that works across multiple vision datasets. We hope that it serves as a baseline for future works to develop more efficient model stealing attacks in the datafree setting. Furthermore, while we develop and evaluate MAZE in the setting of model-stealing attacks, MAZE is a generalized framework, which enables knowledge distillation in the black-box setting, without requiring any input data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Step 1 :Figure 1 :</head><label>11</label><figDesc>Figure 1: Model stealing attacks: The target model is queried using a set of inputs {x i } n i=1 to obtain a labeled training dataset {x i , y i } n i=1 , which is used to train the clone model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 .</head><label>1</label><figDesc>Data-free setting (Primary goal): The adversary does not have access to the dataset D T used to train the target model or a good way to sample from the target data distribution P T . (Section 3) 2. Partial-data setting (Secondary goal): The adversary has access to a small subset (e.g., 100) of training examples randomly sampled from the training dataset of the target model. (Section 5) Clone Model (C) Target Model (T )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: MAZE Attack Setup: MAZE uses a generative model G to produce the synthetic input queries {x } to perform Model Stealing. The clone model C is trained to match the predictions of the target model T . G is trained to produce queries that maximize the dissimilarity between y T and y C . Optimizing L G requires backpropagation through T to update G. However, we only have black-box access to T , therefore we use zeroth-order gradient estimation to perform gradient descent on L G .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Normalized clone accuracy versus Query budget (Q) for various target models under MAZE attack. Clone accuracy improves with a higher query budget.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Normalized clone accuracy versus number of gradient directions (m) used in MAZE. Increasing m results in a lower gradient estimation error, but consumes more queries to update G, leaving fewer queries to train the clone model.</figDesc><graphic url="image-8.png" coords="8,77.82,172.63,192.20,128.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>For training, MAZE uses experience replay, in which the clone model is retrained on previously seen examples throughout the course of the attack. This is necessary to avoid catastrophic forgetting, wherein the clone model performs poorly on examples from the earlier part of the training process. Additionally, it also ensures that the generator does not produce redundant examples that are similar to the ones seen in the earlier part of the training. To understand the importance of experience replay, we carry out two versions of the attack: one with experience replay and the other without, and compare the accuracy of the respective clone models. The results of this study are shown in Fig. 5. Our results show that on average, experience replay improves the accuracy of the clone model by 7.3%. Thus, experience replay is an important component of MAZE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Impact of Experience Replay on MAZE. On average, Experience Replay improves clone accuracy by 7.3%.</figDesc><graphic url="image-9.png" coords="8,341.98,83.68,192.20,128.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Impact of gradient estimation error by comparing clone accuracy of MAZE and MAZE with perfect gradient information. On average, MAZE suffers a 3.8% accuracy loss due to error in estimating the gradient.</figDesc><graphic url="image-10.png" coords="8,341.98,532.35,192.20,128.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Comparing images produced by the generative models for (a) MAZE and (b) MAZE-PD with (c) actual training data. MAZE-PD produces images that are visually similar to the target distribution by including the WGAN objective to train the generative model, leading to an improvement in query efficiency over MAZE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison with prior works in Knowledge Distillation (KD) and model stealing attacks. MAZE enables data-free model stealing with black-box access to the target model. MAZE-PD extends MAZE to the partial-data setting. While we develop MAZE primarily for the data-free setting, we also propose an extension of MAZE, called MAZE-PD, for scenarios where a small partial dataset (e.g., 100 examples) is available to the attacker. MAZE-PD leverages the available data to produce queries that are closer to the training distribution than in MAZE by using generative adversarial training. Our evaluations show that MAZE-PD provides near-perfect clone accuracy (0.97? to 1.0?) while reducing the number of queries by 2?-24? compared to MAZE. Like MAZE, MAZE-PD also significantly outperforms existing stateof-the-art model stealing methods such as KnockoffNets</figDesc><table><row><cell></cell><cell>White-Box</cell><cell>Black-Box</cell></row><row><cell></cell><cell></cell><cell>KD (full-data)</cell></row><row><cell>Data</cell><cell>KD (full-data)</cell><cell>KnockoffNets (surrogate data) JBDA (partial data)</cell></row><row><cell></cell><cell></cell><cell>MAZE-PD (partial data)</cell></row><row><cell cols="2">No Data Data-Free KD</cell><cell>MAZE</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Dataset, model architecture, and accuracy of the target models T used in our experiments.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Target DNN Arch. (T ) Accuracy(%)</cell></row><row><cell>FashionMNIST</cell><cell>LeNet</cell><cell>91.04</cell></row><row><cell>SVHN</cell><cell>ResNet-20</cell><cell>95.25</cell></row><row><cell>GTSRB</cell><cell>ResNet-20</cell><cell>97.43</cell></row><row><cell>CIFAR-10</cell><cell>ResNet-20</cell><cell>92.26</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of clone accuracies obtained from various attacks. Numbers in the bracket express the accuracy as a multiple of the target model accuracy. MAZE obtains high accuracy (0.91? to 0.99?), despite not using any data.</figDesc><table><row><cell>Dataset</cell><cell>Target Accuracy (%)</cell><cell>MAZE (data-free)</cell><cell cols="3">KnockoffNets (surrogate data) (partial-data) (data-free) JBDA Noise</cell></row><row><cell>FashionMNIST</cell><cell>91.04</cell><cell>82.9 (0.91?)</cell><cell>47.26 (0.52?)</cell><cell cols="2">62.65 (0.69?) 37.59 (0.41?)</cell></row><row><cell>SVHN</cell><cell>95.25</cell><cell>94.32(0.99?)</cell><cell>92.77 (0.97?)</cell><cell>17.16(0.18?)</cell><cell>77.68 (0.82?)</cell></row><row><cell>GTSRB</cell><cell>97.43</cell><cell>88.31 (0.91?)</cell><cell>89.86 (0.92?)</cell><cell>12.80(0.13?)</cell><cell>18.61 (0.19?)</cell></row><row><cell>CIFAR-10</cell><cell>92.26</cell><cell>89.85 (0.97?)</cell><cell>82.56 (0.89?)</cell><cell cols="2">25.11 (0.27?) 10.00 (0.11?)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>100 random examples from the training data of the target model, which is roughly 0.2% of the total training data used to train the target model. We use ? = 10 in Eqn 18 and N d = 10 in Algorithm 2. Note that critic training does not require extra queries to the target model. The rest of the parameters are kept the same as before. We carry out the attack for 7 different query budgets Q ?</figDesc><table><row><cell>{0.625M, 1.25M, 2.5M, 5M, 10M, 20M, 30M }. Fig 7 shows the nor-</cell></row><row><cell>malized clone accuracy obtained with MAZE-PD and MAZE (data-</cell></row><row><cell>free). For a given query budget, MAZE-PD obtains a higher clone</cell></row><row><cell>accuracy compared to MAZE. Furthermore, MAZE-PD reaches</cell></row><row><cell>near-perfect clone accuracy (0.97?-1.0?).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Our results show that MAZE-PD offers a reduction of 2? to 24? in the query budget compared to MAZE. While we see a considerable reduction in query budget with just 100 examples, we expect this to reduce further with more training examples.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison of query budgets needed to reach a normalized clone accuracy of 0.90? with MAZE-PD and MAZE. MAZE-PD reduces the query budget by up to 24?.</figDesc><table><row><cell cols="4">Target Models MAZE MAZE-PD Reduction</cell></row><row><cell>FashionMNIST</cell><cell>&gt;30 M</cell><cell>2.5 M</cell><cell>&gt;12?</cell></row><row><cell>SVHN</cell><cell>1.25 M</cell><cell>0.675 M</cell><cell>2?</cell></row><row><cell>GTSRB</cell><cell>30 M</cell><cell>1.25 M</cell><cell>24?</cell></row><row><cell>CIFAR-10</cell><cell>30 M</cell><cell>10 M</cell><cell>3?</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>We found that improvement in accuracy stagnates beyond 6 augmentation rounds for the JBDA attack. This is in line with the observations made by Juuti et al.<ref type="bibr" target="#b17">[18]</ref>.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="9">ACKNOWLEDGEMENTS</head><p>We thank <rs type="person">M. Emre Gursoy</rs>, <rs type="person">Ryan Feng</rs>, <rs type="person">Poulami Das</rs> and <rs type="person">Gururaj Saileshwar</rs> for their feedback. We also thank <rs type="institution">NVIDIA</rs> for the donation of the Titan V GPU used for this research.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">There are no bit parts for sign bits in black-box attacks</title>
		<author>
			<persName><forename type="first">Abdullah</forename><surname>Al</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Dujaili</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Una-May O'</forename><surname>Reilly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06894</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Wasserstein gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Dream distillation: A data-independent model compression framework</title>
		<author>
			<persName><forename type="first">Kartikeya</forename><surname>Bhardwaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Marculescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07072</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Boundary attack++: Query-efficient decision-based adversarial attack</title>
		<author>
			<persName><forename type="first">Jianbo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02144</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models</title>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yash</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security</title>
		<meeting>the 10th ACM Workshop on Artificial Intelligence and Security</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="15" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Query-efficient hard-label black-box attack: An optimization-based approach</title>
		<author>
			<persName><forename type="first">Minhao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.04457</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Optimal rates for zero-order convex optimization: The power of two function evaluations</title>
		<author>
			<persName><surname>John C Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Michael I Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><surname>Wibisono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="2788" to="2806" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Gongfan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengchao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingli</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11006</idno>
		<title level="m">Data-Free Adversarial Distillation</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Model inversion attacks that exploit confidence information and basic countermeasures</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Ristenpart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security</title>
		<meeting>the 22nd ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1322" to="1333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Stochastic first-and zeroth-order methods for nonconvex stochastic programming</title>
		<author>
			<persName><forename type="first">Saeed</forename><surname>Ghadimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanghui</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="2341" to="2368" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Explaining and Harnessing Adversarial Examples</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6572" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of data</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Norvig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="8" to="12" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Matan</forename><surname>Haroush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01274</idno>
		<title level="m">The Knowledge Within: Methods for Data-Free Model Compression</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Blackbox adversarial attacks with limited queries and information</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.08598</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">PRADA: protecting against DNN model stealing attacks</title>
		<author>
			<persName><forename type="first">Mika</forename><surname>Juuti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Szyller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Marchal</surname></persName>
		</author>
		<author>
			<persName><surname>Asokan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="512" to="527" />
			<date type="published" when="2019">2019. 2019</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Defending Against Model Stealing Attacks with Adaptive Misinformation</title>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Kariyappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moinuddin K</forename><surname>Qureshi</surname></persName>
		</author>
		<idno>arXiv:stat.ML/1911.07100</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Defending against model stealing attacks using deceptive perturbations</title>
		<author>
			<persName><forename type="first">Taesung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Molloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00054</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfred</forename><forename type="middle">O</forename><surname>Hero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.07804</idno>
		<title level="m">Zeroth-order online alternating direction method of multipliers: Convergence analysis and applications</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neal</forename><forename type="middle">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology of learning and motivation</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="165" />
			<date type="published" when="1989">1989</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Zero-shot knowledge transfer via adversarial belief matching</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Micaelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amos</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9547" to="9557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Comprehensive privacy analysis of deep learning: Stand-alone and federated learning under passive and active white-box inference attacks</title>
		<author>
			<persName><forename type="first">Milad</forename><surname>Nasr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Houmansadr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00910</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Random gradient-free minimization of convex functions</title>
		<author>
			<persName><forename type="first">Yurii</forename><surname>Nesterov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Spokoiny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational Mathematics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="527" to="566" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2011 Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Knockoff nets: Stealing functionality of black-box models</title>
		<author>
			<persName><forename type="first">Tribhuvanesh</forename><surname>Orekondy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4954" to="4963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Prediction Poisoning: Towards Defenses Against DNN Model Stealing Attacks</title>
		<author>
			<persName><forename type="first">Tribhuvanesh</forename><surname>Orekondy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Practical black-box attacks against machine learning</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Berkay Celik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Asia conference on computer and communications security</title>
		<meeting>the 2017 ACM on Asia conference on computer and communications security</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="506" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Introduction to optimization. optimization software</title>
		<author>
			<persName><forename type="first">T</forename><surname>Boris</surname></persName>
		</author>
		<author>
			<persName><surname>Polyak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987. 1987</date>
			<publisher>Inc., Publications Division</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Model Weight Theft With Just Noise Inputs: The Curious Case of the Petulant Attacker</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinay</forename><surname>Uday Prabhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Mcateer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.08987</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Membership inference attacks against machine learning models</title>
		<author>
			<persName><forename type="first">Reza</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Stronati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congzheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="323" to="332" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<ptr target="http://arxiv.org/abs/1312.6199" />
	</analytic>
	<monogr>
		<title level="m">CoRR abs/1312</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page">6199</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Stealing machine learning models via prediction apis</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tram?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Juels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">K</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Ristenpart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th {USENIX} Security Symposium</title>
		<title level="s">{USENIX} Security</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="601" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tram??r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1704.03453" />
		<title level="m">The Space of Transferable Adversarial Examples</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Autozoom: Autoencoder-based zeroth order optimization method for attacking black-box neural networks</title>
		<author>
			<persName><forename type="first">Chun-Chen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paishun</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="742" to="749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Privacy risk in machine learning: Analyzing the connection to overfitting</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Yeom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Giacomelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE 31st Computer Security Foundations Symposium (CSF)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="268" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Pavlo</forename><surname>Hongxu Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhizhong</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Niraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.08795</idno>
		<title level="m">Dreaming to Distill: Data-free Knowledge Transfer via DeepInversion</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Knowledge Extraction with No Observable Data</title>
		<author>
			<persName><forename type="first">Jaemin</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minyong</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taebum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2701" to="2710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03928</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide residual networks</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Yuheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoxi</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengzhi</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07135</idno>
		<title level="m">The Secret Revealer: Generative Model-Inversion Attacks Against Deep Neural Networks</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
