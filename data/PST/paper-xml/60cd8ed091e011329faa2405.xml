<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Embedding-based Product Retrieval in Taobao Search</title>
				<funder ref="#_Ve8kT6V">
					<orgName type="full">Alibaba Innovative Research</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-06-17">17 Jun 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sen</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fuyu</forename><surname>Lv</surname></persName>
							<email>fuyu.lfy@alibaba-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Taiwei</forename><surname>Jin</surname></persName>
							<email>taiwei.jtw@alibaba-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Guli</forename><surname>Lin</surname></persName>
							<email>guli.lingl@alibaba-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Keping</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiaoyi</forename><surname>Zeng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Hong Kong Polytechnic University Hong Kong</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qianli</forename><surname>Ma</surname></persName>
							<email>qianlima@scut.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiao- Ming</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Hong Kong Polytechnic University Hong Kong</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group Hangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Embedding-based Product Retrieval in Taobao Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-06-17">17 Jun 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3447548.3467101</idno>
					<idno type="arXiv">arXiv:2106.09297v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Embedding-based retrieval system; E-commerce search</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Nowadays, the product search service of e-commerce platforms has become a vital shopping channel in people's life. The retrieval phase of products determines the search system's quality and gradually attracts researchers' attention. Retrieving the most relevant products from a large-scale corpus while preserving personalized user characteristics remains an open question. Recent approaches in this domain have mainly focused on embedding-based retrieval (EBR) systems. However, after a long period of practice on Taobao, we find that the performance of the EBR system is dramatically degraded due to its: (1) low relevance with a given query and (2) discrepancy between the training and inference phases. Therefore, we propose a novel and practical embedding-based product retrieval model, named Multi-Grained Deep Semantic Product Retrieval (MGDSPR). Specifically, we first identify the inconsistency between the training and inference stages, and then use the softmax cross-entropy loss as the training objective, which achieves better performance and faster convergence. Two efficient methods are further proposed to improve retrieval relevance, including smoothing noisy training data and generating relevance-improving hard negative samples without requiring extra knowledge and training procedures. We evaluate MGDSPR on Taobao Product Search with significant metrics gains observed in offline experiments and online A/B tests. MGDSPR has been successfully deployed to the existing multi-channel retrieval system in Taobao Search. We also introduce the online deployment scheme and share practical lessons of our retrieval system to contribute to the community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Information systems ? Retrieval models and ranking.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Nowadays, online shopping has become a daily habit in people's lives. The top E-commerce shopping platforms (such as eBay, Amazon, Taobao, and JD) have hundreds of millions of daily active users and thus facilitate billion-level transaction records <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34]</ref>. Therefore, product search engines are designed to discover products that satisfy users, which is also our working goal. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, our search system uses the "match-prerank-rank-rerank" architecture to screen and sort thousands of products from billions of candidates to possess controllable and high-efficiency characteristics. We finally return dozens of products to display to users. Obviously, the match (retrieval) phase plays an important role in determining the quality of the item candidate set fed to the followup ranking stage. The problem gradually receives more and more attention from academia and industry.</p><p>Search retrieval in e-commerce poses different challenges than in web (document) search: the text in e-commerce is usually shorter and lacks grammatical structure, while it is important to consider the massive historical user behaviors <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. The lexical matching engine (typically an inverted index <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b37">38]</ref>), despite its widely criticized semantic gap issue <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31]</ref>, remains a vital part of current retrieval systems due to its reliability and controllability of search relevance (exactly matching query terms). However, it hardly distinguishes users' interests in the same query and cannot flexibly capture user-specific characteristics. Hence, how to effectively retrieve the most relevant products satisfying users while considering the relationship between query semantics and historical user behaviors is the main challenge facing e-commerce platforms.</p><p>With the development of deep learning <ref type="bibr" target="#b34">[35]</ref>, Amazon <ref type="bibr" target="#b20">[21]</ref> and JD <ref type="bibr" target="#b33">[34]</ref> built their respective two-tower embedding-based retrieval (EBR) systems to provide relevant or personalized product candidates in their e-commerce search engines. Both of them reported the success of EBR without further discussing its low controllability of search relevance (compared to the lexical matching engine). We have also built an EBR system that can dynamically capture the relationship between query semantics and personalized user behaviors, and launched it on Taobao 1 Product Search for quite a long time. In the first deployment, it can achieve a good improvement in various metrics. However, after long observation, we have found that the embedding-based method's controllability of relevance is relatively low due to the inability to exactly matching query terms <ref type="bibr" target="#b10">[11]</ref>, resulting in increasing user complaints and bad cases that cannot be fixed. To improve its controllability of relevance (i.e., resolving bad cases), we have adopted a relevance control module to filter the retrieved products. The control module only keeps those products that meet the relevance standards of exact matching signals and feed them to the follow-up ranking module. However, we statistically find it usually filters out thirty percent of candidates due to the low relevance of retrieved products. It is quite a waste of online computing resources because the filtered products cannot participate in the ranking stage, thus degrading the EBR system's performance. Therefore, the practical challenge for our search system is to enable the embedding-based model to retrieve more relevant products and increase the number of participants in the subsequent ranking stage.</p><p>Moreover, random negative samples are widely used to train large-scale deep retrieval models to ensure the sample space in training is consistent with that of the inference phase <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b33">34]</ref>. Nevertheless, there remains a discrepancy in existing e-commerce product search methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b33">34]</ref> due to the inconsistent behavior between the training and inference stages. Specifically, during inference, the model needs to select the top-? products closest to the current query from all candidates, requiring the ability for global comparison. However, <ref type="bibr" target="#b20">[21]</ref> and <ref type="bibr" target="#b33">[34]</ref> both adopt hinge (pairwise) loss as the training objective, which can only do local comparison.</p><p>This paper introduces the design of the proposed Multi-Grained Deep Semantic Product Retrieval (MGDSPR) model, its effect on each stage of the search system, and the lessons learned from applying it to product search. To tackle the above problems, we first use the softmax cross-entropy loss as the training objective to equip the model with global comparison ability, making training and inference more consistent. We further propose two effective methods without extra training procedures to enable MGDSPR to retrieve more relevant products. Specifically, we smooth the relevance noise introduced by using user implicit feedback (i.e., click data) logs as training data <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b30">31]</ref> by including a temperature parameter to the softmax function. Also, we mix the positive and random negative samples to generate relevance-improving hard negative samples. Moreover, we adapt the relevance control module to enhance the EBR system's controllability of search relevance. The effectiveness of MGDSPR is verified by an industrial dataset collected from the Taobao search system and online A/B tests.</p><p>The main contributions of this work are summarized as follows:</p><p>1 https://www.taobao.com/ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Deep Matching in Search</head><p>With the booming interest in deep NLP techniques, various neural models have been proposed to address the semantic gap problem raised by traditional lexical matching in the last few years. Those approaches fall into two categories: representation-based learning and interaction-based learning. The two-tower structure is the typical characteristic of representation-based models, such as DSSM <ref type="bibr" target="#b14">[15]</ref>, CLSM <ref type="bibr" target="#b24">[25]</ref>, LSTM-RNN <ref type="bibr" target="#b21">[22]</ref>, and ARC-I <ref type="bibr" target="#b12">[13]</ref>. Each tower uses a siamese/distinct neural network to generate semantic representations of query/document. Then a simple matching function (e.g., inner product) is applied to measure the similarity between the query and document. Interaction-based methods learn the complicated text/relevance patterns between the query and document. Popular models include MatchPyramid <ref type="bibr" target="#b22">[23]</ref>, Match-SRNN <ref type="bibr" target="#b27">[28]</ref>, DRMM <ref type="bibr" target="#b10">[11]</ref>, and K-NRM <ref type="bibr" target="#b31">[32]</ref>. Other than semantic and relevance matching, more complex factors/trade-offs, e.g., user personalization <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b9">10]</ref> and retrieval efficiency <ref type="bibr" target="#b4">[5]</ref>, need to be considered when applying deep models to a large-scale online retrieval system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Retrieval in Industry Search</head><p>Representation-based models with an ANN (approximate near neighbor) algorithm have become the mainstream trend to efficiently deploy neural retrieval models in industry. For social networks, Facebook developed an EBR system to take text matching and searcher's context into consideration <ref type="bibr" target="#b13">[14]</ref>. They introduced various tricks and experiences (e.g., hard mining, ensemble embedding, and inverted index-based ANN) to achieve hybrid retrieval (fuzzy matching). For display advertising, Baidu proposed MOBIUS <ref type="bibr" target="#b7">[8]</ref> for CPM (cost per mile) maximization in the web ads retrieval phase, reducing the objective distinction between ranking and matching. For web search, Google <ref type="bibr" target="#b29">[30]</ref> adopted transfer learning to learn semantic embeddings from data in recommendation systems to alleviate the cold start problem. Due to more text features and fewer user behaviors, their search scenarios are characterized by strong semantic matching and weak personalization. For e-commerce search, Amazon developed a two-tower model to address the semantic gap issue in a lexical matching engine for semantic product retrieval <ref type="bibr" target="#b20">[21]</ref>,</p><p>where one side uses n-gram query features and the other side exploits item features, without considering user personalization. Recently, JD <ref type="bibr" target="#b33">[34]</ref> proposed a deep personalized and semantic retrieval model (DPSR) to combine text semantics and user behaviors. However, DPSR aggregates user behaviors through average pooling, weakening personalization characteristics. Furthermore, neither Amazon nor JD studies the problem of insufficient product relevance caused by the EBR method. This paper will discuss the low relevance issue of the EBR system encountered in Taobao Product Search and propose our solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MODEL</head><p>Here, we introduce our model called Multi-Grained Deep Semantic Product Retrieval (MGDSPR) to simultaneously model query semantics and historical behavior data, aiming at retrieving more products with good relevance. The general structure of MGDSPR is illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. We first define the problem and then introduce our design of the two-tower model, including the user tower and the item (product) tower. Finally, we elaborate on the training objective and proposed methods to retrieve more relevant products.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>We first formulate the e-commerce product retrieval problem and our solution as well as the notations used in this paper. Let U = {? 1 , ..., ? ? , ..., ? ? } denote a collection of ? users, Q = {? 1 , ..., ? ? , ..., ? ? } denote the corresponding queries, and I = {? 1 , ..., ? ? , ..., ? ? } denote a collection of ? items (products). Also, we divide the user ?'s historical behaviors into three subsets according to the time interval from the current time ?: real-time (denoted as R ? = {? ? 1 , ..., ? ? ? , ..., ? ? ? }, before the current time step), short-term (denoted as S ? = {? ? 1 , ..., ? ? ? , ..., ? ? ? }, before R and within ten days) and long-term sequences (denoted as L ? = {? ? 1 , ..., ? ? ? , ..., ? ? ? }, before S and within one month), where ? is the length of the sequence.</p><p>We now define the task. Given the historical behaviors (R ? , S ? , L ? ) of a user ? ? U, after he/she submits query ? ? at time ?, we would like to return a set of items ? ? I that satisfy his/her search request. Typically, we predict top-? item candidates from I at time ? based on the scores ? between the user (query, behaviors) and items, i.e.,</p><formula xml:id="formula_0">? = F (? (? ? , R ? , S ? , L ? ),? (?)),<label>(1)</label></formula><p>where F (?), ? (?), ? (?) denote the scoring function, query and behaviors encoder, and item encoder, respectively. Here, we adapt the two-tower retrieval model for efficiency. We instantiate F with the inner product function. In the following, we introduce the design of the user and item towers, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">User Tower</head><formula xml:id="formula_1">? R 1?? , ? ? ? R 1??</formula><p>and ? ? ? R 1?? , we can obtain its six granular representations ? ??? ? R 6?? . It is done by concatenating ? ? 's unigram meanpooling ? 1_???? ? R 1?? , 2-gram mean-pooling ? 2_???? ? R 1?? , word segmentation mean-pooling ? ??? ? R 1?? , word segmentation sequence ? ???_??? ? R 1?? , historical query words ? ???_??? ? R 1?? , and mixed ? ??? ? R 1?? representations. ?, ?, and ? denote the embedding size, the number of word segmentation, and the number of words in each segment, respectively. Formally, the Multi-Granular Semantic representation ? ??? is obtained as follows:</p><formula xml:id="formula_2">? 1_???? = ????_???????(? 1 , ..., ? ? ),<label>(2)</label></formula><formula xml:id="formula_3">? 2_???? = ????_???????(? 1 ? 2 , ..., ? ?-1 ? ? ),<label>(3)</label></formula><formula xml:id="formula_4">? ??? = ????_???????(? 1 , ..., ? ? ),<label>(4)</label></formula><formula xml:id="formula_5">? ???_??? = ????_???????(???(? 1 , ..., ? ? )),<label>(5)</label></formula><formula xml:id="formula_6">? ???_??? = ?? ? ???? (? ??? ? (? ??? ) ? )? ??? ,<label>(6)</label></formula><formula xml:id="formula_7">? ??? = ? 1_???? + ? 2_???? + ? ??? + ? ???_??? + ? ???_??? ,<label>(7)</label></formula><formula xml:id="formula_8">? ??? = ?????? (? 1_???? , ? 2_???? , ? ??? , ? ???_??? , ? ???_??? , ? ??? ),<label>(8)</label></formula><p>where ???, ????_???????, and ?????? denote the Transformer <ref type="bibr" target="#b26">[27]</ref>, average, and vertical concatenation operation, respectively. We average all the outputs of the last layer of the Transformer in Eq. ( <ref type="formula" target="#formula_5">5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">User Behaviors Attention.</head><p>User behaviors are recorded by their history of items clicked (or bought). Taking user ?'s shortterm behaviors S ? as an example, ? ? ? ? S ? denotes the user clicks on item ? at time ?, and each item ? is described by its ID and side information F (e.g., leaf category, first-level category, brand and, shop) <ref type="bibr" target="#b17">[18]</ref>. Specifically, each input item ? ? ? ? S ? is defined by:</p><formula xml:id="formula_9">? ? ? = ? ? ? ? ? ? ,<label>(9)</label></formula><formula xml:id="formula_10">? ? ? = ?????? ({? ? ? |? ? F }),<label>(10)</label></formula><p>where ? ? is the embedding matrix and ? ? ? is a one-hot vector. ? ? ? ? R 1?? ? is the corresponding embedding vector of size ? ? and ? denotes matrix multiplication. We concatenate the embeddings of item ?'s ID and side information in Eq. <ref type="bibr" target="#b9">(10)</ref>. We use the same way to embed items in real-time R ? and long-term L ? sequences.</p><p>Unlike the target-item attention <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref> used in advertising and recommendation, here we use query attention to capture user history behaviors related to the current query semantics. Moreover, inspired by <ref type="bibr" target="#b1">[2]</ref>, we put an all-zero vector into user behavior data to remove potential noise and deal with situations where the historical behaviors may not be related to the current query. In the following, we introduce the fusion of real-time, short-term, and long-term sequences, respectively.</p><p>For real-time sequences R ? = {? ? 1 , ..., ? ? ? , ..., ? ? ? }, we apply Long Short-Term Memory (LSTM) <ref type="bibr" target="#b11">[12]</ref> to capture the evolution and collect all hidden states R ? ???? = {? ? 1 , ..., ? ? ? , ..., ? ? ? }. Next, we use multi-head self-attention to aggregate multiple potential points of interest <ref type="bibr" target="#b17">[18]</ref>   ? in the attention mechanism), which is defined by:</p><formula xml:id="formula_11">? ???? = ?? ? ???? (? ??? ? ? ? ????_??? ) ? ? ? ????_??? .<label>(11)</label></formula><p>For short-term sequences S ? = {? ? 1 , ..., ? ? ? , ..., ? ? ? }, we apply multihead self-attention to aggregate S ? into S ? ??? ? _??? = {? ? 1 , ..., ? ? ? , ..., ? ? ? }. We add a zero vector at the first position of S ? ??? ? _??? , resulting in S ? ????_??? = {0, ? ? 1 , ..., ? ? ? , ..., ? ? ? } ? R (? +1)?? . Finally, the shortterm personalized representation ? ????? ? R 6?? is defined by:</p><formula xml:id="formula_12">? ????? = ?? ? ???? (? ??? ? ? ? ????_??? ) ? ? ? ????_??? .<label>(12)</label></formula><p>The real-time and short-term sequences are composed of click sequences.</p><p>We use four attribute behaviors to describe the long-term sequence (within one month), including item (L ? ???? ), shop (L ? ???? ), leaf category (L ? ???? ) and brand (L ? ????? ). Each attribute behavior is described by a user's click, buy and collecting actions. For example, L ? ???? consists of multiple action sequences: L ?????_???? , L ???_???? and L ??????? _???? . Entries in each action sequence are embedded by Eq. ( <ref type="formula" target="#formula_9">9</ref>) and aggregated into a vector through meanpooling with consideration of quick response in online environment, resulting in L ? ???? = {0, ? ????? , ? ??? , ? ??????? }. The representation of item attribute behavior ? ?_???? ? R 6?? is then defined by:</p><formula xml:id="formula_13">? ?_???? = ?? ? ???? (? ??? ? ? ? ???? ) ? ? ? ???? .<label>(13)</label></formula><p>Finally, the long-term personalized representation ? ???? ? R 6?? is defined as follows:</p><formula xml:id="formula_14">? ???? = ? ?_???? + ? ?_???? + ? ?_???? + ? ?_????? ,<label>(14)</label></formula><p>where ? ?_???? , ? ?_???? , and ? ?_????? denote the representation of the attribute behaviors of shop, leaf category, and brand respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Fusion of Semantics and Personalization.</head><p>To retrieve products relevant to the current user's query and preserve personalized characteristics, we take the multi-granular semantic representation ? ??? and personalized representations (? ???? , ? ????? , ? ???? ) as the input of self-attention to dynamically capture the relationship between the two. Specifically, we add a "[CLS]" token at the first position of the input ? = {[???], ? ??? , ? ???? , ? ????? , ? ???? } of selfattention and regard the output as the user tower's representation ? ?? ? R 1?? , which is defined as follows:</p><formula xml:id="formula_15">? ?? = ??? ? _??? ? ???? ([[???], ? ??? , ? ???? , ? ????? , ? ???? ]). (<label>15</label></formula><formula xml:id="formula_16">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Item Tower</head><p>For the item tower, we experimentally use item ID and title to obtain the item representation ? ???? . Given the representation of item ?'s ID, ? ? ? R 1?? , and its title segmentation result ? ? = {? ? 1 , ..., ? ? ? }, ? ???? ? R 1?? is calculated as follows:</p><formula xml:id="formula_17">? ???? = ? + ????(? ? ? ? ?=1 ? ? ? ),<label>(16)</label></formula><p>where ? ? is the transformation matrix. We empirically find that applying LSTM <ref type="bibr" target="#b11">[12]</ref> or Transformer <ref type="bibr" target="#b26">[27]</ref> to capture the context of the title is not as effective as simple mean-pooling since the title is stacked by keywords and lacks grammatical structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Loss Function</head><p>To make the sample space where the model is trained consistent with that of online inference, Huang et al. <ref type="bibr" target="#b13">[14]</ref>, Nigam et al. <ref type="bibr" target="#b20">[21]</ref>, and Zhang et al. <ref type="bibr" target="#b33">[34]</ref> use random samples as negative samples. However, they use pairwise (hinge) loss as the training objective, making training and testing behavior inconsistent. Specifically, during inference, the model needs to pick the top-? items that are closest to the current query from all candidates, which requires the model to have the ability of global comparison. However, hinge loss can only do local comparison. Also, hinge loss introduces a cumbersome tuning margin, which has a significant impact on performance <ref type="bibr" target="#b13">[14]</ref>. Here, we adapt the softmax cross-entropy loss as the training objective, achieving faster convergence and better performance without additional hyper-parameter tuning. Given a user ? and his/her query ? ? , the positive item ? + is the item clicked by ? under ? ? . The training objective is defined by:</p><formula xml:id="formula_18">?(? + |? ? ) = exp(F (? ? , ? + )) ? ? ?? exp(F (? ? , ? ? )) ,<label>(17)</label></formula><formula xml:id="formula_19">?(?) = - ?? ? ?? ? ? log( ?? ),<label>(18)</label></formula><p>where F , ? , ? + , and ? ? denote the inner product, the full item pool, the item tower's representation ? ???? , and the user tower's representation ? ?? , respectively. Note that Eq. ( <ref type="formula" target="#formula_18">17</ref>) endows the model with global comparison ability. The softmax involves calculating an expensive partition function, which scales linearly to the number of items. In practice, we use sampled softmax (an unbiased approximation of full-softmax) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref> for training. Similar to <ref type="bibr" target="#b33">[34]</ref>, we also experimentally find that using the same set of random negative samples for every training example in the current batch results in similar performance as using a different set for each one. We adopt the former training method to reduce computing resources.</p><p>To improve the EBR system's relevance in retrieval and increase the number of products participating in the follow-up ranking stage while maintaining high efficiency, we propose two efficient methods without relying on additional knowledge to make our model retrieve more relevant products.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Smoothing Noisy Training Data</head><p>. In e-commerce search, users' click and purchase records are used as supervisory signals to train a model. However, these signals are noisy since they are influenced not only by query-product relevance but also by images, prices, and user preferences <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b30">31]</ref>. Hence, we introduce a temperature parameter ? into softmax to smooth the overall fitted distribution of the training data. If ?-&gt;0, the fitted distribution is close to onehot distribution, which means that the model completely fits the supervisory signals. The model will be trained to push positive items far away from negative ones, even if the relevance of a positive item is low. If ?-&gt;?, the fitted distribution is close to a uniform distribution, indicating that the model does not fit the supervisory signals at all. We can increase ? to reduce the noise in training data and thus alleviate the impact of low relevance caused by fully fitting users' click records, which does not require additional knowledge and is verified by our experiments. Formally, the softmax function with the temperature parameter ? is defined as follows:</p><formula xml:id="formula_20">?(? + |? ? ) = exp(F (? ? , ? + )/?) ? ? ?? exp(F (? ? , ? ? )/?) . (<label>19</label></formula><formula xml:id="formula_21">)</formula><p>3.4.2 Generating Relevance-improving Hard Negative Samples. Unlike prior works <ref type="bibr" target="#b19">[20]</ref> that require additional annotated training data and training process, we propose a method to generate relevanceimproving hard negative samples in the embedding space. Specifically, given a training example (? ? , ? + , ? -), where ? -denotes a set of random negative samples sampled from item pool ? . For simplicity, we use ? ? , ? + , and ? -to refer to their respective representations. We first select the negative items of ? -that have the top-? inner product scores with ? ? to form the hard sample set ? ???? , and then mix ? + ? R 1?? and ? ???? ? R ? ?? by interpolation to obtain the generated sample set ? ??? ? R ? ?? , which is defined as follows:</p><formula xml:id="formula_22">? ??? = ?? + + (1 -?)? ???? ,<label>(20)</label></formula><p>where ? ? R ? ?1 is sampled from the uniform distribution ? (?, ?) (0 ? ? &lt; ? ? 1). The closer ? is to 1, the closer the generated sample is to the positive samples ? + in the embedding space, indicating the harder the generated sample is. We take ? ??? as the set of relevanceimproving hard negative samples and include it in the denominator of the softmax function to make the model distinguish the positive sample ? + and its nearby samples. Formally, the softmax function with relevance-improving hard samples ? ??? is defined as follows:</p><formula xml:id="formula_23">?(? + |? ? ) = exp(F (? ? , ? + )/?) ? ? ?? ?? ??? exp(F (? ? , ? ? )/?) . (<label>21</label></formula><formula xml:id="formula_24">)</formula><p>Note that we can tune the maximum ? and minimum ? of the uniform distribution ? to determine the "hardness" of the generated relevance-improving negative samples. This generation process only needs a linear interpolation after calculating the inner product scores between the current query ? ? and the negative samples of ? -, which is quite efficient. As illustrated in Figure <ref type="figure" target="#fig_2">3</ref>, at a high level, the Taobao search engine works as follows: a user issues a query, which triggers a multi-channel retrieval system, producing an unordered candidate set without duplication. Before the most relevant items are finally displayed to users, the candidates are passed through multi-stages of ranking, including pre-ranking, relevance ranking (removing products that are inconsistent with the predictions of the query's category), ranking, re-ranking, and mix-ranking. Our embeddingbased retrieval module is the third matching channel as a supplement to the existing two-channel retrieval. In the following, we introduce how to deploy MGDSPR in the production environment once we have trained the model and the relevance control module on the EBR system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SYSTEM ARCHITECTURE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Offline Training and Indexing</head><p>We use search logs in the past one week to train the model by distributed Tensorflow <ref type="bibr" target="#b0">[1]</ref> and update the model parameters daily. Note that we do not use sequential training <ref type="bibr" target="#b32">[33]</ref> to do the A/B test. Since the base model has been trained by lots of data (several months or even one year), it is difficult to catch up with the same volume of data for the new testing model. As illustrated in Figure <ref type="figure" target="#fig_3">4</ref>, the deployment system of MGDSPR is an offline to online architecture. At the offline phrase, build service optimizes and constructs a user/query network extracted from the user tower, which is passed to real-time prediction platform. All the item embeddings are simultaneously exported from the item tower and transmitted to an approximate near neighbor (ANN) indexing system. The total number of items is about one hundred millions. They are placed in multiple columns (6 in our system) because of the enormous amounts. Each column of the ANN builds indexes of embeddings by HC (hierarchical clustering) algorithm with K-means and INT8 quantization to promote storage and search efficiency. The training sample size is 4 million for HC, and the max scan ratio is 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Online Serving</head><p>The user/query network and item embedding indexes are published in an online environment after offline indexing. When a user issues a query, user history behaviors and the query are fed into a real-time prediction platform for online inference. The ANN search module then distributively seeks top-? (? = 9600 in our system) results from indexes of multi-columns (referred to as ? = 6 columns). Each column returns the same size of ?/?. The indexing retrieval accuracy is 98% accompanied with 10 milliseconds of retrieval latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Relevance Control</head><p>After a long period of practice, we find that although embeddingbased retrieval has advantages in personalization and fuzzy matching, it often leads to more search bad cases due to lack of exact matching [? ] to the key terms of a query. The key terms of a query are referred to as words of brand, type, color, etc., which are significant to product search relevance. For instance, a user is searching for Adidas sports shoes. Items of Nike sports shoes are similar to the query in the embedding space and hence will appear in the top-? results with high probability. However, this is not the user intent and will harm user experience. Hence, we add an inverted index based boolean matching module on top of the ANN results. Boolean matching aims to filter out items that do not contain key query terms in their titles. The final search results can then be expressed as:</p><p>(ANN results) and (Brand: Adidas) and (Category: Shoes).</p><p>Generally, we predefine the rule of key terms according to query understanding, e.g., words of brand, color, style, and audience. Note that Facebook <ref type="bibr" target="#b13">[14]</ref> uses an embedding-based method to enhance boolean expression and achieves fuzzy matching, while we use boolean matching to improve retrieval relevance of the EBR system. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>Here, we introduce evaluation metrics, implementation details, datasets, and offline and online experimental results of our method including its effect in the search system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation Metrics</head><p>5.1.1 Offline Evaluation. We use the metric of Recall@? to evaluate the offline performance. Specifically, given a query ? ? , the items clicked or purchased by the user ? are regarded as the target set ? = {? 1 , ..., ? ? }, and the top-? items returned by a model are regarded as the retrieval set ? = {? 1 , ..., ? ? }. Recall@? is defined as</p><formula xml:id="formula_25">??????@? = ? ?=1 ? ? ? ? ? .<label>(22)</label></formula><p>Empirically, during the retrieval phase, we find that the AUC metric has no positive correlation with the online Gross Merchandise Volume (GMV) metric, while the recall metric does. Also, we add ? ??? , the records relevant to the current query that were not purchased in search but elsewhere (e.g., recommender system) in Taobao Mobile App, to the testing set.</p><p>In Taobao search, we also pay attention to the relevance of retrieved products (related to user experience). Due to the large amount of test data, we use an online well-trained relevance model (its AUC for human-labeled data is 0.915) instead of expensive human evaluation to calculate the proportion of products with good relevance (abbreviated as good rate and denoted as ? ???? ) in the retrieval set ? , which is defined as</p><formula xml:id="formula_26">? ???? = ? ?=1 I(? ? ) ? ,<label>(23)</label></formula><p>where I(?) is an indicator function. When item ? is rated as good by the relevance model, the function value is 1, otherwise 0. It is not appropriate to use the AUC metric to evaluate whether our model can return more products with good relevance because it evaluates the order of the set elements rather than the number of the "good" elements. ? is experimentally set to be 1, 000. Meanwhile, to analyze the effect of our model on each stage of the search system, we also count the number of items in the retrieval set ? that participate in each follow-up stage. Given a retrieval set ? = {? 1 , ..., ? ? }, every time it goes through a stage (such as the relevance control module, pre-ranking, and ranking), the number of items will decrease, resulting in ? ?? ? ? = {? 1 , ..., ? ? }, ? &lt; ?. Therefore, we calculate the number of items in ? ?? ? ? after going through each phase, and use ??? ????? and ??? ???? to denote the number of items that enter the pre-ranking and ranking stages. For a total retrieval set I = {? 1 , ..., ? ? , ..., ? ? } of ? queries, the calculation of ??? ????? and ??? ???? are averaged by ? . 5.1.2 Online Evaluation. We consider the most important online metrics: GMV, P good , and P h_good . GMV is the Gross Merchandise Volume, which is defined as</p><formula xml:id="formula_27">??? = #pay amount. (<label>24</label></formula><formula xml:id="formula_28">)</formula><p>In addition to the amount of online income, we also consider user search experience by the P good and P h_good metrics (defined in Eq. ( <ref type="formula" target="#formula_26">23</ref>)). Precisely, both ? ???? and ? ?_???? calculate the good rate of the item set displayed to users, but ? ???? is determined by the relevance model while ? ?_???? is determined by humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation Details</head><p>The maximum length ? of real-time, short-term, and long-term sequences are 50, 100, and 100, respectively. We use attention with a mask to calculate those sequences whose length is less than ? . The dimensions of the user tower, item tower, behavior sequence, and hidden unit of LSTM are all set to 128. The batch size is set to 256. We use LSTM of two layers with dropout (probability 0.2) and residual network <ref type="bibr" target="#b18">[19]</ref> between vertical LSTM stacks. The number of heads in self-attention is set to 8. The parameters ? and ? of uniform distribution ? and the number of generated samples ? are set to 0.4, 0.6 and 684, respectively. The temperature parameter ? of softmax is set to 2. All parameters are orthogonally initialized and learned from scratch. The experiments are run on the distributed TensorFlow platform <ref type="bibr" target="#b0">[1]</ref> using 20 parameter servers and 100 GPU (Tesla P100) workers. The AdaGrad optimizer <ref type="bibr" target="#b6">[7]</ref> is employed with an initial learning rate of 0.1, which can improve the robustness of SGD for training large-scale networks <ref type="bibr" target="#b5">[6]</ref>. We also adopt gradient clip when the norm of gradient exceeds a threshold of 3. The training process converges at about 35 million steps for about 54 hours. The training set comprises samples from the first 7 consecutive days (a total of 4.7 billion records). For evaluation, we randomly sample 1 million search records ? and 0.5 million purchase logs ? ??? from the recommender system in the 8-th day. We have also tried to extend the timeframe of training data to 10 days, but there is no significant benefit, indicating billions of data can effectively prevent the model from overfitting. The size of the candidate item set is consistent with the online environment, i.e., about 100 million.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Online</head><p>Dataset. We deploy a well-trained MGDSPR in the Taobao search production environment containing hundreds of millions of user query requests. The size of the item candidate set is about 100 million, covering the most active products at Taobao. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Offline Experimental Results</head><p>Previously, our embedding-based retrieval system adopts the DNN architecture proposed in <ref type="bibr" target="#b4">[5]</ref>, but uses more user behaviors and statistical features (inherited from the ranking model), which has been experimentally verified to be effective to some extent. Specifically, we concatenate the vectors of user behaviors (obtained by meanpooling) and statistical features (e.g., Unique Visitor (UV), Item Page View (IPV)) and feed it into a multi-layer feed-forward neural network. We refer to it as a strong baseline ?-DNN. In addition, adding statistical features to MGDSPR has no benefit in the metric of ??????, so we delete them but keep the user behavior sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Comparison with the Strong Baseline.</head><p>As mentioned in Section 5.1.1, we report the metrics of Recall@?, good rate, and ??? ????? . Note that we report ? ???? on both the retrieval set ? (denoted as ? ???? ) and the filtered set ? ?? ? ? (denoted as ? ? _???? ). As shown in Table <ref type="table" target="#tab_3">1</ref>, MGDSPR improves over ?-DNN by 2.5%, 13.3% and 6.0% in Recall@1000, ? ???? and ? ? _???? respectively, indicating it can retrieve more products with good relevance and improve the quality of the retrieval set. Comparing ? ???? and ? ? _???? shows our relevance control module enhances retrieval relevance.  <ref type="formula" target="#formula_8">8</ref>), denoted as mgs); 2) dynamic fusion of semantics and personalization (i.e., Eq. ( <ref type="formula" target="#formula_15">15</ref>), denoted as trm); 3) the temperature parameter ? of softmax (denoted as ?); 4) the relevance-improving hard negative samples (denoted as ? ??? ).</p><p>Note that here we focus on the model's performance, so good rate ? ???? is calculated on the retrieval set ? instead of ? ?? ? ? . As shown in Table <ref type="table" target="#tab_4">2</ref>, both the multi-granular semantics unit mgs and trm can improve the metrics of Recall@1000 and ? ???? , indicating the effectiveness of multi-granular semantics and dynamic fusion. The temperature parameter ? and relevance-improving hard negative samples ? ??? make the model retrieve more relevant products in terms of much higher good rate ? ???? . Comparing MGDSPR+all and MGDSPR or MGDSPR+mgs+trm+?, we observe there is a trade-off between recall and relevance even in search scenarios, which may indicate excessive personalization in our system. Recall@1000 (%)  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Convergence Analysis.</head><p>We investigate the performance of MGDSPR using softmax cross-entropy and pairwise loss <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b33">34]</ref> as the training objective, respectively. We report the test Recall@1000 score with respect to the nubmer of training steps. As shown in Figure <ref type="figure" target="#fig_6">5</ref>, the softmax function's global comparison capability make training and testing more consistent, achieving faster convergence and better performance. In fact, it only takes about three days for the softmax loss to converge while about six days for the pairwise loss. Note that the margin parameter used in the hinge loss has been carefully tuned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.4.4</head><p>Hyper-parameter Analysis. We perform an investigation of the hyper-parameters ? (for noise smoothing) and ? (the number of generated relevance-improving hard negative samples) to demonstrate how they affect the good rate ? ???? . We conduct the evaluation by varying ? (or ? ) while fixing the other parameters. As mentioned in Section 3.4.1, we can increase ? to smooth the noisy training data and thus alleviate the effect of insufficient relevance due to overfitting users' click records. As shown in Figure <ref type="figure" target="#fig_7">6</ref>, ? = 0.1 decreases relevance, indicating that the training data does have noise. Also, every non-zero value of ? gives better relevance than ? = 0, showing that the generated hard negative samples can improve good rate ? ???? . Further, the good rate ? ???? reaches its maximum at ? = 684 and then decreases, indicating that simply increasing the number of samples cannot bring more benefits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Online A/B Test</head><p>We deploy MGDSPR on Taobao Product Search and compare it with the strong baseline ?-DNN. As aforementioned, to improve user experience, our relevance control module (introduced in Section 4.3)  will filter out some products retrieved by the EBR system, resulting in low utilization of online computing resources. Therefore, apart from GMV, P good , and P h_good , we report the number of products that participate in the pre-ranking and ranking phases (denoted as ??? ????? and ??? ???? ) to analyze the model's effect on our search system. As shown in Table <ref type="table" target="#tab_7">3</ref>, after being filtered by the relevance control module, the number of products retrieved by MGDSPR that enter the pre-ranking and ranking phases increases by 22.53% and 36.76%, respectively. Obviously, MGDSPR retrieves more products with good relevance and effectively improves the utilization of computing resources. Besides, MGDSPR achieves higher good rates ? ???? and ? ?_???? of exposure relevance, and thus can display more relevant products to users. Finally, we report the 10-day average of GMV improvements (by removing cheating traffic) achieved by MGDSPR. We also include the corresponding number of transactions (denoted as #Transactions) to increase results confidence. As shown in Table <ref type="table" target="#tab_8">4</ref>, MGDSPR improves GMV and #Transactions by 0.77% and 0.33%, respectively. Considering the billions of transaction amounts per day in Taobao Search, 0.77% improvement is already tens of millions of transaction amounts, indicating MGDSPR can significantly better satisfy users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>This paper proposes a practical embedding-based product retrieval model, named Multi-Grained Deep Semantic Product Retrieval (MGDSPR). It addresses model performance degradation and online computing resource waste due to the low retrieval relevance in the previous EBR system of Taobao Product Search. Meanwhile, we share the lessons learned from solving those problems, including model design and its effect on each stage of the search system, selection of offline metrics and test data, and relevance control of the EBR system. We verify the effectiveness of MGDSPR experimentally by offline and online A/B tests. Furthermore, we have deployed MGDSPR on Taobao Product Search to serve hundreds of millions of users in real time. Moreover, we also introduce the online architecture of our search system and the deployment scheme of the retrieval model to promote development of the community.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of the product search system in Taobao. The head of each circle denotes different phase. The bottom is the scale of the corresponding candidate set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: General architecture of the proposed Multi-Grained Deep Semantic Product Retrieval model (MGDSPR).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Overview of Taobao search engine.</figDesc><graphic url="image-14.png" coords="5,320.24,390.32,235.69,139.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Deployment system of our MGDSPR model.</figDesc><graphic url="image-15.png" coords="6,325.20,83.69,225.75,153.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>5. 3 . 1</head><label>31</label><figDesc>Large-scale Industrial Offline DataSet. We collect search logs of user clicks and purchases for 8 consecutive days from online Mobile Taobao App in December 2020, and filter the spam users.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Convergence comparison of the softmax crossentropy and hinge (pairwise) loss functions. The X-axis denotes the number of training steps, and the Y-axis denotes the corresponding test Recall@1000 score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The influence of ? and ? on the good rate ? ???? .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>?</head><label></label><figDesc>We propose a Multi-Grained Deep Semantic Product Retrieval (MGDSPR) model to dynamically capture the relationship between user query semantics and his/her personalized behaviors and share its online deployment solution.? We identify the discrepancy between training and inference in existing e-commerce retrieval systems and suggest using the softmax cross-entropy loss as the training objective to achieve better performance and faster convergence. ? We propose two methods to make the embedding-based model retrieve more relevant products without additional knowledge and training time. We further adapt the relevance control module to improve the EBR system's controllability of relevance. ? Experiments conducted on a large-scale industrial dataset and online Product Search of Taobao demonstrate the effectiveness of MGDSPR. Moreover, we analyze the effect of MGDSPR on each stage of the search system.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Comparison with the strong baseline ?-DNN on a large-scale industrial offline dataset. ??? ????? is the number of products that flow into the follow-up pre-ranking phase. ? ???? is the good rate. Relative improvements are shown in parentheses.</figDesc><table><row><cell>Methods</cell><cell>Recall@1000</cell><cell>? ????</cell><cell>? ? _????</cell><cell>??? ?????</cell></row><row><cell>?-DNN [5]</cell><cell>82.6%</cell><cell>70.6%</cell><cell>83.2%</cell><cell>769</cell></row><row><cell>MGDSPR</cell><cell cols="4">84.7%(+2.5%) 80.0%(+13.3%) 84.1%(+1.1%) 815(+6.0%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Ablation study of MGDSPR.</figDesc><table><row><cell>Methods</cell><cell cols="2">Recall@1000 ? ????</cell></row><row><cell>MGDSPR</cell><cell>85.6%</cell><cell>71.2%</cell></row><row><cell>MGDSPR + mgs</cell><cell>86.0%</cell><cell>71.6%</cell></row><row><cell>MGDSPR + trm</cell><cell>86.4%</cell><cell>71.4%</cell></row><row><cell>MGDSPR + ?</cell><cell>85.5%</cell><cell>79.0%</cell></row><row><cell>MGDSPR + mgs + trm + ?</cell><cell>86.8%</cell><cell>79.2%</cell></row><row><cell>MGDSPR + ? ???</cell><cell>83.6%</cell><cell>75.6%</cell></row><row><cell>MGDSPR + all</cell><cell>84.7%</cell><cell>80.0%</cell></row><row><cell cols="3">5.4.2 Ablation Study. We study the effectiveness of each compo-</cell></row><row><cell cols="3">nent of MGDSPR by adding only one component at a time. Specif-</cell></row><row><cell cols="3">ically, MGDSPR have the following four components: 1) Multi-</cell></row><row><cell>Granular Semantic unit (i.e., Eq. (</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>The improvements of MGDSPR in ??? ????? , ??? ???? , ? ???? , and ? ?_???? compared with the previous model deployed on Taobao Product Search. The last two columns only report relative values that are calculated on the exposed item set.</figDesc><table><row><cell>Methods</cell><cell>Num ?????</cell><cell>Num ????</cell><cell cols="2">? ???? ? ?_????</cell></row><row><cell>Baseline</cell><cell>4070</cell><cell>1390</cell><cell>-</cell><cell>-</cell></row><row><cell cols="5">MGDSPR 4987(+22.53%) 1901(+36.76%) +1.0% +0.35%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Online A/B test of MGDSPR. The improvements are averaged over 10 days in Jan 2021.</figDesc><table><row><cell>Launched Platform</cell><cell cols="2">GMV #Transactions</cell></row><row><cell cols="2">Taobao Search on Mobile +0.77%</cell><cell>+0.33%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We would like to thank the anonymous reviewers for their helpful feedbacks. This research was supported by the <rs type="funder">Alibaba Innovative Research</rs> project <rs type="grantNumber">?0034058</rs> (ZGAL).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Ve8kT6V">
					<idno type="grant-number">?0034058</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A zero attention model for personalized product search</title>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 28th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="379" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning a hierarchical embedding model for personalized product search</title>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keping</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="645" to="654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive importance sampling to accelerate training of a neural probabilistic language model</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-S?bastien</forename><surname>Sen?cal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="713" to="722" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep neural networks for youtube recommendations</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Sargin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Conference on Recommender Systems</title>
		<meeting>the 10th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Large scale distributed deep networks</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">Z</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><surname>Tucker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">MOBIUS: towards the next generation of query-ad matching in baidu&apos;s sponsored search</title>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Miao Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2509" to="2517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep session interest network for click-through rate prediction</title>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuyu</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keping</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06482</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Personalizing search results using hierarchical RNN with query-aware attention</title>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Songwei Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 27th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="347" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A deep relevance matching model for ad-hoc retrieval</title>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 25th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03244</idno>
		<title level="m">Convolutional neural network architectures for matching natural language sentences</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Embeddingbased retrieval in facebook search</title>
		<author>
			<persName><forename type="first">Jui-Ting</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuying</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Pronin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janani</forename><surname>Padmanabhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Ottaviano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2553" to="2561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using click through data</title>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 22nd ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName><forename type="first">S?bastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.2007</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cascade ranking for operational e-commerce search</title>
		<author>
			<persName><forename type="first">Shichen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1557" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SDM: Sequential deep matching model for online large-scale recommender system</title>
		<author>
			<persName><forename type="first">Fuyu</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taiwei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changlong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keping</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilfred</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 28th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2635" to="2643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02182</idno>
		<title level="m">Regularizing and optimizing LSTM language models</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Thanh V Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><surname>Subbian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.03624</idno>
		<title level="m">Learning Robust Models for e-Commerce Product Search</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic product search</title>
		<author>
			<persName><forename type="first">Priyanka</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijai</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vihan</forename><surname>Lakshman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weitian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Shingavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Choon</forename><surname>Hui Teo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2876" to="2885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep sentence embedding using long short-term memory networks: Analysis and application to information retrieval</title>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinying</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rabab</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="694" to="707" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Text matching as image recognition</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengxian</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prabhakar</forename><surname>Raghavan</surname></persName>
		</author>
		<title level="m">Introduction to information retrieval</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">39</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A latent semantic model with convolutional-pooling structure for information retrieval</title>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gr?goire</forename><surname>Mesnil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 23rd ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Amazon search: The joy of ranking products</title>
		<author>
			<persName><forename type="first">Daria</forename><surname>Sorokina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erick</forename><surname>Cantu-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="459" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Shengxian</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04378</idno>
		<title level="m">Match-srnn: Modeling the recursive matching structure with spatial rnn</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.09945</idno>
		<title level="m">Click&quot; Is Not Equal to&quot; Like&quot;: Counterfactual Recommendation for Mitigating Clickbait Issue</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Zero-Shot Heterogeneous Transfer Learning from Recommender Systems to Cold-Start Search Retrieval</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Ka-In</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng-Tze</forename><surname>Chio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dima</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ritesh</forename><surname>Kuzmin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarvjeet</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 29th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2821" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Weakly Supervised Co-Training of Query Rewriting and Semantic Matching for e-Commerce</title>
		<author>
			<persName><forename type="first">Rong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhui</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoliang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haihong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghua</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Ju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM International Conference on Web Search and Data Mining</title>
		<meeting>the 12th ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="402" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">End-to-end neural ad-hoc ranking with kernel pooling</title>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sampling-bias-corrected neural modeling for large corpus item recommendations</title>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">Zhiyuan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Heldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditee</forename><surname>Kumthekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Conference on Recommender Systems</title>
		<meeting>the 13th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="269" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiling</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunjiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weipeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Yun</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02282</idno>
		<title level="m">Towards Personalized and Semantic Retrieval: An End-to-End Solution for E-commerce Search via Embedding Learning</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Empowering Things with Intelligence: A Survey of the Progress, Challenges, and Opportunities in Artificial Intelligence of Things</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet of Things Journal</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep interest evolution network for click-through rate prediction</title>
		<author>
			<persName><forename type="first">Guorui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Na</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5941" to="5948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep interest network for click-through rate prediction</title>
		<author>
			<persName><forename type="first">Guorui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenru</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junqi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1059" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Inverted files for text search engines</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Zobel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alistair</forename><surname>Moffat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
