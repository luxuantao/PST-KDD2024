<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Real-time human action recognition based on depth motion maps</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chen</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Dallas</orgName>
								<address>
									<settlement>Richardson</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Dallas</orgName>
								<address>
									<settlement>Richardson</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kui</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Dallas</orgName>
								<address>
									<settlement>Richardson</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nasser</forename><surname>Kehtarnavaz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Dallas</orgName>
								<address>
									<settlement>Richardson</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Á</forename><forename type="middle">K</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Dallas</orgName>
								<address>
									<settlement>Richardson</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Á</forename><forename type="middle">N</forename><surname>Kehtarnavaz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Dallas</orgName>
								<address>
									<settlement>Richardson</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Real-time human action recognition based on depth motion maps</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CF0550876DAADA6DD7DEB14F51814348</idno>
					<idno type="DOI">10.1007/s11554-013-0370-1</idno>
					<note type="submission">Received: 8 April 2013 / Accepted: 25 July 2013 Ó Springer-Verlag Berlin Heidelberg 2013</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Human action recognition</term>
					<term>Depth motion map</term>
					<term>RGBD camera</term>
					<term>Collaborative representation classifier</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a human action recognition method by using depth motion maps (DMMs). Each depth frame in a depth video sequence is projected onto three orthogonal Cartesian planes. Under each projection view, the absolute difference between two consecutive projected maps is accumulated through an entire depth video sequence forming a DMM. An l 2 -regularized collaborative representation classifier with a distance-weighted Tikhonov matrix is then employed for action recognition. The developed method is shown to be computationally efficient allowing it to run in real-time. The recognition results applied to the Microsoft Research Action3D dataset indicate superior performance of our method over the existing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human action recognition is an active research area in computer vision. Earlier attempts at action recognition have involved using video sequences captured by video cameras. Spatio-temporal features are widely used for recognizing human actions, e.g. <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>. As imaging technology advances, it has become possible to capture depth information in real-time. Compared with conventional images, depth maps are insensitive to changes in lighting conditions and can provide 3D information toward distinguishing actions that are difficult to characterize using conventional images. Figure <ref type="figure" target="#fig_0">1</ref> shows two examples consisting of nine depth maps of the action Golf swing and the action Forward kick. Since the release of low cost depth sensors, in particular Microsoft Kinect and ASUS Xtion, many research works have been carried out on human action recognition using depth imagery, e.g. <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>. As noted in <ref type="bibr" target="#b13">[14]</ref>, 3D joint positions of a person's skeleton estimated from depth images provide additional information to achieve action recognition.</p><p>In this paper, the problem of human action recognition from depth map sequences is examined from the perspective of computational efficiency. These images are captured by an RGBD camera. Specifically, the depth motion maps (DMMs) generated by accumulating motion energy of projected depth maps in three projective views (front view, side view, and top view) are used as feature descriptors. Compared with 3D depth maps, DMMs are 2D images that provide an encoding of motion characteristics of an action. Motivated by the success of sparse representation in face recognition <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref> and image classification <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, an l 2 -regularized collaborative representation classifier is utilized which seeks a match of an unknown sample via a linear combination of training samples from all the classes. The class label is then derived according to the class which best approximates the unknown sample. Basically, our introduced method involves a spatio-temporal motion representation based on DMMs followed by an l 2regularized collaborative representation classifier with a distance-weighted Tikhonov matrix to perform computationally efficient action recognition.</p><p>The rest of the paper is organized as follows. In Sect. 2, related works are presented. In Sect. 3, the details of generating DMMs feature descriptors are stated. In Sect. 4, the sparse representation classifier (SRC) is first introduced and then the l 2 -regularized collaborative representation classifier is described for performing action recognition.</p><p>The experimental results are reported in Sect. 5. Finally, in Sect. 6, concluding remarks are stated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related works</head><p>Space-time based methods such as space-time volumes, spatio-temporal features, and trajectories have been widely utilized for human action recognition from video sequences captured by traditional RGB cameras. In <ref type="bibr" target="#b0">[1]</ref>, spatio-temporal interest points coupled with an SVM classifier was used to achieve human action recognition. Cuboid descriptors were employed in <ref type="bibr" target="#b1">[2]</ref> for action representation. In <ref type="bibr" target="#b2">[3]</ref>, SIFT-feature trajectories modeled in a hierarchy of three abstraction levels were used to recognize actions in video sequences. Various local motion features were gathered as spatio-temporal bag-offeatures (BoF) in <ref type="bibr" target="#b3">[4]</ref> to perform action classification. Motionenergy images (MEI) and motion-history images (MHI) were introduced in <ref type="bibr" target="#b4">[5]</ref> as motion templates to model spatial and temporal characteristics of human actions in videos. In <ref type="bibr" target="#b5">[6]</ref>, a hierarchical extension for computing dense motion flow from MHI was presented. A major shortcoming associated with using these intensity-based or color-based methods is the sensitivity of recognition to illumination variations, limiting the recognition robustness.</p><p>With the release of RGBD sensors, research into action recognition based on depth information has grown. Skeleton-based approaches utilize locations of skeletal joints extracted from depth images. In <ref type="bibr" target="#b6">[7]</ref>, a view invariant posture representation was devised using histograms of 3D joint locations (HOJ3D) within a modified spherical coordinate system. HOJ3D were re-projected using LDA and clustered into k posture visual words. The temporal evolutions of these visual words were modeled by a discrete hidden Markov model. In <ref type="bibr" target="#b7">[8]</ref>, a Naive-Bayes-Nearest-Neighbor (NBNN) classifier was employed to recognize human actions based on Eigen Joints (i.e., position differences of joints) combining static posture, motion, and offset information. Such skeleton-based approaches have limitations due to inaccuracies in skeletal estimation. Moreover, the skeleton information is not always available in many applications.</p><p>There are methods that involve extracting spatio-temporal features from the entire set of points in a depth map sequence to distinguish different actions. An action graph was employed in <ref type="bibr" target="#b8">[9]</ref> to model the dynamics of actions and a collection of 3D points were used to characterize postures. However, the 3D points sampling scheme used generated a large amount of data leading to a computationally expensive training step. In <ref type="bibr" target="#b9">[10]</ref>, a DMM-based histogram of oriented gradients (HOG) was utilized to compactly represent the body shape and movement information toward distinguishing actions. In <ref type="bibr" target="#b10">[11]</ref>, random occupancy pattern (ROP) features were extracted from depth images using a weighted sampling scheme. A sparse coding approach was utilized to robustly encode ROP features for action recognition and the features were shown to be robust to occlusion. In <ref type="bibr" target="#b11">[12]</ref>, 4D space-time occupancy patterns were used as features which preserved spatial and temporal contextual information coping with intra-class variations. A simple classifier based on the cosine distance was then used for action recognition.</p><p>In <ref type="bibr" target="#b12">[13]</ref>, a hybrid solution combining skeleton and depth information was used for action recognition. 3D joint position and local occupancy patterns were used as features. An actionlet ensemble model was then learnt to represent each action and to capture intra-class variations.</p><p>In general, the above references do not elaborate on the computational complexity aspect of their solutions and do not provide actual real-time processing times. In contrast to the existing methods, in this work, both the computational complexity and the processing times associated with each component of our method are reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Depth motion maps as features</head><p>A depth map can be used to capture the 3D structure and shape information. Yang et al. <ref type="bibr" target="#b9">[10]</ref> proposed to project depth frames onto three orthogonal Cartesian planes for the purpose of characterizing the motion of an action. Due to its computational simplicity, the same approach in <ref type="bibr" target="#b9">[10]</ref> is adopted in this work while the procedure to obtain DMMs is modified. More specifically, each 3D depth frame is used to generate three 2D projected maps corresponding to front, side, and top views, denoted by map v where v 2 f ; s; t f g. For a point ðx; y; zÞ in a depth frame with z denoting the depth value in a right-handed coordinate system, the pixel value in three projected maps is indicated by z, x, and y, respectively. Different from <ref type="bibr" target="#b9">[10]</ref>, for each projected map, the motion energy is calculated here as the absolute difference between two consecutive maps without thresholding. For a depth video sequence with N frames, DMM v is obtained by stacking the motion energy across an entire depth video sequence as follows:</p><formula xml:id="formula_0">DMM v ¼ X b i¼a map i v À map iÀ1 v ;<label>ð1Þ</label></formula><p>where i represents the frame index; map i v is the projected map of ith frame under projection view v; a 2 f2; . . .; Ng and b 2 f2; . . .; Ng denote the starting frame and the end frame index. It should be noted that not all the frames in a depth video sequence are used to generate DMMs. This point is discussed further in the experimental setup section. A bounding box is then set to extract the non-zero region as the foreground in each DMM.</p><p>Let the foreground extracted DMM be denoted by DMM v hereafter. Two examples of DMM v generated from the Tennis serve and Forward kick video sequences are shown in Fig. <ref type="figure" target="#fig_1">2</ref>. DMMs from the three projection views effectively capture the characteristics of the motion in a distinguishable way. That is the reason here for using DMMs as feature descriptors for action recognition. Since DMM v of different action video sequences may have different sizes, bicubic interpolation is used to resize all DMM v under the same projection view to a fixed size in order to reduce the intraclass variability, for example due to different subject heights. The size of DMM f is m f Â n f , the size of DMM s is m s Â n s , and the size of DMM t is m t Â n t . Since pixel values are used as features, they are normalized between 0 and 1 to avoid large pixel values dominating the feature set. The resized and normalized DMM is denoted by DMM v . For an action video sequence with three DMMs, a feature vector of size</p><formula xml:id="formula_1">m f Â n f þ m s Â n s þ m t Â n t À Á Â 1 is thus formed to be h ¼ vec DMM f À Á ; vec DMM s À Á ; vec DMM t À Á Â Ã T by concatenat-</formula><p>ing the three vectorized DMMs; vecðÁÞ indicates the vectorization operator and T the matrix transpose. The feature vector encodes the 4D characteristics of an action video sequence. Note that the HOG descriptors of the DMMs are not computed here as done in <ref type="bibr" target="#b9">[10]</ref> and image resizing is applied to DMMs but not to each projected depth map as done in <ref type="bibr" target="#b9">[10]</ref>. As a result, the computational complexity of the feature extraction process is greatly reduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">l 2 -regularized collaborative representation classifier</head><p>Sparse representation (or sparse coding) has been an active research area in the machine learning community due to its success in face recognition <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref> and image classification <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. The central idea of the SRC is to represent a test sample according to a small number of atoms sparsely chosen out of an over-complete dictionary formed by all the available training samples. Consider a dataset with C classes of training samples arranged column-wise A ¼ A 1 ; ½ A 2 ; . . .; A C 2 R dÂn , where A j ðj ¼ 1; . . .; CÞ is the subset of the training samples associated with class j, d is the dimension of training samples and n is the total number of training samples from all the classes. A test sample g 2 R d can be represented as a sparse linear combination of the training samples, which can be formulated as,</p><formula xml:id="formula_2">g ¼ Aa;<label>ð2Þ</label></formula><p>where a ¼ a 1 ; a 2 ; . . .; a C ½ is an n Â 1 vector of coefficients corresponding to all the training samples and a j ðj ¼ 1; . . .; CÞ denotes the subset of the coefficients associated with the training samples from the jth class, i.e. A j . From a practical standpoint, one cannot directly solve for a since (2) is typically under-determined <ref type="bibr" target="#b16">[17]</ref>. To reach a solution, one can solve the following l 1 norm minimization problem,</p><formula xml:id="formula_3">â ¼ arg min a g À Aa k k 2 2 þh a k k 1 n o ;<label>ð3Þ</label></formula><p>where h is a scalar regularization parameter which balances the influence of the residual and the sparsity term. The class label of g is then obtained via,</p><formula xml:id="formula_4">classðgÞ ¼ arg min j e j È É<label>ð4Þ</label></formula><p>where e j ¼ g À A j âj 2 . The reader is referred to <ref type="bibr" target="#b14">[15]</ref> for more details.</p><p>As described in <ref type="bibr" target="#b19">[20]</ref>, it is the collaborative representation, i.e. use of all the training samples as a dictionary, but not the l 1 -norm sparsity constraint, that improves the classification accuracy. The l 2 -regularized approach generates comparable results but with significantly lower computational complexity <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. Therefore, here the l 2 -regularized approach is used for action recognition. As mentioned in Sect. 3, each depth video sequence generates a feature vector h 2 R m f Ân f þm s Ân s þm t Ân t , therefore the dictionary is A ¼ h 1 ; h 2 ; . . .; h K ½ with K being the total number of available training samples from all the action classes. Let y q 2 R m f Ân f þm s Ân s þm t Ân t denote the feature vector of an unknown action sample. Tikhonov regularization <ref type="bibr" target="#b21">[22]</ref> is employed here to calculate the coefficient vector according to,</p><formula xml:id="formula_5">â ¼ arg min a y q À Aa 2 2 þk La k k 2 2 n o ;<label>ð5Þ</label></formula><p>where L is the Tikhonov regularization matrix and k is the regularization parameter. The term L allows the imposition of prior knowledge on the solution. Normally, L is chosen to be the identity matrix. The approach proposed in <ref type="bibr" target="#b22">[23]</ref> is adopted here by giving less weight to the situations which are dissimilar from the unknown sample than those which are similar. Specifically, a diagonal matrix L in the following form is considered.</p><formula xml:id="formula_6">L ¼ y q À h 1 2 0 . . . 0 y q À h K 2 2 6 4 3 7 5:<label>ð6Þ</label></formula><p>The coefficient vector â is calculated as follows <ref type="bibr" target="#b23">[24]</ref>:</p><formula xml:id="formula_7">â ¼ A T A þ kL T L À Á À1 A T y q :<label>ð7Þ</label></formula><p>The class label for each unknown sample is then found from (4). Algorithm 1 provides more details of the l 2 -regularized collaborative representation classifier utilized .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental setup</head><p>In this section, it is explained how our method was applied to the public domain Microsoft Research (MSR) Action3D dataset <ref type="bibr" target="#b8">[9]</ref> with the depth map sequences captured by an RGBD camera. Our method is then compared with the existing methods.</p><p>The MSR-Action3D dataset includes 20 actions performed by 10 subjects. Each subject performed each action 2 or 3 times. Each subject performed the same action differently. As a result, the dataset incorporated the intra-class variation. For example, the speed of performing an action varied with different subjects. The resolution of each depth map was 320 9 240. To facilitate a fair comparison, the same experimental settings as done in <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b11">12]</ref> were considered. The actions were divided into three subsets as listed in Table <ref type="table">1</ref>. For each action subset, three different tests were performed. In Test One, 1/3 of the samples were used as training samples and the rest as test samples; in Test Two, 2/3 of the samples were used as training samples and the rest as test samples; in Cross Subject Test (or Test Three), half of the subjects were used as training and the rest as test subjects. In the experimental setup reported in <ref type="bibr" target="#b8">[9]</ref>, in Test One (or Two), for each action and each subject, the first (or first two) action sequences were used for training; while in Cross Subject Test, subjects 1, 3, 5, 7, 9 (if existed) were used for training. Noting that the samples or subjects used for training and testing were fixed, they are referred to as Fixed Tests here.</p><p>Another experiment was conducted by randomly choosing training samples or training subjects corresponding to the three tests. In other words, the action sequences of each subject for each action were randomly chosen to serve as training samples in Test One and Test Two. For Cross Subject Test, half of the subjects were randomly chosen for training and the rest used for testing. These tests are referred to as Random Tests here.</p><p>For each depth video sequence, the first five frames and the last five frames were removed and the remaining frames were used to generate DMM v . The purpose of this frame removal was two-fold. First, at the beginning and the end, the subjects were mostly at a stand-still position with only small body movements, which did not contribute to the motion characteristics of the video sequences. Second, in our process of generating DMMs, small movements at the beginning and the end resulted in a stand-still body shape with large pixel values along the edges which contributed to a large amount of reconstruction error. Therefore, the initial and end frame removal was done to remove no motion condition. Other frame selection methods may be used here to achieve the same.</p><p>To have three fixed sizes for DMM v , the sizes of the DMMs of all the samples (training and test samples) were found under each projection view. The fixed size of each DMM simply set half of the mean value of all the sizes. For the training feature set and the test feature set, principal component analysis (PCA) was applied to reduce the dimensionality. The PCA transform matrix was calculated using the training feature set and then applied to the test feature set. This dimensionality reduction step provided computational efficiency for the classification. In our experiments, the largest 85 % of the eigenvalues were kept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Parameter selection</head><p>In the l 2 -regularized collaborative representation classifier, a key parameter is k which controls the relative effect of the Tikhonov regularization term in the optimization stated in <ref type="bibr" target="#b4">(5)</ref>. Many approaches have been presented in the literature-such as the L-curve <ref type="bibr" target="#b24">[25]</ref>, discrepancy principle, and generalized cross-validation (GCV)-for finding an optimal value for this regularization parameter. To find an optimal k, a set of values were examined. Figure <ref type="figure">3</ref> shows the recognition rates with different values of k for Fixed Cross Subject Test. Random Cross Subject Test was also performed with the same set of values. For each value of k, the testing was repeated 50 times. The average recognition rates are shown in Fig. <ref type="figure">4</ref>. From Figs. <ref type="figure">3</ref> and<ref type="figure">4</ref>, one can see that the recognition accuracy was quite stable for a large range of k values. As a result, in all the experiments reported here, the value of k ¼ 0:001 was thus chosen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Rejection option</head><p>An option was added to reject an action which did not belong to an action set. For example, since action Jump was not included in the MSR-Action3D dataset, it was rejected. This was done setting a rejection threshold for the minimum reconstruction error calculated from (4). This threshold was set according to the degree of similarity of the action not included in the recognition set. Let e min indicate the minimum reconstruction error, the decision of rejecting or accepting an unknown action sample was made as follows:</p><formula xml:id="formula_8">Decision (action) ¼ Reject; if e min [ threshold Accept; otherwise &amp;<label>ð8Þ</label></formula><p>To find an appropriate rejection threshold, Random Tests on the MSR-Action3D dataset were done by repeating each test for each subset 200 times. Noting K T to be the total number of test samples in a subset for a random test, 200 test trials generated 200 Â K T minimum reconstruction errors thus forming a vector E ¼ e 1  min ; e 2 min ; . . .; e 200ÂK T min Â Ã . The mean of E was then calculated and used as the rejection threshold. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Recognition results</head><p>Our method compared with existing methods using the MSR-Action3D dataset. The comparison results are reported in Table <ref type="table" target="#tab_0">2</ref>. The best recognition rate achieved is highlighted in bold. From Table <ref type="table" target="#tab_0">2</ref>, it can be seen that our method outperformed the method reported in <ref type="bibr" target="#b8">[9]</ref> in all the test cases. For the challenging Cross Subject Test, our method produced 90.5 % recognition rate which was slightly lower than the method reported in <ref type="bibr" target="#b9">[10]</ref>. However, it should be noted that our method did not require the calculation of HOG descriptors and thus it was computationally much more efficient. The confusion matrix of our method for Fixed Cross Subject Test is shown in Fig. <ref type="figure">5</ref>. For a compact representation, numbers are used to indicate the actions listed in Table <ref type="table">1</ref>. There are three possible reasons for the misclassifications in the Cross Subject Test. First, large intra-class variations existed due to considerable variations in the same action performed by different subjects. Although the DMM v of all the samples were normalized to have the same sizes, the normalization could not eliminate the intra-class variations entirely. Second, the feature formed by DMM v did not exhibit enough discriminatory power to distinguish similar motions. For example, Hammer was confused with Forward punch and High throw was confused with Tennis serve since they had similar motion characteristics. In other words, the DMM v generated by these actions were similar. Finally, since our classification decision was based on the reconstruction errors of different training classes in (4), the class with the smallest reconstruction error was favored. Hence, a misclassification occurred when two actions were similar and the wrong class had a smaller reconstruction error.</p><p>To verify that our method did not depend on specific training data, another experiment was done by randomly choosing training samples or training subjects for the three tests. Each test was run for each subset 200 times and the mean performance (mean accuracy ± standard deviation) was computed, see Table <ref type="table" target="#tab_1">3</ref>. For Test One and Test Two, the average recognition rate over the subsets was found comparable with the outcome shown in Table <ref type="table" target="#tab_0">2</ref>. In the Cross Subject Test, the average recognition rate dropped by about 10 % mainly due to a large intra-class variation. However, our method still achieved 80 % recognition rate overall which was higher than the rates reported in <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b8">[9]</ref> using the Fixed Cross Subject Test.</p><p>Furthermore, l 1 -regularized SRC (denoted by L1) and SVM <ref type="bibr" target="#b26">[27]</ref> were considered in order to compare the recognition performance with our l 2 -regularized collaborative representation classifier (denoted by L2). These three classifiers were tested on the same training and test samples of the Random Cross Subject Test with 200 trials. The SPAMS toolbox <ref type="bibr" target="#b25">[26]</ref> was employed to solve the optimization problem in (3) due to its fast implementation. Radial basis function (RBF) kernel was used for the SVM and its two parameters (penalty parameter and kernel width) were tuned for optimal recognition rates. The average recognition rates using the three classifiers are shown in Fig. <ref type="figure">6</ref>. As exhibited in this figure, our l 2 -regularized collaborative representation classifier was on par with the SCR and consistently outperformed the SVM classifier in all the three subsets. A disadvantage of SVM was also the requirement to tune its two parameters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Real-time operation</head><p>There are four main components in method: projected map generation (three views) for each depth frame, DMMs feature generation, dimensionality reduction (PCA), and action recognition (l 2 -regularized collaborative representation classifier). Our real-time action recognition timeline is displayed in Fig. <ref type="figure" target="#fig_4">7</ref>. The numbers in Fig. <ref type="figure" target="#fig_4">7</ref> indicate the main components in our method. The generation of the projected map and DMMs are executed right after each depth frame is captured while the dimensionality reduction and action recognition are performed after an action sequence gets completed. Since the PCA transform matrix is calculated using the training feature set, it can be directly applied to the feature vector of a test sample. Our code is written in Matlab and the processing time reported is for a PC with 2.67 GHz Intel Core I7 CPU with 4 GB RAM. The average processing time of each component is listed in Table <ref type="table" target="#tab_2">4</ref>. Note that the average number of depth frames in an action video sequence (after frame removal) is about 30.</p><p>The computational complexity aspect of the major components involved in different methods are provided in   Table <ref type="table" target="#tab_3">5</ref>. In <ref type="bibr" target="#b8">[9]</ref>, the bi-gram maximum likelihood decoding (BMLD) for Gaussian Mixture Model (GMM) was adopted mitigate the computational complexity with the complexity of O(J 9 K D 2 ) <ref type="bibr" target="#b27">[28]</ref>, where J denotes the number of iterations, K h the number of samples in the dataset and D the dimensionality of the state. As was reported in <ref type="bibr" target="#b6">[7]</ref>, the complexity is mostly due to the Fisher's linear discriminant analysis (LDA) and HMM. The computation of voting of joints into the bins is relatively trivial. The computational complexity for LDA is O(K h MP ? P 3 ), where M is the number of extracted features and P = min(K h ,M). The computational complexity for HMM is O(N h H 2 ) <ref type="bibr" target="#b28">[29]</ref>, where N h denotes the total number of states and H the length of the observation sequence. In <ref type="bibr" target="#b7">[8]</ref>, the computational complexity for PCA <ref type="bibr" target="#b29">[30]</ref> and Naive-Bayes-Nearest-Neighbor (NBNN) classifier is stated as O(m 3 ? m 2 r)</p><p>and O[r 9 n c 9 n d 9 log(n c 9 n d )], respectively, where m denotes the dimension of a sample vector, r denotes the number of training samples, n c represents the number of classes, and n d represents the number of descriptors. In <ref type="bibr" target="#b9">[10]</ref>, the computational complexity of SVM is stated as O(r 3 ) <ref type="bibr" target="#b30">[31]</ref>. In <ref type="bibr" target="#b11">[12]</ref>, the computational complexity of PCA and the classifier are stated as O(m 3 m 2 r) and O(n c 9 r 2 ), respectively. Table <ref type="table" target="#tab_3">5</ref> provides the speedup for a typical set of parameters: J = 50, K h = 200, D = 30, N h = 6, H = 27, M = 125, r = 100, m = 50, n c = 8, n d = 40. As can be seen from this table, our method is the most computationally efficient one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, a computationally efficient DMM-based human action recognition method using l 2 -regularized collaborative representation classifier was introduced. The DMMs generated from three projection views were used to capture the motion characteristics of an action sequence. An average recognition rate of 90.5 % on the MSR-Action3D dataset was achieved, outperforming the existing methods. In addition, the utilization of l 2 -regularized collaborative representation classifier was shown to be computationally efficient leading to a real-time implementation. Yang et al. <ref type="bibr" target="#b9">[10]</ref> O(r 3 ) 9</p><p>Vieira et al. <ref type="bibr" target="#b11">[12]</ref> O(m 3 ? m 2 r) ? O(n c 9 r 2 ) 2 0</p><p>Our method O(m 3 ? m 2 r) ? O(n c 9 r) 2 4</p><p>J Real-Time Image Proc</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1 Examples of depth map sequences for a Golf swing action, and b Forward kick action</figDesc><graphic coords="2,53.81,59.24,232.80,169.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>Fig. 2 DMM v generated from a Tennis serve, and b Forward kick depth action video sequences</figDesc><graphic coords="3,308.99,59.24,232.80,219.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 Table 1 Fig. 4</head><label>314</label><figDesc>Fig. 3 Recognition rates of Fixed Cross Subject Test for various values of k</figDesc><graphic coords="5,308.99,59.24,232.80,185.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 Fig. 6</head><label>56</label><figDesc>Fig. 5 Confusion matrix of our method for Fixed Cross Subject Test. a Subset AS1. b Subset AS2. c Subset AS3</figDesc><graphic coords="7,307.29,235.78,232.80,125.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7</head><label>7</label><figDesc>Fig. 7 Real-time action recognition timeline</figDesc><graphic coords="7,53.87,345.28,232.80,190.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc>Recognition rates (%) comparison of Fixed Tests for msr-action3D dataset</figDesc><table><row><cell></cell><cell>Li et al. [9]</cell><cell>Lu et al. [7]</cell><cell>Yang et al. [8]</cell><cell>Yang et al. [10]</cell><cell>Vieira et al. [12]</cell><cell>Our method</cell></row><row><cell>Test one</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>AS1</cell><cell>89.5</cell><cell>98.5</cell><cell>94.7</cell><cell>97.3</cell><cell>98.2</cell><cell>97.3</cell></row><row><cell>AS2</cell><cell>89.0</cell><cell>96.7</cell><cell>95.4</cell><cell>92.2</cell><cell>94.8</cell><cell>96.1</cell></row><row><cell>AS3</cell><cell>96.3</cell><cell>93.5</cell><cell>97.3</cell><cell>98.0</cell><cell>97.4</cell><cell>98.7</cell></row><row><cell>Average</cell><cell>91.6</cell><cell>96.2</cell><cell>95.8</cell><cell>95.8</cell><cell>96.8</cell><cell>97.4</cell></row><row><cell>Test two</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>AS1</cell><cell>93.4</cell><cell>98.6</cell><cell>97.3</cell><cell>98.7</cell><cell>99.1</cell><cell>98.6</cell></row><row><cell>AS2</cell><cell>92.9</cell><cell>97.2</cell><cell>98.7</cell><cell>94.7</cell><cell>97.0</cell><cell>98.7</cell></row><row><cell>AS3</cell><cell>96.3</cell><cell>94.9</cell><cell>97.3</cell><cell>98.7</cell><cell>98.7</cell><cell>100</cell></row><row><cell>Average</cell><cell>94.2</cell><cell>97.2</cell><cell>97.8</cell><cell>97.4</cell><cell>98.3</cell><cell>99.1</cell></row><row><cell>Cross subject test</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>AS1</cell><cell>72.9</cell><cell>88.0</cell><cell>74.5</cell><cell>96.2</cell><cell>84.7</cell><cell>96.2</cell></row><row><cell>AS2</cell><cell>71.9</cell><cell>85.5</cell><cell>76.1</cell><cell>84.1</cell><cell>81.3</cell><cell>83.2</cell></row><row><cell>AS3</cell><cell>79.2</cell><cell>63.6</cell><cell>96.4</cell><cell>94.6</cell><cell>88.4</cell><cell>92.0</cell></row><row><cell>Average</cell><cell>74.7</cell><cell>79.0</cell><cell>82.3</cell><cell>91.6</cell><cell>84.8</cell><cell>90.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc>Average and standard deviation recognition rates (%) of our method for msr-action3D dataset in Random Tests</figDesc><table><row><cell></cell><cell>Test one</cell><cell>Test two</cell><cell>Cross subject test</cell></row><row><cell>AS1</cell><cell>97.4 ± 0.9</cell><cell>98.5 ± 1.1</cell><cell>84.8 ± 4.4</cell></row><row><cell>AS2</cell><cell>96.1 ± 1.5</cell><cell>97.8 ± 1.4</cell><cell>67.8 ± 4.3</cell></row><row><cell>AS3</cell><cell>97.7 ± 1.2</cell><cell>98.9 ± 1.1</cell><cell>87.1 ± 3.7</cell></row><row><cell>Average</cell><cell>97.1 ± 1.2</cell><cell>98.4 ± 1.2</cell><cell>79.9 ± 4.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 Average</head><label>4</label><figDesc></figDesc><table /><note><p><p><p>and standard deviation of processing time of the components of our method</p>Components</p>Processing time (ms)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5</head><label>5</label><figDesc>Computational complexity and speed-up performanceLu et al.<ref type="bibr" target="#b6">[7]</ref> O(K h MP ? P 3 ) ? O(N h H 2 ) 3Yang et al.<ref type="bibr" target="#b7">[8]</ref> O(m 3 ? m 2 r) ? O(r 9 n c 9 n d 9 log(n c 9 n d )) 16</figDesc><table><row><cell>Method</cell><cell>Computational complexity</cell><cell>Approximate speedup for a</cell></row><row><cell></cell><cell>of major components</cell><cell>typical set of parameters</cell></row><row><cell>Li et al. [9]</cell><cell>O(J 9 K h D 2 )</cell><cell>1</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Author Biographies <ref type="bibr">Chen</ref>  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recognition human actions: a local SVM approach</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schuldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Pattern Recognition</title>
		<meeting>IEEE International Conference on Pattern Recognition<address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="32" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Behavior recognition via sparse spatio-temporal features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hierarchical spatio-temporal context modeling for action recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Cheong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Miami, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2004" to="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Anchorage, AK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The recognition of human movement using temporal templates</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bobick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="257" to="267" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical motion history images for recognizing human motion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Workshop on Detection and Recognition of Events in Video</title>
		<meeting>IEEE Workshop on Detection and Recognition of Events in Video<address><addrLine>Vancouver, BC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="39" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">View invariant human action recognition using histograms of 3D joints</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-K</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition Workshops<address><addrLine>Providence, RI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Eigen joints-based action recognition using Naı ¨ve-Bayes-Nearest-Neighbor</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition Workshops<address><addrLine>Province, RI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="14" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Action recognition based on a bag of 3D points</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition Workshops<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="9" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recognizing actions using depth motion maps-based histograms of oriented gradients</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM International Conference on Multimedia</title>
		<meeting>ACM International Conference on Multimedia<address><addrLine>Nara, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1057" to="1060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust 3D action recognition with random occupancy patterns</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE European Conference on Computer Vision</title>
		<meeting>IEEE European Conference on Computer Vision<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="872" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Stop: Space-time occupancy patterns for 3D action recognition from depth map sequences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vieira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Campos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Iberoamerican Congress on Pattern Recognition</publisher>
			<biblScope unit="page" from="252" to="259" />
			<pubPlace>Buenos Aires, Argentina</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mining actionlet ensemble for action recognition with depth cameras</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Province, RI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1290" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Real-time human pose ognition in parts from single depth images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of IEEE on Computer Vision and Pattern Recognition</title>
		<meeting>eeding of IEEE on Computer Vision and Pattern Recognition<address><addrLine>Colorado springs, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1297" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust face recognition via sparse representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="227" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dense error correction via l1 minimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal Processing<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="3033" to="3036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sparse representation for computer vision and pattern recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="1031" to="1044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Kernel sparse representation for image classification and face recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-H</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE European Conference on Computer Vision</title>
		<meeting>IEEE European Conference on Computer Vision<address><addrLine>Crete, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Linear spatial pyramid matching using sparse coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Miami, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1794" to="1801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Sparse representation or collaborative representation: which helps face recognition?</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="471" to="478" />
			<pubPlace>Barcelona, Spain</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Is face recognition really a compressive sensing problem?</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Colorado springs, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="553" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Solutions of Ill-Posed Problems</title>
		<author>
			<persName><forename type="middle">A</forename><surname>Tikhonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Arsenin</surname></persName>
		</author>
		<editor>V. H. Winston &amp; Sons</editor>
		<imprint>
			<date type="published" when="1977">1977</date>
			<pubPlace>Washington, DC</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Compressed-sensing recovery of images and video using multi hypothesis predictions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tramel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fowler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Asilomar Conference on Signals, Systems, and Computer</title>
		<meeting>Asilomar Conference on Signals, Systems, and Computer<address><addrLine>Pacific Grove, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1193" to="1198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tikhonov regularization and total least squares</title>
		<author>
			<persName><forename type="first">G</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>O'leary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="185" to="194" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The use of the L-curve in the regularization of discrete ill-posed problems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>O'leary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1487" to="1503" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<ptr target="spams-devel.gforge.inria.fr" />
		<title level="m">SPArse Modeling Software)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">LIBSVM: a library for support vector machines</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/*cjlin/libsvm/" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Expandable data-driven graphical modeling of human actions based on salient postures</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1499" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The hierarchical hidden Markov model: analysis and applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="62" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fast Motion Detection from Airborne Videos Using Graphics computing units</title>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J.Appl. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Core vector machines: Fast SVM training on very large data sets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-M</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="363" to="392" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
