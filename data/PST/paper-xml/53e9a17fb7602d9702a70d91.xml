<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Object Tracking via Sparsity-based Collaborative Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wei</forename><surname>Zhong</surname></persName>
							<email>zhongwei@mail.dlut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Dalian University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
							<email>mhyang@ucmerced.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of California at Merced</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Object Tracking via Sparsity-based Collaborative Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6F1148604E74448A90411C1506EB4488</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we propose a robust object tracking algorithm using a collaborative model. As the main challenge for object tracking is to account for drastic appearance change, we propose a robust appearance model that exploits both holistic templates and local representations. We develop a sparsity-based discriminative classifier (SD-C) and a sparsity-based generative model (SGM). In the S-DC module, we introduce an effective method to compute the confidence value that assigns more weights to the foreground than the background. In the SGM module, we propose a novel histogram-based method that takes the spatial information of each patch into consideration with an occlusion handing scheme. Furthermore, the update scheme considers both the latest observations and the original template, thereby enabling the tracker to deal with appearance change effectively and alleviate the drift problem. Numerous experiments on various challenging videos demonstrate that the proposed tracker performs favorably against several state-of-the-art algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The goal of object tracking is to estimate the states of the target in image sequences. It plays a critical role in numerous vision applications such as motion analysis, activity recognition, video surveillance and traffic monitoring. While much progress has been made in recent years, it is still a challenging problem to develop a robust algorithm for complex and dynamic scenes due to large appearance change caused by varying illumination, camera motion, occlusions, pose variation and shape deformation (See Figure <ref type="figure" target="#fig_0">1</ref>).</p><p>In a fixed frame, an appearance model is used to represent the object with proper features and verify predictions using object representations. In the successive frames, a motion model is applied to predict the likely state of an object (e.g., Kalman filter <ref type="bibr" target="#b5">[6]</ref> and particle filter <ref type="bibr" target="#b22">[20,</ref><ref type="bibr" target="#b16">14]</ref>). In this paper, we focus on the appearance model since it is usually the most crucial component of any tracking algorithm.  <ref type="bibr">(board)</ref>. The results of the Frag <ref type="bibr" target="#b0">[1]</ref>, IVT <ref type="bibr" target="#b23">[21]</ref>, MIL <ref type="bibr" target="#b3">[4]</ref>, ℓ1 <ref type="bibr" target="#b21">[19]</ref>, PN <ref type="bibr" target="#b14">[12]</ref>, VTD <ref type="bibr" target="#b15">[13]</ref> tracking methods and our tracker are represented by cyan, blue, magenta, green, black, yellow and red rectangles, respectively.</p><p>Several factors need to be considered for an effective appearance model. First, an object can be represented by different features such as intensity <ref type="bibr" target="#b23">[21]</ref>, color <ref type="bibr" target="#b22">[20]</ref>, texture <ref type="bibr" target="#b2">[3]</ref>, superpixels <ref type="bibr" target="#b27">[25]</ref>, and Haar-like features <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">11,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">12]</ref>. Meanwhile, the representation schemes can be based on holistic templates <ref type="bibr" target="#b5">[6]</ref> or local histograms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b30">28]</ref>. In this work, we use intensity values for representation because of their simplicity and efficiency. Furthermore, our approach exploits both the strength of holistic templates to distinguish the target from the background, and the effectiveness of local patches in handling partial occlusion.</p><p>Second, a model needs to be developed to verify any state prediction, which can be either generative or discriminative. For generative methods, tracking is formulated as searching for the most similar region to the target object within a neighborhood <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b23">21,</ref><ref type="bibr" target="#b21">19,</ref><ref type="bibr" target="#b18">16,</ref><ref type="bibr" target="#b17">15]</ref>. For discriminative methods, tracking is treated as a binary classification problem which aims at designing a classifier to distinguish the target object from the background <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b25">23,</ref><ref type="bibr" target="#b13">11,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">12]</ref>. Furthermore, several algorithms have been proposed to exploit the advantages of both generative and discriminative models <ref type="bibr" target="#b33">[31,</ref><ref type="bibr" target="#b19">17,</ref><ref type="bibr" target="#b24">22,</ref><ref type="bibr" target="#b20">18,</ref><ref type="bibr" target="#b6">7]</ref>. We develop a simple yet robust model that makes use of the generative model to account for appearance change and the discriminative classifier to effectively separate the foreground target from the background.</p><p>The third issue is concerned with online update schemes so that the tracker can adapt to appearance variations of the target object and the background. Numerous successful update approaches have been proposed <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b23">21,</ref><ref type="bibr" target="#b21">19]</ref>. However, straightforward and frequent updates of tracking results may gradually result in drifts due to accumulated errors, especially when the occlusion occurs. To address this problem, Babenko et al. <ref type="bibr" target="#b3">[4]</ref> devise a strategy for choosing positive and negative samples during update and introduce multiple instance learning (MIL) to learn the true target object which is included in the positive bag. Kalal et al. <ref type="bibr" target="#b14">[12]</ref> propose a bootstrapping classifier. They explore the structure of unlabeled data via positive and negative constraints which help to select potential samples for update. In order to capture appearance variations as well as reduce tracking drifts, we propose a method that takes occlusions into consideration for updating appearance model.</p><p>In this paper, we propose a robust object tracking algorithm with an effective and adaptive appearance model. We use intensity to generate holistic templates and local representations in each frame. Within our tracking scheme, the collaboration of generative models and discriminative classifiers contributes to a more flexible and robust likelihood function for particle filter. The appearance model is adaptively updated with the consideration of occlusions to account for variations and alleviate drifts. Numerous experiments on various challenging sequences show that the proposed algorithm performs favorably against the state-ofthe-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Sparse representation has recently been applied to vision problems <ref type="bibr" target="#b28">[26]</ref>, including image enhancement <ref type="bibr" target="#b31">[29]</ref>, object recognition <ref type="bibr" target="#b29">[27]</ref>, and visual tracking <ref type="bibr" target="#b21">[19,</ref><ref type="bibr" target="#b18">16,</ref><ref type="bibr" target="#b17">15]</ref>. Mei and Ling <ref type="bibr" target="#b21">[19]</ref> apply sparse representation to visual tracking and deal with occlusions via trivial templates. Despite of demonstrated success, there are still several issues to be addressed. First, the algorithm is able to deal with occlusion with ℓ 1 minimization formulation using trivial templates at the expense of high computational cost. Second, the trivial templates can be used to model any kind of image regions whether they are from the target objects or the background. Thus, the reconstruction errors of images from the occluded target and the background may be both small. As a result of generative formulation where the sample with minimal reconstruction error is regarded as the tracking result, ambi-guities are likely to accumulate and cause tracking failure. Liu et al. <ref type="bibr" target="#b18">[16]</ref> propose a method which selects a sparse and discriminative set of features to improve tracking efficiency and robustness. One potential problem with this approach is that the number of discriminative features is fixed, which may not be effective for tracking in dynamic and complex scenes. In <ref type="bibr" target="#b17">[15]</ref>, a tracking algorithm based on histograms of local sparse representation is proposed. The target object is located via mean-shift of voting maps constructed basing on reconstruction errors. In contrast to the histogram generation scheme in <ref type="bibr" target="#b17">[15]</ref> that does not differentiate foreground and background patches, we propose a weighting method to ensure that the occluded patches are not used to account for appearance change of the target object, thereby resulting a more robust model. Furthermore, the average pooling method in <ref type="bibr" target="#b17">[15]</ref> does not consider geometric information between patches while our method exploits the spatial information of local patches with histograms. In addition to model object appearance with local histograms, we also maintain a holistic template set that further helps identify the target object.</p><p>Occlusion is one of the most challenging problems in object tracking. Adam et al. <ref type="bibr" target="#b0">[1]</ref> propose a fragments-based method to handle occlusions. The target is located by a voting map formed by comparing histograms of the candidate patches and the corresponding template patches. However, the template is not updated and sensitive to large appearance variations. Yang et al. <ref type="bibr" target="#b30">[28]</ref> present the "bag of features" algorithm to visual tracking. Nevertheless, each local feature is assigned to the nearest codeword, which may result in loss of visual information <ref type="bibr" target="#b4">[5]</ref> and ambiguity, especially when the features lie near the center of several codewords. This may lead to poor and unstable appearance representation of the target object and cause tracking failure. We develop an effective method which estimates and rejects possible occluded patches to improve robustness of appearance representation when occlusions occur. In addition, our tracker is adaptively updated with consideration of whether patches are occluded or not to better account for appearance change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Algorithm</head><p>In this section, we present the proposed algorithm in details. We first discuss the motivation of this work. Next, we describe how the holistic and local visual information are exploited. The update scheme of our appearance method is then presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>The representation schemes for object tracking mainly consist of holistic templates and local histograms. While most tracking algorithms use either holistic or local representations, our approach exploits the collaborative strength of both schemes. Most tracking methods use rectangle to represent the tracking result, yet the pixels within the tracking rectangle are not all from foreground. As a result, the local representation-based classifier may be affected when updated with the background patches as positive samples. On the contrary, the holistic templates are often distinct to be foreground or background. Thus, the holistic templates are more suitable for discriminative models. Meanwhile, local representations are more amenable for generative models because of their flexibility. Therefore, we develop a collaborative model that integrates a discriminative classifier based on holistic templates and a generative model using local representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Sparsity-based Discriminative Classifier (SDC)</head><p>Motivated by the demonstrated success of sparse representation classifier <ref type="bibr" target="#b29">[27]</ref>, we propose our sparsity-based discriminative classifier for object tracking. For simplicity, we use the vector x to represent the gray-scale values of a target image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Templates</head><p>The training image set is composed of N p positive templates and N n negative templates. Initially, we sample N p images around the manually selected target location (e.g., within a radius of a few pixels). Then, the selected images are normalized to the same size (32 × 32 in our experiments) for efficiency. Each downsampled image is stacked to form the corresponding positive template vector. Similarly, the negative training set is composed of images further away from the marked location (e.g., within an annular region a few pixels away from the target object). In this way, the negative training set consists of both the background and images of parts of the target object. This facilities better object localization as samples containing only partial appearance of the target are treated as the negative samples and their confidence values are restricted to be small.</p><p>In each frame, we draw N candidates around the tracked result in the previous frame with a particle filter. To better track the target, we employ affine transformation <ref type="bibr" target="#b23">[21]</ref> to model object motion. In addition, we assume that the affine parameters are independent and can be modeled with six scalar Gaussian distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Feature Selection</head><p>The above-mentioned gray-scale feature space is rich yet redundant, from which determinative ones that distinguish foreground from background can be extracted. We select discriminative features by</p><formula xml:id="formula_0">min s A ⊤ s -p 2 2 + λ s 1 ,<label>(1)</label></formula><p>where A ∈ R K×(Np+Nn) is composed of N p positive templates A + and N n negative templates A -, and K is the feature dimension before feature selection. Each element of the vector p ∈ R (Np+Nn)×1 represents the property of each template in the training set A, i.e., +1 for positive templates and -1 for negative templates. The solution of Eq. 1 is the sparse vector s, whose nonzero elements correspond to discriminative features selected from the original K-dimension feature space. Note that the feature selection scheme adaptively chooses suitable number of discriminative features in the dynamic environment. We project the original feature space to the selected feature space via a project matrix S. It is formed by removing all-zero rows from a diagonal matrix S ′ where the elements are determined by</p><formula xml:id="formula_1">S ′ ii = 0, s i = 0 1, otherwise,<label>(2)</label></formula><p>where the diagonal element S ′ ii is zero when s i of s is zero. Both the training template set and the candidates sampled by a particle filter are projected to the selected and discriminative feature space. Thus, the training template set and candidates in the projected space are A ′ = SA and x ′ = Sx.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Confidence Measure</head><p>The proposed SDC is developed based on the assumption that the target can be better represented by the linear combination of positive templates while the background can be better represented by the span of negative templates. Given the candidate, it is represented by the training template set with the coefficients α computed by</p><formula xml:id="formula_2">min α x ′ -A ′ α 2 2 + λ α 1 .<label>(3)</label></formula><p>A candidate with smaller reconstruction error using the foreground template set indicates it is more likely to be a target object, and vice versa. Thus, we formulate the confidence value H c of the candidate x by</p><formula xml:id="formula_3">H c = exp (-(ε f -ε b ) /σ) ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_4">ε f = x ′ -A ′ + α ′ + 2 2</formula><p>is the reconstruction error of the candidate x with the foreground template set A + , and α + is the corresponding sparse coefficient vector. Similarly,</p><formula xml:id="formula_5">ε b = x ′ -A ′ -α ′ - 2</formula><p>2 is the reconstruction error of the candidate x using the background template set A -, and α - is the related sparse coefficient vector. The variable σ is fixed to be a small constant that balances the weight of the discriminative classifier and the generative model presented in Section 3.3.</p><p>In <ref type="bibr" target="#b29">[27]</ref>, the authors employ the reconstruction error on the target (positive) templates. It is not quite appropriate for tracking, since both the negative samples and the indistinguishable samples have large reconstruction errors on the target (positive) templates. Thus, it introduces ambiguity for the tracker. Our confidence measure exploits the distinction between the foreground and the background; its benefit is presented in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Sparsity-based Generative Model (SGM)</head><p>Motivated by the success of sparse coding for image classification <ref type="bibr" target="#b32">[30,</ref><ref type="bibr" target="#b26">24,</ref><ref type="bibr" target="#b8">9]</ref> as well as object tracking <ref type="bibr" target="#b17">[15]</ref>, we present a generative model for object representation that considers the location information of patches and takes occlusion into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Histogram Generation</head><p>For simplicity, we use the gray-scale features to represent the local information. We use overlapped sliding windows on the normalized images to obtain M patches and each patch is converted to a vector y i ∈ R G×1 , where G denotes the size of the patch. The sparse coefficient vector β of each patch is computed by</p><formula xml:id="formula_6">min βi y i -Dβ i 2 2 + λ β i 1 ,<label>(5)</label></formula><p>where the dictionary D ∈ R G×J is generated from k-means cluster centers (J denotes the number of cluster centers) via the patches belonging to the labeled target object in the first frame and it consists of the most representative patterns of the target object.</p><p>In this work, the sparse coefficient vector</p><formula xml:id="formula_7">β i ∈ R J×1 of each patch is concatenated to form a histogram by ρ = [β 1 , β 2 , • • • , β M ] ⊤ ,<label>(6)</label></formula><p>where ρ ∈ R (J×M )×1 is the proposed histogram for one candidate.</p><p>The average pooling scheme for histogram generation used in <ref type="bibr" target="#b17">[15]</ref> is efficient, yet the strategy may miss the spatial information of each patch. For example, if we change the location of the left part and the right part of a human face image, the average pooling scheme neglects the exchange while our method will discover it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Occlusion Handling</head><p>In order to deal with occlusions, we modify the constructed histogram to exclude the occluded patches when describing the target object. The patch with large reconstruction error is regarded as occlusion and the corresponding sparse coefficient vector is set to be zero. Thus, a weighted histogram is generated by</p><formula xml:id="formula_8">ϕ = ρ ⊙ o,<label>(7)</label></formula><p>where ⊙ denotes the element-wise multiplication. Each element of o is an indicator of occlusion of the corresponding patch and is obtained by</p><formula xml:id="formula_9">o i = 1 ε i &lt; ε 0 0 otherwise ,<label>(8)</label></formula><p>where</p><formula xml:id="formula_10">ε i = y i -Dβ i 2 2</formula><p>is the reconstruction error of patch y i , and ε 0 is a predefined threshold which determines the patch is occluded or not.</p><p>We thus have a sparsity-based histogram ϕ for each candidate. The proposed representation scheme takes spatial information of local patches and occlusion into account, thereby making it more effective and robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Similarity Function</head><p>We use the histogram intersection function to compute the similarity of histograms between the candidate and the template due to its effectiveness <ref type="bibr" target="#b8">[9]</ref> by</p><formula xml:id="formula_11">L c = J×M j=1 min ϕ j c , ψ j ,<label>(9)</label></formula><p>where ϕ c and ψ are the histograms for the c-th candidate and the template.</p><p>The histogram of the template (denoted by ψ) is generated by Eqs. 5-7. The patches y in Eq. 5 are all from the first frame and the template histogram is computed only once for each image sequence. It is updated every several frames and the update scheme is presented in Section 3.5. The vector o in Eq. 8 reflects the occlusion condition of the corresponding candidate. The comparison between the candidate and the template should be carried out under the same occlusion condition, so the template and the c-th candidate share the same vector o c in Eq. 7. For example, when the template is compared with the c-th candidate, the vector o of the template in Eq. 7 is set to o c .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Collaborative Model</head><p>We propose a collaborative model using SDC and SGM within the particle filter framework. In our tracking algorithm, the confidence value based on the holistic templates and the similarity function based on the local patches jointly contribute to an effective and robust description of the probability. The likelihood function of the c-th candidate is constructed by</p><formula xml:id="formula_12">p c = H c L c = exp (-(ε f -ε b ) /σ) J×M j=1 min ϕ j c , ψ j ,<label>(10)</label></formula><p>and the tracking result is the candidate with the highest probability.</p><p>The multiplicative formula is more effective in our tracking scheme compared with the alternative additive scheme.</p><p>The confidence value H c gives higher weights to the candidates considered as positive samples (i.e., ε f smaller than ε b ) and penalizes the others. As a result, it can be considered as the weight of the local similarity function. Moreover, the confidence value of indistinguishable candidate (i.e., it can be equally constructed by positive and negative template sets when ε f is almost equal to ≈ ε b ) is equal to 1 and it has no effect on the likelihood function when multiplying with the local similarity function. Consequently, in the collaborative model, the SGM module plays a more important role in object tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Update Scheme</head><p>Since the appearance of an object often changes significantly during the tracking process, the update scheme is important and necessary. We develop an update scheme in which the SDC and SGM are updated independently.</p><p>For the SDC model, we update the negative templates every several frames (5 in our experiments) from image regions away (e.g., more than 8 pixels) from the current tracking result. The positive templates remain the same in the entire sequence. As the SDC model aims at distinguishing the foreground from the background, it must make sure that the positive templates and the negative templates are all correct and distinct. In this way, the SDC model is adaptive and discriminative.</p><p>For the SGM model, the dictionary D is fixed for the same sequence. Therefore, the dictionary is not deteriorated by the update of tracking failures or occlusions. In order to capture the new appearance and recover the object from occlusions, the template histogram is updated by</p><formula xml:id="formula_13">ψ n = µψ f + (1 -µ) ψ l if O n &lt; O 0 ,<label>(11)</label></formula><p>where the new histogram ψ n is composed of the histogram ψ f at the first frame and the histogram ψ l last stored according to the weights assigned by the constant µ. The variable O n denotes the occlusion condition of the tracking result in the new frame. It is computed by the corresponding occlusion indication vector o n (by Eq. 8) using</p><formula xml:id="formula_14">O n = J×M i=1 1 -o i n . (<label>12</label></formula><formula xml:id="formula_15">)</formula><p>The update is performed as long as the occlusion condition O n in this frame is smaller than a predefined constant O 0 . The update scheme preserves the first template which is usually correct and takes the newly arrived template into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In order to evaluate the performance of our tracker, we conduct experiments on ten challenging image sequences. These sequences cover most challenging situations in object tracking: heavy occlusion, motion blur, in-plane and out-of-plane rotation, large illumination change, scale variation and complex background (See Figure <ref type="figure">3</ref>). For comparison, we run six state-of-the-art algorithms with the same initial position of the target. These algorithms are the Frag tracking <ref type="bibr" target="#b0">[1]</ref>, IVT tracking <ref type="bibr" target="#b23">[21]</ref>, MIL tracking <ref type="bibr" target="#b3">[4]</ref>, ℓ 1 tracking <ref type="bibr" target="#b21">[19]</ref>, PN tracking <ref type="bibr" target="#b14">[12]</ref> and VTD tracking <ref type="bibr" target="#b15">[13]</ref> methods. We present some representative results in this section. All the MATLAB source codes and datasets are available on our web sites (http://ice.dlut.edu.cn/lu/publications.html, http://faculty.ucmerced.edu/mhyang/pubs.html). The parameters are presented as follows. Note that they are fixed for all sequences. The numbers of positive templates N p and negative templates N n are 50 and 200 respectively. The variable λ in Eq. 1 is fixed to be 0.001. The variable λ in Eqs. 3 and 5 is fixed to be 0.01. The row number G and column number J of dictionary D in Eq. 5 are 36 and 50. The threshold ε 0 in Eq. 8 is 0.04. The update rate µ is set to be 0.95. The threshold O 0 in Eq. 11 is 0.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Quantitative Comparison</head><p>We evaluate the above-mentioned algorithms using the center location error as well as the overlapping rate <ref type="bibr" target="#b7">[8]</ref>, and the results are shown in Table <ref type="table" target="#tab_0">1</ref> and<ref type="table" target="#tab_1">Table 2</ref>. Figure <ref type="figure" target="#fig_1">2</ref> shows the center location errors of the evaluated algorithms on all test sequences. Overall, the proposed tracker performs well against the other state-of-the-art algorithms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Qualitative Comparison</head><p>Heavy occlusion: Occlusion is one of the most general yet crucial problems in object tracking. In fact, several trackers including the FragTrack method <ref type="bibr" target="#b0">[1]</ref>, the MIL tracking algorithm <ref type="bibr" target="#b3">[4]</ref>, the ℓ 1 tracking method <ref type="bibr" target="#b21">[19]</ref> and our tracker are developed to solve this problem. In contrast, the IVT tracking method <ref type="bibr" target="#b23">[21]</ref>, the PN tracking method <ref type="bibr" target="#b14">[12]</ref> and the VTD tracking system <ref type="bibr" target="#b15">[13]</ref> are less effective in handling occlusions as shown in Figure <ref type="figure">3</ref> <ref type="bibr">(a)</ref>, especially at frames 175, 497, 819 of the faceocc2 sequence. In our SGM module, we estimate the possible occluded patches and develop a robust histogram which only compares the patches that are not occluded. Thus, the occlusion handling scheme effectively alleviates the affect of occlusions. Aside from tracking a target object under occlusion, our method updates appearance change correctly especially when heavy occlusions occur. In addition, our tracker is able to deal with in-plane rotation when the target is occluded at frame 497, owing to the appearance model we employ. Our tracker can accurately locate the target object at frame 819 as our generated histogram takes the spatial information of local patches into consideration.</p><p>In the caviar sequence, the target is occluded by two people at times and one of them is similar in color and shape to the target. The other trackers all fail before frame 134 due to heavy occlusion (Figure <ref type="figure">3(a)</ref>). Furthermore, for most template-based trackers, simple update with occluded portion often leads to drifts (frame 442 of Figure <ref type="figure">3(a)</ref>). In contrast, our tracker achieves stable performance in the entire sequence when there is a large scale change with heavy occlusion. This can be attributed to our SGM model that reduces the effect of occlusions and only compares the foreground with the stored histograms. Besides, our update scheme doesn't introduce heavy occlusions which may lead to drift problem.</p><p>Motion blur: Fast motion of the target object or the camera leads to blurred image appearance which is difficult to account for in object tracking. Figure <ref type="figure">3</ref>(b) presents the tracking results on the animal sequence in which the appearance of the target object is almost indistinguishable due to the motion blur. Most tracking algorithms fail to follow the target right at the beginning of this sequence. At frame 42, the PN tracking method <ref type="bibr" target="#b14">[12]</ref> mistakenly locates a similar object instead of the correct target. The reason is that the true target is blurred and it is difficult for the detector of P-N <ref type="bibr" target="#b14">[12]</ref> to distinguish it from the background. The proposed algorithm well handles the situation with similar objects as the SDC module selects the discriminative features to better separate the target from the background. By updating the negative templates online, the proposed algorithm successfully tracks the target object throughout the sequence.</p><p>The appearance change caused by motion blur in the jumping sequence is drastic that the Frag <ref type="bibr" target="#b0">[1]</ref> and VTD <ref type="bibr" target="#b15">[13]</ref> methods fail before frame 31. The IVT <ref type="bibr" target="#b23">[21]</ref> method is able to track the target in some frames (e.g., frame 100) but fails when the motion blur occurs (e.g., frame 238). Our tracker successfully keeps track of the target object with small errors. The main reason is that we use the SDC module which separates the foreground from the background. Meanwhile, the confidence measure by Eq. 4 assigns smaller weights to the candidate of background. Thus, the tracking result will not drift to the background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rotation:</head><p>The girl sequence in Figure <ref type="figure">3</ref>(c) consists of both in-plane and out-of-plane rotations. The PN tracking method <ref type="bibr" target="#b14">[12]</ref> and the VTD tracking method <ref type="bibr" target="#b15">[13]</ref> fail when the girl rotates her head. Compared with other algorithms, our tracker is more robust and accurate as seen from frame 312 and frame 430. In our tracking scheme, the background candidates are assigned quite small weights according to Eq. 4. Therefore, the tracking result will not shift to the background when the girl rotates (e.g., frame 111 and frame 312).</p><p>The target object in the panda sequence experiences more and larger in-plane rotations. As seen from frame 53, the IVT method <ref type="bibr" target="#b23">[21]</ref> fails due to occlusion and fast movement. Most trackers drift after the target undergoes large rotations (e.g., frame 154) whereas our method performs well throughout this sequence. As the other trackers often account for object motion with translational or similarity transforms, they are not able to deal with complex movements. In addition, the use of local histograms helps in accounting for appearance change due to complex motion. Furthermore, the target object in the panda sequence also undergoes occlusions as shown in frame 53 and frame 214. The PN tracking method <ref type="bibr" target="#b14">[12]</ref> fails to detect occlusions and track the target object after frame 214 while our tracker still performs well.</p><p>Illumination change: Figure <ref type="figure">3(d</ref>) presents the tracking results on sequences with dramatic illumination changes. In the singer1 sequence, the stage light changes drastically seen from frame 121 and frame 321. The PN tracking method <ref type="bibr" target="#b14">[12]</ref> is not able to detect and track the target object (e.g., frame 121). On the other hand, our tracker accurately locates the target object even when there is a large scale change at frame 321. In the shaking sequence, the target object undergoes large appearance variation due to drastic illumination change and unpredictable motion. Our SDC module introduces the backgrounds and the images with parts of the target as negative templates so the confidence values of these candidates calculated by Eq .4 are small. Thus, the tracking result is accurately located on the true target without much offset.</p><p>For the car11 sequence, there is low contrast between the foreground and the background (frame 284) as well as illumination change. The FragTrack method <ref type="bibr" target="#b0">[1]</ref> fails at the beginning (at frame 19) because it only uses the local information and does not maintain a holistic representation of the target. The IVT tracking method <ref type="bibr" target="#b23">[21]</ref> achieves good results in this sequence. It can be attributed to the fact that subspace learning method is robust to illumination changes. In our SDC module, we select several discriminative features which can better separate the target from the background. Thus, our tracker performs well in spite of the low contrast between the foreground and the background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complex background:</head><p>The board sequence is challenging as the background is cluttered and the target object experiences out-of-plane rotations as seen from Figure <ref type="figure">3</ref>(e). In frame 55, most trackers fail as holistic representations inevitably include background pixels that may be considered as part of foreground object through straightforward update schemes. Using fixed templates, the FragTrack method <ref type="bibr" target="#b0">[1]</ref> is able to track the target as long as there is no drastic appearance change (e.g., frame 55 and frame 183), but fails when the target moves quickly or rotates (e.g., frame 78, frame 395 and frame 528). Our tracker performs well in this sequence as the target can be differentiated from the cluttered background with the use of our SDC module. In addition, the update scheme uses the newly arrived negative templates that facilitate separation of the foreground object and the background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose and demonstrate an effective and robust tracking method based on the collaboration of generative and discriminative modules. In our tracker, holistic templates are incorporated to construct a discriminative classifier that can effectively deal with cluttered and complex background. Local representations are adopted to form a robust histogram that considers the spatial information among local patches with an occlusion handling module, which enables our tracker to better handle heavy occlusion. The contributions of these holistic discriminative and local generative modules are integrated in a unified manner. Moreover, the online update scheme reduces drifts and enhances the proposed method to adaptively account for appearance change in dynamic scenes. Quantitative and qualitative comparisons with six state-of-the-art algorithms on ten challenging image sequences demonstrate the robustness of our tracker.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Tracking in challenging environments including heavy occlusions (caviar), rotation (panda), illumination change (shaking) and cluttered background (board). The results of the Frag [1],IVT<ref type="bibr" target="#b23">[21]</ref>, MIL<ref type="bibr" target="#b3">[4]</ref>, ℓ1<ref type="bibr" target="#b21">[19]</ref>, PN<ref type="bibr" target="#b14">[12]</ref>, VTD<ref type="bibr" target="#b15">[13]</ref> tracking methods and our tracker are represented by cyan, blue, magenta, green, black, yellow and red rectangles, respectively.</figDesc><graphic coords="1,316.94,347.78,70.87,53.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Quantitative evaluation in terms of center location error (in pixel). The proposed algorithm is compared with six state-of-the-art methods on ten challenging image sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Average center location error (in pixel). The best and second best results are shown in red and blue fonts.</figDesc><table><row><cell></cell><cell>Frag</cell><cell>IVT</cell><cell>MIL</cell><cell>ℓ 1</cell><cell>PN</cell><cell>VTD</cell><cell>Our</cell></row><row><cell>animal</cell><cell>92.1</cell><cell>127.5</cell><cell>66.5</cell><cell>15.3</cell><cell>-</cell><cell>12.0</cell><cell>10.8</cell></row><row><cell>board</cell><cell>45.4</cell><cell>165.4</cell><cell>66.7</cell><cell>184.0</cell><cell>90.1</cell><cell>105.0</cell><cell>12.7</cell></row><row><cell>car11</cell><cell>64.0</cell><cell>2.2</cell><cell>43.5</cell><cell>33.3</cell><cell>25.2</cell><cell>27.1</cell><cell>1.8</cell></row><row><cell cols="2">caviar 116.1</cell><cell>66.0</cell><cell>100.2</cell><cell>65.7</cell><cell>44.5</cell><cell>58.3</cell><cell>2.7</cell></row><row><cell>faceocc2</cell><cell>15.5</cell><cell>10.3</cell><cell>14.1</cell><cell>11.2</cell><cell>18.6</cell><cell>10.5</cell><cell>4.8</cell></row><row><cell>girl</cell><cell>18.1</cell><cell>48.5</cell><cell>32.3</cell><cell>62.5</cell><cell>23.2</cell><cell>21.5</cell><cell>9.8</cell></row><row><cell>jumping</cell><cell>58.5</cell><cell>36.9</cell><cell>9.9</cell><cell>12.5</cell><cell>3.6</cell><cell>63.0</cell><cell>3.8</cell></row><row><cell>shaking</cell><cell>52.8</cell><cell>152.7</cell><cell>11.2</cell><cell>118.7</cell><cell>-</cell><cell>6.1</cell><cell>9.4</cell></row><row><cell>singer1</cell><cell>22.1</cell><cell>8.5</cell><cell>15.2</cell><cell>4.6</cell><cell>32.7</cell><cell>4.1</cell><cell>3.8</cell></row><row><cell>panda</cell><cell>90.1</cell><cell>169.8</cell><cell>103.4</cell><cell>94.0</cell><cell>-</cell><cell>94.8</cell><cell>2.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Average overlap rate based on<ref type="bibr" target="#b7">[8]</ref>. The best and second best results are shown in red and blue fonts.</figDesc><table><row><cell></cell><cell>Frag</cell><cell>IVT</cell><cell>MIL</cell><cell>ℓ 1</cell><cell>PN</cell><cell>VTD</cell><cell>Our</cell></row><row><cell>animal</cell><cell>0.07</cell><cell>0.21</cell><cell>0.21</cell><cell>0.53</cell><cell>0.41</cell><cell>0.57</cell><cell>0.59</cell></row><row><cell>board</cell><cell>0.65</cell><cell>0.14</cell><cell>0.46</cell><cell>0.12</cell><cell>0.34</cell><cell>0.32</cell><cell>0.78</cell></row><row><cell>car11</cell><cell>0.08</cell><cell>0.80</cell><cell>0.17</cell><cell>0.43</cell><cell>0.37</cell><cell>0.43</cell><cell>0.79</cell></row><row><cell>caviar</cell><cell>0.13</cell><cell>0.14</cell><cell>0.13</cell><cell>0.13</cell><cell>0.16</cell><cell>0.15</cell><cell>0.85</cell></row><row><cell>faceocc2</cell><cell>0.60</cell><cell>0.58</cell><cell>0.61</cell><cell>0.67</cell><cell>0.49</cell><cell>0.59</cell><cell>0.81</cell></row><row><cell>girl</cell><cell>0.68</cell><cell>0.42</cell><cell>0.51</cell><cell>0.32</cell><cell>0.57</cell><cell>0.51</cell><cell>0.69</cell></row><row><cell>jumping</cell><cell>0.13</cell><cell>0.28</cell><cell>0.52</cell><cell>0.55</cell><cell>0.69</cell><cell>0.07</cell><cell>0.73</cell></row><row><cell>shaking</cell><cell>0.24</cell><cell>0.02</cell><cell>0.65</cell><cell>0.03</cell><cell>0.12</cell><cell>0.73</cell><cell>0.67</cell></row><row><cell>singer1</cell><cell>0.34</cell><cell>0.66</cell><cell>0.33</cell><cell>0.70</cell><cell>0.41</cell><cell>0.79</cell><cell>0.85</cell></row><row><cell>panda</cell><cell>0.23</cell><cell>0.15</cell><cell>0.35</cell><cell>0.16</cell><cell>0.60</cell><cell>0.36</cell><cell>0.69</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements W. Zhong and H. Lu are supported by the National Natural Science Foundation of China #61071209. M.-H. Yang is supported by the NSF CAREER Grant #1149783 and NSF IIS Grant #1152576.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust fragments-based tracking using the integral histogram</title>
		<author>
			<persName><forename type="first">A</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Support vector tracking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1064" to="1072" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ensemble tracking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="261" to="271" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Visual tracking with online multiple instance learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">In defense of nearestneighbor based image classification</title>
		<author>
			<persName><forename type="first">O</forename><surname>Boiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Kernel-based object tracking</title>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">R</forename><surname>Member</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="564" to="575" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Co-training framework of generative and discriminative trackers with partial occlusion handling</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Workshop on Applications of Computer Vision</title>
		<meeting>IEEE Workshop on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="642" to="649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">The PASCAL Visual Object Classes Challenge 2010 (VOC2010) Results</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Local features are not lonely -laplacian sparse coding for image classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-H</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-T</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On-line boosting and vision</title>
		<author>
			<persName><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Tracking results on sequence faceocc2 and caviar with severe occlusions. (b) Tracking results on sequence animal and jumping with motion blur</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Tracking results on sequence singer1, shaking and car11 with dramatic illumination change. (e) Tracking results on sequence board with complex background</title>
		<imprint/>
	</monogr>
	<note>Tracking results on sequence girl and panda with in-plane and out-of-plane rotations</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Sample tracking results of evaluated algorithms on ten challenging image sequences</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised on-line boosting for robust tracking</title>
		<author>
			<persName><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">P-N learning: Bootstrapping binary classifiers by structural constraints</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visual tracking decomposition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tracking in low frame rate video: a cascade particle filter with discriminative observers of different life spans</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kawade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1728" to="1740" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robust tracking using local sparse appearance model and k-selection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kulikowsk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robust and fast collaborative tracking with two stage sparse optimization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kulikowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A robust boosting tracker with minimum error bound in a co-training framework</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A co-training framework for visual tracking with multiple instance learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FG</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust visual tracking using ℓ 1 minimization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Color-based probabilistic tracking</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vermaak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gangnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Incremental learning for robust visual tracking</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R.-S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="125" to="141" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">PROST: Parallel robust online simple tracking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Santner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Co-tracking using semisupervised support vector machines</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Localityconstrained linear coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Superpixel tracking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Sparse representation for computer vision and pattern recognition. Proceedings of the IEEE</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Maral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="1031" to="1044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust face recognition via sparse representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="227" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bag of features tracking</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image super-resolution via sparse representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Linear spatial pyramid matching using sparse coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Online tracking and reacquisition using co-trained generative and discriminative trackers</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
