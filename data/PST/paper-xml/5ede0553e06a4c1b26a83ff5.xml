<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-06-30">30 Jun 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Franc</forename><surname>¸ois Fleuret</surname></persName>
						</author>
						<title level="a" type="main">Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-06-30">30 Jun 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2006.16236v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from O N 2 to O (N ), where N is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Transformer models were originally introduced by <ref type="bibr" target="#b36">Vaswani et al. (2017)</ref> in the context of neural machine translation <ref type="bibr" target="#b34">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref> and have demonstrated impressive results on a variety of tasks dealing with natural language <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>, audio <ref type="bibr" target="#b32">(Sperber et al., 2018)</ref>, and images <ref type="bibr" target="#b23">(Parmar et al., 2019)</ref>. Apart from tasks with ample supervision, transformers are also effective in transferring knowledge to tasks with limited or no supervision when they are pretrained with autoregressive <ref type="bibr" target="#b26">(Radford et al., 2018;</ref><ref type="bibr">2019)</ref> or masked language modeling objectives <ref type="bibr" target="#b8">(Devlin et al., 2019;</ref><ref type="bibr" target="#b37">Yang et al., 2019;</ref><ref type="bibr" target="#b31">Song et al., 2019;</ref><ref type="bibr" target="#b19">Liu et al., 2020)</ref>.</p><p>However, these benefits often come with a very high computational and memory cost. The bottleneck is mainly caused 1 Idiap Research Institute, Martigny, Switzerland 2 EPFL, Lausanne, Switzerland 3 University of Washington, Seattle, USA. Correspondence to: Angelos Katharopoulos &lt;firstname.lastname@idiap.ch&gt;.</p><p>Proceedings of the 37 th International Conference on Machine <ref type="bibr">Learning, Vienna, Austria, PMLR 119, 2020.</ref> Copyright 2020 by the author(s).</p><p>by the global receptive field of self-attention, which processes contexts of N inputs with a quadratic memory and time complexity O N 2 . As a result, in practice transformers are slow to train and their context is limited. This disrupts temporal coherence and hinders the capturing of long-term dependencies. <ref type="bibr" target="#b6">Dai et al. (2019)</ref> addressed the latter by attending to memories from previous contexts albeit at the expense of computational efficiency.</p><p>Lately, researchers shifted their attention to approaches that increase the context length without sacrificing efficiency. Towards this end, <ref type="bibr" target="#b2">Child et al. (2019)</ref> introduced sparse factorizations of the attention matrix to reduce the selfattention complexity to O N √ N . <ref type="bibr" target="#b13">Kitaev et al. (2020)</ref> further reduced the complexity to O (N log N ) using localitysensitive hashing. This made scaling to long sequences possible. Even though the aforementioned models can be efficiently trained on large sequences, they do not speed-up autoregressive inference.</p><p>In this paper, we introduce the linear transformer model that significantly reduces the memory footprint and scales linearly with respect to the context length. We achieve this by using a kernel-based formulation of self-attention and the associative property of matrix products to calculate the self-attention weights ( § 3.2). Using our linear formulation, we also express causal masking with linear complexity and constant memory <ref type="bibr">( § 3.3)</ref>. This reveals the relation between transformers and RNNs, which enables us to perform autoregressive inference orders of magnitude faster ( § 3.4).</p><p>Our evaluation on image generation and automatic speech recognition demonstrates that linear transformer can reach the performance levels of transformer, while being up to three orders of magnitude faster during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we provide an overview of the most relevant works that seek to address the large memory and computational requirements of transformers. Furthermore, we discuss methods that theoretically analyze the core component of the transformer model, namely self-attention. Finally, we present another line of work that seeks to alleviate the softmax bottleneck in the attention computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Efficient Transformers</head><p>Existing works seek to improve memory efficiency in transformers through weight pruning <ref type="bibr" target="#b20">(Michel et al., 2019)</ref>, weight factorization <ref type="bibr" target="#b16">(Lan et al., 2020)</ref>, weight quantization <ref type="bibr" target="#b38">(Zafrir et al., 2019)</ref> or knowledge distillation. <ref type="bibr" target="#b3">Clark et al. (2020)</ref> proposed a new pretraining objective called replaced token detection that is more sample efficient and reduces the overall computation. <ref type="bibr" target="#b15">Lample et al. (2019)</ref> used product-key attention to increase the capacity of any layer with negligible computational overhead.</p><p>Reducing the memory or computational requirements with these methods leads to training or inference time speedups, but, fundamentally, the time complexity is still quadratic with respect to the sequence length which hinders scaling to long sequences. In contrast, we show that our method reduces both memory and time complexity of transformers both theoretically ( § 3.2) and empirically ( § 4.1).</p><p>Another line of research aims at increasing the "context" of self-attention in transformers. Context refers to the maximum part of the sequence that is used for computing selfattention. <ref type="bibr" target="#b6">Dai et al. (2019)</ref> introduced Transformer-XL which achieves state-of-the-art in language modeling by learning dependencies beyond a fixed length context without disrupting the temporal coherence. However, maintaining previous contexts in memory introduces significant additional computational cost. In contrast, <ref type="bibr" target="#b33">Sukhbaatar et al. (2019)</ref> extended the context length significantly by learning the optimal attention span per attention head, while maintaining control over the memory footprint and computation time. Note that both approaches have the same asymptotic complexity as the vanilla model. In contrast, we improve the asymptotic complexity of the self-attention, which allows us to use significantly larger context.</p><p>More related to our model are the works of <ref type="bibr" target="#b2">Child et al. (2019)</ref> and <ref type="bibr" target="#b13">Kitaev et al. (2020)</ref>. The former <ref type="bibr" target="#b2">(Child et al., 2019)</ref> introduced sparse factorizations of the attention matrix reducing the overall complexity from quadratic to O N √ N for generative modeling of long sequences. More recently, <ref type="bibr" target="#b13">Kitaev et al. (2020)</ref> proposed Reformer. This method further reduces complexity to O (N log N ) by using locality-sensitive hashing (LSH) to perform fewer dot products. Note that in order to be able to use LSH, Reformer constrains the keys, for the attention, to be identical to the queries. As a result this method cannot be used for decoding tasks where the keys need to be different from the queries. In comparison, linear transformers impose no constraints on the queries and keys and scale linearly with respect to the sequence length. Furthermore, they can be used to perform inference in autoregressive tasks three orders of magnitude faster, achieving comparable performance in terms of validation perplexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Understanding Self-Attention</head><p>There have been few efforts to better understand selfattention from a theoretical perspective. <ref type="bibr" target="#b35">Tsai et al. (2019)</ref> proposed a kernel-based formulation of attention in transformers which considers attention as applying a kernel smoother over the inputs with the kernel scores being the similarity between inputs. This formulation provides a better way to understand attention components and integrate the positional embedding. In contrast, we use the kernel formulation to speed up the calculation of self-attention and lower its computational complexity. Also, we observe that if a kernel with positive similarity scores is applied on the queries and keys, linear attention converges normally.</p><p>More recently, <ref type="bibr" target="#b5">Cordonnier et al. (2020)</ref> provided theoretical proofs and empirical evidence that a multi-head selfattention with sufficient number of heads can express any convolutional layer. Here, we instead show that a selfattention layer trained with an autoregressive objective can be seen as a recurrent neural network and this observation can be used to significantly speed up inference time of autoregressive transformer models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Linearized softmax</head><p>For many years, softmax has been the bottleneck for training classification models with a large number of categories <ref type="bibr" target="#b9">(Goodman, 2001;</ref><ref type="bibr" target="#b22">Morin &amp; Bengio, 2005;</ref><ref type="bibr" target="#b21">Mnih &amp; Hinton, 2009)</ref>. Recent works <ref type="bibr" target="#b1">(Blanc &amp; Rendle, 2017;</ref><ref type="bibr" target="#b28">Rawat et al., 2019)</ref>, have approximated softmax with a linear dot product of feature maps to speed up the training through sampling. Inspired from these works, we linearize the softmax attention in transformers. Concurrently with this work, <ref type="bibr" target="#b30">Shen et al. (2020)</ref> explored the use of linearized attention for the task of object detection in images. In comparison, we do not only linearize the attention computation, but also develop an autoregressive transformer model with linear complexity and constant memory for both inference and training. Moreover, we show that through the lens of kernels, every transformer can be seen as a recurrent neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Linear Transformers</head><p>In this section, we formalize our proposed linear transformer. We present that changing the attention from the traditional softmax attention to a feature map based dot product attention results in better time and memory complexity as well as a causal model that can perform sequence generation in linear time, similar to a recurrent neural network.</p><p>Initially, in § 3.1, we introduce a formulation for the transformer architecture introduced in <ref type="bibr" target="#b36">(Vaswani et al., 2017)</ref>. Subsequently, in § 3.2 and § 3.3 we present our proposed linear transformer and finally, in § 3.4 we rewrite the transformer as a recurrent neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Transformers</head><p>Let x ∈ R N ×F denote a sequence of N feature vectors of dimensions F . A transformer is a function T : R N ×F → R N ×F defined by the composition of L transformer layers T 1 (•), . . . , T L (•) as follows,</p><formula xml:id="formula_0">T l (x) = f l (A l (x) + x).</formula><p>(1)</p><p>The function f l (•) transforms each feature independently of the others and is usually implemented with a small two-layer feedforward network. A l (•) is the self attention function and is the only part of the transformer that acts across sequences.</p><p>The self attention function A l (•) computes, for every position, a weighted average of the feature representations of all other positions with a weight proportional to a similarity score between the representations. Formally, the input sequence x is projected by three matrices</p><formula xml:id="formula_1">W Q ∈ R F ×D , W K ∈ R F ×D and W V ∈ R F ×M to corresponding rep- resentations Q, K and V .</formula><p>The output for all positions, A l (x) = V , is computed as follows,</p><formula xml:id="formula_2">Q = xW Q , K = xW K , V = xW V , A l (x) = V = softmax QK T √ D V.</formula><p>(2)</p><p>Note that in the previous equation, the softmax function is applied rowwise to QK T . Following common terminology, the Q, K and V are referred to as the "queries", "keys" and "values" respectively.</p><p>Equation 2 implements a specific form of self-attention called softmax attention where the similarity score is the exponential of the dot product between a query and a key.</p><p>Given that subscripting a matrix with i returns the i-th row as a vector, we can write a generalized attention equation for any similarity function as follows,</p><formula xml:id="formula_3">V i = N j=1 sim (Q i , K j ) V j N j=1 sim (Q i , K j ) . (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>Equation 3 is equivalent to equation 2 if we substitute the similarity function with sim (q, k) = exp q T k √ D .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Linearized Attention</head><p>The definition of attention in equation 2 is generic and can be used to define several other attention implementations such as polynomial attention or RBF kernel attention <ref type="bibr" target="#b35">(Tsai et al., 2019)</ref>. Note that the only constraint we need to impose to sim (•), in order for equation 3 to define an attention function, is to be non-negative. This includes all kernels k(x, y) : R 2×F → R + .</p><p>Given such a kernel with a feature representation φ (x) we can rewrite equation 2 as follows,</p><formula xml:id="formula_5">V i = N j=1 φ (Q i ) T φ (K j ) V j N j=1 φ (Q i ) T φ (K j ) ,<label>(4)</label></formula><p>and then further simplify it by making use of the associative property of matrix multiplication to</p><formula xml:id="formula_6">V i = φ (Q i ) T N j=1 φ (K j ) V T j φ (Q i ) T N j=1 φ (K j )</formula><p>.</p><p>(5)</p><p>The above equation is simpler to follow when the numerator is written in vectorized form as follows,</p><formula xml:id="formula_7">φ (Q) φ (K) T V = φ (Q) φ (K) T V .<label>(6)</label></formula><p>Note that the feature map φ (•) is applied rowwise to the matrices Q and K.</p><p>From equation 2, it is evident that the computational cost of softmax attention scales with O N 2 , where N represents the sequence length. The same is true for the memory requirements because the full attention matrix must be stored to compute the gradients with respect to the queries, keys and values. In contrast, our proposed linear transformer from equation 5 has time and memory complexity O (N ) because we can compute N j=1 φ (K j ) V T j and N j=1 φ (K j ) once and reuse them for every query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">FEATURE MAPS AND COMPUTATIONAL COST</head><p>For softmax attention, the total cost in terms of multiplications and additions scales as O N 2 max (D, M ) , where D is the dimensionality of the queries and keys and M is the dimensionality of the values. On the contrary, for linear attention, we first compute the feature maps of dimensionality C. Subsequently, computing the new values requires O (N CM ) additions and multiplications.</p><p>The previous analysis does not take into account the choice of kernel and feature function. Note that the feature function that corresponds to the exponential kernel is infinite dimensional, which makes the linearization of exact softmax attention infeasible. On the other hand, the polynomial kernel, for example, has an exact finite dimensional feature map and has been shown to work equally well with the exponential or RBF kernel <ref type="bibr" target="#b35">(Tsai et al., 2019)</ref>. The computational cost for a linearized polynomial transformer of degree 2 is O N D 2 M . This makes the computational complexity favorable when N &gt; D 2 . Note that this is true in practice since we want to be able to process sequences with tens of thousands of elements.</p><p>For our experiments, that deal with smaller sequences, we employ a feature map that results in a positive similarity function as defined below,</p><formula xml:id="formula_8">φ (x) = elu(x) + 1,<label>(7)</label></formula><p>where elu(•) denotes the exponential linear unit <ref type="bibr" target="#b4">(Clevert et al., 2015)</ref> activation function. We prefer elu(•) over relu(•) to avoid setting the gradients to 0 when x is negative. This feature map results in an attention function that requires O (N DM ) multiplications and additions. In our experimental section, we show that the feature map of equation 7 performs on par to the full transformer, while significantly reducing the computational and memory requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Causal Masking</head><p>The transformer architecture can be used to efficiently train autoregressive models by masking the attention computation such that the i-th position can only be influenced by a position j if and only if j ≤ i, namely a position cannot be influenced by the subsequent positions. Formally, this causal masking changes equation 3 as follows,</p><formula xml:id="formula_9">V i = i j=1 sim (Q i , K j ) V j i j=1 sim (Q i , K j ) .<label>(8)</label></formula><p>Following the reasoning of § 3.2, we linearize the masked attention as described below,</p><formula xml:id="formula_10">V i = φ (Q i ) T i j=1 φ (K j ) V T j φ (Q i ) T i j=1 φ (K j ) . (<label>9</label></formula><formula xml:id="formula_11">)</formula><p>By introducing S i and Z i as follows,</p><formula xml:id="formula_12">S i = i j=1 φ (K j ) V T j ,<label>(10)</label></formula><formula xml:id="formula_13">Z i = i j=1 φ (K j ) ,<label>(11)</label></formula><p>we can simplify equation 9 to</p><formula xml:id="formula_14">V i = φ (Q i ) T S i φ (Q i ) T Z i . (<label>12</label></formula><formula xml:id="formula_15">)</formula><p>Note that, S i and Z i can be computed from S i−1 and Z i−1 in constant time hence making the computational complexity of linear transformers with causal masking linear with respect to the sequence length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">GRADIENT COMPUTATION</head><p>A naive implementation of equation 12, in any deep learning framework, requires storing all intermediate values S i in order to compute the gradients. This increases the memory consumption by max (D, M ) times; thus hindering the applicability of causal linear attention to longer sequences or deeper models. To address this, we derive the gradients of the numerator in equation 9 as cumulative sums. This allows us to compute both the forward and backward pass of causal linear attention in linear time and constant memory. A detailed derivation is provided in the supplementary material.</p><p>Given the numerator Vi and the gradient of a scalar loss function with respect to the numerator ∇ Vi L, we derive ∇ φ(Qi) L, ∇ φ(Ki) L and ∇ Vi L as follows,</p><formula xml:id="formula_16">∇ φ(Qi) L = ∇ Vi L   i j=1 φ (K j ) V T j   T ,<label>(13)</label></formula><formula xml:id="formula_17">∇ φ(Ki) L =   N j=i φ (Q j ) ∇ Vj L T   V i ,<label>(14)</label></formula><formula xml:id="formula_18">∇ Vi L =   N j=i φ (Q j ) ∇ Vj L T   T φ (K i ) . (<label>15</label></formula><formula xml:id="formula_19">)</formula><p>The cumulative sum terms in equations 9, 13-15 are computed in linear time and require constant memory with respect to the sequence length. This results in an algorithm with computational complexity O (N CM ) and memory O (N max (C, M )) for a given feature map of C dimensions. A pseudocode implementation of the forward and backward pass of the numerator is given in algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">TRAINING AND INFERENCE</head><p>When training an autoregressive transformer model the full ground truth sequence is available. This makes layerwise parallelism possible both for f l (•) of equation 1 and the attention computation. As a result, transformers are more efficient to train than recurrent neural networks. On the other hand, during inference the output for timestep i is the input for timestep i + 1. This makes autoregressive models impossible to parallelize. Moreover, the cost per timestep for transformers is not constant; instead, it scales with the square of the current sequence length because attention must be computed for all previous timesteps.</p><p>Our proposed linear transformer model combines the best of both worlds. When it comes to training, the computations can be parallelized and take full advantage of GPUs or other accelerators. When it comes to inference, the cost per time and memory for one prediction is constant for our model. This means we can simply store the φ (K j ) V T j matrix as an internal state and update it at every time step like a recurrent neural network. This results in inference thousands of times faster than other transformer models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Transformers are RNNs</head><p>In literature, transformer models are considered to be a fundamentally different approach to recurrent neural networks. However, from the causal masking formulation in § 3.3 and the discussion in the previous section, it becomes evident that any transformer layer with causal masking can be written as a model that, given an input, modifies an internal state and then predicts an output, namely a Recurrent Neural Network (RNN). Note that, in contrast to Universal Transformers <ref type="bibr" target="#b7">(Dehghani et al., 2018)</ref>, we consider the recurrence with respect to time and not depth.</p><p>In the following equations, we formalize the transformer layer of equation 1 as a recurrent neural network. The resulting RNN has two hidden states, namely the attention memory s and the normalizer memory z. We use subscripts to denote the timestep in the recurrence.</p><formula xml:id="formula_20">s 0 = 0, (<label>16</label></formula><formula xml:id="formula_21">) z 0 = 0,<label>(17)</label></formula><formula xml:id="formula_22">s i = s i−1 + φ (x i W K ) (x i W V ) T , (<label>18</label></formula><formula xml:id="formula_23">) z i = z i−1 + φ (x i W K ) ,<label>(19)</label></formula><formula xml:id="formula_24">y i = f l φ (x i W Q ) T s i φ (x i W Q ) T z i + x i .<label>(20)</label></formula><p>In the above equations, x i denotes the i-th input and y i the i-th output for a specific transformer layer. Note that our formulation does not impose any constraint on the feature function and it can be used for representing any transformer model, in theory even the ones using softmax attention. This formulation is a first step towards better understanding the relationship between transformers and popular recurrent networks <ref type="bibr" target="#b11">(Hochreiter &amp; Schmidhuber, 1997)</ref> and the processes used for storing and retrieving information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we analyze experimentally the performance of the proposed linear transformer. Initially, in § 4.1, we evaluate the linearized attention in terms of computational cost, memory consumption and convergence on synthetic data. To further showcase the effectiveness of linear transformers, we evaluate our model on two real-world applications, image generation in § 4.2 and automatic speech recognition in § 4.3. We show that our model achieves competitive performance with respect to the state-of-the-art transformer architectures, while requiring significantly less GPU memory and computation.</p><p>Throughout our experiments, we compare our model with two baselines, the full transformer with softmax attention and the Reformer <ref type="bibr" target="#b13">(Kitaev et al., 2020)</ref>, the latter being a state-of-the-art accelerated transformer architecture. For the Reformer, we use a PyTorch reimplementation of the pub-Algorithm 1 Linear transformers with causal masking function forward(φ (Q), φ (K), V ):</p><formula xml:id="formula_25">V ← 0, S ← 0 for i = 1, . . . , N do S ← S + φ (K i ) V T i equation 10 Vi ← φ (Q i ) S end return V end function backward(φ (Q), φ (K), V , G):</formula><p>/ * G is the gradient of the loss with respect to the output of forward * /</p><formula xml:id="formula_26">S ← 0, ∇ φ(Q) L ← 0 for i = 1, . . . , N do S ← S + φ (K i ) V T i ∇ φ(Qi) L ← G i S T equation 13 end S ← 0, ∇ φ(K) L ← 0, ∇ V L ← 0 for i = N, . . . , 1 do S ← S + φ (Q i ) G T i ∇ Vi L ← S T φ (K i ) equation 15 ∇ φ(Ki) L ← SV i equation 14 end return ∇ φ(Q) L, ∇ φ(K) L, ∇ V L end</formula><p>lished code and for the full transformer we use the default PyTorch implementation. Note that for Reformer, we do not use the reversible layers, however, this does not affect the results as we only measure the memory consumption with respect to the self attention layer. In all experiments, we use softmax <ref type="bibr" target="#b36">(Vaswani et al., 2017)</ref> to refer to the standard transformer architecture, linear for our proposed linear transformers and lsh-X for Reformer <ref type="bibr" target="#b13">(Kitaev et al., 2020)</ref>, where X denotes the hashing rounds.</p><p>For training the linear transformers, we use the feature map of equation 7. Our PyTorch <ref type="bibr" target="#b24">(Paszke et al., 2019)</ref> code with documentation and examples can be found at https:// linear-transformers.com/. The constant memory gradient computation of equations 13-15 is implemented in approximately 200 lines of CUDA code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Synthetic Tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">CONVERGENCE ANALYSIS</head><p>To examine the convergence properties of linear transformers we train on an artifical copy task with causal masking. Namely, the transformers have to copy a series of symbols similar to the sequence duplication task of <ref type="bibr" target="#b13">Kitaev et al. (2020)</ref>. We use a sequence of maximum length 128 with 10 Figure <ref type="figure">1</ref>: Comparison of the computational requirements for a forward/backward pass for Reformer (lsh-X), softmax attention and linear attention. Linear and Reformer models scale linearly with the sequence length unlike softmax which scales with the square of the sequence length both in memory and time. Full details of the experiment can be found in § 4.1. different symbols separated by a dedicated separator symbol. For all three methods, we train a 4 layer transformer with 8 attention heads using a batch size of 64 and the RAdam optimizer <ref type="bibr" target="#b18">(Liu et al., 2019)</ref> with a learning rate of 10 −3 which is reduced to 10 −4 after 3000 updates. Figure <ref type="figure" target="#fig_0">2</ref> depicts the loss with respect to the number of gradient steps. We observe that linear converges smoothly and reaches a lower loss than lsh due to the lack of noise introduced by hashing. In particular, it reaches the same loss as softmax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">MEMORY AND COMPUTATIONAL REQUIREMENTS</head><p>In this subsection, we compare transformers with respect to their computational and memory requirements. We compute the attention and the gradients for a synthetic input with varying sequence lengths N ∈ {2 9 , 2 10 , . . . , 2 16 } and measure the peak allocated GPU memory and required time for each variation of transformer. We scale the batch size inversely with the sequence length and report the time and memory per sample in the batch.</p><p>Every method is evaluated up to the maximum sequence length that fits the GPU memory. For this benchmark we use an NVidia GTX 1080 Ti with 11GB of memory. This results in a maximum sequence length of 4,096 elements for softmax and 16,384 for lsh-4 and lsh-8. As expected, softmax scales quadratically with respect to the sequence length. Our method is faster and requires less memory than the baselines for every configuration, as seen in figure <ref type="figure">1</ref>. We observe that both Reformer and linear attention scale linearly with the sequence length. Note that although the asymptotic complexity for Reformer is O (N log N ), log N is small enough and does not affect the computation time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Image Generation</head><p>Transformers have shown great results on the task of conditional or unconditional autoregressive generation <ref type="bibr" target="#b27">(Radford et al., 2019;</ref><ref type="bibr" target="#b2">Child et al., 2019)</ref>, however, sampling from transformers is slow due to the task being inherently sequential and the memory scaling with the square of the sequence length. In this section, we train causally masked transformers to predict images pixel by pixel. Our achieved performance in terms of bits per dimension is on par with softmax attention while being able to generate images more than 1,000 times faster and with constant memory per image from the first to the last pixel. We refer the reader to our supplementary for comparisons in terms of training evolution, quality of generated images and time to generate a single image. In addition, we also compare with a faster softmax transformer that caches the keys and values during inference, in contrast to the PyTorch implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">MNIST</head><p>First, we evaluate our model on image generation with autoregressive transformers on the widely used MNIST dataset <ref type="bibr" target="#b17">(LeCun et al., 2010)</ref> Since the sequence length is realtively small, namely only 784 pixels, to remove differences due to different batch sizes we use a batch size of 10 for all methods.</p><p>Table <ref type="table" target="#tab_1">1</ref> summarizes the results. We observe that linear transformers achieve almost the same performance, in terms of final perplexity, as softmax transformers while being able to generate images more than 300 times faster. This is achieved due to the low memory requirements of our model, which is able to simultaneously generate 10,000 MNIST images with a single GPU. In particular, the memory is constant with respect to the sequence length because the only thing that needs to be stored between pixels are the s i and z i values as described in equations 18 and 19. On the other hand, both softmax and Reformer require memory that increases with the length of the sequence.</p><p>Image completions and unconditional samples from our MNIST model can be seen in figure <ref type="figure" target="#fig_1">3</ref>. We observe that our linear transformer generates very convincing samples with sharp boundaries and no noise. In the case of image completion, we also observe that the transformer learns to use the same stroke style and width as the original image effectively attending over long temporal distances. Note that as the achieved perplexity is more or less the same for all models, we do not observe qualitative differences between the generated samples from different models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">CIFAR-10</head><p>The benefits of our linear formulation increase as the sequence length increases. transformers to generate CIFAR-10 images <ref type="bibr" target="#b14">(Krizhevsky et al., 2009)</ref>. For each layer we use the same configuration as in the previous experiment. For Reformer, we use again 64 buckets and 83 chunks of 37 elements, which is approximately 32, as suggested in the paper. Since the sequence length is almost 4 times larger than for the previous experiment, the full transformer can only be used with a batch size of 1 in the largest GPU that is available to us, namely an NVidia P40 with 24GB of memory. For both the linear transformer and reformer, we use a batch size of 4. All models are trained for 7 days. We report results in terms of bits per dimension and image generation throughput in table 2. Note that although the main point of this experiment is not the final perplexity, it is evident that as the sequence length grows, the fast transformer models become increasingly more efficient per GPU hour, achieving better scores than their slower counterparts.</p><p>As the memory and time to generate a single pixel scales quadratically with the number of pixels for both Reformer and softmax attention, the increase in throughput for our linear transformer is even more pronounced. In particular, for every image generated by the softmax transformer, our method can generate 4,460 images. Image completions and unconditional samples from our model can be seen in figure <ref type="figure" target="#fig_2">4</ref>. We observe that our model generates images with spatial consistency and can complete images convincigly without significantly hindering the recognition of the image category. For instance, in figure <ref type="figure" target="#fig_2">4b</ref>, all images have successfully completed the dog's nose (first row) or the windshield of the truck (last row).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Automatic Speech Recognition</head><p>To show that our method can also be used for nonautoregressive tasks, we evaluate the performance of linear transformers in end-to-end automatic speech recognition using Connectionist Temporal Classification (CTC) loss <ref type="bibr" target="#b10">(Graves et al., 2006)</ref>. In this setup, we predict a distribution over phonemes for each input frame in a non autore-  gressive fashion. We use the 80 hour WSJ dataset <ref type="bibr" target="#b25">(Paul &amp; Baker, 1992)</ref> with 40-dimensional mel-scale filterbanks without temporal differences as features. The dataset contains sequences with 800 frames on average and a maximum sequence length of 2,400 frames. For this task, we also compare with a bidirectional LSTM <ref type="bibr" target="#b11">(Hochreiter &amp; Schmidhuber, 1997)</ref> with 3 layers of hidden size 320. We use the Adam optimizer <ref type="bibr" target="#b12">(Kingma &amp; Ba, 2014)</ref> with a learning rate of 10 −3 which is reduced when the validation error stops decreasing. For the transformer models, we use 9 layers with 6 heads with the same embedding dimensions as for the image experiments. As an optimizer, we use RAdam with an initial learning rate of 10 −4 that is divided by 2 when the validation error stops decreasing.</p><p>All models are evaluated in terms of phoneme error rate (PER) and training time per epoch. We observe that linear outperforms the recurrent network baseline and Reformer both in terms of performance and speed by a large margin, as seen in table <ref type="table" target="#tab_3">3</ref>. Note that the softmax transformer, achieves lower phone error rate in comparison to all baselines, but is significantly slower. In particular, linear transformer is more than 3× faster per epoch. We provide training evolution plots in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this work, we presented linear transformer, a model that significantly reduces the memory and computational cost of the original transformers. In particular, by exploiting the associativity property of matrix products we are able to compute the self-attention in time and memory that scales linearly with respect to the sequence length. We show that our model can be used with causal masking and still retain its linear asymptotic complexities. Finally, we express the transformer model as a recurrent neural network, which allows us to perform inference on autoregressive tasks thousands of time faster.</p><p>This property opens a multitude of directions for future research regarding the storage and retrieval of information in both RNNs and transformers. Another line of research to be explored is related to the choice of feature map for linear attention. For instance, approximating the RBF kernel with random Fourier features could allow us to use models pretrained with softmax attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material for</head><p>Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Gradient Derivation</head><p>In the first section of our supplementary material, we derive in detail the gradients for causally masked linear transformers and show that they can be computed in linear time and constant memory. In particular, we derive the gradients of a scalar loss with respect to the numerator of the following equation,</p><formula xml:id="formula_27">V i = φ (Q i ) T i j=1 φ (K j ) V T j φ (Q i ) T i j=1 φ (K j ) . (<label>21</label></formula><formula xml:id="formula_28">)</formula><p>The gradient with respect to the denominator and the fraction are efficiently handled by autograd. Without loss of generality, we can assume that Q and K already contain the vectors mapped by φ (•), hence given the numerator</p><formula xml:id="formula_29">Vi = Q T i i j=1 K j V T j ,<label>(22)</label></formula><p>and</p><formula xml:id="formula_30">∇ V L we seek to compute ∇ Q L, ∇ K L and ∇ V L. Note that Q ∈ R N ×D , K ∈ R N ×D and V ∈ R N ×M .</formula><p>To derive the gradients, we first express the above equation for a single element without using vector notation,</p><formula xml:id="formula_31">Vie = D d=1 Q id i j=1 K jd V je = D d=1 i j=1 Q id K jd V je .<label>(23)</label></formula><p>Subsequently we can start deriving the gradients for Q by taking the partial derivative for any Q lt , as follows</p><formula xml:id="formula_32">∂L ∂Q lt = M e=1 ∂L ∂ Vle ∂ Vle ∂Q lt = M e=1 ∂L ∂ Vle   l j=1 K jt V je   . (<label>24</label></formula><formula xml:id="formula_33">)</formula><p>If we write the above equation as a matrix product of gradients it becomes,</p><formula xml:id="formula_34">∇ Qi L = ∇ Vi L   i j=1 K j V T j   T ,<label>(25)</label></formula><p>proving equation 13 from the main paper. In equation 24 we made use of the fact that Q lt only affects Vl hence we do not need to sum over i to compute the gradients. However, for K and V this is not the case. In particular, K j affects all Vi where i ≥ j. Consequently, we can write the partial derivative of the loss with respect to K lt as follows,</p><formula xml:id="formula_35">∂L ∂K lt = M e=1 N i=l ∂L ∂ Vie ∂ Vie ∂K lt = M e=1 N i=l ∂L ∂ Vie ∂ D d=1 i j=1 Q id K jd V je ∂K lt = M e=1 N i=l ∂L ∂ Vie Q it V le .<label>(26)</label></formula><p>As for Q we can now write the gradient in vectorized form,</p><formula xml:id="formula_36">∇ Ki L =   N j=i Q j ∇ Vj L T   V i ,<label>(27)</label></formula><p>proving equation 14 from the paper. Following the same reasoning, we can compute the partial derivative of the loss with respect to V lt and prove equation 15. Note that the cumulative sum matrices for the gradient with respect to Q and K have the same size, however one is computed in the forward direction (summing from 1 to N ) similarly to the forward pass and the other is computed in the backwards direction (summing from N to 1) similar to backpropagation through time done in RNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training Evolution</head><p>In figure <ref type="figure" target="#fig_3">5</ref> we present the training evolution of all transformer models in our experiments. For the MNIST experiment (Fig. <ref type="figure" target="#fig_3">5a</ref>) we train all methods for 250 epochs. The sequence length is small enough so that the training time does not vary significantly for all methods. We observe that our method converges on par with softmax attention outperforming significantly both reformer variants.</p><p>On the other hand, for CIFAR-10 (Fig. <ref type="figure" target="#fig_3">5b</ref>) we train all methods for a fixed amount of time, namely 7 days. We observe that lsh-1 and linear complete significantly more epochs than softmax and lsh-4 and achieve better performance. This gap is expected to increase with a further increase in sequence length.</p><p>Finally, in our last experiment on automatic speech recognition (Fig. <ref type="figure" target="#fig_3">5c</ref>), softmax outperforms significantly both Reformer and linear in terms of convergence. Note that linear is 3× faster per epoch which means it has completed approximately 4 times more epochs in comparison to softmax. Even though softmax attention is better in this task, we observe that linear transformers significantly outperform Reformer both in terms of convergence and final performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Image Generation Throughput Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Stateful softmax attention</head><p>In § 4.2 of the main paper, we report the image generation throughput and we compare with softmax transformer and lsh. In this section we create another baseline, denoted as stateful-softmax, that implements a softmax autoregressive transformer as a recurrent model. Namely, all the keys and values are saved and then passed to the model again when predicting the next element of the sequence. The state of this recurrent model is the set of keys and values which has size proportional to the sequence length. This is qualitatively different to our proposed model that has a state with fixed dimensions and computing the i-th state given the previous one has fixed computational cost regardless of i.</p><p>Table <ref type="table" target="#tab_4">4</ref> summarizes the results. We observe that stateful-softmax is significantly faster than vanilla transformers. However, its complexity is still quadratic with respect to the sequence length and our forumlation is more than 50× faster for CIFAR-10. Moreover, we would like to point out that implementing a similar stateful attention for Reformer is not a trivial task as the sorting and chunking operations need to be performed each time a new input is provided. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Equalizing the batch size</head><p>In the previous sections we evaluate the throughput of all transformer variants for the task of autoregressive image generation. However, another important factor to consider is latency, namely the total time required to produce a single image. To this end, we use a batch size of 1 and measure the time required by all methods to generate a single image. In addition to running the inference on the GPU, we also evaluate the time required on CPU.  . We run all methods with a batch size of 1 both on CPU and GPU and report the total time in seconds. For all numbers in the table, lower is better.</p><p>We observe that all methods underutilize the GPU and achieve significantly smaller image generation throughput than the one shown in table <ref type="table" target="#tab_4">4</ref>. The proposed linear transformer is faster than all the methods and in particular it is almost 6.6× faster than softmax transformers for generating an image on CIFAR-10. Note that our linear autoregressive transformer is the only method that is faster on the CPU than on the GPU in every case. This is due to the fact that computing the attention as an RNN has such a low cost that the main computational bottleneck becomes the inevitable outer loop over the sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Qualitative Results on Image Generation</head><p>In this section we provide qualitative results for our image generation experiments. Since the perplexity of all models is approximately the same, as expected, the qualitative differences are not significant. A rather interesting observation however is that the Reformer models provide significantly fewer variations in their unconditional samples. Moreover, we observe that image completion is a significantly easier task than unconditional generation as all models perform significantly better.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Convergence comparison of softmax, linear and reformer attention on a sequence duplication task. linear converges stably and reaches the same final performance as softmax. The details of the experiment are in § 4.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Unconditional samples and image completions generated by our method for MNIST. (a) depicts the occluded orignal images, (b) the completions and (c) the original. Our model achieves comparable bits/dimension to softmax, while having more than 300 times higher throughput, generating 142 images/second. For details see § 4.2.1.</figDesc><graphic url="image-3.png" coords="8,102.24,172.56,140.41,69.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Unconditional samples and image completions generated by our method for CIFAR-10. (a) depicts the occluded orignal images, (b) the completions and (c) the original. the sequence length grows linear transformers become more efficient compared to softmax attention. Our model achieves more than 4,000 times higher throughput and generates 17.85 images/second. For details see § 4.2.2.</figDesc><graphic url="image-7.png" coords="8,354.24,172.63,140.40,69.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Training evolution of transformers for all our experiments. It can be observed that linear transformers converge consistently faster than Reformer and in the autoregressive experiments on par with softmax. For MNIST all methods are trained for 250 epochs while for CIFAR we train for 7 days. In the speech recognition experiments all methods are trained to convergence. The details of the experiments can be found in § 4.2.1, § 4.2.2 and § 4.3 in the main paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Unconditional samples from the transformer models trained with MNIST. See § 4.2.1 in the main paper.</figDesc><graphic url="image-11.png" coords="14,97.82,385.58,194.40,194.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: MNIST digit completion from all trained models. See § 4.2.1 in the main paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Unconditional samples from the transformer models trained with CIFAR-10. See § 4.2.2 in the main paper.</figDesc><graphic url="image-21.png" coords="16,97.82,385.58,194.41,194.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: CIFAR-10 image from all trained transformer models. See § 4.2.2 in the main paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>. The architecture for this experiment comprises 8 attention layers with 8 attention heads each. We Comparison of autoregressive image generation of MNIST images. Our linear transformers achieve almost the same bits/dim as the full softmax attention but more than 300 times higher throughput in image generation. The full details of the experiment are in § 4.2.1. set the embedding size to 256 which is 32 dimensions per head. Our feed forward dimensions are 4 times larger than our embedding size. We model the output with a mixture of 10 logistics as introduced by<ref type="bibr" target="#b29">Salimans et al. (2017)</ref>. We use the RAdam optimizer with a learning rate of 10 −4 and train all models for 250 epochs. For the reformer baseline, we use 1 and 4 hashing rounds. Furthermore, as suggested in<ref type="bibr" target="#b13">Kitaev et al. (2020)</ref>, we use 64 buckets and chunks with approximately 32 elements. In particular, we divide the 783 long input sequence to 27 chunks of 29 elements each.</figDesc><table><row><cell>Method</cell><cell>Bits/dim</cell><cell>Images/sec</cell></row><row><cell>Softmax</cell><cell>0.621</cell><cell>0.45 (1×)</cell></row><row><cell>LSH-1</cell><cell>0.745</cell><cell>0.68 (1.5×)</cell></row><row><cell>LSH-4</cell><cell>0.676</cell><cell>0.27 (0.6×)</cell></row><row><cell>Linear (ours)</cell><cell>0.644</cell><cell>142.8 (317×)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>To showcase that, we train 16 layer</figDesc><table><row><cell>Method</cell><cell>Bits/dim</cell><cell>Images/sec</cell></row><row><cell>Softmax</cell><cell>3.47</cell><cell>0.004 (1×)</cell></row><row><cell>LSH-1</cell><cell>3.39</cell><cell>0.015 (3.75×)</cell></row><row><cell>LSH-4</cell><cell>3.51</cell><cell>0.005 (1.25×)</cell></row><row><cell>Linear (ours)</cell><cell>3.40</cell><cell>17.85 (4,462×)</cell></row><row><cell cols="3">Table 2: We train autoregressive transformers for 1 week</cell></row><row><cell cols="3">on a single GPU to generate CIFAR-10 images. Our linear</cell></row><row><cell cols="3">transformer completes 3 times more epochs than softmax,</cell></row><row><cell cols="3">which results in better perplexity. Our model generates</cell></row><row><cell cols="3">images 4,000× faster than the baselines. The full details of</cell></row><row><cell cols="2">the experiment are in  § 4.2.2.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>For details see § 4.2.1. Performance comparison in automatic speech recognition on the WSJ dataset. The results are given in the form of phoneme error rate (PER) and training time per epoch. Our model outperforms the LSTM and Reformer while being faster to train and evaluate. Details of the experiment can be found in § 4.3.</figDesc><table><row><cell>Method</cell><cell cols="2">Validation PER Time/epoch (s)</cell></row><row><cell>Bi-LSTM</cell><cell>10.94</cell><cell>1047</cell></row><row><cell>Softmax</cell><cell>5.12</cell><cell>2711</cell></row><row><cell>LSH-4</cell><cell>9.33</cell><cell>2250</cell></row><row><cell>Linear (ours)</cell><cell>8.08</cell><cell>824</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison of autoregressive image generation throughput of MNIST and CIFAR-10 images. The experiment can be found in § 4.2 in the main paper. For stateful-softmax we save the keys and values and reuse them for predicting the next element. A detailed description of this extra baseline can be found in § C.1.</figDesc><table><row><cell>Method</cell><cell>Bits/dim</cell><cell>Images/sec</cell><cell>Method</cell><cell>Bits/dim</cell><cell>Images/sec</cell></row><row><cell>Softmax</cell><cell>0.621</cell><cell>0.45 (1×)</cell><cell>Softmax</cell><cell>3.47</cell><cell>0.004 (1×)</cell></row><row><cell>Stateful-softmax</cell><cell>0.621</cell><cell>7.56 (16.8×)</cell><cell>Stateful-softmax</cell><cell>3.47</cell><cell>0.32 (80×)</cell></row><row><cell>LSH-1</cell><cell>0.745</cell><cell>0.68 (1.5×)</cell><cell>LSH-1</cell><cell>3.39</cell><cell>0.015 (3.75×)</cell></row><row><cell>LSH-4</cell><cell>0.676</cell><cell>0.27 (0.6×)</cell><cell>LSH-4</cell><cell>3.51</cell><cell>0.005 (1.25×)</cell></row><row><cell>Linear (ours)</cell><cell>0.644</cell><cell>142.8 (317×)</cell><cell>Linear (ours)</cell><cell>3.40</cell><cell>17.85 (4,462×)</cell></row><row><cell cols="3">(a) Image generation on MNIST</cell><cell cols="3">(b) Image generation on CIFAR-10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>The results are reported in table 5.</figDesc><table><row><cell>Method</cell><cell cols="2">Seconds (CPU) Seconds (GPU)</cell><cell>Method</cell><cell>Seconds (CPU)</cell><cell>Seconds (GPU)</cell></row><row><cell>Softmax</cell><cell cols="2">72.6 (13.2×) 10.2 (1.4×)</cell><cell>Softmax</cell><cell cols="2">8651.4 (191.8×) 300.1 (4.9×)</cell></row><row><cell>Stateful-softmax</cell><cell>7.4 (1.3×)</cell><cell>10.4 (1.42×)</cell><cell>Stateful-softmax</cell><cell>71.9 (1.6×)</cell><cell>70.4 (1.14×)</cell></row><row><cell>LSH-1</cell><cell>46.0 (8.3×)</cell><cell>19.2 (2.6×)</cell><cell>LSH-1</cell><cell>2318.9 (51.4×)</cell><cell>221.6 (3.6×)</cell></row><row><cell>LSH-4</cell><cell>112.0 (20×)</cell><cell>55.8 (7.6×)</cell><cell>LSH-4</cell><cell cols="2">5263.7 (116.7×) 683.9 (11.1×)</cell></row><row><cell>Linear (ours)</cell><cell>5.5 (1×)</cell><cell>7.3 (1×)</cell><cell>Linear (ours)</cell><cell>45.1 (1×)</cell><cell>61.3 (1×)</cell></row><row><cell cols="3">(a) Image generation on MNIST</cell><cell></cell><cell></cell><cell></cell></row></table><note>(b) Image generation on CIFAR-10</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison of the time required to generate a single image with autoregressive transformers on MNIST and CIFAR-10</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Angelos Katharopoulos was supported by the Swiss National Science Foundation under grant numbers FNS-30209 "ISUL" and FNS-30224 "CORTI". Apoorv Vyas was supported by the Swiss National Science Foundation under grant number FNS-30213 "SHISSM". Nikolaos Pappas was supported by the Swiss National Science Foundation under grant number P400P2 183911 "UNISON".</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
				<meeting>the 5th International Conference on Learning Representations<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Adaptive sampled softmax with kernel based sampling</title>
		<author>
			<persName><forename type="first">G</forename><surname>Blanc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00527</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ELECTRA: Pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (ELUs)</title>
		<author>
			<persName><forename type="first">D.-A</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the relationship between self-attention and convolutional layers</title>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07">July 2019</date>
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiser</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03819</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Ł. Universal transformers. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Classes for fast maximum entropy training</title>
		<author>
			<persName><forename type="first">J</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No. 01CH37221)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="561" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
				<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04451</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large memory layers with product keys</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Álché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8546" to="8557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Mnist handwritten digit database</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03265</idno>
		<title level="m">On the variance of the adaptive learning rate and beyond</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Roberta</surname></persName>
		</author>
		<title level="m">A robustly optimized BERT pretraining approach</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Are sixteen heads really better than one?</title>
		<author>
			<persName><forename type="first">P</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="14014" to="14024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hierarchical probabilistic neural network language model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Aistats</title>
				<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="246" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="68" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The design for the wall street journal-based csr corpus</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on Speech and Natural Language</title>
				<meeting>the workshop on Speech and Natural Language</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="357" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improving language understanding by generative pretraining</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OpenAI report</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sampled softmax with random fourier features</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">X X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="13834" to="13844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Improving the pixelcnn with discretized logistic mixture likelihood and other modifications</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Pixelcnn++</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05517</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Efficient attention: Attention with linear complexities</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01243</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Masked sequence to sequence pre-training for language generation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Mass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
				<editor>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06-15">09-15 Jun 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="5926" to="5936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Self-attentional acoustic models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sperber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th Annual Conference of the International Speech Communication Association</title>
				<meeting><address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09">InterSpeech 2018. September 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adaptive attention span in transformers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07">July 2019</date>
			<biblScope unit="page" from="331" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Transformer dissection: An unified understanding for transformer&apos;s attention via the lens of kernel</title>
		<author>
			<persName><forename type="first">Y.-H</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11">November 2019</date>
			<biblScope unit="page" from="4343" to="4352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>CoRR, abs/1906.08237</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Q8BERT: quantized 8bit BERT</title>
		<author>
			<persName><forename type="first">O</forename><surname>Zafrir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Boudoukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Izsak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wasserblat</surname></persName>
		</author>
		<idno>CoRR, abs/1910.06188</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
