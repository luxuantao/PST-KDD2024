<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Microarchitectural Implications of Event-driven Server-side Web Applications</title>
				<funder>
					<orgName type="full">AMD Corporation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuhao</forename><surname>Zhu</surname></persName>
							<email>yzhu@utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Richins</surname></persName>
							<email>drichins@utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthew</forename><surname>Halpern</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vijay</forename><forename type="middle">Janapa</forename><surname>Reddi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Microarchitectural Implications of Event-driven Server-side Web Applications</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Microarchitecture</term>
					<term>Event-driven</term>
					<term>JavaScript</term>
					<term>Prefetcher</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Enterprise Web applications are moving towards serverside scripting using managed languages. Within this shifting context, event-driven programming is emerging as a crucial programming model to achieve scalability. In this paper, we study the microarchitectural implications of server-side scripting, JavaScript in particular, from a unique event-driven programming model perspective. Using the Node.js framework, we come to several critical microarchitectural conclusions. First, unlike traditional server-workloads such as CloudSuite and BigDataBench that are based on the conventional threadbased execution model, event-driven applications are heavily single-threaded, and as such they require significant singlethread performance. Second, the single-thread performance is severely limited by the front-end inefficiencies of today's server processor microarchitecture, ultimately leading to overall execution inefficiencies. The front-end inefficiencies stem from the unique combination of limited intra-event code reuse and large inter-event reuse distance. Third, through a deep understanding of event-specific characteristics, architects can mitigate the front-end inefficiencies of the managed-languagebased event-driven execution via a combination of instruction cache insertion policy and prefetcher.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Processor architecture advancements have been largely driven by careful observations made of software. By examining and leveraging the inherent workload characteristics, such as instruction-, thread-, and data-level parallelism, processor architects have been able to deliver more efficient computing by mapping software efficiently to the hardware substrate. We must continue to track the developments in the software ecosystem in order to sustain architecture innovation.</p><p>At the cusp of the software evolution are managed scripting languages, which provide portability, enhanced security guarantees, extensive library support, and automatic memory management. In particular, JavaScript is the peak of all the programming languages, surpassing C, C++, and Java to be the most widely used language by developers <ref type="bibr" target="#b0">[1]</ref>. From interactive applications in mobile systems to large-scale analytics software in datacenters, JavaScript is ushering in a new era of execution challenges for the underlying processor architecture.</p><p>In this paper, we focus on server-side JavaScript, specifically its implications on the design of future server processor architectures. While there are numerous studies that have focused on various aspects of dynamic languages on hardware, such as garbage collection <ref type="bibr" target="#b1">[2]</ref>, type checking <ref type="bibr" target="#b2">[3]</ref>, exploiting parallelisms <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, and leveraging hardware heterogeneity <ref type="bibr" target="#b5">[6]</ref>, we study the implications of the programming model that is emerging in server-side JavaScript applications, i.e., asynchronous event-driven programming.</p><p>In server-side asynchronous event-driven programming <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, user requests are treated as application events and inserted into an event queue. Each event is associated with an event callback. The event-driven system employs a single-threaded event loop that traverses the event queue and executes any available callbacks sequentially. Event callbacks may initiate additional I/O events that are executed asynchronously to the event loop in order to not block other requests. The event-driven model has a critical scalability advantage over the conventional thread-based model because it eliminates the major inefficiencies associated with heavy threading, e.g., context switching and thread-local storage <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. Thus, the eventdriven model has been widely adopted in building scalable Web applications, mainly through the Node.js <ref type="bibr" target="#b10">[11]</ref> framework.</p><p>We find that event-driven server applications are fundamentally bounded by single-core performance because of their reliance on the single-threaded event loop. However, unlike conventional single-threaded benchmarks (e.g., <ref type="bibr">SPEC CPU 2006)</ref> for which current processor architectures are highly optimized, event-driven server applications suffer from severe microarchitecture inefficiencies, particularly front-end bottlenecks, i.e., high instruction cache and TLB misses and branch misprediction rate. Moreover, unlike conventional heavily threaded enterprise workloads that also suffer from front-end   issues, the front-end bottleneck of an event-driven server application stems from the single-threaded event execution model, rather than microarchitectural resources being clobbered by multiple threads. With the front-end constituting up to half of the execution cycles, it is clear that current server processor designs are suboptimal for executing event-driven workloads.</p><p>To improve the front-end efficiency of event-driven server applications, we study them from an event perspective. We find that the severe front-end issue arises fundamentally because events have large instruction footprints with little intraevent code reuse. Recent studies on client-side event-driven applications also derive similar conclusions <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. We take this research a step further to make the key observation that event-driven programming inherently exposes strong interevent code reuse. Taking the L1 I-cache as a starting point, we show that coordinating the cache insertion policy and the instruction prefetcher can exploit the unique inter-event code reuse and reduce the I-cache MPKI by 88%.</p><p>In summary, we make the following contributions: ? To the best of our knowledge, we are the first to systematically characterize server-side event-driven execution inefficiencies, particularly the front-end bottlenecks. ? We tie the root-cause of front-end inefficiencies to characteristics inherent to the event-driven programming model, which gives critical microarchitectural optimization insights. ? We show that it is possible to drastically optimize away the instruction cache inefficiencies by coordinating the cache insertion policy and prefetching strategy. The remainder of the paper is structured as follows. Sec. 2 provides a background into asynchronous event-driven programming and why it has emerged as a crucial tipping point in server-side programming. Sec. 3 presents the workloads we study and shows the event-driven applications' single-threaded nature. Sec. 4 discusses our microarchitecture analysis and presents the extreme front-end bottlenecks. Sec. 5 shows that it is possible to leverage the inherent event-driven execution characteristics to largely mitigate instruction cache inefficiencies through a combined effort between cache insertion policy and a suitable prefetcher. Sec. 6 discusses the related work, and Sec. 7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>Web applications employ server-side scripting to respond to network requests and provide dynamic content to end-users. The traditional approach to providing responsiveness to endusers at scale has been thread-based programming, i.e., to increase the number of threads as the number of incoming requests increases. Recently, because of several fundamental limitations of heavy multi-threading that limit system scalability, many industry leaders, such as eBay, PayPal, and LinkedIn, have started to adopt event-driven programming as an alternative to achieve scalability more efficiently.</p><p>In this section, we first discusses thread-based execution and its limitations (Sec. 2.1). On that basis, we explain why event-driven programming is emerging as an alternative for developing large-scale Web applications (Sec. 2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Thread-based Programming</head><p>Traditional server-side scripting frameworks, such as PHP and Ruby, pair user requests with threads, commonly known as the "thread-per-request" or "single-request-per-script" execution model. These thread-based execution models, shown in generality in Fig. <ref type="figure" target="#fig_0">1</ref>, consist of a dispatch thread that assigns each incoming request to a worker thread for processing. The result is that at any given time, the server abounds with the same number of threads as the number of requests.</p><p>While the thread-based execution model is intuitive to program with, it suffers from fundamental drawbacks. As the number of client requests increases, so does the number of active threads in the system. As a result, the operating system overhead, such as thread switching and aggregated memory footprint, grows accordingly. Therefore, operating systems typically have a maximum number of threads that they support, which fundamentally limits the server scalability <ref type="bibr" target="#b9">[10]</ref>.</p><p>To address the scalability limitations of thread-based execution on a single node, modern datacenters scale-out the hardware resources. Instead of scaling the number of threads on a single compute node, threads are distributed and balanced across a large number of compute nodes. However, increasing the number of physical compute notes has direct financial im- Let's Chat <ref type="bibr" target="#b14">[15]</ref> Messaging Multi-person messaging platform, akin to Google Chat and WhatsApp, where users participate in group chats. Each enters a group chat and then sends and receives messages.</p><p>Lighter <ref type="bibr" target="#b15">[16]</ref> Content Management A blogging platform comparable to Blogspot. Each of the users requests resources, such as HTML, CSS, JavaScript, and images, corresponding to blog post webpages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mud [17]</head><p>Gaming A real-time multi-user dungeon game with multiple users playing the game simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Todo [18]</head><p>Task Management A productivity tool and service, similar to the Google Task list management service within Gmail. Users create, modify, and delete multiple tasks within their task lists.</p><p>Word Finder <ref type="bibr" target="#b17">[19]</ref> API Services A word search engine that finds words matching a user-specified pattern. The pattern is searched against a 200,000-word corpus. Users execute several pattern queries.</p><p>plications for the service provider. Scaling out requires more power, cooling, and administrative demands which directly affect total cost of ownership <ref type="bibr" target="#b18">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Event-driven Programming</head><p>A more scalable alternative to the conventional thread-perrequest execution model is event-driven execution, as employed in Node.js <ref type="bibr" target="#b10">[11]</ref>. Fig. <ref type="figure" target="#fig_1">2</ref> shows the event-driven execution model. Incoming I/O requests are translated to events, each associated with an event handler, also referred to as a callback function. Events are pushed into an event queue, which is processed by the single-threaded event loop. Each event loop iteration checks if any new I/O events are waiting in the queue and executes the corresponding event handlers sequentially. An event handler may also initiate additional I/O operations, which are executed asynchronously with respect to the event loop in order to free the main event loop. Event-driven server designs achieve orders of magnitude performance improvements over their thread-based counterparts in both industry <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b20">22]</ref> and academia <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, because they are not limited by the number of threads a system supports. Rather, their scalability depends on the performance of the single-threaded event loop. As such, event-driven programming restores the emphasis on scale-up single-core processor designs for large-scale Web applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Event-driven Server Applications</head><p>Event-driven server-side scripting has not been extensively investigated in the past. In this section, we identify several important Web application domains that have embraced eventdriven programming and describe the workloads we use to represent them (Sec. 3.1). These applications use Node.js, which is the most popular server-side event-driven platform based on JavaScript <ref type="bibr" target="#b10">[11]</ref>. Numerous companies <ref type="bibr" target="#b21">[23]</ref> such as eBay, PayPal, and LinkedIn have adopted it to improve the efficiency and scalability of their application services <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b22">24]</ref>. We then describe how we generate loads to study realistic usage scenarios of these applications (Sec. 3.2).</p><p>Given the selected applications and their loads, we conduct system-level performance analysis on the Node.js software architecture to understand its various applications' execution behaviors (Sec. 3.3). The key observation is that while Node.js is multi-threaded, the single-threaded event loop that sequentially executes all the event callbacks dominates the CPU time. We use this observation as our rationale to focus on analyzing the event loop execution in the rest of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Workloads</head><p>We study important Web application domains that have begun to adopt event-driven programming on the server-side, as shown in Table <ref type="table" target="#tab_1">1</ref>. To enable future research, as well as for reproducibility of our results, we intentionally choose applications that are open-sourced. We release the workload suite at https://github.com/nodebenchmark.</p><p>Document Collaboration Services such as Google Docs and Microsoft Office Online allow multiple users to collaborate on documents in real-time. As users edit documents, they communicate with the application server to report document updates. We study Etherpad Lite <ref type="bibr" target="#b13">[14]</ref>, a real-time collaborative text editing application. Each user creates a new document, makes several edits to that document, and then finally deletes it once all edits have been made.</p><p>Messaging Messengers are amongst the most popular applications used today <ref type="bibr" target="#b23">[25]</ref>. Each user sends and receives messages by communicating with the application server that manages a message database. We study the Let's Chat <ref type="bibr" target="#b14">[15]</ref> messaging application. Each user initiates a new chat, sends multiple messages to other users, and exits the service.</p><p>Content Management Many applications and services from news outlets (e.g. CNN) to blogging platforms (e.g. WordPress) rely on content management platforms to serve a variety of file resources to users. We study Lighter <ref type="bibr" target="#b15">[16]</ref>, which is a blogging platform that serves webpage resources corresponding to a blog post. We focus on users who request different blog entry resources using a regular Web browser.</p><p>Gaming Online computer gaming, already very popular, is increasingly moving to support multiplayer interaction. These (f) Word Finder. games, such as Farmville and Words With Friends, have to manage the shared game state across multiple users. We study the multiplayer game Mud <ref type="bibr" target="#b16">[17]</ref>, wherein each player is assigned a position on a grid. The players can then update their positions to navigate the environment. Task Management Cloud-based task management tools such as Asana and Trello have become increasingly popular for organizations and individuals to stay organized. Tasks can be created, modified, and deleted based on what the user desires. We study a simplified form of task management, using the Todo [18] task management application. Users can create, view, and modify various tasks in a task list.</p><p>API Services Third-party API services provide functionalities that otherwise would not be available in an application. On such example is the Google autocomplete API that aids applications automatically filling out forms <ref type="bibr" target="#b24">[26]</ref>. We restrict our study to Word Finder <ref type="bibr" target="#b17">[19]</ref>. It is a pattern matching API service used for autocompletion and spell checking. Each user queries the API with various word patterns, which are matched against an English dictionary of 200,000 words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Load Generation</head><p>Our study is focused on processor design at the microarchitecture level, thus we focus on a single instance of Node.js. Typically, a production server will run multiple instances of Node.js to handle a massive number of requests <ref type="bibr" target="#b25">[27]</ref>. However, by design, a single instance of Node.js runs primarily in a single thread, coupled with a handful of helper threads.</p><p>All of our applications run at the most recent stable software release of Node.js at the time of writing (version 0.12.3). To exercise the Node.js applications, we develop a query generator to emulate multiple users making requests to the Node.js application under study. We model individual users making requests to the server under realistic usage scenarios, which we obtain by observing requests made by real users. We also interleave and serialize concurrent user requests to the server. This does not change the internal execution characteristics of Node.js application-because events will eventually be serialized by the single-threaded event loop-but enables crucial reproducibility across experiments. Unless otherwise noted, our results are based on simulating 100 users to minimize experiment runtime, as our detailed microarchitectural analyses are based on simulations. We verified that load-testing with a larger number of users or using different request inter-leavings did not change the results and conclusions presented throughout the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Performance Analysis</head><p>In order to understand how these applications exercise the Node.js runtime and guide the simulations used throughout the remainder of the paper, we conduct a system-level performance analysis using the Intel VTune system profiler tool on a quad-core Intel i5 processor. Because we are running on real hardware, in this section we are able to conduct performance analysis using 100,000 users for each application.</p><p>While Node.js is actually multi-threaded, we find that the execution time of each Node.js application is primarily computebound within the single-threaded event loop that is responsible for executing JavaScript-based event callbacks. Our measured results emphasize the need to deeply understand the eventdriven execution nature of these workloads.</p><p>Event Loop Although the Node.js architecture possesses multiple threads for handling asynchronous I/O, its computation is bounded by the single-threaded event loop. The thread-level parallelism (TLP <ref type="bibr" target="#b26">[28]</ref>) column in Table <ref type="table" target="#tab_3">2</ref> shows that Node.js applications are effectively single-threaded, indicating the importance of single core performance. This aligns with the experience reported from industry <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b27">29]</ref>.</p><p>To further understand the importance of single core performance, we study the compute versus memory boundedness of the single-threaded event loop by measuring how the performance changes with the CPU frequency. Fig. <ref type="figure" target="#fig_3">4</ref> shows each application's normalized execution time as the CPU frequency scales from the peak (3.2 GHz) down to 50%. We observe that the overall performance scales almost linearly with the CPU frequency. For example, for Mud, halving the CPU frequency translates to almost 2X slowdown, emphasizing the importance of single-core performance.  Due to the dominant nature of the event loop thread, we provide an execution breakdown of the event loop for all Node.js applications in Fig. <ref type="figure" target="#fig_2">3</ref>. We divide the event loop execution time into four major categories: event callback execution, event loop management (through libuv [30]), idle, and other. We see that event callback execution dominates the event loop execution time. It consumes between 85% (Let's Chat) and nearly 100% (Word Finder) of the event loop execution time. In contrast, the event loop management overhead is minimal. In all applications but Mud, the event loop management is responsible for less than 1% of the execution time. Idleness due to I/O is also relatively small across all of the applications. Except for Let's Chat whose idleness is 10%, the other applications exhibit idleness of less than 5%.</p><p>Event Callbacks Because of the dominance of event callback execution in the event loop thread, we provide further details of callback execution behaviors. Event callbacks are written in JavaScript. To execute event callbacks, Node.js relies on Google's V8 JavaScript engine <ref type="bibr" target="#b28">[31]</ref>. V8 supports JavaScript callback execution through various functionalities such as just-in-time (JIT) compilation, garbage collection, and built-in libraries. Table <ref type="table" target="#tab_3">2</ref> dissects the callback function execution time into three major categories. Generated indicates the execution of dynamically compiled code of callback functions. VM corresponds to the virtual machine management such as garbage collection and code cache handling. Code-Gen corresponds to the JIT compilation.</p><p>We make two important observations. First, the majority of the callback time is spent executing the application code (Generated). Second, V8 spends little time generating code (Code-Gen), with the largest time spent in Let's Chat at 5.3%. This is important to verify because it confirms that our analysis is conducted on each application's steady state, which is the normal state for server applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Microarchitectural Analysis</head><p>Given the single-threaded nature of the event loop, we conduct microarchitectural analysis to identify the bottlenecks for efficient processing of event-driven server applications. We conduct microarchitectural bottleneck analysis using cycle-perinstruction (CPI) statistics to show that instruction delivery dominates execution overhead (Sec. 4.1). Our finding motivates us to perform analysis on the three major microarchitectural structures that impact instruction delivery efficiency:  Throughout our analysis, we compare the Node.js applications against SPEC CPU 2006 because the latter has long been the de facto benchmark for studying single-threaded performance. A head-to-head comparison between Node.js and SPEC CPU 2006 workloads reveals critical insights into the unique microarchitecture bottlenecks of Node.js. Other serverside applications, such as CloudSuite <ref type="bibr" target="#b29">[32]</ref>, MapReduce <ref type="bibr" target="#b30">[33]</ref>, BigDataBench <ref type="bibr" target="#b31">[34]</ref>, and OLTP <ref type="bibr" target="#b32">[35]</ref> do not specifically emphasize single-thread performance.</p><p>We focus on CPU microarchitecture due to the importance of single-core performance. Hence, our experimental setup is geared toward studying core activities and does not capture the I/O effects (i.e., storage and network). Node.js applications may also be I/O intensive. However, a complete I/O characterization is beyond the scope of our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Microarchitectural Bottleneck Analysis</head><p>We analyze the microarchitecture bottlenecks for eventdriven Node.js applications by examining their cycle-perinstruction (CPI) stacks. A CPI stack breaks down the execution time of an application into different microarchitectural activities (e.g., accessing cache), showing the relative contribution of each activity. Optimizing the largest component(s) in the CPI stack leads to the largest performance improvement. Therefore, CPI stacks are used to identify sources of microarchitecture inefficiencies <ref type="bibr" target="#b33">[36,</ref><ref type="bibr" target="#b34">37]</ref>.</p><p>We use SniperSim <ref type="bibr" target="#b35">[38]</ref> to simulate all the Node.js applications and generate their corresponding CPI stacks. The CPI stack for the main event loop thread within each application is shown in Fig. <ref type="figure" target="#fig_4">5</ref>. Components on each bar represents the percentage of cycles that an application spends on a particular type of microarchitectural activity. For example, the base component represents an application's execution time if there were no pipeline stalls. The ifetch and branch components indicate the total processing overhead due to instruction cache misses and branch mispredictions, respectively. The memcomponents indicate the time spent accessing different memory hierarchy levels. The syncand imbalance-end components correspond to multithreaded execution overheads.</p><p>We make two observations from Fig. <ref type="figure" target="#fig_4">5</ref>. First, about 80% of the processing time is spent on various types of on-chip microarchitectural activities. Amongst all sources of overall processing overhead, fetching instructions (ifetch) and branch  prediction (branch) emerge as the two most significant sources. These two components alone contribute about 50% of the total execution overhead, which implies that delivering instructions to the backend of the pipeline is of critical importance to improve the performance of event-driven Node.js applications. Second, the processing overhead for synchronization (syncsleep and sync-futex) is negligible. We corroborate this result by performing VTune analysis on 50,000 users. We find that the time spent on synchronization is only about 0.5%. This is not an unexpected result because Node.js uses a single thread to execute event callbacks, and all I/O operations are asynchronous without having to explicitly synchronize. This implies that improving the performance of synchronization primitives in the processor will likely not yield much benefit for event-driven server applications.</p><p>Thus, we devote the rest of the section to performing detailed analysis on the three microarchitectural resources that significantly impact the front-end's execution efficiency: the L1 instruction cache and L1 instruction TLB (for instruction fetching) and the branch predictor (for branch prediction).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Instruction Cache Analysis</head><p>To understand the front-end execution inefficiency of Node.js applications, we start by examining the workloads' instruction cache (I-cache) behavior. We sweep a wide range of cache configurations in order to study the workloads' instruction footprints. We show that all the Node.js applications suffer from significantly higher misses per kilo-instruction (MPKI) than the vast majority of SPEC CPU 2006 applications on a standard cache configuration. To achieve SPEC CPU-like MPKI, the processor core would require an I-cache so large that it cannot be practically implemented in hardware.</p><p>Current Design Performance Implications We study a modern CPU cache with 32 KB capacity, 64-byte line size, and 8-way set associativity. At this default configuration, we examine the I-cache's efficiency using the MPKI metric. We compare the Node.js programs against the SPEC CPU 2006 workloads' MPKIs, and present the results in Fig. <ref type="figure" target="#fig_5">6a</ref>. We make two important observations. First, the average I-cache MPKI of Node.js applications is 4.2 times higher than that of the SPEC applications. Even Etherpad, which has the lowest I-cache MPKI of all the Node.js applications, shows over twice the MPKI of the SPEC average (indicated by the horizontal dotted line in the figure). At the other extreme, Word Finder and Mud have I-cache MPKIs higher than all but one of the SPEC applications.</p><p>Second, the typical behavior of event-driven applications is on par with the worst-case behavior of single-threaded SPEC CPU 2006 applications that are known to stress the microarchitecture. The event-driven Node.js applications have MPKIs comparable to some of the worst MPKI of SPEC applications, such as gobmk, omnetpp, and cactusADM.</p><p>To understand the reason for the poor I-cache performance, we study the instruction reuse distance to quantify the applications' working set size. Reuse distance is defined as the number of distinct instructions between two dynamic instances of the same instruction <ref type="bibr" target="#b36">[39]</ref>. Fig. <ref type="figure">7</ref> shows the instruction reuse distances for all of the Node.js application. Each (x, y) point corresponds to the percentage of instructions (y) that are at or below a particular reuse distance (x). For comparative purposes, we also include two extreme applications from the SPEC CPU 2006 suite: lbm has the lowest I-cache MPKIs and omnetpp suffers from the one of the highest I-cache MPKIs.</p><p>The event-driven Node.js applications have very large reuse distances. The instruction footprint of omnetpp, the worst SPEC CPU 2006 application, can be effectively captured within a reuse distance of 2 11 . In our measurement, the average instruction size is about 4 bytes; this means an I-cache of just 8 KB would be sufficient to capture omnetpp's instruction locality (assuming a fully-associative cache). In contrast, Let's Chat has a significantly larger reuse distance of up to 2 18 instructions, requiring a cache of 1 MB to capture.</p><p>For comparison purposes, we also examine the data cache behavior of Node.js applications, and compare and contrast it against the SPEC CPU 2006 applications. Fig. <ref type="figure" target="#fig_5">6b</ref> shows the D-cache MPKI of Node.js and SPEC CPU 2006 applications. Event-driven Node.js applications do not appear to stress the data cache heavily. All the Node.js applications have MPKIs that are significantly lower than the SPEC CPU average. Even the extreme cases, Word Finder and Mud, which have the highest MPKIs of 37 and 34, are comparable to the lowest MPKI of SPEC CPU applications.</p><p>Ideal Resource Requirements To determine the ideal instruction cache resource requirements for our event-driven Node.js applications, we sweep the I-cache size and determine application sensitivity under a variety of resource configurations. We find that the instruction working set sizes approach 1 MB, which far exceeds the typical L1 cache capacity.  In Fig. <ref type="figure">8</ref>, the cache size is swept from 16 KB to 1024 KB on the x-axis (in log-scale) and the resulting MPKIs of the Node.js applications are shown on the y-axis. The SPEC CPU 2006 average I-cache MPKI for a cache of 32 KB is indicated by the horizontal dotted line.</p><p>The most significant observation from Fig. <ref type="figure">8</ref> is that the I-cache MPKI keeps improving as the cache size increases for all of the Node.js applications. Some applications such as Word Finder and Etherpad show a knee in their MPKIs at the 128 KB cache size. However, it is not until 256 KB, or even 512 KB, that all the event-driven applications have MPKIs that are comparable to the SPEC CPU 2006 average. Such a large L1 I-cache is infeasible for practical implementation.</p><p>Instruction cache performance on the event-driven applications cannot be easily improved by adjusting conventional cache parameters, such as line size and associativity. Using Mud, which has an MPKI close to the average of all Node.js applications, as an example, Fig. <ref type="figure">9</ref> shows the impact of line size and associativity while keeping the same cache capacity. The line size is held constant while sweeping the associativity from 4 to 16 ways, and then holding the associativity at 8 ways while sweeping the line size from 32 to 128 bytes. The I-cache MPKI is improved when the two parameters are properly selected. For example, increasing the line size from 64 bytes to 128 bytes improves the MPKI by nearly 25%, indicating that Node.js applications exhibit a noticeable level of spatial locality in their code execution behavior. However, the 43 MPKI on a 128-byte line is still significantly higher to the average 14 MPKI of SPEC CPU 2006 applications.</p><p>Comparing the impact of the two parameters, cache line size and associativity, changing the associativity has less impact than changing the cache line size. Increasing the cache associativity actually worsens the MPKI on average by about 10 for Mud. Between increasing associativity and line size while keeping the cache size the same, increasing the line size to capture spatial locality is a better design trade-off than increasing the associativity to reduce cache conflict misses. But even this cannot reduce the cache misses to the average level of SPEC CPU 2006 workloads. The difference is still as much as two orders of magnitude or more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Branch Prediction Analysis</head><p>Event-driven Node.js applications suffer from bad branch prediction performance. Such behavior stems from the large number of branch instructions in the Node.js applications. In SPEC CPU 2006, only 12% of all dynamic instructions are branches. In Node.js applications, 20% of all instructions are branches. As such, different branch instructions tend to alias into the same branch prediction counters and thus are likely to pollute each other's predictions. We further show that reducing branch aliasing by attempting to simply scale the branch predictor structures would require excessive resources that are infeasible to implement.</p><p>Current Design Performance Implications We compare Node.js applications with SPEC CPU 2006 applications under three common branch predictor designs-global, local, and tournament predictor. Intel and AMD do not provide the necessary details to mimic the actual implementation. However, they do provide sufficient information about program optimization <ref type="bibr" target="#b37">[40]</ref> that indirectly indicate reasonable predictor parameters. Based on those informational resources, we mimic branch predictor parameters that are typically adopted in today's processors. For all three predictors, we use history registers of 12 bits, which leads to 4 K unique bimodal predictors. The local predictor is configured to use 256 local branch histories. The tournament predictor further utilizes another 4 K bimodal prediction array of its own. Branch misprediction results are shown in Fig. <ref type="figure" target="#fig_0">10</ref>. We draw two conclusions. First, even under the best-performing predictor (the tournament predictor), Node.js applications have an average misprediction rate (8.8%) over 2 times higher than that of SPEC CPU (3.7%). Four Node.js applications (Todo, Mud, Let's Chat, and Lighter) are as hard to predict as the hardest of the SPEC CPU programs (e.g., gobmk and astar).</p><p>Second, the performance difference between the local and global predictors depends heavily on the applications. There-    fore, a tournament predictor is necessary to achieve better prediction results. The local predictor is equivalent to or more accurate than the global predictor for Word Finder and Etherpad but performs worse in the other four applications.</p><p>To understand the high branch misprediction rate for the event-driven applications, we study the possibility for destructive branch aliasing to occur. Destructive branch aliasing arises when two or more branch instructions rely on the same prediction counter. We quantify branch aliasing by capturing the number of unique branches between two dynamic instances of the same branch instruction being predicted. We call this number the "branch aliasing distance," which, conceptually, is similar to instruction reuse distance that indicates the number of unique instructions that occur between two dynamic instances of a specific static instruction.</p><p>We bin the branch aliasing distance of all the bimodal predictors into 18 bins, each represents a single distance from 0 to 16 and 17+. Zero-aliasing distance is ideal because it indicates that the branch predictor predicts for the same branch instruction back-to-back without any intervening interference from the other branches. A non-zero value for the reuse distance indicates the degree of branch aliasing.</p><p>Fig. <ref type="figure" target="#fig_8">11</ref> shows the branch aliasing distances for Node.js applications. It also includes the average for SPEC CPU 2006 applications. Each (x, y) point in the figure corresponds to the percentage of dynamic branch instruction instances (y) that are at a particular branch aliasing distance (x).</p><p>Node.js applications suffer from heavier branch aliasing  than SPEC CPU 2006. For the global predictor, Fig. <ref type="figure" target="#fig_8">11a</ref> shows that about 90% of the branch predictions in the SPEC applications have zero-aliasing distance. By comparison, in the Node.js applications that figure drops to about 60%. Furthermore, about 10% of the predictions in Node.js applications have an aliasing distance 17+. SPEC has none that far. The contrast between Node.js and SPEC applications is more prominent in the local predictor (Fig. <ref type="figure" target="#fig_8">11b</ref>). Over 50% of the Node.js predictions have aliasing distance 17+ while only about 10% do in the SPEC applications. The local aliasing is higher than the global aliasing because local histories are more varied than the global history. Note that we omit the tournament predictor's aliasing as it is indexed identically to the global predictor and so would produce the same results.</p><p>Ideal Resource Requirements To determine whether scaling the hardware resources will address the branch prediction and aliasing issues, we sweep the global and local predictor sizes. Even with much larger predictors, the full set of Node.js applications never becomes universally well-predicted.</p><p>Fig. <ref type="figure" target="#fig_10">12a</ref> shows the misprediction rates of the Node.js applications as the number of prediction table entries in the global predictor changes from 128 (2 7 ) to 64 K (2 16 ). Even with 64 K entries, Word Finder, Todo, Let's Chat, and Lighter still exceed the average SPEC CPU 2006 misprediction rate at the much smaller 4 K entry predictors. In addition, for most of the applications, as predictor size increases, we observe a remarkably linear trend without a knee of the curve. This indicates that the branch misprediction is far entering the diminishing return area, and further reducing the misprediction requires significantly more hardware resources.</p><p>Local predictor trends are similar to the global predictor trends. Fig. <ref type="figure" target="#fig_10">12b</ref> shows the misprediction rates as the number   of local histories increases from 128 (2 7 ) to 16 K (2 14 ). Mud is a notable example in that it approaches the prediction accuracy of Word Finder and Etherpad, which are the easier to predict (see Fig. <ref type="figure" target="#fig_0">10</ref>). The remaining three applications, however, require heavy branch prediction hardware investment to even approach the average of SPEC CPU 2006 applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Instruction TLB Analysis</head><p>Traditionally, I-TLBs have not been the primary focus in TLBrelated studies due to their extremely low miss rates <ref type="bibr" target="#b38">[41]</ref><ref type="bibr" target="#b39">[42]</ref><ref type="bibr" target="#b40">[43]</ref>. However, I-TLB performance is crucial because every instruction fetch requires a TLB lookup, and TLB misses result in expensive page table walks. Our analyses show that the eventdriven Node.js applications suffer from high I-TLB misses. Scaling the number of TLB entries reduces the miss ratio, but only at prohibitive hardware cost.</p><p>Current Design Performance Implications We simulate a TLB using a Pin tool that resembles the TLB found in modern Intel server processors with 64 entries and 4-way set associativity. Because TLB results are typically sensitive to system-level activities and Pin only captures user-level code, we validated our tool's accuracy using hardware performance counters. Its accuracy is within 95% of the measured hardware TLB results on an Intel processor.</p><p>The I-TLB MPKIs of Node.js applications dwarf those of the SPEC CPU 2006 suite. Fig. <ref type="figure" target="#fig_11">13a</ref> compares the I-TLB MPKI of Node.js applications with the SPEC applications. SPEC applications hardly experience any I-TLB misses whereas almost all Node.js applications have close to 3 MPKI. In stark contrast, Node.js applications fall far short of the worst applications in SPEC in terms of D-TLB MPKIs. As Fig. <ref type="figure" target="#fig_11">13b</ref> shows, Node.js are roughly comparable to the average D-TLB miss rate of SPEC applications.</p><p>To understand whether the poor I-TLB performance is caused by a large code footprint, we analyze the contribution of static code footprint to dynamic instruction execution behavior. Specifically, we study if the event-driven Node.js applications contain a few hot instructions or a lot of cold instructions that contribute to a majority of the dynamic instructions that impact the TLB's performance.</p><p>We discover that Node.js applications have a small number of hot instructions that contribute to a large percentage of the total dynamic instruction count. Fig. <ref type="figure" target="#fig_14">16</ref> shows the hotness of static instructions as a cumulative distribution function.</p><p>On average, 5% of the static instructions are responsible for 90% of the dynamic instructions. This behavior is similar to many SPEC CPU 2006 applications whose code footprints are attributed to a few hot static instructions <ref type="bibr" target="#b41">[44]</ref> and yet do not suffer from poor I-TLB performance.</p><p>The data in Fig. <ref type="figure" target="#fig_14">16</ref> suggests that the poor I-TLB performance of Node.js applications is not due to a lack of hot code pages; rather it must be due to the poor locality of execution. Sec. 3.3 showed that the Node.js applications rely heavily on native call bindings that are supported by the V8 VM, thus we hypothesize that the user-level context switches between the Node.js event callbacks and native code (inside the VM) are the main reason for the poor I-TLB performance.</p><p>Ideal Resource Requirements The event-driven Node.js applications require unconventionally large I-TLB sizes to achieve SPEC-like I-TLB performance. Fig. <ref type="figure" target="#fig_13">14</ref> shows the I-TLB behavior as the TLB size is progressively increased from 8 to 512 entries. In order to get SPEC-like behavior (indicated by the arrow and so close to the 0 line as to be nearly indistinguishable from it), the I-TLB would have to be increased to 256 or more entries.</p><p>Building such a large I-TLB is inefficient. Current TLB lookups already impose non-negligible energy costs, and therefore scaling the TLB sizes will likely increase energy per access <ref type="bibr" target="#b42">[45,</ref><ref type="bibr" target="#b43">46]</ref>. In fact, TLB sizes have largely remained stable over the past several generations <ref type="bibr" target="#b44">[47]</ref>.</p><p>The alternative to increasing the TLB size is to use superpages. In event-driven applications, switching to a large page size reduces the MPKI significantly. Fig. <ref type="figure" target="#fig_4">15</ref> compares the I-TLB MPKI under 4 KB and 2 MB (i.e., superpage) page sizes. Although superpages are traditionally used for reducing D-TLB misses <ref type="bibr" target="#b40">[43,</ref><ref type="bibr" target="#b45">48]</ref>, our results indicate that large pages would be helpful for improving I-TLB performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Event-based Optimizations</head><p>To improve the execution efficiency of event-driven applications, we must mitigate several front-end inefficiencies. However, given all of the major bottlenecks in the front-end, this section specifically focuses on alleviating the instruction cache inefficiencies. The insights are likely to be generalizable to the other structures (i.e., TLB and branch predictor).</p><p>We study I-cache misses from an event callback perspective. We find that individual events have large instruction footprints    with little reuse, which leads to cache thrashing. Fortunately, event-driven programming inherently exposes strong interevent instruction reuses (Sec. 5.1). Such heavy inter-event instruction reuse exposes a unique opportunity for improving the instruction cache performance. We demonstrate that it is necessary to coordinate cache insertion policy and instruction prefetcher (Sec. 5.2). The combined efforts reduce the instruction cache MPKI by 88% (Sec. 5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Optimization Opportunity</head><p>We examine event execution along two important dimensions to discover opportunities for mitigating I-cache inefficiencies: intra-event and inter-event. In the intra-event case, execution characteristics correspond to one event, whereas in inter-event execution the characteristics correspond to the shared execution activity across two or more events. We analyze intra-event and inter-event instruction reuse to understand the poor I-cache behavior of event-driven applications. Fig. <ref type="figure" target="#fig_17">17</ref> shows the percentage of instructions (y-axis) that are reused a certain amount of times (x-axis) both within and across events for all six Node.js applications. The reuses are reported as buckets on the x-axis. The n th bucket represents reuses between X n-1 and X n with the exception of the first bucket, which represents less than 32 reuses and the last bucket which represents 256 or more reuses.</p><p>When we consider the event callbacks in isolation (i.e., intra-event) almost 100% of the instructions across all the Node.js are reused less than 32 times. The low intra-event reuse is inherent to event-driven programming. Developers consciously program the event callbacks to avoid hot, computeintensive loops to ensure application responsiveness. Recall that events in the event queue are executed sequentially by the single-threaded event loop, thus all of the events must execute quickly, similar to interrupts (Sec. <ref type="bibr">3.3)</ref>.</p><p>When the low intra-event code reuse is coupled with the large instruction footprint of each event, it leads to the large instruction reuse distance that results in poor I-cache performance (as previously shown in Fig. <ref type="figure">7</ref>). In Fig. <ref type="figure" target="#fig_0">18</ref>, we show the code footprint (i.e., total byte size of static instructions) for all the events in each application. Each (x, y) point in the figure indicates the percentage of events (x) whose footprints are at or below a particular size (y). We overlay a 32 KB line marker to indicate the L1 I-cache capacity. Almost all of the events have a footprint greater than the standard 32 KB I-cache capacity. In some events, the footprints exceed 1 MB. In contrast, instruction reuse is much higher for inter-event application activity. Fig. <ref type="figure" target="#fig_17">17</ref> shows that over 40% of the instructions are reused over 256 times in the inter-event case. Such frequent reuse implies that events share a considerable number of instructions, otherwise the inter-event behavior would be similar to intra-event behavior.</p><p>Inter-event reuse is the direct result of the event-driven programming paradigm: events exercise the same JavaScript VM system functionalities (i.e., the VM category in Table <ref type="table" target="#tab_3">2</ref>). In particular, for Node.js applications, different event callbacks need the same V8 JavaScript runtime features, which provides support for compiler optimizations, inline cache handling, garbage collection, various built-in functions, etc.</p><p>To quantitatively demonstrate that different events indeed share similar code paths within V8's VM, we show instructionlevel similarity between different events. Fig. <ref type="figure" target="#fig_0">19</ref> is a heat map where each (i, j) point in the figure corresponds to an event pair (i, j), where event i appears earlier than event j in the application. Each (i, j) point indicates the percentage of V8 instructions that event j uses that can also be found in event i. The darkness of the heatmap at any given point is proportional to the percentage of code sharing between those two events, as indicated by the color scale on the right side of the figure. For the purposes of presentation, we limit the data in the figure to 100 randomly chosen consecutive events. We verified that the results hold true when we expand the graph to include all events in the application. The figure confirms that most of the events share close to 100% of the V8 code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Optimization Strategy</head><p>The low intra-event reuse coupled with large event footprints suggests that even an optimal cache cannot fully capture the entire working set of all the events. However, the heavy interevent reuse indicates the potential available locality. Intuitively, the instruction cache needs to first retain the "hot" fraction of the event working set in the cache so that at least that portion reduces cache misses. In addition, it is necessary to deploy an instruction prefetcher that can always prefetch instructions that are not fully retained in the cache by capturing the instruction-miss sequence pattern.</p><p>Caching We propose to use the LRU Insertion Policy (LIP) <ref type="bibr" target="#b46">[49]</ref> for the instruction cache (while still maintaining the LRU eviction policy) to retain the hot portion of the event footprint. LIP is known for being able to effectively preserve a subset of a large working set in the cache by inserting all the incoming cache lines into the LRU way instead of the MRU way and only promoting the LRU way to the MRU way if it has a cache hit. As such, the less frequently-used instructions that cause the instruction footprint to exceed the cache capacity will be quickly evicted from the LRU way instead of thrashing the cache. A critical advantage of LIP is that it requires little hardware and design effort and can be readily adopted in existing designs. LIP was originally proposed for last-level caches and used primarily for addressing large data working sets. To the best of our knowledge, we are the first to apply LIP to the instruction stream and show its benefits.</p><p>Prefetching Although LIP preserves a subset of event footprints in the I-cache, improvement is still fundamentally limited by cache capacity. As discussed in Sec. 4.2, simply increasing the cache size will lead to practical design issues. To compensate for cache capacity limitations, we must orchestrate the prefetcher to accurately fetch instructions in the miss sequence. Our key observation is that the instruction miss sequence in event-driven applications exhibits strong recurring patterns, primarily because inter-event execution has significant code similarities. For instance, as Fig. <ref type="figure" target="#fig_0">19</ref> shows, different events heavily share code from the V8 JavaScript engine.</p><p>To quantify the recurring patterns in Node.js applications, we perform oracle analysis to determine the number of repetitive patterns in the instruction cache miss sequence. We use the SEQUITUR <ref type="bibr" target="#b47">[50]</ref> tool, which is widely used to detect patterns in a given stream, to analyze the miss instruction stream of an LIP cache. We classify instruction cache misses into three categories as originally defined in <ref type="bibr" target="#b48">[51]</ref>. Non-repetitive misses do not belong to any recurring pattern. New misses are those instructions misses that appear in a pattern when it first occurs. The subsequent misses in an recurring pattern are classified as Opportunity misses.</p><p>The oracle repetitive pattern analysis results are shown in Fig. <ref type="figure" target="#fig_1">20</ref>. For all the Node.js applications, over 50% of the cache misses are opportunity misses. This means up to 50% of the instruction misses can be eliminated if the prefetcher can capture all the recurring patterns and accurately match instruction misses to their corresponding patterns.</p><p>We propose to use the Temporal Instruction Fetch Streaming (TIFS) prefetcher <ref type="bibr" target="#b48">[51]</ref> to prefetch recurring missing instructions. TIFS predicts and prefetches future instruction misses through recording and replaying the recurring instruction miss pattern. Specifically, it records all the missing instructions into an instruction missing log (IML). Upon an instruction miss, TIFS finds the location in IML where the miss address was most recently seen and begins prefetching subsequent instructions from the addresses in the IML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Evaluations</head><p>We evaluate our proposal using an in-house instruction cache Pin tool. The reason we choose to only simulate the instruction cache is that it isolates other microarchitecture effects and provides crisp insights into the I-cache issue.</p><p>We implemented LIP as it was described by Qureshi et al. <ref type="bibr" target="#b46">[49]</ref>. We do not find noticeable differences between LIP and the bimodal insertion policy (BIP) as observed for the LLC in <ref type="bibr" target="#b46">[49]</ref>. Because of the additional hardware cost, we choose LIP instead of BIP. TIFS is implemented as described by Ferdman et al. <ref type="bibr" target="#b48">[51]</ref>. We find that it is sufficient for the IML to keep track of 8 K instruction misses. More IML entries only lead to marginal improvements. The total storage overhead of TIFS is about 100 KB per core.</p><p>The baseline we compare against is the standard LRU cache with a 32 KB I-cache, 64-byte line size, and 8-way set associativity. We compare it with the following schemes. First, we compare it against an instruction cache with LIP insertion policy to understand the effectiveness of retaining the hot fraction of the event working set. Second, we compare it against a LIP-enabled cache with a next-line prefetcher to understand the effectiveness the common prefetching scheme. Third, we compare it against a LIP-enabled instruction cache enhanced with the TIFS prefetcher to understand the benefits of prefetching recurring instruction misses. Finally, we compare against a LIP instruction cache of 128 KB size without TIFS to understand whether the storage overhead introduced by TIFS can be simply used to increase the cache size.</p><p>The I-cache MPKI comparison results are shown in Fig. <ref type="figure" target="#fig_18">21</ref>. We also overlay the average MPKI of SPEC CPU 2006 applications at 32 KB. We see that LIP drastically improves the MPKI by at least 45% and 70% on average compared to the LRU-only cache policy. This suggests that without any prefetching scheme, simple changes to the cache insertion policy can already reduce the I-cache MPKI for Node.js application significantly. In two applications, Word Finder and Etherpad, LIP is able to eliminate almost all of the cache misses. For other applications, however, their MPKIs are still higher than the SPEC applications' average.</p><p>The TIFS-based instruction prefetcher reduces the MPKI by another 60% on top of the cache improvements. As a comparison, using a next-line prefetcher only reduces the MPKI by 33.6%. With TIFS, all applications' MPKI fall below SPEC CPU 2006's average. This shows the necessity of capturing the instruction misses' recurring pattern for prefetching. Combining the LIP cache with TIFS prefetching effectively reduces the I-cache MPKI by 88%, which would otherwise be impossible to achieve without event-specific optimization. LIP+TIFS is almost as effective as an extremely large L1 I-cache. In all but one applications (Mud), LIP+TIFS achieves an equivalent or better MPKI than a 128 KB I-cache.</p><p>Cost Analysis The cost of LIP is negligible. TIFS operations (e.g., logging miss sequences in IML, updating the Index Table ) are off the critical path, following the general design principle of prefetching structures <ref type="bibr" target="#b49">[52,</ref><ref type="bibr" target="#b50">53]</ref>. Hence, TIFS is not likely to affect the cycle time. We also estimate that the additional power consumption of TIFS-related structures is only about 92 mW based on CACTI v5.3 [54].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>Characterization of Emerging Paradigms At the time multicore was starting to become ubiquitous on commodity hardware, Bienia et al. developed the PARSEC multicore benchmark suite <ref type="bibr" target="#b51">[55]</ref>. Similarly, Ranger et al. characterized the implications of MapReduce applications when MapReduce was becoming prevalent in developing large-scale data-center applications. More recently, numerous research efforts have been devoted to characterizing warehouse-scale and big data workloads <ref type="bibr" target="#b29">[32,</ref><ref type="bibr" target="#b31">34,</ref><ref type="bibr" target="#b32">35,</ref><ref type="bibr" target="#b52">[56]</ref><ref type="bibr" target="#b53">[57]</ref><ref type="bibr" target="#b54">[58]</ref>.</p><p>We address a new and emerging computing paradigm, i.e., event-driven programming, as others have done in other domains at the time those domains were becoming important. Although event-driven programming has existed for many years for highly concurrent server architecture <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b55">59,</ref><ref type="bibr" target="#b56">60]</ref>, large-scale simulations <ref type="bibr" target="#b57">[61,</ref><ref type="bibr" target="#b58">62]</ref>, and interactive graphical user interface (GUI) application design <ref type="bibr" target="#b59">[63]</ref>, server-side event-driven applications that are tightly coupled with scripting languages have only recently become important. In this context, our work is the first to present a comprehensive analysis of the microarchitectural bottlenecks of scripting-language-based server-side event-driven applications.</p><p>Asynchronous/Event Execution Analysis Prior work on event-driven applications primarily focus on client-side applications <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> whereas we study server-side applications. While prior art also attributes front-end bottlenecks to little intra-event code reuse and proposes instruction prefetching and pre-execution techniques, we take the event-level analysis a step further to demonstrate heavy inter-event code reuse. As a result, we show that simple changes to the instruction cache insertion policy can drastically improve the front-end efficiency, even without the prefetching. Hempstead et al. <ref type="bibr" target="#b60">[64]</ref> designed a specialized event-driven architecture for embedded wireless sensor network applications. Our work focuses on server-side event-driven programming and studies its implica-tions on the general purpose processor. EBS <ref type="bibr" target="#b61">[65]</ref> improves the energy-efficiency of client-side event-driven Web applications and is orthogonal to the performance study of our paper.</p><p>Scripting Languages Richards et al. explore languagelevel characteristics of client-side JavaScript programs <ref type="bibr" target="#b62">[66]</ref>. Our work studies server-side JavaScript and focuses on the nature of events and their microarchitectural implications. Prior work on improving the performance of JavaScript, especially its dynamic typing system <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b63">67]</ref>, complements our event-level optimizations. Ogasawara conducted source code analysis of server-side JavaScript applications, also using Node.js, and found that little time is spent on dynamically compiled code, leading to limited optimization opportunity <ref type="bibr" target="#b64">[68]</ref>. We take an event perspective and demonstrate significant optimization opportunities by exploiting event-specific characteristics. In addition, the prior work does not focus on or investigate the microarchitectural implications of event-driven execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Concluding Remarks</head><p>As computer architects, it is important to understand how to optimize (micro)architecture in light of emerging application paradigms. This paper systematically studies microarchitectural implications of Node.js applications, which represent the unique intersection between two trends in emerging server applications: managed language systems and event-driven programming. We show that Node.js applications are bottlenecked by front-end inefficiencies. By leveraging heavy inter-event code reuse inherent to event-driven programming, we drastically improve the front-end efficiency by orchestrating the instruction cache insertion policy with an instruction prefetcher. Our results are readily useful for building an optimized server architecture for event-driven workloads and provide a baseline, which further research can build upon.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: In the thread-based execution model, each incoming client request is assigned to a unique thread, which is responsible for returning a request to the client.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: In the event-based execution model, each incoming client request is handled by the single-threaded event loop. I/O operations are handled asynchronously.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Execution time distribution of the single-threaded event loop within the Node.js workloads we study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Application performance as CPU frequency scales.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: CPI stacks for the different Node.js applications. the L1 I-cache (Sec. 4.2), branch predictor (Sec. 4.3), and L1 I-TLB (Sec. 4.4) to understand execution behavior.Throughout our analysis, we compare the Node.js applications against SPEC CPU 2006 because the latter has long been the de facto benchmark for studying single-threaded performance. A head-to-head comparison between Node.js and SPEC CPU 2006 workloads reveals critical insights into the unique microarchitecture bottlenecks of Node.js. Other serverside applications, such as CloudSuite<ref type="bibr" target="#b29">[32]</ref>, MapReduce<ref type="bibr" target="#b30">[33]</ref>, BigDataBench<ref type="bibr" target="#b31">[34]</ref>, and OLTP<ref type="bibr" target="#b32">[35]</ref> do not specifically emphasize single-thread performance.We focus on CPU microarchitecture due to the importance of single-core performance. Hence, our experimental setup is geared toward studying core activities and does not capture the I/O effects (i.e., storage and network). Node.js applications may also be I/O intensive. However, a complete I/O characterization is beyond the scope of our paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: I-and D-cache MPKIs comparison for Node.js applications and SPEC CPU 2006.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :Figure 8 :Figure 9 :</head><label>789</label><figDesc>Figure 7: Instruction reuse distances for Node.js and SPEC CPU applications.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: The branch "reuse" distance for the global and local branch predictors across Node.js and SPEC CPU 2006.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: The sensitivity of branch misprediction rate with respect to global predictor size and local history table size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: I-TLB and D-TLB MPKIs for the Node.js applications and SPEC CPU 2006 applications.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: I-TLB sensitivity to varying TLB sizes for Node.js applications.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Hotness of instructions as a CDF for Node.js applications.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Frequency of instruction reuse across intra-event and inter-event execution behavior for Node.js applications.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 21 :</head><label>21</label><figDesc>Figure 20: Repetition study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 : Summary of event-driven server-side Node.js workloads studied in this paper</head><label>1</label><figDesc>processing engine, similar to services such as Google Docs and Microsoft Office Online, for real-time document editing and management. The users we simulate create documents, add and edit document contents, and delete documents.</figDesc><table><row><cell>Workload</cell><cell>Domain</cell><cell>Description</cell></row><row><cell>Etherpad Lite [14]</cell><cell>Document Collaboration</cell><cell>An online word</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 : Execution characteristics of Node.js applications.</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">System-level</cell><cell cols="2">Callback-level</cell><cell></cell></row><row><cell cols="7">Application User System TLP Generated VM Code Gen</cell></row><row><cell cols="2">Etherpad Lite 98%</cell><cell>2%</cell><cell>1.002</cell><cell>79.8 %</cell><cell>19.21%</cell><cell>0.5%</cell></row><row><cell>Let's Chat</cell><cell>95%</cell><cell>5%</cell><cell>1.010</cell><cell>56.0%</cell><cell>38.21%</cell><cell>5.3%</cell></row><row><cell>Lighter</cell><cell>96%</cell><cell>4%</cell><cell>1.011</cell><cell>55.6 %</cell><cell>40.59%</cell><cell>4.2%</cell></row><row><cell>Mud</cell><cell>99%</cell><cell>1%</cell><cell>1.000</cell><cell>53.7 %</cell><cell>44.09%</cell><cell>1.8%</cell></row><row><cell>Todo</cell><cell cols="3">85% 15% 1.002</cell><cell>63.1 %</cell><cell>33.95%</cell><cell>3.1%</cell></row><row><cell cols="4">Word Finder 99% &lt;1% 1.000</cell><cell>63.8 %</cell><cell>30.66%</cell><cell>5.2%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8.">Acknowledgments</head><p>We are thankful to our colleagues as well as the anonymous reviewers for the many comments that have contributed to this work. This work is partially supported by <rs type="funder">AMD Corporation</rs>. Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The redmonk programming language rankings: January 2015</title>
		<ptr target="http://redmonk.com/sogrady/2015/01/14/language-rankings-1-15/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Myths and realities: The performance impact of garbage collection</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Blackburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGMET-RICS</title>
		<meeting>of SIGMET-RICS</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Checked load: Architectural support for javascript type-checking on mobile processors</title>
		<author>
			<persName><forename type="first">O</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fortuna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eggers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HPCA</title>
		<meeting>of HPCA</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dynamic parallelization of javascript applications using an ultra-lightweight speculation mechanism</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mehrara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Samadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mahlke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc.of HPCA</title>
		<meeting>.of HPCA</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dynamically Accelerating Client-side Web Applications through Decoupled Execution</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mehrara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mahlke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CGO</title>
		<meeting>of CGO</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The yin and yang of power and performance for asymmetric hardware and managed software</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Blackburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ISCA</title>
		<meeting>of ISCA</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Event-driven programming for robust software</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dabek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zeldovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mazi?res</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGOPS European Workshop</title>
		<meeting>of SIGOPS European Workshop</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Why threads are a bad idea (for most purposes)</title>
		<ptr target="http://web.stanford.edu/~ouster/cgi-bin/papers/threads.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SEDA: an architecture for wellconditioned, scalable internet services</title>
		<author>
			<persName><forename type="first">M</forename><surname>Welsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Culler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brewer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SOSP</title>
		<meeting>of SOSP</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A design framework for highly concurrent systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Welsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Gribble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Brewer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Culler</surname></persName>
		</author>
		<idno>No. UCB/CSD-00-1108</idno>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Node.js</title>
		<author>
			<persName><forename type="first">Inc</forename><surname>Joyent</surname></persName>
		</author>
		<ptr target="https://nodejs.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efetch: optimizing instruction fetch for event-driven webapplications</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chadha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mahlke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanasamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of PACT</title>
		<meeting>of PACT</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Accelerating asynchronous programs through event sneak peek</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chadha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mahlke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanasamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ISCA</title>
		<meeting>of ISCA</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Etherpad Lite</title>
		<ptr target="https://github.com/ether/etherpad-lite" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Let&apos;s Chat</title>
		<ptr target="https://github.com/sdelements/lets-chat" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Lighter</title>
		<ptr target="https://github.com/mehfuzh/lighter" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Mud</title>
		<ptr target="https://github.com/gumho/simple-node.js-mud" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Word Finder</title>
		<ptr target="https://github.com/amirrajan/word-finder" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cost-effective parallel computing</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Exclusive: How linkedin used node.js and html5 to build a better, faster app</title>
		<ptr target="https://www.paypal-engineering.com/2013/11/22/node-js-at-paypal/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Node.js at paypal</title>
		<ptr target="https://www.paypal-engineering.com/2013/11/22/node-js-at-paypal/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Projects, applications, and companies using node</title>
		<ptr target="https://github.com/joyent/node/wiki/Projects" />
		<imprint/>
		<respStmt>
			<orgName>Applications, -and-Companies-Using-Node</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">How we built ebay&apos;s first node.js application</title>
		<ptr target="http://www.ebaytechblog.com/2013/05/17/how-we-built-ebays-first-node-js-application/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Why Apps for Messaging Are Trending</title>
		<ptr target="http://www.nytimes.com/2015/01/26/technology/why-apps-for-messaging-are-trending.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Autocomplete for Addresses and Search Terms</title>
		<ptr target="https://developers.google.com/maps/documentation/javascript/places-autocomplete" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Node.js Cluster</title>
		<author>
			<persName><forename type="first">Inc</forename><surname>Joyent</surname></persName>
		</author>
		<ptr target="https://nodejs.org/api/cluster.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Thread-level parallelism and interactive performance of desktop applications</title>
		<author>
			<persName><forename type="first">K</forename><surname>Flautner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Uhlig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mudge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ASPLOS</title>
		<meeting>of ASPLOS</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Node.js high availability at box</title>
		<ptr target="https://www.box.com/blog/node-js-high-availability-at-box/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Chrome V8</title>
		<ptr target="https://developers.google.com/v8/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Clearing the clouds: A study of emerging scale-out workloads on modern hardware</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kocberber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alisafaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ASPLOS</title>
		<meeting>of ASPLOS</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Evaluating mapreduce for multi-core and multiprocessor systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ranger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raghuraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Penmetsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HPCA</title>
		<meeting>of HPCA</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bigdatabench: a big data benchmark suite from internet services</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HPCA</title>
		<meeting>of HPCA</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Slicc: Self-assembly of instruction cache collectives for oltp workloads</title>
		<author>
			<persName><forename type="first">I</forename><surname>Atta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tozun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of MICRO</title>
		<meeting>of MICRO</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="188" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A performance counter architecture for computing accurate cpi components</title>
		<author>
			<persName><forename type="first">S</forename><surname>Eyerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karkhanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ASPLOS</title>
		<meeting>of ASPLOS</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Using cycle stacks to understand scaling bottlenecks in multi-threaded workloads</title>
		<author>
			<persName><forename type="first">W</forename><surname>Heirman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Carlson1</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Skadron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IISWC</title>
		<meeting>of IISWC</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An evaluation of high-level mechanistic core models</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Heirman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eyerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Microarchitecture-independent workload characterization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>IEEE Micro</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Intel 64 and ia-32 architectures optimization reference manual</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Colt: Coalesced large-reach tlbs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vaidyanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>in Prof. of MICRO</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Characterizing the d-tlb behavior of spec cpu2000 benchmarks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Kandiraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sivasubramaniam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGMETRICS</title>
		<meeting>of SIGMETRICS</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Practical, transparent operating system support for superpages</title>
		<author>
			<persName><forename type="first">J</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Druschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of OSDI</title>
		<meeting>of OSDI</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Analysis of redundancy and application balance in the spec cpu2006 benchmark suite</title>
		<author>
			<persName><forename type="first">A</forename><surname>Phansalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ISCA</title>
		<meeting>of ISCA</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Reducing memory reference energy with opportunistic virtual caching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ISCA</title>
		<meeting>of ISCA</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Exascale: Opportunities and challenges</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sodani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO 2011 Keynote address</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Intel 64 and ia-32 architectures optimization reference manual</title>
		<ptr target="http://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-optimization-manual.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Tradeoffs in supporting two page sizes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Talluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ISCA</title>
		<meeting>of ISCA</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Adaptive insertion policies for high performance caching</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C S</forename><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ISCA</title>
		<meeting>of ISCA</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Identifying hierarchical structure in sequences: A linear-time algorithm</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Nevill-Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Temporal instruction fetch streaming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of MICRO</title>
		<meeting>of MICRO</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Coordinated control of multiple prefetchers in multi-core systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of MICRO</title>
		<meeting>of MICRO</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Techniques for bandwidthefficient prefetching of linked data structures in hybrid prefetching systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The parsec benchmark suite: Characterization and architectural implications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bienia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of PACT</title>
		<meeting>of PACT</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Profiling a warehouse-scale computer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kanev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Darago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ISCA</title>
		<meeting>of ISCA</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Thin servers with smart pipes: Designing soc accelerators for memcached</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meisner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Said</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ISCA</title>
		<meeting>of ISCA</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Understanding and designing new server architectures for emerging warehouse-computing environments</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mudge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ISCA</title>
		<meeting>of ISCA</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Flash: An efficient and portable web server</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Druschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zwaenepoel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of USENIX ATC</title>
		<meeting>of USENIX ATC</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">High performance web servers on windows nt: Design and performance</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pyarali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of USENIX Windows NT Workshop</title>
		<meeting>of USENIX Windows NT Workshop</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Hardware support for fine-grained event-driven computation in anton 2</title>
		<author>
			<persName><forename type="first">J</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Kuskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Ierardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">B</forename><surname>Schafer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Towles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ASPLOS</title>
		<meeting>of ASPLOS</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Distributed time, conservative parallel logic simulation on gpus</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of DAC</title>
		<meeting>of DAC</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The x window system</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Scheifler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gettys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM Transactions on Graphics</title>
		<meeting>of ACM Transactions on Graphics</meeting>
		<imprint>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">An ultra low power system architecture for sensor network applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hempstead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mauro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ISCA</title>
		<meeting>of ISCA</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Event-based scheduling for energy-efficient qos (eqos) in mobile web applications</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HPCA</title>
		<meeting>of HPCA</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">An analysisof the dynamic behavior of javascript programs</title>
		<author>
			<persName><forename type="first">G</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lebresne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Burg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vitek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc.of PLDI</title>
		<meeting>.of PLDI</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Improving javascript performance by deconstructing the type system</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Garzar?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Torrellas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>in Prof. of PLDI</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Workload characterization of server-side javascript</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ogasawara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IISWC</title>
		<meeting>of IISWC</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
