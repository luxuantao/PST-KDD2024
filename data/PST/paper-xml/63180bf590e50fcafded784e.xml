<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">XSimGCL: Towards Extremely Simple Graph Contrastive Learning for Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-09-06">6 Sep 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Xia</surname></persName>
							<email>x.xia@uq.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Tong</forename><surname>Chen</surname></persName>
							<email>tong.chen@uq.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Lizhen</forename><surname>Cui</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nguyen</forename><surname>Quoc</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Viet</forename><surname>Hung</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
							<email>h.yin1@uq.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">?</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Information Technology and Electrical Engineering</orgName>
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<settlement>Brisbane</settlement>
									<region>Queensland</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute for Integrated and Intelligent Systems</orgName>
								<orgName type="institution">Griffith University</orgName>
								<address>
									<addrLine>Gold Coast</addrLine>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Shandong University</orgName>
								<address>
									<settlement>Jinan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">XSimGCL: Towards Extremely Simple Graph Contrastive Learning for Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-09-06">6 Sep 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2209.02544v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Recommendation</term>
					<term>Self-Supervised Learning</term>
					<term>Contrastive Learning</term>
					<term>Data Augmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Contrastive learning (CL) has recently been demonstrated critical in improving recommendation performance. The fundamental idea of CL-based recommendation models is to maximize the consistency between representations learned from different graph augmentations of the user-item bipartite graph. In such a self-supervised manner, CL-based recommendation models are expected to extract general features from the raw data to tackle the data sparsity issue. Despite the effectiveness of this paradigm, we still have no clue what underlies the performance gains. In this paper, we first reveal that CL enhances recommendation through endowing the model with the ability to learn more evenly distributed user/item representations, which can implicitly alleviate the pervasive popularity bias and promote long-tail items. Meanwhile, we find that the graph augmentations, which were considered a necessity in prior studies, are relatively unreliable and less significant in CL-based recommendation. On top of these findings, we put forward an eXtremely Simple Graph Contrastive Learning method (XSimGCL) for recommendation, which discards the ineffective graph augmentations and instead employs a simple yet effective noise-based embedding augmentation to create views for CL. A comprehensive experimental study on three large and highly sparse benchmark datasets demonstrates that, though the proposed method is extremely simple, it can smoothly adjust the uniformity of learned representations and outperforms its graph augmentation-based counterparts by a large margin in both recommendation accuracy and training efficiency. The code is released at https://github.com/Coder-Yu/SELFRec.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>R Ecently, a revival of contrastive learning (CL) [1], [2], [3]   has swept across many fields of deep learning, leading to a series of major advances <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Since the ability of CL to learn general features from unlabeled raw data is a sliver bullet for addressing the data sparsity issue <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, it also pushes forward the frontier of recommendation. A flurry of enthusiasm on CL-based recommendation <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> has recently been witnessed, followed by a string of promising results. Based on these studies a paradigm of contrastive recommendation can be clearly profiled. It mainly includes two steps: first augmenting the original user-item bipartite graph with structure perturbations (e.g., edge/node dropout at a certain ratio), and then maximizing the consistency of representations learned from different graph augmentations <ref type="bibr" target="#b2">[3]</ref> under a joint learning framework (shown in Fig. <ref type="figure">1</ref>).</p><p>Despite the effectiveness of this paradigm, we still have no clue what underlies the performance gains. Intuitively, encouraging the agreement between related graph augmentations can learn representations invariant to slight structure perturbations and capturing the essential information of the original user-item bipartite <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b18">[19]</ref>. However, it is than graph augmentations. Optimizing this contrastive loss always leads to more evenly distributed user/item representations regardless of graph augmentations, which implicitly alleviates the pervasive popularity bias <ref type="bibr" target="#b22">[23]</ref> and promotes long-tail items. On the other hand, though not as effective as expected, some types of graph augmentations indeed improve the recommendation performance, which are analogous to the cherry on the cake. However, to pick out the useful ones, a long process of trial-and-error is needed. Otherwise a random selection may degrade the recommendation performance. Besides, it should be aware that repeatedly creating graph augmentations and constructing adjacency matrices bring extra expense to model training. In view of these weakness, it is sensible to substitute graph augmentations with better alternatives. A followup question then arises: Are there more effective and efficient augmentation approaches?</p><p>In our preliminary study <ref type="bibr" target="#b23">[24]</ref>, we had given an affirmative response to this question. On top of our finding that learning more evenly distributed representations is critical for boosting recommendation performance, we proposed a graph-augmentation-free CL method which makes the uniformity more controllable, and named it SimGCL (short for Simple Graph Contrastive Learning). SimGCL conforms to the paradigm presented in Fig. <ref type="figure">1</ref>, but it discards the ineffective graph augmentations and instead adds uniform noises to the learned representations for a far more efficient representation-level data augmentation. We empirically demonstrated that this noise-based augmentation can directly regularize the embedding space towards a more even representation distribution. Meanwhile, by modulating the magnitude of noises, SimGCL can smoothly adjust the uniformity of representations. Benefitting from these characteristics, SimGCL shows superiorities over its graph augmentation-based counterparts in both recommendation accuracy and training efficiency.</p><p>However, in spite of these advantages, the cumbersome architecture of SimGCL makes it less than perfect. In addition to the forward/backward pass for the recommendation task, it requires two extra forward/backward passes for the contrastive task in a mini-batch (shown in Fig. <ref type="figure">2</ref>). Actually, this is a universal problem for all the CL-based recommendation models <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> under the paradigm in Fig. <ref type="figure">1</ref>. What makes it worse is that these methods require that all nodes in the user-item bipartite graph to be present during training, which means their computational complexity is almost triple that of conventional recommendation models. This flaw greatly limits the use of CL-based models at scale.</p><p>In order to address this issue, in this work we put forward an eXtremely Simple Graph Contrastive Learning method (XSimGCL) for recommendation. XSimGCL not only inherits SimGCL's noise-based augmentation but also drastically reduces the computational complexity by streamlining the propagation process. As shown in Fig. <ref type="figure">2</ref>, the recommendation task and the contrastive task of XSimGCL share the forward/backward propagation in a mini-batch instead of owning separate pipelines. To be specific, both SimGCL and XSimGCL are fed with the same input: the initial embeddings and the adjacency matrix. The difference is that SimGCL contrasts two final representations learned </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Layer Contrast</head><p>Fig. <ref type="figure">2</ref>: The architectures of SimGCL and XSimGCL.</p><p>with different noises and relies on the ordinary representations for recommendation, whereas XSimGCL uses the same perturbed representations for both tasks, and replaces the final-layer contrast in SimGCL with the cross-layer contrast. With this new design, XSimGCL is nearly as lightweight as the conventional recommendation model LightGCN <ref type="bibr" target="#b27">[28]</ref>.</p><p>And to top it all, XSimGCL even outperforms SimGCL with its simpler architecture.</p><p>Overall, as an extension to our conference paper <ref type="bibr" target="#b23">[24]</ref>, this work makes the following contributions:</p><p>? We reveal that CL enhances graph recommendation models by learning more evenly distributed representations where the InfoNCE loss is far more important than graph augmentations. ? We propose a simple yet effective noise-based augmentation approach, which can smoothly adjust the uniformity of the representation distribution through contrastive learning.</p><p>? We put forward a novel CL-based recommendation model XSimGCL which is even more effective and efficient than its predecessor SimGCL. ? We conduct a comprehensive experimental study on three large and highly sparse benchmark datasets (two of which were not used in our preliminary study) to demonstrate that XSimGCL is an ideal alternative of its graph augmentation-based counterparts. The rest of this paper is organized as follows. Section 2 investigates the necessity of graph augmentations in the contrastive recommendation and explores how CL enhances recommendation. Section 3 proposes the noise-based augmentation approach and the CL-based recommendation model XSimGCL. The experimental study is presented in Section 4. Section 5 provides a brief review of the related literature. Finally, we conclude this work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">REVISITING GRAPH CONTRASTIVE LEARNING FOR RECOMMENDATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Contrastive Recommendation with Graph Augmentations</head><p>Generally, data augmentations are the prerequisite for CLbased recommendation models <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b24">[25]</ref>. In this section, we investigate the widely used dropout-based augmentations on graphs <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b3">[4]</ref>. They assume that the learned representations which are invariant to partial structure perturbations are high-quality. We target a representative stateof-the-art CL-based recommendation model SGL <ref type="bibr" target="#b11">[12]</ref>, which performs the node/edge dropout to augment the user-item graph. The joint learning scheme in SGL is formulated as:</p><formula xml:id="formula_0">L = L rec + ?L cl ,<label>(1)</label></formula><p>which consists of the recommendation loss L rec and the contrastive loss L cl . Since the goal of SGL is to recommend items, the CL task plays an auxiliary role and its effect is modulated by a hyperparameter ?. As for the concrete forms of these two losses, SGL adopts the standard BPR loss <ref type="bibr" target="#b28">[29]</ref> for recommendation and the InfoNCE loss <ref type="bibr" target="#b21">[22]</ref> for CL. The standard BPR loss is defined as:</p><formula xml:id="formula_1">L rec = - (u,i)?B log ?(e u e i -e u e j ) ,<label>(2)</label></formula><p>where ? is the sigmoid function, e u is the user representation, e i is the representation of an item that user u has interacted with, e j is the representation of a randomly sampled item, and B is a mini-batch. The InfoNCE loss <ref type="bibr" target="#b21">[22]</ref> is formulated as:</p><formula xml:id="formula_2">L cl = i?B -log exp(z i z i /? ) j?B exp(z i z j /? ) ,<label>(3)</label></formula><p>where i and j are users/items in B, z i and z i are L 2 normalized representations learned from two different dropoutbased graph augmentations (namely z i = e i e i 2 ), and ? &gt; 0 (e.g., 0.2) is the temperature which controls the strength of penalties on hard negative samples. The InfoNCE loss encourages the consistency between z i and z i which are the positive sample of each other, whilst minimizing the agreement between z i and z j , which are the negative samples of each other. Optimizing the InfoNCE loss is actually maximizing a tight lower bound of mutual information.</p><p>To learn representations from the user-item graph, SGL employs LightGCN <ref type="bibr" target="#b27">[28]</ref> as its encoder, whose message passing process is defined as:</p><formula xml:id="formula_3">E = 1 1 + L (E (0) + AE (0) + ... + A L E (0) ),<label>(4)</label></formula><p>where More details can be found in the original papers <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b27">[28]</ref>. </p><formula xml:id="formula_4">E (0) ? R |N</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Necessity of Graph Augmentations</head><p>The phenomenon reported in <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>  </p><formula xml:id="formula_5">L cl = i?B -log exp(1/? ) j?B exp(z i z j /? ) .<label>(5)</label></formula><p>Because no augmentations are used in SGL-WA, we have</p><formula xml:id="formula_6">z i = z i = z i .</formula><p>The performance comparison is conducted on three benchmark datasets: Yelp2018 <ref type="bibr" target="#b27">[28]</ref>, Amazon-Kindle <ref type="bibr" target="#b29">[30]</ref> and Alibaba-iFashion <ref type="bibr" target="#b11">[12]</ref>. A 3-layer setting is adopted and the hyperparameters are tuned according to the original paper of SGL (more experimental settings can be found in Section 4.1). The results are presented in Table <ref type="table" target="#tab_2">1</ref> where the highest values are marked in bold type. As can be observed, all the graph augmentation-based variants of SGL outperform LightGCN, which demonstrates the effectiveness of CL. However, to our surprise SGL-WA is also competitive. Its performance is on par with that of SGL-ED and SGL-RW and is even better than that of SGL-ND on all the datasets. Given these results, we can draw two conclusions: (1) graph augmentations indeed work but they are not as effective as expected; the large proportion of performance gains derive from the contrastive loss In-foNCE and this can explain why even very sparse graph augmentations seem to be informative in <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>; (2) not all graph augmentations have a positive impact; a long process of trial-and-error is required to pick out the useful ones. As for (2), the possible reason could be that some graph augmentations highly distort the original graph. For example, the node dropout is very likely to drop the key nodes (e.g., hub) and their associated edges and hence breaks the correlated subgraphs into disconnected pieces. Such graph augmentations share little learnable invariance with the original graph and therefore it is unreasonable to encourage the consistency between them. By contrast, the edge dropout is at a lower risk to largely perturb the original graph so that SGL-ED/RW can hold a slim advantage over SGL-WA. However, in view of the expense of regular reconstruction of the adjacency matrices during training, it is sensible to search for better alternatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Uniformity Is What Really Matters</head><p>The last section has revealed that the contrastive loss In-foNCE is the key. However, we still have no idea how it operates. The previous research <ref type="bibr" target="#b30">[31]</ref> on the visual representation learning has identified that pre-training with the In-foNCE loss intensifies two properties: alignment of features from positive pairs, and uniformity of the normalized feature distribution on the unit hypersphere. It is unclear if the CL-based recommendation methods exhibit similar patterns under a joint learning setting. Since in recommendation the goal of L rec is to align the interacted user-item pair, here we focus on investigating the uniformity.</p><p>In our preliminary study <ref type="bibr" target="#b23">[24]</ref>, we have displayed the distribution of 2,000 randomly sampled users after optimizing the InfoNCE loss. For a thorough understanding, in this version we sample both users and items. Specifically, we first rank users and items according to their popularity. Then 500 hot items are randomly sampled from the item group with the top 5% interactions; the other 500 cold items are randomly sampled from the group with the bottom 80% interactions, and so are the users. Afterwards, we map the learned representations to 2-dimensional space with t-SNE <ref type="bibr" target="#b31">[32]</ref>. All the representations are collected when the models reach their best. Then we plot the 2D feature distributions in Fig. <ref type="figure" target="#fig_1">3</ref>. For a clearer presentation, the Gaussian kernel density estimation <ref type="bibr" target="#b32">[33]</ref> of arctan(feature y/feature x) on the unit hypersphere S 1 is also visualized.</p><p>From Fig. <ref type="figure" target="#fig_1">3</ref> an obvious contrast between the features/density estimations learned by LightGCN and CLbased recommendation models can be observed. In the leftmost column, LightGCN learns highly clustered features and the density curves are with steep rises and falls. Besides, it is easy to notice that the hot users and hot items have similar distributions and the cold users also cling to the hot items; only a small number of users are scattered among the cold items. Technically, this is a biased pattern that will lead the model to continually expose hot items to most users and generate run-of-the-mill recommendations. We think that two issues may cause this biased distribution. One is that in recommender systems a fraction of items often account for most interactions <ref type="bibr" target="#b33">[34]</ref>, and the other is the notorious oversmoothing problem <ref type="bibr" target="#b34">[35]</ref> which makes embeddings become locally similar and hence aggravates the Matthew Effect. By contrast, in the second and the third columns, the features learned by SGL variants are more evenly distributed, and the density estimation curves are less sharp, regardless of if the graph augmentations are applied. For reference, in the forth column we plot the features learned only by optimizing the InfoNCE loss in SGL-ED. Without the effect of L rec , the features are almost subject to uniform distributions. The following inference provides a theoretical justification for this pattern. By rewriting Eq. ( <ref type="formula" target="#formula_2">3</ref>) we can derive,</p><formula xml:id="formula_7">L cl = i?B -z i z i /? + log j?B exp(z i z j /? ) . (6)</formula><p>When the representations of different augmentations of the same node are perfectly aligned (SGL-WA is analogous to this case), we have</p><formula xml:id="formula_8">L cl = i?B -1/? +log j?B/{i} exp(z i z j /? )+exp(1/? ) . (7)</formula><p>Since 1/? is a constant, optimizing the CL loss is actually towards minimizing the cosine similarity between different node representations, which will push different nodes away from each other.</p><p>By aligning the results in Table <ref type="table" target="#tab_2">1</ref> with the distributions in Fig. <ref type="figure" target="#fig_1">3</ref>, it is natural to speculate that the increased uniformity of the learned distribution is what really matters to the performance gains. It implicitly alleviates the popularity bias and promotes the long-tail items (discussed in section 4.2) because more evenly distributed representations can preserve the intrinsic characteristics of nodes and improve the generalization ability. This can also justifies the unexpected remarkable performance of SGL-WA. Finally, it also should be noted that, a positive correlation between the uniformity and the performance only holds in a limited scope. The excessive pursuit to the uniformity will weaken the effect of the recommendation loss to align interacted pairs and similar users/items, and hence degrades the recommendation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TOWARDS EXTREMELY SIMPLE GRAPH CON-TRASTIVE LEARNING FOR RECOMMENDATION</head><p>In this section we propose a substitute of graph augmentations and develop a lightweight architecture for CL-based recommendation.</p><formula xml:id="formula_9">r = ? ? ? ? j ? ? ? ?? ? ? ?? ? ? ? ? 1 ? 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embedding Space</head><p>Fig. <ref type="figure">4</ref>: An illustration of the proposed random noise-based data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Noise-Based Augmentation</head><p>Based on the findings above, we speculate that by adjusting the uniformity of the learned representation in a certain scope, contrastive recommendation models can reach a better performance. Since manipulating the graph structure for controllable uniformity is intractable and time-consuming, we shift attention to the embedding space. Inspired by the adversarial examples <ref type="bibr" target="#b35">[36]</ref> which are constructed through adding imperceptible perturbation to the images, we propose to directly add random noises to the representation for an efficient augmentation.</p><p>Formally, given a node i and its representation e i in the d-dimensional embedding space, we can implement the following representation-level augmentation:</p><formula xml:id="formula_10">e i = e i + ? i , e i = e i + ? i ,<label>(8)</label></formula><p>where the added noise vectors ? i and ? i are subject to ? 2 = and is a small constant. Technically, this constraint of magnitude makes ? numerically equivalent to points on a hypersphere with the radius . Besides, it is required that:</p><formula xml:id="formula_11">? = X sign(e i ), X ? R d ? U (0, 1),<label>(9)</label></formula><p>which forces e i , ? and ? to be in the same hyperoctant, so that adding the noises to e i will not result in a large deviation and construct less informative augmentations of e i . Geometrically, by adding the scaled noise vectors to e i , we can rotate it by two small angles (? 1 and ? 2 shown in Fig. <ref type="figure">4</ref>). Each rotation corresponds to a deviation of e i , and generates an augmented representation (e i and e i ). Since the rotation is small enough, the augmented representation retains most information of the original representation and meanwhile also brings some difference. Particularly, we also hope the learned representations can spread out in the entire embedding space so as to fully utilize the expression power of the space. Zhang et al. <ref type="bibr" target="#b36">[37]</ref> proved that uniform distribution has such a property. We then choose to generate the noises from uniform distribution. Though it is technically difficult to make the learned distribution approximate uniform distribution in this way, it can statistically bring a hint of uniformity to the augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Proposed Contrastive Recommendation Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">A Review of SimGCL</head><p>Before presenting XSimGCL, we first briefly review SimGCL proposed in our conference paper <ref type="bibr" target="#b23">[24]</ref>, which will help understand the new contributions.</p><p>As shown in Fig. <ref type="figure">2</ref>, SimGCL follows the paradigm of graph CL-based recommendation portrayed in Fig. <ref type="figure">1</ref>. It consists of three encoders: one is for the recommendation task and the other two are for the contrastive task. SimGCL likewise adopts LightGCN as the backbone to learn graph representations. Since LightGCN is network-parameter-free, the input user/item embeddings are the only parameters to be learned. The ordinary graph encoding is used for recommendation which follows Eq. ( <ref type="formula" target="#formula_3">4</ref>) to propagate node information. Meanwhile, in the other two encoders SimGCL employs the proposed noise-based augmentation approach, and adds different uniform random noises to the aggregated embeddings at each layer to obtain perturbed representations. This noise-involved representation learning can be formulated as:</p><formula xml:id="formula_12">E = 1 L (AE (0) + ? (1) ) + (A(AE (0) + ? (1) ) + ? (2) ))+ ... + (A L E (0) + A L-1 ? (1) + ... + A? (L-1) + ? (L) )<label>(10)</label></formula><p>Note that we skip the input embedding E (0) in all the three encoders when calculating the final representations, because we experimentally find that CL cannot consistently improve non-aggregation-based models and skipping it will lead to a slight performance improvement. Finally, we substitute the learned representations into the joint loss presented in Eq.</p><p>(1) then use Adam to optimize it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">XSimGCL -Simpler Than Simple</head><p>Compared with SGL, SimGCL is much simpler because the constant graph augmentation is no longer required. However, the cumbersome architecture of SimGCL makes it less than perfect. For each computation, it requires three forward/backward passes to obtain the loss and then backpropagate the error to update the input node embeddings.</p><p>Though it seems a convention to separate the pipelines of the recommendation task and the contrastive task in CL-based recommender systems <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b24">[25]</ref>, we question the necessity of this architecture. As suggested by <ref type="bibr" target="#b37">[38]</ref>, there is a sweet pot when utilizing CL where the mutual information between views is neither too high nor too low. However, in the architecture of SimGCL, the mutual information between two final embeddings could always be very high since these two embeddings both contain information from L hops of neighbors. Due to the minor difference between them, contrasting them with each other may be less effective in learning general features. It is natural to think what if we contrast different layer embeddings? They share some common information but differ in aggregated neighbors and added noises, which conform to the sweet pot theory. Besides, considering that the magnitude of added noises is small enough, we can directly use the perturbed representations for the recommendation task. The noises are analogous to the widely used dropout trick and are only applied in training. In the test phase, the model switches to the ordinary encoding without noises.</p><p>Benefitting from this design of cross-layer contrast, we can streamline the architecture of SimGCL by merging its encoding processes. As a result, the new architecture has only one-time forward/backward pass in a mini-batch computation. We name this new method XSimGCL, which is short for eXtremely Simple Graph Contrastive Learning. We illustrate it in Fig. <ref type="figure">2</ref>. The perturbed representation learning of XSimGCL is as the same as that of SimGCL. The joint loss of XSimGCL is formulated as:</p><formula xml:id="formula_13">L = - (u,i)?B log ?(e u e i -e u e j ) + ? i?B -log exp(z i z l * i /? ) j?B exp(z i z l * j /? ) ,<label>(11)</label></formula><p>where l * denotes the layer to be contrasted with the final layer. Contrasting two intermediate layers is optional, but the experiments in section 4.3 show that involving the final layer leads to the optimal performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Adjusting Uniformity Through Changing</head><p>In XSimGCL, we can explicitly control how far the augmented representations deviate from the original through changing the value of . Intuitively, a larger will result in a more uniform representation distribution. This is because when optimizing the contrastive loss, the added noises are also propagated as part of the gradients. As the noises are sampled from uniform distribution, the original representation is roughly regularized towards higher uniformity. We conduct the following experiment to demonstrate it.</p><p>According to <ref type="bibr" target="#b30">[31]</ref>, the logarithm of the average pairwise Gaussian potential (a.k.a. the Radial Basis Function (RBF) kernel) can well measure the uniformity of representations, which is defined as:</p><formula xml:id="formula_14">L uniform (f ) = log E i.i.d u,v ? p node e -2 f (u)-f (v) 2 2 . (<label>12</label></formula><formula xml:id="formula_15">)</formula><p>where f (u) outputs the L 2 normalized embedding of u.</p><p>We choose the popular items (with more than 200 interactions) and randomly sample 5,000 users in the dataset of Yelp2018 to form the user-item pairs, and then measure the uniformity of representations learned by XSimGCL with Eq. ( <ref type="formula" target="#formula_14">12</ref>). We fix ? = 0.2 and use a 3-layer setting, and then tune to observe how the uniformity changes. The uniformity is checked after every epoch until XSimGCL reaches convergence. As clearly shown in Fig. <ref type="figure" target="#fig_2">5</ref>, similar trends are observed on all the curves. At the beginning, all curves have highly evenly-distributed representations because we use Xavier initialization, which is a special uniform distribution, to initialize the input embeddings. With the training proceeding, the uniformity declines due to the effect of L rec . After reaching the peak the uniformity increases again till convergence. It is also obvious that with the increase of , XSimGCL tends to learn a more even representation distribution. Meanwhile, the better performance is achieved, which evidences our claim that more uniformity brings a better performance in scope. Besides, we notice that there is a correlation between the convergence speed and the magnitude of noises, which will be discussed in section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Complexity</head><p>In this section, we analyze the theoretical time complexity of XSimGCL and compare it with LightGCN, SGL-ED and its predecessor SimGCL. The discussion is within the scope of a single batch since the in-batch negative sampling is a widely used trick in CL <ref type="bibr" target="#b4">[5]</ref>. Here we let |A| be the edge number in the user-item bipartite graph, d be the embedding dimension, B denote the batch size, M represent the node number in a batch, L be the layer number, and ? denote the edge keep rate in SGL-ED. We can derive: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Datasets. For reliable and convincing results, we conduct experiments on three public large-scale datasets: Yelp2018 <ref type="bibr" target="#b27">[28]</ref>, Amazon-kindle <ref type="bibr" target="#b11">[12]</ref> and Alibaba-iFashion <ref type="bibr" target="#b11">[12]</ref> to evaluate XSimGCL/SimGCL. The statistics of these datasets are presented in Table <ref type="table" target="#tab_5">3</ref>. We split the datasets into three parts (training set, validation set, and test set) with a 7:1:2 ratio. Following <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b27">[28]</ref>, we first search the best hyperparameters on the validation set, and then we merge the training set and the validation set to train the model and evaluate it on the test set where the relevancy-based metric Recall@20 and the ranking-aware metric NDCG@20 are used. For a rigorous and unbiased evaluation, the reported result are the average values of 5 runs, with all the items being ranked.</p><p>Baselines. Besides LightGCN and the SGL variants, the following recent data augmentation-based/CL-based recommendation models are compared.</p><p>? DNN+SSL <ref type="bibr" target="#b26">[27]</ref> is a recent DNN-based recommendation method which adopts the similar architecture in Fig. <ref type="figure">1</ref>, and conducts feature masking for CL.</p><p>? BUIR <ref type="bibr" target="#b20">[21]</ref> has a two-branch architecture which consists of a target network and an online network, and only uses positive examples for self-supervised recommendation.</p><p>? MixGCL <ref type="bibr" target="#b38">[39]</ref> designs the hop mixing technique to synthesize hard negatives for graph collaborative filtering by embedding interpolation.</p><p>? NCL <ref type="bibr" target="#b17">[18]</ref> is a very recent contrastive model which designs a prototypical contrastive objective to capture the correlations between a user/item and its context. Hyperparameters. For a fair comparison, we referred to the best hyperparameter settings reported in the original papers of the baselines and then fine-tuned them with the grid search. As for the general settings, we create the user and item embeddings with the Xavier initialization of dimension 64; we use Adam to optimize all the models with the learning rate 0.001; the L 2 regularization coefficient 10 -4 and the batch size 2048 are used, which are common in many papers <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b39">[40]</ref>. In SimGCL, XSimGCL and SGL, we empirically let the temperature ? = 0.2 because this value is often reported a great choice in papers on CL <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b30">[31]</ref>. An exception is that we let ? = 0.15 for XSimGCL on Yelp2018, which brings a slightly better performance. Note that although the paper of SGL <ref type="bibr" target="#b11">[12]</ref> uses Yelp2018 and Alibaba-iFashion as well, we cannot reproduce their results on Alibaba-iFashion with their given hyperparameters under the same experimental setting. So we re-search the hyperparameters of SGL and choose to present our results on this dataset in Table <ref type="table" target="#tab_6">4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SGL vs. XSimGCL: From A Comprehensive Perspective</head><p>In this part, we compare XSimGCL with SGL in a comprehensive way. The experiments focus on three important aspects: recommendation performance, training time, and the ability to promote long-tail items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Performance Comparison</head><p>We first show the performances of SGL and XSimGCL/ SimGCL with different layers. The best hyperparameters of them are provided in Table <ref type="table" target="#tab_7">5</ref> for an easy reproduction of our results. The figures of the best performance are presented in bold and the runner-ups are presented with underline. The improvements are calculated based on the performance of LightGCN. Note that we contrast the final layer with itself in the 1-layer XSimGCL. Based on Table <ref type="table" target="#tab_6">4</ref>, we have the following observations:</p><p>? In the vast majority of cases, the SGL variants, SimGCL and XSimGCL can largely outperform LightGCN. The largest improvements are observed on Alibaba-iFashion where the performance of XSimGCL surpasses that of LightGCN by more than 25% under the 1-layer and 3layer settings. ? SGL-ED and SGL-RW have very close performances and outperform SGL-ND by large margins. In many cases, SGL-WA show advantages over SGL-WA but still falls behind SGL-ED and SGL-RW. These results further corroborate that the InfoNCE loss is the primary factor which accounts for the performance gains, and meanwhile heuristic graph augmentations are not as effective as expected and some of them even degrade the performance. ? XSimGCL/SimGCL show the best/second best performance in almost all the cases, which demonstrates the effectiveness of the random noised-based data augmentation. Particularly, on the largest and sparsest dataset -Alibaba-iFashion, they significantly outperforms the SGL variants. In addition, it is obvious that the evolution from SimGCL to XSimGCL is successful, bringing nonnegligible performance gains.</p><p>To further demonstrate XSimGCL's outstanding performance, we also compare it with a few recent augmentationbased or CL-based recommendation models. The implementations of these methods are available in our GitHub repository SELFRec as well. According to Table <ref type="table" target="#tab_8">6</ref>, XSimGCL and SimGCL still outperform with a great lead, achieving the best and the second best performance, respectively. NCL and MixGCF, which employ LightGCN as their backbones, also show their competitiveness. By contrast, DNN+SSL and BUIR are not as powerful as expected and even not comparable to LightGCN. We attribute their failure to: <ref type="bibr" target="#b0">(1)</ref>. DNNs are proved effective when abundant user/item features are provided. In our datasets, features are available and the self-supervision signals are created by masking item embeddings, so it cannot fulfill itself in this situation.</p><p>(2). In the paper of BUIR, the authors removed long-tail users and items to guarantee a good result, but we use all the data. We also notice that BUIR performs very well on suggesting popular items but poorly on long-tail items. This may explain why the original paper uses a biased experimental setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Comparison of Training Efficiency</head><p>As has been claimed, XSimGCL is almost as lightweight as LightGCN in theory. In this part, we report the actual training time, which is more informative than the theoretical analysis. The reported figures are collected on a workstation with an Intel(R) Xeon(R) Gold 5122 CPU and a GeForce RTX 2080Ti GPU. These methods are implemented with Tensorflow 1.14, and a 2-layer setting is applied to all. According to Fig. <ref type="figure" target="#fig_3">6</ref>, we have the following observations:</p><p>? SGL-ED takes the longest time to finish the computation in a single batch, which is almost four times that of Light-GCN on all the datasets. SimGCL ranks second due to its three-encoder architecture, which is almost two times that of LightGCN. Since SGL-WA, XSimGCL and LightGCN have the same architecture, there training costs for a batch are very close. The former two need a bit of extra time for the contrastive task. ? LightGCN is trained with hundreds of epochs, which is at least an order of magnitude more than the epochs that other methods need. By contrast, XSimGCL needs the fewest epochs to reach convergence and its predecessor SimGCL falls behind by several epochs. SGL-WA and SGL-ED require the same number of epochs to get converged and are slower than SimGCL. When it comes to the total training time, LightGCN is still the method trained with the longest time, followed by SGL-ED and SimGCL. Due to the simple architecture, SGL-WA and XSimGCL are the last two but XSimGCL only needs about half of the cost SGL-WA spends in total. With these observations, we can easily draw some conclusions. First, CL can tremendously accelerate the training. Second, graph augmentations cannot contribute to the training efficiency. Third, the cross-layer contrasts not only brings performance improvement but also leads to faster convergence. By analyzing the gradients from the CL loss, we find that the noises in XSimGCL and SimGCL will add an small increment to the gradients, which works like a momentum and can explain the speedup. Compared with the final-layer contrast, the cross-layer has shorter route for gradient propagation. This can explain why XSimGCL needs fewer epochs compared with SimGCL.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alibaba-iFashion</head><p>LightGCN SGL-WA SGL-ED SimGCL XSimGCL Fig. <ref type="figure">7</ref>: The ability to promote long-tail items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Comparison of Ability to Promote Long-tail Items</head><p>Optimizing the InfoNCE loss is found to learn more evenly distributed representations, which is supposed to alleviate the popularity bias. To verify that XSimGCL upgrades this ability with the noise-based augmentation, we follow <ref type="bibr" target="#b11">[12]</ref> to divide the test set into ten groups, labeled with IDs from 1 to 10. Each group includes the same number of interactions. The larger ID the group has, the more popular items it contains. We then conduct experiments with a 2layer setting to check the Recall@20 value that each group achieves. The results are illustrated in Fig. <ref type="figure">7</ref>.</p><p>According to Fig. <ref type="figure">7</ref>, LightGCN is inclined to recommend popular items and achieves the highest recall value on the last group. By contrast, XSimGCL and SimGCL do not show outstanding performance on group 10, but they have distinct advantages over LightGCN on other groups. Particularly, SimGCL is the standout on Yelp2018 and XSimGCL keeps strong on iFashion. Their extraordinary performance in recommending long-tail items largely compensates for their loss on the popular item group. As for the SGL variants, they fall between LightGCN and SimGCL on exploring long-tail items and exhibit similar recommendation performance on Yelp2018. SGL-ED shows a slight advantage over SGL-WA on iFashion. Combining Fig. <ref type="figure" target="#fig_1">3</ref> with Fig. <ref type="figure">7</ref>, we can easily find that the ability to promote long-tail items seems to positively correlate with the uniformity of representations. Since a good recommender system should suggest items that are most pertinent to a particular user instead of recommending popular items that might have been known, SimGCL and XSimGCL significantly outperforms other methods in this regard.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Noised-Based CL on Other Structures</head><p>We choose MF and the vanilla GCN as the backbones to be tested because these two simple backbones are widely used in practice. For MF that cannot adopt the cross-layer contrast, we add different uniform noises to the input embeddings for different augmentations. We tried many combinations of ? and on these two structures and report the best results in Table <ref type="table" target="#tab_11">7</ref> where NBC is short for noisebased CL. As can be been, NBC can likewise improve GCN. We guess this is because GCN also has an aggregation mechanism. However, NBC cannot consistently improve MF. On the dataset of Amazon-Kindle it works, whereas on the other two datasets, it lowers the performance. This inconsistent effect cannot be easily concluded, and we leave it to our future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">GNNs-Based Recommendation Models</head><p>In recent years, graph neural networks (GNNs) <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref> have brought the regime of DNNs to an end, and become a routine in recommender systems for its extraordinary ability to model the user behavior data. A great number of recommendation models developed from GNNs have achieved greater than ever performances in different recommendation scenarios <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>. Among numerous variants of GNNs, GCN <ref type="bibr" target="#b44">[45]</ref> is the most prevalent one and drives many state-of-the-art graph neural recommendation models such as NGCF <ref type="bibr" target="#b45">[46]</ref>, LightGCN <ref type="bibr" target="#b27">[28]</ref>, LR-GCCF <ref type="bibr" target="#b46">[47]</ref> and LCF <ref type="bibr" target="#b47">[48]</ref>. Despite varying implementation details, all these GCN-based models share a common scheme which is to aggregate information from the neighborhood in the user-item graph layer by layer <ref type="bibr" target="#b41">[42]</ref>. Benefitting from its simple structure, LightGCN becomes one of the most popular GCN-based recommendation models. It follows SGC <ref type="bibr" target="#b48">[49]</ref> to remove the redundant operations in the vanilla GCN including transformation matrices and nonlinear activation functions. This design is proved efficient and effective for recommendation where only the user-item interactions are provided. It also inspires a lot of CL-based recommendation models such as SGL <ref type="bibr" target="#b11">[12]</ref>, NCL <ref type="bibr" target="#b17">[18]</ref> and SimGCL <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Contrastive Learning for Recommendation</head><p>Contrastive learning <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> recently has drawn considerable attention in many fields due to its ability to deal with massive unlabeled data <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b3">[4]</ref>. As CL usually works in a self-supervised manner <ref type="bibr" target="#b2">[3]</ref>, it is inherently a sliver bullet to the data sparsity issue <ref type="bibr" target="#b49">[50]</ref> in recommender systems.</p><p>Inspired by the success of CL in other fields, the community also has started to integrate CL into recommendation <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b15">[16]</ref>. The fundamental idea behind existing contrastive recommendation methods is to regard every instance (e.g., user or item) as a class, and then pull views of the same instance closer and push views of different instances apart when learning representations, where the views are augmented by applying different transformations to the original data. In brief, the goal is to maximize the mutual information between views from the same instance. Through this process, the recommendation model is expected to learn the essential information of the user-item interactions.</p><p>To the best of our knowledge, S 3 -Rec <ref type="bibr" target="#b14">[15]</ref> is the first work that combines CL with sequential recommendation. It first randomly masks part of attributes and items to create sequence augmentations, and then pre-trains the Transformer <ref type="bibr" target="#b52">[53]</ref> by encouraging the consistency between different augmentations. The similar idea is also found in a concurrent work CL4SRec <ref type="bibr" target="#b24">[25]</ref>, where more augmentation approaches including item reordering and cropping are used. Besides, S 2 -DHCN <ref type="bibr" target="#b13">[14]</ref> and ICL <ref type="bibr" target="#b17">[18]</ref> adopt advanced augmentation strategies by re-organizing/clustering the sequential data for more effective self-supervised signals. Qiu et al. proposed DuoRec <ref type="bibr" target="#b53">[54]</ref> which adopts a model-level augmentation by conducting dropout on the encoder. In the same period, CL was also introduced to different graph-based recommendation scenarios. S 2 -MHCN <ref type="bibr" target="#b26">[27]</ref> and SMIN <ref type="bibr" target="#b54">[55]</ref> integrate CL into social recommendation. HHGR <ref type="bibr" target="#b25">[26]</ref> proposes a double-scale augmentation approach for group recommendation and develops a finer-grained contrastive objective for users and groups. CCDR <ref type="bibr" target="#b55">[56]</ref> and CrossCBR <ref type="bibr" target="#b56">[57]</ref> have explored the use of CL in cross-domain and bundle recommendation. Yao et al. <ref type="bibr" target="#b26">[27]</ref> proposed a feature dropoutbased two-tower architecture for large-scale item recommendation. NCL <ref type="bibr" target="#b17">[18]</ref> designs a prototypical contrastive objective to capture the correlations between a user/item and its context. SEPT <ref type="bibr" target="#b16">[17]</ref> and COTREC <ref type="bibr" target="#b57">[58]</ref> further propose to mine multiple positive samples with semi-supervised learning on the perturbed graph for social/session-based recommendation. The most widely used model is SGL <ref type="bibr" target="#b11">[12]</ref> which replies edge/node dropout to augment the graph data. Although these methods have demonstrated their effectiveness, they pay little attention to why CL can enhance recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we revisit the graph CL in recommendation and investigate how it enhances graph recommendation models. The findings are surprising that the InfoNCE loss is the decisive factor which accounts for most of the performance gains, whilst the elaborate graph augmentations only play a secondary role. Optimizing the InfoNCE loss leads to a more even representation distribution, which helps to promote the long-tail items in the scenario of recommendation. In light of this, we propose a simple yet effective noise-based augmentation approach, which can smoothly adjust the uniformity of the representation distribution through CL. An extremely simple model XSimGCL is also put forward, which brings an ultralight architecture for CL-based recommendation. The extensive experiments on three large and highly sparse datasets demonstrate that XSimGCL is an ideal alternative of its graph augmentationbased counterparts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: The distribution of representations learned from three datasets. The top of each figure plots the learned 2D features and the bottom of each figure plots the Gaussian kernel density estimation of atan2(y, x) for each point (x,y) ? S 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Trends of uniformity with different . Lower values on the y-axis are better. We present the Recall@20 values of XSimGCL with different when it reaches convergence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: The training speed of compared methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: The influence of ? and .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: The influence of the layer selection for contrast.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1 :</head><label>1</label><figDesc>Performance comparison of different SGL variants.</figDesc><table><row><cell>Method</cell><cell cols="2">Yelp2018</cell><cell cols="2">Kindle</cell><cell cols="2">iFashion</cell></row><row><cell></cell><cell>Recall</cell><cell>NDCG</cell><cell>Recall</cell><cell>NDCG</cell><cell>Recall</cell><cell>NDCG</cell></row><row><cell>LightGCN</cell><cell>0.0639</cell><cell>0.0525</cell><cell>0.2053</cell><cell>0.1315</cell><cell>0.0955</cell><cell>0.0461</cell></row><row><cell>SGL-ND</cell><cell>0.0644</cell><cell>0.0528</cell><cell>0.2069</cell><cell>0.1328</cell><cell>0.1032</cell><cell>0.0498</cell></row><row><cell>SGL-ED</cell><cell>0.0675</cell><cell>0.0555</cell><cell>0.2090</cell><cell>0.1352</cell><cell>0.1093</cell><cell>0.0531</cell></row><row><cell>SGL-RW</cell><cell>0.0667</cell><cell>0.0547</cell><cell>0.2105</cell><cell>0.1351</cell><cell>0.1095</cell><cell>0.0531</cell></row><row><cell>SGL-WA</cell><cell>0.0671</cell><cell>0.0550</cell><cell>0.2084</cell><cell>0.1347</cell><cell>0.1065</cell><cell>0.0519</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 2 :</head><label>2</label><figDesc>The comparison of time complexity In these four models, SGL-ED and SimGCL are obviously the two with the highest computation costs. SimGCL needs more time in the graph encoding but SGL-ED requires constant graph augmentations. Since this part is usually performed on CPUs, which brings SGL-ED more expense of time in practice. By comparison, XSimGCL needs neither graph augmentations nor extra encoders. Without considering the computation for the contrastive task, XSimGCL is theoretically as lightweight as LightGCN and only spends one-third of SimGCL's training expense in graph encoding. When the actual number of epochs for training is considered, XSimGCL will show more efficiency beyond what we can observe from this theoretical analysis.</figDesc><table><row><cell></cell><cell>LightGCN</cell><cell>SGL-ED</cell><cell>SimGCL</cell><cell>XSimGCL</cell></row><row><cell>Adjacency Matrix</cell><cell>O(2|A|)</cell><cell>O(2|A|+4?|A|)</cell><cell>O(2|A|)</cell><cell>O(2|A|)</cell></row><row><cell>Graph Encoding</cell><cell>O(2|A|Ld)</cell><cell>O((2+4?)|A|Ld)</cell><cell>O(6|A|Ld)</cell><cell>O(2|A|Ld)</cell></row><row><cell>Prediction</cell><cell>O(2Bd)</cell><cell>O(2Bd)</cell><cell>O(2Bd)</cell><cell>O(2Bd)</cell></row><row><cell>Contrast</cell><cell>-</cell><cell>O(BM d)</cell><cell>O(BM d)</cell><cell>O(BM d)</cell></row><row><cell cols="5">? Since LightGCN, SimGCL and XSimGCL do not need</cell></row><row><cell cols="5">graph augmentations, they only construct the normalized</cell></row><row><cell cols="5">adjacency matrix which has 2|A| non-zero elements. For</cell></row><row><cell cols="5">SGL-ED, two graph augmentations are used and each has</cell></row><row><cell cols="5">2?|A| non-zero elements in their adjacency matrices.</cell></row><row><cell cols="5">? In the phase of graph encoding, a three-encoder architec-</cell></row><row><cell cols="5">ture is adopted in both SGL-ED and SimGCL to learn two</cell></row><row><cell cols="5">different augmentations, so the encoding expense of SGL-</cell></row><row><cell cols="5">ED and SimGCL are almost three times that of LightGCN.</cell></row><row><cell cols="5">In contrast, the encoding expense of XSimGCL is as the</cell></row><row><cell cols="3">same as that of LightGCN.</cell><cell></cell><cell></cell></row><row><cell cols="5">? As for the prediction, all methods are trained with the</cell></row><row><cell cols="5">BPR loss and each batch contains B interactions, so they</cell></row><row><cell cols="4">have exactly the same time cost in this regard.</cell><cell></cell></row><row><cell cols="5">? The computational cost of CL comes from the contrast</cell></row><row><cell cols="5">between the positive/negative samples, which are O(Bd)</cell></row><row><cell cols="5">and O(BM d), respectively, because each node regards the</cell></row></table><note><p>views of itself as the positives and views of other nodes as the negatives. For the sake of brevity, we mark it as O(BM d) since M 1.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3 :</head><label>3</label><figDesc>Dataset Statistics</figDesc><table><row><cell>Dataset</cell><cell>#User</cell><cell>#Item</cell><cell cols="2">#Feedback Density</cell></row><row><cell>Yelp2018</cell><cell>31,668</cell><cell>38,048</cell><cell>1,561,406</cell><cell>0.13%</cell></row><row><cell>Amazon-Kindle</cell><cell cols="2">138,333 98,572</cell><cell>1,909,965</cell><cell>0.014%</cell></row><row><cell>Alibaba-iFashion</cell><cell cols="2">300,000 81,614</cell><cell>1,607,813</cell><cell>0.007%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4 :</head><label>4</label><figDesc>Performance Comparison for different CL methods on three benchmarks.</figDesc><table><row><cell cols="2">Method</cell><cell cols="2">Yelp2018</cell><cell cols="2">Amazon-Kindle</cell><cell cols="2">Alibaba-iFashion</cell></row><row><cell></cell><cell></cell><cell>Recall@20</cell><cell>NDCG@20</cell><cell>Recall@20</cell><cell>NDCG@20</cell><cell>Recall@20</cell><cell>NDCG@20</cell></row><row><cell></cell><cell>LightGCN</cell><cell>0.0631</cell><cell>0.0515</cell><cell>0.1871</cell><cell>0.1186</cell><cell>0.0845</cell><cell>0.0390</cell></row><row><cell></cell><cell>SGL-ND</cell><cell>0.0643 (+1.9%)</cell><cell>0.0529 (+2.7%)</cell><cell>0.1880 (+0.5%)</cell><cell>0.1192 (+0.5%)</cell><cell>0.0896 (+6.0%)</cell><cell>0.0432(+10.8%)</cell></row><row><cell>1-Layer</cell><cell>SGL-ED SGL-RW</cell><cell>0.0637 (+1.0%) 0.0637 (+1.0%)</cell><cell>0.0526 (+2.1%) 0.0526 (+2.1%)</cell><cell>0.1936 (+3.5%) 0.1936 (+3.5%)</cell><cell>0.1231 (+3.8%) 0.1231 (+3.8%)</cell><cell>0.0932 (+10.3%) 0.0932 (+10.3%)</cell><cell>0.0447 (+14.6%) 0.0447 (+14.6%)</cell></row><row><cell></cell><cell>SGL-WA</cell><cell>0.0628 (-0.4%)</cell><cell>0.0525 (+1.9%)</cell><cell>0.1918 (+2.5%)</cell><cell>0.1221 (+2.9%)</cell><cell>0.0913 (+8.0%)</cell><cell>0.0440 (12.8%)</cell></row><row><cell></cell><cell>SimGCL</cell><cell>0.0689 (+9.2%)</cell><cell>0.0572 (+11.1%)</cell><cell cols="2">0.2087 (+11.5%) 0.1361 (+14.8%)</cell><cell>0.1036 (+22.6%)</cell><cell>0.0505 (+29.5%)</cell></row><row><cell></cell><cell>XSimGCL</cell><cell>0.0692 (+9.7%)</cell><cell>0.0582 (+13.0%)</cell><cell>0.2071 (+10.7%)</cell><cell>0.1339 (+12.9%)</cell><cell cols="2">0.1069 (+26.5%) 0.0527 (+35.1%)</cell></row><row><cell></cell><cell>LightGCN</cell><cell>0.0622</cell><cell>0.0504</cell><cell>0.2033</cell><cell>0.1284</cell><cell>0.1053</cell><cell>0.0505</cell></row><row><cell></cell><cell>SGL-ND</cell><cell>0.0658 (+5.8%)</cell><cell>0.0538 (+6.7%)</cell><cell>0.2020 (-0.6%)</cell><cell>0.1307 (+1.8%)</cell><cell>0.0993 (-5.7%)</cell><cell>0.0484 (-4.2%)</cell></row><row><cell></cell><cell>SGL-ED</cell><cell>0.0668 (+7.4%)</cell><cell>0.0549 (+8.9%)</cell><cell>0.2084 (+2.5%)</cell><cell>0.1341 (+4.4%)</cell><cell>0.1062 (+0.8%)</cell><cell>0.0514 (+1.8%)</cell></row><row><cell>2-Layer</cell><cell>SGL-RW</cell><cell>0.0644 (+3.5%)</cell><cell>0.0530 (+5.2%)</cell><cell>0.2088 (+2.7%)</cell><cell>0.1345 (+4.8%)</cell><cell>0.1053 (+0.0%)</cell><cell>0.0512 (+1.4%)</cell></row><row><cell></cell><cell>SGL-WA</cell><cell>0.0653 (+5.0%)</cell><cell>0.0544 (+7.9%)</cell><cell>0.2068 (+1.7%)</cell><cell>0.1330 (+3.6%)</cell><cell>0.1028 (-2.4%)</cell><cell>0.0501 (-0.8%)</cell></row><row><cell></cell><cell>SimGCL</cell><cell>0.0719 (+15.6%)</cell><cell>0.0601 (+19.2%)</cell><cell>0.2071 (+1.9%)</cell><cell>0.1341 (+4.4%)</cell><cell>0.1119 (+6.3%)</cell><cell>0.0548 (+8.5%)</cell></row><row><cell></cell><cell cols="3">XSimGCL 0.0722 (+16.1%) 0.0604 (+19.8%)</cell><cell>0.2114 (+4.0%)</cell><cell>0.1382 (+7.6%)</cell><cell>0.1143 (+8.5%)</cell><cell>0.0559 (+10.7%)</cell></row><row><cell></cell><cell>LightGCN</cell><cell>0.0639</cell><cell>0.0525</cell><cell>0.2057</cell><cell>0.1315</cell><cell>0.0955</cell><cell>0.0461</cell></row><row><cell></cell><cell>SGL-ND</cell><cell>0.0644 (+0.8%)</cell><cell>0.0528 (+0.6%)</cell><cell>0.2069 (+0.6%)</cell><cell>0.1328 (+1.0%)</cell><cell>0.1032 (+8.1%)</cell><cell>0.0498 (+8.0%)</cell></row><row><cell></cell><cell>SGL-ED</cell><cell>0.0675 (+5.6%)</cell><cell>0.0555 (+5.7%)</cell><cell>0.2090 (+1.6%)</cell><cell>0.1352 (+2.8%)</cell><cell>0.1093 (+14.5%)</cell><cell>0.0531 (+15.2%)</cell></row><row><cell>3-Layer</cell><cell>SGL-RW</cell><cell>0.0667 (+4.4%)</cell><cell>0.0547 (+4.5%)</cell><cell>0.2105 (+2.3%)</cell><cell>0.1351 (+2.7%)</cell><cell>0.1095 (+14.7%)</cell><cell>0.0531 (+15.2%)</cell></row><row><cell></cell><cell>SGL-WA</cell><cell>0.0671 (+5.0%)</cell><cell>0.0550 (+4.8%)</cell><cell>0.2084 (+1.3%)</cell><cell>0.1347 (+2.4%)</cell><cell>0.1065 (+11.5%)</cell><cell>0.0519 (+12.6%)</cell></row><row><cell></cell><cell>SimGCL</cell><cell>0.0721 (+12.8%)</cell><cell>0.0601 (+14.5%)</cell><cell>0.2104 (+2.3%)</cell><cell>0.1374 (+4.5%)</cell><cell>0.1151 +(20.5%)</cell><cell>0.0567 (+23.0%)</cell></row><row><cell></cell><cell cols="3">XSimGCL 0.0723 (+13.1%) 0.0604 (+15.0%)</cell><cell>0.2147 (+4.4%)</cell><cell>0.1415 (+7.6%)</cell><cell>0.1196 (+25.2%)</cell><cell>0.0586 (27.1%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 5 :</head><label>5</label><figDesc>The best hyperparameters of compared methods.</figDesc><table><row><cell>Dataset</cell><cell>Yelp2018</cell><cell>Amazon-Kindle</cell><cell>Alibaba-iFashion</cell></row><row><cell>SGL</cell><cell>?=0.1, ?=0.1</cell><cell>?=0.05, ?=0.1</cell><cell>?=0.05, ?=0.2</cell></row><row><cell>SimGCL XSimGCL</cell><cell>?=0.5, =0.1 ?=0.2, =0.2, l  *  =1</cell><cell>?=0.1, =0.1 ?=0.2, =0.1, l  *  =1</cell><cell>?=0.05, =0.1 ?=0.05, =0.05, l</cell></row></table><note><p>* =3</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 6 :</head><label>6</label><figDesc>Performance comparison with other models.</figDesc><table><row><cell>Method</cell><cell cols="2">Yelp2018</cell><cell cols="2">Kindle</cell><cell cols="2">iFashion</cell></row><row><cell></cell><cell>Recall</cell><cell>NDCG</cell><cell>Recall</cell><cell>NDCG</cell><cell>Recall</cell><cell>NDCG</cell></row><row><cell>LightGCN</cell><cell>0.0639</cell><cell>0.0525</cell><cell>0.2057</cell><cell>0.1315</cell><cell>0.1053</cell><cell>0.0505</cell></row><row><cell>NCL</cell><cell>0.0670</cell><cell>0.0562</cell><cell>0.2090</cell><cell>0.1348</cell><cell>0.1088</cell><cell>0.0528</cell></row><row><cell>BUIR</cell><cell>0.0487</cell><cell>0.0404</cell><cell>0.0922</cell><cell>0.0528</cell><cell>0.0830</cell><cell>0.0384</cell></row><row><cell>DNN+SSL</cell><cell>0.0483</cell><cell>0.0382</cell><cell>0.1520</cell><cell>0.0989</cell><cell>0.0818</cell><cell>0.0375</cell></row><row><cell>MixGCF</cell><cell>0.0713</cell><cell>0.0589</cell><cell>0.2098</cell><cell>0.1355</cell><cell>0.1085</cell><cell>0.0520</cell></row><row><cell>SimGCL</cell><cell>0.0721</cell><cell>0.0601</cell><cell>0.2104</cell><cell>0.1374</cell><cell>0.1151</cell><cell>0.0567</cell></row><row><cell>XSimGCL</cell><cell>0.0723</cell><cell>0.0604</cell><cell>0.2147</cell><cell>0.1415</cell><cell>0.1196</cell><cell>0.0586</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 7 :</head><label>7</label><figDesc>Performance comparison of different backbones. XSimGCL with Other Types of Noises We test three other types of noises in this experiment including adversarial perturbation obtained by following FGSM<ref type="bibr" target="#b35">[36]</ref> (denoted by XSimGCL a ), positive uniform noises without the sign of learned embeddings (denoted by XSimGCL p ), and Gaussian noises (denoted by XSimGCL g ). We tried many combinations of ? and for different types of noises and presented the results in Table8. As observed, the vanilla XSimGCL with signed uniform noises outperforms other variants. Although positive uniform noises and Gaussian noises also bring hefty performance gains compared with LightGCN, adding adversarial noises unexpectedly leads to a large drop of performance. This indicates that only a few particular distributions can generate helpful noises. Besides, the result that XSimGCL outperforms XSimGCL p demonstrates the necessity of the sign constraint.</figDesc><table><row><cell>Method</cell><cell cols="2">Yelp2018</cell><cell cols="2">Kindle</cell><cell cols="2">iFashion</cell></row><row><cell></cell><cell>Recall</cell><cell>NDCG</cell><cell>Recall</cell><cell>NDCG</cell><cell>Recall</cell><cell>NDCG</cell></row><row><cell>MF</cell><cell>0.0543</cell><cell>0.0445</cell><cell>0.1751</cell><cell>0.1068</cell><cell>0.996</cell><cell>0.468</cell></row><row><cell>MF + NBC</cell><cell>0.0517</cell><cell>0.0433</cell><cell>0.1878</cell><cell>0.1175</cell><cell>0.975</cell><cell>0.0453</cell></row><row><cell>GCN</cell><cell>0.0556</cell><cell>0.0452</cell><cell>0.1833</cell><cell>0.1137</cell><cell>0.952</cell><cell>0.0458</cell></row><row><cell>GCN + NBC</cell><cell>0.0632</cell><cell>0.0530</cell><cell>0.1989</cell><cell>0.1290</cell><cell>0.1017</cell><cell>0.0486</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 8 :</head><label>8</label><figDesc>Performance comparison of different XSimGCL variants.</figDesc><table><row><cell>Method</cell><cell cols="2">Yelp2018</cell><cell cols="2">Kindle</cell><cell cols="2">iFashion</cell></row><row><cell></cell><cell>Recall</cell><cell>NDCG</cell><cell>Recall</cell><cell>NDCG</cell><cell>Recall</cell><cell>NDCG</cell></row><row><cell>LightGCN</cell><cell>0.0639</cell><cell>0.0525</cell><cell>0.2057</cell><cell>0.1315</cell><cell>0.1053</cell><cell>0.0505</cell></row><row><cell>XSimGCL a</cell><cell>0.0558</cell><cell>0.0464</cell><cell>0.1267</cell><cell>0.0833</cell><cell>0.0158</cell><cell>0.0065</cell></row><row><cell>XSimGCL p</cell><cell>0.0714</cell><cell>0.0596</cell><cell>0.2121</cell><cell>0.1398</cell><cell>0.1183</cell><cell>0.0577</cell></row><row><cell>XSimGCL g</cell><cell>0.0722</cell><cell>0.0602</cell><cell>0.2140</cell><cell>0.1410</cell><cell>0.1190</cell><cell>0.0583</cell></row><row><cell>XSimGCL</cell><cell>0.0723</cell><cell>0.0604</cell><cell>0.2147</cell><cell>0.1415</cell><cell>0.1196</cell><cell>0.0586</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Hyperparameter Investigation</head><p>XSimGCL has three important hyperparameters: ? -the coefficient of the contrastive task, -the magnitude of added noises, and l * -the layer to be contrasted. In this part, we investigate the model's sensitivity to these hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Influence of ? and</head><p>We try different combinations of ? and with the set [0.01, 0.05, 0.1, 0.2, 0.5, 1] for ? and [0, 0.01, 0.05, 0.1, 0.2, 0.5] for . We fix l * =1 and conduct experiments with a 2-layer setting, but we found that the best values of these two hyperparameters are also applicable to other settings. As shown in Fig. <ref type="figure">8</ref>, on all the datasets XSimGCL reaches its best performance when is in [0.05, 0.1, 0.2]. Without the added noises ( =0), we can see an obvious performance drop. When is too large ( =0.05) or too small ( =0.01), the performance declines as well. The similar trend is also observed by changing the value of ?. The performance is at the peak when ?=0.2 on Yelp2018, ? = 0.2 on Amazon-Kindle, and ? = 0.05 on Alibaba-iFashion. According to our experience, XSimGCL (SimGCL) is more sensitive to ?, and = 0.1 is usually a good and safe choice on most datasets. Besides, we also find that a larger leads to faster convergence. But when it is overlarge (e.g., greater than 1), it will act like a large learning rate, causing the progressive zigzag optimization which will overshoot the minimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Layer Selection for Contrast</head><p>In XSimGCL, two layers are chosen to be contrasted. We report the results of different choices in Fig. <ref type="figure">9</ref> where a 3-layer setting is used. Since these matrix-like heat maps are symmetric, we only display the lower triangular parts. The figures in the diagonal cells represent the results of contrasting the same layer. As can be seen, although the best choice varies from dataset to dataset, it always appears as the contrast between the final layer and one of the previous layers. We analyzed the similarities between representations of different layers and tried to find if l * is related to the similarity but no evidence was found. Fortunately, XSimGCL usually achieves the best performance with a 3-layer setting, which means three attempts are enough. The amount of manual work for tuning l * is therefore greatly reduced. A compromised way without tuning l * is to randomly choose a layer in every mini-batch and contrast its embeddings with the final embeddings. We report the results of this random selection in the upper right of the heatmap. They are acceptable but much lower than the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Applicability Investigation</head><p>The noise-based CL has been proved effective when combining with LightGCN. We further wonder whether this method is applicable to other common backbones such as MF and GCN. Besides, whether uniform noises are the best choice remains unknown. In this part, we investigate the applicability of the noise-based augmentation and different types of noises. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A survey on contrastive self-supervised learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Z</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Makedon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technologies</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Self-supervised learning: Generative or contrastive</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08218</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Self-supervised learning for recommender systems: A survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Simcse: Simple contrastive learning of sentence embeddings</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6894" to="6910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to selfsupervised learning</title>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scalability and sparsity issues in recommender datasets: a survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="43" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Retaining data from streams of social platforms with minimal regret</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Weidlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Thang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Q V</forename><surname>Hung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="2850" to="2856" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sequence-aware factorization machines for temporal predictive analytics</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 36th International Conference on Data Engineering (ICDE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1405" to="1416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Selfsupervised graph learning for recommendation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="726" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Selfsupervised multi-channel hypergraph convolutional network for social recommendation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Q V</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="413" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Selfsupervised hypergraph convolutional networks for session-based recommendation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4503" to="4511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1893" to="1902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Contrastive learning for debiased candidate generation in large-scale recommender systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<biblScope unit="page" from="3985" to="3995" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Socially-aware self-supervised tri-training for recommendation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Q V</forename><surname>Hung</surname></persName>
		</author>
		<editor>KDD, F. Zhu, B. C. Ooi, and C. Miao</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="2084" to="2092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Improving graph collaborative filtering with neighborhood-enriched contrastive learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="2320" to="2329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">519</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Selfcf: A simple framework for self-supervised collaborative filtering</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Miao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03019</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Bootstrapping user and item representations for one-class collaborative filtering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<editor>SIGIR, F. Diaz, C. Shah, T. Suel, P. Castells, R. Jones, and T. Sakai</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1513" to="1522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.03240</idno>
		<title level="m">Bias and debias in recommender system: A survey and future directions</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Are graph augmentations necessary? simple graph contrastive learning for recommendation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V H</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1294" to="1303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Contrastive learning for sequential recommendation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1259" to="1273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Double-scale self-supervised hypergraph learning for group recommendation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2557" to="2567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Self-supervised learning for large-scale item recommendations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tjoa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4321" to="4330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Lightgcn: Simplifying and powering graph convolution network for recommendation</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bpr: Bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering</title>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9929" to="9939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Kernel density estimation via diffusion</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">I</forename><surname>Botev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Grotowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kroese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2916" to="2957" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Challenging the long tail recommendation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="896" to="907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Measuring and relieving the over-smoothing problem for graph neural networks from the topological view</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="3438" to="3445" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<editor>ICLR, Y. Bengio and Y. LeCun</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning spreadout local feature descriptors</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4595" to="4603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">What makes for good views for contrastive learning?</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6827" to="6839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Mixgcf: An improved training method for graph neural network-based recommender systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="665" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Disentangled graph collaborative filtering</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1001" to="1010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Graph neural networks for recommender system</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1623" to="1625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Graph neural networks in recommender systems: a survey</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CSUR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Enhance social recommendation with adversarial graph convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Session-based recommendation with graph neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="346" to="353" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Neural graph collaborative filtering</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Revisiting graph based collaborative filtering: A linear residual graph convolutional network approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="27" to="34" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Graph convolutional network for recommendation with low-pass collaborative filters</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">945</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Adaptive implicit friends identification over heterogeneous network for social recommendation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="357" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Disentangled self-supervision in sequential recommenders</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<biblScope unit="page" from="483" to="491" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Contrastive learning for cold-start recommendation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia, 2021</title>
		<imprint>
			<biblScope unit="page" from="5382" to="5390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Contrastive learning for representation degeneration problem in sequential recommendation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>WSDM</publisher>
			<biblScope unit="page" from="813" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Social recommendation with self-supervised metagraph informax network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1160" to="1169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Contrastive cross-domain recommendation in matching</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<biblScope unit="page" from="4226" to="4236" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Crosscbr: Crossview contrastive learning for bundle recommendation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<biblScope unit="page" from="1233" to="1241" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Self-supervised graph co-training for session-based recommendation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2180" to="2190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Currently, he is a final-year Ph.D. candidate at the School of Information Technology and Electrical Engineering, the University of Queensland. His research interests include recommender systems, social media analytics, and self-supervised learning</title>
	</analytic>
	<monogr>
		<title level="m">Junliang Yu received his B.S. and M.S degrees in Software Engineering from Chongqing University</title>
		<meeting><address><addrLine>China</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
