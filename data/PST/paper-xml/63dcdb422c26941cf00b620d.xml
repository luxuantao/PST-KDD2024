<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SPECFORMER: SPECTRAL GRAPH NEURAL NET-WORKS MEET TRANSFORMERS</title>
				<funder ref="#_2jEb8NT #_3FyABC4">
					<orgName type="full">NSERC Discovery Grants</orgName>
				</funder>
				<funder ref="#_bgZecVC">
					<orgName type="full">BUPT Excellent Ph.D. Students Foundation</orgName>
				</funder>
				<funder ref="#_Jv62Jmb">
					<orgName type="full">Advanced Research Computing</orgName>
				</funder>
				<funder ref="#_NXpwzmU">
					<orgName type="full">NSERC Collaborative Research and Development Grant</orgName>
				</funder>
				<funder>
					<orgName type="full">Compute Canada</orgName>
				</funder>
				<funder ref="#_9amNcqx #_MUfmkyD #_UYTeG4X #_SWKG7Qn #_4zSHcZs #_zjdh5wj">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-03-02">2 Mar 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Deyu</forename><surname>Bo</surname></persName>
							<email>bodeyu@bupt.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
							<email>shichuan@bupt.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Lele</forename><surname>Wang</surname></persName>
							<email>lelewang@ece.ubc.ca</email>
						</author>
						<author>
							<persName><forename type="first">Renjie</forename><surname>Liao</surname></persName>
							<email>rjliao@ece.ubc.ca</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts</orgName>
								<address>
									<addrLine>Telecommunications 1</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of British Columbia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SPECFORMER: SPECTRAL GRAPH NEURAL NET-WORKS MEET TRANSFORMERS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-03-02">2 Mar 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2303.01028v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Spectral graph neural networks (GNNs) learn graph representations via spectraldomain graph convolutions. However, most existing spectral graph filters are scalar-to-scalar functions, i.e., mapping a single eigenvalue to a single filtered value, thus ignoring the global pattern of the spectrum. Furthermore, these filters are often constructed based on some fixed-order polynomials, which have limited expressiveness and flexibility. To tackle these issues, we introduce Specformer, which effectively encodes the set of all eigenvalues and performs self-attention in the spectral domain, leading to a learnable set-to-set spectral filter. We also design a decoder with learnable bases to enable non-local graph convolution. Importantly, Specformer is equivariant to permutation. By stacking multiple Specformer layers, one can build a powerful spectral GNN. On synthetic datasets, we show that our Specformer can better recover ground-truth spectral filters than other spectral GNNs. Extensive experiments of both node-level and graph-level tasks on real-world graph datasets show that our Specformer outperforms state-ofthe-art GNNs and learns meaningful spectrum patterns. Code and data are available at https://github.com/bdy9527/Specformer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph neural networks (GNNs), firstly proposed in <ref type="bibr" target="#b33">(Scarselli et al., 2008)</ref>, become increasingly popular in the field of machine learning due to their empirical successes. Depending on how the graph signals (or features) are leveraged, GNNs can be roughly categorized into two classes, namely spatial GNNs and spectral GNNs. Spatial GNNs often adopt a message passing framework <ref type="bibr" target="#b14">(Gilmer et al., 2017;</ref><ref type="bibr" target="#b1">Battaglia et al., 2018)</ref>, which learns useful graph representations via propagating local information on graphs. Spectral GNNs <ref type="bibr" target="#b3">(Bruna et al., 2013;</ref><ref type="bibr" target="#b7">Defferrard et al., 2016)</ref> instead perform graph convolutions via spectral filters (i.e., filters applied to the spectrum of the graph Laplacian), which can learn to capture non-local dependencies in graph signals. Although spatial GNNs have achieved impressive performances in many domains, spectral GNNs are somewhat under-explored.</p><p>There are a few reasons why spectral GNNs have not been able to catch up. First, most existing spectral filters are essentially scalar-to-scalar functions. In particular, they take a single eigenvalue as input and apply the same filter to all eigenvalues. This filtering mechanism could ignore the rich information embedded in the spectrum, i.e., the set of eigenvalues. For example, we know from the spectral graph theory that the algebraic multiplicity of the eigenvalue 0 tells us the number of connected components in the graph. However, such information can not be captured by scalarto-scalar filters. Second, the spectral filters are often approximated via fixed-order (or truncated) orthonormal bases, e.g., Chebyshev polynomials <ref type="bibr" target="#b7">(Defferrard et al., 2016;</ref><ref type="bibr" target="#b17">He et al., 2022)</ref> and graph wavelets <ref type="bibr" target="#b15">(Hammond et al., 2011;</ref><ref type="bibr" target="#b44">Xu et al., 2019)</ref>, in order to avoid the costly spectral decomposition of the graph Laplacian. Although the orthonormality is a nice property, this truncated approximation is less expressive and may severely limit the graph representation learning.</p><p>Therefore, in order to improve spectral GNNs, it is natural to ask: how can we build expressive spectral filters that can effectively leverage the spectrum of graph Laplacian? To answer this question, we first note that eigenvalues of graph Laplacian represent the frequency, i.e., total variation of the corresponding eigenvectors. The magnitudes of frequencies thus convey rich information. Moreover, the relative difference between two eigenvalues also reflects important frequency information, e.g., the spectral gap. To capture both magnitudes of frequency and relative frequency, we propose a Transformer <ref type="bibr">(Vaswani et al., 2017b)</ref> based set-to-set spectral filter, termed Specformer. Our Specformer first encodes the range of eigenvalues via positional embedding and then exploits the self-attention mechanism to learn relative information from the set of eigenvalues. Relying on the learned representations of eigenvalues, we also design a decoder with a bank of learnable bases. Finally, by combining these bases, Specformer can construct a permutation-equivariant and non-local graph convolution. In summary, our contributions are as follows:</p><p>? We propose a novel Transformer-based set-to-set spectral filter along with learnable bases, called Specformer, which effectively captures both magnitudes and relative differences of all eigenvalues of the graph Laplacian.</p><p>? We show that Specformer is permutation equivariant and can perform non-local graph convolutions, which is non-trivial to achieve in many spatial GNNs.</p><p>? Experiments on synthetic datasets show that Specformer learns to better recover the given spectral filters than other spectral GNNs.</p><p>? Extensive experiments on various node-level and graph-level benchmarks demonstrate that Specformer outperforms state-of-the-art GNNs and learns meaningful spectrum patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Existing GNNs can be roughly divided into two categories: spatial and spectral GNNs.</p><p>Spatial GNNs. Spatial GNNs like GAT <ref type="bibr" target="#b39">(Velickovic et al., 2018)</ref> and MPNN <ref type="bibr" target="#b14">(Gilmer et al., 2017)</ref> leverage message passing to aggregate local information from neighborhoods. By stacking multiple layers, spatial GNNs can possibly learn long-range dependencies but suffer from over-smoothing <ref type="bibr" target="#b28">(Oono &amp; Suzuki, 2020)</ref> and over-squashing <ref type="bibr" target="#b36">(Topping et al., 2022)</ref>. Therefore, how to balance local and global information is an important research topic for spatial GNNs. We refer readers to <ref type="bibr" target="#b43">(Wu et al., 2021;</ref><ref type="bibr" target="#b48">Zhou et al., 2020;</ref><ref type="bibr">Liao, 2021)</ref> for a more detailed discussion about spatial GNNs.</p><p>Spectral GNNs. Spectral GNNs <ref type="bibr" target="#b29">(Ortega et al., 2018;</ref><ref type="bibr" target="#b9">Dong et al., 2020;</ref><ref type="bibr" target="#b42">Wu et al., 2019;</ref><ref type="bibr" target="#b5">Zhu et al., 2021;</ref><ref type="bibr" target="#b2">Bo et al., 2021;</ref><ref type="bibr" target="#b5">Chang et al., 2021;</ref><ref type="bibr" target="#b45">Yang et al., 2022)</ref> leverage the spectrum of graph Laplacian to perform convolutions in the spectral domain. A popular subclass of spectral GNNs leverages different kinds of orthogonal polynomials to approximate arbitrary filters, including Monomial <ref type="bibr" target="#b6">(Chien et al., 2021)</ref>, Chebyshev <ref type="bibr" target="#b7">(Defferrard et al., 2016;</ref><ref type="bibr" target="#b21">Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b17">He et al., 2022)</ref>, Bernstein <ref type="bibr" target="#b16">(He et al., 2021)</ref>, and Jacobi <ref type="bibr" target="#b41">(Wang &amp; Zhang, 2022)</ref>. Relying on the diagonalization of symmetric matrices, they avoid direct spectral decomposition and guarantee localization. However, all such polynomial filters are scalar-to-scalar functions, and the bases are pre-defined, which limits their expressiveness. Another subclass requires either full or partial spectral decomposition, such as SpectralCNN <ref type="bibr" target="#b13">(Estrach et al., 2014)</ref> and LanczosNet <ref type="bibr" target="#b24">(Liao et al., 2019)</ref>. They parameterize the spectral filters by neural networks, thus being more expressive than truncated polynomials. However, such spectral filters are still limited as they do not capture the dependencies among multiple eigenvalues.</p><p>Graph Transformer. Transformers and GNNs are closely relevant since the attention weights of Transformer can be seen as a weighted adjacency matrix of a fully connected graph. Graph Transformers Dwivedi &amp; Bresson (2020) combine both and have gained popularity recently. Graphormer <ref type="bibr" target="#b46">(Ying et al., 2022)</ref>, SAN <ref type="bibr" target="#b22">(Kreuzer et al., 2021)</ref>, and GPS <ref type="bibr" target="#b31">(Ramp?sek et al., 2022)</ref> design powerful positional and structural embeddings to further improve their expressive power. Graph Transformers still belong to spatial GNNs, although the high-cost self-attention is non-local. The limitation of spatial attention compared to spectral attention has been discussed in <ref type="bibr" target="#b0">(Bastos et al., 2022)</ref>.</p><p>Preliminary. Assume that we have a graph G = (V, E), where V denotes the node set with |V| = n and E is the edge set. The corresponding adjacency matrix A ? {0, 1} n?n , where A ij = 1 if there is an edge between nodes i and j, and A ij = 0 otherwise. The normalized graph Laplacian matrix is defined as</p><formula xml:id="formula_0">L = I n -D -1/2 AD -1/2</formula><p>, where I n is the n ? n identity matrix and D is the diagonal degree matrix with diagonal entries D ii = j A ij for all i ? V and off-diagonal entries D ij = 0 for i = j. We assume G is undirected. Hence, L is a real symmetric matrix, whose spectral decomposition can be written as L = U ?U , where the columns of U are the eigenvectors and</p><formula xml:id="formula_1">? = diag([? 1 , ? 2 , . . . , ? n ]) are the corresponding eigenvalues ranged in [0, 2].</formula><p>Graph Signal Processing (GSP). Spectral GNNs rely on several important concepts from GSP, namely, spectral filtering, graph Fourier transform and its inverse. The graph Fourier transform is written as x = U x, where x ? R n?1 is a graph signal and x ? R n?1 represents the Fourier coefficients. Then a spectral filter G ? is used to scale x. Finally, the inverse graph Fourier transform is applied to yield the filtered signal in spatial domain x = U G ? x. The key task in GSP is to design a powerful spectral filter G ? so that we can exploit the useful frequency information.</p><p>Transformer. Transformer is a powerful deep learning model, which is widely used in natural language processing <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>, vision <ref type="bibr" target="#b10">(Dosovitskiy et al., 2021)</ref>, and graphs <ref type="bibr" target="#b46">(Ying et al., 2022;</ref><ref type="bibr" target="#b31">Ramp?sek et al., 2022)</ref>. Each Transformer layer consists of two components: a multihead self-attention (MHA) module and a token-wise feed-forward network (FFN). Given the input representations H = [h 1 , . . . , h n ] ? R n?d , where d is the hidden dimension, MHA first projects H into query, key and value through three matrices (W Q , W K and W V ) to calculate attentions. And FFN is then used to add transformation. The model can be written as follows where we denote the query dimension as d q . For simplicity, we omit the bias and the description of multi-head attention.</p><formula xml:id="formula_2">Attention(Q, K, V ) = Softmax( QK d q )V , Q = HW Q , K = HW K , V = HW V .</formula><p>(1) In this section, we introduce our Specformer model. Fig. <ref type="figure">1</ref> illustrates the model architecture. We first explain how we encode the eigenvalues and use Transformer to capture their dependencies to yield useful representations. Then we turn to the decoder, which learns new eigenvalues from the representations and reconstructs the graph Laplacian matrix for graph convolution. Finally, we discuss the relationship between our Specformer and other methods, including MPNNs, polynomial GNNs, and graph Transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SPECFORMER</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-head Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EIGENVALUE ENCODING</head><p>We design a powerful set-to-set spectral filter using Transformer to leverage both magnitudes and relative differences of all eigenvalues. However, the expressiveness of self-attention will be restricted heavily if we directly use the scalar eigenvalues to calculate the attention maps. Therefore, it is important to find a suitable function, ?(?) : R 1 ? R d , to map each eigenvalue from a scalar to a meaningful vector. We use an eigenvalue encoding function as follows.</p><p>?(?, 2i) = sin( ?/10000 2i/d ),</p><formula xml:id="formula_3">?(?, 2i + 1) = cos( ?/10000 2i/d ), (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>where i is the dimension of the representations and is a hyperparameter. The benefits of ?(?) are three-fold: (1) It can capture the relative frequency shifts of eigenvalues and provides high-dimension vector representations. ( <ref type="formula" target="#formula_3">2</ref>) It has the wavelengths from 2? to 10000 ? 2?, which forms a multi-scale representation for eigenvalues. (3) It can control the influence of ? by adjusting the value of .</p><p>The choice of is crucial because we find that only the first few dimensions of ?(?) can distinguish different eigenvalues if we simply set = 1. The reason is that eigenvalues lie in the range [0, 2], and the value of ?/10000 2i/d will change slightly when i becomes larger. Therefore, it is important to assign a large value of to enlarge the influence of ?. Experiments can be seen in Appendix C.1.</p><p>Notably, although the eigenvalue encoding (EE) is similar to the positional encoding (PE) of Transformer, they act quite differently. PE describes the information of discrete positions in the spatial domain. While EE represents the information of continuous eigenvalues in the spectral domain.</p><p>Applying PE to the spatial positions (i.e., indices) of eigenvalues will destroy the permutation equivariance property, thereby impairing the learning ability.</p><p>The initial representations of eigenvalues are the concatenation of eigenvalues and their encodings, d+1) . Then a standard Transformer block is used to learn the dependency between eigenvalues. We first apply layer normalization (LN) on the representations before feeding them into other sub-layers, i.e., MHA and FFN. This pre-norm trick has been used widely to improve the optimization of Transformer <ref type="bibr" target="#b46">(Ying et al., 2022)</ref>:</p><formula xml:id="formula_5">Z = [? 1 ?(? 1 ), ? ? ? , ? n ?(? n )] ? R n?(</formula><formula xml:id="formula_6">Z = MHA(LN(Z)) + Z, ? = FFN(LN( Z)) + Z.<label>(3)</label></formula><p>After stacking multiple Transformer blocks, we obtain the expressive representations of the spectrum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">EIGENVALUE DECODING</head><p>Based on the representations returned by the encoder, the decoder can learn new eigenvalues for spectral filtering. Recent studies <ref type="bibr" target="#b45">(Yang et al., 2022;</ref><ref type="bibr" target="#b41">Wang &amp; Zhang, 2022)</ref> show that assigning each feature dimension a separate spectral filter improves the performance of GNNs. Motivated by this discovery, our decoder first decodes several bases. An FFN is then used to combine these bases to construct the final graph convolution.</p><p>Spectral filters. In general, the bases should learn to cover different information of the graph signal space as much as possible. For this purpose, we utilize the multi-head attention mechanism because each head has its own self-attention module. Specifically, the representations learned by each head will be fed into the decoder to perform spectral filtering to get the new eigenvalues.</p><formula xml:id="formula_7">Z m = Attention(QW Q m , KW K m , V W V m ), ? m = ?(Z m W ? ),<label>(4)</label></formula><p>where Z m denotes the representations learned by the m-th heads, and ? is the activation, e.g., ReLU or Tanh, which is optional. ? m ? R n?1 is the m-th eigenvalues after the spectral filtering.</p><p>Learnable bases. After get M filtered eigenvalues, we use a FFN: R M +1 ? R d to construct the learnable bases. We first reconstruct individual new bases, concatenate them along the channel dimension, and feed them to a FFN as below,</p><formula xml:id="formula_8">S m = U diag(? m )U , ? = FFN([I n ||S 1 || ? ? ? ||S M ]),<label>(5)</label></formula><p>where S m ? R n?n is the m-th new basis and ? ? R n?n?d is the combined version. Note that our bases here serve similar purpose as those polynomial bases in the literature. But the way they are combined is learned rather than following certain recursions as in Chebyshev polynomials.</p><p>We have three optional ways to leverage this design of new bases. (1) Shared filters and shared FFN. This model has the least parameters, where the basis ? is shared across all graph convolutional layers.</p><p>(2) Shared filters and layer-specific FFN, which compromises between parameters and performance, e.g.,</p><formula xml:id="formula_9">?(l) = FFN (l) ([I n ||S 1 || ? ? ? ||S M ])</formula><p>where the superscript l denotes the index of layer.</p><p>(3) Layer-specific filters and layer-specific FFN. This model has the most parameters and each layer has its own encoder and decoder, e.g.,</p><formula xml:id="formula_10">?(l) = FFN (l) ([I n ||S (l) 1 || ? ? ? ||S (l) M ]</formula><p>). We refer these three models as Specformer-Small, Specformer-Medium, and Specformer-Large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">GRAPH CONVOLUTION</head><p>Finally, we assign each feature dimension a separate graph Laplacian matrix based on the learned basis ?, which can be written as follows:</p><p>X(l-1)</p><formula xml:id="formula_11">:,i = ?:,:,i X (l-1) :,i , X (l) = ? X(l-1) W (l-1) x + X (l-1) ,<label>(6)</label></formula><p>where X (l) is the node representations in the l-th layer, X(l-1)</p><formula xml:id="formula_12">:,i is the i-th channel dimension, W (l-1) x</formula><p>is the transformation, and ? is the activation. The residual connection is an optional choice. By stacking multiple graph convolutional layers, Specformer can effectively learn node representations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">KEY PROPERTIES COMPARED</head><formula xml:id="formula_13">G ? U = ? 1 u 1 u 1 + ? ? ? + ? n u n u n .</formula><p>Because u i is a eigenvector, u i u i constructs a fully-connected graph. Therefore, our Specformer can break the localization of MPNNs and leverage global information. Specformer v.s. Graph Transformers. Graphormer <ref type="bibr" target="#b46">(Ying et al., 2022)</ref> has shown that graph Transformer can perform well on graph-level tasks. However, existing graph Transformers do not show competitiveness in the node-level tasks, e.g., node classification. Recent studies <ref type="bibr" target="#b0">(Bastos et al., 2022;</ref><ref type="bibr">Wang et al., 2022;</ref><ref type="bibr" target="#b34">Shi et al., 2022)</ref> provide some evidence for this phenomenon. They show that Transformer is essentially a low-pass filter. Therefore, graph Transformers cannot handle the complex node label distribution, e.g., homophilic and heterophilic. On the contrary, as we will see in the experiment section, Specformer can learn arbitrary bases for graph convolution and perform well on both node-level and graph-level tasks.</p><p>Besides the above advantages, we show that our Specformer has the following theoretical properties. Scalability. When applying Specformer to large graphs, one can use the Sparse Generalized Eigenvalue (SGE) algorithms <ref type="bibr" target="#b4">(Cai et al., 2021)</ref> to calculate q eigenvalues and eigenvectors, in which case the forward complexity will reduce to (q 2 (d + M ) + nd(L + d)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we conduct experiments on a synthetic dataset and a wide range of real-world graph datasets to verify the effectiveness of our Specformer.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">LEARNING SPECTRAL FILTERS ON SYNTHETIC DATA</head><p>Dataset description. We take 50 images with the resolution of 100?100 from the Image Processing Toolbox<ref type="foot" target="#foot_0">1</ref> . Each image is processed as a 2D regular 4-neighborhood grid graph, and the values of pixels are the node features. Therefore, these images share the same adjacency matrix A ? R 10000?10000 and the m-th image has its graph signal x m ? R 10000?1 . Five predefined graph filters are used to generate ground truth graph signals. For example, if we use the low-pass filter with G ? = exp(-10? 2 ), the filtered graph signal is calculated by xm = U diag[exp(-10? 2 1 ), . . . , exp(-10? 2 n )]U x m . Setup. We choose six Spectral GNNs as baselines: GCN <ref type="bibr" target="#b21">(Kipf &amp; Welling, 2017)</ref>, GAT <ref type="bibr" target="#b39">(Velickovic et al., 2018)</ref>, ChebyNet <ref type="bibr" target="#b7">(Defferrard et al., 2016)</ref>, GPR-GNN <ref type="bibr" target="#b6">(Chien et al., 2021)</ref>, BernNet <ref type="bibr" target="#b16">(He et al., 2021)</ref>, and JacobiConv <ref type="bibr" target="#b41">(Wang &amp; Zhang, 2022)</ref>. Each method takes A and x m as inputs, and tries to minimize the sum of squared error between the outputs xm and the pre-filtered graph signal xm . We tune the number of hidden units to ensure that each method has nearly 2K trainable parameters. The polynomial order is set to 10 for ChebyNet, GPR-GNN, and BernNet. For our model, we use Specformer-Small with 16 hidden units and 1 head. In training, the maximum number of epochs is set to 2000, and the model will be stopped early if the loss does not descend 200 epochs. All regularization tricks are removed. The learning rate is set to 0.01 for all models. We use two metrics to evaluate each method: sum of squared error and R 2 score. Results. The quantitative experiment results are shown in Table <ref type="table" target="#tab_2">1</ref>, from which we can see that Specformer achieves the best performance on all synthetic graphs. Especially, it has more improvements on challenging graphs, such as Band-rejection and Comb. This validates the effectiveness of Specformer in learning complex graph filters. In addition, we can see that GCN and GAT only perform better on the homophilic graph, which reflects that only using low-frequency information is not enough. Polynomial-based GNNs, i.e., ChebyNet, GPR-GNN, BernNet, and JacobiConv, have more stable performances. But their expressiveness is still weaker than Specformer. We visualize the graph filters learned by GPR-GNN, BernNet, and Specformer in Figure <ref type="figure" target="#fig_1">2</ref>, which further validates our claims. The horizontal axis presents the original eigenvalues, and the vertical axis indicates the corresponding new eigenvalues. For clarity, we uniformly downsample the eigenvalues at a ratio of 1:200 and only visualize three graphs because the situations of Low-pass and High-pass are similar. The same goes for Band-pass and Band-rejection. It can be seen that all methods can fit the easy filters well, i.e., High-pass. However, the polynomial-based GNNs cannot learn the narrow bands in Band-rejection and Comb, e.g., ? ? [0.75, 1.25], which harms their performance. On the contrary, Specformer fits the ground truth precisely, reflecting the superior learning ability over polynomials. The spatial results, i.e., filtered images, can be seen in Appendix C.3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">NODE CLASSIFICATION</head><p>Datasets. For the node classification task, we perform experiments on four homophilic datasets, i.e., Cora, Citeser, Amazon-Photo and ogbn-arXiv, and four heterophilic datasets, i.e., Chameleon, Squirrel, Actor and Penn94. Penn94 <ref type="bibr" target="#b25">(Lim et al., 2021)</ref> and arXiv <ref type="bibr" target="#b18">(Hu et al., 2020)</ref> are two large scale datasets. Other datasets, provided by <ref type="bibr">(Rozemberczki et al., 2021;</ref><ref type="bibr" target="#b30">Pei et al., 2020)</ref>, are commonly used to evaluate the performance of GNNs on heterophilic and homophilic graphs.</p><p>Baselines and settings. We benchmark our model against a series of competitive baselines, including spatial GNNs, spectral GNNs, and graph Transformers. For all datasets, we use the full-supervised split, i.e., 60% for training, 20% for validation, and 20% for testing, as suggested in <ref type="bibr" target="#b16">(He et al., 2021)</ref>. All methods run 10 times and report the mean accuracy with a 95% confidence interval. For polynomial GNNs, we set the order of polynomials K = 10. For other methods, we use a 2-layer module.</p><p>To ensure all models have similar numbers of parameters, in six small datasets, we set the hidden size d = 64 for spatial and spectral GNNs and d = 32 for graph Transformers and Specformer. The total numbers of parameters on Photo dataset are shown in Table <ref type="table" target="#tab_3">2</ref>. On two large datasets, we use truncated spectral decomposition to improve the scalability. Based on the filters learned on the small datasets, we find that band-rejection filters are important for heterophilic datasets and low-pass filters are suitable for homophilic datasets. See Figures <ref type="figure">4(b</ref>) and 4(d). Therefore, we use eigenvectors with the smallest 3000 (low-frequency) and largest 3000 eigenvalues (high-frequency) for Penn94, and eigenvectors with the smallest 5000 eigenvalues (low-frequency) for arXiv. We use one layer for Specformer and set d = 64 in Penn94 and d = 512 in arXiv for all methods, as suggested by <ref type="bibr" target="#b25">(Lim et al., 2021;</ref><ref type="bibr" target="#b17">He et al., 2022)</ref>. More details, e.g., optimizers, can be found in Appendix A.</p><p>Results. In Table <ref type="table" target="#tab_3">2</ref>, we can find that Specformer outperforms state-of-the-art baselines on 7 out of 8 datasets and achieves 12% relative improvement on the Squirrel dataset, which validates the superior learning ability of Specformer. In addition, the improvement is more pronounced on heterophilic datasets than on homophilic datasets. This is probably caused by the easier fitting of the low-pass filters in homophilic datasets. The same phenomenon is also observed in the synthetic graphs. An interesting observation is that the improvement on larger graphs, e.g., Actor and Photo, is less than that on smaller graphs. One possible reason is that the role of the self-attention mechanism is weakened, i.e., the attention values become uniform due to a large number of tokens. We notice that Specformer has a slightly higher variance than the baselines. This is because we set a large dropout rate to prevent overfitting. On the two large graph datasets, we can see that graph Transformers are memory-consuming due to the self-attention. On the contrary, Specformer reduces the time and space costs by using the truncated decomposition and shows better scalability than graph Transformers. The time and space overheads are listed in Appendix C.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">GRAPH CLASSIFICATION AND REGRESSION</head><p>Datasets. We conduct experiments on three graph-level datasets with different scales. ZINC <ref type="bibr">(Dwivedi et al., 2020)</ref> is a small subset of a large molecular dataset, which contains 12K graphs in total. MolHIV and MolPCBA are taken from the Open Graph Benchmark (OGB) datasets <ref type="bibr" target="#b18">(Hu et al., 2020)</ref>. MolHIV is a medium dataset that has nearly 41K graphs. MolPCBA is the largest, containing 437K graphs. For all datasets, nodes represent the atoms, and edges indicate the bonds.</p><p>Baselines and Settings. We choose popular MPNNs (GCN, GIN, and GatedGNN), graph Transformers with positional or structural embedding (SAN, Graphormer, and GPS), and other state-ofthe-art GNNs (CIN, GIN-AK+, etc.) as the baselines of graph-level tasks. For the ZINC dataset, we tune the hyperparameters of Specformer to ensure that the total parameters are around 500K.</p><p>Results. We apply Specformer-Small, Medium, and Large for ZINC, MolHIV, and MolPCBA, respectively. The results are shown in Table <ref type="table" target="#tab_4">3</ref>. It can be seen that Specformer outperforms the state-of-the-art models in ZINC and MolPCBA datasets, without using any hand-crafted features or pre-defined polynomials. This phenomenon proves that directly using neural networks to learn the graph spectrum is a promising way to construct powerful GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">ABLATION STUDIES</head><p>We perform ablation studies on two node-level datasets and one graph-level dataset to evaluate the effectiveness of each component. The results are shown in Table <ref type="table" target="#tab_5">4</ref>. The top three lines show the effect of the encoder, i.e., eigenvalue encoding (EE) and self-attention. It can be seen that EE is more important on Squirrel than on Citeseer. The reason is that the spectral filter of Squirrel is more difficult to learn. Therefore, the model needs the encoding to learn better representations. The attention module consistently improves performance by capturing the dependency among eigenvalues.</p><p>The bottom three lines verify the performance of graph filters at different scales. We can see that in the easy task, e.g., Citeseer, the Small and Medium models have similar performance, but the Large model causes serious overfitting. In the hard task, e.g., Squirrel and MolPCBA, the Large model is slightly better than the Medium model but outperforms the Small model a lot, implying that adding the number of parameters can boost the performance. In summary, it is important to consider the difficulty of tasks when selecting models. The results are shown in Figures <ref type="figure">3, 4,</ref> and<ref type="figure">5</ref>, from which we have some interesting observations. (1) Similar dependency patterns can be learned on different graphs. In low-pass filtering, e.g., Citeseer and Low-pass, all frequency bands tend to use the low-frequency information. While, in the bandrelated filtering, e.g., Squirrel, Band-pass, and Band-rejection, the low-and high-frequency highly depend on the medium-frequency, and the situation of the medium-frequency is opposite. (2) The more difficult the task, the less obvious the dependency. On Comb and ZINC, the dependency of eigenvalues is inconspicuous. (3) On graph-level datasets, the decoder can learn different filters. Figure <ref type="figure">5</ref> shows two basic filters. It can be seen that ?1 and ?2 have different dependencies and patterns, which are different from node-level tasks, where only one filter is needed. The finding suggests that these graph-level tasks are more difficult than node-level tasks and are still challenging for spectral GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we propose Specformer that leverages Transformer to build a set-to-set spectral filter along with learnable bases. Specformer effectively captures magnitudes and relative dependencies of the eigenvalues in a permutation-equivariant fashion and can perform non-local graph convolution. Experiments on synthetic and real-world datasets demonstrate that Specformer outperforms various GNNs and learns meaningful spectrum patterns. A promising future direction is to improve the efficiency of Specformer through sparsifying the self-attention matrix of Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A EXPERIMENTAL DETAILS</head><p>A.1 DATASETS  <ref type="formula">2020</ref>) provides one time-based split for arXiv dataset. Therefore, we run the Penn94 dataset five times, each with a different split. And we run the arXiv dataset ten times, each with the same split and a different initialization. For other datasets, we run the experiments ten times, each with a different split and initialization because there is no official splitting. In the graph-level tasks, we use the official splitting provided by <ref type="bibr" target="#b18">Hu et al. (2020)</ref> and run the experiments ten times, each with a different initialization.</p><p>Optimizer. For the node classification task, we use the Adam <ref type="bibr" target="#b20">(Kingma &amp; Ba, 2015)</ref> optimizer, as suggested by <ref type="bibr" target="#b16">He et al. (2021)</ref>; <ref type="bibr" target="#b41">Wang &amp; Zhang (2022)</ref>. For graph-level tasks, we use the AdamW <ref type="bibr" target="#b26">(Loshchilov &amp; Hutter, 2019)</ref> optimizer, with the default parameters of =1e-8 and (? 1 , ? 2 ) = (0.99, 0.999), as suggested by <ref type="bibr" target="#b46">Ying et al. (2022)</ref>; <ref type="bibr" target="#b31">Ramp?sek et al. (2022)</ref>. Besides, we also use a learning rate scheduler for graph-level tasks, which is first a linear warm-up stage followed by a cosine decay.</p><p>Model selection. In the node classification task, we run the experiments with 2000 epochs and stop the training in advance if the validation loss does not continuously decrease for 200 epochs. In the graph-level tasks, we run the experiments without early stop. Then we choose the model checkpoint with the lowest validation loss for evaluation.</p><p>Environment. The environment in which we run experiments is:</p><p>? Operating system: Linux version 3.10.0-693.el7.x86 64</p><p>? CPU information: Intel(R) Xeon(R) Silver 4210 CPU @ 2.20GHz</p><p>? GeForce RTX 3090 (24GB)</p><p>Hyperparameters. The hyperparameters of specformer can be seen in Tables <ref type="table">7</ref> and<ref type="table">8</ref>.</p><p>Since B is row-normalized, we hope the condensation B is still approximately row-normalized. For this purpose, we first sum the columns of B through the pre-defined frequency bands, e.g., Low, Medium, and High, and then average the rows.</p><p>Bi,j = ?p?fi ?q?fj</p><formula xml:id="formula_14">B p,q |1 ??fi |,<label>(9)</label></formula><p>where f i and f j are the frequency bands, and |1 ??fi | indicates the number of eigenvalues belonging to the frequency band f i .</p><p>Through this condensation strategy, we can approximately preserve the self-attention matrix's information and find the frequency bands' dependency patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C MORE EXPERIMENTAL RESULTS</head><p>C  Here we visualize the outputs of eigenvalue encoding with different values of . Specifically, we uniformly sample 50 eigenvalues from the region [0, 2] and map them into representations with d = 64. The results are shown in Figure <ref type="figure" target="#fig_4">6</ref>. We can find that when = 1, only the first 20 dimensions can distinguish different eigenvalues. As increases, the resolution of eigenvalue encoding becomes higher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 TIME AND SPACE OVERHEAD</head><p>We test the time and space overheads of Specformer and two popular polynomial GNNs, i.e., GPR-GNN and BernNet. Polynomial GNNs are implemented with sparse matrices and sparse matrix multiplication. We choose three datasets, i.e., Squirrel, Penn94, and ZINC, two for node classification and one for graph regression. For ZINC dataset, we sample 2,000 molecular graphs as the inputs of the forward process and omit the edge features that cannot be used by polynomial GNNs.</p><p>Setup. In the complexity analysis, we mentioned that spectral decomposition only needs to be calculated once and can be reused in the forward process. To perform a fair comparison, we run each model for 1000 epochs and report the total time and space costs. For polynomial GNNs, we set K = 10 as suggested by the original papers; for Specformer, we use full eigenvectors for Squirrel and ZINC, and 6,000 eigenvectors for Penn94. The hidden dimension is d = 64 for all methods.</p><p>Results. From the time overheads in Table <ref type="table" target="#tab_8">9</ref>, we can find that the spectral decomposition of small graphs, e.g., Squirrel and ZINC, does not bring much computational cost. And the forward time of Specformer is close to GPR-GNN and less than BernNet. This is because polynomial GNNs need to calculate AX or LX recurrently. Specformer only needs to calculate U diag(?)U X once, due to the non-local capability. Besides, BernNet needs to calculate all the combinations of L and 2I -L, i.e., K k=1 K k (2I -L) K-k (L) k , which requires a lot of computations. In the Penn94 dataset, we use truncated spectral decomposition to reduce the forward complexity from</p><formula xml:id="formula_15">O(n 2 (d + M ) + nd(L + d)) to O(q 2 (d + M ) + nd(L + d))</formula><p>, where q is the number of eigenvalues.</p><p>Table <ref type="table" target="#tab_8">9</ref> shows the space overheads, where Specformer is higher than polynomials GNNs because of the dense eigenvectors. One can use fewer eigenvectors to reduce the space cost. In addition to visual comparisons of learned spectral filters, we also compare Specformer and polynomial GNNs from the spatial perspective. In Figure . 7, we show the raw images, ground truth images filtered by Comb filter, i.e. |sin(??)|, and images filtered by Specformer and GPR-GNN. We can see that Specformer is similar to the ground truth, and the contrast of GPR-GNN is darker than the ground truth, implying that Specformer is better than polynomial GNNs at capturing global information.</p><p>Raw Image Specformer GroundTruth GPR-GNN PCQM4Mv2 is a large-scale graph regression dataset <ref type="bibr" target="#b19">(Hu et al., 2021)</ref>, which has 3.7M graphs, and the goal is to regress the HOMO-LUMO gap. We follow the experimental setting of GPS <ref type="bibr" target="#b31">(Ramp?sek et al., 2022)</ref>. Because the original test set is unreachable, we use the original validation set as the test set and randomly sample 150K molecules for validation Due to the time limitation, we only run Specformer-Medium on the largest molecular datasets. The results are shown in Table <ref type="table" target="#tab_9">11</ref>. We can see that due to the share of learnable bases, the number of parameters of Specformer-Medium is relatively small. That is to say, there is only one Transformer block in Specformer-Medium. But the performance of Specformer-Medium is better than the baselines with similar parameters, e.g., GCN, GIN, and GPS-small. Proof. We show that Specformer is permutation equivariant by proving that all the components of Specformer are permutation equivariant. First, the element-wise functions, i.e., eigenvalue encoding, feed-forward networks, and layer normalization, are permutation equivariant because they are applied in node-independent manner. Second, the self-attention mechanism is permutation equivariant because (P ZP )(P ZP ) = P (ZZ )P , where Z is the data representation matrix and P is an arbitrary permutation matrix. Third, the construction of learnable bases is permutation equivariant because (P U P )(P ?P )(P U P ) = P (U ?U )P .</p><p>Based on all the conclusions above, we prove that Specformer is permutation equivariant. with continuous outer and inner functions ? : R 2M +1 ? R and ? : R ? R 2M +1 . The inner function ? is independent of the function f . Proposition 2. Specformer can approximate any univariate and multivariate continuous functions.</p><p>Proof. We first prove that Specformer can approximate any univariate continuous functions. We set the self-attention matrix to be an identity matrix. Then Specformer becomes a scalar-to-scalar function, and the spectral filter is learned through the eigenvalue encoding ?(?) in Equation <ref type="formula" target="#formula_3">2</ref>. Given a linear transformation w ? R d+1 , the eigenvalue encoding becomes a Fourier series: To choose orthogonal bases, one can set ?(?, 2i) = sin(i?), ?(?, 2i + 1) = cos(i?).</p><p>We then prove that Specformer can approximate any multivariate continuous functions. Theorem 2 states that any multivariate function is a superposition of continuous functions of a single variable.</p><p>Let ? be the eigenvalue encoding, ? m be the self-attention weight, and ? be the FFN decoder in Equation <ref type="formula" target="#formula_7">4</ref>. Because eigenvalue encoding can approximate any continuous univariate functions and <ref type="bibr" target="#b27">Montanelli &amp; Yang (2020)</ref> proves that deep ReLU networks can approximate the outer function ?, Specformer can approximate any continuous multivariate functions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: Illustration of the proposed Specformer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustrations of filters learned by two polynomial GNNs and Specformer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :Figure 5 :</head><label>345</label><figDesc>Figure 3: The dependency of eigenvalues on synthetic graphs.</figDesc><graphic url="image-11.png" coords="9,131.59,196.86,51.92,51.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Eigenvalue encoding with different values of . Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Synthetic data filtered by GPR-GNN and Specformer. Best viewed in color.</figDesc><graphic url="image-23.png" coords="16,126.87,505.29,77.35,77.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>D. 2</head><label>2</label><figDesc>APPROXIMATING UNIVARIATE AND MULTIVARIATE FUNCTIONS Theorem 1. (Uniform convergence of Fourier series) (Stein &amp; Shakarchi, 2011) For any continuous real-valued function f (x) on [a, b] and f (x) is piece-wise continuous on [a, b] and any &gt; 0, there exists a Fourier series P (x) converges to f (x) uniformly such that max a?x?b |P (x) -f (x)| &lt; . (10) Theorem 2. (Kolmogorov-Arnold representation theorem) (Zaheer et al., 2017) Let f : [0, 1] M ? R be an arbitrary multivariate continuous function. Then it has the representation f (x 1 , . . . , x M ) = ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>is determined by 10000 2i/d . Because the eigenvalues fall in the range [0, 2], based on Theorem 1, Specformer can approximate any univariate continuous functions in the interval [0, 2].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>TO RELATED MODELS Specformer v.s. Polynomial GNNs. Specformer replaces the fixed bases of polynomials, e.g., ?, ? 2 , ? ? ? , ? k , with learnable bases, which has two major advantages: (1) Universality. Polynomial GNNs are the special cases of Specformer because the learnable bases can approximate any polynomials. (2) Flexibility. Polynomial GNNs are designed to learn a shared function for all eigenvalues whereas Specformer can learn eigenvalue-specific functions, thus being more flexible.</figDesc><table /><note><p>Specformer v.s. MPNNs. MPNNs aggregate the local information from neighborhood one hop per layer. This localization capability enables MPNNs with high computational efficiency but weakens the ability in capturing the global information. Specformer is inherently non-local due to the use of (often dense) eigenvectors. The learned graph Laplacian U</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The overall complexity of Specformer is the sum of the forward complexity and the decomposition complexity amortized over the number of uses in training and inference, rather than a simple summation of the two. See Appendix C.2 for more discussion.</figDesc><table /><note><p><p>Proposition 1. Specformer is permutation equivariant. Proposition 2. Specformer can approximate any univariate and multivariate continuous functions.</p>Proposition 1 shows that Specformer can learn permutation-equivariant node representations. Proposition 2 states that Specformer is more expressive than other graph filters. First, the ability to approximate any univariate functions generalizes existing scalar-to-scalar filters. Besides, Specformer can handle multiple eigenvalues and learn multivariate functions, so it can approximate a broader range of filter functions than scalar-to-scalar filters. All proofs are provided in Appendix D Complexity. Specformer has two parts of computation: spectral decomposition and forward process. Spectral decomposition is pre-computed and has the complexity of O(n 3 ). The forward complexity has three parts: Transformer, learnable bases, and graph convolution. Their corresponding complexities are O(n 2 d + nd 2 ), O(M n 2 ) and O(Lnd), respectively, where n, M, L represent the number of nodes, filters, and layers, and d is the hidden dimension. The overall forward complexity is O(n 2 (d + M ) + nd(L + d)).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Node regression results, mean of the sum of squared error &amp; R 2 score, on synthetic data.</figDesc><table><row><cell cols="4">Model (?2k param.)</cell><cell>Low-pass exp(-10? 2 )</cell><cell>High-pass 1 -exp(-10? 2 )</cell><cell>Band-pass exp(-10(? -1) 2 )</cell><cell>Band-rejection 1 -exp(-10(? -1) 2 )</cell><cell>Comb |sin(??)|</cell></row><row><cell cols="3">GCN</cell><cell></cell><cell>3.4799(.9872)</cell><cell>67.6635(.2364)</cell><cell>25.8755(.1148)</cell><cell>21.0747(.9438)</cell><cell>50.5120(.2977)</cell></row><row><cell cols="3">GAT</cell><cell></cell><cell>2.3574(.9905)</cell><cell>21.9618(.7529)</cell><cell>14.4326(.4823)</cell><cell>12.6384(.9652)</cell><cell>23.1813(.6957)</cell></row><row><cell cols="3">ChebyNet</cell><cell></cell><cell>0.8220(.9973)</cell><cell>0.7867(.9903)</cell><cell>2.2722(.9104)</cell><cell>2.5296(.9934)</cell><cell>4.0735(.9447)</cell></row><row><cell cols="3">GPR-GNN</cell><cell></cell><cell>0.4169(.9984)</cell><cell>0.0943(.9986)</cell><cell>3.5121(.8551)</cell><cell>3.7917(.9905)</cell><cell>4.6549(.9311)</cell></row><row><cell cols="3">BernNet</cell><cell></cell><cell>0.0314(.9999)</cell><cell>0.0113(.9999)</cell><cell>0.0411(.9984)</cell><cell>0.9313(.9973)</cell><cell>0.9982(.9868)</cell></row><row><cell cols="4">JacobiConv</cell><cell>0.0003(.9999)</cell><cell>0.0064(.9999)</cell><cell>0.0213(.9999)</cell><cell>0.0156(.9999)</cell><cell>0.2933(.9995)</cell></row><row><cell cols="4">Specformer</cell><cell>0.0002(.9999)</cell><cell>0.0026(.9999)</cell><cell>0.0017(.9999)</cell><cell>0.0014(.9999)</cell><cell>0.0057(.9999)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">1 exp( 10 2 )</cell><cell></cell></row><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>New Eigenvalues (g )</cell><cell>0.0 0.2 0.4 0.6 0.8</cell><cell>0.0</cell><cell cols="2">0.5 Raw Eigenvalues ( ) 1.0 1.5 GPR-GNN BernNet Specformer 2.0 GroundTruth</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results on real-world node classification tasks. Mean accuracy (%) ? 95% confidence interval. * means re-implemented baselines. "OOM" means out of GPU memory.</figDesc><table><row><cell></cell><cell>Param.</cell><cell></cell><cell cols="2">Heterophilic</cell><cell></cell><cell></cell><cell cols="2">Homophilic</cell></row><row><cell></cell><cell>on Photo</cell><cell>Chameleon</cell><cell>Squirrel</cell><cell>Actor</cell><cell>Penn94</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Photo</cell><cell>arXiv</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Spatial-based GNNs</cell><cell></cell><cell></cell><cell></cell></row><row><cell>GCN</cell><cell>48K</cell><cell cols="8">59.61?2.21 46.78?0.87 33.23?1.16 82.47?0.27 87.14?1.01 79.86?0.67 88.26?0.73 71.74?0.29</cell></row><row><cell>GAT</cell><cell>49K</cell><cell cols="8">63.13?1.93 44.49?0.88 33.93?2.47 81.53?0.55 88.03?0.79 80.52?0.71 90.94?0.68 71.82?0.23</cell></row><row><cell>H 2 GCN</cell><cell>60K</cell><cell cols="3">57.11?1.58 36.42?1.89 35.86?1.03</cell><cell>OOM</cell><cell cols="3">86.92?1.37 77.07?1.64 93.02?0.91</cell><cell>OOM</cell></row><row><cell>GCNII</cell><cell>49K</cell><cell cols="8">63.44?0.85 41.96?1.02 36.89?0.95 82.92?0.59 88.46?0.82 79.97?0.65 89.94?0.31 72.04?0.19</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Spectral-based GNNs</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LanczosNet*</cell><cell>50K</cell><cell cols="8">64.81?1.56 48.64?1.77 38.16?0.91 81.55?0.26 87.77?1.45 80.05?1.65 93.21?0.85 71.46?0.39</cell></row><row><cell>ChebyNet</cell><cell>48K</cell><cell cols="8">59.28?1.25 40.55?0.42 37.61?0.89 81.09?0.33 86.67?0.82 79.11?0.75 93.77?0.32 71.12?0.22</cell></row><row><cell>GPR-GNN</cell><cell>48K</cell><cell cols="8">67.28?1.09 50.15?1.92 39.92?0.67 81.38?0.16 88.57?0.69 80.12?0.83 93.85?0.28 71.78?0.18</cell></row><row><cell>BernNet</cell><cell>48K</cell><cell cols="8">68.29?1.58 51.35?0.73 41.79?1.01 82.47?0.21 88.52?0.95 80.09?0.79 93.63?0.35 71.96?0.27</cell></row><row><cell>ChebNetII</cell><cell>48K</cell><cell cols="8">71.37?1.01 57.72?0.59 41.75?1.07 83.12?0.22 88.71?0.93 80.53?0.79 94.92?0.33 72.32?0.23</cell></row><row><cell>JacobiConv</cell><cell>48K</cell><cell cols="8">74.20?1.03 57.38?1.25 41.17?0.64 83.35?0.11 88.98?0.46 80.78?0.79 95.43?0.23 72.14?0.17</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Graph Transformers</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Transformer*</cell><cell>37K</cell><cell cols="3">46.39?1.97 31.90?3.16 39.95?1.64</cell><cell>OOM</cell><cell cols="3">71.83?1.68 70.55?1.20 90.05?1.50</cell><cell>OOM</cell></row><row><cell>Graphormer*</cell><cell>139K</cell><cell cols="3">54.49?3.11 36.96?1.75 38.45?1.38</cell><cell>OOM</cell><cell cols="3">67.71?0.78 73.30?1.21 85.20?4.12</cell><cell>OOM</cell></row><row><cell>Specformer</cell><cell>32K</cell><cell cols="8">74.72?1.29 64.64?0.81 41.93?1.04 84.32?0.32 88.57?1.01 81.49?0.94 95.48?0.32 72.37?0.18</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results on graph-level datasets. ? means lower the better, and ? means higher the better.</figDesc><table><row><cell>Model</cell><cell>ZINC(?)</cell><cell>MolHIV(?)</cell><cell>MolPCBA(?)</cell></row><row><cell>GCN</cell><cell cols="3">0.367 ? 0.011 0.7599 ? 0.0119 0.2424 ? 0.0034</cell></row><row><cell>GIN</cell><cell cols="3">0.526 ? 0.051 0.7707 ? 0.0149 0.2703 ? 0.0023</cell></row><row><cell>GatedGCN</cell><cell>0.090 ? 0.001</cell><cell>-</cell><cell>0.267 ? 0.002</cell></row><row><cell>CIN</cell><cell cols="2">0.079 ? 0.006 0.8094 ? 0.0057</cell><cell>-</cell></row><row><cell>GIN-AK+</cell><cell cols="3">0.080 ? 0.001 0.7961 ? 0.0119 0.2930 ? 0.0044</cell></row><row><cell>GSN</cell><cell cols="2">0.101 ? 0.010 0.7799 ? 0.0100</cell><cell>-</cell></row><row><cell>DGN</cell><cell cols="3">0.168 ? 0.003 0.7970 ? 0.0097 0.2885 ? 0.0030</cell></row><row><cell>PNA</cell><cell cols="3">0.188 ? 0.004 0.7905 ? 0.0132 0.2838 ? 0.0035</cell></row><row><cell>Spec-GN</cell><cell>0.070 ? 0.002</cell><cell>-</cell><cell>0.2965 ? 0.0028</cell></row><row><cell>SAN</cell><cell cols="3">0.139 ? 0.006 0.7785 ? 0.0025 0.2765 ? 0.0042</cell></row><row><cell cols="4">Graphormer 2 0.122 ? 0.006 0.7640 ? 0.0022 0.2643 ? 0.0017</cell></row><row><cell>GPS</cell><cell cols="3">0.070 ? 0.004 0.7880 ? 0.0101 0.2907 ? 0.0028</cell></row><row><cell>Specformer</cell><cell cols="3">0.066 ? 0.003 0.7889 ? 0.0124 0.2972 ? 0.0023</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies on node-level and graph-level tasks.</figDesc><table><row><cell>Encoder</cell><cell>Decoder</cell><cell cols="2">Node-level</cell><cell>Graph-level</cell></row><row><cell cols="5">?(?) Attention Small Medium Large Squirrel (?) Citeseer (?) MolPCBA (?)</cell></row><row><cell></cell><cell></cell><cell>33.05</cell><cell>80.57</cell><cell>0.2696</cell></row><row><cell></cell><cell></cell><cell>63.78</cell><cell>81.17</cell><cell>0.2933</cell></row><row><cell></cell><cell></cell><cell>64.64</cell><cell>81.49</cell><cell>0.2970</cell></row><row><cell></cell><cell></cell><cell>64.51</cell><cell>81.47</cell><cell>0.2912</cell></row><row><cell></cell><cell></cell><cell>65.10</cell><cell>80.00</cell><cell>0.2972</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Detailed information of node-level datasets.</figDesc><table><row><cell></cell><cell>Graphs</cell><cell>Nodes</cell><cell>Edges</cell><cell cols="2">Features Classes</cell></row><row><cell>Chameleon</cell><cell>1</cell><cell>2,277</cell><cell>36,101</cell><cell>2,325</cell><cell>5</cell></row><row><cell>Squirrel</cell><cell>1</cell><cell>5,201</cell><cell>217,073</cell><cell>2,089</cell><cell>5</cell></row><row><cell>Actor</cell><cell>1</cell><cell>7,600</cell><cell>33,544</cell><cell>932</cell><cell>5</cell></row><row><cell>Cora</cell><cell>1</cell><cell>2,708</cell><cell>5,429</cell><cell>1,433</cell><cell>7</cell></row><row><cell>Citeseer</cell><cell>1</cell><cell>3,327</cell><cell>4,732</cell><cell>3,703</cell><cell>6</cell></row><row><cell>Photo</cell><cell>1</cell><cell>7,650</cell><cell>110,081</cell><cell>745</cell><cell>8</cell></row><row><cell>Penn94</cell><cell>1</cell><cell cols="2">41,554 1,362,229</cell><cell>4,814</cell><cell>2</cell></row><row><cell>arXiv</cell><cell cols="3">1 169,343 1,116,243</cell><cell>128</cell><cell>40</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Detailed information of graph-level datasets.</figDesc><table><row><cell></cell><cell cols="5"># Graphs Avg. # nodes Avg. # edges Min # nodes Max # nodes</cell><cell>Tasks</cell><cell>Metric</cell></row><row><cell>ZINC</cell><cell>12,000</cell><cell>23.2</cell><cell>24.9</cell><cell>9</cell><cell>37</cell><cell>Regression</cell><cell>MAE</cell></row><row><cell>MolHIV</cell><cell>41,127</cell><cell>25.5</cell><cell>27.5</cell><cell>2</cell><cell cols="3">222 Classification AUROC</cell></row><row><cell>MolPCBA</cell><cell>437,929</cell><cell>26.0</cell><cell>28.1</cell><cell>1</cell><cell cols="2">332 Classification</cell><cell>AP</cell></row><row><cell cols="2">PCQM4Mv2 3,746,620</cell><cell>14.1</cell><cell>14.6</cell><cell>1</cell><cell>20</cell><cell>Regression</cell><cell>MAE</cell></row><row><cell cols="3">A.2 DETAILED EXPERIMENTAL SETUP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">Data splitting. In the node classification task, Lim et al. (2021) provides five official splits for</cell></row><row><cell cols="2">Penn94 dataset, Hu et al. (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Time</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>overheads (s)</cell><cell></cell></row><row><cell></cell><cell cols="4">GPR-GNN BernNet Specformer Decomposition</cell></row><row><cell>Squirrel</cell><cell>4.65</cell><cell>14.86</cell><cell>7.11</cell><cell>2.71</cell></row><row><cell>Penn94</cell><cell>14.77</cell><cell>41.26</cell><cell>21.74</cell><cell>629.6</cell></row><row><cell>ZINC-2k</cell><cell>15.06</cell><cell>49.78</cell><cell>30.59</cell><cell>1.28</cell></row><row><cell></cell><cell cols="4">Table 10: Space overheads (MB)</cell></row><row><cell></cell><cell></cell><cell cols="3">GPR-GNN BernNet Specformer</cell></row><row><cell></cell><cell>Squirrel</cell><cell>1,257</cell><cell>1,277</cell><cell>1,845</cell></row><row><cell></cell><cell>Penn94</cell><cell>3,787</cell><cell>3,799</cell><cell>4,993</cell></row><row><cell></cell><cell>ZINC</cell><cell>1,417</cell><cell>1,585</cell><cell>4,397</cell></row><row><cell cols="4">C.3 SPATIAL PERSPECTIVE OF SYNTHETIC DATA</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 11 :</head><label>11</label><figDesc>Results on large-scale graph dataset PCQM4Mv2.</figDesc><table><row><cell></cell><cell cols="2">PCQM4Mv2</cell></row><row><cell>Model</cell><cell cols="2">MAE(?) Param.</cell></row><row><cell>GCN</cell><cell>0.1379</cell><cell>2.0M</cell></row><row><cell>GCN-VN</cell><cell>0.1153</cell><cell>4.9M</cell></row><row><cell>GIN</cell><cell>0.1195</cell><cell>3.8M</cell></row><row><cell>GIN-VN</cell><cell>0.1083</cell><cell>6.7M</cell></row><row><cell>GPS-small</cell><cell>0.0938</cell><cell>6.2M</cell></row><row><cell>Specformer-medium</cell><cell>0.0916</cell><cell>4.1M</cell></row><row><cell>GRPE</cell><cell>0.0890</cell><cell>46.2M</cell></row><row><cell>EGT</cell><cell>0.0869</cell><cell>89.3M</cell></row><row><cell>Graphormer</cell><cell>0.0864</cell><cell>48.3M</cell></row><row><cell>GPS-medium</cell><cell>0.0858</cell><cell>19.4M</cell></row><row><cell>D THEORETICAL RESULTS</cell><cell></cell><cell></cell></row><row><cell>D.1 PERMUTATION</cell><cell></cell><cell></cell></row></table><note><p><p>EQUIVARIANCE</p>Proposition 1. Specformer is permutation equivariant.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://ww2.mathworks.cn/products/image.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We retrain Graphomer on MolHIV and MolPCBA datasets without using pre-training and augmentation.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>ACKNOWLEDGMENTS This work is supported in part by the <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">U20B2045</rs>, <rs type="grantNumber">62192784</rs>, <rs type="grantNumber">62172052</rs>, <rs type="grantNumber">62002029</rs>, <rs type="grantNumber">62172052</rs>, <rs type="grantNumber">U1936014</rs>), <rs type="funder">BUPT Excellent Ph.D. Students Foundation</rs> (No. <rs type="grantNumber">CX2022310</rs>), the <rs type="funder">NSERC Discovery Grants</rs> (No. <rs type="grantNumber">RGPIN-2019-05448</rs>, No. <rs type="grantNumber">RGPIN-2022-04636</rs>), and the <rs type="funder">NSERC Collaborative Research and Development Grant</rs> (No. <rs type="grantNumber">CRDPJ 543676-19</rs>). Resources used in preparing this research were provided, in part, by <rs type="funder">Advanced Research Computing</rs> at the <rs type="institution">University of British Columbia</rs>, the <rs type="programName">Oracle for Research program</rs>, and <rs type="funder">Compute Canada</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_9amNcqx">
					<idno type="grant-number">U20B2045</idno>
				</org>
				<org type="funding" xml:id="_MUfmkyD">
					<idno type="grant-number">62192784</idno>
				</org>
				<org type="funding" xml:id="_UYTeG4X">
					<idno type="grant-number">62172052</idno>
				</org>
				<org type="funding" xml:id="_SWKG7Qn">
					<idno type="grant-number">62002029</idno>
				</org>
				<org type="funding" xml:id="_4zSHcZs">
					<idno type="grant-number">62172052</idno>
				</org>
				<org type="funding" xml:id="_zjdh5wj">
					<idno type="grant-number">U1936014</idno>
				</org>
				<org type="funding" xml:id="_bgZecVC">
					<idno type="grant-number">CX2022310</idno>
				</org>
				<org type="funding" xml:id="_2jEb8NT">
					<idno type="grant-number">RGPIN-2019-05448</idno>
				</org>
				<org type="funding" xml:id="_3FyABC4">
					<idno type="grant-number">RGPIN-2022-04636</idno>
				</org>
				<org type="funding" xml:id="_NXpwzmU">
					<idno type="grant-number">CRDPJ 543676-19</idno>
				</org>
				<org type="funding" xml:id="_Jv62Jmb">
					<orgName type="program" subtype="full">Oracle for Research program</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Here we explain how to incorporate the Specformer layer with edge features. Specifically, we first broadcast the node features to the edges and filter the mixed edge features. Finally, the filtered edge features are aggregated to yield new node features.</p><p>where H ? R N ?d is the node feature matrix and E ? R N ?N ?d is the edge feature matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 CONDENSATION OF SELF-ATTENTION</head><p>In this section, we explain the details of the condensation of self-attention. We use B and B to represent the self-attention matrix and its condensation.</p><p>(8)</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Toyotaro Suzumura, and Isaiah Onando Mulang&apos;. How expressive are transformers in spectral domain for graphs?</title>
		<author>
			<persName><forename type="first">Anson</forename><surname>Bastos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Nadgeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuldeep</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroki</forename><surname>Kanezashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Machine Learning Research</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Peter W Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Beyond low-frequency information in graph convolutional networks</title>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huawei</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3950" to="3957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A note on sparse generalized eigenvalue problem</title>
		<author>
			<persName><forename type="first">Yunfeng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanhua</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="23036" to="23048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Not all low-pass filters are robust in graph convolutional networks</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yatao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiji</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="25058" to="25071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptive universal generalized pagerank graph neural network</title>
		<author>
			<persName><forename type="first">Eli</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3837" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph signal processing for machine learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Xiaowen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dorina</forename><surname>Thanou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Toni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="117" to="127" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A generalization of transformer networks to graphs</title>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dwivedi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<idno>CoRR, abs/2012.09699</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Benchmarking graph neural networks</title>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<idno>CoRR, abs/2003.00982</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spectral networks and deep locally connected networks on graphs</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna Estrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Wavelets on graphs via spectral graph theory</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>David K Hammond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?mi</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="150" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Bernnet: Learning arbitrary graph spectral filters via bernstein approximation</title>
		<author>
			<persName><forename type="first">Mingguo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongteng</forename><surname>Xu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Convolutional neural networks on graphs with chebyshev approximation, revisited</title>
		<author>
			<persName><forename type="first">Mingguo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">OGB-LSC: A large-scale challenge for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maho</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Datasets and Benchmarks</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Rethinking graph transformers with spectral attention</title>
		<author>
			<persName><forename type="first">Devin</forename><surname>Kreuzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>L?tourneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prudencio</forename><surname>Tossou</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep Learning on Graphs: Theory, Models, Algorithms and Applications</title>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Toronto (Canada</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lanczosnet: Multi-scale deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhizhen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods</title>
		<author>
			<persName><forename type="first">Derek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hohne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linda</forename><surname>Sijia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaishnavi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omkar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ser-Nam</forename><surname>Bhalerao</surname></persName>
		</author>
		<author>
			<persName><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="20887" to="20902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster). OpenReview.net</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Error bounds for deep relu networks using the kolmogorovarnold superposition theorem</title>
		<author>
			<persName><forename type="first">Hadrien</forename><surname>Montanelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haizhao</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taiji</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR. OpenReview.net</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graph signal processing: Overview, challenges, and applications</title>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jelena</forename><surname>Kovacevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Jos?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="808" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Geom-gcn: Geometric graph convolutional networks</title>
		<author>
			<persName><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingzhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR. OpenReview.net</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Recipe for a general, powerful, scalable graph transformer</title>
		<author>
			<persName><forename type="first">Ladislav</forename><surname>Ramp?sek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Galkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><forename type="middle">Prakash</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<idno>CoRR, abs/2205.12454</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-Scale Attributed Node Embedding</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Benedek Rozemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rik</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Complex Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Revisiting over-smoothing in BERT from the perspective of graph</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Kwok</surname></persName>
		</author>
		<idno>CoRR, abs/2202.08625</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Fourier analysis: an introduction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Elias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><surname>Shakarchi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Princeton University Press</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Understanding over-squashing and bottlenecks on graphs via curvature</title>
		<author>
			<persName><forename type="first">Jake</forename><surname>Topping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><forename type="middle">Di</forename><surname>Giovanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Paul Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR. OpenReview.net</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Anti-oversmoothing in deep vision transformers via the fourier domain analysis: From theory to practice</title>
		<author>
			<persName><forename type="first">Peihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<idno>CoRR, abs/2203.05962</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">How powerful are spectral graph neural networks</title>
		<author>
			<persName><forename type="first">Xiyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="23341" to="23362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><forename type="middle">H</forename><surname>Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="24" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Bingbing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huawei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunqi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07785</idno>
		<title level="m">Graph wavelet neural network</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A new perspective on the effects of spectrum in graph neural networks</title>
		<author>
			<persName><forename type="first">Mingqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baocai</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="25261" to="25279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Do transformers really perform bad for graph representation</title>
		<author>
			<persName><forename type="first">Chengxuan</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnab?s</forename><surname>P?czos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3391" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Graph neural networks: A review of methods and applications</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Open</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="57" to="81" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Interpreting and unifying graph neural networks with an optimization framework</title>
		<author>
			<persName><forename type="first">Meiqi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houye</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>ACM / IW3C2</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
