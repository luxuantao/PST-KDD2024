<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Michael</forename><surname>Bryant</surname></persName>
							<email>mbryantj@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Brown University</orgName>
								<address>
									<settlement>Providence</settlement>
									<region>RI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Erik</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
							<email>sudderth@cs.brown.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Brown University</orgName>
								<address>
									<settlement>Providence</settlement>
									<region>RI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4260B13C96FDCAA953F717822FC94A04</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Variational methods provide a computationally scalable alternative to Monte Carlo methods for large-scale, Bayesian nonparametric learning. In practice, however, conventional batch and online variational methods quickly become trapped in local optima. In this paper, we consider a nonparametric topic model based on the hierarchical Dirichlet process (HDP), and develop a novel online variational inference algorithm based on split-merge topic updates. We derive a simpler and faster variational approximation of the HDP, and show that by intelligently splitting and merging components of the variational posterior, we can achieve substantially better predictions of test data than conventional online and batch variational algorithms. For streaming analysis of large datasets where batch analysis is infeasible, we show that our split-merge updates better capture the nonparametric properties of the underlying model, allowing continual learning of new topics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Bayesian nonparametric methods provide an increasingly important framework for unsupervised learning from structured data. For example, the hierarchical Dirichlet process (HDP) <ref type="bibr" target="#b0">[1]</ref> provides a general approach to joint clustering of grouped data, and leads to effective nonparametric topic models. While nonparametric methods are best motivated by their potential to capture the details of large datasets, practical applications have been limited by the poor computational scaling of conventional Monte Carlo learning algorithms.</p><p>Mean field variational methods provide an alternative, optimization-based framework for nonparametric learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Aiming at larger-scale applications, recent work <ref type="bibr" target="#b3">[4]</ref> has extended online variational methods <ref type="bibr" target="#b4">[5]</ref> for the parametric, latent Dirichlet allocation (LDA) topic model <ref type="bibr" target="#b5">[6]</ref> to the HDP. While this online approach can produce reasonable models of large data streams, we show that the variational posteriors of existing algorithms often converge to poor local optima. Multiple runs are usually necessary to show robust performance, reducing the desired computational gains. Furthermore, by applying a fixed truncation to the number of posterior topics or clusters, conventional variational methods limit the ability of purportedly nonparametric models to fully adapt to the data.</p><p>In this paper, we propose novel split-merge moves for online variational inference for the HDP (oHDP) which result in much better predictive performance. We validate our approach on two corpora, one with millions of documents. We also propose an alternative, direct assignment HDP representation which is faster and more accurate than the Chinese restaurant franchise representation used in prior work <ref type="bibr" target="#b3">[4]</ref>. Additionally, the inclusion of split-merge moves during posterior inference allows us to dynamically vary the truncation level throughout learning. While conservative truncations can be theoretically justifed for batch analysis of fixed-size datasets <ref type="bibr" target="#b1">[2]</ref>, our data-driven adaptation of the trunction level is far better suited to large-scale analysis of streaming data. Split-merge proposals have been previously investigated for Monte Carlo analysis of nonparametric models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. They have also been used for maximum likelihood and variational analysis of parametric models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. These deterministic algorithms validate split-merge proposals by evaluating a batch objective on the entire dataset, an approach which is unexplored for nonparametric models and infeasible for online learning. We instead optimize the variational objective via stochastic gradient ascent, and split or merge based on only a noisy estimate of the variational lower bound. Over time, these local decisions lead to global estimates of the number of topics present in a given corpus. We review the HDP and conventional variational methods in Sec. 2, develop our novel split-merge procedure in Sec. 3, and evaluate on various document corpora in Sec. 4.</p><p>2 Variational Inference for Bayesian Nonparametric Models</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Hierarchical Dirichlet processes</head><p>The HDP is a hierarchical nonparametric prior for grouped mixed-membership data. In its simplest form, it consists of a top-level DP and a collection of D bottom-level DPs (indexed by j) which share the top-level DP as their base measure:</p><formula xml:id="formula_0">G 0 ∼ DP(γH), G j ∼ DP(αG 0 ), j = 1, . . . , D.</formula><p>Here, H is a base measure on some parameter space, and γ &gt; 0, α &gt; 0 are concentration parameters. Using a stick-breaking representation <ref type="bibr" target="#b0">[1]</ref> of the global measure G 0 , the HDP can be expressed as</p><formula xml:id="formula_1">G 0 = ∞ k=1 β k δ φ k , G j = ∞ k=1 π jk δ φ k .</formula><p>The global weights β are drawn from a stick-breaking distribution β ∼ GEM(γ), and atoms are independently drawn as φ k ∼ H. Each G j shares atoms with the global measure G, and the lowerlevel weights are drawn π j ∼ DP(αβ). For this direct assignment representation, the k indices for each G j index directly into the global set of atoms. To complete the definition of the general HDP, parameters ψ jn ∼ G j are then drawn for each observation n in group j, and observations are drawn x jn ∼ F (ψ jn ) for some likelihood family F . Note that ψ jn = φ zjn for some discrete indicator z jn .</p><p>In this paper we focus on an application of the HDP to modeling document corpora. The topics φ k ∼ Dirichlet(η) are distributions on a vocabulary of W words. The global topic weights, β ∼ GEM(γ), are still drawn from a stick-breaking prior. For each document j, document-specific topic frequencies are drawn π j ∼ DP(αβ). Then for each word index n in document j, a topic indicator is drawn z jn ∼ Categorical(π j ), and finally a word is drawn w jn ∼ Categorical(φ zjn ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Batch Variational Inference for the HDP</head><p>We use variational inference <ref type="bibr" target="#b13">[14]</ref> to approximate the posterior of the latent variables (φ, β, π, z)the topics, global topic weights, document-specific topic weights, and topic indicators, respectively -with a tractable distribution q, indexed by a set of free variational parameters. Appealing to mean field methods, our variational distribution is fully factorized, and is of the form</p><formula xml:id="formula_2">q(φ, β, π, z | λ, θ, ϕ) = q(β) ∞ k=1 q(φ k | λ k ) D j=1 q(π j | θ j ) Nj n=1 q(z jn | ϕ jn ),<label>(1)</label></formula><p>where D is the number of documents in the corpus and N j is the number of words in document j. Individual distributions are selected from appropriate exponential families:</p><formula xml:id="formula_3">q(β) = δ β * (β) q(φ k | λ k ) = Dirichlet(φ k | λ k ) q(π j | θ j ) = Dirichlet(π j | θ j ) q(z jn ) = Categorical(z jn | ϕ jn )</formula><p>where δ β * (β) denotes a degenerate distribution at the point β * . <ref type="foot" target="#foot_0">1</ref> In our update derivations below, we use ϕ jw to denote the shared ϕ jn for all word tokens in document j of type w.</p><p>Selection of an appropriate truncation strategy is crucial to the accuracy of variational methods for nonparametric models. Here, we truncate the topic indicator distributions by fixing q(z jn = k) = 0 for k &gt; K, where K is a threshold which varies dynamically in our later algorithms. With this assumption, the topic distributions with indices greater than K are conditionally independent of the observed data; we may thus ignore them and tractably update the remaining parameters with respect to the true, infinite model. A similar truncation has been previously used in the context of an otherwise more complex collapsed variational method <ref type="bibr" target="#b2">[3]</ref>. Desirably, this truncation is nested such that increasing K always gives potentially improved bounds, but does not require the computation of infinite sums, as in <ref type="bibr" target="#b15">[16]</ref>. In contrast, approximations based on truncations of the stick-breaking topic frequency prior <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref> are not nested, and their artifactual placement of extra mass on the final topic K is less suitable for our split-merge online variational inference.</p><p>Via standard convexity arguments <ref type="bibr" target="#b13">[14]</ref>, we lower bound the marginal log likelihood of the observed data using the expected complete-data log likelihood and the entropy of the variational distribution,</p><formula xml:id="formula_4">L(q) def = E q [log p(φ, β, π, z, w | α, γ, η)] -E q [log q(φ, π, z | λ, θ, ϕ)] = E q [log p(w | z, φ)] + E q [log p(z | π)] + E q [log p(π | αβ)] + E q [log p(φ | η)] + E q [log p(β | γ)] -E q [log q(z | ϕ)] -E q [log q(π | θ)] -E q [log q(φ | λ)] = D j=1 E q [log p(w j | z j , φ)] + E q [log p(z j | π j )] + E q [log p(π j | αβ)] -E q [log q(z j | ϕ j )] -E q [log q(π j | θ j )] + 1 D E q [log p(φ | η)] + E q [log p(β | γ)] -E q [log q(φ | λ)] ,<label>(2)</label></formula><p>and maximize this quantity by coordinate ascent on the variational parameters. The expectations are with respect to the variational distribution. Each expectation is dependent on only a subset of the variational parameters; we leave off particular subscripts for notational clarity. Note that the expansion of the variational lower bound in (2) contains all terms inside a summation over documents. This is the key observation that allowed <ref type="bibr" target="#b4">[5]</ref> to develop an online inference algorithm for LDA. A full expansion of the variational objective is given in the supplemental material. Taking derivatives of L(q) with respect to each of the variational parameters yields the following updates:</p><formula xml:id="formula_5">ϕ jwk ∝ exp {E q [log φ kw ] + E q [log π jk ]}<label>(3)</label></formula><formula xml:id="formula_6">θ jk ← αβ k + W w=1 n w(j) ϕ jwk<label>(4)</label></formula><formula xml:id="formula_7">λ kw ← η + D j=1 n w(j) ϕ jwk ,<label>(5)</label></formula><p>Here, n w(j) is the number of times word w appears in document j. The expectations in ( <ref type="formula" target="#formula_5">3</ref>) are</p><formula xml:id="formula_8">E q [log φ kw ] = Ψ(λ kw ) -Ψ( i λ ki ), E q [log π jk ] = Ψ(θ jk ) -Ψ( i θ ji ),</formula><p>where Ψ(x) is the digamma function, the first derivative of the log of the gamma function.</p><p>In evaluating our objective, we represent β * as a (K + 1)-dim. vector containing the probabilities of the first K topics, and the total mass of all other topics. While β * cannot be optimized in closed form, it can be updated via gradient-based methods; we use a variant of L-BFGS. Drawing a parallel between variational inference and the expectation maximization (EM) algorithm, we label the document-specific updates of (ϕ j , θ j ) the E-step, and the corpus-wide updates of (λ, β) the M-step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Online Variational Inference</head><p>Batch variational inference requires a full pass through the data at each iteration, making it computationally infeasible for large datasets and impossible for streaming data. To remedy this, we adapt and improve recent work on online variational inference algorithms <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>The form of the lower bound in (2), as a scaled expectation with respect to the document collection, suggests an online learning algorithm. Given a learning rate ρ t satisfying ∞ t=0 ρ t = ∞ and ∞ t=0 ρ<ref type="foot" target="#foot_1">2</ref> t &lt; ∞, we can optimize the variational objective stochastically. Each update begins by sampling a "mini-batch" of documents S, of size |S|. After updating the mini-batch of documentspecific parameters (ϕ j , θ j ) by iterating <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b3">4)</ref>, we update the corpus-wide parameters as</p><formula xml:id="formula_9">λ kw ← (1 -ρ t )λ kw + ρ t λkw ,<label>(6)</label></formula><formula xml:id="formula_10">β * k ← (1 -ρ t )β * k + ρ t βk , (<label>7</label></formula><formula xml:id="formula_11">)</formula><p>where λkw is a set of sufficient statistics for topic k, computed from a noisy estimate of ( <ref type="formula" target="#formula_7">5</ref>):</p><formula xml:id="formula_12">λkw = η + D |S| j∈S n w(j) ϕ jwk .<label>(8)</label></formula><p>The candidate topic weights β are found via gradient-based optimization on S. The resulting inference algorithm is similar to conventional batch methods, but is applicable to streaming, big data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Split-Merge Updates for Online Variational Inference</head><p>We develop a data-driven split-merge algorithm for online variational inference for the HDP, referred to as oHDP-SM. The algorithm dynamically expands and contracts the truncation level K by splitting and merging topics during specialized moves which are interleaved with standard online variational updates. The resulting model truly allows the number of topics to grow with the data. As such, we do not have to employ the technique of <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3]</ref> and other truncated variational approaches of setting K above the expected number of topics and relying on the inference to infer a smaller number. Instead, we initialize with small K and let the inference discover new topics as it progresses, similar to the approach used in <ref type="bibr" target="#b16">[17]</ref>. One can see how this property would be desirable in an online setting, as documents seen after many inference steps may still create new topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Split: Creation of New Topics</head><p>Given the result of analyzing one mini-batch q * = (ϕ j , θ j ) |S| j=1 , λ, β * , and the corresponding value of the lower bound L(q * ), we consider splitting topic k into two topics k ′ , k ′′ . 2 The split procedure proceeds as follows: (1) initialize all variational posteriors to break symmetry between the new topics, using information from the data; (2) refine the new variational posteriors using a restricted iteration; (3) accept or reject the split via the change in variational objective value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initialize new variational posteriors</head><p>To break symmetry, we initialize the new topic posteriors (λ k ′ , λ k ′′ ), and topic weights (β * k ′ , β * k ′′ ), using sufficient statistics from the previous iteration:</p><formula xml:id="formula_13">λ k ′ = (1 -ρ t )λ k , λ k ′′ = ρ t λk , β * k ′ = (1 -ρ t )β * k , β * k ′′ = ρ t βk .</formula><p>Intuitively, we expect the sufficient statistics to provide insight into how a topic was actually used during the E-step. The minibatch-specific parameters {ϕ j , θ j } |S| j=1 are then initialized as follows,</p><formula xml:id="formula_14">ϕ jwk ′ = ω k ϕ jwk , ϕ jwk ′′ = (1 -ω k )ϕ jwk , θ jk ′ = ω k θ jk , θ jk ′′ = (1 -ω k )θ jk ,</formula><p>with the weights defined as</p><formula xml:id="formula_15">ω k = β k ′ /(β k ′ + β k ′′ ). Algorithm 1 Restricted iteration 1: initialize (λ ℓ , β ℓ ) for ℓ ∈ {k ′ , k ′′ } 2: for j ∈ S do 3: initialize (ϕ j , θ j ) for ℓ ∈ {k ′ , k ′′ } 4:</formula><p>while not converged do 5:</p><p>update (ϕ j , θ j ) for ℓ ∈ {k ′ , k ′′ } using <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b3">4)</ref> 6:</p><p>end while</p><formula xml:id="formula_16">7:</formula><p>update (λ ℓ , β ℓ ) for ℓ ∈ {k ′ , k ′′ } using (6, 7) 8: end for Restricted iteration After initializing the variational parameters for the new topics, we update them through a restricted iteration of online variational inference. The restricted iteration consists of restricted analogues to both the E-step and the M-step, where all parameters except those for the new topics are held constant. This procedure is similar to, and inspired by, the "partial E-step" for split-merge EM <ref type="bibr" target="#b9">[10]</ref> and restricted Gibbs updates for split-merge MCMC methods <ref type="bibr" target="#b6">[7]</ref>.</p><p>All values of ϕ jwℓ and θ jℓ , ℓ / ∈ {k ′ , k ′′ }, remain unchanged. It is important to note that even though these values are not updated, they are still used in the calculations for both the variational expectation of π j and the normalization of ϕ. In particular,</p><formula xml:id="formula_17">ϕ jwk ′ = exp {E q [log φ k ′ w ] + E q [log π jk ′ ]} ℓ∈T exp {E q [log φ ℓw ] + E q [log π jℓ ]} , E q [log π jk ′ ] = Ψ(θ jk ′ ) -Ψ( k∈T θ jk ),</formula><p>where T is the original set of topics, minus k, plus k ′ and k ′′ . The expected log word probabilities E q [log φ k ′ w ] and E q [log φ k ′′ w ] are computed using the newly updated λ values.</p><p>Evaluate Split Quality Let ϕ split for minibatch S be ϕ as defined above, but with ϕ jwk replaced by the ϕ jwk ′ and ϕ jwk ′′ learned in the restricted E-step. Let θ split , λ split and β * split be defined similarly. Now we have a new model state q split(k) = (ϕ split , θ split ) |S| j=1 , λ split , β * split . We calculate L q split(k) , and if L q split(k) &gt; L(q * ), we update the new model state q * ← q split(k) , accepting the split. If L q split(k) &lt; L(q * ), then we go back and test another split, until all splits are tested. In practice we limit the maximum number of allowed splits each iteration to a small constant. If we wish to allow the model to expand the number of topics more quickly, we can increase this number. Finally, it is important to note that all aspects of the split procedure are driven by the data -the new topics are initialized using data-driven proposals, refined by re-running the variational E-step, and accepted based on an unbiased estimate of the change in the variational objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Merge: Removal of Redundant Topics</head><p>Consider a candidate merge of two topics, k ′ and k ′′ , into a new topic k. For batch variational methods, it is straightforward to determine whether such a merge will increase or decrease the variational objective by combining all parameters for all documents,</p><formula xml:id="formula_18">ϕ jwk = ϕ jwk ′ + ϕ jwk ′′ , θ jk = θ jk ′ + θ jk ′′ , β k = β k ′ + β k ′′ , λ k = λ k ′ + λ k ′′ ,</formula><p>and computing the difference in the variational objective before and after the merge. Because many terms cancel, computing this bound change is fairly computationally inexpensive, but it can still be computationally infeasible to consider all pairs of topics for large K. Instead, we identify potential merge candidates by looking at the sample covariance of the θ j vectors across the corpus (or minibatch). Topics with positive covariance above a certain threshold have the quantitative effects of their merge evaluated. Intuitively, if there are two copies of a topic or a topic is split into two pieces, they should tend to be used together, and therefore have positive covariance. For consistency in notation, we call the model state with topics k ′ and k ′′ merged q merge(k ′ ,k ′′ ) .</p><p>Combining this merge procedure with the previous split proposals leads to the online variational method of Algorithm 2. In an online setting, we can only compute unbiased noisy estimates of the true difference in the variational objective; split or merge moves that increase the expected variational objective are not guaranteed to do so for the objective evaluated over the entire corpus. The Algorithm 2 Online variational inference for the HDP + split-merge 1: initialize (λ, β * ) 2: for t = 1, 2, . . . do 3:</p><p>for j ∈ minibatch S do 4:</p><p>initialize (ϕ j , θ j ) 5:</p><p>while not converged do 6:</p><p>update (ϕ j , θ j ) using <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b3">4)</ref> 7:</p><p>end while 8:</p><p>end for 9:</p><p>for pairs of topics {k ′ , k ′′ } ∈ K × K with Cov(θ jk ′ , θ jk ′′ ) &gt; 0 do 10:</p><p>if L q merge(k ′ ,k ′′ ) &gt; L(q) then 11:</p><p>q ← q merge(k ′ ,k ′′ )</p><p>12:</p><p>end if</p><formula xml:id="formula_19">13:</formula><p>end for 14:</p><p>update (λ, β * ) using (6, 7)</p><p>15:</p><p>for k = 1, 2, . . . , K do 16:</p><p>compute L q split(k) via restricted iteration 17:</p><p>if L q split(k) &gt; L(q) then 18:</p><p>q ← q split(k)</p><p>19:</p><p>end if</p><formula xml:id="formula_20">20:</formula><p>end for 21: end for uncertainty associated with the online method can be mitigated to some extent by using large minibatches. Confidence intervals for the expected change in the variational objective can be computed, and might be useful in a more sophisticated acceptance rule. Note that our usage of a nested family of variational bounds is key to the accuracy and stability of our split-merge acceptance rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>To demonstrate the effectiveness of our split-merge moves, we compare three algorithms: batch variational inference (bHDP), online variational inference without split-merge (oHDP), and online variational inference with split-merge (oHDP-SM). On the NIPS corpus we also compare these three methods to collapsed Gibbs sampling (CGS) and the CRF-style oHDP model (oHDP-CRF) proposed by <ref type="bibr" target="#b3">[4]</ref>. <ref type="foot" target="#foot_2">3</ref> We test the models on one synthetic and two real datasets: Bars A 20-topic bars dataset of the type introduced in <ref type="bibr" target="#b17">[18]</ref>, where topics can be viewed as bars on a 10 × 10 grid. The vocabulary size is 100, with a training set of 2000 documents and a test set of 200 documents, 250 words per document. NIPS 1,740 documents from the Neural Information Processing Systems conference proceedings, 1988-2000. The vocabulary size is 13,649, and there are 2.3 million tokens in total. We randomly divide the corpus into a 1,392-document training set and a 348-document test set. New York Times The New York Times Annotated Corpus<ref type="foot" target="#foot_3">4</ref> consists of over 1.8 million articles appearing in the New York Times between 1987 and 2007. The vocabulary is pruned to 8,000 words. We hold out a randomly selected subset of 5,000 test documents, and use the remainder for training.</p><p>All values of K given for oHDP-SM models are initial values -the actual truncation levels fluctuate during inference. While the truncation level K is different from the actual number of topics assigned non-negligible mass, the split-merge model tends to merge away unused topics, so these numbers are usually fairly close. Hyperparameters are initialized to consistent values across all algorithms and datasets, and learned via Newton-Raphson updates (or in the case of CGS, resampled). We use a constant learning rate across all online algorithms. As suggested by <ref type="bibr" target="#b3">[4]</ref>, we set ρ t = (τ + t) -κ where τ = 1, κ = 0.5. Empirically, we found that slower learning rates could result in greatly reduced performance, across all models and datasets.</p><p>To compare algorithm performance, we use per-word heldout likelihood, similarly to the metrics of <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b3">4]</ref>. We randomly split each test document in D test into 80%-20% pieces, w j1 and w j2 . Then, using φ as the variational expectation of the topics from training, we learn πj on w j1 and approximate the probability of w j2 as w∈wj2 k πjk φkw . The overall test metric is then</p><formula xml:id="formula_21">E = j∈D test w∈wj2 log k πjk φkw j∈D test |w j2 | 4.1 Bars</formula><p>For the bars data, we initialize eight oHDP-SM runs with K = {2, 5, 10, 20, 40, 50, 80, 100}, eight runs of oHDP with K = 20, and eight runs with K = 50. As seen in Figure <ref type="figure">2</ref>(a), the oHDP algorithm converges to local optima, while the oHDP-SM runs all converge to the global optimum. More importantly, all split-merge methods converge to the correct number of topics, while oHDP uses either too few or too many topics. Note that the data-driven split-merge procedure allows splitting and merging of topics to mostly cease once the inference has converged (Figure <ref type="figure">2(d)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">NIPS</head><p>We compare oHDP-SM, oHDP, bHDP, oHDP-CRF, and CGS in Figure <ref type="figure">2</ref>. Shown are two runs of oHDP-SM with K = {100, 300}, two runs each of oHDP and bHDP with K = {300, 1000}, and one run each of oHDP-CRF and CGS with K = 300. All the runs displayed are the best runs from a larger sample of trials. Since oHDP and bHDP will use only a subset of topics under the truncation, setting K much higher results in comparable numbers of topics as oHDP-SM. We set |S| = 200 for the online algorithms, and run all methods for approximately 40 hours of CPU time.</p><p>The non split-merge methods reach poor local optima relatively quickly, while the split-merge algorithms continue to improve. Notably, both oHDP-CRF and CGS perform much worse than any of our methods. It appears that the CRF model performs very poorly for small datasets, and CGS reaches a mode quickly but does not mix between modes. Even though the split-merge algorithms improve in part by adding topics, they are using their topics much more effectively (Figure <ref type="figure">2(h)</ref>). We speculate that for the NIPS corpus especially, the reason that models achieve better predictive likelihoods with more topics is due to the bursty properties of text data <ref type="bibr" target="#b19">[20]</ref>. Figure <ref type="figure">3</ref> illustrates the topic refinement and specialization which occurs in successful split proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">New York Times</head><p>As batch variational methods and samplers are not feasible for such a large dataset, we compare two runs of oHDP with K = {300, 500} to a run of oHDP-SM with K = 200 initial topics. We also use a larger minibatch size of |S| = 10,000; split-merge acceptance decisions can sometimes be unstable with overly small minibatches. Figure <ref type="figure">2(c</ref>) shows an inherent problem with oHDP for very large datasets -when truncated to K = 500, the algorithms uses all of its available topics and exhibits overfitting. For the oHDP-SM, however, predictive likelihood improves over a substantially longer period and overfitting is greatly reduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>We have developed a novel split-merge online variational algorithm for the hierarchical DP. This approach leads to more accurate models and better predictive performance, as well as a model that is able to adapt the number of topics more freely than conventional approximations based on fixed truncations. Our moves are similar in spirit to split-merge samplers, but by evaluating their quality stochastically using streaming data, we can rapidly adapt model structure to large-scale datasets.</p><p>While many papers have tried to improve conventional mean field methods via higher-order variational expansions <ref type="bibr" target="#b20">[21]</ref>, local optima can make the resulting algorithms compare unfavorably to Monte Carlo methods <ref type="bibr" target="#b2">[3]</ref>. Here we pursue the complementary goal of more robust, scalable optimization of simple variational objectives. Generalization of our approach to more complex hierarchies of DPs, or basic DP mixtures, is feasible. We believe similar online learning methods will prove effective for the combinatorial structures of other Bayesian nonparametric models.    Figure <ref type="figure">3</ref>: The evolution of a split topic. The left column shows the topic directly prior to the split. After 240,000 more documents have been analyzed, subtle differences become apparent: the top topic covers terms relating to general neuronal behavior, while the bottom topic deals more specifically with neuron firing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Directed graphical representation of a hierarchical Dirichlet process topic model, in which an unbounded collection of topics φ k model the Nj words in each of D documents. Topics occur with frequency πj in document j, and with frequency β across the full corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Trace plots of heldout likelihood and number of topics used. Across all datasets, common color indicates common algorithm, while for NIPS and New York Times, line type indicates different initializations. Top: Test log likelihood for each dataset. Middle: Number of topics used per iteration. Bottom: A plot of per-word log likelihood against number of topics used. Note particularly plot (h), where for every cardinality of used topics shown, there is a split-merge method outperforming a conventional method.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>-7.4</cell><cell></cell><cell></cell><cell>-7.56</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-7.58</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>-7.5</cell><cell></cell><cell></cell><cell>-7.6</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>per-word log likelihood</cell><cell>-7.9 -7.8 -7.7 -7.6</cell><cell></cell><cell>per-word log likelihood</cell><cell>-7.72 -7.7 -7.68 -7.66 -7.64 -7.62</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>-8</cell><cell></cell><cell></cell><cell>-7.74</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-7.76</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-7.78</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>150 -7.8</cell><cell>200</cell><cell>250</cell><cell>300</cell><cell>350</cell><cell>400</cell><cell>450</cell><cell>500</cell><cell>550</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell># topics used</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell># topics used</cell></row><row><cell>(g)</cell><cell></cell><cell></cell><cell></cell><cell>(h)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(i)</cell></row><row><cell>Figure 2: Original topic</cell><cell>40,000</cell><cell cols="2">80,000</cell><cell>120,000</cell><cell>160,000</cell><cell cols="3">200,000</cell><cell>240,000</cell></row><row><cell></cell><cell>patterns pattern</cell><cell cols="2">patterns pattern</cell><cell>patterns pattern</cell><cell>patterns pattern</cell><cell cols="3">patterns pattern</cell><cell>patterns pattern</cell></row><row><cell></cell><cell>cortex neurons</cell><cell cols="2">cortex neurons</cell><cell>cortex neurons</cell><cell>cortex neurons</cell><cell cols="3">cortex neurons</cell><cell>cortex responses</cell></row><row><cell></cell><cell>neuronal</cell><cell cols="2">neuronal</cell><cell>responses</cell><cell>responses</cell><cell cols="3">responses</cell><cell>types</cell></row><row><cell>patterns</cell><cell cols="3">responses responses</cell><cell>neuronal</cell><cell>type</cell><cell></cell><cell>type</cell><cell></cell><cell>type</cell></row><row><cell>pattern</cell><cell>single</cell><cell cols="2">single</cell><cell>single</cell><cell>behavioral</cell><cell cols="3">behavioral</cell><cell>behavioral</cell></row><row><cell>cortex</cell><cell>inputs</cell><cell cols="2">temporal</cell><cell>type</cell><cell>types</cell><cell></cell><cell cols="2">types</cell><cell>form</cell></row><row><cell>neurons</cell><cell>temporal</cell><cell cols="2">inputs</cell><cell>number</cell><cell>neuronal</cell><cell></cell><cell cols="2">form</cell><cell>neurons</cell></row><row><cell>neuronal</cell><cell>activation</cell><cell cols="2">type</cell><cell>temporal</cell><cell>single</cell><cell cols="3">neuronal</cell><cell>areas</cell></row><row><cell>single</cell><cell>patterns</cell><cell cols="2">neuronal</cell><cell>neuronal</cell><cell>neuronal</cell><cell cols="3">neuronal</cell><cell>neuronal</cell></row><row><cell>responses</cell><cell>neuronal</cell><cell cols="2">patterns</cell><cell>neurons</cell><cell>dendritic</cell><cell cols="3">dendritic</cell><cell>dendritic</cell></row><row><cell>inputs</cell><cell>pattern</cell><cell cols="2">pattern</cell><cell>activation</cell><cell>peak</cell><cell></cell><cell>fire</cell><cell></cell><cell>postsynaptic</cell></row><row><cell>type</cell><cell>neurons</cell><cell cols="2">neurons</cell><cell>cortex</cell><cell>activation</cell><cell></cell><cell>peak</cell><cell></cell><cell>fire</cell></row><row><cell>activation</cell><cell>cortex</cell><cell cols="2">cortex</cell><cell>dendrite</cell><cell>cortex</cell><cell cols="3">activation</cell><cell>cortex</cell></row><row><cell></cell><cell>inputs</cell><cell cols="2">activation</cell><cell>preferred</cell><cell>pyramidal</cell><cell></cell><cell cols="2">msec</cell><cell>activation</cell></row><row><cell></cell><cell>activation</cell><cell cols="2">dendrite</cell><cell>patterns</cell><cell>msec</cell><cell cols="3">pyramidal</cell><cell>peak</cell></row><row><cell></cell><cell>type</cell><cell cols="2">inputs</cell><cell>peak</cell><cell>fire</cell><cell></cell><cell cols="2">cortex</cell><cell>msec</cell></row><row><cell></cell><cell>preferred</cell><cell cols="2">peak</cell><cell>pyramidal</cell><cell>dendrites</cell><cell cols="4">postsynaptic</cell><cell>pyramidal</cell></row><row><cell></cell><cell>peak</cell><cell cols="2">preferred</cell><cell>inputs</cell><cell>inputs</cell><cell></cell><cell cols="2">inputs</cell><cell>inputs</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We expect β to have small posterior variance in large datasets, and using a point estimate β * simplifies variational derivations for our direct assignment formulation. As empirically explored for the HDP-PCFG<ref type="bibr" target="#b14">[15]</ref>, updates to the global topic weights have much less predictive impact than improvements to topic distributions.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Technically, we replace topic k with topic k ′ and add k ′′ as a new topic. In practice, we found that the order of topics in the global stick-breaking distribution had little effect on overall algorithm performance.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>For CGS we use the code available at http://www.gatsby.ucl.ac.uk/∼ywteh/research/npbayes/npbayes-r21.tgz, and for oHDP-CRF we use the code at http://www.cs.princeton.edu/∼chongw/software/onlinehdp.tar.gz.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p><ref type="bibr" target="#b3">4</ref> http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2008T19</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments We thank Dae Il Kim for his assistance with the experimental results.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Beal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hierarchical Dirichlet processes</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Variational methods for Dirichlet process mixtures</title>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian Analysis</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="121" to="144" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Collapsed variational inference for HDP</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kurihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Online variational inference for the hierarchical Dirichlet process</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Online learning for latent Dirichlet allocation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A split-merge Markov chain Monte Carlo procedure for the Dirichlet process mixture model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="158" to="182" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Sequentially-allocated merge-split sampler for conjugate and nonconjugate Dirichlet process mixture models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Dahl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>Texas A&amp;M University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A split-merge MCMC algorithm for the hierarchical Dirichlet process</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-01">January 2012</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SMEM algorithm for mixture models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ueda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bayesian K-means as a &apos;Maximization-Expectation&apos; algorithm</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kurihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM conference on data mining SDM06</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bayesian model search for mixture models based on optimizing variational bounds</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ueda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Variational inference for Bayesian mixtures of factor analysers</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Beal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Introduction to variational methods for graphical models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The infinite PCFG using hierarchical Dirichlet processes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Accelerated variational Dirichlet process mixtures</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kurihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vlassis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Variational inference for the nested Chinese restaurant process</title>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Finding scientific topics</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PNAS</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">On smoothing and inference for topic models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>UAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Accounting for word burstiness in topic models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Graphical models, exponential families, and variational inference</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="305" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
