<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Directed Statistical Warming through Time Traveling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nikos</forename><surname>Nikoleris</surname></persName>
							<email>nikos.nikoleris@arm.com</email>
						</author>
						<author>
							<persName><forename type="first">Erik</forename><surname>Hagersten</surname></persName>
							<email>erik.hagersten@it.uu.se</email>
						</author>
						<author>
							<persName><forename type="first">Trevor</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
							<email>tcarlson@comp.nus.edu.sg</email>
						</author>
						<author>
							<persName><forename type="first">Lieven</forename><surname>Eeckhout</surname></persName>
							<email>lieven.eeckhout@ugent.be</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Arm Research Cambridge</orgName>
								<address>
									<settlement>Lieven Eeckhout</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Ghent University</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Uppsala University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<address>
									<addrLine>MICRO-52, October 12-16</addrLine>
									<postCode>2019</postCode>
									<settlement>Columbus</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Directed Statistical Warming through Time Traveling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3352460.3358264</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>performance analysis</term>
					<term>architectural simulation</term>
					<term>sampled simulation</term>
					<term>statistical cache modeling</term>
					<term>cache warming</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Improving the speed of computer architecture evaluation is of paramount importance to shorten the time-to-market when developing new platforms. Sampling is a widely used methodology to speed up workload analysis and performance evaluation by extrapolating from a set of representative detailed regions. Installing an accurate cache state for each detailed region is critical to achieving high accuracy. Prior work requires either huge amounts of storage (checkpoint-based warming), an excessive number of memory accesses to warm up the cache (functional warming), or the collection of a large number of reuse distances (randomized statistical warming) to accurately predict cache warm-up effects.</p><p>This work proposes DeLorean, a novel statistical warming and sampling methodology that builds upon two key contributions: directed statistical warming and time traveling. Instead of collecting a large number of randomly selected reuse distances as in randomized statistical warming, directed statistical warming collects a select number of key reuse distances, i.e., the most recent reuse distance for each unique memory location referenced in the detailed region. Time traveling leverages virtualized fast-forwarding to quickly 'look into the future' -to determine the key cachelines -and then 'go back in time' -to collect the reuse distances for those key cachelines at near-native hardware speed through virtualized directed profiling.</p><p>Directed statistical warming reduces the number of warm-up references by 30× compared to randomized statistical warming. Time traveling translates this reduction into a 5.7× simulation speedup. In addition to improving simulation speed, DeLorean reduces the prediction error from around 9% to around 3% on average. We further demonstrate how to amortize warm-up cost across multiple parallel simulations in design space exploration studies. Implementing DeLorean in gem5 enables detailed cycle-accurate simulation at a speed of 126 MIPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Computing methodologies → Modeling and simulation; • Hardware → Analysis and design of emerging devices and systems; • Computer systems organization → Serial architectures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Computer architects use a variety of tools and methodologies to analyze workloads and evaluate new design enhancements. Instrumentation and profiling using tools such as Pin <ref type="bibr" target="#b17">[18]</ref> and Valgrind <ref type="bibr" target="#b20">[21]</ref> are useful to analyze a workload's characteristics, e.g., generate working set curves to visualize cache miss rate as a function of cache size. Cycle-accurate architecture simulation, e.g., gem5 <ref type="bibr" target="#b7">[8]</ref>, models cycle-by-cycle execution behavior to predict a workload's performance on a particular microprocessor configuration. Although these tools and methodologies have proven their merit, they all suffer from limited speed. Analyzing even a few minutes of real hardware execution leads to days, if not weeks, of experimentation time.</p><p>Improving the speed of computer architecture evaluation is critically important. Shortening the design cycle provides a competitive time-to-market advantage in the computer industry. Moreover, a fast evaluation methodology allows for a more thorough exploration of the design space with more workloads, which leads to an improved design. A commonly used technique to speed up architecture evaluation is to sample the workload, i.e., evaluate (a) small region(s) of the execution in detail and then extrapolate to the entire execution <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b33">34]</ref>. A major challenge in sampled evaluation however is to quickly and accurately warm up the microarchitecture state for each detailed region, which is particularly challenging for the largest structures in the processor, i.e., caches. Although sampling is not a new methodology, fast and accurate cache warming is still an unsolved problem, especially in light of emerging very large DRAM caches.</p><p>Although important advances have been made in the past, there are still major limitations. Checkpointed Warming (CW) takes a checkpoint of the microarchitecture state prior to each region <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>. Unfortunately, checkpoints require huge amounts of disk space, and are not reusable across software changes (i.e., compiler updates, changing compiler options, dynamically generated code through Just-in-Time optimization, etc.) nor hardware changes -although there exist solutions to make checkpoints transferable across cache structures <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>. Functional Warming (FW) on the other hand does not incur any storage overhead and is transferable across both hardware and software changes <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34]</ref>. Functional warming  warms up the microarchitecture state by simulating the microarchitecture structures in the warm-up interval prior to each detailed region, without doing any actual performance measurements, see Figure <ref type="figure" target="#fig_1">1</ref>(a). Unfortunately, functional warming is slow because it simulates the cache for every memory access within the warm-up interval prior to the detailed region <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref>. More recently, Randomized Statistical Warming (RSW) <ref type="bibr" target="#b22">[23]</ref> takes a random set of memory accesses in the warm-up interval for which it computes reuse distances (i.e., number of memory references between two references to the same memory location), see Figure <ref type="figure" target="#fig_1">1</ref>(b). Statistical models <ref type="bibr" target="#b10">[11]</ref> transform reuse distances into stack distances (i.e., number of unique memory references between two references to the same memory location) to then predict miss rates for a range of cache configurations. RSW is substantially faster than FW because fewer memory references need to be profiled <ref type="bibr" target="#b22">[23]</ref>. Nevertheless, RSW requires a large number of randomly selected reuse distances, most of which are useless and only a few of which are critical to accurately predict a detailed region's cache behavior. This paper proposes DeLorean, which builds upon two major contributions: (i) directed statistical warming and (ii) time traveling. Directed Statistical Warming (DSW) makes the observation that to obtain an accurate picture of the cache state prior to a detailed region, we only need a few select reuse distances, in contrast to RSW. More specifically, we only need the most recent reuse for each unique memory location referenced in the detailed region, i.e., we do not need to collect reuse distances that fall entirely within the warm-up interval, see Figure <ref type="figure" target="#fig_1">1(c)</ref>. DSW dramatically reduces the number of reuse distances that need to be collected, thereby reducing total work spent during warm-up compared to RSW.</p><p>DSW is not a panacea though: to direct reuse distance collection, we need to know which memory locations to compute the reuse distance for, i.e., these are the ones referenced in the detailed region. The problem now is that we only get to know these key memory locations once we have evaluated the detailed region, which appears after the warm-up interval. This is where Time Traveling (TT) comes in. TT first quickly advances to the next detailed region to collect socalled key cachelines, which are all the unique cachelines referenced in the region. TT then goes back in time to collect the last reuse distance for each key cacheline as needed for DSW. This is done in an iterative way, using multiple rollbacks if needed, to obtain high simulation speed while collecting reuse distances for all key cachelines. TT is implemented through a combination of virtualized execution (to quickly fast-forward to the next detailed region to determine the key cachelines) and virtualized directed profiling (to sample the key reuse distances at near-native hardware speed). The name 'DeLorean' is chosen after the time-travel vehicle in the 'Back to the Future' feature films to represent going back and forth in time. DeLorean is implemented in the gem5 detailed cyclelevel processor simulator using the Linux Kernel Virtual Machine (KVM) to fast-forward between detailed regions and sample reuse distances at near-native hardware speed.</p><p>DeLorean is widely applicable to speed up architecture evaluation. We experimentally demonstrate the speed and accuracy of De-Lorean for collecting working set curves and speeding up sampled simulation, using the SPEC CPU benchmarks and the gem5 simulation infrastructure. DSW reduces the number of warm-up samples (number of collected reuse distances) by 30× and 100,000× compared to RSW and FW, respectively. TT translates this reduction in a simulation speedup of 5.7× compared to RSW and 96× compared to FW. Moreover, because DSW builds upon a microarchitectureindependent characteristic, namely reuse distance, warming overhead can be amortized across multiple parallel simulations during design space exploration studies, further increasing simulation speed advances over traditional simulation-based approaches. In addition to substantially improving simulation speed, DeLorean also improves accuracy: prediction error is reduced from around 9% for RSW to around 3% across different cache sizes. Ultimately, DeLorean enables detailed cycle-accurate gem5 simulations at a speed of 126 MIPS. We make the source code and scripts of our simulation framework publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>Before describing DeLorean in detail, we first provide critical background information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sampling</head><p>Sampling is a widely used technique to speed up workload analysis and computer architecture performance evaluation by considering a select number of representative detailed regions that are evaluated in detail to then extrapolate from <ref type="bibr" target="#b33">[34]</ref>. The key challenge in sampling is to get (i) the correct architecture state and (ii) an accurate microarchitecture state at the beginning of each detailed region. Common techniques to obtain the correct architecture state are functional fast-forwarding, checkpointing and virtualized fastforwarding. Functional fast-forwarding <ref type="bibr" target="#b33">[34]</ref> leverages functional simulation to get to the next representative region, which is slow. Checkpointing <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref> takes a snapshot of the architecture state for each region, which is fast but does not allow for changes to the software. Virtualized Fast-Forwarding (VFF) <ref type="bibr" target="#b25">[26]</ref> leverages hardware virtualization to quickly get to the next region, while enabling software changes. Time Traveling (TT), as proposed in DeLorean, builds upon virtualized fast-forwarding.</p><p>Obtaining the microarchitecture state at the beginning of each region is an accuracy/speed/overhead trade-off. As mentioned in the introduction, checkpointed warming (CW) <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref> is fast, requires huge storage overhead, and does not allow for software changes. Functional warming (FW) <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34]</ref> does not incur any storage overhead, allows for software changes, but is slow. Randomized statistical warming (RSW) <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> shares all the benefits of FW but is faster because many fewer memory references need to be analyzed in the warm-up interval prior to each detailed region. In this work, we make the observation that collecting that many warm-up references under RSW is wasteful, which provides an opportunity to reduce warm-up by an order of magnitude through directed statistical warming (DSW).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Statistical Cache Modeling</head><p>Statistical warming -both RSW and DSW -builds upon statistical cache modeling. By leveraging a workload characteristic that is independent of the underlying microarchitecture and cache hierarchy, statistical cache models estimate cache miss rates for any size caches from a single workload profile. The workload characteristic that underpins statistical cache modeling is stack distance, which is defined as the number of unique references (cachelines) between two accesses to the same cacheline <ref type="bibr" target="#b19">[20]</ref>. Stack distance allows for accurate modeling of fully-associative, LRU caches <ref type="bibr" target="#b14">[15]</ref>: a stack distance larger than the size of the cache leads to a miss; if not, it is a hit.</p><p>Unfortunately, obtaining the exact stack distance distribution is a costly operation because one needs to inspect every memory access between the two consecutive accesses to the same cacheline. Eklov and Hagersten <ref type="bibr" target="#b10">[11]</ref> provide a solution by showing that stack distance can be accurately estimated using its corresponding reuse distance, which is defined as the number of memory accesses (not necessarily unique accesses) between two accesses to the same cacheline. Computing reuse distance is much faster than computing stack distance because one only needs to count the number of memory accesses between two accesses to the same memory location, and does not need to process their addresses to determine the unique accesses. Moreover, prior work has shown that the reuse distance distribution can be accurately approximated by tracking a subset of randomly selected reuse distances and memory locations <ref type="bibr" target="#b4">[5]</ref>. Finally, statistical cache modeling has been generalized and demonstrated for various cache replacement policies, as well as for multiprogrammed and multi-threaded workloads, as we will elaborate on in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Randomized Statistical Warming</head><p>Recent work proposed randomized statistical warming (RSW) to tackle the warm-up problem in sampled evaluation <ref type="bibr" target="#b22">[23]</ref>. Instead of warming up the cache by effectively populating the cache with simulated accesses and evictions as in functional warming, randomized statistical warming predicts whether a memory access in the detailed region would have been a hit or a miss if the cache would have been perfectly warmed up prior to the detailed region. To this end, RSW computes an (approximate) reuse distance distribution during the warm-up interval, which then serves as input to a statistical cache model to predict hits and misses.</p><p>RSW tracks randomly selected memory locations and computes their reuses during the warm-up interval prior to a detailed region. Prior work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23]</ref> uses watchpoints to do so during native execution: hardware performance counters are used to count the number of memory accesses between a (randomly selected) reuse. A reuse is detected by setting a watchpoint using the operating system's page protection mechanism; a memory location touched by the workload is randomly selected, and a watchpoint is set on that same address to compute its reuse distance. Execution between watchpoints runs at native speed, and watchpoints stop the execution when there is an access to the protected page. Note that a stop on a watchpoint does not necessarily imply a reuse. Any access to the protected page incurs a watchpoint stop, which could be a false positive. In case a reuse is detected (i.e., true positive), its distance is computed and the watchpoint is removed.</p><p>Computing an (approximate) reuse distance distribution is much faster than simulating (and effectively warming up) a cache. Prior work reports that RSW improves detailed simulation speed by 17× compared to functional warming <ref type="bibr" target="#b22">[23]</ref>. In this work, though, we find there is substantial room for improvement. To accurately predict hits and misses for specific memory accesses in the detailed region, RSW needs to collect a large number of reuse distances, hoping that a sufficient number of reuse distances will have been collected for all the load PCs that occur in the detailed region. The statistical model that underpins RSW uses reuse distance distributions per load PC and thus needs a sufficiently large number of reuse distances per PC for an accurate prediction. Because the detailed region is relatively small compared to the much larger warm-up interval, the likelihood of sampling a reuse distance in the warm-up interval for a particular load PC in the detailed region is not that high. Hence, RSW needs to collect a large number of reuse distances, many of which turn out to be useless because they are collected for load PCs that do not occur in the detailed region. This inefficiency incurs a non-trivial warm-up cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DELOREAN</head><p>To lower the warm-up cost compared to RSW, a directed approach is needed. Ideally, we would like to reduce the number of reuse distances that we need to track in the warm-up interval (to achieve high speed), while, at the same time, capturing the exact reuse information for every memory access in the detailed region (to maintain high accuracy). This is effectively what DeLorean accomplishes. DeLorean is a novel statistical warming and sampling methodology that builds on two major contributions: (i) directed statistical sampling and (ii) time traveling. We now discuss these in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Directed Statistical Warming</head><p>Directed statistical warming (DSW) reduces the overhead of statistical warming by directing reuse distance collection to a set of so-called key cachelines. The key reuse distances and their vicinity distributions are then used to predict warm-up effects using statistical cache models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Warmup Interval D</head><p>Figure <ref type="figure">2:</ref> A key cacheline is accessed in the detailed region D.</p><p>Its key reuse distance is the last reuse in the warmup interval (shown as a solid arc). The reuse distances in the vicinity are shown as dashed arcs.</p><p>3.1.1 Key Reuses and their Vicinity. DSW first identifies the small set of cachelines (key cachelines) that are accessed during the detailed region, and for each key cacheline, DSW records the reuse distance since it was last accessed before the detailed region. More specifically, DSW determines the first access to each unique cacheline in the detailed region and then measures its 'backward' reuse distance, i.e., the nearest previous access to the same cacheline in the warm-up interval. We refer to these reuses as key reuse distances. This is illustrated in Figure <ref type="figure">2</ref>.</p><p>In addition to the key reuse distances, we also compute the vicinity reuse distance distribution, or the distribution of reuse distances in the vicinity of the key reuse distances, see Figure <ref type="figure">2</ref>. The vicinity is defined as the interval surrounding the key reuse distance -any reuse that completely falls within or crosses the key reuse distance is part of the vicinity. The vicinity reuse distribution is approximated by taking randomly selected reuse distance samples. Note one key difference with how RSW collects reuse distance samples. As mentioned in the previous section, RSW needs to collect a large number of reuse distances to cover a sufficient number of reuse distances for the load PCs in the detailed region. The vicinity reuse distribution on the other hand needs an order of magnitude fewer reuse distance samples because its sole purpose is to estimate the average behavior of the accesses close to the key reuse distances, and not to estimate the per-PC reuse distances of the key cachelines. The vicinity distribution is used to predict whether the key reuse distances will lead to a hit or a miss in the detailed region, as we will describe next. The order of magnitude reduction in reuse distance samples needed under DSW leads to a substantial speedup during warmup.</p><p>DSW has two key advantages over RSW: (i) the exact reuse distances for all memory accesses at the detailed region are known (high accuracy), and (ii) much fewer reuse distances need to be collected by focusing on the key reuse distances and their vicinity (high speed).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Statistical</head><p>Warming. We use the key reuse distances and their vicinity distributions to statistically 'warm' the caches prior to a detailed region. DSW does not actually warm up the cache by emulating the cache's operation through accesses and evictions. Instead, DSW predicts whether a specific memory access in the detailed region is a hit or a miss based on the distribution of the key reuse distances and their vicinities.</p><p>Statistical cache warming builds upon the key concept of the warming miss, which is a request in the detailed region that misses due to lack of warming. While cold, capacity and conflict misses correspond to real workload behavior, warming misses are a sampling artifact. The insight behind statistical warming is to determine the The lukewarm cache determines accesses with short reuses as cache and MSHR hits. Then, the limitedassociativity model determines conflict misses and the statistical cache model determines capacity misses. All other accesses that appear to be misses are due to insufficient warming and are treated as hits.</p><p>warming misses and handle them as cache hits. We now describe how we determine warming misses, see also Figure <ref type="figure" target="#fig_2">3</ref>.</p><p>Lukewarm Cache and MSHR Hits: It is common in sampled simulation to warm up the microarchitecture state (processor pipeline, predictors, prefetchers, caches) through a detailed warm-up using a small number of instructions (e.g., 30,000) prior to the detailed region <ref type="bibr" target="#b33">[34]</ref>. With this small amount of warming, only a small part of the cache state is warm, which we refer to as the lukewarm cache. A hit in the lukewarm cache in the detailed region would definitely have been a hit in the cache if it were perfectly warmed up. Our experiments show that for the SPEC CPU2006 benchmarks, the hit rate for a detailed region of 10,000 instructions and a 64 KiB lukewarm D-cache ranges between 27.5% and 100% with an average of 93.5%.</p><p>A number of memory accesses miss in the lukewarm cache when there is already an outstanding miss for the same cacheline. These accesses are typically handled as Miss Status Holding Register (MSHR) hits <ref type="bibr" target="#b2">[3]</ref>. DSW models MSHR hits as a cache hit (in case of cache simulation) or a delayed hit (in case of processor simulation). Our experiments show that between 46.1% and 100% of the requests (96.7% on average) are hits or delayed hits in a lukewarm 64 KiB D-cache with 8 MSHRs.</p><p>Conflict Misses: If a referenced set in the lukewarm cache is full in the detailed region, the access certainly is a conflict miss. Hence, the access is modeled as a miss.</p><p>For some (outlier) benchmarks, we note that some load PCs exhibit a dominant large stride, which results in uneven usage of the cache sets. For example, a 512-byte stride will only touch upon one eighth of the cache sets assuming an 64-byte cacheline. In other words, dominant large strides limit the effective usage of the cache, which ultimately leads to conflict misses. We leverage the previously proposed limited-associativity model <ref type="bibr" target="#b22">[23]</ref> to determine such conflict misses.  Capacity Misses: DSW uses the key reuse distances and their vicinity distributions to determine capacity misses. We first convert the key reuse distances and their vicinity reuse distance distributions into stack distances and stack distance distributions, respectively, using the well-established statistical model <ref type="bibr" target="#b10">[11]</ref> as previously described in Section 2.2. If the stack distance of the memory access is larger than the total number of cachelines in the cache, the memory access is classified and modeled as a capacity miss.</p><p>Warming Misses: All remaining memory accesses are warming misses. They miss in the lukewarm cache and MSHRs, and are not predicted to be conflict nor capacity misses. Instead, they are an artifact for insufficient cache warming. DSW models warming misses as cache hits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Time Traveling</head><p>DSW reduces the number of reuse distances to collect by an order of magnitude compared to RSW, as we will quantify in the evaluation section. One key implicit assumption underlying DSW though is that we need future knowledge, i.e., we need to know the key cachelines in the detailed region to know which reuse distances to collect in the warm-up interval. This is a non-trivial problem to solve in a single evaluation run while maintaining (very) high evaluation speed.</p><p>Time traveling (TT) solves exactly this problem by using multiple passes, as illustrated in Figure <ref type="figure" target="#fig_3">4</ref>. We utilize a Scout pass, multiple Explorer passes, and finally an Analyst pass. Each of the passes is a separate instance (process) of the same evaluation. TT performs all passes for each detailed region and it does so in a pipelined fashion, i.e., it first performs Scout, then Explorer-1 through Explorer-N , and finally Analyst. However, the evaluation of subsequent detailed regions is parallelized over time. As soon as a pass finishes its current detailed region, it moves on to the next detailed region. For example, when Scout is done with detailed region m, it moves on to region m + 1, while Explorer-1 processes region m, etc. This way we can pipeline the evaluation process as long as we have enough cores to run the different passes concurrently. OS pipes are used to synchronize and communicate between passes. We now describe the different passes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scout:</head><p>The Scout identifies the key cachelines in the detailed region. It uses virtualized fast-forwarding (VFF) at near-native speed to advance the execution to the next detailed region, at which point it switches to functional simulation to record the set of key cachelines. These are the unique cachelines accessed within the detailed region. Interestingly, the number of key cachelines is rather small: for SPEC CPU2006 and a detailed region of 10,000 instructions, we find that the number of key cachelines varies between 1 and 2,907, with an average of 151.</p><p>Explorers: The Explorers collect the key reuse distances and their vicinity distributions. For each key cacheline recorded by the Scout, the Explorers determine its last access prior to the detailed region. To do this, the Explorers set watchpoints on the key cachelinesa technique which we call directed profiling (DP). As the number of key cachelines for a detailed region is relatively small, it may appear that the task of measuring reuse distances is fairly trivial. However, we need to find the last access to the cacheline before the detailed region. This implies that we keep DP active for the entire warm-up interval. A naive implementation that effectively keeps DP active for the entire warm-up interval is too slow.</p><p>Instead, we use multiple Explorers, as shown in Figure <ref type="figure" target="#fig_3">4</ref>. Explorer-1 fast-forwards using VFF and switches to DP 5 M instructions before the detailed region. As soon as it reaches the end of the warm-up interval, it determines which key reuse distances have been computed. The remaining key cachelines have a reuse distance that is larger than 5 M instructions -these key cachelines (if any) are then passed on to Explorer-2.</p><p>Explorer-2 switches to DP 50 M instructions before the detailed region. Its task is to find the reuses for the key cachelines that were outside the reach of Explorer-1. The subset of undiscovered reuses is not only significantly smaller than the original set of key cachelines, it also tends to contain cachelines with lower temporal locality. As a result, watchpoints for these key cachelines do not trigger as often, hence overhead is lower. Key cachelines that had reuses outside of the reach of Explorer-2 (if any) are fed to Explorer-3 which triggers DP 100 M instructions before the detailed region. This iterative process is stopped as soon as the whole set of key cachelines has been covered. We find that a small number of Explorers is typically enough -our implementation considers at most four Explorersand for many benchmarks and detailed regions, we find that only a few Explorers are needed, as we will quantify in the evaluation section.</p><p>The vicinity reuse distances are recorded as previously described in Section 2.3, i.e., a sparse set of randomly chosen memory accesses are selected and their next (forward) reuse distance is recorded using watchpoints. Once a reuse within the vicinity has been recorded, the corresponding watchpoint is removed and its reuse distance is recorded.</p><p>Analyst: Finally, Explorer-N feeds the obtained key reuse distances and vicinity distributions to the Analyst. This final evaluation pass uses DSW to predict the impact of warming on the detailed region.</p><p>The evaluation in the detailed region could be functional cache simulation (e.g., to determine the number of hits and misses) or detailed cycle-accurate processor simulation (e.g., to determine an application's IPC on a particular microarchitecture configuration).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation Details</head><p>Time Traveling enables DeLorean to collect all the key reuse distances and the vicinity distributions while limiting the number of watchpoints that need to be triggered. By doing so, DeLorean maintains high accuracy while improving evaluation speed by an order of magnitude compared to the state-of-the-art.</p><p>gem5 and KVM: We implement DeLorean using KVM <ref type="bibr" target="#b15">[16]</ref> and the gem5 cycle-accurate full-system simulation infrastructure <ref type="bibr" target="#b7">[8]</ref>. De-Lorean switches and exchanges full-system state between KVM and gem5 at the boundaries of each detailed region. For example, at the end of a warm-up interval, KVM transfers the entire system state to gem5 after which gem5 takes over to conduct cycle-accurate simulation of the detailed region; similarly, the state is transferred back from gem5 to KVM at the end the detailed region. As DeLorean uses full-system simulation, to align the location of the detailed regions across the different passes, we count the number of dynamically executed user-space instructions <ref type="bibr" target="#b32">[33]</ref>.</p><p>Explorers: We develop multiple instances of gem5: Scout, Explorers and Analyst. We use 4 Explorers to profile 5 M, 50 M, 100 M and 1 B instructions before each detailed region. The actual implementation of directed profiling (DP) varies across the different Explorers. Explorer-1 profiles a relatively short interval of 5 M instructions for the full set of key cachelines. We use functional simulation to implement DP in Explorer-1 using gem5's so-called 'atomic' CPU model. For the other Explorers, we use virtualized directed profiling (VDP) which sets watchpoints by leveraging the operating system's page protection mechanism, as previously explained in Section 2.3. This is implemented in KVM <ref type="bibr" target="#b15">[16]</ref>. VDP advances between watchpoints at near-native speed. A watchpoint stops the native execution when there is an access to a protected page, at which point the reuse distance is computed if the watchpoint is a true positive (i.e., a reuse). To minimize the overhead from false positives, and to achieve overall high evaluation speed with TT, we limit VDP to Explorer-2 through 4, and use functional simulation for Explorer-1.</p><p>In addition to collecting key reuse distances, all Explorers also collect 'vicinity' reuse distances. These reuses are randomly collected with a sample rate of 1 over 100 k memory instructions -we quantify the impact of the sample rate on accuracy and evaluation speed in the results section. The same sample rate is used by all Explorers.</p><p>RSW versus DSW: There is a subtle but important difference when implementing RSW and DSW using watchpoints and VDP. As previously discussed in Section 2.3, RSW sets watchpoints at random memory locations. Once a reuse is detected, the watchpoint is removed. RSW however needs to collect many reuses per load PC to accurately predict warming misses for the particular load PCs in the detailed regions.</p><p>In DSW on the other hand, watchpoints for the key reuse distances are not set at random memory locations <ref type="foot" target="#foot_1">1</ref> . Instead, watchpoints are set at specific memory locations, namely the key cachelines. For each key cacheline, DSW needs to collect the nearest previous reuse in the warm-up interval, i.e., the key reuse distance. This implies that the watchpoints need to be on during the entire warm-up interval to compute the last reuse for each key cacheline. The overhead for collecting key reuse distances is very high, up to the point that it negates the benefit from having to collect fewer reuse distances under DSW compared to RSW. The reason for the high overhead is that DSW detects many reuses for a single key cacheline out of which it only needs the last one. Hence, although DSW needs to collect few key reuse distances, the overhead for collecting them in a naive implementation is high. This is why TT is needed to translate the small number of key reuse distances under DSW into a substantially higher evaluation speed compared to RSW. Employing a multi-pass approach allows for progressively reducing the number of key cachelines to track in the different Explorers: Explorer-1 sets watchpoints for all key cachelines for a short interval; Explorer-2 then sets watchpoints for a smaller number of cachelines (i.e., the remaining key cachelines after Explorer-1) for a longer interval; follow-on Explorers set even fewer watchpoints for progressively longer intervals. By doing so, DeLorean limits the number of watchpoints that need to be set while being able to collect all key reuse distances. This translates into a substantial improvement in evaluation speed compared to RSW.</p><p>Design Space Exploration: It is interesting to note that the overhead due to warm-up can be amortized across multiple parallel simulations, which is particularly appealing when performing design space exploration studies, as we will demonstrate and quantify in the evaluation section. In particular, DeLorean allows for running multiple detailed evaluations concurrently and warm them all up from the same warm-up. In TT terminology, this means there is a single Scout and a single set of Explorers that feed a number of Analysts, with each Analyst simulating a different cache configuration or processor architecture configuration. As soon as Explorer-N has reached the beginning of a detailed region, control is transferred to the different Analysts to simulate the different cache and processor configurations in parallel. This is possible because the reuse distance which underpins DSW is independent of the underlying cache hierarchy. Hence, we need to collect the reuse distances in the warm-up interval only once, after which we can predict warming misses in the detailed region for a range of cache and processor configurations. Collecting the reuse distance warm-up information is thus amortized across multiple parallel detailed evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GENERAL APPLICABILITY</head><p>The core contributions in DeLorean -DSW and TT -build upon statistical cache modeling which has been demonstrated for a range of architectures and configurations, including (i) cache replacement policies, (ii) multiprogrammed workloads, and (iii) multi-threaded execution. We hence believe that DeLorean is more generally applicable beyond the concrete implementation in this paper which considers LRU cache replacement and single-threaded execution. (In fact, the evaluation section includes one example to illustrate DeLorean's wider applicability.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Cache Replacement</head><p>The very first work demonstrating how sparse (approximate) reuse distance distributions can be used to statistically model caches considered caches with random replacement <ref type="bibr" target="#b5">[6]</ref>. Follow-on work demonstrates similar accuracy for LRU caches <ref type="bibr" target="#b10">[11]</ref>. More recent work extends statistical cache modeling to cover other replacement algorithms, including pseudo-LRU and NMRU <ref type="bibr" target="#b24">[25]</ref>. Sen and Wood <ref type="bibr" target="#b26">[27]</ref> suggest that stack distance -which can be estimated using the reuse distance distribution -can be used to model other replacement algorithms as well. Beckmann and Sanchez <ref type="bibr" target="#b1">[2]</ref> propose probabilistic methods to model age-based replacement policies, such as RRIP. This body of prior work makes us confident that techniques to model other replacement algorithms are already available or can be designed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multi-Programming</head><p>StatCC <ref type="bibr" target="#b9">[10]</ref> is a method in which sparse reuse information, collected separately for each application, can be used to model how several independent applications in a multi-programmed workload interact when sharing a cache. StatCC recursively uses a simplistic CPU performance model to determine the CPI for each application in the workload mix. The per-application's reuse information is scaled according to its CPI, which results in a new CPI for each application to be used in the next iterative CPI estimation. After a few iterations already, a stable solution is found. Combining StatCC with DeLorean will likely improve the accuracy of StatCC by replacing the simplistic CPI estimation with DeLorean's detailed simulation to estimate the per-application CPI for the next iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Multi-Threading</head><p>StatCache-MP <ref type="bibr" target="#b6">[7]</ref> shows how sparse reuse information, collected from a single execution of multiple threads, can be used to model the execution of multi-threaded applications across a wide selection of different cache sizes and cache topologies with MSI coherence. StatCache-MP models both constructive and destructive cache sharing in a way that fits DeLorean's approach very well. If a key access by thread A during detailed simulation is known to be preceded by a write to the same memory location by another thread B, and threads A and B do not share the cache in the modeled topology, detailed simulation should model a coherence miss for thread A. However, if both threads share the modeled cache, a constructive sharing cache hit should be modeled -provided that the reuse distance between the two accesses is short enough, otherwise a capacity cache miss should be modeled. We believe that this approach can be further extended to model other coherence states (e.g., O and E states) but this is left for future work. through emulation of unimplemented instructions in KVM. Workloads that offload (a) part(s) of their execution to an accelerator can also leverage DeLorean. One can fast-forward the execution and then, for the portion of the workload that is executed on the accelerator, switch to detailed simulation with statistical modeling of the on-chip caches using DeLorean to reduce simulation time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">ISA Extensions and Accelerators</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL SETUP</head><p>We perform full-system gem5 simulations of a 64-bit x86 superscalar 8-wide out-of-order processor with split 2-way 64 KiB L1 instruction and data caches and a unified 8-way LLC with sizes ranging from 1 MiB to 512 MiB. A summary of the important simulation parameters can be found in Table <ref type="table" target="#tab_1">1</ref>. We use gem5 revision 2c9f7ebca: we use the default superscalar out-of-order model as our timing model in the detailed regions with the 'classic' memory system. We use KVM hardware virtualization on the simulation host machine to fast-forward between detailed regions at near-native hardware speed. We run Ubuntu 12.04.5 LTS running Linux 3.2.44 in full-system simulation.</p><p>We consider the SPEC CPU2006 benchmarks with reference inputs in the evaluation<ref type="foot" target="#foot_2">2</ref> . All benchmarks were compiled with GCC 4.6.3 and optimization flag -O2. All simulations are started from the same checkpoint of a booted system that has executed 100 B instructions. Evaluation speed is measured on a dual-socket Intel Xeon E5520 with 4 cores per CPU and 2 threads per core, running at 2.26 GHz.</p><p>Due to the high overhead of the reference experiments, we use 10 detailed regions spread uniformly across 10 B instructions (1 B instructions apart). For each detailed region, we use gem5's out-oforder CPU model and run for 10,000 instructions. Prior research shows that the highest accuracy is achieved for small detailed regions <ref type="bibr" target="#b33">[34]</ref>; larger detailed regions will likely make DeLorean even more accurate since small regions make the penalty for mispredicting the outcome of a single key access high. Prior to each detailed region, we warm up microarchitecture state (processor pipeline, caches, branch predictor) for 30,000 instructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION</head><p>We now evaluate DeLorean's speed and accuracy. DeLorean's key advantage over the state-of-the-art is its significantly improved evaluation speed while delivering similar accuracy. We also present a sensitivity analysis and several use cases. We consider the following sampling strategies in the evaluation:</p><p>• SMARTS: Functional Warming (FW) is used to keep the caches warm using functional simulation in-between detailed regions as done in SMARTS <ref type="bibr" target="#b33">[34]</ref>. • CoolSim: Randomized Statistical Warming (RSW) is employed to collect randomly selected reuse distances at nearnative hardware speed in-between detailed regions. This is the state-of-the-art statistical cache warming strategy called CoolSim <ref type="bibr" target="#b22">[23]</ref>. We pick the best possible configuration for CoolSim, which encompasses an adaptive sampling strategy: sample one memory location every 40k memory instructions for the first 750M instructions, then one every 20k for the next 200M instructions, and finally one every 10k for the last 50M instructions. • DeLorean employs DSW to determine the key reuse distances which are collected through TT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Speed</head><p>Figure <ref type="figure" target="#fig_4">5</ref> reports simulation speed normalized to SMARTS. De-Lorean improves simulation speed by 96× on average compared to SMARTS. Compared to CoolSim, DeLorean improves simulation speed by 5.7× on average. We note there is some variation across benchmarks in simulation speedup compared to CoolSim. The highest speedup (49×) is reported for bwaves: this benchmark features a small number of key accesses, and the corresponding key reuse distances are small so that they can all be profiled by Explorer-1. This implies that DeLorean essentially fast-forwards through most of the benchmark. CoolSim on the other hand collects a large number of reuse distances which are not needed to accurately model cache warm-up. The smallest speedups are reported for povray (1.05×) and gems (1.4×). The reason is slightly different though for the two benchmarks. gems features a large working set and key accesses with very long reuse distances; hence, it engages all four Explorers. The working set size for povray is much smaller but there is one detailed region with a few key accesses with very long reuse distances, which engages all four Explorers. These long reuse distances, in addition, incur a large number of false positive watchpoint triggers. This is due to the use of the page protection mechanism to collect the reuse distances at near-native hardware speed, as previously described, i.e., cachelines with large reuses map into the same physical page as cachelines with very short reuses, making the collection of those longer reuses expensive due to false positives. These simulation speedups lead to high absolute simulation speeds. We report that DeLorean achieves a simulation speed of 126 MIPS, compared to 21.9 MIPS for CoolSim and 1.3 MIPS for SMARTS. In other words, DeLorean is within one order of magnitude compared to native hardware execution. This high simulation speed enables a new range of experiments to be run with much longer running, and hence more realistic, workloads than the relatively small workloads that are typically run with detailed cycle-accurate simulation.</p><p>The reason for DeLorean's high simulation speed is a result of both DSW and TT, the two key contributions in this paper. We provide deeper insight where the improved simulation speed is coming from in the next two subsections.</p><p>6.1.1 DSW. Directed statistical warming (DSW) substantially reduces the number of reuse distances that need to be collected during the warm-up interval compared to RSW. Figure <ref type="figure">6</ref> quantifies the total number of collected reuse distances across the 10 detailed regions (note the logarithmic scale): DSW reduces the number of reuse distances by 30× on average (and up to 6,800×) compared to RSW. Whereas RSW collects 340,000 reuse distance on average, DSW collects 11,000 reuse distances. The reason is that DSW collects a select number of reuse distances, namely the key reuse distances and the vicinity reuse distances, whereas RSW collects a much larger number of random reuse distances.</p><p>6.1.2 Time Traveling. Time traveling (TT) translates the reduction in reuse distances collected during warm-up through DSW into a significant evaluation speedup. Recall that the number of engaged Explorers depends on the number of key accesses and their reuse distances. Figure <ref type="figure">7</ref> breaks down the key reuse distances as they are collected by the respective Explorers. Most key reuse distances are collected by Explorer-1, however, additional Explorers are engaged for a number of benchmarks. Figure <ref type="figure">8</ref> quantifies the average number of Explorers engaged. The number of Explorers varies across the benchmarks depending on how long the reuse distances are. For example, bwaves features short key reuse distances, hence the number of Explorers needed is small, even less than one on average (vast majority of memory operations hit in the lukewarm cache or MSHRs). In contrast, benchmarks such as zeus, cactus, gems and lbm feature a relatively large number of long reuse distances, hence they require up to four Explorers on average. A couple benchmarks have few long reuse distances across all detailed regions, see for example mcf, gromacs, leslie3d, sjeng and astar, hence they also engage a relatively large number of Explorers. calculix is an exception having a relatively large number of long reuse distances, yet the number of Explorers is relatively small; the reason is that the long reuse distances originate from a single detailed region, hence we need to engage up to four Explorers only for a single detailed region and not the other regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Accuracy</head><p>Figures 9 and 10 quantify DeLorean's simulation accuracy for predicting CPI for two cache sizes to reflect a modern-day LLC size (8 MiB) as well as a large-scale DRAM cache (512 MiB), respectively. Note that SMARTS is our reference here because of the full cache warming done in-between detailed regions. DeLorean predicts CPI with an average error of 3.5% and 2.9% for the 8 MiB and 512 MiB LLCs, respectively. DeLorean is substantially more accurate than CoolSim for soplex and gems. The average error for CoolSim equals 9.1% and 9.3% on average for the 8 MiB and 512 MiB LLCs, respectively. The reason for the high error for CoolSim for soplex and gems is a result of an overestimation of the number of LLC misses.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Sensitivity Analyses</head><p>We now consider two sensitivity analyses related to the vicinity distributions and hardware prefetching.   simulation) versus predicted to be in the cache (for DeLorean) are nullified to save memory bandwidth. Figure <ref type="figure" target="#fig_7">12</ref> reports CPI error for our baseline processor with and without an LLC stride prefetcher with 8 streams. We conclude that DeLorean is slightly more accurate for an architecture with hardware prefetching because there are fewer cache misses to be predicted in the first place.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Use Cases</head><p>Having demonstrated the speed and accuracy of DeLorean, we now consider two case studies to illustrate its usage in practical design studies. We consider application working set characterization and design space exploration.</p><p>6.4.1 Working Set Curves. Working set curves are widely used to characterize an application's working set size. A working set curve shows cache miss rate (or MPKI) as a function of cache size, and typically incurs a point (cache size), or multiple points, at which the miss rate falls off. This is commonly referred to as the 'knee' of the curve. This knee indicates the working set size of the application at hand. Figure <ref type="figure" target="#fig_2">13</ref> provides a number of interesting examples, i.e., cactus, leslie3d and lbm; the solid line shows the reference curve while the dashed line shows the curve obtained through DeLorean. (We observe similar results for the other benchmarks; not provided because of space constraints.) The key observation is that DeLorean tracks the reference curves obtained using SMARTS well. For example, lbm has a knee in the curve around 8 MiB and 512 MiB, which is accurately predicted by DeLorean; cactus and leslie3d, on the other hand, do not have a pronounced knee in the curve, which is also accurately predicted by DeLorean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.2">Design Space Exploration.</head><p>DeLorean is a fast simulation methodology, significantly speeding up design space exploration studies. Figure <ref type="figure" target="#fig_8">14</ref> shows performance curves (CPI) as a function of cache size for the same set of benchmarks as in Figure <ref type="figure" target="#fig_2">13</ref>. DeLorean tracks the reference (SMARTS) accurately and accurately predicts performance sensitivity to LLC size.</p><p>Note that all 10 points in Figure <ref type="figure" target="#fig_8">14</ref> were obtained from the same warm-up in a parallel simulation run. As explained in Section 3.  time and most of the simulation host resources compared to timing simulation of the detailed regions -the time spent in warming versus detailed simulation is about a factor 235× for DeLorean. The marginal cost for parallel simulation in DeLorean is thus small in terms of required simulation resources -less than 1.05× for 10 parallel Analysts. This is much smaller than the 10× marginal cost for parallelizing 10 detailed simulations as is commonly done.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK</head><p>Checkpointed Warming. One popular approach to the cache warm-up problem in sampled processor evaluation is checkpointed warming, which takes a checkpoint of the microarchitecture state prior to each detailed region. Wenisch et al. <ref type="bibr" target="#b31">[32]</ref> store the state of the caches and other micro-architectural structures in checkpoints. These so-called Flex points eliminate warming overhead in SMARTS. Flex points are large (20 MiB to 100 MiB). In follow-up work, they introduce Live points <ref type="bibr" target="#b32">[33]</ref> and reduce the space requirements for each checkpoint down to 142 KiB. Van Biesbrouck et al. <ref type="bibr" target="#b29">[30]</ref> propose a similar approach called memory hierarchy state (MHS) to minimize checkpoint size, in the context of SimPoint <ref type="bibr" target="#b27">[28]</ref>. Barr et al. <ref type="bibr" target="#b0">[1]</ref> propose the Memory Timestamp Record (MTR), a method to record memory patterns, compress and store them for use in checkpoints for sampled simulation of multi-threaded workloads on multi-processor system. Hassani et al. <ref type="bibr" target="#b13">[14]</ref> use sampled simulation combined with MTR and in-memory checkpoints to evaluate benchmarks in a few seconds by simulating detailed regions in parallel. A major limitation of checkpointed warming is that is not re-usable across software changes. In contrast, DeLorean does not incur any storage overhead and is re-usable across software and hardware changes.</p><p>Functional Warming. Instead to storing a checkpoint on disk, functional warming warms up microarchitecture state on the fly by simulating microarchitecture structures prior to each detailed region. Functional warming does not incur any storage overhead and is re-usable across software changes. Traditionally, functional warming warms up microarchitecture state using all memory references between two consecutive detailed regions, which is very slow <ref type="bibr" target="#b33">[34]</ref>. Various approaches have been proposed to reduce the warm-up length prior to each detailed region. Haskins and Skadron <ref type="bibr" target="#b11">[12]</ref> and Luo et al. <ref type="bibr" target="#b18">[19]</ref> use heuristics to find the minimum number of instructions needed to warm a cache of specified size. Haskins and Skadron <ref type="bibr" target="#b12">[13]</ref> introduce the concept of Memory Reference Reuse Latencies (MRRLs) which is the number of completed instructions between consecutive references to the same memory location. The number of instructions that provides a large enough cumulative distribution of MRRLs is used as the warming interval. Eeckhout et al. <ref type="bibr" target="#b8">[9]</ref> introduce the Boundary Line Reuse Latency (BLRL) which extends the MRRL concept. They apply similar heuristics to find a shorter warm-up period. Van Ertvelde et al. <ref type="bibr" target="#b30">[31]</ref> extend on the concept of BLRL using a form of hardware state checkpoints. Sandberg et al. <ref type="bibr" target="#b25">[26]</ref> propose a method that uses two parallel simulations, pessimistic and optimistic, to bound the maximum error due to warming. While minimizing the number of instructions needed to warm up microarchitecture state improves evaluation speed, all of these functional warming techniques suffer from the inherent limitation that they need to simulate all memory references in the warm-up interval. In other words, even though the interval is shortened, these techniques still need to simulate all of them, and most of these references are not needed to accurately warm up the cache hierarchy. In contrast, DeLorean limits the number of warm-up references that need to be inspected, dramatically improving warm-up efficiency.</p><p>Statistical Cache Modeling. Stack distance analysis has been extensively used to model caches. <ref type="bibr">Mattson et al. [20]</ref> use a simple stack algorithm to inspect every access and collect stack distance information. To improve performance, researchers use k-ary <ref type="bibr" target="#b3">[4]</ref> and AVL <ref type="bibr" target="#b23">[24]</ref> trees instead of linked lists. However, all of the proposed methods have to inspect all memory accesses and measure stack distance. Other works <ref type="bibr" target="#b28">[29]</ref> have proposed hardware-accelerated stack distance collection. Liu and Mellor-Crummey <ref type="bibr" target="#b16">[17]</ref> use a technique based on shadow profiling that forks off a redundant copy of an application, instrumented by Pin, to measure the stack distances for a selected set of references <ref type="bibr" target="#b16">[17]</ref>.</p><p>A major limitation of stack distance analysis is that measuring stack distances is computationally demanding because all references between two reuses of the same cacheline need to be inspected to compute the number of unique cachelines. Eklov and Hagersten <ref type="bibr" target="#b10">[11]</ref> demonstrate how reuse distance, which is computationally less demanding to collect, can be used to predict stack distance. Reuse distance based statistical cache modeling was successfully demonstrated to model caches with different replacement policies <ref type="bibr" target="#b24">[25]</ref>, multi-programming workloads <ref type="bibr" target="#b9">[10]</ref> as well as multithreaded workloads <ref type="bibr" target="#b6">[7]</ref>. Randomized statistical warming leverages statistical cache models to predict which memory references in a detailed region are warming misses. As extensively argued in this paper, randomized statistical warming requires a large number of reuse distances, many of which are useless to accurately predict warm-up effects during sampled evaluation. Directed statistical warming dramatically reduces the number of reuse distances that need to be collected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>Sampling allows for realistic workload evaluation by reducing the number of instructions that need to be evaluated in detail. Unfortunately, warming caches dominates simulation overhead and prevents simulation frameworks from realizing a proportional reduction in simulation time while maintaining flexibility to make changes in software and hardware.</p><p>DeLorean delivers a substantial simulation speedup compared to the state-of-the-art through two key innovations: directed statistical warming and time traveling. Directed statistical warming collects a select number of key reuse distances and their vicinity distributions -a reduction by 30× compared to the state-of-the-art CoolSim. Time traveling measures these key reuse distances in a single evaluation run by first quickly looking into the future (through virtualized fast-forwarding) to determine the key cachelines, and then going back in time to compute the key reuse distances and vicinity distributions at near-native hardware speed (through virtualized directed profiling). Time traveling translates the reduction in reuse distances that need to be profiled into a simulation speedup of 5.7× compared to CoolSim. Warm-up cost can be amortized across multiple parallel simulations when conducting design space exploration studies. DeLorean also improves simulation accuracy: prediction error is reduced from around 9% for CoolSim to around 3% on average. Ultimately, DeLorean enables detailed cycle-accurate gem5 simulations at a speed of 126 MIPS.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>MICRO- 52 ,</head><label>52</label><figDesc>October 12-16, 2019, Columbus, OH, USA Nikos Nikoleris, Lieven Eeckhout, Erik Hagersten, and Trevor E. Carlson Warm-up Interval D (a) Functional warming. (b) Randomized statistical warming. (c) Directed statistical warming.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Three approaches to warm up a cache for the detailed region D: (a) functional warming, (b) randomized statistical warming, and (c) directed statistical warming.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Statistical warming.The lukewarm cache determines accesses with short reuses as cache and MSHR hits. Then, the limitedassociativity model determines conflict misses and the statistical cache model determines capacity misses. All other accesses that appear to be misses are due to insufficient warming and are treated as hits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Time Traveling. Scout leverages VFF to quickly advance to identify key cachelines in the next detailed region. Then, the Explorers go back in time and collect the key reuse distances and the respective vicinity reuse distributions. Finally, the Analyst uses DSW to evaluate the detailed region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Normalized simulation speed for DeLorean, CoolSim and SMARTS. DeLorean improves simulation speed by 96× on average compared to SMARTS, and by 5.7× compared to CoolSim.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :Figure 7 :Figure 8 :</head><label>678</label><figDesc>Figure 6: Number of reuse distances collected by CoolSim (RSW) versus DeLorean (DSW). DeLorean reduces the number of reuse distances by 30× on average compared to CoolSim.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :Figure 10 :Figure 11 :</head><label>91011</label><figDesc>Figure 9: Reported CPI for DeLorean, CoolSim and SMARTS (reference) for an 8 MB LLC. DeLorean's simulation accuracy is within 3.5% on average for CPI compared to SMARTS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: CPI error with and without prefetching for an 8 MB LLC. DeLorean is slightly more accurate for an architecture with hardware prefetching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Performance (CPI) as a function of cache size for three example benchmarks. The reference (SMARTS) is shown as a solid line whereas DeLorean is shown as a dashed line. DeLorean tracks the reference curves well.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Simulated processor architecture.</figDesc><table><row><cell>DeLorean leverages hardware virtualization which is widely sup-</cell></row><row><cell>ported across ISAs. Research into ISA extensions can be handled</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Lorean to trigger the hardware prefetcher using misses as predicted by the statistical cache model. In other words, DeLorean feeds the hardware prefetcher with predicted cache miss information rather than actual (simulated) cache miss information. Likewise, prefetch requests to cachelines that are in the cache already (in detailed</figDesc><table><row><cell>terms of simulation speed and accuracy with respect to sampling</cell></row><row><cell>density in the vicinity. Increasing sampling density improves accu-</cell></row><row><cell>racy by collecting more reuse distances. On the other hand, higher</cell></row><row><cell>density also increases the profiling overhead of the Explorers. Fig-</cell></row><row><cell>ure 11 quantifies this trade-off in simulation speed versus accuracy</cell></row><row><cell>for an 8 MB LLC. With a density of 1 over 100 k memory instruc-</cell></row><row><cell>tions, we can simulate at 126 MIPS with an error of 3.5%. Increasing</cell></row><row><cell>density to 1 over 10 k instructions brings down the error to 2.2% at</cell></row><row><cell>a simulation speed of 71.3 MIPS.</cell></row><row><cell>6.3.2 Hardware Prefetching. As argued in Section 4, DeLorean is</cell></row><row><cell>broadly applicable because it builds upon statistical cache modeling</cell></row><row><cell>which has been demonstrated for a range of different architectures</cell></row><row><cell>and configurations. We now consider hardware prefetching to il-</cell></row><row><cell>lustrate DeLorean's broader applicability. Hardware prefetching</cell></row><row><cell>improves performance by speculatively fetching cachelines before</cell></row><row><cell>the application actually needs it. Hardware prefetch requests are</cell></row><row><cell>typically triggered by cache misses, i.e., cache misses with particu-</cell></row><row><cell>lar patterns (e.g., a stride) trigger prefetch requests. We extend De-</cell></row><row><cell>6.3.1 Vicinity Reuse Distance Distribution. Recall that DeLorean</cell></row><row><cell>samples the reuse distance distribution in the vicinity as input to</cell></row><row><cell>statistical cache modeling. The sampling rate is a parameter that</cell></row><row><cell>can be freely set. The default sampling rate is set to 1 out of 100 k</cell></row><row><cell>memory instructions. We now evaluate DeLorean's sensitivity in</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc><ref type="bibr" target="#b2">3</ref>, warm-up cost can be amortized across multiple parallel simulations by feeding multiple parallel Analysts from a single Scout and a single set of Explorers. Collecting reuse distances takes up most of the Working set curves for three example benchmarks. The reference (SMARTS) is shown as a solid line whereas De-Lorean is shown as a dashed line. DeLorean tracks the reference working set curves well.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">cactusADM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">leslie3d</cell><cell></cell><cell></cell><cell>lbm</cell></row><row><cell></cell><cell>8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40</cell></row><row><cell>MPKI</cell><cell>2 4 6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MPKI</cell><cell>10 20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MPKI</cell><cell>10 20 30</cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell></row><row><cell></cell><cell>1</cell><cell cols="2">2</cell><cell cols="2">4</cell><cell cols="2">8</cell><cell>1 6</cell><cell cols="2">3 2</cell><cell cols="2">6 4</cell><cell cols="2">1 2 8 2 5 6 5 1 2</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>1 6</cell><cell cols="2">3 2</cell><cell>6 4</cell><cell>1 2 8 2 5 6 5 1 2</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>1 6</cell><cell>3 2</cell><cell>6 4</cell><cell>1 2 8 2 5 6 5 1 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">Cache Size (MB)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Cache Size (MB)</cell><cell></cell><cell>Cache Size (MB)</cell></row><row><cell cols="2">0.2 0.4 0.6 0.8 Figure 13: 0 CPI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">cactusADM</cell><cell></cell><cell>CPI</cell><cell>0 0.5 1 1.5 2</cell><cell></cell><cell></cell><cell></cell><cell cols="3">leslie3d</cell><cell></cell><cell>CPI</cell><cell>0 1 2 3 4</cell><cell>lbm</cell></row><row><cell></cell><cell cols="2">1</cell><cell cols="2">2</cell><cell cols="2">4</cell><cell>8</cell><cell cols="2">1 6</cell><cell cols="2">3 2</cell><cell cols="2">6 4</cell><cell>1 2 8 2 5 6 5 1 2</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell cols="2">1 6</cell><cell>3 2</cell><cell>6 4</cell><cell>1 2 8 2 5 6 5 1 2</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>1 6</cell><cell>3 2</cell><cell>6 4</cell><cell>1 2 8 2 5 6 5 1 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">Cache Size (MB)</cell><cell></cell><cell></cell><cell></cell><cell cols="7">Cache Size (MB)</cell><cell>Cache Size (MB)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">MICRO-52, October 12-16, 2019, Columbus, OH, USA Nikos Nikoleris, Lieven Eeckhout, Erik Hagersten, and Trevor E. Carlson</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">Watchpoints for collecting the vicinity distribution are set at random memory locations, however, an order of magnitude fewer reuse distances need to be collected than for RSW, as mentioned before.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">We were unable to run 403.gcc, 433.milc., 447.dealII, 481.wrf and 482.sphinx3 because these benchmarks either produced outputs that could not be verified with the reference input or did not run to completion.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank the anonymous reviewers for their helpful feedback. This work was supported by the Swedish Foundation for Strategic Research CoDeR-MP project and the Swedish Research Council Linnaeus centre of excellence UPMARC. The simulations were performed on resources provided by the Swedish National Infrastructure for Computing (SNIC) at UPPMAX and NSC. Additional support is provided through European Research Council (ERC) Advanced Grant agreement no. 741097, and FWO grants no. G.0434.16N and G.0144.17N. This work was also supported by a Start-up Grant from the National University of Singapore (NUS).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Accelerating Multiprocessor Simulation with a Memory Timestamp Record</title>
		<author>
			<persName><forename type="first">Kenneth</forename><forename type="middle">C</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heidi</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krste</forename><surname>Asanovic</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISPASS.2005.1430560</idno>
		<ptr target="https://doi.org/10.1109/ISPASS.2005.1430560" />
	</analytic>
	<monogr>
		<title level="m">Proc. International Symposium on Performance Analysis of Systems &amp; Software (ISPASS)</title>
				<meeting>International Symposium on Performance Analysis of Systems &amp; Software (ISPASS)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="66" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modeling Cache Performance Beyond LRU</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Sanchez</surname></persName>
		</author>
		<idno type="DOI">10.1109/HPCA.2016.7446067</idno>
		<ptr target="https://doi.org/10.1109/HPCA.2016.7446067" />
	</analytic>
	<monogr>
		<title level="m">Proc. International Symposium on High-Performance Computer Architecture (HPCA)</title>
				<meeting>International Symposium on High-Performance Computer Architecture (HPCA)</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="225" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Discussion on Nonblocking/Lockup-free Caches</title>
		<author>
			<persName><forename type="first">Samson</forename><surname>Belayneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Kaeli</surname></persName>
		</author>
		<idno type="DOI">10.1145/381718.381727</idno>
		<ptr target="https://doi.org/10.1145/381718.381727" />
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="18" to="25" />
			<date type="published" when="1996-06">1996. June 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">LRU Stack Processing</title>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">T</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Kruskal</surname></persName>
		</author>
		<idno type="DOI">10.1147/rd.194.0353</idno>
		<ptr target="https://doi.org/10.1147/rd.194.0353" />
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="353" to="357" />
			<date type="published" when="1975-07">1975. July 1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">StatCache: A Probabilistic Approach to Efficient and Accurate Data Locality Analysis</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Hagersten</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISPASS.2004.1291352</idno>
		<ptr target="https://doi.org/10.1109/ISPASS.2004.1291352" />
	</analytic>
	<monogr>
		<title level="m">Proc. International Symposium on Performance Analysis of Systems &amp; Software (ISPASS)</title>
				<meeting>International Symposium on Performance Analysis of Systems &amp; Software (ISPASS)</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast Data-Locality Profiling of Native Execution</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Hagersten</surname></persName>
		</author>
		<idno type="DOI">10.1145/1064212.1064232</idno>
		<ptr target="https://doi.org/10.1145/1064212.1064232" />
	</analytic>
	<monogr>
		<title level="m">Proc. Internationcal Conference on Measuring and Modeling of Computer Systems (SIGMETRICS)</title>
				<meeting>Internationcal Conference on Measuring and Modeling of Computer Systems (SIGMETRICS)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="169" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Statistical Multiprocessor Cache Model</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Håkan</forename><surname>Zeffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Hagersten</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISPASS.2006.1620793</idno>
		<ptr target="https://doi.org/10.1109/ISPASS.2006.1620793" />
	</analytic>
	<monogr>
		<title level="m">Proc. International Symposium on Performance Analysis of Systems &amp; Software (ISPASS)</title>
				<meeting>International Symposium on Performance Analysis of Systems &amp; Software (ISPASS)</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="89" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Binkert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bradford</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Saidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arkaprava</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">R</forename><surname>Hower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somayeh</forename><surname>Sardashti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rathijit</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Korey</forename><surname>Sewell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Shoaib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nilay</forename><surname>Vaish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
		<idno type="DOI">10.1145/2024716.2024718</idno>
		<ptr target="https://doi.org/10.1145/2024716.2024718" />
	</analytic>
	<monogr>
		<title level="m">The gem5 Simulator</title>
				<imprint>
			<date type="published" when="2011-08">2011. Aug. 2011</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BLRL: Accurate and Efficient Warmup for Sampled Processor Simulation</title>
		<author>
			<persName><forename type="first">Lieven</forename><surname>Eeckhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koen</forename><surname>De Bosschere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizy</forename><forename type="middle">K</forename><surname>John</surname></persName>
		</author>
		<idno type="DOI">10.1093/comjnl/bxh103</idno>
		<ptr target="https://doi.org/10.1093/comjnl/bxh103" />
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="451" to="459" />
			<date type="published" when="2005-07">2005. July 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">StatCC: A Statistical Cache Contention Model</title>
		<author>
			<persName><forename type="first">David</forename><surname>Eklov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Black-Schaffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Hagersten</surname></persName>
		</author>
		<idno type="DOI">10.1145/1854273.1854347</idno>
		<ptr target="https://doi.org/10.1145/1854273.1854347" />
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
				<meeting>International Conference on Parallel Architectures and Compilation Techniques (PACT)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="551" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">StatStack: Efficient Modeling of LRU Caches</title>
		<author>
			<persName><forename type="first">David</forename><surname>Eklov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Hagersten</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISPASS.2010.5452069</idno>
		<ptr target="https://doi.org/10.1109/ISPASS.2010.5452069" />
	</analytic>
	<monogr>
		<title level="m">Proc. International Symposium on Performance Analysis of Systems &amp; Software (ISPASS)</title>
				<meeting>International Symposium on Performance Analysis of Systems &amp; Software (ISPASS)</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="55" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Minimal Subset Evaluation: Rapid Warm-Up for Simulated Hardware State</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">W</forename><surname>Haskins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Skadron</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCD.2001.955000</idno>
		<ptr target="https://doi.org/10.1109/ICCD.2001.955000" />
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Design: VLSI in Computers and Processors (ICCD)</title>
				<meeting>International Conference on Computer Design: VLSI in Computers and essors (ICCD)</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="32" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Memory Reference Reuse Latency: Accelerated Warmup for Sampled Microarchitecture Simulation</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">W</forename><surname>Haskins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Skadron</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISPASS.2003.1190246</idno>
		<ptr target="https://doi.org/10.1109/ISPASS.2003.1190246" />
	</analytic>
	<monogr>
		<title level="m">Proc. International Symposium on Performance Analysis of Systems &amp; Software (ISPASS)</title>
				<meeting>International Symposium on Performance Analysis of Systems &amp; Software (ISPASS)</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="195" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">LiveSim: Going Live with Microarchitecture Simulation</title>
		<author>
			<persName><forename type="first">Sina</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Southern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Renau</surname></persName>
		</author>
		<idno type="DOI">10.1109/HPCA.2016.7446098</idno>
		<ptr target="https://doi.org/10.1109/HPCA.2016.7446098" />
	</analytic>
	<monogr>
		<title level="m">Proc. International Symposium on High-Performance Computer Architecture (HPCA)</title>
				<meeting>International Symposium on High-Performance Computer Architecture (HPCA)</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="606" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Evaluating Associativity in CPU Caches</title>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.1109/12.40842</idno>
		<ptr target="https://doi.org/10.1109/12.40842" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1612" to="1630" />
			<date type="published" when="1989-12">1989. Dec. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">KVM: The Linux virtual machine monitor</title>
		<author>
			<persName><forename type="first">Avi</forename><surname>Kivity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaniv</forename><surname>Kamay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dor</forename><surname>Laor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Lublin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Liguori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Linux Symposium</title>
				<meeting>Linux Symposium</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="225" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pinpointing Data Locality Bottlenecks with Low Overhead</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Mellor-Crummey</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISPASS.2013.6557169</idno>
		<ptr target="https://doi.org/10.1109/ISPASS.2013.6557169" />
	</analytic>
	<monogr>
		<title level="m">Proc. International Symposium on Performance Analysis of Systems &amp; Software (ISPASS)</title>
				<meeting>International Symposium on Performance Analysis of Systems &amp; Software (ISPASS)</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="183" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pin: Building Customized Program Analysis Tools with Dynamic Instrumentation</title>
		<author>
			<persName><forename type="first">Chi-Keung</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Muth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harish</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artur</forename><surname>Klauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Lowney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><forename type="middle">Janapa</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Hazelwood</surname></persName>
		</author>
		<idno type="DOI">10.1145/1065010.1065034</idno>
		<ptr target="https://doi.org/10.1145/1065010.1065034" />
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)</title>
				<meeting>ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-Monitored Adaptive Cache Warm-Up for Microprocessor Simulation</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizy</forename><forename type="middle">K</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lieven</forename><surname>Eeckhout</surname></persName>
		</author>
		<idno type="DOI">10.1109/SBAC-PAD.2004.38</idno>
		<ptr target="https://doi.org/10.1109/SBAC-PAD.2004.38" />
	</analytic>
	<monogr>
		<title level="m">Proc. Symposium on Computer Architecture and High Performance Computing (SBAC-PAD)</title>
				<meeting>Symposium on Computer Architecture and High Performance Computing (SBAC-PAD)</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Evaluation Techniques for Storage Hierarchies</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">L</forename><surname>Mattson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Gecsei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">R</forename><surname>Slutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irvine</forename><forename type="middle">L</forename><surname>Traiger</surname></persName>
		</author>
		<idno type="DOI">10.1147/sj.92.0078</idno>
		<ptr target="https://doi.org/10.1147/sj.92.0078" />
	</analytic>
	<monogr>
		<title level="j">IBM Systems Journal</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="78" to="117" />
			<date type="published" when="1970-06">1970. June 1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Valgrind: A Framework for Heavyweight Dynamic Binary Instrumentation</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Nethercote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Seward</surname></persName>
		</author>
		<idno type="DOI">10.1145/1250734.1250746</idno>
		<ptr target="https://doi.org/10.1145/1250734.1250746" />
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)</title>
				<meeting>ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="89" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Extending Statistical Cache Models to Support Detailed Pipeline Simulators</title>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Nikoleris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Eklov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Hagersten</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISPASS.2014.6844464</idno>
		<ptr target="https://doi.org/10.1109/ISPASS.2014.6844464" />
	</analytic>
	<monogr>
		<title level="m">Proc. International Symposium on Performance Analysis of Systems &amp; Software (ISPASS)</title>
				<meeting>International Symposium on Performance Analysis of Systems &amp; Software (ISPASS)</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="86" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">CoolSim: Statistical Techniques to Replace Cache Warming with Efficient, Virtualized Profiling</title>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Nikoleris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Sandberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Hagersten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<idno type="DOI">10.1109/SAMOS.2016.7818337</idno>
		<ptr target="https://doi.org/10.1109/SAMOS.2016.7818337" />
	</analytic>
	<monogr>
		<title level="m">Proc. Symposium on Systems, Architectures, Modeling, and Simulation (SAMOS)</title>
				<meeting>Symposium on Systems, Architectures, Modeling, and Simulation (SAMOS)</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="106" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Efficient Methods for Calculating the Success Function of Fixed Space Replacement Policies</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>Olken</surname></persName>
		</author>
		<idno type="DOI">10.2172/6051879</idno>
		<ptr target="https://doi.org/10.2172/6051879" />
		<imprint>
			<date type="published" when="1981">1981</date>
		</imprint>
		<respStmt>
			<orgName>Lawrence Berkeley Laboratory, University of California</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A Modeling Framework for Reuse Distancebased Estimation of Cache Performance</title>
		<author>
			<persName><forename type="first">Xiaoyue</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bengt</forename><surname>Jonsson</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISPASS.2015.7095785</idno>
		<ptr target="https://doi.org/10.1109/ISPASS.2015.7095785" />
	</analytic>
	<monogr>
		<title level="m">Proc. International Symposium on Performance Analysis of Systems &amp; Software (ISPASS)</title>
				<meeting>International Symposium on Performance Analysis of Systems &amp; Software (ISPASS)</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="62" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Full Speed Ahead: Detailed Architectural Simulation at Near-Native Speed</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Sandberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Nikoleris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Hagersten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Kaxiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Black-Schaffer</surname></persName>
		</author>
		<idno type="DOI">10.1109/IISWC.2015.29</idno>
		<ptr target="https://doi.org/10.1109/IISWC.2015.29" />
	</analytic>
	<monogr>
		<title level="m">Proc. International Symposium on Workload Characterization (IISWC)</title>
				<meeting>International Symposium on Workload Characterization (IISWC)</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Reuse-based Online Models for Caches</title>
		<author>
			<persName><forename type="first">Rathijit</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
		<idno type="DOI">10.1145/2465529.2465756</idno>
		<ptr target="https://doi.org/10.1145/2465529.2465756" />
	</analytic>
	<monogr>
		<title level="m">Proc. Internationcal Conference on Measuring and Modeling of Computer Systems (SIGMETRICS)</title>
				<meeting>Internationcal Conference on Measuring and Modeling of Computer Systems (SIGMETRICS)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="279" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automatically Characterizing Large Scale Program Behavior</title>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erez</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Hamerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brad</forename><surname>Calder</surname></persName>
		</author>
		<idno type="DOI">10.1145/605397.605403</idno>
		<ptr target="https://doi.org/10.1145/605397.605403" />
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
				<meeting>International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="45" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">RapidMRC: Approximating L2 Miss Rate Curves on Commodity Systems for Online Optimizations</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">K</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Azimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Livio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><surname>Stumm</surname></persName>
		</author>
		<idno type="DOI">10.1145/1508244.1508259</idno>
		<ptr target="https://doi.org/10.1145/1508244.1508259" />
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
				<meeting>International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="121" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient Sampling Startup for SimPoint</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Van Biesbrouck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brad</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lieven</forename><surname>Eeckhout</surname></persName>
		</author>
		<idno type="DOI">10.1109/MM.2006.68</idno>
		<ptr target="https://doi.org/10.1109/MM.2006.68" />
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="32" to="42" />
			<date type="published" when="2006-07">2006. July 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">NSL-BLRL: Efficient CacheWarmup for Sampled Processor Simulation</title>
		<author>
			<persName><forename type="first">Luk</forename><surname>Van Ertvelde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Hellebaut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lieven</forename><surname>Eeckhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koen</forename><surname>De Bosschere</surname></persName>
		</author>
		<idno type="DOI">10.1109/ANSS.2006.33</idno>
		<ptr target="https://doi.org/10.1109/ANSS.2006.33" />
	</analytic>
	<monogr>
		<title level="m">Proc. Annual Symposium on Simulation</title>
				<meeting>Annual Symposium on Simulation</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">TurboSMARTS: Accurate Microarchitecture Simulation Sampling in Minutes</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><forename type="middle">E</forename><surname>Wunderlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">C</forename><surname>Hoe</surname></persName>
		</author>
		<idno type="DOI">10.1145/1071690.1064278</idno>
		<ptr target="https://doi.org/10.1145/1071690.1064278" />
	</analytic>
	<monogr>
		<title level="j">ACM SIGMETRICS Performance Evaluation Review</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="408" to="409" />
			<date type="published" when="2005-06">2005. June 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">SimFlex: Statistical Sampling of Computer System Simulation</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><forename type="middle">E</forename><surname>Wunderlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastassia</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">C</forename><surname>Hoe</surname></persName>
		</author>
		<idno type="DOI">10.1109/MM.2006.79</idno>
		<ptr target="https://doi.org/10.1109/MM.2006.79" />
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2006-07">2006. July 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">SMARTS: Accelerating Microarchitecture Simulation via Rigorous Statistical Sampling</title>
		<author>
			<persName><forename type="first">Roland</forename><forename type="middle">E</forename><surname>Wunderlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">C</forename><surname>Hoe</surname></persName>
		</author>
		<idno type="DOI">10.1145/859618.859629</idno>
		<ptr target="https://doi.org/10.1145/859618.859629" />
	</analytic>
	<monogr>
		<title level="m">Proc. International Symposium on Computer Architecture (ISCA)</title>
				<meeting>International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="84" to="97" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
