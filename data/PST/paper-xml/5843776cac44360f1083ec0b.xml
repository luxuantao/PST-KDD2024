<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A High Performance FPGA-based Accelerator for Large-Scale Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Huimin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of ASIC and System Fudan University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xitian</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of ASIC and System Fudan University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Jiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of ASIC and System Fudan University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Wei</forename><surname>Cao</surname></persName>
							<email>caow@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of ASIC and System Fudan University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuegong</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of ASIC and System Fudan University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lingli</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of ASIC and System Fudan University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A High Performance FPGA-based Accelerator for Large-Scale Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">46B9257D8DD09D24C6D3DF9DAA2B4339</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T05:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>convolutional neural networks</term>
					<term>pipeline</term>
					<term>AlexNet</term>
					<term>parallelism</term>
					<term>memory bandwidth Value: 0.1962 Type: n01795545 black grouse Value: 0.1662 Type: n02111889 Samoyed, Samoyede Value: 0.1243 Type: n01807496 partridge Value: 0.1099 Type: n01532829 house finch, linnet, Carpodacus mexicanus Value: 0.1065 Type: n02437616 llama Value: 0.4986 Type: n04562935 water tower Value: 0.3619 Type: n04612504 yawl Value: 0.1267 Type: n02981792 catamaran Value: 0.0089 Type: n04147183 schooner Value: 0.0022 Type: n04483307 trimaran Value: 0.4291 Type: n02326432 hare Value: 0.1490 Type: n02325366 wood rabbit, cottontail, Cottontail rabbit Value: 0.1035 Type: n01753488 horned viper, cerastes, Sand viper, horned asp, Cerastes cornutus Value: 0.0792 Type: n01496331 electric ray, crampfish, Numbfish, torpedo Value: 0.0718 Type: n02137549 mongoose</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, convolutional neural networks (CNNs) based machine learning algorithms have been widely applied in computer vision applications. However, for large-scale CNNs, the computation-intensive, memory-intensive and resource-consuming features have brought many challenges to CNN implementations. This work proposes an end-to-end FPGAbased CNN accelerator with all the layers mapped on one chip so that different layers can work concurrently in a pipelined structure to increase the throughput. A methodology which can find the optimized parallelism strategy for each layer is proposed to achieve high throughput and high resource utilization. In addition, a batch-based computing method is implemented and applied on fully connected layers (FC layers) to increase the memory bandwidth utilization due to the memory-intensive feature. Further, by applying two different computing patterns on FC layers, the required on-chip buffers can be reduced significantly. As a case study, a state-of-the-art large-scale CNN, AlexNet, is implemented on Xilinx VC709. It can achieve a peak performance of 565.94 GOP/s and 391 FPS under 156MHz clock frequency which outperforms previous approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In recent years, there has been a growing interest in the study of convolutional neural networks (CNNs), inspired by the nervous system in human brain. Owing to the high accuracy and good performance, CNNs have been widely adopted in many applications such as image classification <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, face recognition <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, digital video surveillance <ref type="bibr" target="#b4">[5]</ref> and speech recognition <ref type="bibr" target="#b5">[6]</ref>, etc.</p><p>As CNN models become larger and deeper, numerous operations and data accesses are demanded for CNN-based implementations. Hence, general purpose processors are not so efficient while a number of CNN implementations based on GPU <ref type="bibr" target="#b0">[1]</ref>, FPGA <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr">15]</ref> even ASIC <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b17">19]</ref> have been proposed. Among these platforms, FPGA is especially suitable as the hardware accelerator owing to its flexibility and efficiency. However, there are still many challenges. Since CNNs are known for their computation complexity and frequent data access, previous work mainly focuses on two aspects: maximizing the parallelism for computation and reducing required memory bandwidth. In the existing implementations <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">17]</ref>, the same parallelism strategy is used to implement different layers of a CNN model.</p><p>Consequently, the single parallelism strategy can't meet the requirements for different layers, which reduces the overall performance. Moreover, only one layer is working at one time instead of all the layers working concurrently in a pipelined style. Therefore, there is still a large space to improve the overall performance. With the development of FPGA technology, the on-chip resources have been increased significantly. It is possible to map all the layers onto one chip and adopt different parallelism strategies for different layers, which can achieve high performance and high resource utilization. In addition, the limited memory bandwidth is a bottleneck, especially for fully connected layers (FC layers). To overcome this problem, weights are stored in the on-chip memory to reduce the data accesses in <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b16">18]</ref>. However, for large-scale CNNs, it is infeasible because too many on-chip buffers are required. Thus, we optimize the data access of FC layers in this work.</p><p>The key contributions of this work are as follows:</p><p>• An end-to-end high performance CNN accelerator is proposed with all the layers working concurrently in a pipelined style.</p><p>• A methodology is proposed on how to determine the optimized parallelism strategy for each layer to achieve high throughput and high resource utilization.</p><p>• As the convolutional layers (CONV layers) are computation-intensive while FC layers are memoryintensive <ref type="bibr" target="#b8">[9]</ref>, different computing methods are employed.</p><p>In FC layers, a batch-based computing method is used to reduce the required memory bandwidth for weights. Further, two computing patterns are applied on FC layers alternately, which can reduce the required on-chip buffers in FC layers significantly.</p><p>• A state-of-the-art large-scale CNN, AlexNet <ref type="bibr" target="#b0">[1]</ref>, is implemented on VC709 FPGA board with the proposed architecture and methodology. It can achieve a peak performance with 391 FPS and 565.94 GOP/s at a 156MHz clock which outperforms previous approaches.</p><p>The rest of paper is organized as follows. Section II describes the related work. In Section III, a brief introduction to CNNs and a CNN model AlexNet are given. Section IV explores the design space and proposes a methodology to find the optimized parallelism strategy along with an optimization method for data access. Section V describes our implementation details. A comparison is made between our design and previous implementations in Section VI. Section VII concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>To design CNN accelerators, researchers mainly focus on two aspects. One is to reduce the required memory bandwidth. The other is to explore the parallelism space and increase the throughput.</p><p>During data processing, numerous input data and weights need to be read, which demands a high required bandwidth. References <ref type="bibr" target="#b15">[17]</ref> and <ref type="bibr" target="#b16">[18]</ref> have stored the weights on chip to reduce the required bandwidth. However, as CNN models become larger, it is infeasible to store all the weights on chip. If all the weights are stored off chip, then a high memory bandwidth will be required. By delicate data reuse or data precision reduction, the bandwidth utilization can be increased in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. Moreover, reference <ref type="bibr" target="#b8">[9]</ref> has shown that FC layers are memory-intensive, so Singular Value Decomposition (SVD) is applied on the weight matrix. In our design, on-chip buffers are used to store the intermediate data between layers while the weights are stored off chip. In addition, we apply a batch-based computing method on FC layers to reduce the required bandwidth, which is based on the batch processing in <ref type="bibr" target="#b0">[1]</ref>. Thus, multiple input images can be processed in parallel as a batch with the same weights.</p><p>In order to determine the proper parallelism strategy and optimize the computation, much work has been done such as <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. References <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b12">[13]</ref> use parallelism in feature maps and convolution operations. This work adopts 3 types of parallelism as proposed in <ref type="bibr" target="#b7">[8]</ref>: intra-output parallelism, interoutput parallelism and parallelism within a convolution operation. However, <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> haven't optimized the memory bandwidth. References <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b9">[10]</ref> have optimized both the bandwidth and the computation, which achieve a good balance. Even so, due to the on-chip resource limitation, different layers can't work concurrently in these approaches. Moreover, a single parallelism strategy is adopted for different layers. As the onchip resources have been increased significantly, this work employs a pipelined structure, so that different layers can work simultaneously on one FPGA chip to increase the throughput remarkably. Furthermore, different parallelism strategies are used in different layers to meet their requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. BACKGROUND</head><p>In this section, a typical CNN structure is briefly introduced. Then the state-of-the-art CNN model AlexNet and some statistical information are presented for the architecture design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Primer on the CNN Structure</head><p>A typical CNN model is composed of several layers. In each layer, there are numerous neurons connected with the neurons in the adjacent layers. When an input is given, it passes through layers to generate a feature vector. After that, a classifier is applied on the generated feature vector to produce the classification result. There are mainly three types of layers in a CNN model during the feed-forward computation, CONV layers, pooling layers (POOL layers) and FC layers.</p><p>CONV layers are the core building blocks in the CNN model. A CONV layer accepts several input feature maps. For each input map, a convolution kernel is applied to generate a temporary output map. All the temporary maps and a bias are added to generate a final output map. Next, a nonlinear activation function will be applied on every neuron of the output map. In AlexNet, Rectified Linear Unit (ReLU) is adopted as the activation function as in <ref type="bibr" target="#b0">(1)</ref>. Other output maps can be generated in the same way. The operation in a CONV layer is expressed in <ref type="bibr" target="#b1">(2)</ref>. Here, and represent the i-th input feature map and the j-th output feature map respectively;</p><p>is the convolution kernel applied on the i-th input map to generate the j-th output map;</p><p>is the number of input maps in this layer; is the bias for the j-th output map.</p><formula xml:id="formula_0">( ) = ( , 0)<label>(1)</label></formula><formula xml:id="formula_1">= ∑ ⨂ +<label>(2)</label></formula><p>Pooling layers usually follow the CONV layers. Pooling is a form of non-linear sub-sampling which can bring translation invariance and avoid over-fitting. After a pooling operation, the size of the input map is reduced because the neighboring neurons in a sliding window are merged into one single neuron. The commonly used pooling methods include max pooling and average pooling. In AlexNet model, max pooling is adopted.</p><p>FC layers are usually in the posterior of a CNN model. In an FC layer, each neuron is connected with all the neurons in the previous layer, so the number of weights in FC layers is very large. Equation <ref type="bibr" target="#b2">(3)</ref> shows the operation in an FC layer. Here, and are the i-th neuron of the input vector and the jth neuron of the output vector respectively; is the weight; is the number of the input neurons. For each neuron of the output vector, there is a corresponding bias .</p><formula xml:id="formula_2">= ∑ × +<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. State-of-the-art CNN Model AlexNet</head><p>Nowadays, several large-scale state-of-the-art CNN models have been proposed with high classification accuracy. For instance, AlexNet is proposed in the 2012 Image-Net Large-Scale Vision Recognition Challenge (ILSVRC) <ref type="bibr" target="#b18">[20]</ref> with top-1 and top-5 classification error rates of 37.5% and 17% on ImageNet dataset <ref type="bibr" target="#b0">[1]</ref>. Fig. <ref type="figure" target="#fig_0">1</ref> shows the AlexNet model which includes 5 CONV layers and 3 FC layers. Three max pooling layers follow the first, the second and the fifth CONV layers. ReLU function is applied on every layer except the last one. During the computation, it has been divided into two groups. The configurations of AlexNet are in Table <ref type="table" target="#tab_0">I</ref>, where and are the numbers of input maps and output maps; and are the sizes of the input map and output map;</p><p>is the size for the convolution kernel; is the shifting stride of the kernel during convolution operations.   Reference <ref type="bibr" target="#b8">[9]</ref> has shown that in the VGG model <ref type="bibr" target="#b19">[21]</ref>, CONV layers are computation-intensive while FC layers are memoryintensive. Thus, the operation number and weight number for each layer in the AlexNet model are collected as shown in Fig. <ref type="figure">2</ref> and Fig. <ref type="figure" target="#fig_1">3</ref>, which reflects the computation-intensive feature of CONV layers and the memory-intensive feature of FC layers. On this account, different methods are used to implement CONV layers and FC layers. For CONV layers, a higher parallel degree and more resources are used than FC layers. For FC layers, we focus on reducing the required memory bandwidth. The pooling layers are usually included in the large CONV layers in the architecture design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPLORATION AND OPTIMIZATION</head><p>In this section, the overall architecture of our accelerator is presented firstly. Secondly, the basic computation modules for different types of layers are introduced. Thirdly, we explore the design space and discuss how to find the optimized parallelism strategy for each layer. Finally, an optimization method for FC layers is presented to reduce the required bandwidth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Accelerator Architecture Overview</head><p>As described in the previous section, in the existing FPGAbased CNN implementations, there is still a large space to improve the overall performance. To further improve the performance and increase the throughput, a pipelined structure can be applied on the CNN model with different parallelism strategies for different layers. Fig. <ref type="figure" target="#fig_2">4</ref> is the overall architecture of our accelerator, taken AlexNet as an example.</p><p>In the architecture, the 5 CONV layers and 3 FC layers run in the 8 modules concurrently in a pipelined style. Consequently, numerous input data and weights are required, which leads to a high data access workload. To solve this problem, intermediate results between layers are stored in on-chip buffers. In this way, the data access workload can be reduced significantly. Weights are stored in the external memory because the number of weights is too large. Furthermore, on-chip buffers also facilitate data reuse. There are 2 ping-pong buffers for each stage, where the former layer may write to or read from one buffer while the next layer reads data from the other buffer. Dual-port memory blocks are used as the ping-pong buffers. Hence, ping-pong buffers can avoid the overlap and improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Basic Computation Modules</head><p>To implement a CONV layer, various structures have been developed as listed in <ref type="bibr" target="#b15">[17]</ref>, where the input data propagate in a systolic style in most of the structures. This work adopts a 1dimensional processing element (1-D PE) structure in Fig. <ref type="figure" target="#fig_3">5</ref>, where 3 × 3 kernel size and stride 1 are used as an example. Fig. <ref type="figure" target="#fig_3">5</ref> shows the 1-D PE for applying 3 × 3 convolution operations on a 5 × 5 input map. The input data flow into the PE row by row as shown by the arrows in the input map. A 1-D PE is mainly composed of three accumulators and three multipliers along with three multiplexers to choose the required weights. The computation process is as follows. First, the input data pass through three shift registers. When the first input arrives at the input of the rightmost multiplier, the 3 multipliers start calculating. After 3 clock cycles, the rightmost multiplier generates a temporary result of the first convolution window with value × + × + × , while the other two multipliers generate the temporary results of and . When the first three rows finish calculating, the final results of , and are generated. Other results in the output map are generated in the same way. As there are overlaps between sliding windows, all the rows should be read for 3 times except the 2 rows in the top and the bottom of the input map. If a higher speed up ratio is required, the 1-D PE can be extended into a 2-D PE, as illustrated in Fig. <ref type="figure" target="#fig_4">6</ref>. A 2-D PE is composed of multiple 1-D PEs. During computing, the input data is broadcast to these 1-D PEs. Thus, each row of the input map can be calculated in multiple 1-D PEs concurrently by applying different weights. In this way, each row needs to be read for only once. Hence, the speed up ratio can be increased by three times compared to 1-D PE in this case.</p><formula xml:id="formula_3">CONV1 CONV2 CONV3 CONV4 CONV5 FC6 FC7 FC8 0 1 2 3 4 5</formula><p>x 10   This structure is generally applicable for any convolution operations. The numbers of the used multipliers, accumulators and multiplexers change with the kernel size and shifting stride. Given the kernel size and the corresponding shifting stride , the numbers of multipliers, accumulators and multiplexers required for 1-D and 2-D PE are in ( <ref type="formula">4</ref>) and ( <ref type="formula" target="#formula_4">5</ref>).</p><formula xml:id="formula_4">= / (4) =<label>(5)</label></formula><p>For a pooling layer, the architecture is similar with the CONV layer, only the convolution operations should be replaced by the max or average operations.</p><p>FC layer can be regarded as matrix multiplication. The FC6 layer in AlexNet is taken as an example to illustrate the computing pattern, where there are a 9216-dimensional input feature vector and a 4096-dimensional output feature vector. The weights are expressed as a 9216 × 4096 matrix in Fig. <ref type="figure" target="#fig_5">7</ref> (a). To implement such a large-scale matrix multiplication with limited hardware resources, we divide it into multiple smallscale matrix multiplications as shown in Fig. <ref type="figure" target="#fig_5">7 (b)</ref>. Firstly, to are taken as the input vector to generate the temporary results of to by applying × weights. Then we take to with another × weights as the input to update the  temporary results of to . After the 9216 input data and the first n rows in the weight matrix have been traversed, the final results of to can be generated. The other output results can be generated in the same way.</p><formula xml:id="formula_5">× = … x 1 -x m y 1 -y n m m n n × = 1*</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Parallelism Space Exploration</head><p>As CNN models become larger and deeper, various parallelism strategies have emerged with different performance and resource utilization. Thus, it is necessary to explore the parallelism space. This work adopts 3 types of parallelism to implement a CNN layer as proposed in <ref type="bibr" target="#b7">[8]</ref>: parallelism within a convolution operation, parallelism among multiple input maps and parallelism among multiple output maps. The first type of parallelism is exhibited in the basic computation modules as in 1-D PE and 2-D PE, where the parallel degrees are and respectively. Thus, we mainly discuss the other two types. To determine the parallelism strategy for a layer means to find the input map parallelism and output map parallelism. There are many factors to consider, such as on-chip resource limitation and the data dependency between layers. In addition, to maximize the throughput in a pipelined structure, the time cost on each pipeline stage should be approximately the same.</p><p>In this design, 1-D PE is adopted as the basic computation module in CONV layers, so the parallelism inside a convolution operation is ⁄ . Moreover, the input map should be read for ⁄ times to apply one convolution kernel.</p><p>and are used to denote the input map parallelism and the output map parallelism. Given a clock frequency f, the product of and for layer i can be expressed in <ref type="bibr" target="#b5">(6)</ref>, which is the product of the computation time for a single input map, the number of input maps and the number of output maps in the i-th layer. In ( <ref type="formula" target="#formula_6">6</ref>), represents the time used in each pipeline stage;</p><p>is the size of the input map. For an FC layer, and can be regarded as the input number and output number of neurons respectively;</p><p>, and equal to 1. Our target is to determine the proper and for each layer.</p><formula xml:id="formula_6">× = × × × ×<label>(6)</label></formula><p>Since DSP blocks in FPGA are used to implement multiplication operations, the total number of multipliers should be subject to the DSP block resources on chip as in <ref type="bibr" target="#b6">(7)</ref>. is the number of layers in a CNN model;</p><p>is the total number of DSPs on an FPGA chip. In addition, as on-chip Block RAMs (BRAMs) are used to store intermediate results between layers, the on-chip memory resources should be considered. Firstly, the BRAMs used for one layer should be enough to store the output maps of this layer as described in <ref type="bibr" target="#b7">(8)</ref>,where is the number of BRAMs used in the i-th layer;</p><p>is the storage capacity of one BRAM. Secondly, should be larger than the input parallelism of the next layer and the output parallelism of this layer as in ( <ref type="formula">9</ref>) and <ref type="bibr" target="#b9">(10)</ref>. Thus, multiple data can be written into or read from the BRAMs simultaneously. Thirdly, the BRAMs used in all layers should be subject to the on-chip BRAM resources as in <ref type="bibr" target="#b10">(11)</ref>. Besides ( <ref type="formula">7</ref>) to <ref type="bibr" target="#b10">(11)</ref>, there are some other constraints we need to pay attention to in practice. One is that and should be a divisor of and respectively. Moreover, should be a multiple of and . These two constraints can reduce the control complexity significantly.</p><formula xml:id="formula_7">∑ × × ≤ (7) ≥ ×( )<label>(8)</label></formula><formula xml:id="formula_8">≥ (9) ≥ (10) ∑ ≤<label>(11)</label></formula><p>Based on the above analysis, a methodology to determine the optimized parallelism strategy for each layer can be proposed with the following 3 steps. Firstly, given a value for , the product of × for each layer can be calculated by <ref type="bibr" target="#b5">(6)</ref>. If <ref type="bibr" target="#b6">(7)</ref> is not satisfied, should be increased until ( <ref type="formula">7</ref>) is satisfied. Secondly, equation ( <ref type="formula" target="#formula_7">8</ref>) is used to calculate the minimum number of BRAMs for each layer. Thirdly, equations ( <ref type="formula">9</ref>), ( <ref type="formula">10</ref>), <ref type="bibr" target="#b10">(11)</ref> and the product of × are used to determine the and for layer i. In this step, , and should be adjusted to make and divisors of and . In addition, and should be the divisors of . In this way, the control complexity can be reduced significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Optimization for Memory Bandwidth</head><p>During computing, 8 layers read weights from the off-chip memory simultaneously, where high memory bandwidth is still required. Fig. <ref type="figure" target="#fig_1">3</ref> has shown that FC layers contribute to most of the data accesses. Therefore, a batch-based computing method for FC layers is employed. Fig. <ref type="figure">8</ref> illustrates the scenario in which multiple input feature vectors are computed as a batch in FC layers. In Fig. <ref type="figure">8</ref>, N is the number of input feature vectors, which can be regarded as the batch size. As a result, the operation amount can be increased by N times without increasing the number of data accesses for weights. Hence, the required memory bandwidth can be reduced.</p><p>In Fig. <ref type="figure">9</ref>, the input parallelism and output parallelism are represented by m and n respectively. With × DSP blocks, × multiplications can operate concurrently, where × weights are required. Thus, it takes N clock cycles to operate × × multiplications. Moreover, during the N clock cycles, the next × weights need to be ready for the next × × multiplications. Therefore, N should be no less than the number of required clock cycles to read × weights, as in <ref type="bibr" target="#b11">(12)</ref>. In this way, the FC layers can compute continuously without waiting for weights. In <ref type="bibr" target="#b11">(12)</ref>, denotes the batch size for the i-th FC layer; ℎ is the bit width of the weight; ℎ is the required memory bandwidth for the i-th layer. The batch size should be subject to the memory bandwidth provided by hardware which is denoted by ℎ , as shown in <ref type="bibr" target="#b12">(13)</ref>. For CONV layers and FC layers, the required memory bandwidth can be calculated by the equations in ( <ref type="formula">14</ref>) and (15). Batch-based computing reduces data access, but sacrifices the latency for the first output result. </p><formula xml:id="formula_9">∑ ℎ ≤ ℎ (<label>13</label></formula><formula xml:id="formula_10">) = × × × × / (14) = × ×<label>(15)</label></formula><p>The batch-based computing method can bring another issue because the required on-chip buffers for FC layers are expanded by N times. To address this issue, the computing pattern of FC layers is rearranged to reduce the buffer requirement. Besides the computing pattern in Fig. <ref type="figure">9</ref>, another pattern is employed as in Fig. <ref type="figure" target="#fig_0">10</ref>. In this pattern, the temporary results of all the output neurons are generated first by using to only as the input. Next, the temporary results of all the output neurons are updated by taking to as the input. The window with size × in the weight matrix shifts horizontally which distinguishes the vertically shifting pattern in Fig. <ref type="figure">9</ref>. In this work, these two patterns are both adopted to implement FC layers by applying them alternately on FC layers. For the first FC layer, the vertically shifting pattern is used to generate to . Thus, the second FC layer can start with to as the input by applying the horizontally shifting pattern. Since the second FC layer starts before the whole input vector is generated from the first FC layer, a few BRAMs are enough to store the × neurons. However, in the second FC layer, a number of BRAMs are still required to store the temporary results of all the output neurons. For the third FC layer, the vertically shifting pattern is applied as in the first FC layer. In this way, the required buffers can be reduced remarkably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Implementation of the Large-Scale AlexNet</head><p>As a case study, the proposed architecture and methodology are used to implement the CNN model, AlexNet, where 16-bit fixed point data are adopted for weights, input data and the intermediate results between layers. In addition, 18Kb BRAM is used as the basic memory block for analysis. Based on the AlexNet model, the relations between the number of DSPs, BRAMs, the required memory bandwidth and the number of clock cycles in one pipeline stage are evaluated as in Fig. <ref type="figure" target="#fig_7">11 (a) -(c</ref>). Here, is the number of clock cycles in a pipeline stage, which equals to the value of × . is the time cost on each pipeline stage and is the clock frequency. The batch size is set as the controlling variable with a value 28 while the value of changes. Furthermore, the relations between the number of used DSPs, the number of used BRAMs, the required bandwidth and the batch size are also evaluated, which is described in Fig. <ref type="figure" target="#fig_7">11 (d) -(f</ref>). In this evaluation, is set as the controlling variable with value 400,000 while the batch size changes.</p><p>From Fig. <ref type="figure" target="#fig_7">11</ref> (b), we can find that when the batch size is fixed, there is no obvious relevancy between the number of required BRAMs and the number of clock cycles in a pipeline stage. However, if becomes smaller, which means higher parallel degree required, more DSPs are employed along with a higher required memory bandwidth to support the data accesses for weights as shown in Fig. <ref type="figure" target="#fig_7">11 (a,</ref><ref type="figure">c</ref>). The number of DSPs is not affected by the batch size, while the number of BRAMs increases linearly with the growth of the batch size as in Fig. <ref type="figure" target="#fig_7">11 (d,</ref><ref type="figure">e</ref>). In addition, in Fig. <ref type="figure" target="#fig_7">11 (f)</ref>, the bandwidth can be reduced significantly by applying the batch-based computing method. Nonetheless, when N is larger than 30, there is no more obvious improvement for data access while the number of used BRAMs still keeps increasing. Hence, the batch size is not necessary to be larger than 30 in AlexNet model implementation.</p><p>Based on the above analysis, the large-scale CNN model AlexNet is implemented with batch size 28. In this design, 400,000 is adopted as the value of . If a smaller is adopted, a higher throughput and resource utilization can be achieved. However, a larger memory bandwidth will be demanded. For this reason, 400,000 is adopted to balance the data access workload and the throughput along with the resource utilization. Moreover, 1-D PE is employed to implement the convolution operations in these 5 CONV layers. The input parallelism, the output parallelism and the number of used BRAMs for each layer are listed in Table <ref type="table" target="#tab_3">II</ref>. In layer CONV2, 132 (4 + 64 × 2) BRAMs are used because extra 4 BRAMs are required to store the intermediate results before pooling. In layer CONV5, extra 64 BRAMs are needed for the same reason as in CONV2. However, no more BRAMs are needed to store the results before pooling in CONV1 because the input parallelism equals to the number of input maps in this layer. By applying these two computing patterns presented in Section IV-D, the numbers of BRAMs used in layer FC6 and layer FC8 can be reduced remarkably. In this way, the total number of required BRAMs for the 3 FC layers are 244. If one computing pattern is applied only, 504 BRAMs will be needed. Hence, the number of BRAMs is reduced by 55.6% in FC layers. Moreover, by applying the proposed batch-based computing method with batch size 28, the total required memory bandwidth to read weights is reduced to 288 × bits per second, while if the batch-based method is not used, 3472 × bits per second are required.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. IMPLEMENTATION DETAILS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. System Architecture</head><p>As illustrated in Fig. <ref type="figure" target="#fig_0">12</ref>, the whole system fits in one FPGA chip along with two 4GB DDR3 external memory blocks with bandwidth 14.9 GB/s to store the input data and the weights. The host sends the weights first to the DDR3 interface through 10 Gigabit Ethernet. Then the weights are stored in one of the DDR3 blocks. Next, the host sends the input image data which is stored in another DDR3 block. This DDR3 acts as a large buffer, so that the processing speed of the accelerator can match the data sending speed. In the accelerator, 8 layers are implemented by 8 modules where 2 ping-pong buffers are utilized to store the intermediate data. During computing, these 8 layers request for weights to the "Read Weight Controller". Then, the controller makes an arbitration within the 8 request signals and read weights from the external memory. Here, 156MHz clock is used for the accelerator and the 10 Gigabit Ethernet. Given the with value 400,000, which has been mentioned in Section IV-E, the time cost in each pipeline stage is 2.56ms. In addition, as references <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b21">23]</ref> have shown that low precision quantization is sufficient for CNN implementations,16-bit fixed point data are adopted for the input data, weights and the intermediate data. In Table <ref type="table" target="#tab_3">II</ref>, it is shown that the required memory bandwidth for weights is 288 × bit/s, which is 45 Gb/s with frequency 156MHz, and can be supported by the DDR3. The proposed accelerator can receive the image data from the host and send back the final 1000-dimensional feature vector to the host which then ranks the top 5 classification results and display the results on a website as in Fig. <ref type="figure" target="#fig_2">14</ref>. ImageNet-1K is the test set in this work. The execution time for each pipeline stage is described in Fig. <ref type="figure" target="#fig_9">13</ref>. Firstly, the host sends the weights to the DDR3 memory. Then the 8 layers work in the pipelined style to maximize the throughput. The batch-based computing method is employed in FC layers with batch size 28. Hence, 28 images can be executed together as a batch in FC layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Architecture for a Typical Layer</head><p>The architecture for a typical CNN layer is in Fig. <ref type="figure" target="#fig_3">15</ref> where the input parallelism and output parallelism are and respectively. Therefore, input maps are read concurrently. For each input map, it is broadcast to 1-D PEs to generate output maps concurrently. Then, the output data of the 1-D PEs for the same output map are added together by an adder tree. Before all the input maps are traversed, temporary results are generated from the adder tree and stored in the BRAMs. Then, the temporary results are read from the BRAMs to add the newly generated temporary results until all the input maps are traversed. Then, the pooling function and the activation function ReLU are applied on the "final result" as in Fig. <ref type="figure" target="#fig_3">15</ref> After that, the generated output maps will be stored in the BRAMs. For some layers, pooling and ReLU are not used, so that they can be removed from the architecture. To implement an FC layer, the 1-D PE in Fig. <ref type="figure" target="#fig_3">15</ref> should be replaced by one single DSP block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTAL RESULTS</head><p>In this section, we quantitatively make a comparison between our design and some previous implementations. This design is implemented on Xilinx VC709 with Vivado (v2015.1). The resource utilization after routing is shown in Table <ref type="table" target="#tab_4">III</ref>.</p><p>Since references <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr">15]</ref> have also implemented an accelerator based on the large-scale CNN model AlexNet, a comparison is made between our work and theirs. In <ref type="bibr" target="#b9">[10,</ref><ref type="bibr">15]</ref>, the complete AlexNet model is implemented while <ref type="bibr" target="#b6">[7]</ref> has only implemented the 5 CONV layers. Therefore, the processing time per image of work <ref type="bibr" target="#b6">[7]</ref> is listed in Table IV by using the time for processing the 5 CONV layers only. Furthermore, we compare our work with Caffe tool <ref type="bibr" target="#b22">[24]</ref>, which is running on Intel Xeon E5-2637 CPU (3.5GHz) and GTX TITAN X GPU. The results are included in Table <ref type="table" target="#tab_0">IV</ref>. In our design, by using the pipelined structure among layers, the time of processing one image is 2.56 ms under the peak performance. Thus, the corresponding frame per second (FPS) is 391, which is 7.82x higher than work <ref type="bibr" target="#b9">[10]</ref>, 2.92x higher than work [15] and 78x higher than Caffe tool running on CPU. Although the throughput of Caffe tool running on GPU is 2x higher than our design, it consumes 8.3x more power. Reference <ref type="bibr" target="#b9">[10]</ref> has just measured the power consumption of P395-D8 board after programming AlexNet configuration as 19.1W without measuring the power during classification, so the power for performing classification is larger than 19.1W. In this design, the power consumption is measured on FPGA board during the classification, where the peak value is 30.2W. Specifically, the energy for processing one image is compared by taking the product of the processing time per image and the power consumption. To process one image, our design consumes 4.2x lower energy than GPU, 329.9x lower energy than CPU, 2.4x lower energy than work [15] and 5x lower energy than the most recently work in <ref type="bibr" target="#b9">[10]</ref>.</p><p>Furthermore, we compare our work with some previous work on CNN implementations as illustrated in Table <ref type="table">V</ref>. Our work achieves a high performance with 565.94 GOP/s, which significantly outperforms the previous work. In addition, this implementation can achieve the highest power efficiency with value 22.15 GOP/s/W in comparison with the previous designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS</head><p>This work proposes an end-to-end CNN accelerator with all the layers working concurrently in a pipelined structure, which can improve the performance remarkably. In addition, a methodology is proposed to find the optimized parallelism strategy for each layer. In FC layers, a batch-based computing method is adopted to reduce the required memory bandwidth. Further, two computing patterns are applied on FC layers which can reduce the on-chip buffer requirement significantly. As a case study, a large-scale CNN model AlexNet is implemented on Xilinx VC709; it achieves 565.94 GOP/s and 391 FPS which outperforms the previous work. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. An overview of the large-scale CNN model AlexNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. Number of weights for different layers in AlexNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.4. An overview of the accelerator architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. The 1-dimensional Processing Element (1-D PE) to implement convolution operations with a 3 × 3 weight kernel and a 5 × 5 input map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Architecture of the 2-dimensional Processing Element (2-D PE).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig.7. Matrix multiplication in an FC layer, taken the FC6 layer in AlexNet model as an example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .Fig. 9 .Fig. 10 .</head><label>8910</label><figDesc>Fig.8. Batch-based computing method in the FC layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 .</head><label>11</label><figDesc>Fig.11. An analysis on the relations between number of DSPs, number of BRAMs , batch size, required bandwidth and based on AlexNet model. In (a-c), batch size N is the controlling variable with value 28 while changes; in (d-f), is the controlling variable with value 400,000 while N changes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Fig.12. System Architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 .</head><label>13</label><figDesc>Fig.13. Execution time of each layer working in a pipelined style.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 14 Fig. 15</head><label>1415</label><figDesc>Fig.14 Implementation platform and the result display. Xilinx VC709 is used to implement the accelerator; the host sends data and receives results with the 10 gigabit Ethernet; the classification results are displayed on a website.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I .</head><label>I</label><figDesc>CONFIGURATIONS OF DIFFERENT LAYERS IN ALEXNET</figDesc><table><row><cell>Layer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Stride</cell></row><row><cell>CONV1</cell><cell>3</cell><cell>96</cell><cell>227x227</cell><cell>55x55</cell><cell>11x11</cell><cell>4</cell></row><row><cell>POOL1</cell><cell>96</cell><cell>96</cell><cell>55x55</cell><cell>27x27</cell><cell>3x3</cell><cell>2</cell></row><row><cell>CONV2</cell><cell>48</cell><cell>256</cell><cell>27x27</cell><cell>27x27</cell><cell>5x5</cell><cell>1</cell></row><row><cell>POOL2</cell><cell>256</cell><cell>256</cell><cell>27x27</cell><cell>13x13</cell><cell>3x3</cell><cell>2</cell></row><row><cell>CONV3</cell><cell>256</cell><cell>384</cell><cell>13x13</cell><cell>13x13</cell><cell>3x3</cell><cell>1</cell></row><row><cell>CONV4</cell><cell>192</cell><cell>384</cell><cell>13x13</cell><cell>13x13</cell><cell>3x3</cell><cell>1</cell></row><row><cell>CONV5</cell><cell>192</cell><cell>256</cell><cell>13x13</cell><cell>13x13</cell><cell>3x3</cell><cell>1</cell></row><row><cell>POOL5</cell><cell>256</cell><cell>256</cell><cell>13x13</cell><cell>6x6</cell><cell>3x3</cell><cell>2</cell></row><row><cell>FC6</cell><cell>9216</cell><cell>4096</cell><cell>1x1</cell><cell>1x1</cell><cell>--</cell><cell>--</cell></row><row><cell>FC7</cell><cell>4096</cell><cell>4096</cell><cell>1x1</cell><cell>1x1</cell><cell>--</cell><cell>--</cell></row><row><cell>FC8</cell><cell>4096</cell><cell>1000</cell><cell>1x1</cell><cell>1x1</cell><cell>--</cell><cell>--</cell></row></table><note><p>Fig.2. Number of operations for different layers in AlexNet.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell>Number of DSPs</cell><cell>0.5 1 1.5 2</cell><cell>x 10 4</cell><cell></cell><cell cols="5">Number of DSPs -Num cc</cell><cell></cell><cell>Number of BRAMs</cell><cell>1420 1440 1460 1480 1500</cell><cell></cell><cell></cell><cell cols="5">Number of BRAMs -Num cc</cell><cell></cell><cell>Required Bandwidth (Kbit/s)</cell><cell>0.5Xf 1.0Xf 1.5Xf 2.0Xf</cell><cell></cell><cell></cell><cell cols="5">Required Bandwidth -Num cc</cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>12</cell><cell>14</cell><cell>16</cell><cell>1400</cell><cell>0</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>12</cell><cell>14</cell><cell>16</cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>12</cell><cell>14</cell><cell>16</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Num cc</cell><cell></cell><cell></cell><cell></cell><cell>x 10 5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Num cc</cell><cell></cell><cell></cell><cell cols="2">x 10 5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Num cc</cell><cell></cell><cell></cell><cell cols="2">x 10 5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>.</cell><cell cols="10">CONFIGURATION FOR ALEXNET HARDWARE IMPLEMENTATION</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Layer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Number Of BRAMs</cell><cell></cell><cell cols="2">Size of 1-D PE</cell><cell cols="2">Number of DSPs</cell><cell>Required bandwidth (bit/s)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CONV1</cell><cell>3</cell><cell>48</cell><cell></cell><cell></cell><cell cols="3">192(96 × 2)</cell><cell>3</cell><cell></cell><cell cols="2">432</cell><cell>16 ×</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">CONV2 32</cell><cell>4</cell><cell></cell><cell cols="4">132(4 + 64 × 2)</cell><cell>5</cell><cell></cell><cell cols="2">640</cell><cell>16 ×</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">CONV3 32</cell><cell>4</cell><cell></cell><cell></cell><cell cols="3">128(64 × 2)</cell><cell>3</cell><cell></cell><cell cols="2">384</cell><cell>48 ×</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">CONV4 32</cell><cell>3</cell><cell></cell><cell></cell><cell cols="3">192(96 × 2)</cell><cell>3</cell><cell></cell><cell cols="2">288</cell><cell>32 ×</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">CONV5 32</cell><cell>2</cell><cell></cell><cell></cell><cell cols="3">576 (64 + 256 × 2)</cell><cell>3</cell><cell></cell><cell cols="2">192</cell><cell>32 ×</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FC6</cell><cell>16</cell><cell>8</cell><cell></cell><cell></cell><cell cols="2">16(8 × 2)</cell><cell></cell><cell>--</cell><cell></cell><cell cols="2">128</cell><cell>64 ×</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FC7</cell><cell>8</cell><cell>8</cell><cell></cell><cell></cell><cell cols="3">224(112 × 2)</cell><cell>--</cell><cell></cell><cell cols="2">64</cell><cell>64 ×</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FC8</cell><cell>8</cell><cell>2</cell><cell></cell><cell></cell><cell cols="2">4 (2 × 2)</cell><cell></cell><cell>--</cell><cell></cell><cell cols="2">16</cell><cell>16 ×</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III</head><label>III</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>.</cell><cell>RESOURCE UTILIZATION</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Resource Utilization on Xilinx VC709</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FF</cell><cell>LUT</cell><cell>DSP48</cell><cell>BRAM(18Kb)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Available</cell><cell>866400</cell><cell>433200</cell><cell>3600</cell><cell>2940</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Utilization</cell><cell>262703</cell><cell>273805</cell><cell>2144</cell><cell>1913</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Percent(%)</cell><cell>30.32</cell><cell>63.21</cell><cell>59.56</cell><cell>65.07</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE IV.</cell><cell>COMPARISON WITH OTHER IMPLEMENTATIONS BASED-ON ALEXNET</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Platform</cell><cell>Processing time/ image(ms)</cell><cell>FPS</cell><cell>Power (W)</cell><cell>Energy/image (J)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Caffe CPU</cell><cell cols="2">Intel Xeon E5-2637</cell><cell>195</cell><cell>5</cell><cell>130</cell><cell>25.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Caffe GPU</cell><cell></cell><cell>GTX TITAN X</cell><cell>1.3</cell><cell>769</cell><cell>250</cell><cell>0.325</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>FPGA 2015 [7]</cell><cell></cell><cell>Virtex-7 VX485T</cell><cell>21.61 layers) (5 CONV</cell><cell>--</cell><cell>18.61</cell><cell>&gt;0.402</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>FPGA 2016 [10]</cell><cell></cell><cell>Stratix-V GSD8</cell><cell>20.1</cell><cell>50</cell><cell>&gt;19.1</cell><cell>&gt;0.384</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Microsoft Catapult [15]</cell><cell cols="2">Catapult Server+ Stratix V D5</cell><cell>7.46</cell><cell>134</cell><cell>25</cell><cell>0.187</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>This Work</cell><cell></cell><cell>Virtex-7 VC709</cell><cell>2.56</cell><cell>391</cell><cell>30.2</cell><cell>0.077</cell></row><row><cell></cell><cell>TABLE V.</cell><cell cols="5">COMPARISON WITH PREVIOUS CNN IMPLEMENTATIONS</cell></row><row><cell></cell><cell>ACM2010[8]</cell><cell></cell><cell>FPGA2015[7]</cell><cell cols="3">FPGA2016[10]</cell><cell>FPGA2016[9]</cell><cell>This Work</cell></row><row><cell>Platform</cell><cell cols="2">Virtex-5 SX240T</cell><cell>Virtex-7 VX485T</cell><cell cols="3">Stratix-V GSD8</cell><cell>Zynq XC7Z045</cell><cell>Virtex-7 VC709</cell></row><row><cell>Clock (MHz)</cell><cell>120</cell><cell></cell><cell>100</cell><cell cols="2">120</cell><cell>150</cell><cell>156</cell></row><row><cell>Precision</cell><cell>48-bit fixed</cell><cell></cell><cell>32-bit float</cell><cell cols="3">(8-16)-bit fixed</cell><cell>16-bit fixed</cell><cell>16-bit fixed</cell></row><row><cell>CNN Size (GOP)</cell><cell>0.52</cell><cell></cell><cell>1.33</cell><cell cols="2">30.9</cell><cell>30.76</cell><cell>1.45</cell></row><row><cell>Performance (GOP/s)</cell><cell>16</cell><cell></cell><cell>61.62</cell><cell cols="2">117.8</cell><cell>136.97</cell><cell>565.94</cell></row><row><cell>Power (W)</cell><cell>14</cell><cell></cell><cell>18.61</cell><cell cols="2">25.8</cell><cell>9.63</cell><cell>30.2</cell></row><row><cell>Power Efficiency (GOP/s/W)</cell><cell>1.14</cell><cell></cell><cell>3.31</cell><cell cols="2">4.57</cell><cell>14.22</cell><cell>22.15</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work is supported by National Natural Science Foundation of China 61131001. We also appreciate careful works and the constructive suggestions of the anonymous reviewers.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep Learning Face Representation from Predicting 10,000 Classes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1891" to="1898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep Learning Face Representation by Joint Identification-Verification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1988" to="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3D Convolutional Neural Networks for Automatic Human Action Recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks for Speech Recognition</title>
		<author>
			<persName><forename type="first">O</forename><surname>Abdel-Hamid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Audio Speech &amp; Language Processing</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Optimizing FPGA-based accelerator design for deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Symposium on Field-Programmable Gate Arrays</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="161" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A dynamically configurable coprocessor for convolutional neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chakradhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sankaradas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Sigarch Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="247" to="257" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Going deeper with embedded FPGA platform for convolutional neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Symposium on Field-Programmable Gate Arrays</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Throughput-optimized OpenCL-based FPGA accelerator for large-scale convolutional neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Symposium on Field-Programmable Gate Arrays</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Memory-centric accelerator design for Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Peemen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st International Conference on Computer Design (ICCD)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="13" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Massively Parallel Coprocessor for Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sankaradas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jakkula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Application-specific Systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="53" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A programmable parallel accelerator for learning and classication</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cadambi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Becchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th International Conference on Parallel Architectures and Compilation Techniques</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="273" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">CNP: An FPGA-based processor for Convolutional Networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Poulet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Field Programmable Logic &amp; Applications</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="32" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">DianNao: a small-footprint high-throughput accelerator for ubiquitous machine-learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Sigplan Notices</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="269" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ShiDianNao: Shifting vision processing closer to the sensor</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fasthuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Symposium Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">DaDianNao: A Machine-Learning Supercomputer</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="609" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">PuDianNao: A Polyvalent Machine Learning Accelerator</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Sigplan Notices</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="369" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<editor>Eprint Arxiv</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Low Precision Storage for Deep Learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deep Learning with Limited Numerical Precision</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02551</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional Architecture for Fast Feature Embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eprint Arxiv</title>
		<imprint>
			<biblScope unit="page" from="675" to="678" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
