<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
							<email>tarvaina@cai.fi</email>
							<affiliation key="aff0">
								<orgName type="institution">The Curious AI Company</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Harri</forename><surname>Valpola</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Curious AI Company</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The recently proposed Temporal Ensembling has achieved state-of-the-art results in several semi-supervised learning benchmarks. It maintains an exponential moving average of label predictions on each training example, and penalizes predictions that are inconsistent with this target. However, because the targets change only once per epoch, Temporal Ensembling becomes unwieldy when learning large datasets. To overcome this problem, we propose Mean Teacher, a method that averages model weights instead of label predictions. As an additional benefit, Mean Teacher improves test accuracy and enables training with fewer labels than Temporal Ensembling. Without changing the network architecture, Mean Teacher achieves an error rate of 4.35% on SVHN with 250 labels, outperforming Temporal Ensembling trained with 1000 labels. We also show that a good network architecture is crucial to performance. Combining Mean Teacher and Residual Networks, we improve the state of the art on CIFAR-10 with 4000 labels from 10.55% to 6.28%, and on ImageNet 2012 with 10% of the labels from 35.24% to 9.11%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning has seen tremendous success in areas such as image and speech recognition. In order to learn useful abstractions, deep learning models require a large number of parameters, thus making them prone to over-fitting (Figure <ref type="figure" target="#fig_0">1a</ref>). Moreover, adding high-quality labels to training data manually is often expensive. Therefore, it is desirable to use regularization methods that exploit unlabeled data effectively to reduce over-fitting in semi-supervised learning.</p><p>When a percept is changed slightly, a human typically still considers it to be the same object. Correspondingly, a classification model should favor functions that give consistent output for similar data points. One approach for achieving this is to add noise to the input of the model. To enable the model to learn more abstract invariances, the noise may be added to intermediate representations, an insight that has motivated many regularization techniques, such as Dropout <ref type="bibr" target="#b26">[27]</ref>. Rather than minimizing the classification cost at the zero-dimensional data points of the input space, the regularized model minimizes the cost on a manifold around each data point, thus pushing decision boundaries away from the labeled data points (Figure <ref type="figure" target="#fig_0">1b</ref>).</p><p>Since the classification cost is undefined for unlabeled examples, the noise regularization by itself does not aid in semi-supervised learning. To overcome this, the model <ref type="bibr" target="#b19">[20]</ref> evaluates each data point with and without noise, and then applies a consistency cost between the two predictions. In this case, the model assumes a dual role as a teacher and a student. As a student, it learns as before; as a teacher, it generates targets, which are then used by itself as a student for learning. Since the model itself generates targets, they may very well be incorrect. If too much weight is given to the generated targets, the cost of inconsistency outweighs that of misclassification, preventing the learning of new (e) An ensemble of models gives an even better expected target. Both Temporal Ensembling and the Mean Teacher method use this approach. information. In effect, the model suffers from confirmation bias (Figure <ref type="figure" target="#fig_0">1c</ref>), a hazard that can be mitigated by improving the quality of targets.</p><p>There are at least two ways to improve the target quality. One approach is to choose the perturbation of the representations carefully instead of barely applying additive or multiplicative noise. Another approach is to choose the teacher model carefully instead of barely replicating the student model. Concurrently to our research, Miyato et al. <ref type="bibr" target="#b14">[15]</ref> have taken the first approach and shown that Virtual Adversarial Training can yield impressive results. We take the second approach and will show that it too provides significant benefits. To our understanding, these two approaches are compatible, and their combination may produce even better outcomes. However, the analysis of their combined effects is outside the scope of this paper.</p><p>Our goal, then, is to form a better teacher model from the student model without additional training. As the first step, consider that the softmax output of a model does not usually provide accurate predictions outside training data. This can be partly alleviated by adding noise to the model at inference time <ref type="bibr" target="#b3">[4]</ref>, and consequently a noisy teacher can yield more accurate targets (Figure <ref type="figure" target="#fig_0">1d</ref>). This approach was used in Pseudo-Ensemble Agreement <ref type="bibr" target="#b1">[2]</ref> and has lately been shown to work well on semi-supervised image classification <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22]</ref>. Laine &amp; Aila <ref type="bibr" target="#b12">[13]</ref> named the method the ⇧ model; we will use this name for it and their version of it as the basis of our experiments.</p><p>The ⇧ model can be further improved by Temporal Ensembling <ref type="bibr" target="#b12">[13]</ref>, which maintains an exponential moving average (EMA) prediction for each of the training examples. At each training step, all the EMA predictions of the examples in that minibatch are updated based on the new predictions. Consequently, the EMA prediction of each example is formed by an ensemble of the model's current version and those earlier versions that evaluated the same example. This ensembling improves the quality of the predictions, and using them as the teacher predictions improves results. However, since each target is updated only once per epoch, the learned information is incorporated into the training process at a slow pace. The larger the dataset, the longer the span of the updates, and in the case of on-line learning, it is unclear how Temporal Ensembling can be used at all. (One could evaluate all the targets periodically more than once per epoch, but keeping the evaluation span constant would require O(n 2 ) evaluations per epoch where n is the number of training examples.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Mean Teacher</head><p>To overcome the limitations of Temporal Ensembling, we propose averaging model weights instead of predictions. Since the teacher model is an average of consecutive student models, we call this the Mean Teacher method (Figure <ref type="figure" target="#fig_1">2</ref>). Averaging model weights over training steps tends to produce a more accurate model than using the final weights directly <ref type="bibr" target="#b17">[18]</ref>. We can take advantage of this during training to construct better targets. Instead of sharing the weights with the student model, the teacher model uses the EMA weights of the student model. Now it can aggregate information after every step instead of every epoch. In addition, since the weight averages improve all layer outputs, not just the top output, the target model has better intermediate representations. These aspects lead to two practical advantages over Temporal Ensembling: First, the more accurate target labels lead to a faster feedback loop between the student and the teacher models, resulting in better test accuracy. Second, the approach scales to large datasets and on-line learning.</p><p>More formally, we define the consistency cost J as the expected distance between the prediction of the student model (with weights ✓ and noise ⌘) and the prediction of the teacher model (with weights ✓ 0 and noise ⌘ 0 ).</p><formula xml:id="formula_0">J(✓) = E x,⌘ 0 ,⌘ h kf (x, ✓ 0 , ⌘ 0 ) f (x, ✓, ⌘)k 2 i</formula><p>The difference between the ⇧ model, Temporal Ensembling, and Mean teacher is how the teacher predictions are generated. Whereas the ⇧ model uses ✓ 0 = ✓, and Temporal Ensembling approximates f (x, ✓ 0 , ⌘ 0 ) with a weighted average of successive predictions, we define ✓ 0 t at training step t as the EMA of successive ✓ weights:</p><formula xml:id="formula_1">✓ 0 t = ↵✓ 0 t 1 + (1 ↵)✓ t</formula><p>where ↵ is a smoothing coefficient hyperparameter. An additional difference between the three algorithms is that the ⇧ model applies training to ✓ 0 whereas Temporal Ensembling and Mean Teacher treat it as a constant with regards to optimization.</p><p>We can approximate the consistency cost function J by sampling noise ⌘, ⌘ 0 at each training step with stochastic gradient descent. Following Laine &amp; Aila <ref type="bibr" target="#b12">[13]</ref>, we use mean squared error (MSE) as the consistency cost in most of our experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>To test our hypotheses, we first replicated the ⇧ model <ref type="bibr" target="#b12">[13]</ref> in TensorFlow <ref type="bibr" target="#b0">[1]</ref> as our baseline. We then modified the baseline model to use weight-averaged consistency targets. The model architecture is a 13-layer convolutional neural network (ConvNet) with three types of noise: random translations and horizontal flips of the input images, Gaussian noise on the input layer, and dropout applied within the network. We use mean squared error as the consistency cost and ramp up its weight from 0 to its final value during the first 80 epochs. The details of the model and the training procedure are described in Appendix B.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Comparison to other methods on SVHN and CIFAR-10</head><p>We ran experiments using the Street View House Numbers (SVHN) and CIFAR-10 benchmarks <ref type="bibr" target="#b15">[16]</ref>.</p><p>Both datasets contain 32x32 pixel RGB images belonging to ten different classes. In SVHN, each example is a close-up of a house number, and the class represents the identity of the digit at the center of the image. In CIFAR-  <ref type="bibr" target="#b14">[15]</ref> performs even better than Mean Teacher on the 1000-label SVHN and the 4000-label CIFAR-10. As discussed in the introduction, VAT and Mean Teacher are complimentary approaches. Their combination may yield better accuracy than either of them alone, but that investigation is beyond the scope of this paper.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SVHN with extra unlabeled data</head><p>Above, we suggested that Mean Teacher scales well to large datasets and on-line learning. In addition, the SVHN and CIFAR-10 results indicate that it uses unlabeled examples efficiently. Therefore, we wanted to test whether we have reached the limits of our approach. Table <ref type="table" target="#tab_3">3</ref> shows the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Analysis of the training curves</head><p>The training curves on Figure <ref type="figure" target="#fig_2">3</ref> help us understand the effects of using Mean Teacher. As expected, the EMA-weighted models (blue and dark gray curves in the bottom row) give more accurate predictions than the bare student models (orange and light gray) after an initial period.</p><p>Using the EMA-weighted model as the teacher improves results in the semi-supervised settings. There appears to be a virtuous feedback cycle of the teacher (blue curve) improving the student (orange) via the consistency cost, and the student improving the teacher via exponential moving averaging. If this feedback cycle is detached, the learning is slower, and the model starts to overfit earlier (dark gray and light gray).</p><p>Mean Teacher helps when labels are scarce. When using 500 labels (middle column) Mean Teacher learns faster, and continues training after the ⇧ model stops improving. On the other hand, in the all-labeled case (left column), Mean Teacher and the ⇧ model behave virtually identically.</p><p>Figure <ref type="figure">4</ref>: Validation error on 250-label SVHN over four runs per hyperparameter setting and their means. In each experiment, we varied one hyperparameter, and used the evaluation run hyperparameters of Table <ref type="table" target="#tab_0">1</ref> for the rest. The hyperparameter settings used in the evaluation runs are marked with the bolded font weight. See the text for details.</p><p>Mean Teacher uses unlabeled training data more efficiently than the ⇧ model, as seen in the middle column. On the other hand, with 500k extra unlabeled examples (right column), ⇧ model keeps improving for longer. Mean Teacher learns faster, and eventually converges to a better result, but the sheer amount of data appears to offset ⇧ model's worse predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation experiments</head><p>To assess the importance of various aspects of the model, we ran experiments on SVHN with 250 labels, varying one or a few hyperparameters at a time while keeping the others fixed.</p><p>Removal of noise (Figures <ref type="figure">4(a</ref>) and 4(b)). In the introduction and Figure <ref type="figure" target="#fig_0">1</ref>, we presented the hypothesis that the ⇧ model produces better predictions by adding noise to the model on both sides. But after the addition of Mean Teacher, is noise still needed? Yes. We can see that either input augmentation or dropout is necessary for passable performance. On the other hand, input noise does not help when augmentation is in use. Dropout on the teacher side provides only a marginal benefit over just having it on the student side, at least when input augmentation is in use. Sensitivity to EMA decay and consistency weight (Figures <ref type="figure">4(c</ref>) and 4(d)). The essential hyperparameters of the Mean Teacher algorithm are the consistency cost weight and the EMA decay ↵. How sensitive is the algorithm to their values? We can see that in each case the good values span roughly an order of magnitude and outside these ranges the performance degrades quickly. Note that EMA decay ↵ = 0 makes the model a variation of the ⇧ model, although somewhat inefficient one because the gradients are propagated through only the student path. Note also that in the evaluation runs we used EMA decay ↵ = 0.99 during the ramp-up phase, and ↵ = 0.999 for the rest of the training. We chose this strategy because the student improves quickly early in the training, and thus the teacher should forget the old, inaccurate, student weights quickly. Later the student improvement slows, and the teacher benefits from a longer memory.</p><p>Decoupling classification and consistency (Figure <ref type="figure">4</ref>(e)). The consistency to teacher predictions may not necessarily be a good proxy for the classification task, especially early in the training. So far our model has strongly coupled these two tasks by using the same output for both. How would decoupling the tasks change the performance of the algorithm? To investigate, we changed the model to have two top layers and produce two outputs. We then trained one of the outputs for classification and the other for consistency. We also added a mean squared error cost between the output logits, and then varied the weight of this cost, allowing us to control the strength of the coupling. Looking at the results (reported using the EMA version of the classification output), we can see that the strongly coupled version performs well and the too loosely coupled versions do not. On the other hand, a moderate decoupling seems to have the benefit of making the consistency ramp-up redundant. Changing from MSE to KL-divergence (Figure <ref type="figure">4</ref>(f)) Following Laine &amp; Aila <ref type="bibr" target="#b12">[13]</ref>, we use mean squared error (MSE) as our consistency cost function, but KL-divergence would seem a more natural choice. Which one works better? We ran experiments with instances of a cost function family ranging from MSE (⌧ = 0 in the figure) to KL-divergence (⌧ = 1), and found out that in this setting MSE performs better than the other cost functions. See Appendix C for the details of the cost function family and for our intuition about why MSE performs so well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Mean Teacher with residual networks on CIFAR-10 and ImageNet</head><p>In the experiments above, we used a traditional 13-layer convolutional architecture (ConvNet), which has the benefit of making comparisons to earlier work easy. In order to explore the effect of the model architecture, we ran experiments using a 12-block (26-layer) Residual Network <ref type="bibr" target="#b7">[8]</ref> (ResNet) with Shake-Shake regularization <ref type="bibr" target="#b4">[5]</ref> on CIFAR-10. The details of the model and the training procedure are described in Appendix B.2. As shown in Table <ref type="table" target="#tab_4">4</ref>, the results improve remarkably with the better network architecture.</p><p>To test whether the methods scales to more natural images, we ran experiments on Imagenet 2012 dataset <ref type="bibr" target="#b20">[21]</ref> using 10% of the labels. We used a 50-block (152-layer) ResNeXt architecture <ref type="bibr" target="#b31">[32]</ref>, and saw a clear improvement over the state of the art. As the test set is not publicly available, we measured the results using the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related work</head><p>Noise regularization of neural networks was proposed by Sietsma &amp; Dow <ref type="bibr" target="#b24">[25]</ref>. More recently, several types of perturbations have been shown to regularize intermediate representations effectively in deep learning. Adversarial Training <ref type="bibr" target="#b5">[6]</ref> changes the input slightly to give predictions that are as different as possible from the original predictions. Dropout <ref type="bibr" target="#b26">[27]</ref> zeroes random dimensions of layer outputs. Dropconnect <ref type="bibr" target="#b29">[30]</ref> generalizes Dropout by zeroing individual weights instead of activations.</p><p>Stochastic Depth <ref type="bibr" target="#b10">[11]</ref> drops entire layers of residual networks, and Swapout <ref type="bibr" target="#b25">[26]</ref> generalizes Dropout and Stochastic Depth. Shake-shake regularization <ref type="bibr" target="#b4">[5]</ref> duplicates residual paths and samples a linear combination of their outputs independently during forward and backward passes.</p><p>Several semi-supervised methods are based on training the model predictions to be consistent to perturbation. The Denoising Source Separation framework (DSS) <ref type="bibr" target="#b27">[28]</ref> uses denoising of latent variables to learn their likelihood estimate. The variant of Ladder Network <ref type="bibr" target="#b19">[20]</ref> implements DSS with a deep learning model for classification tasks. It produces a noisy student predictions and clean teacher predictions, and applies a denoising layer to predict teacher predictions from the student predictions. The ⇧ model <ref type="bibr" target="#b12">[13]</ref> improves the model by removing the explicit denoising layer and applying noise also to the teacher predictions. Similar methods had been proposed already earlier for linear models <ref type="bibr" target="#b28">[29]</ref> and deep learning <ref type="bibr" target="#b1">[2]</ref>. Virtual Adversarial Training <ref type="bibr" target="#b14">[15]</ref> is similar to the ⇧ model but uses adversarial perturbation instead of independent noise.</p><p>The idea of a teacher model training a student is related to model compression <ref type="bibr" target="#b2">[3]</ref> and distillation <ref type="bibr" target="#b8">[9]</ref>. The knowledge of a complicated model can be transferred to a simpler model by training the simpler model with the softmax outputs of the complicated model. The softmax outputs contain more information about the task than the one-hot outputs, and the requirement of representing this knowledge regularizes the simpler model. Besides its use in model compression, distillation can be used to harden trained models against adversarial attacks <ref type="bibr" target="#b16">[17]</ref>. The difference between distillation and consistency regularization is that distillation is performed after training whereas consistency regularization is performed on training time.</p><p>Consistency regularization can be seen as a form of label propagation <ref type="bibr" target="#b32">[33]</ref>. Training samples that resemble each other are more likely to belong to the same class. Label propagation takes advantage of this assumption by pushing label information from each example to examples that are near it according to some metric. Label propagation can also be applied to deep learning models <ref type="bibr" target="#b30">[31]</ref>. However, ordinary label propagation requires a predefined distance metric in the input space. In contrast, consistency targets employ a learned distance metric implied by the abstract representations of the model. As the model learns new features, the distance metric changes to accommodate these features. Therefore, consistency targets guide learning in two ways. On the one hand they spread the labels according to the current distance metric, and on the other hand, they aid the network learn a better distance metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Temporal Ensembling, Virtual Adversarial Training and other forms of consistency regularization have recently shown their strength in semi-supervised learning. In this paper, we propose Mean Teacher, a method that averages model weights to form a target-generating teacher model. Unlike Temporal Ensembling, Mean Teacher works with large datasets and on-line learning. Our experiments suggest that it improves the speed of learning and the classification accuracy of the trained network.</p><p>In addition, it scales well to state-of-the-art architectures and large image sizes.</p><p>The success of consistency regularization depends on the quality of teacher-generated targets. If the targets can be improved, they should be. Mean Teacher and Virtual Adversarial Training represent two ways of exploiting this principle. Their combination may yield even better targets. There are probably additional methods to be uncovered that improve targets and trained models even further.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A sketch of a binary classification task with two labeled examples (large blue dots) and one unlabeled example, demonstrating how the choice of the unlabeled target (black circle) affects the fitted function (gray curve). (a) A model with no regularization is free to fit any function that predicts the labeled training examples well. (b) A model trained with noisy labeled data (small dots) learns to give consistent predictions around labeled data points. (c) Consistency to noise around unlabeled examples provides additional smoothing. For the clarity of illustration, the teacher model (gray curve) is first fitted to the labeled examples, and then left unchanged during the training of the student model. Also for clarity, we will omit the small dots in figures d and e. (d) Noise on the teacher model reduces the bias of the targets without additional training. The expected direction of stochastic gradient descent is towards the mean (large blue circle) of individual noisy targets (small blue circles).(e) An ensemble of models gives an even better expected target. Both Temporal Ensembling and the Mean Teacher method use this approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The Mean Teacher method. The figure depicts a training batch with a single labeled example.Both the student and the teacher model evaluate the input applying noise (⌘, ⌘ 0 ) within their computation. The softmax output of the student model is compared with the one-hot label using classification cost and with the teacher output using consistency cost. After the weights of the student model have been updated with gradient descent, the teacher model weights are updated as an exponential moving average of the student weights. Both model outputs can be used for prediction, but at the end of the training the teacher prediction is more likely to be correct. A training step with an unlabeled example would be similar, except no classification cost would be applied.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Smoothened classification cost (top) and classification error (bottom) of Mean Teacher and our baseline ⇧ model on SVHN over the first 100000 training steps. In the upper row, the training classification costs are measured using only labeled data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Besides the primary training data, SVHN includes also an extra dataset of 531131 examples. We picked 500 samples from the primary training as our labeled training examples. We used the rest of the primary training set together with the extra training set as unlabeled examples. We ran experiments with Mean Teacher and our baseline ⇧ model, and used either 0, 100000 or 500000 extra examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Error rate percentage on SVHN over 10 runs (4 runs when using all labels). We use exponential moving average weights in the evaluation of all our models. All the methods use a similar 13-layer ConvNet architecture. See Table5in the Appendix for results without input augmentation.</figDesc><table><row><cell></cell><cell>250 labels</cell><cell>500 labels</cell><cell>1000 labels</cell><cell>73257 labels</cell></row><row><cell></cell><cell>73257 images</cell><cell>73257 images</cell><cell>73257 images</cell><cell>73257 images</cell></row><row><cell cols="2">GAN [24] ⇧ model [13] Temporal Ensembling [13] VAT+EntMin [15]</cell><cell>18.44 ± 4.8 6.65 ± 0.53 5.12 ± 0.13</cell><cell>8.11 ± 1.3 4.82 ± 0.17 4.42 ± 0.16 3.86 3.86 3.86</cell><cell>2.54 ± 0.04 2.74 ± 0.06</cell></row><row><cell>Supervised-only ⇧ model Mean Teacher</cell><cell>27.77 ± 3.18 9.69 ± 0.92 4.35 ± 0.50 4.35 ± 0.50 4.35 ± 0.50</cell><cell>16.88 ± 1.30 6.83 ± 0.66 4.18 ± 0.27 4.18 ± 0.27 4.18 ± 0.27</cell><cell>12.32 ± 0.95 4.95 ± 0.26 3.95 ± 0.19</cell><cell>2.75 ± 0.10 2.50 ± 0.07 2.50 ± 0.05 2.50 ± 0.05 2.50 ± 0.05</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Error rate percentage on CIFAR-10 over 10 runs (4 runs when using all labels).</figDesc><table><row><cell></cell><cell>1000 labels</cell><cell>2000 labels</cell><cell>4000 labels</cell><cell>50000 labels</cell></row><row><cell></cell><cell>50000 images</cell><cell>50000 images</cell><cell>50000 images</cell><cell>50000 images</cell></row><row><cell cols="2">GAN [24] ⇧ model [13] Temporal Ensembling [13] VAT+EntMin [15]</cell><cell></cell><cell>18.63 ± 2.32 12.36 ± 0.31 12.16 ± 0.31 10.55 10.55 10.55</cell><cell>5.56 ± 0.10 5.60 ± 0.10 5.60 ± 0.10 5.60 ± 0.10</cell></row><row><cell>Supervised-only ⇧ model Mean Teacher</cell><cell>46.43 ± 1.21 27.36 ± 1.20 21.55 ± 1.48 21.55 ± 1.48 21.55 ± 1.48</cell><cell>33.94 ± 0.73 18.02 ± 0.60 15.73 ± 0.31 15.73 ± 0.31 15.73 ± 0.31</cell><cell>20.66 ± 0.57 13.20 ± 0.27 12.31 ± 0.28</cell><cell>5.82 ± 0.15 6.06 ± 0.11 5.94 ± 0.15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>10, each example is a natural image belonging to a class such as horses, cats, cars and airplanes. SVHN contains of 73257 training samples and 26032 test samples. CIFAR-10 consists of 50000 training samples and 10000 test samples. Tables 1 and 2 compare the results against recent state-of-the-art methods. All the methods in the comparison use a similar 13-layer ConvNet architecture. Mean Teacher improves test accuracy over the ⇧ model and Temporal Ensembling on semi-supervised SVHN tasks. Mean Teacher also improves results on CIFAR-10 over our baseline ⇧ model.</figDesc><table /><note>The recently published version of Virtual Adversarial Training by Miyato et al.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Error percentage over 10 runs on SVHN with extra unlabeled training data.</figDesc><table><row><cell></cell><cell>500 labels</cell><cell>500 labels</cell><cell>500 labels</cell></row><row><cell></cell><cell>73257 images</cell><cell>173257 images</cell><cell>573257 images</cell></row><row><cell>⇧ model (ours) Mean Teacher</cell><cell>6.83 ± 0.66 4.18 ± 0.27 4.18 ± 0.27 4.18 ± 0.27</cell><cell>4.49 ± 0.27 3.02 ± 0.16 3.02 ± 0.16 3.02 ± 0.16</cell><cell>3.26 ± 0.14 2.46 ± 0.06 2.46 ± 0.06 2.46 ± 0.06</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Error rate percentage of ResNet Mean Teacher compared to the state of the art. We report the test results from 10 runs on CIFAR-10 and validation results from 2 runs on ImageNet.</figDesc><table><row><cell></cell><cell>CIFAR-10</cell><cell>ImageNet 2012</cell></row><row><cell></cell><cell>4000 labels</cell><cell>10% of the labels</cell></row><row><cell>State of the art ConvNet Mean Teacher ResNet Mean Teacher State of the art using all labels</cell><cell>10.55 [15] 12.31 ± 0.28 6.28 ± 0.15 6.28 ± 0.15 6.28 ± 0.15 2.86 [5]</cell><cell>35.24 ± 0.90 [19] 9.11 ± 0.12 9.11 ± 0.12 9.11 ± 0.12 3.79 [10]</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Samuli Laine and Timo Aila for fruitful discussions about their work, and Phil Bachman and Colin Raffel for corrections to the pre-print version of this paper. We also thank everyone at The Curious AI Company for their help, encouragement, and ideas.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><surname>Eugene</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Zhifeng</surname></persName>
		</author>
		<author>
			<persName><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><surname>Craig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devin</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName><surname>Matthieu</surname></persName>
		</author>
		<author>
			<persName><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><surname>Harp</surname></persName>
		</author>
		<author>
			<persName><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><surname>Yangqing</surname></persName>
		</author>
		<author>
			<persName><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><surname>Rafal</surname></persName>
		</author>
		<author>
			<persName><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><surname>Lukasz</surname></persName>
		</author>
		<author>
			<persName><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><surname>Josh</surname></persName>
		</author>
		<author>
			<persName><surname>Mané</surname></persName>
		</author>
		<author>
			<persName><surname>Dan</surname></persName>
		</author>
		<author>
			<persName><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><surname>Rajat</surname></persName>
		</author>
		<author>
			<persName><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><surname>Sherry</surname></persName>
		</author>
		<author>
			<persName><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><surname>Derek</surname></persName>
		</author>
		<author>
			<persName><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><surname>Chris</surname></persName>
		</author>
		<author>
			<persName><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><surname>Mike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><surname>Benoit</surname></persName>
		</author>
		<author>
			<persName><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><surname>Kunal</surname></persName>
		</author>
		<author>
			<persName><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><surname>Vijay</surname></persName>
		</author>
		<author>
			<persName><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><surname>Fernanda</surname></persName>
		</author>
		<author>
			<persName><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><surname>Pete</surname></persName>
		</author>
		<author>
			<persName><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Tensorflow</surname></persName>
		</author>
		<title level="m">Large-Scale Machine Learning on Heterogeneous Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName><surname>Ouais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.4864</idno>
		<idno>arXiv: 1412.4864</idno>
		<title level="m">Learning with Pseudo-Ensembles</title>
				<imprint>
			<date type="published" when="2014-12">December 2014</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Alexandru. Model compression</title>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Buciluǎ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niculescu-Mizil</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning</title>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
				<meeting>The 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Gastaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07485</idno>
		<idno>arXiv: 1705.07485</idno>
		<title level="m">Shake-Shake regularization</title>
				<imprint>
			<date type="published" when="2017-05">May 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Explaining and Harnessing Adversarial Examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014-12">December 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><surname>Geoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04599</idno>
		<idno>arXiv: 1706.04599</idno>
		<title level="m">On Calibration of Modern Neural Networks</title>
				<imprint>
			<date type="published" when="2017-06">June 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><surname>Shaoqing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<idno>arXiv: 1512.03385</idno>
		<title level="m">Deep Residual Learning for Image Recognition</title>
				<imprint>
			<date type="published" when="2015-12">December 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<idno>arXiv: 1503.02531</idno>
		<title level="m">Distilling the Knowledge in a Neural Network</title>
				<imprint>
			<date type="published" when="2015-03">March 2015</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Squeeze-And-Excitation</surname></persName>
		</author>
		<author>
			<persName><surname>Networks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<idno>arXiv: 1709.01507</idno>
		<imprint>
			<date type="published" when="2017-09">September 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.09382</idno>
		<idno>arXiv: 1603.09382</idno>
		<title level="m">Deep Networks with Stochastic Depth</title>
				<imprint>
			<date type="published" when="2016-03">March 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<idno>arXiv: 1412.6980</idno>
		<title level="m">A Method for Stochastic Optimization</title>
				<imprint>
			<date type="published" when="2014-12">December 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02242</idno>
		<idno>arXiv: 1610.02242</idno>
		<title level="m">Temporal Ensembling for Semi-Supervised Learning</title>
				<imprint>
			<date type="published" when="2016-10">October 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Awni</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
				<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Virtual Adversarial Training: a Regularization Method for Supervised and Semi-supervised Learning</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><surname>Shin-Ichi, Koyama</surname></persName>
		</author>
		<author>
			<persName><surname>Masanori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin</forename><surname>Ishii</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03976</idno>
		<idno>arXiv: 1704.03976</idno>
		<imprint>
			<date type="published" when="2017-04">April 2017</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><surname>Somesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04508</idno>
		<idno>arXiv: 1511.04508</idno>
		<title level="m">Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks</title>
				<imprint>
			<date type="published" when="2015-11">November 2015</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Acceleration of Stochastic Approximation by Averaging</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Juditsky</surname></persName>
		</author>
		<idno type="DOI">10.1137/0330046</idno>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Control Optim</title>
		<idno type="ISSN">0363-0129</idno>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="838" to="855" />
			<date type="published" when="1992-07">July 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Variational Autoencoder for Deep Learning of Images, Labels and Captions</title>
		<author>
			<persName><forename type="first">Yunchen</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><surname>Zhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Chunyuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08976</idno>
		<idno>arXiv: 1609.08976</idno>
		<imprint>
			<date type="published" when="2016-09">September 2016</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semisupervised Learning with Ladder Networks</title>
		<author>
			<persName><forename type="first">Antti</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName><surname>Mikko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tapani</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="3546" to="3554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><surname>Su</surname></persName>
		</author>
		<author>
			<persName><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><surname>Sanjeev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhiheng</surname></persName>
		</author>
		<author>
			<persName><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><surname>Andrej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0575</idno>
		<idno>arXiv: 1409.0575</idno>
		<title level="m">ImageNet Large Scale Visual Recognition Challenge</title>
				<imprint>
			<date type="published" when="2014-09">September 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Regularization With Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehran</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tolga</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1163" to="1171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="901" to="901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><surname>Wojciech</surname></persName>
		</author>
		<author>
			<persName><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><surname>Vicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2226" to="2234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Creating artificial neural networks that generalize</title>
		<author>
			<persName><forename type="first">Jocelyn</forename><surname>Sietsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Dow</surname></persName>
		</author>
		<author>
			<persName><surname>Jf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="79" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName><surname>Swapout</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06465</idno>
		<idno>arXiv: 1605.06465</idno>
		<title level="m">Learning an ensemble of deep architectures</title>
				<imprint>
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<idno type="ISSN">1532-4435</idno>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014-01">January 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Denoising Source Separation</title>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Särelä</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<idno type="ISSN">1533-7928</idno>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="233" to="272" />
			<date type="published" when="2005-03">Mar. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Wager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1307.1493</idno>
		<idno>arXiv: 1307.1493</idno>
		<title level="m">Dropout Training as Adaptive Regularization</title>
				<imprint>
			<date type="published" when="2013-07">July 2013</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Sixin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<title level="m">Regularization of Neural Networks using DropConnect</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep learning via semi-supervised embedding</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName><surname>Frédéric</surname></persName>
		</author>
		<author>
			<persName><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName><surname>Hossein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="639" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Aggregated Residual Transformations for Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><surname>Piotr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05431</idno>
		<idno>arXiv: 1611.05431</idno>
		<imprint>
			<date type="published" when="2016-11">November 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning from labeled and unlabeled data with label propagation</title>
		<author>
			<persName><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
